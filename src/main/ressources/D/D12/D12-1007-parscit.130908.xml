<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<title confidence="0.98123">
Generative Goal-Driven User Simulation for Dialog Management
</title>
<author confidence="0.730582">
Aciel Eshky
</author>
<affiliation confidence="0.881931">
ILCC
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.984443">
a.eshky@sms.ed.ac.uk
</email>
<author confidence="0.611388">
Ben Allison
</author>
<affiliation confidence="0.818363666666667">
ILCC
School of Informatics
University of Edinburgh
</affiliation>
<author confidence="0.864203">
Mark Steedman
</author>
<affiliation confidence="0.919108333333333">
ILCC
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.990125">
{ballison, steedman}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99458" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998234423076923">
User simulation is frequently used to train
statistical dialog managers for task-oriented
domains. At present, goal-driven simula-
tors (those that have a persistent notion of
what they wish to achieve in the dialog) re-
quire some task-specific engineering, making
them impossible to evaluate intrinsically. In-
stead, they have been evaluated extrinsically
by means of the dialog managers they are in-
tended to train, leading to circularity of argu-
ment. In this paper, we propose the first fully
generative goal-driven simulator that is fully
induced from data, without hand-crafting or
goal annotation. Our goals are latent, and take
the form of topics in a topic model, clustering
together semantically equivalent and phoneti-
cally confusable strings, implicitly modelling
synonymy and speech recognition noise. We
evaluate on two standard dialog resources,
the Communicator and Let’s Go datasets, and
demonstrate that our model has substantially
better fit to held out data than competing ap-
proaches. We also show that features derived
from our model allow significantly greater im-
provement over a baseline at distinguishing
real from randomly permuted dialogs.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99962875">
Automatically simulating user behaviour in human-
machine dialogs has become vital for training sta-
tistical dialog managers in task-oriented domains.
These managers are often trained with some vari-
ant of reinforcement learning (Sutton and Barto,
1998), where optimal behaviour is sought or learnt
through the exploration of the space of possible di-
alogs. Although learning by interacting with human
</bodyText>
<page confidence="0.97957">
71
</page>
<bodyText confidence="0.99997275">
subjects is a possibility (Ga˘si´c et al., 2011), it has
been argued that user simulation avoids the expen-
sive, labour intensive, and error-prone experience of
exposing real humans to fledgling dialog systems
(Eckert et al., 1997).
Training effective dialog managers should benefit
from exposure to properties exhibited by real users.
Table 1 shows an example dialog in a domain such
as we consider, where the objective is to simulate at
the semantic level. In such task oriented domains,
the user has a goal (in this case, to book a flight
from New York to Osaka), and the machine is tasked
with fulfilling it. Notice that the user is consistent
with this goal throughout the dialog, in that they do
not provide contradictory information (although an
ASR error is present), but that every mention of their
destination city uses a different string. This moti-
vates our first desideratum: that simulation be con-
sistent over the course of a dialog. Furthermore, one
can imagine users not always responding identically
in identical situations: we thus additionally require
variability. In this paper we demonstrate a fully gen-
erative, latent variable probability model exhibiting
both of these properties.
Thus far, consistent simulators have been par-
tially deterministic and have required some hand-
engineering. As a result, it has only been possible to
evaluate them extrinsically using dialog managers.
This is circular because we need simulators to train
managers, but need managers to evaluate simulators.
The issue is that judgements of quality of each de-
pend on the specifics of the other and that a proper
evaluation of one depends on the correct function-
ing of the other. Furthermore, there is little reason to
assume that because a simulator performs well with
a certain dialog manager, it would perform similarly
</bodyText>
<note confidence="0.994177666666667">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 71–81, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
Speech Semantic Representation
M: Hello, How Can I help? M: GREETING
M: META REQUEST INFO
U: A trip from New York City to Osaka, U: PROVIDE orig city New York City
please. U: PROVIDE dest city Salt Lake City
M: Leaving from New York City to Salt Lake M: IMPLICIT CONFIRM orig dest city
City. What day would you like to travel? M: REQUEST depart date
U: No, no. Leaving from New York to Osaka U: NO ANSWER null no
in Japan. U: PROVIDE orig city New York
U: PROVIDE dest city Osaka Japan
M: Leaving from New York to Osaka Japan, M: EXPLICIT CONFIRM orig city
correct? M: EXPLICIT CONFIRM dest city
U: Yes. U: YES ANSWER null yes
</note>
<tableCaption confidence="0.996147">
Table 1: An example of a dialog in speech and its semantic equivalent. M and U denote machine and user utterances
</tableCaption>
<bodyText confidence="0.997005913043478">
respectively. Note how a single speech utterance is split by the semantic parser into multiple logical utterances, each
of which is broken down to an ACT, slot, and value. We consider resources where gold standard transcriptions are not
available; thus there will be speech recognition noise, e.g. Osaka rendered as Salt Lake City, something our model
is able to capture.
well with other managers. In contrast, a probabilistic
formulation such as we propose allows us to evalu-
ate our models intrinsically using standard machine
learning metrics, and without reference to a specific
manager, thus breaking the circularity, and guarding
against such experimental biases.
We demonstrate the efficacy of our model on
two tasks, and compare it to two other approaches.
Firstly we use a standard bigram model as conceived
by Eckert et al. (1997) and Levin and Pieraccini
(2000); secondly we compare to a probabilistic goal-
based simulator where the goals are string literals,
as envisaged by Scheffler and Young (2002) and
Schatzmann et al. (2007b). We demonstrate sub-
stantial improvement over these models in terms of
predicting heldout data on two standard dialog re-
sources: DARPA Communicator (Levin et al., 2000;
Georgila et al., 2005b) and Let’s Go (Black and Es-
kenazi, 2009).
</bodyText>
<sectionHeader confidence="0.999873" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.783339">
2.1 Related Work on User Simulation
</subsectionHeader>
<bodyText confidence="0.999975424242424">
User simulation as a stochastic process was first en-
visioned by Eckert et al. (1997): their Bigram model
conditions user utterances exclusively on the pre-
ceding machine utterance. This was extended by
Levin and Pieraccini (2000), who manually restrict
the model to estimating “sensible” pairs of user and
machine utterances by assigning all others probabil-
ity zero.
Bigram models ensure that a locally sensible
response to a machine utterance is provided by
the simulator; however, they do not ensure that
it provides responses consistent with one another
throughout the dialog. Several approaches have at-
tempted to overcome this problem. Pietquin (2004),
for example, explicitly models a user goal as a set
of slot-value pairs randomly generated once per dia-
log. He then hand selects parameters to ensure that
the user’s actions are in accordance with their goal.
Jung et al. (2009) use large amounts of dialog
state annotations (e.g. what information has been
provided so far) to learn Conditional Random Fields
over the user utterances, and assume that those fea-
tures ensure user consistency. Georgila et al. (2005a)
instead consider only act-slot pairs, and thus incon-
sistency is not a factor.
Scheffler and Young (2002) simulate user be-
haviour by introducing rules for actions that depend
on the user goal, and probabilistic modelling for ac-
tions that are not goal-dependent. They then map
out a decision network that determines user actions
at every node prior to the start of the dialog. Agenda-
based user simulation, another approach from the lit-
erature, assumes a probability distribution over the
</bodyText>
<page confidence="0.996788">
72
</page>
<bodyText confidence="0.999956428571429">
user goal which is either induced from data (Schatz-
mann et al., 2007b), or is manually set when no data
is available (Schatzmann et al., 2007a). An agenda,
which is a stack-like structure of utterances to be
produced given the goal, is then devised determin-
istically. Keizer et al. (2010) combine the decision
network with the agenda and goal to allow for some
variability for some actions. These models ensure
consistency but restrict the variability in user be-
haviour that can be accommodated. Furthermore,
because these approaches do not define a complete
probability distribution over user behaviour, they re-
strict possibilities for their evaluation, a point to
which we now turn.
</bodyText>
<subsectionHeader confidence="0.728182">
2.2 Related Work on Simulator Evaluation
</subsectionHeader>
<bodyText confidence="0.999833179487179">
No standardised metric of evaluation has been estab-
lished for user simulators largely because they have
been so inextricably linked to dialog managers. The
most popular method of evaluation relies on gener-
ating synthetic dialogs through the interaction of the
user simulator with some dialog manager. Schatz-
mann et al. (2005) hand-craft a simple determin-
istic dialog manager based on finite automata, and
compute similarity measures between these synthet-
ically produced dialogs and real dialogs. Georgila
et al. (2006) use a scoring function to evaluate syn-
thetic dialogs using accuracy, precision, recall, and
perplexity, while Schatzmann et al. (2007b) rely
on dialog completion rates. Williams (2008) use
a Cramer–von Mises test, a hypothesis test to de-
termine whether simulated and real dialogs are sig-
nificantly different, while Janarthanam and Lemon
(2009) use Kullback Leibler Divergence between the
empirical distributions over acts in real and simu-
lated dialogs. Singh et al. (2000) and Ai and Lit-
man (2008) judge the consistency of human quality
ranked synthetic dialogs generated by different sim-
ulators interacting with the IT-SPOKE dialog sys-
tem.
Schatzmann et al. (2007b) use a simulator to train
a statistical dialog manager and then evaluate the
learned policy. Because this only indirectly evalu-
ates the simulator, it is inappropriate as a sole mea-
sure of quality.
There has been far less evaluation of simulators
without a dialog manager. The main approach is
to compute precision and recall on an utterance ba-
sis, which is intended to measure the similarity be-
tween real user responses in the corpora and simu-
lated user responses produced under similar circum-
stances (Schatzmann et al., 2005; Georgila et al.,
2006). However, this is a harsh evaluation as it as-
sumes a correct or “best” answer, and penalises valid
variability in user behaviour.
</bodyText>
<sectionHeader confidence="0.846143" genericHeader="method">
3 Dialog as a Statistical Process
</sectionHeader>
<bodyText confidence="0.999517307692308">
We consider a dialog to be a series of turns, com-
prised of multiple utterances. Each Utterance con-
sists of an ACT, a slot, and a value, as shown in Ta-
ble 1. Dialogs proceed by the user and the machine
alternating turns. Because the dialogs are of mixed
initiative, there is no restriction on the number of
contiguous machine or user utterances.
Our aim is to model the user, and are interested
in the conditional distribution of the user utterances
given the dialog up to that point. In other words, we
are interested in the distribution p (ui|d1 ... di−1),
where dn is either a machine utterance mn or a user
utterance un.
</bodyText>
<subsectionHeader confidence="0.508118">
4 Models of Users in Dialogs
</subsectionHeader>
<bodyText confidence="0.999950142857143">
This section describes several models of increas-
ing complexity: a Bigram model, which serves as
a baseline; an upper-bound on String-Goal models,
which we design to mimic the behaviour of previous
goal-based approaches, but with a probabilistic for-
mulation; and finally our approach, the Topic-Goal
model.
</bodyText>
<subsectionHeader confidence="0.979078">
4.1 Bigram Model
</subsectionHeader>
<bodyText confidence="0.999858">
The simplest model we define over dialogs is the bi-
gram model of Eckert et al. (1997):
</bodyText>
<equation confidence="0.976950333333333">
p (ui|m) = p (ui|mi−1) (1)
p (u|m) = � p (ui|m) (2)
i
</equation>
<bodyText confidence="0.999947714285714">
The probability of each user utterance ui (the com-
plete {ACT, slot, value} triple) is dependent only on
the machine utterance immediately preceding it (the
slight abuse of notation mi−1 here does not mean the
utterance at i−1 in the machine utterance list, but the
utterance immediately preceding the i-th), and utter-
ances in the dialog are conditionally independent of
</bodyText>
<page confidence="0.993436">
73
</page>
<bodyText confidence="0.9989572">
one another. (Georgila et al. (2006) found no bene-
fit from increasing the Markov horizon). Since each
utterance is generated independently of others in the
dialog with the same context, there is no enforced
consistency between utterances.
Since we require a distribution over all possible
utterances, assigning non–zero probability to cases
outside of the training data, our bigram model is in-
terpolated with a unigram model, which itself is in-
terpolated with a smoothing model which assumes
independence between the act, slot, and value el-
ements of the utterance. Interpolation weights are
set to maximise probability of a development set of
dialogs. Each sub-model uses the maximum like-
lihood estimator (the relative frequency of the ut-
terance), and unseen machine utterances place full
weight on the unigram/smoothed model (ignoring
the bigram probability since it has no meaning if
mi−1 is unobserved). We label this model the Bi-
gram model in subsequent experiments.
</bodyText>
<subsectionHeader confidence="0.872601">
4.2 Goal-Based Models
</subsectionHeader>
<bodyText confidence="0.999988125">
One way to ensure consistency and more realistic
behaviour is to have a goal for the user in the dia-
log, which corresponds to values for slots required in
the problem. For instance, they might be the origin
and destination cities in a flight booking domain. In
standard machine learning terms, the goal becomes
a latent variable g in a probability model. We can
then define a distribution over utterances as:
</bodyText>
<equation confidence="0.996811333333333">
p (ui|m, g) = p (ui|mi−1, g) (3)
p (u|m) = 1: p(g) � p (ui|mi−1, g) (4)
g i
</equation>
<subsectionHeader confidence="0.99677">
4.3 An Upper-Bound on String-Goal Models
</subsectionHeader>
<bodyText confidence="0.995964525423729">
The simplest variant of g has string values for each
of the slots the user is required to provide in order
for the dialog to succeed. Thus we may have:
g = [orig city: New York; dest city: Osaka]
as presented in Schatzmann et al. (2005) and Schatz-
mann et al. (2007b). However, in these simulators,
while the goal is probabilistic, there is no distribu-
tion over utterances given the goal because utter-
ances are assembled deterministically from a series
of rule applications. There is also no marginalisation
over the goal as in (4) above.
The issue with a model of user goals as strings
in this fashion is that users describe the same val-
ues in multiple ways (Osaka Japan, Osaka), and
speech recognition errors corrupt consistent user
input (Osaka mis-recognised as Salt Lake City).
Users also might legitimately switch their goals mid-
dialog. Inference in the model would have allow
for these possibilities: we would have to marginalise
over all possible goal switches.
For the sake of comparison, we compute an upper-
bound on string-goal models, which gives a flavour
for how such models would perform optimistically.
The upper-bound assigns probability to dialogs as
follows: for each utterance ui if the corresponding
value vi has been seen before in the dialog, the prob-
ability used for that utterance is just p (ai, si|mi−1),
that is, the probability of the act ai and slot si only;
there is no penalty for repetition of the value. If the
value is unseen in the dialog, we use the full proba-
bility of the utterance from the bigram model as de-
scribed above. This is optimistic because there is no
penalty for repeated goal changes besides that im-
posed by the bigram model itself, and no penalty is
imposed for choosing between previously sampled
goals as would be necessary in a probability model.
Any string-based model necessarily assigns lower
probabilities to data than the upper bound, because
it would penalise goal changes (in a probabilistic
sense; that is, there would be a term to reflect the
probability of some new goal given the old) to al-
low for the discrepancy in values present in dialogs.
In contrast, our upper bound does not include such
a term. Furthermore, once multiple goal values had
been uttered in the dialog, we would have to sample
one to use for the next utterance, which would again
incur some cost: again, we do not have such a cost
in our upper bound.
We could in theory use an external model of noise
to account for these value discrepancies (and the
ASR errors we model in the next section). However,
this would further decrease the probability, as some
probability mass currently assigned to the heldout
data would have to be reserved for the possibility of
string renderings other than those we observe.
It bears reiterating that our upper bound on string-
goals is not a generative model: however, it allows
us to assign probabilities to unseen data (albeit op-
timistically), and thus provides us with a point of
</bodyText>
<page confidence="0.993541">
74
</page>
<bodyText confidence="0.999245">
comparison. Although not technically a model, we
refer to this as the String-Goal model for the remain-
der of the paper.
</bodyText>
<subsectionHeader confidence="0.651665">
4.4 Topic-Goal Model
</subsectionHeader>
<bodyText confidence="0.998785054054054">
To motivate our proposal, consider that over the
course of a dialog one could look at the set of all
values used for some slot, for example the destina-
tion city, as a count vector:
vdest city = Salt Lake:1; Osaka:2; Osaka Japan:1
The above vector may arise because the user actu-
ally wants to go to Osaka, but the destination is
initially mis-recognised as Salt Lake, and the user
finally disambiguates with the addition of the coun-
try. Such situations are common in the noisy dia-
log resources from which simulators are induced—
however, any string-based goal will necessarily con-
sider these different renderings to be different goals,
and will require resampling or smoothing terms to
deal with them.
Our approach instead treats the count vector as
samples from a topic model; that is, a mixture over
multinomial distributions. Whilst by far the most
popular topic model is LDA (Blei et al., 2003), it
provides too flexible a distribution over count vec-
tors to be used with such small samples (we con-
firmed the poor suitability of this model in pre-
liminary experiments). Instead we use the simpler
Mixture-of-Multinomials model, where the latent
topic is sampled once per dialog instead of once per
value uttered. We describe below how parameters to
this model are estimated, and focus for now on how
the resulting model assigns probability to dialogs.
In this formulation, the latent goal for each slot,
which was previously a string, now becomes an in-
dicator for a topic in a topic model. Each topic can
in theory generate any string (so the model is inher-
ently smoothed), but most strings in most topics will
have only the smoothing weight and most probabil-
ity mass will be on a small number of highly corre-
lated strings. We treat the slots as being independent
of one another in the goal, and thus:
</bodyText>
<equation confidence="0.965846333333333">
rl
p(g) = p(zs) (5)
s
</equation>
<bodyText confidence="0.99768975">
Where zs is the topic indicator for some slot s. If slot
s has associated with it a count vector of values vs,
each looking like the example above, then the distri-
bution over the values used for each slot becomes:
</bodyText>
<equation confidence="0.956204">
Ep (vs) = p(zs)p (vs|zs) (6)
z,
</equation>
<bodyText confidence="0.999904666666667">
We then define a bigram-based Act model to de-
scribe the probabilities of the {ACT, slot} pairs to
which these values belong, so that:
</bodyText>
<equation confidence="0.86894475">
p (u|m) = p (zs) �
rl rl p (ai, si|mi−1) p (vi|zsz)
s i
(7)
</equation>
<bodyText confidence="0.9997845">
In reality, some slots will not have corresponding
values, or will be slots whose values are not appro-
priate to model in the above way. Dates and times,
for example, have ordinal and structural relations be-
tween them, and a model which treats them as dis-
connected entities is inappropriate. For utterances
defined over such slots we use a standard bigram
model as in (1), and for appropriate utterances we
use a topic-goal model as in (7). This constitutes
the only domain knowledge necessary to adapt the
model for a new resource. We refer to this model as
the Topic-Goal model.
</bodyText>
<sectionHeader confidence="0.471695" genericHeader="method">
4.4.1 Topic Model Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999965318181818">
Our topic model is a Bayesian version of the
Mixture-of-Multinomials model. Under this model,
each dialog has associated with it a latent variable
zs for each slot s in the goal, which indicates which
topic is used to draw the values for that slot. Con-
ditioned on z, independent samples are drawn from
the distribution over words to which that value of
z corresponds—however, the effect in the marginal
distribution over words is to strongly prefer sets
which have co-occurred in training as these are as-
signed to the same topic.
Bayesian inference in mixture models has been
described in detail in Neal (1991) and Griffiths and
Steyvers (2004), so we give only a brief account here
for our particular model. We take r appropriately-
spaced samples from a Gibbs’ sampler over the pos-
terior mixture parameters 0, 0: 0 are the word-topic
parameters and 0 are the mixture proportions. We
assume a uniform Dirichlet prior on 0 and 0, lead-
ing to Dirichlet posteriors which we integrate out in
the predictive distribution over v using the standard
Dirichlet integral. For each of our r samples we have
</bodyText>
<page confidence="0.994495">
75
</page>
<bodyText confidence="0.9995695">
components z parameterised by 1&apos;rz (the Dirichlet
parameter for the z-th mixture component in the r-
th sample) and αrzj for each word j in the z-th topic
for the r-th sample. The • notation indicates a sum
over the corresponding index, i.e. 1&apos;r• = Ez 1&apos;rz.
Then:
</bodyText>
<equation confidence="0.999022833333333">
1 1&apos;rz
p (v) = p (v|αrz) (8)
|r |1&apos;r•
r z
Γ(α•) Γ(vj + αj)
p (v |α) = Γ (α• + v•) jl Γ(αj) (9)
</equation>
<bodyText confidence="0.9986547">
This states that each of the r samples has topics z
which are multinomial distributions with posteriors
governed by parameters αrz. For any of these top-
ics, the distribution over v is as given in Equation
(9) (we suppress the subscripting of α here for the
different samples and topics, since this holds what-
ever its value). The final predictive probability given
in Equation (8) averages over the samples r and the
topics z (with topics weighted by their parameters
1&apos;rz).
</bodyText>
<sectionHeader confidence="0.997688" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999581041666667">
Our experiments use two standard corpora, the
first of which is DARPA Communicator (DC), a
flight booking domain collected between 2000-2001
through the interaction of real users with 10 different
systems (Levin et al., 2000). It was later automati-
cally annotated by Georgila et al. (2005b) to include
semantic information. The second corpora is Let’s
Go (LG), years 2007, 2008, and 2009, distributed as
part of the Spoken Dialog Challenge (Black and Es-
kenazi, 2009). Let’s Go is a bus routing domain in
Pittsburgh collected by having the general public in-
teract with the CMU dialog system to find their way
through the city. The dialogs in both corpora are of
mixed-initiative, having a free number of contiguous
system and user responses.
We preprocessed the corpora, converting Com-
municator XML-tagged files and Let’s Go system
log files into sequences of ACT, slot, and value ut-
terances. Table 2 gives examples. We then divided
the corpora into training, development and test sets
as follows: Communicator contains 2285 dialogs in
total, and Let’s Go contains 17992, and in each case
we selected 80% of dialogs at random for training,
10% for development, and 10% for testing.
</bodyText>
<table confidence="0.984394333333333">
DC: PROVIDE INFO orig city Boston
LG: INFORM place [departure place CMU,
arrival place airport]
</table>
<tableCaption confidence="0.995427">
Table 2: Example utterances from the two corpora. Note
how in addition to the value, the Let’s Go utterances con-
tain properties (departure place and arrival place).
</tableCaption>
<bodyText confidence="0.99989925">
Let’s Go is a noisy corpus that contains far more
speech recognition errors than Communicator. In
addition, users tend to be more flexible with their
bus routes than they are with their flight destinations,
and so values are a lot more varied throughout the
course of Let’s Go dialogs than Communicator ones.
Furthermore, Let’s Go semantic parses contain am-
biguity not present in Communicator; the parser
fails to distinguish departure from arrival places over
90% of the time, and instead assigns them a generic
Single Place property. Our current model assumes
the decisions made by the semantic parser are cor-
rect. In reality however, a better model would in-
corporate potential noise in the semantic parse in a
joint model. We defer this more complex treatment
for future work.
Free model parameters are set by a simple search
on the development set, where the objective is
likelihood—for the bigram model the parameters are
the interpolation weights, and for the topic model we
search for the number of topics and smoothing con-
stant for the topic distributions. For Let’s Go, since
we can have multiple places provided in a single act,
we treat each utterance as containing a set of values
and build the count vector for the topic model as the
union of these sets over the whole dialog. The slots
over which the topic model is defined for Commu-
nicator are dest city and orig city (this takes into ac-
count PROVIDE and REPROVIDE acts). For Let’s Go
we derive the model over the three properties: sin-
gle place, arrival place and departure place, as op-
posed to the less informative slot place.
</bodyText>
<sectionHeader confidence="0.988295" genericHeader="method">
6 Evaluating the Simulators
</sectionHeader>
<bodyText confidence="0.999971">
We evaluate each of the models in terms of the prob-
ability they assign to the test data. This metric is
more suitable than the precision and recall metrics
which have been previously used, because it ac-
knowledges that, rather than each user response be-
ing “correct” at the point which it is observed, there
</bodyText>
<page confidence="0.973102">
76
</page>
<table confidence="0.9996958">
Model DC(A) DC(P) LG(A) LG(P)
Topic 252.78 860.2 113.45 1417.06
String 270.09 1286.03 169.87 4578.23
Bigram 347.88 5979.53 223.23 10125.87
Act 9.56 5.2 2.77 2.34
</table>
<tableCaption confidence="0.5769488">
Table 3: The mean per-utterance perplexity on heldout
data. DC-A is all acts for Communicator, while DC-P is
the calculated on PROVIDE acts alone (the acts on which
our model is designed to improve prediction). LG-A and
LG-P have the same meaning for Let’s Go.
</tableCaption>
<bodyText confidence="0.999885172413793">
is a distribution over possible responses. Because
the models we define are full probability models, we
are able to compute this metric and do not need to
use an arbitrarily selected dialog manager for evalu-
ation.
The heldout probability metric should be under-
stood as a means of comparing the relative viabil-
ity of different models of the same data. Note that
we are reporting the probability of unobserved data,
rather than data from which the models were in-
duced, and are thus measuring the generalisability
of the models (in contrast, maximising the proba-
bility of the training data would simply encourage
overfitting). The absolute numbers are hard to inter-
pret, as there is no hard upper bound; while it may
be appealing to think of an upper bound of 1, this is
incorrect as it would imply that there was no vari-
ability in the data. However, it should be understood
that assigning particular behaviour higher probabil-
ity means that the model is more likely to exhibit
it when run in simulation mode—and since the user
behaviour in question has not been seen at training
time, this measures the extent to which the models
have generalised beyond the training data relative to
one another.
We report the mean per-utterance log probability
of unseen data, that is, the probability of the whole
heldout corpus divided by the number of user utter-
ances.
</bodyText>
<subsectionHeader confidence="0.726615">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.998654">
Figure 1 shows the results of our evaluation. We see
that the Bigram model is weak on both resources.
The results of the String Goal model suggest that,
even using the generous evaluation we do here, there
</bodyText>
<figure confidence="0.9772535">
20 40 60 80 100
Percent of Training Examples
</figure>
<figureCaption confidence="0.8357996">
Figure 1: Heldout probability of the two resources for
varying percentages of training dialogs. Note that while
the percentages match across resources, Let’s Go is much
larger and thus the absolute numbers of dialogs are differ-
ent, which explains the better performance on Let’s Go.
</figureCaption>
<bodyText confidence="0.999944631578947">
is much variability due to synonymy and recognition
errors which string goals are unable to capture (in
contrast to our Topic Goal model). The Topic Goal
model explains this much more easily by grouping
commonly co–occurring values into the same topic.
Table 3 shows the perplexities corresponding to the
performances with 100% training data for all acts
and just PROVIDE acts (perplexity is 2−lp where lp is
the log probability). Improvements are more appar-
ent when we compute the probability over PROVIDE
acts alone, which the models are designed to handle.
And since perplexity is not on a log scale, the differ-
ences are more pronounced. The Act model, which
is a bigram model over {ACT, slot} pairs alone ex-
cluding the values, demonstrates the vast discrep-
ancy in uncertainty between the full problem and the
valueless prediction problem. We note that the per-
plexity of our Act model on Communicator is com-
parable to that of Georgila et al. (2006).
</bodyText>
<subsectionHeader confidence="0.999882">
6.2 Example Simulator Behaviour
</subsectionHeader>
<bodyText confidence="0.998708333333333">
In this section we give examples of our Topic
Goal model simulator in generation mode, which
corresponds to sampling from the induced model.
</bodyText>
<figure confidence="0.712376375">
Topic (DC)
String (DC)
Bigram (DC)
Topic (LG)
String (LG)
Bigram (LG)
−8.0 −7.5 −7.0 −6.5 −6.0 −5.5 −5.0 −4.5
Mean Per−Utterance Log Probability
</figure>
<page confidence="0.765097">
77
</page>
<table confidence="0.99958365">
d zdest city [probability] proportion user utterance given topic zdest city and
of samples machine utterance REQUEST INFO dest city
Norfolk Virginia [0.562] 0.264 PROVIDE INFO dest city Norfolk Virginia
Norfolk [0.234] 0.111 PROVIDE INFO dest city Norfolk
1 Newark Virginia [0.088] 0.039 PROVIDE INFO dest city Newark Virginia
Virginia Beach [0.0412] 0.028 PROVIDE INFO orig city Las Vegas Nevada
Newark [0.040] 0.028 NO ANSWER null no
0.025 COMMAND start over start over
Chicago [0.350] 0.164 PROVIDE INFO dest city Chicago
Chicago Illinois [0.182] 0.082 PROVIDE INFO dest city Chicago Illinois
2 Duluth Minnesota [0.124] 0.057 PROVIDE INFO dest city New Orleans
New Orleans [0.122] 0.055 PROVIDE INFO dest city Duluth Minnesota
New Orleans Louisiana [0.085] 0.039 PROVIDE INFO dest city New Orleans Louisiana
0.028 NO ANSWER null no
Anchorage [0.539] 0.252 PROVIDE INFO dest city Anchorage
Anchorage Alaska [0.148] 0.072 PROVIDE INFO dest city Anchorage Alaska
3 Jacksonville Florida [0.124] 0.056 PROVIDE INFO dest city Jacksonville Florida
Great Anchorage Alaska [0.098] 0.048 PROVIDE INFO dest city Great Anchorage Alaska
Duluth Minnesota [0.057] 0.047 PROVIDE INFO orig city Hartford Connecticut
0.026 PROVIDE INFO dest city Duluth Minnesota
</table>
<tableCaption confidence="0.915105333333333">
Table 4: Examples of sampling from the topic goal model. Left: top 5 strings (with probabilities) sampled from topics
for three different dialogs d. Right: top 6 utterances (plus fraction of samples in 10,000) generated in response to the
machine utterance “REQUEST INFO dest city” and conditioned on the topic zeest city.
</tableCaption>
<bodyText confidence="0.998447071428571">
Our examples are drawn from the model induced
for the Communicator data. Sampling from stan-
dard distributions can be implemented following
the algorithms in Bishop (2006) and other statisti-
cal resources. Utterances are sampled by sampling
ACT, slot pairs from the distribution p (ai, si|mi_1)
(drawing a value from a multinomial distribution). If
we sample a PROVIDE INFO act, we check whether
we have sampled a topic for the corresponding slot
thus far in the dialog. If not, we sample one by
drawing a topic indicator from p(zs) = &apos;Irz ���and then
drawing a multinomial distribution over strings from
the Dirichlet posterior corresponding to z. Once the
topic for the slot is set, we sample values as draws
from the fixed multinomial and add these to the ACT,
slot pair.
Table 4 shows some examples drawn from the
model. For each row in the table (corresponding to a
new dialog d), we sample a topic for the dest city
and orig city as needed, and sample 10000 utter-
ances given that topic. The left hand side of the ta-
ble shows the top five strings in the sampled topic,
while the right hand side shows the top six utter-
ances in response to REQUEST INFO dest city. Note
that the proportion of utterances on the right does
not match the probability of the values on the left
because of the presence of other user acts besides
PROVIDE dest city.
</bodyText>
<sectionHeader confidence="0.994937" genericHeader="method">
7 Evaluating Model Consistency
</sectionHeader>
<bodyText confidence="0.999785">
Having shown in the previous section that our Topic
Goal model is a much better predictor of heldout
data than the String Goal model or Bigram model,
we now turn to a demonstration of the model’s cap-
turing of consistency.
In the face of value synonymy and ASR errors,
we define inconsistent dialogs to be ones that are lo-
cally coherent but lack the structure of a real dialog
from one turn to the next. We then suggest that an
appropriate task for consistent models is distinguish-
ing between consistent and inconsistent dialogs.
To test this hypothesis, we devise the following
classification problem: can we discriminate between
</bodyText>
<page confidence="0.994593">
78
</page>
<table confidence="0.998930111111111">
Baseline Dialog length (turns)
Mean, standard deviation, min and max acts per turn
Presence of special machine acts (flight offer and confirm)
Presence of user acts (provide a dest city and arrival city)
Proportion of acts which were provides
String Consistency Did the user provide inconsistent information about dest city?
Did the user provide inconsistent information about orig city?
Topic Model Ranked list of posterior probabilities of top 50 topics
Normalised probability of dialog for topic model
</table>
<tableCaption confidence="0.99949">
Table 5: Feature sets for consistency experiments
</tableCaption>
<bodyText confidence="0.999978">
real dialogs and those generated by randomly sam-
pling turns from different dialogs? In this section we
induce classifiers over various feature sets to demon-
strate that we can, and that the Topic Goal model
contains far more useful information in this regard
than string-based consistency features. (The bigram
model by definition provides no help here, since the
units of which dialogs consist contain the entire win-
dow of context used for the bigram model).
We take our training and development data from
the Communicator corpus in the previous section,
and create a classification problem as follows: real
dialogs form positive examples in the classification
problem. To create negative examples, we sample
{machine, user} turns at random from the appropri-
ate resource. We keep a histogram over real dia-
log lengths, and sample a number of turns for our
“fake” dialogs proportional to this histogram. We
then sample this many turns from the frequency dis-
tribution over turns in the real data, and create ex-
actly as many dialogs in this fashion as real dialogs
in the data. The result is an equal number of dialogs
comprised of real turns, of (expected) real length,
but where the sequence of turns is highly unlikely to
be coherent given the random sampling. The classi-
fication problem is thus far from trivial. We do this
from our training data to produce data with which to
train the classifier, and from our development data
to provide test instances. This gives rise to 2500
training instances, and 500 test instances.
We learn linear SVMs with various features de-
scribed in Table 6. These feature sets are designed
to capture different aspects of consistency: the base-
line features are intended to capture surface level
features of the dialogs, inspired by (Schatzmann et
al., 2005) where they provide trivial separation of
real from simulated dialogs. However, our setting
is different: we do not seek to tell real dialogs from
fully simulated ones, but real dialogs from scram-
bled versions of real dialogs. In addition to length-
based features, we add binary presence indicator for
several user and machine acts highly correlated with
the completion of dialogs, as well as for acts which
indicate the provision of information and the propor-
tion of all acts occupied by these. The table gives a
complete list of these Baseline (B) features.
We derive a second set of features intended to
replicate the utility of string-based goals: we set up
binary features to fire if contradictory information is
provided for the slots over the course of the dialog.
These are our String Consistency (SC) features.
Finally, we use our topic-model simulator to de-
rive consistency features. Our features are the pos-
terior distribution over topics for each slot given that
dialog. Our topics are induced from the real training
dialogs, and their posterior probabilities computed
for all dialogs relative to this model. We take poste-
rior probabilities of the fifty most probable topics for
each of the dest city and orig city slots as features,
as well as the normalised log probability of the di-
alog (the log probability divided by the number of
user utterances). These form our Topic Model (TM)
features.
Our classifiers are linear SVMs, and we use lib-
svm (Chang and Lin, 2011), scaling features to the
range [0 − 1]. All other parameters are left at their
defaults.
</bodyText>
<page confidence="0.994481">
79
</page>
<table confidence="0.9991945">
Feature Set Accuracy
Baseline (B) 74.34 ±3.77
String Consistency (SC) 63.60 ±4.27
B + SC 77.63 ±3.58
Topic Model (TM) 79.61 ±3.44
B + SC + TM 85.96 ±2.89
</table>
<tableCaption confidence="0.91431">
Table 6: Performances for the classifiers. Errors are 95%
intervals to the accuracies assuming they are parameters
to a binomial distribution
</tableCaption>
<subsectionHeader confidence="0.516147">
7.1 Results
</subsectionHeader>
<bodyText confidence="0.999993411764706">
The results of the classifiers are shown in Table 5.
Since we have an equally balanced binary classifi-
cation task, accuracy is the most appropriate metric.
Here we see that the baseline and string consistency
features have roughly the same discriminatory po-
tential, and their union produces a slight improve-
ment. The topic model features are far superior to
this, and the union of all three sets gives a further
improvement.
These results demonstrate that our model encodes
notions of consistency which go substantially be-
yond those defined at the level of strings. Features
defined over the latent topic goal space substantially
improve performance in a difficult discrimination
task, demonstrating that our model captures an im-
portant notion of how real dialogs appear that is not
shared by the other models we consider.
</bodyText>
<sectionHeader confidence="0.663282" genericHeader="conclusions">
8 Concluding Remarks and Future Work
</sectionHeader>
<bodyText confidence="0.999995586206897">
This paper presents a fully generative goal driven
user simulator, the first to merge both consistency
and variability within a fully probabilistic frame-
work. We evaluate our model on two task-based di-
alog domains, Let’s Go and Communicator, and find
it to outperform both a simple bigram model and an
upper bound on probability models where the strings
are represented as goals, in terms of the probability
the model assigns to heldout dialogs.
We then move on to show that features derived
from the model lead to substantial improvement in
detecting real dialogs from those where the turns
have been selected at random from all turns in the
training data: this is a fairly difficult task, but our
model allows significant improvement over strong
and sensible baselines.
Our model could be extended in a number of
ways. It could be improved to incorporate noise
resulting from the decisions made by the semantic
parser. Another possible improvement is to explore
the effects of introducing dependency between the
slots in the user goal, which would enforce more
plausible values pairings and would potentially im-
prove the simulator’s performance. The effects of a
dependence assumption between the different utter-
ances occurring in a single user turn under the act
model can also be explored. We would also like to
use our simulator to train a POMDP-based dialog
manager using a form of reinforcement learning.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999081166666667">
Hua Ai and Diane J. Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proceedings of ACL-08: HLT.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc.
Alan W. Black and Maxine Eskenazi. 2009. The Spo-
ken Dialogue Challenge. In Proceedings of SIGDIAL
2009, SIGDIAL ’09.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, March.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proceedings of IEEE Workshop on Auto-
matic Speech Recognition and Understanding.
M. Ga˘si´c, F. Jurcicek, B. Thomson, K. Yu, and S. Young.
2011. On-line policy optimisation of spoken dia-
logue systems via live interaction with human subjects.
In Automatic Speech Recognition and Understanding,
2011 IEEE Workshop on, Hawaii, December.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2005a. Learning user simulations for information state
update dialogue systems. In Proceedings InterSpeech
2005.
Kallirroi Georgila, Oliver Lemon, and James Henderson.
2005b. Automatic annotation of communicator dia-
logue data for learning dialogue strategies and user
simulations. In Proceedings Ninth Workshop on the
Semantics and Pragmatics of Dialogue.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User Simulation for Spoken Dialogue Systems:
</reference>
<page confidence="0.964387">
80
</page>
<reference confidence="0.9998765">
Learning and Evaluation. In Proceedings InterSpeech
2006.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228–5235.
Srinivasan Janarthanam and Oliver Lemon. 2009. A two-
tier user simulation model for reinforcement learn-
ing of adaptive referring expression generation poli-
cies. In Proceedings of SIGDIAL 2009, SIGDIAL ’09,
pages 120–123.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer Speech &amp; Lan-
guage, 23(4):479–509.
Simon Keizer, Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Franc¸ois
Mairesse, Blaise Thomson, Kai Yu, and Steve Young.
2010. Parameter estimation for agenda-based user
simulation. In Proceedings of SIGDIAL 2010.
Esther Levin and Roberto Pieraccini. 2000. A stochastic
model of human-machine interaction for learning di-
alog strategies. In IEEE Transactions on Speech and
Audio Processing.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&amp;T-DARPA communicator mixed-
initiative spoken dialog system. In In ICSLP.
Radford Neal. 1991. Bayesian Mixture Modeling by
Monte Carlo Simulation. Technical report, University
of Toronto.
Olivier Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Facult´e
Polytechnique de Mons, TCTS Lab (Belgique), apr.
Jost Schatzmann, Kallirroi Georgila, and Steve Young.
2005. Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proceeings of
6th SIGDIAL Workshop.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007a. Agenda-based user
simulation for bootstrapping a POMDP dialogue sys-
tem. In HLT-NAACL (Short Papers), NAACL-Short
’07.
Jost Schatzmann, Blaise Thomson, and Steve Young.
2007b. Statistical User Simulation with a Hidden
Agenda. In Proceedings 8th SIDdial Workshop on
Discourse and Dialogue, September.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT 2002.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 2000. Empirical evaluation of
a reinforcement learning spoken dialogue system. In
Proceedings of the Seventeenth National Conference
on Artificial Intelligence and Twelfth Conference on
Innovative Applications of Artificial Intelligence.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Jason D. Williams. 2008. Evaluating user simulations
with the Cramer-von Mises divergence. Speech Com-
munication, 50(10):829–846, October.
</reference>
<page confidence="0.999265">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671508">
<title confidence="0.993315">Generative Goal-Driven User Simulation for Dialog Management</title>
<author confidence="0.804109">Aciel</author>
<affiliation confidence="0.995972">School of University of</affiliation>
<email confidence="0.917029">a.eshky@sms.ed.ac.uk</email>
<author confidence="0.979041">Ben</author>
<affiliation confidence="0.998884">School of University of Edinburgh</affiliation>
<author confidence="0.959337">Mark</author>
<affiliation confidence="0.99921">School of University of Edinburgh</affiliation>
<abstract confidence="0.998889074074074">User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let’s Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hua Ai</author>
<author>Diane J Litman</author>
</authors>
<title>Assessing dialog system user simulation evaluation measures using human judges.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="9364" citStr="Ai and Litman (2008)" startWordPosition="1481" endWordPosition="1485">ata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The main approach is to compute precision and recall on an utterance basis, which is intended to measure the similarity between real user responses in the corp</context>
</contexts>
<marker>Ai, Litman, 2008</marker>
<rawString>Hua Ai and Diane J. Litman. 2008. Assessing dialog system user simulation evaluation measures using human judges. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning (Information Science and Statistics).</booktitle>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="29893" citStr="Bishop (2006)" startWordPosition="4981" endWordPosition="4982">uluth Minnesota [0.057] 0.047 PROVIDE INFO orig city Hartford Connecticut 0.026 PROVIDE INFO dest city Duluth Minnesota Table 4: Examples of sampling from the topic goal model. Left: top 5 strings (with probabilities) sampled from topics for three different dialogs d. Right: top 6 utterances (plus fraction of samples in 10,000) generated in response to the machine utterance “REQUEST INFO dest city” and conditioned on the topic zeest city. Our examples are drawn from the model induced for the Communicator data. Sampling from standard distributions can be implemented following the algorithms in Bishop (2006) and other statistical resources. Utterances are sampled by sampling ACT, slot pairs from the distribution p (ai, si|mi_1) (drawing a value from a multinomial distribution). If we sample a PROVIDE INFO act, we check whether we have sampled a topic for the corresponding slot thus far in the dialog. If not, we sample one by drawing a topic indicator from p(zs) = &apos;Irz ���and then drawing a multinomial distribution over strings from the Dirichlet posterior corresponding to z. Once the topic for the slot is set, we sample values as draws from the fixed multinomial and add these to the ACT, slot pai</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan W Black</author>
<author>Maxine Eskenazi</author>
</authors>
<title>The Spoken Dialogue Challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL 2009, SIGDIAL ’09.</booktitle>
<contexts>
<context position="5960" citStr="Black and Eskenazi, 2009" startWordPosition="940" endWordPosition="944">experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that it provides responses consistent with one</context>
<context position="21648" citStr="Black and Eskenazi, 2009" startWordPosition="3610" endWordPosition="3614">redictive probability given in Equation (8) averages over the samples r and the topics z (with topics weighted by their parameters 1&apos;rz). 5 Experimental Setup Our experiments use two standard corpora, the first of which is DARPA Communicator (DC), a flight booking domain collected between 2000-2001 through the interaction of real users with 10 different systems (Levin et al., 2000). It was later automatically annotated by Georgila et al. (2005b) to include semantic information. The second corpora is Let’s Go (LG), years 2007, 2008, and 2009, distributed as part of the Spoken Dialog Challenge (Black and Eskenazi, 2009). Let’s Go is a bus routing domain in Pittsburgh collected by having the general public interact with the CMU dialog system to find their way through the city. The dialogs in both corpora are of mixed-initiative, having a free number of contiguous system and user responses. We preprocessed the corpora, converting Communicator XML-tagged files and Let’s Go system log files into sequences of ACT, slot, and value utterances. Table 2 gives examples. We then divided the corpora into training, development and test sets as follows: Communicator contains 2285 dialogs in total, and Let’s Go contains 17</context>
</contexts>
<marker>Black, Eskenazi, 2009</marker>
<rawString>Alan W. Black and Maxine Eskenazi. 2009. The Spoken Dialogue Challenge. In Proceedings of SIGDIAL 2009, SIGDIAL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="17246" citStr="Blei et al., 2003" startWordPosition="2829" endWordPosition="2832">lly wants to go to Osaka, but the destination is initially mis-recognised as Salt Lake, and the user finally disambiguates with the addition of the country. Such situations are common in the noisy dialog resources from which simulators are induced— however, any string-based goal will necessarily consider these different renderings to be different goals, and will require resampling or smoothing terms to deal with them. Our approach instead treats the count vector as samples from a topic model; that is, a mixture over multinomial distributions. Whilst by far the most popular topic model is LDA (Blei et al., 2003), it provides too flexible a distribution over count vectors to be used with such small samples (we confirmed the poor suitability of this model in preliminary experiments). Instead we use the simpler Mixture-of-Multinomials model, where the latent topic is sampled once per dialog instead of once per value uttered. We describe below how parameters to this model are estimated, and focus for now on how the resulting model assigns probability to dialogs. In this formulation, the latent goal for each slot, which was previously a string, now becomes an indicator for a topic in a topic model. Each t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<contexts>
<context position="35554" citStr="Chang and Lin, 2011" startWordPosition="5934" endWordPosition="5937">r to derive consistency features. Our features are the posterior distribution over topics for each slot given that dialog. Our topics are induced from the real training dialogs, and their posterior probabilities computed for all dialogs relative to this model. We take posterior probabilities of the fifty most probable topics for each of the dest city and orig city slots as features, as well as the normalised log probability of the dialog (the log probability divided by the number of user utterances). These form our Topic Model (TM) features. Our classifiers are linear SVMs, and we use libsvm (Chang and Lin, 2011), scaling features to the range [0 − 1]. All other parameters are left at their defaults. 79 Feature Set Accuracy Baseline (B) 74.34 ±3.77 String Consistency (SC) 63.60 ±4.27 B + SC 77.63 ±3.58 Topic Model (TM) 79.61 ±3.44 B + SC + TM 85.96 ±2.89 Table 6: Performances for the classifiers. Errors are 95% intervals to the accuracies assuming they are parameters to a binomial distribution 7.1 Results The results of the classifiers are shown in Table 5. Since we have an equally balanced binary classification task, accuracy is the most appropriate metric. Here we see that the baseline and string co</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="2120" citStr="Eckert et al., 1997" startWordPosition="307" endWordPosition="310">lly simulating user behaviour in humanmachine dialogs has become vital for training statistical dialog managers in task-oriented domains. These managers are often trained with some variant of reinforcement learning (Sutton and Barto, 1998), where optimal behaviour is sought or learnt through the exploration of the space of possible dialogs. Although learning by interacting with human 71 subjects is a possibility (Ga˘si´c et al., 2011), it has been argued that user simulation avoids the expensive, labour intensive, and error-prone experience of exposing real humans to fledgling dialog systems (Eckert et al., 1997). Training effective dialog managers should benefit from exposure to properties exhibited by real users. Table 1 shows an example dialog in a domain such as we consider, where the objective is to simulate at the semantic level. In such task oriented domains, the user has a goal (in this case, to book a flight from New York to Osaka), and the machine is tasked with fulfilling it. Notice that the user is consistent with this goal throughout the dialog, in that they do not provide contradictory information (although an ASR error is present), but that every mention of their destination city uses a</context>
<context position="5526" citStr="Eckert et al. (1997)" startWordPosition="871" endWordPosition="874">criptions are not available; thus there will be speech recognition noise, e.g. Osaka rendered as Salt Lake City, something our model is able to capture. well with other managers. In contrast, a probabilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditio</context>
<context position="11308" citStr="Eckert et al. (1997)" startWordPosition="1813" endWordPosition="1816">erances given the dialog up to that point. In other words, we are interested in the distribution p (ui|d1 ... di−1), where dn is either a machine utterance mn or a user utterance un. 4 Models of Users in Dialogs This section describes several models of increasing complexity: a Bigram model, which serves as a baseline; an upper-bound on String-Goal models, which we design to mimic the behaviour of previous goal-based approaches, but with a probabilistic formulation; and finally our approach, the Topic-Goal model. 4.1 Bigram Model The simplest model we define over dialogs is the bigram model of Eckert et al. (1997): p (ui|m) = p (ui|mi−1) (1) p (u|m) = � p (ui|m) (2) i The probability of each user utterance ui (the complete {ACT, slot, value} triple) is dependent only on the machine utterance immediately preceding it (the slight abuse of notation mi−1 here does not mean the utterance at i−1 in the machine utterance list, but the utterance immediately preceding the i-th), and utterances in the dialog are conditionally independent of 73 one another. (Georgila et al. (2006) found no benefit from increasing the Markov horizon). Since each utterance is generated independently of others in the dialog with the</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ga˘si´c</author>
<author>F Jurcicek</author>
<author>B Thomson</author>
<author>K Yu</author>
<author>S Young</author>
</authors>
<title>On-line policy optimisation of spoken dialogue systems via live interaction with human subjects.</title>
<date>2011</date>
<booktitle>In Automatic Speech Recognition and Understanding, 2011 IEEE Workshop on,</booktitle>
<location>Hawaii,</location>
<marker>Ga˘si´c, Jurcicek, Thomson, Yu, Young, 2011</marker>
<rawString>M. Ga˘si´c, F. Jurcicek, B. Thomson, K. Yu, and S. Young. 2011. On-line policy optimisation of spoken dialogue systems via live interaction with human subjects. In Automatic Speech Recognition and Understanding, 2011 IEEE Workshop on, Hawaii, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning user simulations for information state update dialogue systems.</title>
<date>2005</date>
<booktitle>In Proceedings InterSpeech</booktitle>
<contexts>
<context position="5918" citStr="Georgila et al., 2005" startWordPosition="933" endWordPosition="936">ircularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that</context>
<context position="21470" citStr="Georgila et al. (2005" startWordPosition="3582" endWordPosition="3585">istribution over v is as given in Equation (9) (we suppress the subscripting of α here for the different samples and topics, since this holds whatever its value). The final predictive probability given in Equation (8) averages over the samples r and the topics z (with topics weighted by their parameters 1&apos;rz). 5 Experimental Setup Our experiments use two standard corpora, the first of which is DARPA Communicator (DC), a flight booking domain collected between 2000-2001 through the interaction of real users with 10 different systems (Levin et al., 2000). It was later automatically annotated by Georgila et al. (2005b) to include semantic information. The second corpora is Let’s Go (LG), years 2007, 2008, and 2009, distributed as part of the Spoken Dialog Challenge (Black and Eskenazi, 2009). Let’s Go is a bus routing domain in Pittsburgh collected by having the general public interact with the CMU dialog system to find their way through the city. The dialogs in both corpora are of mixed-initiative, having a free number of contiguous system and user responses. We preprocessed the corpora, converting Communicator XML-tagged files and Let’s Go system log files into sequences of ACT, slot, and value utteranc</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2005a. Learning user simulations for information state update dialogue systems. In Proceedings InterSpeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>Oliver Lemon</author>
<author>James Henderson</author>
</authors>
<title>Automatic annotation of communicator dialogue data for learning dialogue strategies and user simulations.</title>
<date>2005</date>
<booktitle>In Proceedings Ninth Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<contexts>
<context position="5918" citStr="Georgila et al., 2005" startWordPosition="933" endWordPosition="936">ircularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that</context>
<context position="21470" citStr="Georgila et al. (2005" startWordPosition="3582" endWordPosition="3585">istribution over v is as given in Equation (9) (we suppress the subscripting of α here for the different samples and topics, since this holds whatever its value). The final predictive probability given in Equation (8) averages over the samples r and the topics z (with topics weighted by their parameters 1&apos;rz). 5 Experimental Setup Our experiments use two standard corpora, the first of which is DARPA Communicator (DC), a flight booking domain collected between 2000-2001 through the interaction of real users with 10 different systems (Levin et al., 2000). It was later automatically annotated by Georgila et al. (2005b) to include semantic information. The second corpora is Let’s Go (LG), years 2007, 2008, and 2009, distributed as part of the Spoken Dialog Challenge (Black and Eskenazi, 2009). Let’s Go is a bus routing domain in Pittsburgh collected by having the general public interact with the CMU dialog system to find their way through the city. The dialogs in both corpora are of mixed-initiative, having a free number of contiguous system and user responses. We preprocessed the corpora, converting Communicator XML-tagged files and Let’s Go system log files into sequences of ACT, slot, and value utteranc</context>
</contexts>
<marker>Georgila, Lemon, Henderson, 2005</marker>
<rawString>Kallirroi Georgila, Oliver Lemon, and James Henderson. 2005b. Automatic annotation of communicator dialogue data for learning dialogue strategies and user simulations. In Proceedings Ninth Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>User Simulation for Spoken Dialogue Systems: Learning and Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings InterSpeech</booktitle>
<contexts>
<context position="8866" citStr="Georgila et al. (2006)" startWordPosition="1404" endWordPosition="1407">sibilities for their evaluation, a point to which we now turn. 2.2 Related Work on Simulator Evaluation No standardised metric of evaluation has been established for user simulators largely because they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators int</context>
<context position="11773" citStr="Georgila et al. (2006)" startWordPosition="1894" endWordPosition="1897">ation; and finally our approach, the Topic-Goal model. 4.1 Bigram Model The simplest model we define over dialogs is the bigram model of Eckert et al. (1997): p (ui|m) = p (ui|mi−1) (1) p (u|m) = � p (ui|m) (2) i The probability of each user utterance ui (the complete {ACT, slot, value} triple) is dependent only on the machine utterance immediately preceding it (the slight abuse of notation mi−1 here does not mean the utterance at i−1 in the machine utterance list, but the utterance immediately preceding the i-th), and utterances in the dialog are conditionally independent of 73 one another. (Georgila et al. (2006) found no benefit from increasing the Markov horizon). Since each utterance is generated independently of others in the dialog with the same context, there is no enforced consistency between utterances. Since we require a distribution over all possible utterances, assigning non–zero probability to cases outside of the training data, our bigram model is interpolated with a unigram model, which itself is interpolated with a smoothing model which assumes independence between the act, slot, and value elements of the utterance. Interpolation weights are set to maximise probability of a development </context>
<context position="27829" citStr="Georgila et al. (2006)" startWordPosition="4662" endWordPosition="4665">raining data for all acts and just PROVIDE acts (perplexity is 2−lp where lp is the log probability). Improvements are more apparent when we compute the probability over PROVIDE acts alone, which the models are designed to handle. And since perplexity is not on a log scale, the differences are more pronounced. The Act model, which is a bigram model over {ACT, slot} pairs alone excluding the values, demonstrates the vast discrepancy in uncertainty between the full problem and the valueless prediction problem. We note that the perplexity of our Act model on Communicator is comparable to that of Georgila et al. (2006). 6.2 Example Simulator Behaviour In this section we give examples of our Topic Goal model simulator in generation mode, which corresponds to sampling from the induced model. Topic (DC) String (DC) Bigram (DC) Topic (LG) String (LG) Bigram (LG) −8.0 −7.5 −7.0 −6.5 −6.0 −5.5 −5.0 −4.5 Mean Per−Utterance Log Probability 77 d zdest city [probability] proportion user utterance given topic zdest city and of samples machine utterance REQUEST INFO dest city Norfolk Virginia [0.562] 0.264 PROVIDE INFO dest city Norfolk Virginia Norfolk [0.234] 0.111 PROVIDE INFO dest city Norfolk 1 Newark Virginia [0.</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2006. User Simulation for Spoken Dialogue Systems: Learning and Evaluation. In Proceedings InterSpeech 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="19877" citStr="Griffiths and Steyvers (2004)" startWordPosition="3297" endWordPosition="3300"> is a Bayesian version of the Mixture-of-Multinomials model. Under this model, each dialog has associated with it a latent variable zs for each slot s in the goal, which indicates which topic is used to draw the values for that slot. Conditioned on z, independent samples are drawn from the distribution over words to which that value of z corresponds—however, the effect in the marginal distribution over words is to strongly prefer sets which have co-occurred in training as these are assigned to the same topic. Bayesian inference in mixture models has been described in detail in Neal (1991) and Griffiths and Steyvers (2004), so we give only a brief account here for our particular model. We take r appropriatelyspaced samples from a Gibbs’ sampler over the posterior mixture parameters 0, 0: 0 are the word-topic parameters and 0 are the mixture proportions. We assume a uniform Dirichlet prior on 0 and 0, leading to Dirichlet posteriors which we integrate out in the predictive distribution over v using the standard Dirichlet integral. For each of our r samples we have 75 components z parameterised by 1&apos;rz (the Dirichlet parameter for the z-th mixture component in the rth sample) and αrzj for each word j in the z-th </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>A twotier user simulation model for reinforcement learning of adaptive referring expression generation policies.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL 2009, SIGDIAL ’09,</booktitle>
<pages>120--123</pages>
<contexts>
<context position="9210" citStr="Janarthanam and Lemon (2009)" startWordPosition="1456" endWordPosition="1459">gh the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The m</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2009. A twotier user simulation model for reinforcement learning of adaptive referring expression generation policies. In Proceedings of SIGDIAL 2009, SIGDIAL ’09, pages 120–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangkeun Jung</author>
<author>Cheongjae Lee</author>
<author>Kyungduk Kim</author>
<author>Minwoo Jeong</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Datadriven user simulation for automated evaluation of spoken dialog systems.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="6896" citStr="Jung et al. (2009)" startWordPosition="1091" endWordPosition="1094">estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that it provides responses consistent with one another throughout the dialog. Several approaches have attempted to overcome this problem. Pietquin (2004), for example, explicitly models a user goal as a set of slot-value pairs randomly generated once per dialog. He then hand selects parameters to ensure that the user’s actions are in accordance with their goal. Jung et al. (2009) use large amounts of dialog state annotations (e.g. what information has been provided so far) to learn Conditional Random Fields over the user utterances, and assume that those features ensure user consistency. Georgila et al. (2005a) instead consider only act-slot pairs, and thus inconsistency is not a factor. Scheffler and Young (2002) simulate user behaviour by introducing rules for actions that depend on the user goal, and probabilistic modelling for actions that are not goal-dependent. They then map out a decision network that determines user actions at every node prior to the start of </context>
</contexts>
<marker>Jung, Lee, Kim, Jeong, Lee, 2009</marker>
<rawString>Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Minwoo Jeong, and Gary Geunbae Lee. 2009. Datadriven user simulation for automated evaluation of spoken dialog systems. Computer Speech &amp; Language, 23(4):479–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Milica Gaˇsi´c</author>
<author>Filip Jurˇc´ıˇcek</author>
<author>Franc¸ois Mairesse</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Parameter estimation for agenda-based user simulation.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGDIAL</booktitle>
<marker>Keizer, Gaˇsi´c, Jurˇc´ıˇcek, Mairesse, Thomson, Yu, Young, 2010</marker>
<rawString>Simon Keizer, Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Franc¸ois Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2010. Parameter estimation for agenda-based user simulation. In Proceedings of SIGDIAL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing.</booktitle>
<contexts>
<context position="5558" citStr="Levin and Pieraccini (2000)" startWordPosition="876" endWordPosition="879">le; thus there will be speech recognition noise, e.g. Osaka rendered as Salt Lake City, something our model is able to capture. well with other managers. In contrast, a probabilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively o</context>
</contexts>
<marker>Levin, Pieraccini, 2000</marker>
<rawString>Esther Levin and Roberto Pieraccini. 2000. A stochastic model of human-machine interaction for learning dialog strategies. In IEEE Transactions on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
<author>K Biatov</author>
<author>E Bocchieri</author>
<author>G Di Fabbrizio</author>
<author>W Eckert</author>
<author>S Lee</author>
<author>A Pokrovsky</author>
<author>M Rahim</author>
<author>P Ruscitti</author>
<author>M Walker</author>
</authors>
<title>The AT&amp;T-DARPA communicator mixedinitiative spoken dialog system. In</title>
<date>2000</date>
<booktitle>In ICSLP.</booktitle>
<marker>Levin, Narayanan, Pieraccini, Biatov, Bocchieri, Di Fabbrizio, Eckert, Lee, Pokrovsky, Rahim, Ruscitti, Walker, 2000</marker>
<rawString>E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker. 2000. The AT&amp;T-DARPA communicator mixedinitiative spoken dialog system. In In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Bayesian Mixture Modeling by Monte Carlo Simulation.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="19843" citStr="Neal (1991)" startWordPosition="3294" endWordPosition="3295"> Our topic model is a Bayesian version of the Mixture-of-Multinomials model. Under this model, each dialog has associated with it a latent variable zs for each slot s in the goal, which indicates which topic is used to draw the values for that slot. Conditioned on z, independent samples are drawn from the distribution over words to which that value of z corresponds—however, the effect in the marginal distribution over words is to strongly prefer sets which have co-occurred in training as these are assigned to the same topic. Bayesian inference in mixture models has been described in detail in Neal (1991) and Griffiths and Steyvers (2004), so we give only a brief account here for our particular model. We take r appropriatelyspaced samples from a Gibbs’ sampler over the posterior mixture parameters 0, 0: 0 are the word-topic parameters and 0 are the mixture proportions. We assume a uniform Dirichlet prior on 0 and 0, leading to Dirichlet posteriors which we integrate out in the predictive distribution over v using the standard Dirichlet integral. For each of our r samples we have 75 components z parameterised by 1&apos;rz (the Dirichlet parameter for the z-th mixture component in the rth sample) and</context>
</contexts>
<marker>Neal, 1991</marker>
<rawString>Radford Neal. 1991. Bayesian Mixture Modeling by Monte Carlo Simulation. Technical report, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
</authors>
<title>A Framework for Unsupervised Learning of Dialogue Strategies.</title>
<date>2004</date>
<booktitle>Ph.D. thesis, Facult´e Polytechnique de Mons, TCTS Lab (Belgique),</booktitle>
<contexts>
<context position="6667" citStr="Pietquin (2004)" startWordPosition="1053" endWordPosition="1054"> was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine utterances by assigning all others probability zero. Bigram models ensure that a locally sensible response to a machine utterance is provided by the simulator; however, they do not ensure that it provides responses consistent with one another throughout the dialog. Several approaches have attempted to overcome this problem. Pietquin (2004), for example, explicitly models a user goal as a set of slot-value pairs randomly generated once per dialog. He then hand selects parameters to ensure that the user’s actions are in accordance with their goal. Jung et al. (2009) use large amounts of dialog state annotations (e.g. what information has been provided so far) to learn Conditional Random Fields over the user utterances, and assume that those features ensure user consistency. Georgila et al. (2005a) instead consider only act-slot pairs, and thus inconsistency is not a factor. Scheffler and Young (2002) simulate user behaviour by in</context>
</contexts>
<marker>Pietquin, 2004</marker>
<rawString>Olivier Pietquin. 2004. A Framework for Unsupervised Learning of Dialogue Strategies. Ph.D. thesis, Facult´e Polytechnique de Mons, TCTS Lab (Belgique), apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Kallirroi Georgila</author>
<author>Steve Young</author>
</authors>
<title>Quantitative evaluation of user simulation techniques for spoken dialogue systems.</title>
<date>2005</date>
<booktitle>In Proceeings of 6th SIGDIAL Workshop.</booktitle>
<contexts>
<context position="8673" citStr="Schatzmann et al. (2005)" startWordPosition="1375" endWordPosition="1379">restrict the variability in user behaviour that can be accommodated. Furthermore, because these approaches do not define a complete probability distribution over user behaviour, they restrict possibilities for their evaluation, a point to which we now turn. 2.2 Related Work on Simulator Evaluation No standardised metric of evaluation has been established for user simulators largely because they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distribu</context>
<context position="10058" citStr="Schatzmann et al., 2005" startWordPosition="1595" endWordPosition="1598">ted by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The main approach is to compute precision and recall on an utterance basis, which is intended to measure the similarity between real user responses in the corpora and simulated user responses produced under similar circumstances (Schatzmann et al., 2005; Georgila et al., 2006). However, this is a harsh evaluation as it assumes a correct or “best” answer, and penalises valid variability in user behaviour. 3 Dialog as a Statistical Process We consider a dialog to be a series of turns, comprised of multiple utterances. Each Utterance consists of an ACT, a slot, and a value, as shown in Table 1. Dialogs proceed by the user and the machine alternating turns. Because the dialogs are of mixed initiative, there is no restriction on the number of contiguous machine or user utterances. Our aim is to model the user, and are interested in the conditiona</context>
<context position="13492" citStr="Schatzmann et al. (2005)" startWordPosition="2186" endWordPosition="2189">lots required in the problem. For instance, they might be the origin and destination cities in a flight booking domain. In standard machine learning terms, the goal becomes a latent variable g in a probability model. We can then define a distribution over utterances as: p (ui|m, g) = p (ui|mi−1, g) (3) p (u|m) = 1: p(g) � p (ui|mi−1, g) (4) g i 4.3 An Upper-Bound on String-Goal Models The simplest variant of g has string values for each of the slots the user is required to provide in order for the dialog to succeed. Thus we may have: g = [orig city: New York; dest city: Osaka] as presented in Schatzmann et al. (2005) and Schatzmann et al. (2007b). However, in these simulators, while the goal is probabilistic, there is no distribution over utterances given the goal because utterances are assembled deterministically from a series of rule applications. There is also no marginalisation over the goal as in (4) above. The issue with a model of user goals as strings in this fashion is that users describe the same values in multiple ways (Osaka Japan, Osaka), and speech recognition errors corrupt consistent user input (Osaka mis-recognised as Salt Lake City). Users also might legitimately switch their goals middi</context>
<context position="34075" citStr="Schatzmann et al., 2005" startWordPosition="5686" endWordPosition="5689">h, but where the sequence of turns is highly unlikely to be coherent given the random sampling. The classification problem is thus far from trivial. We do this from our training data to produce data with which to train the classifier, and from our development data to provide test instances. This gives rise to 2500 training instances, and 500 test instances. We learn linear SVMs with various features described in Table 6. These feature sets are designed to capture different aspects of consistency: the baseline features are intended to capture surface level features of the dialogs, inspired by (Schatzmann et al., 2005) where they provide trivial separation of real from simulated dialogs. However, our setting is different: we do not seek to tell real dialogs from fully simulated ones, but real dialogs from scrambled versions of real dialogs. In addition to lengthbased features, we add binary presence indicator for several user and machine acts highly correlated with the completion of dialogs, as well as for acts which indicate the provision of information and the proportion of all acts occupied by these. The table gives a complete list of these Baseline (B) features. We derive a second set of features intend</context>
</contexts>
<marker>Schatzmann, Georgila, Young, 2005</marker>
<rawString>Jost Schatzmann, Kallirroi Georgila, and Steve Young. 2005. Quantitative evaluation of user simulation techniques for spoken dialogue systems. In Proceeings of 6th SIGDIAL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Karl Weilhammer</author>
<author>Hui Ye</author>
<author>Steve Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a POMDP dialogue system.</title>
<date>2007</date>
<booktitle>In HLT-NAACL (Short Papers), NAACL-Short ’07.</booktitle>
<contexts>
<context position="5726" citStr="Schatzmann et al. (2007" startWordPosition="903" endWordPosition="906">babilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine </context>
<context position="7690" citStr="Schatzmann et al., 2007" startWordPosition="1220" endWordPosition="1224">t those features ensure user consistency. Georgila et al. (2005a) instead consider only act-slot pairs, and thus inconsistency is not a factor. Scheffler and Young (2002) simulate user behaviour by introducing rules for actions that depend on the user goal, and probabilistic modelling for actions that are not goal-dependent. They then map out a decision network that determines user actions at every node prior to the start of the dialog. Agendabased user simulation, another approach from the literature, assumes a probability distribution over the 72 user goal which is either induced from data (Schatzmann et al., 2007b), or is manually set when no data is available (Schatzmann et al., 2007a). An agenda, which is a stack-like structure of utterances to be produced given the goal, is then devised deterministically. Keizer et al. (2010) combine the decision network with the agenda and goal to allow for some variability for some actions. These models ensure consistency but restrict the variability in user behaviour that can be accommodated. Furthermore, because these approaches do not define a complete probability distribution over user behaviour, they restrict possibilities for their evaluation, a point to wh</context>
<context position="9000" citStr="Schatzmann et al. (2007" startWordPosition="1424" endWordPosition="1427">luation has been established for user simulators largely because they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then eva</context>
<context position="13520" citStr="Schatzmann et al. (2007" startWordPosition="2191" endWordPosition="2195"> For instance, they might be the origin and destination cities in a flight booking domain. In standard machine learning terms, the goal becomes a latent variable g in a probability model. We can then define a distribution over utterances as: p (ui|m, g) = p (ui|mi−1, g) (3) p (u|m) = 1: p(g) � p (ui|mi−1, g) (4) g i 4.3 An Upper-Bound on String-Goal Models The simplest variant of g has string values for each of the slots the user is required to provide in order for the dialog to succeed. Thus we may have: g = [orig city: New York; dest city: Osaka] as presented in Schatzmann et al. (2005) and Schatzmann et al. (2007b). However, in these simulators, while the goal is probabilistic, there is no distribution over utterances given the goal because utterances are assembled deterministically from a series of rule applications. There is also no marginalisation over the goal as in (4) above. The issue with a model of user goals as strings in this fashion is that users describe the same values in multiple ways (Osaka Japan, Osaka), and speech recognition errors corrupt consistent user input (Osaka mis-recognised as Salt Lake City). Users also might legitimately switch their goals middialog. Inference in the model</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007a. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In HLT-NAACL (Short Papers), NAACL-Short ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Statistical User Simulation with a Hidden Agenda.</title>
<date>2007</date>
<booktitle>In Proceedings 8th SIDdial Workshop on Discourse and Dialogue,</booktitle>
<contexts>
<context position="5726" citStr="Schatzmann et al. (2007" startWordPosition="903" endWordPosition="906">babilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible” pairs of user and machine </context>
<context position="7690" citStr="Schatzmann et al., 2007" startWordPosition="1220" endWordPosition="1224">t those features ensure user consistency. Georgila et al. (2005a) instead consider only act-slot pairs, and thus inconsistency is not a factor. Scheffler and Young (2002) simulate user behaviour by introducing rules for actions that depend on the user goal, and probabilistic modelling for actions that are not goal-dependent. They then map out a decision network that determines user actions at every node prior to the start of the dialog. Agendabased user simulation, another approach from the literature, assumes a probability distribution over the 72 user goal which is either induced from data (Schatzmann et al., 2007b), or is manually set when no data is available (Schatzmann et al., 2007a). An agenda, which is a stack-like structure of utterances to be produced given the goal, is then devised deterministically. Keizer et al. (2010) combine the decision network with the agenda and goal to allow for some variability for some actions. These models ensure consistency but restrict the variability in user behaviour that can be accommodated. Furthermore, because these approaches do not define a complete probability distribution over user behaviour, they restrict possibilities for their evaluation, a point to wh</context>
<context position="9000" citStr="Schatzmann et al. (2007" startWordPosition="1424" endWordPosition="1427">luation has been established for user simulators largely because they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then eva</context>
<context position="13520" citStr="Schatzmann et al. (2007" startWordPosition="2191" endWordPosition="2195"> For instance, they might be the origin and destination cities in a flight booking domain. In standard machine learning terms, the goal becomes a latent variable g in a probability model. We can then define a distribution over utterances as: p (ui|m, g) = p (ui|mi−1, g) (3) p (u|m) = 1: p(g) � p (ui|mi−1, g) (4) g i 4.3 An Upper-Bound on String-Goal Models The simplest variant of g has string values for each of the slots the user is required to provide in order for the dialog to succeed. Thus we may have: g = [orig city: New York; dest city: Osaka] as presented in Schatzmann et al. (2005) and Schatzmann et al. (2007b). However, in these simulators, while the goal is probabilistic, there is no distribution over utterances given the goal because utterances are assembled deterministically from a series of rule applications. There is also no marginalisation over the goal as in (4) above. The issue with a model of user goals as strings in this fashion is that users describe the same values in multiple ways (Osaka Japan, Osaka), and speech recognition errors corrupt consistent user input (Osaka mis-recognised as Salt Lake City). Users also might legitimately switch their goals middialog. Inference in the model</context>
</contexts>
<marker>Schatzmann, Thomson, Young, 2007</marker>
<rawString>Jost Schatzmann, Blaise Thomson, and Steve Young. 2007b. Statistical User Simulation with a Hidden Agenda. In Proceedings 8th SIDdial Workshop on Discourse and Dialogue, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konrad Scheffler</author>
<author>Steve Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT</booktitle>
<contexts>
<context position="5698" citStr="Scheffler and Young (2002)" startWordPosition="898" endWordPosition="901">er managers. In contrast, a probabilistic formulation such as we propose allows us to evaluate our models intrinsically using standard machine learning metrics, and without reference to a specific manager, thus breaking the circularity, and guarding against such experimental biases. We demonstrate the efficacy of our model on two tasks, and compare it to two other approaches. Firstly we use a standard bigram model as conceived by Eckert et al. (1997) and Levin and Pieraccini (2000); secondly we compare to a probabilistic goalbased simulator where the goals are string literals, as envisaged by Scheffler and Young (2002) and Schatzmann et al. (2007b). We demonstrate substantial improvement over these models in terms of predicting heldout data on two standard dialog resources: DARPA Communicator (Levin et al., 2000; Georgila et al., 2005b) and Let’s Go (Black and Eskenazi, 2009). 2 Related Work 2.1 Related Work on User Simulation User simulation as a stochastic process was first envisioned by Eckert et al. (1997): their Bigram model conditions user utterances exclusively on the preceding machine utterance. This was extended by Levin and Pieraccini (2000), who manually restrict the model to estimating “sensible</context>
<context position="7237" citStr="Scheffler and Young (2002)" startWordPosition="1145" endWordPosition="1148">have attempted to overcome this problem. Pietquin (2004), for example, explicitly models a user goal as a set of slot-value pairs randomly generated once per dialog. He then hand selects parameters to ensure that the user’s actions are in accordance with their goal. Jung et al. (2009) use large amounts of dialog state annotations (e.g. what information has been provided so far) to learn Conditional Random Fields over the user utterances, and assume that those features ensure user consistency. Georgila et al. (2005a) instead consider only act-slot pairs, and thus inconsistency is not a factor. Scheffler and Young (2002) simulate user behaviour by introducing rules for actions that depend on the user goal, and probabilistic modelling for actions that are not goal-dependent. They then map out a decision network that determines user actions at every node prior to the start of the dialog. Agendabased user simulation, another approach from the literature, assumes a probability distribution over the 72 user goal which is either induced from data (Schatzmann et al., 2007b), or is manually set when no data is available (Schatzmann et al., 2007a). An agenda, which is a stack-like structure of utterances to be produce</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>Konrad Scheffler and Steve Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder P Singh</author>
<author>Michael J Kearns</author>
<author>Diane J Litman</author>
<author>Marilyn A Walker</author>
</authors>
<title>Empirical evaluation of a reinforcement learning spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence.</booktitle>
<contexts>
<context position="9339" citStr="Singh et al. (2000)" startWordPosition="1476" endWordPosition="1479">er based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirectly evaluates the simulator, it is inappropriate as a sole measure of quality. There has been far less evaluation of simulators without a dialog manager. The main approach is to compute precision and recall on an utterance basis, which is intended to measure the similarity between real u</context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 2000</marker>
<rawString>Satinder P. Singh, Michael J. Kearns, Diane J. Litman, and Marilyn A. Walker. 2000. Empirical evaluation of a reinforcement learning spoken dialogue system. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1739" citStr="Sutton and Barto, 1998" startWordPosition="247" endWordPosition="250"> noise. We evaluate on two standard dialog resources, the Communicator and Let’s Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs. 1 Introduction Automatically simulating user behaviour in humanmachine dialogs has become vital for training statistical dialog managers in task-oriented domains. These managers are often trained with some variant of reinforcement learning (Sutton and Barto, 1998), where optimal behaviour is sought or learnt through the exploration of the space of possible dialogs. Although learning by interacting with human 71 subjects is a possibility (Ga˘si´c et al., 2011), it has been argued that user simulation avoids the expensive, labour intensive, and error-prone experience of exposing real humans to fledgling dialog systems (Eckert et al., 1997). Training effective dialog managers should benefit from exposure to properties exhibited by real users. Table 1 shows an example dialog in a domain such as we consider, where the objective is to simulate at the semanti</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Evaluating user simulations with the Cramer-von Mises divergence.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>10</issue>
<contexts>
<context position="9051" citStr="Williams (2008)" startWordPosition="1433" endWordPosition="1434">cause they have been so inextricably linked to dialog managers. The most popular method of evaluation relies on generating synthetic dialogs through the interaction of the user simulator with some dialog manager. Schatzmann et al. (2005) hand-craft a simple deterministic dialog manager based on finite automata, and compute similarity measures between these synthetically produced dialogs and real dialogs. Georgila et al. (2006) use a scoring function to evaluate synthetic dialogs using accuracy, precision, recall, and perplexity, while Schatzmann et al. (2007b) rely on dialog completion rates. Williams (2008) use a Cramer–von Mises test, a hypothesis test to determine whether simulated and real dialogs are significantly different, while Janarthanam and Lemon (2009) use Kullback Leibler Divergence between the empirical distributions over acts in real and simulated dialogs. Singh et al. (2000) and Ai and Litman (2008) judge the consistency of human quality ranked synthetic dialogs generated by different simulators interacting with the IT-SPOKE dialog system. Schatzmann et al. (2007b) use a simulator to train a statistical dialog manager and then evaluate the learned policy. Because this only indirec</context>
</contexts>
<marker>Williams, 2008</marker>
<rawString>Jason D. Williams. 2008. Evaluating user simulations with the Cramer-von Mises divergence. Speech Communication, 50(10):829–846, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
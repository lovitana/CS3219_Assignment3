<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056524">
<title confidence="0.998515">
A Sequence Labelling Approach to Quote Attribution
</title>
<author confidence="0.980626">
Tim O’Keefe† Silvia Pareti° James R. Curran† Irena Koprinska† Matthew Honnibal‡
</author>
<affiliation confidence="0.987019">
†-lab, School of IT *School of Informatics ‡Centre for Language Technology
University of Sydney University of Edinburgh Macquarie University
</affiliation>
<address confidence="0.645839">
NSW 2006, Australia United Kingdom NSW 2109, Australia
</address>
<email confidence="0.996854">
{tokeefe,james,irena}@it.usyd.edu.au S.Pareti@sms.ed.ac.uk matthew.honnibal@mq.edu.au
</email>
<sectionHeader confidence="0.995545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999347928571429">
Quote extraction and attribution is the task of
automatically extracting quotes from text and
attributing each quote to its correct speaker.
The present state-of-the-art system uses gold
standard information from previous decisions
in its features, which, when removed, results
in a large drop in performance. We treat the
problem as a sequence labelling task, which
allows us to incorporate sequence features
without using gold standard information. We
present results on two new corpora and an aug-
mented version of a third, achieving a new
state-of-the-art for systems using only realis-
tic features.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999854333333334">
News stories are often driven by the quotes made
by politicians, sports stars, musicians, and celebri-
ties. When these stories exit the news cycle, the
quotes they contain are often forgotten by both read-
ers and journalists. A system that automatically ex-
tracts quotes and attributes those quotes to the cor-
rect speaker would enable readers and journalists to
place news in the context of all comments made by
a person on a given topic.
Though quote attribution may appear to be a
straightforward task, the simple rule-based ap-
proaches proposed thus far have produced disap-
pointing results. Going beyond these to machine
learning approaches presents several problems that
make quote attribution surprisingly difficult. The
main challenge is that while a large portion of quotes
can be attributed to a speaker based on simple rules,
the remainder have few or no contextual clues as
to who the correct speaker is. Additionally, many
quote sequences, such as dialogues, rely on the
reader understanding that there is an alternating se-
quence of speakers, which creates dependencies be-
tween attribution decisions made by a classifier.
Elson and McKeown (2010) is the only study that
directly uses machine learning in quote attribution,
treating the task as a classification task, where each
quote is attributed independently of other quotes. To
handle conversations and similar constructs they use
gold standard information about speakers of previ-
ous quotes as features for their model. This is an
unrealistic assumption, since gold standard informa-
tion is not available in practice.
The primary contribution of this paper is that we
reformulate quote attribution as a sequence labelling
task. This allows us to use sequence features with-
out having to use the unrealistic gold standard fea-
tures that were used in Elson and McKeown (2010).
We experiment with three sequence decoding mod-
els including greedy, Viterbi and a linear chain Con-
ditional Random Field (CRF).
Furthermore we present results on two new cor-
pora and an augmented version of a third. The two
new corpora are from news articles from the Wall
Street Journal and the Sydney Morning Herald re-
spectively, while the third corpus is an extension to
the classic literature corpus from Elson and McK-
eown (2010). Our results show that a quote attri-
bution system using only realistic features is highly
feasible for the news domain, with accuracies of
92.4% on the SMH corpus and 84.1% on the WSJ
corpus.
</bodyText>
<page confidence="0.952189">
790
</page>
<note confidence="0.94842">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 790–799, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.967703" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999839638297872">
Early work into quote attribution by Zhang et al.
(2003) focused on identifying when different char-
acters were talking in children’s stories, so that a
speech synthesis system could read the quoted parts
in different voices. While they were able to ex-
tract quotes with high precision and recall, their at-
tribution accuracy was highly dependent on the doc-
ument in question, ranging from 47.6% to 86.7%.
Mamede and Chaleira (2004) conducted similar re-
search on children’s stories written in Portuguese.
Their system proved to be very good at extracting
quotes through simple rules, but when using a hand-
crafted decision tree to attribute those quotes to a
speaker, they achieved an accuracy of only 65.7%.
In the news domain, both Pouliquen et al. (2007)
and Sarmento and Nunes (2009) proposed rule-
based systems that work over large volumes of text.
Both systems aimed for high precision at the ex-
pense of low recall, as their data contained many re-
dundant quotes. More recently, SAPIENS, a French-
language quote extraction and attribution system,
was developed by de La Clergerie et al. (2011). It
conducts a full parse of the text, which allows it to
use patterns to extract direct and indirect quotes, as
well as the speaker of each quote. Their evaluation
found that 19 out of 40 quotes (47.5%) had a correct
span and author, while a further 19 had an incorrect
author, and 4 had an incorrect span. In related work,
Sagot et al. (2010) built a lexicon of French reported
speech verbs, and conducted some analysis of dif-
ferent types of quotes.
Glass and Bangay (2007) approached the task
with a three stage method. For each quote they
first find the nearest speech verb, they then find the
grammatical actor of that speech verb, and finally
they select the appropriate speaker for that actor. To
achieve each of these subtasks they built a model
with several manually weighted features that good
candidates should possess. For each subtask they
then choose the candidate with the largest weighted
sum of features. Their full approach yields an ac-
curacy of 79.4% on a corpus of manually annotated
fiction books.
Schneider et al. (2010) describe PICTOR, which
is principally a quote visualisation tool. Their task
was to find direct and indirect quotes, which they
attribute to a text span representing the speaker.
To do this they constructed a specialised grammar,
which was built with reference to a small develop-
ment corpus. With a permissive evaluation metric
their grammar-based approach yielded 86% recall
and 75% precision, however this dropped to 52% re-
call and 56% precision when measured in terms of
completely correct quote-speaker pairs.
The work most similar to ours is the work by El-
son and McKeown (2010). Their aim was to au-
tomatically identify both quotes and speakers, and
then to attribute each quote to a speaker, in a corpus
of classic literature that they compiled themselves.
To identify potential speakers they used the Stanford
NER tagger (Finkel et al., 2005) and a method out-
lined in Davis et al. (2003) that allowed them to find
nominal character references. They then grouped
name variants and pronominal mentions into a coref-
erence chain.
To attribute a quote to a speaker they first classi-
fied the quotes into categories. Several of the cat-
egories have a speaker explicit in their structure,
so they attribute quotes to those speakers with no
further processing. For the remaining categories,
they cast the attribution problem as a binary clas-
sification task, where each quote-speaker pair has
a “speaker” or “not speaker” label predicted by the
classifier. They then reconciled these independent
decisions using various techniques to produce a sin-
gle speaker prediction for each quote. For the sim-
ple category predictions they achieved 93-99% ac-
curacy, while for the more complicated categories
they achieved 63-64%, with an overall result of 83%
accuracy. This compares favourably with their rule-
based baseline, which achieved an accuracy of 52%.
While the results of Elson and McKeown (2010)
appear encouraging, they are misleading for two rea-
sons. First their corpus does not include quotes
where all three annotators chose different speakers.
While these quotes include some cases where the
annotators chose coreferent spans, it also includes
cases of legitimate disagreement about the speaker.
An automated system would likely find these cases
challenging. Second both their category predictions
and machine learning predictions rely on gold stan-
dard information from previous quotes, which is not
available in practice. In our study we address both
these issues.
</bodyText>
<page confidence="0.996789">
791
</page>
<table confidence="0.999968615384615">
Proportion (%) Accuracy (%)
LIT WSJ SMH LIT WSJ SMH
Quote-Said-Person 17.9 20.2 3.1 98.9 99.8 99.1
Quote-Person-Said 2.8 6.1 16.6 97.7 97.0 98.5
Other Trigram 0.1 2.3 0.3 66.7 56.2 54.5
Quote-Said-Pronoun 1.9 0.1 0.0 38.6 100.0 0.0
Quote-Pronoun-Said 5.9 8.8 13.5 36.5 92.2 93.9
Other Anaphors 0.1 0.1 0.2 0.0 100.0 62.5
Added* 24.6 28.3 23.9 89.7 76.3 97.5
Backoff 11.0 33.9 32.3 - - -
Alone 18.0 0.2 9.7 - - -
Conversation* 17.7 0.2 0.3 85.2 0.0 8.3
Total 100.0 100.0 100.0 60.5 57.2 55.8
</table>
<tableCaption confidence="0.9726985">
Table 1: The proportion of quotes in each category and the accuracy of the speaker prediction based on the category.
The two categories marked with an asterisk (*) depend on previous decisions.
</tableCaption>
<sectionHeader confidence="0.995357" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.999954">
We evaluate our methods on two new corpora com-
ing from the news domain, and an augmented ver-
sion of an existing corpus, which covers classic lit-
erature. They are described below.
</bodyText>
<subsectionHeader confidence="0.998345">
3.1 Columbia Quoted Speech Attribution
Corpus (LIT)
</subsectionHeader>
<bodyText confidence="0.999994103448276">
The first corpus we use was originally created by
Elson and McKeown (2010). It is a set of excerpts
from 11 fictional 19th century works by six well-
known authors, split into 18 documents. In total it
contains 3,126 quotes annotated with their speakers.
Elson and McKeown used an automated system
to find named entity spans and nominal mentions in
the text, with the named entities being linked to form
a coreference chain (they did not link nominal men-
tions). The corpus was built using Amazon’s Me-
chanical Turk, with three annotations per quote. To
ensure quality, all annotations from poorly perform-
ing annotators were removed, as were quotes where
each annotator chose a different speaker. Though
excluding some quotes ensures quality annotations,
it causes gaps in the quote chains, which is a prob-
lem for sequence labelling. Furthermore, the cases
where annotators disagreed are likely to be challeng-
ing, so removing them from the corpus could make
results appear better than they would be in practice.
To rectify this, we conducted additional annota-
tion of the quotes that were excluded by the origi-
nal authors. Two postgraduates annotated 654 addi-
tional quotes, with a raw agreement of 79% over 48
double-annotated quotes. Our annotators reported
seeing some errors in existing annotations, so we
had one annotator check 400 existing annotations for
correctness. This additional check found that 92.5%
of the quotes were correctly annotated.
</bodyText>
<subsectionHeader confidence="0.99759">
3.2 PDTB Attribution Corpus Extension (WSJ)
</subsectionHeader>
<bodyText confidence="0.999972857142857">
Our next corpus is an extension to the attribution
annotations found in the Penn Discourse TreeBank
(PDTB). The original PDTB contains several forms
of discourse, including assertions, beliefs, facts, and
eventualities. These can be attributed to named enti-
ties or to unnamed, pronominal, or implicit sources.
Recent work by Pareti (2012) conducted further an-
notation of this corpus, including reconstructing at-
tributions that were only partially annotated, and in-
troducing additional information. From this corpus
we use only direct quotes and the directly quoted
portions of mixed quotes, giving us 4,923 quotes.
For the set of potential speakers we use the
BBN pronoun coreference and entity type cor-
pus (Weischedel and Brunstein, 2005), with auto-
matically coreferred pronouns. We automatically
matched BBN entities to PDTB extension speakers,
and included the PDTB speaker where no matching
BBN entity could be found. This means an automatic
system has an opportunity to find the correct speaker
for all quotes in the corpus.
</bodyText>
<page confidence="0.98251">
792
</page>
<subsectionHeader confidence="0.995779">
3.3 Sydney Morning Herald Corpus (SMH)
</subsectionHeader>
<bodyText confidence="0.999934470588235">
We compiled the final corpus from a set of news
documents taken from the Sydney Morning Her-
ald website1. We randomly selected 965 documents
published in 2009 that were not obituaries, opin-
ion pages, advertisements or other non-news sto-
ries. To conduct the annotation we employed 11
non-expert annotators via the outsourcing site Free-
lancer2, as well as five expert annotators from our
research group. A total of 400 news stories were
double-annotated, with at least 33 double-annotated
stories per annotator. Raw agreement on the speaker
of each quote was high at 98.3%. These documents
had already been annotated with named entities as
part of a separate research project (Hachey et al.,
2012), which includes manually constructed coref-
erence chains. The resulting corpus contains 965
documents, with 3,535 quotes.
</bodyText>
<subsectionHeader confidence="0.987794">
3.4 Corpus Comparisons
</subsectionHeader>
<bodyText confidence="0.999141666666667">
In order to compare the corpora we categorise the
quotes into the categories defined by Elson and
McKeown (2010), as shown in Table 1. We assigned
quotes to these categories by testing (after text pre-
processing) whether the quote belonged to each cat-
egory, in the order shown below:
</bodyText>
<listItem confidence="0.999012066666667">
1. Trigram – the quote appears consecutively with
a mention of an entity, and a reported speech
verb, in any order;
2. Anaphors – same as above, except that the men-
tion is a pronoun;
3. Added – the quote is in the same paragraph as
another quote that precedes it;
4. Conversation – the quote appears in a para-
graph on its own, and the two paragraphs pre-
ceding the current paragraph each contain a sin-
gle quote, with alternating speakers;
5. Alone – the quote is in a paragraph on its own;
6. Miscellaneous – the quote matches none of the
preceding categories. This category is called
“Backoff” in Elson and McKeown (2010).
</listItem>
<footnote confidence="0.999896">
1http://www.smh.com.au
2http://www.freelancer.com
</footnote>
<bodyText confidence="0.999988571428572">
Unsurprisingly, the two corpora from the news do-
main share similar proportions of quotes in each
category. The main differences are that the SMH
uses a larger number of pronouns compared to the
WSJ, which tends to use explicit attribution more fre-
quently. The SMH also has a significant proportion
of quotes that appear alone in a paragraph, while
the WSJ has almost none. Finally, when attribut-
ing a quote using a trigram pattern, the SMH mostly
uses the Quote-Person-Said pattern, while the WSJ
mostly uses the Quote-Said-Person pattern. These
differences probably reflect the editorial guidelines
of the two newspapers.
The differences between the news corpora and
the literature corpus are more substantial. Most no-
tably the LIT corpus has a much higher proportion
of quotes that fall into the Conversation and Alone
categories. This is unsurprising as both monologues
and dialogues are common in fiction, but are rare in
newswire. The two news corpora have more quotes
in the Trigram and Backoff categories.
</bodyText>
<sectionHeader confidence="0.981075" genericHeader="method">
4 Quote Extraction
</sectionHeader>
<bodyText confidence="0.995780333333333">
Quote extraction is the task of finding the spans that
represent quotes within a document. There are three
types of quotes that can appear:
</bodyText>
<listItem confidence="0.997388">
1. Direct quotes appear entirely between quota-
tion marks, and are used to indicate that the
speaker said precisely what is written;
2. Indirect quotes do not appear between or con-
tain quotation marks, and are used to get the
speaker’s point across without implying that
the speaker used the exact words of the quote;
3. Mixed quotes are indirect quotes that contain a
directly quoted portion.
</listItem>
<bodyText confidence="0.9986736">
In this work, we limit ourselves to detecting direct
quotes and the direct portions of mixed quotes.
To extract quotes we use a regular expression that
searches for text between quotation marks. We also
deal with the special case of multi-paragraph quotes
where one quotation mark opens the quote and every
new paragraph that forms part of the quote, with a fi-
nal quotation mark only at the very end of the quote.
This straightforward approach yields over 99% ac-
curacy on all three corpora.
</bodyText>
<page confidence="0.997479">
793
</page>
<sectionHeader confidence="0.991682" genericHeader="method">
5 Quote Attribution
</sectionHeader>
<bodyText confidence="0.999984304347826">
Given a document with a set of quotes and a set
of entities, quote attribution is the task of finding
the entity that represents the speaker of each quote,
based on the context provided by the document.
Identifying the correct entity can involve choosing
either an entire coreference chain representing an
entity, or identifying a specific span of text that rep-
resents the entity.
In practice, most applications only need to know
which coreference chain represents the speaker, not
which particular span in the text. Despite this, the
best evidence about which chain is the speaker is
found in the context of the individual text spans, and
most existing systems aim to get the particular entity
span correct. This presents a problem for evaluation,
as an incorrect entity span may be identified, but it
might still be part of the correct coreference chain.
We chose to count attributions as correct if they at-
tributed the quote to the correct coreference chain
for both the LIT and SMH corpora, while for the WSJ
corpus, where the full coreference chains do not ex-
ist, we evaluated an attribution as correct if it was to
the correct entity span in the text.
</bodyText>
<subsectionHeader confidence="0.963539">
5.1 Rule-based Baseline
</subsectionHeader>
<bodyText confidence="0.999939333333333">
To establish the effectiveness of our method we built
a rule-based baseline system. For each quote it pro-
ceeds with the following steps:
</bodyText>
<listItem confidence="0.983914909090909">
1. Search backwards in the text from the end of
the sentence the quote appears in for a reported
speech verb
2. If the verb is found return the entity mention
nearest the verb (ignoring mentions in quotes),
in the current sentence or any sentence preced-
ing it
3. If not, return the mention of an entity near-
est the end of the quote (ignoring mentions in
quotes), in the current sentence or any sentence
preceding it
</listItem>
<bodyText confidence="0.999855">
This forms a reasonable baseline as it is able to pick
up the quotes that fall into the more simple cate-
gories, such as the Trigram category and the Added
category. It is also able to make a guess at the more
complicated categories, without using gold standard
information as the category predictions do.
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999976909090909">
We use two classifiers: a logistic regression imple-
mentation available in LIBLINEAR (Fan et al., 2008),
and a Conditional Random Field (CRF) from CRF-
Suite (Okazaki, 2007). Both packages use maxi-
mum likelihood estimation with L2 regularisation.
We experimented with several values for the coef-
ficient on a development set, but found that it had
little impact, so stuck with the default value. All of
our machine learning experiments use the same text
encoding, which is explained below, and all use the
category predictions when they are available.
</bodyText>
<subsectionHeader confidence="0.996059">
6.1 Text Encoding
</subsectionHeader>
<bodyText confidence="0.999388">
We encode our text similarly to Elson and McKeown
(2010). The major steps are:
</bodyText>
<listItem confidence="0.973642333333333">
1. Replace all quotes and speakers with special
symbols;
2. Replace all reported speech verbs with a sym-
bol. Elson and McKeown (2010) provided us
with their list of reported speech verbs;
3. Part-of-Speech (POS) tag the text and remove
adjectives, adverbs, and other parts of speech
that do not contribute useful information. We
used the POS tagger from Curran and Clark
(2003);
4. Remove any paragraphs or sentences where no
quotes, pronouns or names occur.
</listItem>
<bodyText confidence="0.99861025">
All features that will be discussed are calculated
with respect to this encoding (e.g. word distance
would be the number of words in the encoded text,
rather than the number of words in the original text).
</bodyText>
<subsectionHeader confidence="0.966676">
6.2 Features
</subsectionHeader>
<bodyText confidence="0.999437875">
In our experiments we use the feature set from Elson
and McKeown (2010). The features for a particu-
lar pair of target quote (q) and target speaker (s) are
summarised below.
Distance features including number of words be-
tween q and s, number of paragraphs between
q and s, number of quotes between q and s, and
number of entity mentions between q and s
</bodyText>
<page confidence="0.991753">
794
</page>
<table confidence="0.9912868">
Sequence Features
Gold Pred None
LIT 74.7 49.0 49.6
WSJ 87.3 74.1 82.9
SMH 95.0 85.6 92.4
</table>
<tableCaption confidence="0.99655">
Table 2: Accuracy results comparing the E&amp;M approach
with gold standard, predicted or no sequence features.
</tableCaption>
<bodyText confidence="0.999862090909091">
Paragraph features derived from the 10 para-
graphs preceding the quote (including the para-
graph the quote is in), includes number of men-
tions of s, number of mentions of other speak-
ers, number of words in each paragraph, and
number of quotes in each paragraph
Nearby features relating to the two tokens either
side of q and s, includes binary features for
each position indicating whether the position is
punctuation, s, q, a different speaker, a differ-
ent quote, or a reported speech verb
Quote features about q itself, including whether s
is mentioned within it, whether other speakers
are mentioned within it, how far the quote is
from the start of its paragraph and the length in
words of q
Sequence features that depend on the speakers
chosen for the previous quotes, includes num-
ber of quotes in the 10 paragraphs preceding
and including the paragraph where q appears
that were attributed to s, and the number that
were attributed to other speakers
</bodyText>
<subsectionHeader confidence="0.998389">
6.3 Elson and McKeown Reimplementation
</subsectionHeader>
<bodyText confidence="0.999991024390244">
As part of our study we reproduce the core results
of Elson and McKeown (2010) (E&amp;M ), as we be-
lieve it is a state-of-the-art system. This allows us
to determine the effectiveness of our approach when
compared to a state-of-the-art approach, and it also
allows us to determine how well the E&amp;M approach
performs on other corpora. In this section we will
briefly summarise the key elements needed to repro-
duce their work.
The E&amp;M approach makes a binary classification
between “speaker” and ‘not speaker” for up to 15
candidate speakers for each quote. They then recon-
cile these 15 classifications into one speaker predic-
tion for the quote. While E&amp;M experimented with
several different reconciliation methods, we simply
chose the speaker with the highest probability at-
tached to its “speaker” label.
We conducted an experiment using our imple-
mentation of the E&amp;M method on the original,
unaugmented E&amp;M corpus, to see how our result
compared with E&amp;M ’s 83%. On our test set we
achieved 78.2%, however this rose to 82.3% when
performing 10-fold cross validation across the whole
corpus. Though this is a large difference, it is not
necessarily that surprising, as our test set contains
documents by authors which are unseen, whereas
both the original E&amp;M test set and all the cross val-
idation test sets contain documents by authors that
the learner has seen before.
In their work, E&amp;M make a simplifying assump-
tion that all previous attribution decisions were cor-
rect. Due to this, their sequence features use gold
standard labels from previous quotes, which makes
their results unrealistic. In Table 2 we show the ef-
fect of replacing the gold standard sequence features
with features based on the predicted labels, or with
no sequence features at all. All three corpora show a
significant drop in accuracy, with the LIT corpus in
particular suffering a drop of more than 25%. This
motivates our study into including sequence infor-
mation without using gold standard labels.
</bodyText>
<sectionHeader confidence="0.994983" genericHeader="method">
7 Class Models
</sectionHeader>
<bodyText confidence="0.999908">
We consider two class models for our experiments,
which are described in detail below. The binary
model is able to take advantage of more data but has
less competition between decisions, while the n-way
model has more competition with less data. Both
models are used with all the decoding methods, with
the exception that the binary model is unsuitable for
the CRF experiments.
</bodyText>
<subsectionHeader confidence="0.976998">
7.1 Binary
</subsectionHeader>
<bodyText confidence="0.999915714285714">
When working with n previous speakers, a binary
class model works by predicting n independent
“speaker” versus “not speaker” labels, one for each
quote-speaker pair. As the classifications are inde-
pendent the n decisions need to be reconciled, as
more than one speaker might be predicted. We rec-
oncile the n decisions by attributing the quote to the
</bodyText>
<note confidence="0.531092">
Corpus
</note>
<page confidence="0.99725">
795
</page>
<bodyText confidence="0.999869166666667">
speaker with the highest “speaker” probability. Us-
ing a binary class with reconciliation in a greedy
decoding model is equivalent to the method in El-
son and McKeown (2010), except that the gold stan-
dard sequence features are replaced with predicted
sequence features.
</bodyText>
<subsectionHeader confidence="0.944952">
7.2 n-way
</subsectionHeader>
<bodyText confidence="0.99998803030303">
A key advantage of the binary class model is that
when predicting “speaker” versus “not speaker” the
classifier only needs to predict one probability, and
thus can take into account the evidence of all other
quote-speaker pairs. The drawback to the binary
model is that the probabilities assigned to the can-
didate speakers do not need to directly compete
against each other. In other words when assigning
a binary probability to a candidate speaker, the clas-
sifier does not take into account how good the other
candidate speakers are.
To rectify these issues we experiment with a sin-
gle classification for each quote, where the classifier
directly decides between up to n candidate speakers
per quote. As speaker-specific evidence is far too
sparse, we encode the speakers with their ordinal po-
sition backwards from the quote. In other words, the
candidate speaker immediately preceding the quote
would be labelled “speaker1”, the speaker preced-
ing it would be “speaker2” and so on. The classifier
then directly predicts these labels. This representa-
tion means that candidate speakers need to directly
compete for probability mass, although it has the
drawback that the evidence for the higher-numbered
speakers is quite sparse.
The features we use for this representation are
similar to the features used in the E&amp;M binary
model. The key difference is that where there were
individual features that were calculated with respect
to the speaker, there are now n features, one for each
of the speaker candidates. This allows the model to
account for the strength of other candidates when as-
signing a speaker label.
</bodyText>
<sectionHeader confidence="0.869788" genericHeader="method">
8 Sequence Decoding
</sectionHeader>
<bodyText confidence="0.999784772727273">
We noted in the previous section that the E&amp;M re-
sults are based on the unrealistic assumption that
all previous quotes were attributed correctly. In
this section we outline three sequence decoding ap-
proaches that remove this unrealistic assumption,
without removing all of the transition information
that it provides. We believe the transition infor-
mation is important as many quotes have no ex-
plicit attribution in the text, and instead rely on the
reader understanding something about the sequence
of speakers.
For these experiments we regard the set of speaker
attributions in a document as the sequence that we
want to decode. Each individual state therefore rep-
resents a sequence of w previous attribution deci-
sions, and a decision for the current quote. Obtain-
ing a probability for this state can be done in one
of two ways. Either the transition probabilities from
state to state can be learned explicitly, or the w pre-
vious attribution decisions can be used to build the
sequence features for the current state, which im-
plicitly encodes the transition probabilities.
</bodyText>
<subsectionHeader confidence="0.995686">
8.1 Greedy Decoding
</subsectionHeader>
<bodyText confidence="0.999987125">
In sequence decoding the greedy algorithm calcu-
lates the probability of each label at a decision point
based on the predictions it has already made for pre-
vious decisions. More concretely this means we ap-
ply a standard classifier at each step, with the se-
quence features being calculated from the predic-
tions made in previous steps. Greedy decoding is
efficient in that it only considers one possible history
at each decision point, but it is consequently unable
to make trade-offs between good previous choices
and good current choices, which means that in gen-
eral it will not return the optimum sequence of la-
bels. As greedy decoding is an efficient algorithm
we do not restrict w, the number of previous deci-
sions, beyond the 10 paragraph restriction that is al-
ready in place.
</bodyText>
<subsectionHeader confidence="0.999011">
8.2 Viterbi Decoding
</subsectionHeader>
<bodyText confidence="0.9999713">
Viterbi decoding finds the most probable path
through a sequence of decisions. It does this by de-
termining the probabilities of each of the labels at
the current decision point, with each of the possi-
ble histories of decisions within a given window w.
These probabilities can be multiplied together with
the previous decisions to retrieve a joint probability
for the entire sequence. The final decision for each
quote is then just the speaker which is predicted by
the sequence with the largest joint probability.
</bodyText>
<page confidence="0.995892">
796
</page>
<bodyText confidence="0.99998925">
Although they do not come with probabilities,
we chose to include the category predictions in our
Viterbi model. As we already know that they are
accurate indicators of the speaker we assign them
a probability of 100%, which effectively forces the
Viterbi decoder to choose the category predictions
when they are available. It is worth noting that
quotes are only assigned to the Conversation cate-
gory if the two prior quotes had alternating speakers.
As such, during the Viterbi decoding the categori-
sation of the quote actually needs to be recalculated
with regard to the two previous attribution decisions.
By forcing the Viterbi decoder to choose category
predictions when they are available, we get the ad-
vantage that quote sequences with no intervening
text may be forced into the Conversation category,
which is typically under-represented otherwise.
Both the sequences using the binary class and
the n-way class can be decoded using the Viterbi
algorithm, so we experiment with both class mod-
els. We also experiment with varying window sizes
(w), in order to gain insight into how many previous
decisions impact the current decision. Though the
Viterbi algorithm is able to find the best sequence
of probabilities without the need for an exhaustive
search, it can still take an impractical amount of time
to run. As such we ignore all but the 10 most promis-
ing sequences at each decision point.
</bodyText>
<subsectionHeader confidence="0.9975">
8.3 Conditional Random Field (CRF) Decoding
</subsectionHeader>
<bodyText confidence="0.999932689655172">
The key drawback with the logistic regression ex-
periments described thus far is that the sequence
features are trained with gold standard information.
This means that during the training phase the se-
quence features have perfect information about pre-
vious speakers and are thus unrealistically good pre-
dictors of the final outcome. When the resulting
model is used with the less accurate predicted se-
quence features, it is overconfident about the infor-
mation those features provide.
We account for this by using a first-order linear
chain CRF model, which learns the probabilities of
progressing from speaker to speaker more directly.
During training the CRF is able to learn the asso-
ciation between features and labels, as well as the
chance of transitioning from one label to the next.
It also has the advantage of avoiding the label bias
problem that would be present in the equivalent Hid-
den Markov Model (Lafferty et al., 2001).
Though the n-way class model can be used di-
rectly in a CRF, the binary class model is more chal-
lenging. The main problem is that the “speaker”
versus “not speaker” output of the binary classifier
does not directly form a meaningful sequence that
the CRF can learn over. If the reconciliation step is
included it effectively adds an extra layer to the lin-
ear chain, making learning more difficult. Due to
these difficulties we only use the n-way class model
in our CRF experiments.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="evaluation">
9 Results
</sectionHeader>
<bodyText confidence="0.999978911764706">
The main result of our experiments with the E&amp;M
method is the large drop in accuracy that occurs
when the gold standard sequence features are re-
moved, which can be seen in Table 3. When using
the binary class model this results in a drop of 25.1%
for the LIT corpus, while for the WSJ and SMH cor-
pora the drop is less substantial at 4.4% and 2.6%,
respectively. For the LIT corpus the drop is so severe
that it actually performs worse than the simple rule-
based system. Even more surprisingly, when the
predictions from previous decisions are used with a
simple greedy decoder, the accuracy drops even fur-
ther for all three corpora. This indicates that the clas-
sifier is putting too much weight on the gold stan-
dard sequence features during training, and is mis-
led into making poor decisions when the predicted
features are used during test time.
Table 4 shows the results for the n-way class
model. Compared to the binary model, the n-way
class model generally produced lower results, al-
though the results were more stable to changes in
parameters and decoders. The only corpus that pro-
duced better results with the n-way class model was
the WSJ corpus, which does not have full entity
coreference information. This indicates that the n-
way model may be helpful when there is more vari-
ety in the choice of entities.
The final results we would like to discuss here are
the CRF results. On all three corpora the CRF results
are underwhelming. The major issue that we can
see when applying a CRF model to this task is that
the sequences that it needs to learn over are entire
documents. This means that for the LIT corpus the
training set consisted of only 12 sequences, while
</bodyText>
<page confidence="0.990293">
797
</page>
<table confidence="0.9590408">
Corpus E&amp;M Rule No seq. Greedy Viterbi
w = 1 w = 2 w = 5
LIT 74.7 53.3 49.6 49.0 46.0 49.8 45.9
WSJ 87.3 77.9 82.9 74.1 82.3 83.1 83.1
SMH 95.0 91.2 92.4 85.6 91.7 90.5 84.1
</table>
<tableCaption confidence="0.9506135">
Table 3: Accuracy on test set with the binary class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
</tableCaption>
<table confidence="0.999336">
Corpus Gold seq. Rule No seq. Greedy Viterbi CRF
w = 1 w = 2 w = 5
LIT 68.6 53.3 47.1 46.7 42.5 46.5 44.4 48.6
WSJ 88.9 77.9 83.6 77.0 84.1 83.7 83.3 79.6
SMH 94.4 91.2 90.0 89.6 89.5 90.1 90.4 91.0
</table>
<tableCaption confidence="0.9442345">
Table 4: Accuracy on test set with the n-way class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
</tableCaption>
<bodyText confidence="0.999895555555556">
the test set consisted of 6 sequences. With so few
sequences it is unsurprising that the CRF model did
not perform well. The limited range of the first order
linear chain model could also have played a part in
the poor performance of the CRF models. However,
moving to a higher-order model is problematic as
the number of transition probabilities that need to be
calculated increases exponentially with the order of
the model.
</bodyText>
<sectionHeader confidence="0.994698" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999967566666667">
In this paper, we present the first large-scale evalua-
tion of a quote attribution system on newswire from
the 1989 Wall Street Journal (WSJ) and the 2009
Sydney Morning Herald (SMH), as well as compar-
ing against previous work (Elson and McKeown,
2010) on 19th-century literature.
We show that when Elson and McKeown’s unre-
alistic use of gold-standard history information is re-
moved, accuracy on all three corpora drops substan-
tially. We demonstrate that by treating quote attribu-
tion as a sequence labelling task, we can achieve re-
sults that are very close to their results on newswire,
though not for literature.
In future work, we intend to further explore the
sequence features that have a large impact on accu-
racy, and to find similar features or proxies for the
sequence features that would be beneficial. We will
also explore other approaches to representing quote
attribution with a CRF. For the task more broadly,
it would be beneficial to compare methods of find-
ing indirect and mixed quotes, and to evaluate how
well quote attribution performs on those quotes as
opposed to just direct quotes.
Our newswire results, 92.4% for the SMH and
84.1% for the WSJ corpus, demonstrate it is possible
to develop an accurate and practical quote extraction
system. On the LIT corpus our best result was from
the simple rule-based system, which yielded 53.3%.
It is clear that literature poses an ongoing research
challenge.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995272">
We would like to thank David Elson for helping
us to reimplement his method and Bonnie Webber
for her feedback and assistance. O’Keefe has been
supported by a University of Sydney Merit schol-
arship and a Capital Markets CRC top-up scholar-
ship; Pareti has been supported by a Scottish Infor-
matics and Computer Science Alliance (SICSA) stu-
dentship. This work has been supported by ARC
Discovery grant DP1097291 and the Capital Mar-
kets CRC Computable News project.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.953643333333333">
James R. Curran and Stephen Clark. 2003. Investi-
gating GIS and smoothing for maximum entropy
taggers. In Proceedings of the tenth conference on
</reference>
<page confidence="0.97706">
798
</page>
<reference confidence="0.999228721518988">
European chapter of the Association for Compu-
tational Linguistics, pages 91–98.
Peter T. Davis, David K. Elson, and Judith L. Kla-
vans. 2003. Methods for precise named entity
matching in digital collections. In Proceedings of
the 3rd ACM/IEEE-CS Joint Conference on Digi-
tal libraries, pages 125–127.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522–532.
David. K Elson and Kathleen. R McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of AAAI, pages 1013–
1019.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–
1874.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher Manning. 2005. Incorporating non-local in-
formation into information extraction systems by
gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, pages 363–370.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1–6.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2012. Evaluating
entity linking with Wikipedia. Artificial Intelli-
gence. (in press).
John Lafferty, Andrew McCallum, and Fernando
C.N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. International Conference on Ma-
chine Learning, pages 282–289.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82–90.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of Conditional Random Fields
(CRFs). URL http://www.chokkan.org/
software/crfsuite/.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion (LREC’12), pages 3213–3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487–492.
Benoit Sagot, Laurence Danlos, and Rosa Stern.
2010. A lexicon of french quotation verbs for au-
tomatic quotation extraction. In 7th international
conference on Language Resources and Evalua-
tion - LREC 2010.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical Report
CMU-LTI-01-013, Carnegie Mellon University.
Ralph Weischedel and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. Lin-
guistic Data Consortium, Philadelphia.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children’s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041–2044.
</reference>
<page confidence="0.998655">
799
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934781">
<title confidence="0.99996">A Sequence Labelling Approach to Quote Attribution</title>
<author confidence="0.999972">R James</author>
<affiliation confidence="0.999836">School of IT of Informatics for Language Technology University of Sydney University of Edinburgh Macquarie University</affiliation>
<address confidence="0.999724">NSW 2006, Australia United Kingdom NSW 2109, Australia</address>
<email confidence="0.990841">S.Pareti@sms.ed.ac.ukmatthew.honnibal@mq.edu.au</email>
<abstract confidence="0.996255133333333">Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. The present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. We treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. We present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="18806" citStr="Curran and Clark (2003)" startWordPosition="3072" endWordPosition="3075">our machine learning experiments use the same text encoding, which is explained below, and all use the category predictions when they are available. 6.1 Text Encoding We encode our text similarly to Elson and McKeown (2010). The major steps are: 1. Replace all quotes and speakers with special symbols; 2. Replace all reported speech verbs with a symbol. Elson and McKeown (2010) provided us with their list of reported speech verbs; 3. Part-of-Speech (POS) tag the text and remove adjectives, adverbs, and other parts of speech that do not contribute useful information. We used the POS tagger from Curran and Clark (2003); 4. Remove any paragraphs or sentences where no quotes, pronouns or names occur. All features that will be discussed are calculated with respect to this encoding (e.g. word distance would be the number of words in the encoded text, rather than the number of words in the original text). 6.2 Features In our experiments we use the feature set from Elson and McKeown (2010). The features for a particular pair of target quote (q) and target speaker (s) are summarised below. Distance features including number of words between q and s, number of paragraphs between q and s, number of quotes between q </context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter T Davis</author>
<author>David K Elson</author>
<author>Judith L Klavans</author>
</authors>
<title>Methods for precise named entity matching in digital collections.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd ACM/IEEE-CS Joint Conference on Digital libraries,</booktitle>
<pages>125--127</pages>
<contexts>
<context position="6781" citStr="Davis et al. (2003)" startWordPosition="1087" endWordPosition="1090">elopment corpus. With a permissive evaluation metric their grammar-based approach yielded 86% recall and 75% precision, however this dropped to 52% recall and 56% precision when measured in terms of completely correct quote-speaker pairs. The work most similar to ours is the work by Elson and McKeown (2010). Their aim was to automatically identify both quotes and speakers, and then to attribute each quote to a speaker, in a corpus of classic literature that they compiled themselves. To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references. They then grouped name variants and pronominal mentions into a coreference chain. To attribute a quote to a speaker they first classified the quotes into categories. Several of the categories have a speaker explicit in their structure, so they attribute quotes to those speakers with no further processing. For the remaining categories, they cast the attribution problem as a binary classification task, where each quote-speaker pair has a “speaker” or “not speaker” label predicted by the classifier. They then reconciled these independent de</context>
</contexts>
<marker>Davis, Elson, Klavans, 2003</marker>
<rawString>Peter T. Davis, David K. Elson, and Judith L. Klavans. 2003. Methods for precise named entity matching in digital collections. In Proceedings of the 3rd ACM/IEEE-CS Joint Conference on Digital libraries, pages 125–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric de La Clergerie</author>
<author>Benoit Sagot</author>
<author>Rosa Stern</author>
<author>Pascal Denis</author>
<author>Gaelle Recource</author>
<author>Victor Mignot</author>
</authors>
<title>Extracting and visualizing quotations from news wires. Human Language Technology. Challenges for Computer Science and Linguistics,</title>
<date>2011</date>
<pages>522--532</pages>
<contexts>
<context position="4843" citStr="Clergerie et al. (2011)" startWordPosition="757" endWordPosition="760">s stories written in Portuguese. Their system proved to be very good at extracting quotes through simple rules, but when using a handcrafted decision tree to attribute those quotes to a speaker, they achieved an accuracy of only 65.7%. In the news domain, both Pouliquen et al. (2007) and Sarmento and Nunes (2009) proposed rulebased systems that work over large volumes of text. Both systems aimed for high precision at the expense of low recall, as their data contained many redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and attribution system, was developed by de La Clergerie et al. (2011). It conducts a full parse of the text, which allows it to use patterns to extract direct and indirect quotes, as well as the speaker of each quote. Their evaluation found that 19 out of 40 quotes (47.5%) had a correct span and author, while a further 19 had an incorrect author, and 4 had an incorrect span. In related work, Sagot et al. (2010) built a lexicon of French reported speech verbs, and conducted some analysis of different types of quotes. Glass and Bangay (2007) approached the task with a three stage method. For each quote they first find the nearest speech verb, they then find the g</context>
</contexts>
<marker>Clergerie, Sagot, Stern, Denis, Recource, Mignot, 2011</marker>
<rawString>Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pascal Denis, Gaelle Recource, and Victor Mignot. 2011. Extracting and visualizing quotations from news wires. Human Language Technology. Challenges for Computer Science and Linguistics, pages 522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Elson</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Automatic attribution of quoted speech in literary narrative.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1013--1019</pages>
<contexts>
<context position="2192" citStr="Elson and McKeown (2010)" startWordPosition="325" endWordPosition="328">aches proposed thus far have produced disappointing results. Going beyond these to machine learning approaches presents several problems that make quote attribution surprisingly difficult. The main challenge is that while a large portion of quotes can be attributed to a speaker based on simple rules, the remainder have few or no contextual clues as to who the correct speaker is. Additionally, many quote sequences, such as dialogues, rely on the reader understanding that there is an alternating sequence of speakers, which creates dependencies between attribution decisions made by a classifier. Elson and McKeown (2010) is the only study that directly uses machine learning in quote attribution, treating the task as a classification task, where each quote is attributed independently of other quotes. To handle conversations and similar constructs they use gold standard information about speakers of previous quotes as features for their model. This is an unrealistic assumption, since gold standard information is not available in practice. The primary contribution of this paper is that we reformulate quote attribution as a sequence labelling task. This allows us to use sequence features without having to use the</context>
<context position="6470" citStr="Elson and McKeown (2010)" startWordPosition="1032" endWordPosition="1036">d fiction books. Schneider et al. (2010) describe PICTOR, which is principally a quote visualisation tool. Their task was to find direct and indirect quotes, which they attribute to a text span representing the speaker. To do this they constructed a specialised grammar, which was built with reference to a small development corpus. With a permissive evaluation metric their grammar-based approach yielded 86% recall and 75% precision, however this dropped to 52% recall and 56% precision when measured in terms of completely correct quote-speaker pairs. The work most similar to ours is the work by Elson and McKeown (2010). Their aim was to automatically identify both quotes and speakers, and then to attribute each quote to a speaker, in a corpus of classic literature that they compiled themselves. To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references. They then grouped name variants and pronominal mentions into a coreference chain. To attribute a quote to a speaker they first classified the quotes into categories. Several of the categories have a speaker explicit in their structu</context>
<context position="7776" citStr="Elson and McKeown (2010)" startWordPosition="1244" endWordPosition="1247"> remaining categories, they cast the attribution problem as a binary classification task, where each quote-speaker pair has a “speaker” or “not speaker” label predicted by the classifier. They then reconciled these independent decisions using various techniques to produce a single speaker prediction for each quote. For the simple category predictions they achieved 93-99% accuracy, while for the more complicated categories they achieved 63-64%, with an overall result of 83% accuracy. This compares favourably with their rulebased baseline, which achieved an accuracy of 52%. While the results of Elson and McKeown (2010) appear encouraging, they are misleading for two reasons. First their corpus does not include quotes where all three annotators chose different speakers. While these quotes include some cases where the annotators chose coreferent spans, it also includes cases of legitimate disagreement about the speaker. An automated system would likely find these cases challenging. Second both their category predictions and machine learning predictions rely on gold standard information from previous quotes, which is not available in practice. In our study we address both these issues. 791 Proportion (%) Accur</context>
<context position="9356" citStr="Elson and McKeown (2010)" startWordPosition="1506" endWordPosition="1509">32.3 - - - Alone 18.0 0.2 9.7 - - - Conversation* 17.7 0.2 0.3 85.2 0.0 8.3 Total 100.0 100.0 100.0 60.5 57.2 55.8 Table 1: The proportion of quotes in each category and the accuracy of the speaker prediction based on the category. The two categories marked with an asterisk (*) depend on previous decisions. 3 Corpora We evaluate our methods on two new corpora coming from the news domain, and an augmented version of an existing corpus, which covers classic literature. They are described below. 3.1 Columbia Quoted Speech Attribution Corpus (LIT) The first corpus we use was originally created by Elson and McKeown (2010). It is a set of excerpts from 11 fictional 19th century works by six wellknown authors, split into 18 documents. In total it contains 3,126 quotes annotated with their speakers. Elson and McKeown used an automated system to find named entity spans and nominal mentions in the text, with the named entities being linked to form a coreference chain (they did not link nominal mentions). The corpus was built using Amazon’s Mechanical Turk, with three annotations per quote. To ensure quality, all annotations from poorly performing annotators were removed, as were quotes where each annotator chose a </context>
<context position="12799" citStr="Elson and McKeown (2010)" startWordPosition="2051" endWordPosition="2054">e Freelancer2, as well as five expert annotators from our research group. A total of 400 news stories were double-annotated, with at least 33 double-annotated stories per annotator. Raw agreement on the speaker of each quote was high at 98.3%. These documents had already been annotated with named entities as part of a separate research project (Hachey et al., 2012), which includes manually constructed coreference chains. The resulting corpus contains 965 documents, with 3,535 quotes. 3.4 Corpus Comparisons In order to compare the corpora we categorise the quotes into the categories defined by Elson and McKeown (2010), as shown in Table 1. We assigned quotes to these categories by testing (after text preprocessing) whether the quote belonged to each category, in the order shown below: 1. Trigram – the quote appears consecutively with a mention of an entity, and a reported speech verb, in any order; 2. Anaphors – same as above, except that the mention is a pronoun; 3. Added – the quote is in the same paragraph as another quote that precedes it; 4. Conversation – the quote appears in a paragraph on its own, and the two paragraphs preceding the current paragraph each contain a single quote, with alternating s</context>
<context position="18406" citStr="Elson and McKeown (2010)" startWordPosition="3005" endWordPosition="3008">p We use two classifiers: a logistic regression implementation available in LIBLINEAR (Fan et al., 2008), and a Conditional Random Field (CRF) from CRFSuite (Okazaki, 2007). Both packages use maximum likelihood estimation with L2 regularisation. We experimented with several values for the coefficient on a development set, but found that it had little impact, so stuck with the default value. All of our machine learning experiments use the same text encoding, which is explained below, and all use the category predictions when they are available. 6.1 Text Encoding We encode our text similarly to Elson and McKeown (2010). The major steps are: 1. Replace all quotes and speakers with special symbols; 2. Replace all reported speech verbs with a symbol. Elson and McKeown (2010) provided us with their list of reported speech verbs; 3. Part-of-Speech (POS) tag the text and remove adjectives, adverbs, and other parts of speech that do not contribute useful information. We used the POS tagger from Curran and Clark (2003); 4. Remove any paragraphs or sentences where no quotes, pronouns or names occur. All features that will be discussed are calculated with respect to this encoding (e.g. word distance would be the numb</context>
<context position="20732" citStr="Elson and McKeown (2010)" startWordPosition="3407" endWordPosition="3410">ifferent quote, or a reported speech verb Quote features about q itself, including whether s is mentioned within it, whether other speakers are mentioned within it, how far the quote is from the start of its paragraph and the length in words of q Sequence features that depend on the speakers chosen for the previous quotes, includes number of quotes in the 10 paragraphs preceding and including the paragraph where q appears that were attributed to s, and the number that were attributed to other speakers 6.3 Elson and McKeown Reimplementation As part of our study we reproduce the core results of Elson and McKeown (2010) (E&amp;M ), as we believe it is a state-of-the-art system. This allows us to determine the effectiveness of our approach when compared to a state-of-the-art approach, and it also allows us to determine how well the E&amp;M approach performs on other corpora. In this section we will briefly summarise the key elements needed to reproduce their work. The E&amp;M approach makes a binary classification between “speaker” and ‘not speaker” for up to 15 candidate speakers for each quote. They then reconcile these 15 classifications into one speaker prediction for the quote. While E&amp;M experimented with several di</context>
<context position="23564" citStr="Elson and McKeown (2010)" startWordPosition="3878" endWordPosition="3882"> with the exception that the binary model is unsuitable for the CRF experiments. 7.1 Binary When working with n previous speakers, a binary class model works by predicting n independent “speaker” versus “not speaker” labels, one for each quote-speaker pair. As the classifications are independent the n decisions need to be reconciled, as more than one speaker might be predicted. We reconcile the n decisions by attributing the quote to the Corpus 795 speaker with the highest “speaker” probability. Using a binary class with reconciliation in a greedy decoding model is equivalent to the method in Elson and McKeown (2010), except that the gold standard sequence features are replaced with predicted sequence features. 7.2 n-way A key advantage of the binary class model is that when predicting “speaker” versus “not speaker” the classifier only needs to predict one probability, and thus can take into account the evidence of all other quote-speaker pairs. The drawback to the binary model is that the probabilities assigned to the candidate speakers do not need to directly compete against each other. In other words when assigning a binary probability to a candidate speaker, the classifier does not take into account h</context>
<context position="33696" citStr="Elson and McKeown, 2010" startWordPosition="5603" endWordPosition="5606">nsurprising that the CRF model did not perform well. The limited range of the first order linear chain model could also have played a part in the poor performance of the CRF models. However, moving to a higher-order model is problematic as the number of transition probabilities that need to be calculated increases exponentially with the order of the model. 10 Conclusion In this paper, we present the first large-scale evaluation of a quote attribution system on newswire from the 1989 Wall Street Journal (WSJ) and the 2009 Sydney Morning Herald (SMH), as well as comparing against previous work (Elson and McKeown, 2010) on 19th-century literature. We show that when Elson and McKeown’s unrealistic use of gold-standard history information is removed, accuracy on all three corpora drops substantially. We demonstrate that by treating quote attribution as a sequence labelling task, we can achieve results that are very close to their results on newswire, though not for literature. In future work, we intend to further explore the sequence features that have a large impact on accuracy, and to find similar features or proxies for the sequence features that would be beneficial. We will also explore other approaches to</context>
</contexts>
<marker>Elson, McKeown, 2010</marker>
<rawString>David. K Elson and Kathleen. R McKeown. 2010. Automatic attribution of quoted speech in literary narrative. In Proceedings of AAAI, pages 1013– 1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="17886" citStr="Fan et al., 2008" startWordPosition="2920" endWordPosition="2923">nce or any sentence preceding it 3. If not, return the mention of an entity nearest the end of the quote (ignoring mentions in quotes), in the current sentence or any sentence preceding it This forms a reasonable baseline as it is able to pick up the quotes that fall into the more simple categories, such as the Trigram category and the Added category. It is also able to make a guess at the more complicated categories, without using gold standard information as the category predictions do. 6 Experimental Setup We use two classifiers: a logistic regression implementation available in LIBLINEAR (Fan et al., 2008), and a Conditional Random Field (CRF) from CRFSuite (Okazaki, 2007). Both packages use maximum likelihood estimation with L2 regularisation. We experimented with several values for the coefficient on a development set, but found that it had little impact, so stuck with the default value. All of our machine learning experiments use the same text encoding, which is explained below, and all use the category predictions when they are available. 6.1 Text Encoding We encode our text similarly to Elson and McKeown (2010). The major steps are: 1. Replace all quotes and speakers with special symbols; </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871– 1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="6736" citStr="Finkel et al., 2005" startWordPosition="1077" endWordPosition="1080"> which was built with reference to a small development corpus. With a permissive evaluation metric their grammar-based approach yielded 86% recall and 75% precision, however this dropped to 52% recall and 56% precision when measured in terms of completely correct quote-speaker pairs. The work most similar to ours is the work by Elson and McKeown (2010). Their aim was to automatically identify both quotes and speakers, and then to attribute each quote to a speaker, in a corpus of classic literature that they compiled themselves. To identify potential speakers they used the Stanford NER tagger (Finkel et al., 2005) and a method outlined in Davis et al. (2003) that allowed them to find nominal character references. They then grouped name variants and pronominal mentions into a coreference chain. To attribute a quote to a speaker they first classified the quotes into categories. Several of the categories have a speaker explicit in their structure, so they attribute quotes to those speakers with no further processing. For the remaining categories, they cast the attribution problem as a binary classification task, where each quote-speaker pair has a “speaker” or “not speaker” label predicted by the classifi</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Glass</author>
<author>Shaun Bangay</author>
</authors>
<title>A naive salience-based method for speaker identification in fiction books.</title>
<date>2007</date>
<booktitle>In Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA07),</booktitle>
<pages>1--6</pages>
<contexts>
<context position="5319" citStr="Glass and Bangay (2007)" startWordPosition="844" endWordPosition="847"> redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and attribution system, was developed by de La Clergerie et al. (2011). It conducts a full parse of the text, which allows it to use patterns to extract direct and indirect quotes, as well as the speaker of each quote. Their evaluation found that 19 out of 40 quotes (47.5%) had a correct span and author, while a further 19 had an incorrect author, and 4 had an incorrect span. In related work, Sagot et al. (2010) built a lexicon of French reported speech verbs, and conducted some analysis of different types of quotes. Glass and Bangay (2007) approached the task with a three stage method. For each quote they first find the nearest speech verb, they then find the grammatical actor of that speech verb, and finally they select the appropriate speaker for that actor. To achieve each of these subtasks they built a model with several manually weighted features that good candidates should possess. For each subtask they then choose the candidate with the largest weighted sum of features. Their full approach yields an accuracy of 79.4% on a corpus of manually annotated fiction books. Schneider et al. (2010) describe PICTOR, which is princi</context>
</contexts>
<marker>Glass, Bangay, 2007</marker>
<rawString>Kevin Glass and Shaun Bangay. 2007. A naive salience-based method for speaker identification in fiction books. In Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA07), pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with Wikipedia. Artificial Intelligence.</title>
<date>2012</date>
<note>(in press).</note>
<contexts>
<context position="12542" citStr="Hachey et al., 2012" startWordPosition="2013" endWordPosition="2016">ydney Morning Herald website1. We randomly selected 965 documents published in 2009 that were not obituaries, opinion pages, advertisements or other non-news stories. To conduct the annotation we employed 11 non-expert annotators via the outsourcing site Freelancer2, as well as five expert annotators from our research group. A total of 400 news stories were double-annotated, with at least 33 double-annotated stories per annotator. Raw agreement on the speaker of each quote was high at 98.3%. These documents had already been annotated with named entities as part of a separate research project (Hachey et al., 2012), which includes manually constructed coreference chains. The resulting corpus contains 965 documents, with 3,535 quotes. 3.4 Corpus Comparisons In order to compare the corpora we categorise the quotes into the categories defined by Elson and McKeown (2010), as shown in Table 1. We assigned quotes to these categories by testing (after text preprocessing) whether the quote belonged to each category, in the order shown below: 1. Trigram – the quote appears consecutively with a mention of an entity, and a reported speech verb, in any order; 2. Anaphors – same as above, except that the mention is </context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2012</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2012. Evaluating entity linking with Wikipedia. Artificial Intelligence. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="30096" citStr="Lafferty et al., 2001" startWordPosition="4954" endWordPosition="4957">of the final outcome. When the resulting model is used with the less accurate predicted sequence features, it is overconfident about the information those features provide. We account for this by using a first-order linear chain CRF model, which learns the probabilities of progressing from speaker to speaker more directly. During training the CRF is able to learn the association between features and labels, as well as the chance of transitioning from one label to the next. It also has the advantage of avoiding the label bias problem that would be present in the equivalent Hidden Markov Model (Lafferty et al., 2001). Though the n-way class model can be used directly in a CRF, the binary class model is more challenging. The main problem is that the “speaker” versus “not speaker” output of the binary classifier does not directly form a meaningful sequence that the CRF can learn over. If the reconciliation step is included it effectively adds an extra layer to the linear chain, making learning more difficult. Due to these difficulties we only use the n-way class model in our CRF experiments. 9 Results The main result of our experiments with the E&amp;M method is the large drop in accuracy that occurs when the g</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Mamede</author>
<author>Pedro Chaleira</author>
</authors>
<title>Character identification in children stories.</title>
<date>2004</date>
<booktitle>Advances in Natural Language Processing,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="4180" citStr="Mamede and Chaleira (2004)" startWordPosition="646" endWordPosition="649">Methods in Natural Language Processing and Computational Natural Language Learning, pages 790–799, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 Background Early work into quote attribution by Zhang et al. (2003) focused on identifying when different characters were talking in children’s stories, so that a speech synthesis system could read the quoted parts in different voices. While they were able to extract quotes with high precision and recall, their attribution accuracy was highly dependent on the document in question, ranging from 47.6% to 86.7%. Mamede and Chaleira (2004) conducted similar research on children’s stories written in Portuguese. Their system proved to be very good at extracting quotes through simple rules, but when using a handcrafted decision tree to attribute those quotes to a speaker, they achieved an accuracy of only 65.7%. In the news domain, both Pouliquen et al. (2007) and Sarmento and Nunes (2009) proposed rulebased systems that work over large volumes of text. Both systems aimed for high precision at the expense of low recall, as their data contained many redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and att</context>
</contexts>
<marker>Mamede, Chaleira, 2004</marker>
<rawString>Nuno Mamede and Pedro Chaleira. 2004. Character identification in children stories. Advances in Natural Language Processing, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</title>
<date>2007</date>
<note>URL http://www.chokkan.org/ software/crfsuite/.</note>
<contexts>
<context position="17954" citStr="Okazaki, 2007" startWordPosition="2933" endWordPosition="2934">tity nearest the end of the quote (ignoring mentions in quotes), in the current sentence or any sentence preceding it This forms a reasonable baseline as it is able to pick up the quotes that fall into the more simple categories, such as the Trigram category and the Added category. It is also able to make a guess at the more complicated categories, without using gold standard information as the category predictions do. 6 Experimental Setup We use two classifiers: a logistic regression implementation available in LIBLINEAR (Fan et al., 2008), and a Conditional Random Field (CRF) from CRFSuite (Okazaki, 2007). Both packages use maximum likelihood estimation with L2 regularisation. We experimented with several values for the coefficient on a development set, but found that it had little impact, so stuck with the default value. All of our machine learning experiments use the same text encoding, which is explained below, and all use the category predictions when they are available. 6.1 Text Encoding We encode our text similarly to Elson and McKeown (2010). The major steps are: 1. Replace all quotes and speakers with special symbols; 2. Replace all reported speech verbs with a symbol. Elson and McKeow</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs). URL http://www.chokkan.org/ software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Pareti</author>
</authors>
<title>A database of attribution relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>3213--3217</pages>
<contexts>
<context position="11113" citStr="Pareti (2012)" startWordPosition="1787" endWordPosition="1788">ted quotes. Our annotators reported seeing some errors in existing annotations, so we had one annotator check 400 existing annotations for correctness. This additional check found that 92.5% of the quotes were correctly annotated. 3.2 PDTB Attribution Corpus Extension (WSJ) Our next corpus is an extension to the attribution annotations found in the Penn Discourse TreeBank (PDTB). The original PDTB contains several forms of discourse, including assertions, beliefs, facts, and eventualities. These can be attributed to named entities or to unnamed, pronominal, or implicit sources. Recent work by Pareti (2012) conducted further annotation of this corpus, including reconstructing attributions that were only partially annotated, and introducing additional information. From this corpus we use only direct quotes and the directly quoted portions of mixed quotes, giving us 4,923 quotes. For the set of potential speakers we use the BBN pronoun coreference and entity type corpus (Weischedel and Brunstein, 2005), with automatically coreferred pronouns. We automatically matched BBN entities to PDTB extension speakers, and included the PDTB speaker where no matching BBN entity could be found. This means an au</context>
</contexts>
<marker>Pareti, 2012</marker>
<rawString>Silvia Pareti. 2012. A database of attribution relations. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 3213–3217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Clive Best</author>
</authors>
<title>Automatic detection of quotations in multilingual news.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>487--492</pages>
<contexts>
<context position="4504" citStr="Pouliquen et al. (2007)" startWordPosition="700" endWordPosition="703">n’s stories, so that a speech synthesis system could read the quoted parts in different voices. While they were able to extract quotes with high precision and recall, their attribution accuracy was highly dependent on the document in question, ranging from 47.6% to 86.7%. Mamede and Chaleira (2004) conducted similar research on children’s stories written in Portuguese. Their system proved to be very good at extracting quotes through simple rules, but when using a handcrafted decision tree to attribute those quotes to a speaker, they achieved an accuracy of only 65.7%. In the news domain, both Pouliquen et al. (2007) and Sarmento and Nunes (2009) proposed rulebased systems that work over large volumes of text. Both systems aimed for high precision at the expense of low recall, as their data contained many redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and attribution system, was developed by de La Clergerie et al. (2011). It conducts a full parse of the text, which allows it to use patterns to extract direct and indirect quotes, as well as the speaker of each quote. Their evaluation found that 19 out of 40 quotes (47.5%) had a correct span and author, while a further 19 had an</context>
</contexts>
<marker>Pouliquen, Steinberger, Best, 2007</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, and Clive Best. 2007. Automatic detection of quotations in multilingual news. In Proceedings of Recent Advances in Natural Language Processing, pages 487–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Laurence Danlos</author>
<author>Rosa Stern</author>
</authors>
<title>A lexicon of french quotation verbs for automatic quotation extraction.</title>
<date>2010</date>
<booktitle>In 7th international conference on Language Resources and Evaluation - LREC</booktitle>
<contexts>
<context position="5188" citStr="Sagot et al. (2010)" startWordPosition="822" endWordPosition="825">rk over large volumes of text. Both systems aimed for high precision at the expense of low recall, as their data contained many redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and attribution system, was developed by de La Clergerie et al. (2011). It conducts a full parse of the text, which allows it to use patterns to extract direct and indirect quotes, as well as the speaker of each quote. Their evaluation found that 19 out of 40 quotes (47.5%) had a correct span and author, while a further 19 had an incorrect author, and 4 had an incorrect span. In related work, Sagot et al. (2010) built a lexicon of French reported speech verbs, and conducted some analysis of different types of quotes. Glass and Bangay (2007) approached the task with a three stage method. For each quote they first find the nearest speech verb, they then find the grammatical actor of that speech verb, and finally they select the appropriate speaker for that actor. To achieve each of these subtasks they built a model with several manually weighted features that good candidates should possess. For each subtask they then choose the candidate with the largest weighted sum of features. Their full approach yi</context>
</contexts>
<marker>Sagot, Danlos, Stern, 2010</marker>
<rawString>Benoit Sagot, Laurence Danlos, and Rosa Stern. 2010. A lexicon of french quotation verbs for automatic quotation extraction. In 7th international conference on Language Resources and Evaluation - LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Sergio Nunes</author>
</authors>
<title>Automatic extraction of quotes and topics from news feeds.</title>
<date>2009</date>
<booktitle>In 4th Doctoral Symposium on Informatics Engineering.</booktitle>
<contexts>
<context position="4534" citStr="Sarmento and Nunes (2009)" startWordPosition="705" endWordPosition="708">h synthesis system could read the quoted parts in different voices. While they were able to extract quotes with high precision and recall, their attribution accuracy was highly dependent on the document in question, ranging from 47.6% to 86.7%. Mamede and Chaleira (2004) conducted similar research on children’s stories written in Portuguese. Their system proved to be very good at extracting quotes through simple rules, but when using a handcrafted decision tree to attribute those quotes to a speaker, they achieved an accuracy of only 65.7%. In the news domain, both Pouliquen et al. (2007) and Sarmento and Nunes (2009) proposed rulebased systems that work over large volumes of text. Both systems aimed for high precision at the expense of low recall, as their data contained many redundant quotes. More recently, SAPIENS, a Frenchlanguage quote extraction and attribution system, was developed by de La Clergerie et al. (2011). It conducts a full parse of the text, which allows it to use patterns to extract direct and indirect quotes, as well as the speaker of each quote. Their evaluation found that 19 out of 40 quotes (47.5%) had a correct span and author, while a further 19 had an incorrect author, and 4 had a</context>
</contexts>
<marker>Sarmento, Nunes, 2009</marker>
<rawString>Luis Sarmento and Sergio Nunes. 2009. Automatic extraction of quotes and topics from news feeds. In 4th Doctoral Symposium on Informatics Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Rebecca Hwa</author>
<author>Philip Gianfortoni</author>
<author>Dipanjan Das</author>
<author>Michael Heilman</author>
<author>Alan W Black</author>
<author>Frederik L Crabbe</author>
<author>Noah A Smith</author>
</authors>
<title>Visualizing topical quotations over time to understand news discourse.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI-01-013,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5886" citStr="Schneider et al. (2010)" startWordPosition="938" endWordPosition="941">is of different types of quotes. Glass and Bangay (2007) approached the task with a three stage method. For each quote they first find the nearest speech verb, they then find the grammatical actor of that speech verb, and finally they select the appropriate speaker for that actor. To achieve each of these subtasks they built a model with several manually weighted features that good candidates should possess. For each subtask they then choose the candidate with the largest weighted sum of features. Their full approach yields an accuracy of 79.4% on a corpus of manually annotated fiction books. Schneider et al. (2010) describe PICTOR, which is principally a quote visualisation tool. Their task was to find direct and indirect quotes, which they attribute to a text span representing the speaker. To do this they constructed a specialised grammar, which was built with reference to a small development corpus. With a permissive evaluation metric their grammar-based approach yielded 86% recall and 75% precision, however this dropped to 52% recall and 56% precision when measured in terms of completely correct quote-speaker pairs. The work most similar to ours is the work by Elson and McKeown (2010). Their aim was </context>
</contexts>
<marker>Schneider, Hwa, Gianfortoni, Das, Heilman, Black, Crabbe, Smith, 2010</marker>
<rawString>Nathan Schneider, Rebecca Hwa, Philip Gianfortoni, Dipanjan Das, Michael Heilman, Alan W. Black, Frederik L. Crabbe, and Noah A. Smith. 2010. Visualizing topical quotations over time to understand news discourse. Technical Report CMU-LTI-01-013, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus. Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="11514" citStr="Weischedel and Brunstein, 2005" startWordPosition="1847" endWordPosition="1850">The original PDTB contains several forms of discourse, including assertions, beliefs, facts, and eventualities. These can be attributed to named entities or to unnamed, pronominal, or implicit sources. Recent work by Pareti (2012) conducted further annotation of this corpus, including reconstructing attributions that were only partially annotated, and introducing additional information. From this corpus we use only direct quotes and the directly quoted portions of mixed quotes, giving us 4,923 quotes. For the set of potential speakers we use the BBN pronoun coreference and entity type corpus (Weischedel and Brunstein, 2005), with automatically coreferred pronouns. We automatically matched BBN entities to PDTB extension speakers, and included the PDTB speaker where no matching BBN entity could be found. This means an automatic system has an opportunity to find the correct speaker for all quotes in the corpus. 792 3.3 Sydney Morning Herald Corpus (SMH) We compiled the final corpus from a set of news documents taken from the Sydney Morning Herald website1. We randomly selected 965 documents published in 2009 that were not obituaries, opinion pages, advertisements or other non-news stories. To conduct the annotation</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Zhang</author>
<author>Alan Black</author>
<author>Richard Sproat</author>
</authors>
<title>Identifying speakers in children’s stories for speech synthesis.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>2041--2044</pages>
<contexts>
<context position="3808" citStr="Zhang et al. (2003)" startWordPosition="585" endWordPosition="588">espectively, while the third corpus is an extension to the classic literature corpus from Elson and McKeown (2010). Our results show that a quote attribution system using only realistic features is highly feasible for the news domain, with accuracies of 92.4% on the SMH corpus and 84.1% on the WSJ corpus. 790 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 790–799, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 Background Early work into quote attribution by Zhang et al. (2003) focused on identifying when different characters were talking in children’s stories, so that a speech synthesis system could read the quoted parts in different voices. While they were able to extract quotes with high precision and recall, their attribution accuracy was highly dependent on the document in question, ranging from 47.6% to 86.7%. Mamede and Chaleira (2004) conducted similar research on children’s stories written in Portuguese. Their system proved to be very good at extracting quotes through simple rules, but when using a handcrafted decision tree to attribute those quotes to a sp</context>
</contexts>
<marker>Zhang, Black, Sproat, 2003</marker>
<rawString>Jason Zhang, Alan Black, and Richard Sproat. 2003. Identifying speakers in children’s stories for speech synthesis. In Proceedings of EUROSPEECH, pages 2041–2044.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.99224">
Improved Parsing and POS Tagging Using Inter-Sentence
Consistency Constraints
</title>
<note confidence="0.4888405">
Alexander M. Rush&apos;* Roi Reichart&apos;* Michael Collins&apos; Amir Globerson3
&apos;MIT CSAIL, Cambridge, MA, 02139, USA
</note>
<email confidence="0.97388">
{srush|roiri}@csail.mit.edu
</email>
<affiliation confidence="0.988238">
&apos;Department of Computer Science, Columbia University, New-York, NY 10027, USA
</affiliation>
<email confidence="0.98781">
mcollins@cs.columbia.edu
</email>
<affiliation confidence="0.99667">
3School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel
</affiliation>
<email confidence="0.991554">
gamir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.996578" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999425055555556">
State-of-the-art statistical parsers and POS
taggers perform very well when trained with
large amounts of in-domain data. When train-
ing data is out-of-domain or limited, accuracy
degrades. In this paper, we aim to compen-
sate for the lack of available training data by
exploiting similarities between test set sen-
tences. We show how to augment sentence-
level models for parsing and POS tagging with
inter-sentence consistency constraints. To deal
with the resulting global objective, we present
an efficient and exact dual decomposition de-
coding algorithm. In experiments, we add
consistency constraints to the MST parser
and the Stanford part-of-speech tagger and
demonstrate significant error reduction in the
domain adaptation and the lightly supervised
settings across five languages.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939666666667">
State-of-the-art statistical parsers and POS taggers
perform very well when trained with large amounts
of data from their test domain. When training data is
out-of-domain or limited, the performance of the re-
sulting model often degrades. In this paper, we aim
to compensate for the lack of available training data
by exploiting similarities between test set sentences.
Most parsing and tagging models are defined at the
sentence-level, which makes such inter-sentence in-
formation sharing difficult. We show how to aug-
ment sentence-level models with inter-sentence con-
straints to encourage consistent descisions in similar
</bodyText>
<note confidence="0.650568">
* Both authors contributed equally to this work.
</note>
<bodyText confidence="0.999948823529412">
contexts, and we give an efficient algorithm with for-
mal guarantees for decoding such models.
In POS tagging, most taggers perform very well
on word types that they have observed in training
data, but they perform poorly on unknown words.
With a global objective, we can include constraints
that encourage a consistent tag across all occur-
rences of an unknown word type to improve accu-
racy. In dependency parsing, the parser can benefit
from surface-level features of the sentence, but with
sparse or out-of-domain training data these features
are very noisy. Using a global objective, we can add
constraints that encourage similar surface-level con-
texts to exhibit similar syntactic behaviour.
The first contribution of this work is the use of
Markov random fields (MRFs) to model global con-
straints between sentences in dependency parsing
and POS tagging. We represent each word as a node,
the tagging or parse decision as its label, and add
constraints through edges. MRFs allow us to include
global constraints tailored to these problems, and to
reason about inference in the corresponding global
models.
The second contribution is an efficient dual de-
composition algorithm for decoding a global ob-
jective with inter-sentence constraints. These con-
straints generally make direct inference challenging
since they tie together the entire test corpus. To alle-
viate this issue, our algorithm splits the global infer-
ence problem into subproblems - decoding of indi-
vidual sentences, and decoding of the global MRF.
These subproblems can be solved efficiently through
known methods. We show empirically that by iter-
atively solving these subproblems, we can find the
</bodyText>
<page confidence="0.948714">
1434
</page>
<note confidence="0.7827055">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1434–1444, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999479176470588">
exact solution to the global model.
We experiment with domain adaptation and
lightly supervised training. We demonstrate that
global models with consistency constraints can im-
prove upon sentence-level models for dependency
parsing and part-of-speech tagging. For domain
adaptation, we show an error reduction of up to 7.7%
when adapting the second-order projective MST
parser (McDonald et al., 2005) from newswire to
the QuestionBank domain. For lightly supervised
learning, we show an error reduction of up to 12.8%
over the same parser for five languages and an error
reduction of up to 10.3% over the Stanford trigram
tagger (Toutanova et al., 2003) for English POS tag-
ging. The algorithm requires, on average, only 1.7
times the costs of sentence-level inference and finds
the exact solution on the vast majority of sentences.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999898368421052">
Methods that combine inter-sentence information
with sentence-level algorithms have been applied to
a number of NLP tasks. The most similar models to
our work are skip-chain CRFs (Sutton and Mccal-
lum, 2004), relational markov networks (Taskar et
al., 2002), and collective inference with symmetric
clique potentials (Gupta et al., 2010). These mod-
els use a linear-chain CRF or MRF objective mod-
ified by potentials defined over pairs of nodes or
clique templates. The latter model makes use of La-
grangian relaxation. Skip-chain CRFs and collective
inference have been applied to problems in IE, and
RMNs to named entity recognition (NER) (Bunescu
and Mooney, 2004). Finkel et al. (2005) also inte-
grated non-local information into entity annotation
algorithms using Gibbs sampling.
Our model can be applied to a variety of off-the-
shelf structured prediction models. In particular, we
focus on dependency parsing which is characterized
by a more complicated structure compared to the IE
tasks addressed by previous work.
Another line of work that integrates corpus-level
declarative information into sentence-level models
includes the posterior regularization (Ganchev et al.,
2010; Gillenwater et al., 2010), generalized expec-
tation (Mann and McCallum, 2007; Mann and Mc-
Callum, ), and Bayesian measurements (Liang et al.,
2009) frameworks. The power of these methods has
been demonstrated for a variety of NLP tasks, such
as unsupervised and semi-supervised POS tagging
and parsing. The constraints used by these works
differ from ours in that they encourage the posterior
label distribution to have desired properties such as
sparsity (e.g. a given word can take a small number
of labels with a high probability). In addition, these
methods use global information during training as
opposed to our approach which applies test-time in-
ference global constraints.
The application of dual decomposition for infer-
ence in MRFs has been explored by Wainwright et
al. (2005), Komodakis et al. (2007), and Globerson
and Jaakkola (2007). In NLP, Rush et al. (2010)
and Koo et al. (2010) applied dual decomposition to
enforce agreement between different sentence-level
algorithms for parsing and POS tagging. Work on
dual decomposition for NLP is related to the work
of Smith and Eisner (2008) who apply belief prop-
agation to inference in dependency parsing, and to
constrained conditional models (CCM) (Roth and
Yih, 2005) that impose inference-time constraints
through an ILP formulation.
Several works have addressed semi-supervised
learning for structured prediction, suggesting objec-
tives based on the max-margin principles (Altun and
Mcallester, 2005), manifold regularization (Belkin
et al., 2005), a structured version of co-training
(Brefeld and Scheffer, 2006) and an entropy-based
regularizer for CRFs (Wang et al., 2009). The com-
plete literature on domain adaptation is beyond the
scope of this paper, but we refer the reader to Blitzer
and Daume (2010) for a recent survey.
Specifically for parsing and POS tagging, self-
training (Reichart and Rappoport, 2007), co-training
(Steedman et al., 2003) and active learning (Hwa,
2004) have been shown useful in the lightly su-
pervised setup. For parser adaptation, self-training
(McClosky et al., 2006; McClosky and Charniak,
2008), using weakly annotated data from the tar-
get domain (Lease and Charniak, 2005; Rimell and
Clark, 2008), ensemble learning (McClosky et al.,
2010), hierarchical bayesian models (Finkel and
Manning, 2009) and co-training (Sagae and Tsujii,
2007) achieve substantial performance gains. For a
recent survey see Plank (2011). Constraints simi-
lar to those we use for POS tagging were used by
Subramanya et al. (2010) for POS tagger adaptation.
</bodyText>
<page confidence="0.985224">
1435
</page>
<bodyText confidence="0.9998878">
Their work, however, does not show how to decode
a global, corpus-level, objective that enforces these
constraints, which is a major contribution of this pa-
per.
Inter-sentence syntactic consistency has been ex-
plored in the psycholinguistics and NLP literature.
Phenomena such as parallelism and syntactic prim-
ing – the tendency to repeat recently used syntactic
structures – have been demonstrated in human lan-
guage corpora (e.g. WSJ and Brown) (Dubey et al.,
2009) and were shown useful in generative and dis-
criminative parsers (e.g. (Cheung and Penn, 2010)).
We complement these works, which focus on con-
sistency between consecutive sentences, and explore
corpus level consistency.
</bodyText>
<sectionHeader confidence="0.997148" genericHeader="method">
3 Structured Models
</sectionHeader>
<bodyText confidence="0.963289333333333">
We begin by introducing notation for sentence-
level dependency parsing as a structured prediction
problem. The goal of dependency parsing is to
find the best parse y for a tagged sentence x =
(w1/t1, ... , wn/tn) with words w and POS tags t.
Define the index set for dependency parsing as
</bodyText>
<equation confidence="0.9998325">
I(x) = {(m, h) : m ∈ {1... n},
h ∈ {0 ... n}, m =6 h}
</equation>
<bodyText confidence="0.997631625">
where h = 0 represents the root word. A depen-
dency parse is a vector y = {y(m, h) : (m, h) ∈
I(x)} where y(m, h) = 1 if m is a modifier of the
head word h. We define the set Y(x) ⊂ {0,1}|I(x)|
to be the set of all valid dependency parses for a sen-
tence x. In this work, we use projective dependency
parses, but the method also applies to the set of non-
projective parse trees.
Additionally, we have a scoring function f :
Y(x) → R. The optimal parse y∗ for a sentence x is
given by, y∗ = arg maxy∈Y(x) f(y). This sentence-
level decoding problem can often be solved effi-
ciently. For example in commonly used projec-
tive dependency parsing models (McDonald et al.,
2005), we can compute y∗ efficiently using variants
of the Viterbi algorithm.
For this work, we make the assumption that we
have an efficient algorithm to find the argmax of
where u is a vector in R|I(x)|. In practice, u will be
a vector of Lagrange multipliers associated with the
dependencies of y in our dual decomposition algo-
rithm given in Section 6.
We can construct a very similar setting for POS
tagging where the goal is to find the best tagging
y for a sentence x = (w1, ... , wn). We skip the
formal details here.
We next introduce notation for Markov random
fields (MRFs) (Koller and Friedman, 2009). An
MRF consists of an undirected graph G = (V, E),
a set of possible labels for each node Li for i ∈
{1, . . . ,|V |}, and a scoring function g. The index
set for MRFs is
</bodyText>
<equation confidence="0.9947345">
IMRF = {(i,l) : i ∈ {1... |V |},l ∈ Li}
∪ {((i, j),li,lj) : (i, j) ∈ E,li ∈ Li,lj ∈ Lj}
</equation>
<bodyText confidence="0.9704717">
A label assignment in the MRF is a binary vector
z with z(i, l) = 1 if the label l is selected at node i
and z((i, j), li, lj) = 1 if the labels li, lj are selected
for the nodes i, j.
In applications such as parsing and POS tagging,
some of the label assignments are not allowed. For
example, in dependency parsing the resulting struc-
ture must be a tree. Consequently, if every node
in the MRF corresponds to a word in a document
and its label corresponds to the index of its head
word, the resulting dependency structure for each
sentence must be acyclic. The set of all valid la-
bel assignments (one label per node) is given by
Z ⊂ {0, 1}|IMRF|.
We score label assignments in the MRF with a
scoring function g : Z → R. The best assignment
z∗ in an MRF is given by, z∗ = arg maxz∈Z g(z).
We focus on pairwise MRFs where this function g is
a linear function of z whose parameters are denoted
by θ
</bodyText>
<equation confidence="0.99575525">
�g(z) = z · θ = z(i,l)θ(i, l) +
(i,l)∈IMRF
� z((i,j), li, lj)θ((i,j),li,lj)
((i,j),li,lj)∈IMRF
</equation>
<bodyText confidence="0.9986125">
As in parsing, we make the assumption that we
have an efficient algorithm to find the argmax of
</bodyText>
<equation confidence="0.74619525">
� u(m, h)y(m, h) = f(y) + u · y � u(i,l)z(i, l)
f(y) + g(z) +
(m,h)∈I(x) (i,l)∈IMRF(x)
1436
NN
He/PRP saw/VBD an/DT American/JJ man/NN
The/DT smart/JJ girls/NNS stood/VBD outside/RB
Danny/DT walks/VBZ a/DT long/JJ distance/NN
</equation>
<figureCaption confidence="0.882859">
Figure 1: An example constraint from dependency pars-
ing. The black nodes are modifiers observed in the train-
ing data. Each gray node corresponds to a possible mod-
ifier in the test corpus. The constraint applies to all mod-
ifiers in the context DT JJ. The white node corresponds
to the consensus POS tag of the head word of these mod-
ifiers.
</figureCaption>
<sectionHeader confidence="0.986331" genericHeader="method">
4 A Parsing Example
</sectionHeader>
<bodyText confidence="0.999976118644068">
In this section we give a detailed example of global
constraints for dependency parsing. The aim is to
construct a global objective that encourages similar
contexts across the corpus to exhibit similar syntac-
tic behaviour. We implement this objective using an
MRF with a node for each word in the test set. The
label of each node is the index of the word it mod-
ifies. We add edges to this MRF to reward consis-
tency among similar contexts. Furthermore, we add
nodes with a fixed label to incorporate contexts seen
in the training data.
Specifically, we say that the context of a word is
its POS tag and the POS tags of some set of the
words around it. We expand on this notion of con-
text in Section 8; for simplicity we assume here that
the context includes only the previous word’s POS
tag. Our constraints are designed to bias words in
the same context to modify words with similar POS
tags.
Figure 1 shows a global MRF over a small parsing
example with one training sentence and two test sen-
tences. The MRF contains a node associated with
each word instance, where the label of the node is
the index of the word it modifies. In this corpus, the
context DT JJ appears once in training and twice in
testing. We hope to choose head words with similar
POS tags for these two test contexts biased by the
observed training context.
More concretely, for each context c E
{1, . . . , C},we have a set Sc of associated
word indices (s, m) that appear in the context,
where s is a sentence index and m is a position
in that sentence. For instance, in our example
S1 = {(1, 2), (2, 4)} consists of all positions in
the test set where we see JJ preceded by DT.
Futhermore, we have a set Oc of indices (s, m, TR)
of observed instances of the context in the training
data where TR denotes a training index. In our
example O1 = {(1, 4, TR)} consists of the one
training instance. We associate each word instance
with a single context c.
We then define our MRF to include one consensus
node for each set Sc as well as a word node for each
instance in the set Sc U Oc. Thus the set of variables
corresponds to V = {1, ... , C} U (UCc=1 Sc U Oc).
Additionally, we include an edge from each node
i E ScUOc to its consensus node c, E = {(i, c) : c E
{1, . . . ,C}, i E Sc U Oc}. The word nodes from Sc
have the label set of possible head indices L(s,m) =
{0, ... , ns} where ns is the length of the sentence s.
The observed nodes from Oc have a singleton label
set L(s,m,TR) with the observed index. The consen-
sus nodes have the label set Lc = T U {NULL}
where T is the set of POS tags and the NULL sym-
bol represents the constraint being turned off.
We can now define the scoring function g for this
MRF. The scoring function aims to reward consis-
tency among the head POS tag at each word and the
consensus node
</bodyText>
<equation confidence="0.9924315">
{ δ1 if pos(li) = lc
δ2 if pos(li) is close to lc
δ3 lc = NULL
0 otherwise
</equation>
<bodyText confidence="0.9999782">
where pos maps a word index to its POS tag. The pa-
rameters δ1 &gt; δ2 &gt; δ3 &gt; 0 determine the bonus for
identical POS tags, similar POS tags, and for turning
off the constraint.
We construct a similar model for POS tagging.
We choose sets Tc corresponding to the c’th un-
known word type in the corpus. The MRF graph
is identical to the parsing case with Tc replacing Sc
and we no longer have Oc. The label sets for the
word nodes are now L(s,m) = T where the label is
</bodyText>
<equation confidence="0.913526">
θ((i, c), li, lc) =
</equation>
<page confidence="0.912666">
1437
</page>
<bodyText confidence="0.9999214">
the POS tag chosen at that word, and the label set for
the consensus node is L, = T ∪ {NULL}. We use
the same scoring function as in parsing to enforce
consistency between word nodes and the consensus
node.
</bodyText>
<sectionHeader confidence="0.973755" genericHeader="method">
5 Global Objective
</sectionHeader>
<bodyText confidence="0.998147666666667">
Recall the definition of sentence-level parsing,
where the optimal parse y* for a single sentence
x under a scoring function f is given by: y* =
arg maxyEY(x) f(y). We apply this objective to
a set of sentences, specified by the tuple X =
(x1, ..., xr), and the product of possible parses
</bodyText>
<equation confidence="0.940573285714286">
Y(X) = Y(x1) × ... × Y(xr). The sentence-level
decoding problem is to find the optimal dependency
parses Y* = (Y1*, ..., Yr*) ∈ Y(X) under a global
objective
r
Y * = arg max F(Y ) = arg max f(Ys)
YEY(X) YEY(X) s=1
</equation>
<bodyText confidence="0.99933225">
where F : Y(X) → R is the global scoring func-
tion.
We now consider scoring functions where the
global objective includes inter-sentence constraints.
Objectives of this form will not factor directly
into individual parsing problems; however, we can
choose to write them as the sum of two convenient
terms: (1) A simple sum of sentence-level objec-
tives; and (2) A global MRF that connects the local
structures.
For convenience, we define the following index
set.
</bodyText>
<equation confidence="0.9974785">
J (X) = {(s, m, h) : s ∈ {1, ... , r},
(m, h) ∈ I(xs)}
</equation>
<bodyText confidence="0.999707125">
This set enumerates all possible dependencies at
each sentence in the corpus. We say the parses Ys
are consistent with a label assignment z if for all
(s, m, h) ∈ J (X) we have that z((s, m), h) =
Ys(m, h). In other words, the labels in z match the
head words chosen in parse Ys.
With this notation we can write the full global de-
coding objective as
</bodyText>
<equation confidence="0.994882">
(Y *, z*) = arg max F(Y ) + g(z) (1)
YEY(X), zEZ
s.t. ∀(s, m, h) ∈ J (X), z((s, m), h) = Ys(m, h)
Set u(1)(s, m, h) ← 0 for all (s, m, h) ∈ J (X)
for k = 1 to K do
z(k) ← arg max (g(z) +
zEZ
11 u(k)(s, m, h)z((s, m), h))
(s,m,h)EJ(X)
Y (k) ← arg max (F(Y ) −
YEY(X)
11 u(k)(s, m, h)Ys(m, h))
(s,m,h)EJ(X)
if Y (k)
s (m, h) = z(k)((s, m), h)
for all (s, m, h) ∈ J (X) then
return (Y(k),z(k))
for all (s, m, h) ∈ J (X),
u(k+1)(s, m, h) ← u(k)(s, m, h) +
αk(z(k)((s, m), h) − Y (k)
s (m, h))
return (Y(K),z(K))
</equation>
<figureCaption confidence="0.998993">
Figure 2: The global decoding algorithm for dependency
parsing models.
</figureCaption>
<bodyText confidence="0.999988416666667">
The solution to this objective maximizes the local
models as well as the global MRF, while maintain-
ing consistency among the models. Specifically, the
MRF we use in the experiments has a simple naive
Bayes structure with the consensus node connected
to all relevant word nodes.
The global objective for POS tagging has a similar
form. As before we add a node to the MRF for each
word in the corpus. We use the POS tag set as our
labels for each of these nodes. The index set con-
tains an element for each possible tag at each word
instance in the corpus.
</bodyText>
<sectionHeader confidence="0.992526" genericHeader="method">
6 A Global Decoding Algorithm
</sectionHeader>
<bodyText confidence="0.999987142857143">
We now consider the decoding question: how to
find the structure Y * that maximizes the global ob-
jective. We aim for an efficient solution that makes
use of the individual solvers at the sentence-level.
For this work, we make the assumption that the
graph chosen for the MRF has small tree-width, e.g.
our naive Bayes constraints, and can be solved effi-
ciently using dynamic programming.
Before we describe our dual decomposition al-
gorithm, we consider the difficulty of solving the
global objective directly. We have an efficient dy-
namic programming algorithm for solving depen-
dency parsing at the sentence-level, and efficient al-
gorithms for solving the MRF. It follows that we
</bodyText>
<page confidence="0.969094">
1438
</page>
<bodyText confidence="0.985265261904762">
could construct an intersected dynamic program-
ming algorithm that maintains the product of states
over both models. This algorithm is exact, but it
is very inefficient. Solving the intersected dynamic
program requires decoding simultaneously over the
entire corpus, with an additional multiplicative fac-
tor for solving the MRF. On top of this cost, we need
to alter the internal structure of the sentence-level
models.
In contrast, we can construct a dual decomposi-
tion algorithm which is efficient, produces a certifi-
cate when it finds an exact solution, and directly
uses the sentence-level parsing models. Considering
again the global objective of equation 1, we note that
the difficulty in decoding this objective comes en-
tirely from the constraints z((s, m), h) = Ys(m, h).
If these were not there, the problem would factor
into two parts, an optimization of F over the test
corpus Y(X) and an optimization of g over possible
MRF assignments Z. The first problem factors nat-
urally into sentence-level parsing problems and the
second can be solved efficiently given our assump-
tions on the MRF topology G.
Recent work has shown that a relaxation based
on dual decomposition often produces an exact so-
lution for such problems (Koo et al., 2010). To
apply dual decomposition, we introduce Lagrange
multipliers u(s, m, h) for the agreement constraints
between the sentence-level models and the global
MRF. The Lagrangian dual is the function L(u) =
maxz g(z, u) + maxy F(y, u) where
In order to find ming L(u), we use subgradient de-
scent. This requires computing g(z, u) and F(y, u)
for fixed values of u, which by our assumptions from
Section 3 are efficient to calculate.
The full algorithm is given in Figure 2. We start
with the values of u initialized to 0. At each itera-
tion k, we find the best set of parses Y (k) over the
entire corpus and the best MRF assignment z(k). We
then update the value of u based on the difference
between Y (k) and z(k) and a rate parameter α. On
the next iteration, we solve the same decoding prob-
</bodyText>
<table confidence="0.999705">
&gt; 0.7 &gt;0.8 &gt; 0.9 1.0
All Contexts 66.8 57.9 46.8 33.3
Head in Context 76.0 67.9 57.2 42.3
</table>
<tableCaption confidence="0.999797">
Table 1: Exploratory statistics for constraint selection.
</tableCaption>
<bodyText confidence="0.998481923076923">
The table shows the percentage of context types for which
the probability of the most frequent head tag is at least p.
Head in Context refers to the subset of contexts where the
most frequent head is within the context itself. Numbers
are based on Section 22 of the Wall Street Journal and are
given for contexts that appear at least 10 times.
lems modified by the new value of u. If at any point
the current solutions Y (k) and z(k) satisfy the con-
sistency constraint, we return their current values.
Otherwise, we stop at a max iteration K and return
the values from the last iteration.
We now give a theorem for the formal guarantees
of this algorithm.
</bodyText>
<construct confidence="0.6248734">
Theorem 1 Iffor some k E {1... K} in the algo-
rithm in Figure 2, Y (k)
s (m, h) = z(k)(s, m, h) for
all (s, m, h) E J, then (Y (k), z(k)) is a solution to
the maximization problem in equation 1.
</construct>
<bodyText confidence="0.999901">
We omit the proof for brevity. It is a slight variation
of the proof given by Rush et al. (2010).
</bodyText>
<sectionHeader confidence="0.987929" genericHeader="method">
7 Consistency Constraints
</sectionHeader>
<bodyText confidence="0.9998839">
In this section we describe the consistency con-
straints used for the global models of parsing and
tagging.
Parsing Constraints. Recall from Section 4 that
we choose parsing constraints based on the word
context. We encourage words in similar contexts to
choose head words with similar POS tags.
We use a simple procedure to select which con-
straints to add. First define a context template to
be a set of offsets {r, ... , s} with r &lt; 0 &lt; s that
specify the neighboring words to include in a con-
text. In the example of Figure 1, the context tem-
plate {−1, 0, 1, 2} applied to the word girls/NNS
would produce the context JJ NNS VBD RB. For
each word in the corpus, we consider all possible
templates with s − r &lt; 4. We use only contexts that
predict the head POS of the context in the training
data with probability 1 and prefer long over short
contexts. Once we select the context of each word,
we add a consensus node for each context type in
</bodyText>
<equation confidence="0.978607333333333">
�
g(z,u) = g(z) +
u(s, m, h)z((s, m), h)),
(s^h)EJ (X)
F(y, u) = F(Y ) − � u(s, m, h)Ys(m, h)
(s^h)EJ(X)
</equation>
<page confidence="0.975654">
1439
</page>
<bodyText confidence="0.999834277777778">
the corpus. We connect each word node to its corre-
sponding consensus node.
Local context does not fully determine the POS
tag of the head word, but for certain contexts it pro-
vides a strong signal. Table 1 shows context statis-
tics for English. For 46.8% of the contexts, the most
frequent head tag is chosen &gt; 90% of the time. The
pattern is even stronger for contexts where the most
frequent head tag is within the context itself. In
this case, for 57.2% of the contexts the most fre-
quent head tag is chosen &gt; 90% of the time. Con-
sequently, if more than one context can be selected
for a word, we favor the contexts where the most
frequent head POS is inside the context.
POS Tagging Constraints. For POS tagging, our
constraints focus on words not observed in the train-
ing data. It is well-known that each word type ap-
pears only with a small number of POS tags. In Sec-
tion 22 of the WSJ corpus, 96.35% of word types
appear with a single POS tag.
In most test sets we are unlikely to see an un-
known word more than once or twice. To fix this
sparsity issue, we import additional unannotated
sentences for each unknown word from the New
York Times Section of the NANC corpus (Graff,
1995). These sentences give additional information
for unknown word types.
Additionally, we note that morphologically re-
lated words often have similar POS tags. We can
exploit this relationship by connecting related word
types to the same consensus node. We experimented
with various morphological variants and found that
connecting a word type with the type generated by
appending the suffix “s” was most beneficial. For
each unknown word type, we also import sentences
for its morphologically related words.
</bodyText>
<sectionHeader confidence="0.992099" genericHeader="evaluation">
8 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999526375">
We experiment in two common scenarios where
parsing performance is reduced from the fully su-
pervised, in-domain case. In domain adaptation, we
train our model completely in one source domain
and test it on a different target domain. In lightly su-
pervised training, we simulate the case where only
a limited amount of annotated data is available for a
language.
</bodyText>
<table confidence="0.974934333333333">
Base ST Model ER
WSJ —* QTB 89.63 89.99 90.43 7.7
QTB —* WSJ 74.89 74.97 75.76 3.5
</table>
<tableCaption confidence="0.65293325">
Table 2: Dependency parsing UAS for domain adapta-
tion. WSJ is the Penn TreeBank. QTB is the Question-
Bank. ER is error reduction. Results are significant using
the sign test with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.987564166666667">
Data for Domain Adaptation We perform do-
main adaptation experiments in English using the
WSJ PennTreebank (Marcus et al., 1993) and the
QuestionBank (QTB) (Judge et al., 2006). In the
WSJ —* QTB scenario, we train on sections 2-21
of the WSJ and test on the entire QTB (4000 ques-
tions). In the QTB —* WSJ scenario, we train on the
entire QTB and test on section 23 of the WSJ.
Data for Lightly Supervised Training For all
English experiments, our data was taken from the
WSJ PennTreebank: training sentences from Sec-
tion 0, development sentences from Section 22, and
test sentences from Section 23. For experiments
in Bulgarian, German, Japanese, and Spanish, we
use the CONLL-X data set (Buchholz and Marsi,
2006) with training data taken from the official train-
ing files. We trained the sentence-level models with
50-500 sentences. To verify the robustness of our
results, our test sets consist of the official test sets
augmented with additional sentences from the offi-
cial training files such that each test file consists of
25,000 words. Our results on the official test sets are
very similar to the results we report and are omitted
for brevity.
Parameters The model parameters, 61, 62, and 63
of the scoring function (Section 4) and α of the
Lagrange multipliers update rule (Section 6), were
tuned on the English development data. In our dual
decomposition inference algorithm, we use K =
200 maximum iterations and tune the decay rate fol-
lowing the protocol described by Koo et al. (2010).
Sentence-Level Models For dependency parsing
we utilize the second-order projective MST parser
(McDonald et al., 2005)1 with the gold-standard
POS tags of the corpus. For POS tagging we use
the Stanford POS tagger (Toutanova et al., 2003)2.
</bodyText>
<footnote confidence="0.9999615">
1http://sourceforge.net/projects/mstparser/
2http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<page confidence="0.929395">
1440
</page>
<table confidence="0.991642857142857">
Base 50 Model (ER) Base 100 Model (ER) Base 200 Model (ER) Base 500 Model (ER)
ST ST ST ST
Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33)
Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93)
Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35)
Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27)
Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4)
</table>
<tableCaption confidence="0.9897276">
Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,
German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency
parser of McDonald et al. (2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the
same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p &lt; 0.05, all
50, 100, and 200 results are significant, as are Eng and Ger 500.
</tableCaption>
<table confidence="0.99996125">
50 100 200 500
Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER)
Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)
Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
</table>
<tableCaption confidence="0.994222333333333">
Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et
al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All
results are significant using the sign test with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.997466777777778">
Evaluation and Baselines To measure parsing
performance, we use unlabeled attachment score
(UAS) given by the CONLL-X dependency parsing
shared task evaluation script (Buchholz and Marsi,
2006). We compare the accuracy of dependency
parsing with global constraints to the sentence-level
dependency parser of McDonald et al. (2005) and to
a self-training baseline (Steedman et al., 2003; Re-
ichart and Rappoport, 2007). The parsing baseline is
equivalent to a single round of dual decomposition.
For the self-training baseline, we parse the test cor-
pus, append the labeled test sentences to the training
corpus, train a new parser, and then re-parse the test
set. We run this procedure for a single iteration.
For POS tagging we measure token level POS ac-
curacy for all the words in the corpus and also for
unknown words (words not observed in the train-
ing data). We compare the accuracy of POS tagging
with global constraints to the accuracy of the Stan-
ford POS tagger 3.
Domain Adaptation Accuracy Results are pre-
sented in Table 2. The constrained model reduces
the error of the baseline on both cases. Note that
when the base parser is trained on the WSJ corpus its
UAS performance on the QTB is 89.63%. Yet, the
constrained model is still able to reduce the baseline
error by 7.7%.
</bodyText>
<footnote confidence="0.925284">
3We do not run self-training for POS tagging as it has been
shown unuseful for this application (Clark et al., 2003).
</footnote>
<bodyText confidence="0.964565333333334">
Lightly Supervised Accuracy The parsing results
are given in Table 3. Our model improves over
the baseline parser and self-training across all lan-
guages and training set sizes. The best results are
for Japanese and English with error reductions of
2.33 – 12.82% and 1.93 – 6.64% respectively. The
self-training baseline achieves small gains on some
languages, but generally performs similarly to the
standard parser.
The POS tagging results are given in Table 4. Our
model improves over the baseline tagger for the en-
tire training size range. For 50 training sentences
we reduce 10.33% of the overall error, and 11.53%
of the error on unknown words. Although the tagger
performance substantially improves when the train-
ing set grows to 500 sentences, our model still pro-
vides an overall error reduction of 4.64% and of
8.33% for unknown words.
</bodyText>
<sectionHeader confidence="0.988497" genericHeader="evaluation">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999706888888889">
Efficiency Since dual decomposition often re-
quires hundreds of iterations to converge, a naive im-
plementation would be orders of magnitude slower
than the underlying sentence-level model. We use
two techniques to speed-up the algorithm.
First, we follow Koo et al. (2010) and use lazy
decoding as part of dual decomposition. At each it-
eration k, we cache the result of the MRF z(k) and
set of parse tree Y (k). In the next iteration, we only
</bodyText>
<page confidence="0.98269">
1441
</page>
<figure confidence="0.805436">
50 100 150 200
iteration
</figure>
<figureCaption confidence="0.997211">
Figure 3: Efficiency of dependency parsing decoding for
three languages. The plot shows the speed of each iter-
ation of the subgradient algorithm relative to a round of
unconstrained parsing.
</figureCaption>
<table confidence="0.999016714285714">
Most Effective Contexts
WSJ → QTB QTB → WSJ
WRB VBP VBD NN NN ,
DT JJS NN IN IN PRP VBZ
VBP PRP VB JJ JJ NN ,
DT NN NN VB IN JJ JJ NN
RBS JJ NN IN NN POS NN NN
</table>
<tableCaption confidence="0.685709333333333">
Table 5: The five most effective constraint contexts from
the domain adaptation experiments. The bold POS tag
indicates the modifier word of the context.
</tableCaption>
<figureCaption confidence="0.9919654">
Figure 4: Subset of sentences with the context WRB VBP
from WSJ → QTB domain adaptation. In the first round,
the parser chooses VBN for the first sentence, which is in-
consistent with similar contexts. The constraints correct
this choice in later rounds.
</figureCaption>
<bodyText confidence="0.995426375">
recompute the solution Y3 for a sentence s if the
weight u(s, m, h) for some m, h was updated. A
similar technique is applied to the MRF.
Second, during the first iteration of the algorithm
we apply max-marginal based pruning using the
threshold defined by Weiss and Taskar (2010). This
produces a pruned hypergraph for each sentence,
which allows us to avoid recomputing parse features
and to solve a simplified search problem.
To measure efficiency, we compare the time spent
in dual decomposition to the speed of unconstrained
inference. Across experiments, the mean dual de-
composition time is 1.71 times the cost of uncon-
strained inference. Figure 3 shows how this time is
spent after the first iteration. The early iterations are
around 1% of the total cost, and because of lazy de-
coding this quickly drops to almost nothing.
Exactness To measure exactness, we count the
number of sentences for which we should remove
the constraints in order for the model to reach con-
vergence. For dependency parsing, across languages
removing constraints on 0.6% of sentences yields
exact convergence. Removing these constraints has
very little effect on the final outcome of the model.
For POS tagging, the algorithm finds an exact so-
lution after removing constraints from 0.2% of the
sentences.
Constraint Analysis We can also look at the num-
ber, size, and outcome of the constraints chosen in
the experiments. In the lightly supervised experi-
ments, the average number of constraints is 3298 for
25000 tokens, where the median constraint connects
19 different tokens. Of these constraints around 70%
are active (non-NULL). The domain adaptation ex-
periments have a similar number of constraints with
around 75% of constraints active. In both experi-
ments many of the constraints are found to be con-
sistent after the first iteration, but as Figure 3 im-
plies, other constraints take multiple iterations to
converge.
Qualitative Analysis In order to understand why
these simple consistency constraints are effective,
we take a qualitative look at the the domain adap-
tation experiments on the QuestionBank. Table 5
ranks the five most effective contextual constraints
from both experiments. For the WSJ → QTB exper-
iment, the most effective constraint relates the inital
question word with an adjacent verb. Figure 4 shows
</bodyText>
<figure confidence="0.999609021276596">
0.8
0.6
0.4
0.0
0.2
1.4
1.2
1.0
english
german
japanese
�
do/
VBP
Why/
WRB
calluses/
NNS
get/
VB
people/
NNS
VBP
VBN
Where/
WRB
are/
VBP
diamonds/
NNS
mined/
VBN
�
VBP
How/
WRB
do/
VBP
you/
PRP
measure/
VB
earthquakes/
NNS
�
VBP
percentage of parsing time
</figure>
<page confidence="0.992084">
1442
</page>
<bodyText confidence="0.9996822">
sentences where this constraint applies in the Ques-
tionBank. For the QTB -* WSJ experiment, the ef-
fective contexts are mostly long base noun phrases.
These occur often in the WSJ but are rare in the sim-
pler QuestionBank sentences.
</bodyText>
<sectionHeader confidence="0.987241" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999955">
In this work we experiment with inter-sentence
consistency constraints for dependency parsing and
POS tagging. We have proposed a corpus-level ob-
jective that augments sentence-level models with
such constraints and described an exact and effi-
cient dual decomposition algorithm for its decod-
ing. In future work, we intend to explore efficient
techniques for joint parameter learning for both the
global MRF and the local models.
</bodyText>
<sectionHeader confidence="0.658753" genericHeader="acknowledgments">
Acknowledgments Columbia University gratefully
</sectionHeader>
<bodyText confidence="0.8456995">
acknowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or the
US government. Alexander Rush was supported by a
National Science Foundation Graduate Research Fellow-
ship.
</bodyText>
<sectionHeader confidence="0.996064" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997923774647888">
Y. Altun and D. Mcallester. 2005. Maximum margin
semi-supervised learning for structured variables. In
NIPS.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In AISTATS.
John Blitzer and Hal Daume. 2010. Icml 2010 tutorial
on domain adaptation. In ICML.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
R.C. Bunescu and R.J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL.
J.C.K Cheung and G. Penn. 2010. Utilizing extra-
sentential context for parsing. In EMNLP.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping pos taggers using unlabelled data. In
CoNLL.
A. Dubey, F. Keller, and P. Sturt. 2009. A proba-
bilistic corpus-based model of parallelism. Cognition,
109(2):193–210.
Jenny Rose Finkel and Christopher Manning. 2009. Hi-
erarchical bayesian domain adaptation. In NAACL.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured Latent
Variable Models. Journal of Machine Learning Re-
search, 11:2001–2049.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar in-
duction. In Proceedings of the ACL Conference Short
Papers.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
map lp-relaxations. In NIPS.
D. Graff. 1995. North american news text corpus. Lin-
guistic Data Consortium, LDC95T21.
Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan. 2010.
Collective inference for extraction mrfs coupled with
symmetric clique potentials. JMLR.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253–276.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In ACL-COLING.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In IJCNLP.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In ICML.
G.S. Mann and A. McCallum. Generalized expectation
criteria for semi-supervised learning with weakly la-
beled data. Journal of Machine Learning Research,
11:955–984.
G.S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In ICML.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313–330.
</reference>
<page confidence="0.591075">
1443
</page>
<reference confidence="0.999935163636364">
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In ACL, sort papers.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adapatation for parsing. In
NAACL.
R.T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In HLT/EMNLP.
Barbara Plank. 2011. Domain Adaptation for Parsing.
Ph.d. thesis, University of Groningen.
R. Reichart and A. Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains. In
EMNLP.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
Kenji Sagae and Junichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In EMNLP-CoNLL.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In EMNLP.
C. Sutton and A. Mccallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In In ICML Workshop on Statistical Re-
lational Learning and Its Connections.
B. Taskar, P. Abbeel, and d. Koller. 2002. Discriminative
probabilistic models for relational data. In UAI.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697–3717.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields. In NIPS.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284.
</reference>
<page confidence="0.995108">
1444
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.570560">
<title confidence="0.9997495">Improved Parsing and POS Tagging Using Consistency Constraints</title>
<author confidence="0.99948">M Roi Michael</author>
<address confidence="0.8305295">CSAIL, Cambridge, MA, 02139, of Computer Science, Columbia University, New-York, NY 10027,</address>
<email confidence="0.994277">mcollins@cs.columbia.edu</email>
<address confidence="0.851375">of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904,</address>
<email confidence="0.993367">gamir@cs.huji.ac.il</email>
<abstract confidence="0.999430263157895">State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>D Mcallester</author>
</authors>
<title>Maximum margin semi-supervised learning for structured variables.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7310" citStr="Altun and Mcallester, 2005" startWordPosition="1097" endWordPosition="1100"> (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006</context>
</contexts>
<marker>Altun, Mcallester, 2005</marker>
<rawString>Y. Altun and D. Mcallester. 2005. Maximum margin semi-supervised learning for structured variables. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Belkin</author>
<author>P Niyogi</author>
<author>V Sindhwani</author>
</authors>
<title>On manifold regularization.</title>
<date>2005</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="7357" citStr="Belkin et al., 2005" startWordPosition="1103" endWordPosition="1106">10) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly an</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2005</marker>
<rawString>M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On manifold regularization. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Hal Daume</author>
</authors>
<title>Icml 2010 tutorial on domain adaptation.</title>
<date>2010</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7618" citStr="Blitzer and Daume (2010)" startWordPosition="1146" endWordPosition="1149">pendency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance g</context>
</contexts>
<marker>Blitzer, Daume, 2010</marker>
<rawString>John Blitzer and Hal Daume. 2010. Icml 2010 tutorial on domain adaptation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Brefeld</author>
<author>T Scheffer</author>
</authors>
<title>Semi-supervised learning for structured output variables.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7423" citStr="Brefeld and Scheffer, 2006" startWordPosition="1112" endWordPosition="1115"> different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rim</context>
</contexts>
<marker>Brefeld, Scheffer, 2006</marker>
<rawString>U. Brefeld and T. Scheffer. 2006. Semi-supervised learning for structured output variables. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="26707" citStr="Buchholz and Marsi, 2006" startWordPosition="4694" endWordPosition="4697">WSJ PennTreebank (Marcus et al., 1993) and the QuestionBank (QTB) (Judge et al., 2006). In the WSJ —* QTB scenario, we train on sections 2-21 of the WSJ and test on the entire QTB (4000 questions). In the QTB —* WSJ scenario, we train on the entire QTB and test on section 23 of the WSJ. Data for Lightly Supervised Training For all English experiments, our data was taken from the WSJ PennTreebank: training sentences from Section 0, development sentences from Section 22, and test sentences from Section 23. For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set (Buchholz and Marsi, 2006) with training data taken from the official training files. We trained the sentence-level models with 50-500 sentences. To verify the robustness of our results, our test sets consist of the official test sets augmented with additional sentences from the official training files such that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, 61, 62, and 63 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the E</context>
<context position="29684" citStr="Buchholz and Marsi, 2006" startWordPosition="5174" endWordPosition="5177">0.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64) Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33) Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All results are significant using the sign test with p &lt; 0.05. Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script (Buchholz and Marsi, 2006). We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of McDonald et al. (2005) and to a self-training baseline (Steedman et al., 2003; Reichart and Rappoport, 2007). The parsing baseline is equivalent to a single round of dual decomposition. For the self-training baseline, we parse the test corpus, append the labeled test sentences to the training corpus, train a new parser, and then re-parse the test set. We run this procedure for a single iteration. For POS tagging we measure token level POS accuracy for all the words in the corpus a</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5328" citStr="Bunescu and Mooney, 2004" startWordPosition="798" endWordPosition="801">formation with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann a</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>R.C. Bunescu and R.J. Mooney. 2004. Collective information extraction with relational markov networks. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C K Cheung</author>
<author>G Penn</author>
</authors>
<title>Utilizing extrasentential context for parsing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8942" citStr="Cheung and Penn, 2010" startWordPosition="1351" endWordPosition="1354">sed by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. 3 Structured Models We begin by introducing notation for sentencelevel dependency parsing as a structured prediction problem. The goal of dependency parsing is to find the best parse y for a tagged sentence x = (w1/t1, ... , wn/tn) with words w and POS tags t. Define the index set for dependency parsing as I(x) = {(m, h) : m ∈ {1... n}, h ∈ {0 ... n}, m =6 h} where h = 0 represents the root word. A dependency parse is a vector y = {y(m, h) : (m, h) ∈ I(x)} where y(m, h)</context>
</contexts>
<marker>Cheung, Penn, 2010</marker>
<rawString>J.C.K Cheung and G. Penn. 2010. Utilizing extrasentential context for parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
<author>Miles Osborne</author>
</authors>
<title>Bootstrapping pos taggers using unlabelled data. In CoNLL.</title>
<date>2003</date>
<contexts>
<context position="30891" citStr="Clark et al., 2003" startWordPosition="5384" endWordPosition="5387">the corpus and also for unknown words (words not observed in the training data). We compare the accuracy of POS tagging with global constraints to the accuracy of the Stanford POS tagger 3. Domain Adaptation Accuracy Results are presented in Table 2. The constrained model reduces the error of the baseline on both cases. Note that when the base parser is trained on the WSJ corpus its UAS performance on the QTB is 89.63%. Yet, the constrained model is still able to reduce the baseline error by 7.7%. 3We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al., 2003). Lightly Supervised Accuracy The parsing results are given in Table 3. Our model improves over the baseline parser and self-training across all languages and training set sizes. The best results are for Japanese and English with error reductions of 2.33 – 12.82% and 1.93 – 6.64% respectively. The self-training baseline achieves small gains on some languages, but generally performs similarly to the standard parser. The POS tagging results are given in Table 4. Our model improves over the baseline tagger for the entire training size range. For 50 training sentences we reduce 10.33% of the overa</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Stephen Clark, James Curran, and Miles Osborne. 2003. Bootstrapping pos taggers using unlabelled data. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
<author>P Sturt</author>
</authors>
<title>A probabilistic corpus-based model of parallelism.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="8849" citStr="Dubey et al., 2009" startWordPosition="1336" endWordPosition="1339">recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We complement these works, which focus on consistency between consecutive sentences, and explore corpus level consistency. 3 Structured Models We begin by introducing notation for sentencelevel dependency parsing as a structured prediction problem. The goal of dependency parsing is to find the best parse y for a tagged sentence x = (w1/t1, ... , wn/tn) with words w and POS tags t. Define the index set for dependency parsing as I(x) = {(m, h) : m ∈ {1... n}, h ∈ {0 ... n}, m =6 h} where h = 0 represe</context>
</contexts>
<marker>Dubey, Keller, Sturt, 2009</marker>
<rawString>A. Dubey, F. Keller, and P. Sturt. 2009. A probabilistic corpus-based model of parallelism. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8143" citStr="Finkel and Manning, 2009" startWordPosition="1224" endWordPosition="1227">in adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntact</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher Manning. 2009. Hierarchical bayesian domain adaptation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5350" citStr="Finkel et al. (2005)" startWordPosition="802" endWordPosition="805">el algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Ba</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J.R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior Regularization for Structured Latent Variable Models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. Grac¸a, J. Gillenwater, and B. Taskar. 2010. Posterior Regularization for Structured Latent Variable Models. Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL Conference Short Papers.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Sparsity in dependency grammar induction. In Proceedings of the ACL Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Jaakkola</author>
</authors>
<title>Fixing maxproduct: Convergent message passing algorithms for map lp-relaxations.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6690" citStr="Globerson and Jaakkola (2007)" startWordPosition="1006" endWordPosition="1009">iety of NLP tasks, such as unsupervised and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun a</context>
</contexts>
<marker>Globerson, Jaakkola, 2007</marker>
<rawString>A. Globerson and T. Jaakkola. 2007. Fixing maxproduct: Convergent message passing algorithms for map lp-relaxations. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>North american news text corpus. Linguistic Data Consortium,</title>
<date>1995</date>
<contexts>
<context position="24830" citStr="Graff, 1995" startWordPosition="4381" endWordPosition="4382">elected for a word, we favor the contexts where the most frequent head POS is inside the context. POS Tagging Constraints. For POS tagging, our constraints focus on words not observed in the training data. It is well-known that each word type appears only with a small number of POS tags. In Section 22 of the WSJ corpus, 96.35% of word types appear with a single POS tag. In most test sets we are unlikely to see an unknown word more than once or twice. To fix this sparsity issue, we import additional unannotated sentences for each unknown word from the New York Times Section of the NANC corpus (Graff, 1995). These sentences give additional information for unknown word types. Additionally, we note that morphologically related words often have similar POS tags. We can exploit this relationship by connecting related word types to the same consensus node. We experimented with various morphological variants and found that connecting a word type with the type generated by appending the suffix “s” was most beneficial. For each unknown word type, we also import sentences for its morphologically related words. 8 Experiments and Results We experiment in two common scenarios where parsing performance is re</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>D. Graff. 1995. North american news text corpus. Linguistic Data Consortium, LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Gupta</author>
<author>Sunita Sarawagi</author>
<author>Ajit A Diwan</author>
</authors>
<title>Collective inference for extraction mrfs coupled with symmetric clique potentials.</title>
<date>2010</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="5001" citStr="Gupta et al., 2010" startWordPosition="744" endWordPosition="747"> error reduction of up to 10.3% over the Stanford trigram tagger (Toutanova et al., 2003) for English POS tagging. The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. 2 Related Work Methods that combine inter-sentence information with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by </context>
</contexts>
<marker>Gupta, Sarawagi, Diwan, 2010</marker>
<rawString>Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan. 2010. Collective inference for extraction mrfs coupled with symmetric clique potentials. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7794" citStr="Hwa, 2004" startWordPosition="1174" endWordPosition="1175">ised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>R. Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: Creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In ACL-COLING.</booktitle>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parse-annotated questions. In ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10701" citStr="Koller and Friedman, 2009" startWordPosition="1683" endWordPosition="1686">Donald et al., 2005), we can compute y∗ efficiently using variants of the Viterbi algorithm. For this work, we make the assumption that we have an efficient algorithm to find the argmax of where u is a vector in R|I(x)|. In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1, ... , wn). We skip the formal details here. We next introduce notation for Markov random fields (MRFs) (Koller and Friedman, 2009). An MRF consists of an undirected graph G = (V, E), a set of possible labels for each node Li for i ∈ {1, . . . ,|V |}, and a scoring function g. The index set for MRFs is IMRF = {(i,l) : i ∈ {1... |V |},l ∈ Li} ∪ {((i, j),li,lj) : (i, j) ∈ E,li ∈ Li,lj ∈ Lj} A label assignment in the MRF is a binary vector z with z(i, l) = 1 if the label l is selected at node i and z((i, j), li, lj) = 1 if the labels li, lj are selected for the nodes i, j. In applications such as parsing and POS tagging, some of the label assignments are not allowed. For example, in dependency parsing the resulting structure</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Messagepassing revisited.</title>
<date>2007</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="6655" citStr="Komodakis et al. (2007)" startWordPosition="1001" endWordPosition="1004">s been demonstrated for a variety of NLP tasks, such as unsupervised and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF optimization via dual decomposition: Messagepassing revisited. In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6740" citStr="Koo et al. (2010)" startWordPosition="1017" endWordPosition="1020">S tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Bel</context>
<context position="20681" citStr="Koo et al., 2010" startWordPosition="3590" endWordPosition="3593">e global objective of equation 1, we note that the difficulty in decoding this objective comes entirely from the constraints z((s, m), h) = Ys(m, h). If these were not there, the problem would factor into two parts, an optimization of F over the test corpus Y(X) and an optimization of g over possible MRF assignments Z. The first problem factors naturally into sentence-level parsing problems and the second can be solved efficiently given our assumptions on the MRF topology G. Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al., 2010). To apply dual decomposition, we introduce Lagrange multipliers u(s, m, h) for the agreement constraints between the sentence-level models and the global MRF. The Lagrangian dual is the function L(u) = maxz g(z, u) + maxy F(y, u) where In order to find ming L(u), we use subgradient descent. This requires computing g(z, u) and F(y, u) for fixed values of u, which by our assumptions from Section 3 are efficient to calculate. The full algorithm is given in Figure 2. We start with the values of u initialized to 0. At each iteration k, we find the best set of parses Y (k) over the entire corpus an</context>
<context position="27490" citStr="Koo et al. (2010)" startWordPosition="4826" endWordPosition="4829">est sets consist of the official test sets augmented with additional sentences from the official training files such that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, 61, 62, and 63 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the English development data. In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by Koo et al. (2010). Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser (McDonald et al., 2005)1 with the gold-standard POS tags of the corpus. For POS tagging we use the Stanford POS tagger (Toutanova et al., 2003)2. 1http://sourceforge.net/projects/mstparser/ 2http://nlp.stanford.edu/software/tagger.shtml 1440 Base 50 Model (ER) Base 100 Model (ER) Base 200 Model (ER) Base 500 Model (ER) ST ST ST ST Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33) Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68</context>
<context position="32021" citStr="Koo et al. (2010)" startWordPosition="5567" endWordPosition="5570">the entire training size range. For 50 training sentences we reduce 10.33% of the overall error, and 11.53% of the error on unknown words. Although the tagger performance substantially improves when the training set grows to 500 sentences, our model still provides an overall error reduction of 4.64% and of 8.33% for unknown words. 9 Discussion Efficiency Since dual decomposition often requires hundreds of iterations to converge, a naive implementation would be orders of magnitude slower than the underlying sentence-level model. We use two techniques to speed-up the algorithm. First, we follow Koo et al. (2010) and use lazy decoding as part of dual decomposition. At each iteration k, we cache the result of the MRF z(k) and set of parse tree Y (k). In the next iteration, we only 1441 50 100 150 200 iteration Figure 3: Efficiency of dependency parsing decoding for three languages. The plot shows the speed of each iteration of the subgradient algorithm relative to a round of unconstrained parsing. Most Effective Contexts WSJ → QTB QTB → WSJ WRB VBP VBD NN NN , DT JJS NN IN IN PRP VBZ VBP PRP VB JJ JJ NN , DT NN NN VB IN JJ JJ NN RBS JJ NN IN NN POS NN NN Table 5: The five most effective constraint cont</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="8018" citStr="Lease and Charniak, 2005" startWordPosition="1207" endWordPosition="1210"> (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholingu</context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5990" citStr="Liang et al., 2009" startWordPosition="895" endWordPosition="898">local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Bayesian measurements (Liang et al., 2009) frameworks. The power of these methods has been demonstrated for a variety of NLP tasks, such as unsupervised and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has b</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning from measurements in exponential families. In ICML.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G S Mann</author>
<author>A McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--955</pages>
<marker>Mann, McCallum, </marker>
<rawString>G.S. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research, 11:955–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5920" citStr="Mann and McCallum, 2007" startWordPosition="883" endWordPosition="886">NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction models. In particular, we focus on dependency parsing which is characterized by a more complicated structure compared to the IE tasks addressed by previous work. Another line of work that integrates corpus-level declarative information into sentence-level models includes the posterior regularization (Ganchev et al., 2010; Gillenwater et al., 2010), generalized expectation (Mann and McCallum, 2007; Mann and McCallum, ), and Bayesian measurements (Liang et al., 2009) frameworks. The power of these methods has been demonstrated for a variety of NLP tasks, such as unsupervised and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constrai</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G.S. Mann and A. McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="26120" citStr="Marcus et al., 1993" startWordPosition="4591" endWordPosition="4594"> we train our model completely in one source domain and test it on a different target domain. In lightly supervised training, we simulate the case where only a limited amount of annotated data is available for a language. Base ST Model ER WSJ —* QTB 89.63 89.99 90.43 7.7 QTB —* WSJ 74.89 74.97 75.76 3.5 Table 2: Dependency parsing UAS for domain adaptation. WSJ is the Penn TreeBank. QTB is the QuestionBank. ER is error reduction. Results are significant using the sign test with p &lt; 0.05. Data for Domain Adaptation We perform domain adaptation experiments in English using the WSJ PennTreebank (Marcus et al., 1993) and the QuestionBank (QTB) (Judge et al., 2006). In the WSJ —* QTB scenario, we train on sections 2-21 of the WSJ and test on the entire QTB (4000 questions). In the QTB —* WSJ scenario, we train on the entire QTB and test on section 23 of the WSJ. Data for Lightly Supervised Training For all English experiments, our data was taken from the WSJ PennTreebank: training sentences from Section 0, development sentences from Section 22, and test sentences from Section 23. For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set (Buchholz and Marsi, 2006) with trainin</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Selftraining for biomedical parsing.</title>
<date>2008</date>
<booktitle>In ACL, sort</booktitle>
<pages>papers.</pages>
<contexts>
<context position="7940" citStr="McClosky and Charniak, 2008" startWordPosition="1194" endWordPosition="1197">manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this pa</context>
</contexts>
<marker>McClosky, Charniak, 2008</marker>
<rawString>David McClosky and Eugene Charniak. 2008. Selftraining for biomedical parsing. In ACL, sort papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7910" citStr="McClosky et al., 2006" startWordPosition="1190" endWordPosition="1193">and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adapatation for parsing.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8086" citStr="McClosky et al., 2010" startWordPosition="1217" endWordPosition="1220">s (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntact</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adapatation for parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="4218" citStr="McDonald et al., 2005" startWordPosition="619" endWordPosition="622"> 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1434–1444, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics exact solution to the global model. We experiment with domain adaptation and lightly supervised training. We demonstrate that global models with consistency constraints can improve upon sentence-level models for dependency parsing and part-of-speech tagging. For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser (McDonald et al., 2005) from newswire to the QuestionBank domain. For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger (Toutanova et al., 2003) for English POS tagging. The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. 2 Related Work Methods that combine inter-sentence information with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our</context>
<context position="10095" citStr="McDonald et al., 2005" startWordPosition="1573" endWordPosition="1576">endency parse is a vector y = {y(m, h) : (m, h) ∈ I(x)} where y(m, h) = 1 if m is a modifier of the head word h. We define the set Y(x) ⊂ {0,1}|I(x)| to be the set of all valid dependency parses for a sentence x. In this work, we use projective dependency parses, but the method also applies to the set of nonprojective parse trees. Additionally, we have a scoring function f : Y(x) → R. The optimal parse y∗ for a sentence x is given by, y∗ = arg maxy∈Y(x) f(y). This sentencelevel decoding problem can often be solved efficiently. For example in commonly used projective dependency parsing models (McDonald et al., 2005), we can compute y∗ efficiently using variants of the Viterbi algorithm. For this work, we make the assumption that we have an efficient algorithm to find the argmax of where u is a vector in R|I(x)|. In practice, u will be a vector of Lagrange multipliers associated with the dependencies of y in our dual decomposition algorithm given in Section 6. We can construct a very similar setting for POS tagging where the goal is to find the best tagging y for a sentence x = (w1, ... , wn). We skip the formal details here. We next introduce notation for Markov random fields (MRFs) (Koller and Friedman,</context>
<context position="27610" citStr="McDonald et al., 2005" startWordPosition="4842" endWordPosition="4845">h that each test file consists of 25,000 words. Our results on the official test sets are very similar to the results we report and are omitted for brevity. Parameters The model parameters, 61, 62, and 63 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the English development data. In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by Koo et al. (2010). Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser (McDonald et al., 2005)1 with the gold-standard POS tags of the corpus. For POS tagging we use the Stanford POS tagger (Toutanova et al., 2003)2. 1http://sourceforge.net/projects/mstparser/ 2http://nlp.stanford.edu/software/tagger.shtml 1440 Base 50 Model (ER) Base 100 Model (ER) Base 200 Model (ER) Base 500 Model (ER) ST ST ST ST Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33) Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93) Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79</context>
<context position="29821" citStr="McDonald et al. (2005)" startWordPosition="5194" endWordPosition="5197">33) Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All results are significant using the sign test with p &lt; 0.05. Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script (Buchholz and Marsi, 2006). We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of McDonald et al. (2005) and to a self-training baseline (Steedman et al., 2003; Reichart and Rappoport, 2007). The parsing baseline is equivalent to a single round of dual decomposition. For the self-training baseline, we parse the test corpus, append the labeled test sentences to the training corpus, train a new parser, and then re-parse the test set. We run this procedure for a single iteration. For POS tagging we measure token level POS accuracy for all the words in the corpus and also for unknown words (words not observed in the training data). We compare the accuracy of POS tagging with global constraints to th</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R.T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
</authors>
<title>Domain Adaptation for Parsing.</title>
<date>2011</date>
<tech>Ph.d. thesis,</tech>
<institution>University of Groningen.</institution>
<contexts>
<context position="8260" citStr="Plank (2011)" startWordPosition="1243" endWordPosition="1244">ally for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were s</context>
</contexts>
<marker>Plank, 2011</marker>
<rawString>Barbara Plank. 2011. Domain Adaptation for Parsing. Ph.d. thesis, University of Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7725" citStr="Reichart and Rappoport, 2007" startWordPosition="1162" endWordPosition="1165">ce-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used b</context>
<context position="28764" citStr="Reichart and Rappoport (2007)" startWordPosition="5024" endWordPosition="5027">.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35) Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27) Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4) Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian, German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency parser of McDonald et al. (2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p &lt; 0.05, all 50, 100, and 200 results are significant, as are Eng and Ger 500. 50 100 200 500 Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER) Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64) Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33) Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et al. (2003). Our inter-sentence POS tagger augment</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>R. Reichart and A. Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8043" citStr="Rimell and Clark, 2008" startWordPosition="1211" endWordPosition="1214">06) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7076" citStr="Roth and Yih, 2005" startWordPosition="1068" endWordPosition="1071"> our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tag</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6718" citStr="Rush et al. (2010)" startWordPosition="1012" endWordPosition="1015"> and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifo</context>
<context position="22562" citStr="Rush et al. (2010)" startWordPosition="3948" endWordPosition="3951">least 10 times. lems modified by the new value of u. If at any point the current solutions Y (k) and z(k) satisfy the consistency constraint, we return their current values. Otherwise, we stop at a max iteration K and return the values from the last iteration. We now give a theorem for the formal guarantees of this algorithm. Theorem 1 Iffor some k E {1... K} in the algorithm in Figure 2, Y (k) s (m, h) = z(k)(s, m, h) for all (s, m, h) E J, then (Y (k), z(k)) is a solution to the maximization problem in equation 1. We omit the proof for brevity. It is a slight variation of the proof given by Rush et al. (2010). 7 Consistency Constraints In this section we describe the consistency constraints used for the global models of parsing and tagging. Parsing Constraints. Recall from Section 4 that we choose parsing constraints based on the word context. We encourage words in similar contexts to choose head words with similar POS tags. We use a simple procedure to select which constraints to add. First define a context template to be a set of offsets {r, ... , s} with r &lt; 0 &lt; s that specify the neighboring words to include in a context. In the example of Figure 1, the context template {−1, 0, 1, 2} applied t</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Junichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="8184" citStr="Sagae and Tsujii, 2007" startWordPosition="1230" endWordPosition="1233">aper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Junichi Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6946" citStr="Smith and Eisner (2008)" startWordPosition="1048" endWordPosition="1051">ake a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the </context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D.A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>S Clark</author>
<author>R Hwa</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="7762" citStr="Steedman et al., 2003" startWordPosition="1167" endWordPosition="1170">on. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS ta</context>
<context position="29876" citStr="Steedman et al., 2003" startWordPosition="5203" endWordPosition="5206">efers to the maximum entropy trigram tagger of Toutanova et al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All results are significant using the sign test with p &lt; 0.05. Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script (Buchholz and Marsi, 2006). We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of McDonald et al. (2005) and to a self-training baseline (Steedman et al., 2003; Reichart and Rappoport, 2007). The parsing baseline is equivalent to a single round of dual decomposition. For the self-training baseline, we parse the test corpus, append the labeled test sentences to the training corpus, train a new parser, and then re-parse the test set. We run this procedure for a single iteration. For POS tagging we measure token level POS accuracy for all the words in the corpus and also for unknown words (words not observed in the training data). We compare the accuracy of POS tagging with global constraints to the accuracy of the Stanford POS tagger 3. Domain Adaptat</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semi-supervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8351" citStr="Subramanya et al. (2010)" startWordPosition="1258" endWordPosition="1261">co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010), hierarchical bayesian models (Finkel and Manning, 2009) and co-training (Sagae and Tsujii, 2007) achieve substantial performance gains. For a recent survey see Plank (2011). Constraints similar to those we use for POS tagging were used by Subramanya et al. (2010) for POS tagger adaptation. 1435 Their work, however, does not show how to decode a global, corpus-level, objective that enforces these constraints, which is a major contribution of this paper. Inter-sentence syntactic consistency has been explored in the psycholinguistics and NLP literature. Phenomena such as parallelism and syntactic priming – the tendency to repeat recently used syntactic structures – have been demonstrated in human language corpora (e.g. WSJ and Brown) (Dubey et al., 2009) and were shown useful in generative and discriminative parsers (e.g. (Cheung and Penn, 2010)). We com</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semi-supervised learning of structured tagging models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A Mccallum</author>
</authors>
<title>Collective segmentation and labeling of distant entities in information extraction.</title>
<date>2004</date>
<booktitle>In In ICML Workshop on Statistical Relational Learning and Its Connections.</booktitle>
<contexts>
<context position="4871" citStr="Sutton and Mccallum, 2004" startWordPosition="725" endWordPosition="729">ionBank domain. For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger (Toutanova et al., 2003) for English POS tagging. The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. 2 Related Work Methods that combine inter-sentence information with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied t</context>
</contexts>
<marker>Sutton, Mccallum, 2004</marker>
<rawString>C. Sutton and A. Mccallum. 2004. Collective segmentation and labeling of distant entities in information extraction. In In ICML Workshop on Statistical Relational Learning and Its Connections.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>P Abbeel</author>
<author>d Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data. In UAI.</title>
<date>2002</date>
<contexts>
<context position="4921" citStr="Taskar et al., 2002" startWordPosition="733" endWordPosition="736"> an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger (Toutanova et al., 2003) for English POS tagging. The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. 2 Related Work Methods that combine inter-sentence information with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by potentials defined over pairs of nodes or clique templates. The latter model makes use of Lagrangian relaxation. Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Finkel et al. (2005) also integrated non-local information into entity annotation algorithms using Gibbs sampling. Our model can be applied to a variety of off-theshelf structured prediction </context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>B. Taskar, P. Abbeel, and d. Koller. 2002. Discriminative probabilistic models for relational data. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4471" citStr="Toutanova et al., 2003" startWordPosition="662" endWordPosition="665">model. We experiment with domain adaptation and lightly supervised training. We demonstrate that global models with consistency constraints can improve upon sentence-level models for dependency parsing and part-of-speech tagging. For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser (McDonald et al., 2005) from newswire to the QuestionBank domain. For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger (Toutanova et al., 2003) for English POS tagging. The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences. 2 Related Work Methods that combine inter-sentence information with sentence-level algorithms have been applied to a number of NLP tasks. The most similar models to our work are skip-chain CRFs (Sutton and Mccallum, 2004), relational markov networks (Taskar et al., 2002), and collective inference with symmetric clique potentials (Gupta et al., 2010). These models use a linear-chain CRF or MRF objective modified by pot</context>
<context position="27730" citStr="Toutanova et al., 2003" startWordPosition="4863" endWordPosition="4866">e report and are omitted for brevity. Parameters The model parameters, 61, 62, and 63 of the scoring function (Section 4) and α of the Lagrange multipliers update rule (Section 6), were tuned on the English development data. In our dual decomposition inference algorithm, we use K = 200 maximum iterations and tune the decay rate following the protocol described by Koo et al. (2010). Sentence-Level Models For dependency parsing we utilize the second-order projective MST parser (McDonald et al., 2005)1 with the gold-standard POS tags of the corpus. For POS tagging we use the Stanford POS tagger (Toutanova et al., 2003)2. 1http://sourceforge.net/projects/mstparser/ 2http://nlp.stanford.edu/software/tagger.shtml 1440 Base 50 Model (ER) Base 100 Model (ER) Base 200 Model (ER) Base 500 Model (ER) ST ST ST ST Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33) Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93) Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35) Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 </context>
<context position="29325" citStr="Toutanova et al. (2003)" startWordPosition="5121" endWordPosition="5124">s a self-training model based on Reichart and Rappoport (2007). Model is the same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p &lt; 0.05, all 50, 100, and 200 results are significant, as are Eng and Ger 500. 50 100 200 500 Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER) Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64) Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33) Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All results are significant using the sign test with p &lt; 0.05. Evaluation and Baselines To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script (Buchholz and Marsi, 2006). We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of McDonald et al. (2005) and to a self-training baseline (Steedman et al., 2003; Reichart and Rappoport, 2007). The parsing base</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C.D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>MAP estimation via agreement on trees: message-passing and linear programming.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Information Theory,</booktitle>
<volume>51</volume>
<pages>3697--3717</pages>
<contexts>
<context position="6630" citStr="Wainwright et al. (2005)" startWordPosition="997" endWordPosition="1000"> power of these methods has been demonstrated for a variety of NLP tasks, such as unsupervised and semi-supervised POS tagging and parsing. The constraints used by these works differ from ours in that they encourage the posterior label distribution to have desired properties such as sparsity (e.g. a given word can take a small number of labels with a high probability). In addition, these methods use global information during training as opposed to our approach which applies test-time inference global constraints. The application of dual decomposition for inference in MRFs has been explored by Wainwright et al. (2005), Komodakis et al. (2007), and Globerson and Jaakkola (2007). In NLP, Rush et al. (2010) and Koo et al. (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Work on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, sugge</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2005</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP estimation via agreement on trees: message-passing and linear programming. In IEEE Transactions on Information Theory, volume 51, pages 3697–3717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>G Haffari</author>
<author>S Wang</author>
<author>G Mori</author>
</authors>
<title>A rate distortion approach for semi-supervised conditional random fields.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7485" citStr="Wang et al., 2009" startWordPosition="1122" endWordPosition="1125">on dual decomposition for NLP is related to the work of Smith and Eisner (2008) who apply belief propagation to inference in dependency parsing, and to constrained conditional models (CCM) (Roth and Yih, 2005) that impose inference-time constraints through an ILP formulation. Several works have addressed semi-supervised learning for structured prediction, suggesting objectives based on the max-margin principles (Altun and Mcallester, 2005), manifold regularization (Belkin et al., 2005), a structured version of co-training (Brefeld and Scheffer, 2006) and an entropy-based regularizer for CRFs (Wang et al., 2009). The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, selftraining (Reichart and Rappoport, 2007), co-training (Steedman et al., 2003) and active learning (Hwa, 2004) have been shown useful in the lightly supervised setup. For parser adaptation, self-training (McClosky et al., 2006; McClosky and Charniak, 2008), using weakly annotated data from the target domain (Lease and Charniak, 2005; Rimell and Clark, 2008), ensemble learning (McClosky et al., 2010</context>
</contexts>
<marker>Wang, Haffari, Wang, Mori, 2009</marker>
<rawString>Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009. A rate distortion approach for semi-supervised conditional random fields. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Weiss</author>
<author>B Taskar</author>
</authors>
<title>Structured prediction cascades.</title>
<date>2010</date>
<booktitle>In Proc. of AISTATS,</booktitle>
<volume>volume</volume>
<pages>1284</pages>
<contexts>
<context position="33261" citStr="Weiss and Taskar (2010)" startWordPosition="5800" endWordPosition="5803">ain adaptation experiments. The bold POS tag indicates the modifier word of the context. Figure 4: Subset of sentences with the context WRB VBP from WSJ → QTB domain adaptation. In the first round, the parser chooses VBN for the first sentence, which is inconsistent with similar contexts. The constraints correct this choice in later rounds. recompute the solution Y3 for a sentence s if the weight u(s, m, h) for some m, h was updated. A similar technique is applied to the MRF. Second, during the first iteration of the algorithm we apply max-marginal based pruning using the threshold defined by Weiss and Taskar (2010). This produces a pruned hypergraph for each sentence, which allows us to avoid recomputing parse features and to solve a simplified search problem. To measure efficiency, we compare the time spent in dual decomposition to the speed of unconstrained inference. Across experiments, the mean dual decomposition time is 1.71 times the cost of unconstrained inference. Figure 3 shows how this time is spent after the first iteration. The early iterations are around 1% of the total cost, and because of lazy decoding this quickly drops to almost nothing. Exactness To measure exactness, we count the numb</context>
</contexts>
<marker>Weiss, Taskar, 2010</marker>
<rawString>D. Weiss and B. Taskar. 2010. Structured prediction cascades. In Proc. of AISTATS, volume 1284.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
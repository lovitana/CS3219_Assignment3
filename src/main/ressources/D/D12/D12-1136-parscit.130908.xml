<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.999321">
Revisiting the Predictability of Language:
Response Completion in Social Media
</title>
<author confidence="0.952814">
Bo Pang Sujith Ravi
</author>
<affiliation confidence="0.863537">
Yahoo! Research
</affiliation>
<address confidence="0.882718">
4401 Great America Parkway
Santa Clara, CA 95054, USA
</address>
<email confidence="0.995154">
bopang42@gmail.com sujith ravi@yahoo.com
</email>
<sectionHeader confidence="0.997297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850619047619">
The question “how predictable is English?”
has long fascinated researchers. While prior
work has focused on formal English typically
used in news articles, we turn to texts gener-
ated by users in online settings that are more
informal in nature. We are motivated by a
novel application scenario: given the difficulty
of typing on mobile devices, can we help re-
duce typing effort with message completion,
especially in conversational settings? We pro-
pose a method for automatic response comple-
tion. Our approach models both the language
used in responses and the specific context pro-
vided by the original message. Our experi-
mental results on a large-scale dataset show
that both components help reduce typing ef-
fort. We also perform an information-theoretic
study in this setting and examine the entropy
of user-generated content, especially in con-
versational scenarios, to better understand pre-
dictability of user generated English.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908846153846">
How predictable is language? As early as 1951, long
before large quantities of texts (or the means to pro-
cess them) were easily available, Shannon had raised
this question and proceeded to answer it with a set
of clever analytical estimations. He studied the pre-
dictability of printed English, or “how well can the
next letter of a text be predicted when the preced-
ing N letters are known” (Shannon, 1951). This
was quantified as the conditional entropy, which
measures the amount of information conveyed from
statistics over the preceding context. In this paper,
we discuss a novel application setting which mirrors
the predictability study as defined by Shannon.
</bodyText>
<figure confidence="0.995736">
(a) Google (b) Amazon (c) Netflix
</figure>
<figureCaption confidence="0.996053666666667">
Figure 1: Query completion as users type into the “Search
using Google” box on a browser, as well as the search box
in Amazon and Netflix.
</figureCaption>
<bodyText confidence="0.99973975">
Text completion for user-generated texts: Con-
sider a user who is chatting with her contact or post-
ing to a social media site using a mobile device. If
we can predict the next word given the preceding
words that were already typed in, we can help reduce
the typing cost by offering users suggestions of pos-
sible completions of their partially typed messages
(e.g., in a drop-down list). If the intended word is
ranked reasonably high, the user can select the word
instead of typing it. Assuming a lower cost associ-
ated with selections, this could lead to less typing
effort for the user.
An interface like this would be quite familiar to
Web users today. Providing suggestions of possi-
ble completions to partially typed queries, which we
will refer to as query completion,1 is a common fea-
ture of search boxes (Figure 1). In spite of the
similarity in the interface, the underlying technical
challenge can be quite different. Query completion
does not necessarily rely on language models: can-
</bodyText>
<footnote confidence="0.8934785">
1Note that this feature is often tagged as “query suggestion”
in the user interface; we avoid that terminology since it is often
used to refer to query re-formulation (of a completely entered
query) in the literature, which is a very different task.
</footnote>
<page confidence="0.922823">
1489
</page>
<note confidence="0.7914205">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1489–1499, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944666666667">
didate completions can be limited to popular queries
that were previously submitted to the site or entries
in a closed database of available objects, and rank-
ing can be done by overall popularity. In contrast,
our scenario requires generation of unseen texts.
Given the difficulty of generating full-length text,
we consider a more realistic setting, where we per-
form completion on a word-by-word basis. Each
time, we propose candidate completions at the word-
level when the user is about to start a new word,
or has partially entered the first few letters; once
this word is successfully completed, we move on
to the next one. This predict-verify-predict process
exactly mirrors the human experiment described by
Shannon (1951), except we do this at the word-level
rather than the letter-level: having the user examine
and verify predictions at the letter level would not be
a practical solution for the intended application.
The response completion task: In addition, our
task has another interesting difference from Shan-
non’s human experiment. Consider the mobile-
device user mentioned previously. If the user is re-
plying to a piece of text (e.g., an instant message
sent by a contact), we have an additional source of
contextual information in the stimulus, or the text
which triggered the response that the user is try-
ing to type. Can we learn from previously observed
stimulus-response pairs (which we will refer to as
exchanges)? That is, can we take advantage of this
conversational setting and effectively use the infor-
mation provided by stimulus to better predict the
next word in the response? We refer to this task as
the response completion task.
Our task is different from “chatter-bots” (Weizen-
baum, 1966), where the goal is to generate a re-
sponse to an input that would resemble a human con-
versation partner. Instead, we want to complete a
response as the replier intends to. Recently, Ritter
et. al (2011) experimented with automatic response
generation in social media. They had a similar con-
versational setting, but instead of completion based
on partial input, they attempted to generate a re-
sponse in its entirety given only the stimulus. While
many of the generated responses are deemed possi-
ble replies to the stimulus, they have a low chance
of actually matching the real response given by the
user: they reported BLEU scores between 0 and 2
for various systems. This clearly shows the diffi-
culty of the task. While we are addressing a more
modest setting, would the problem prove to be too
difficult even in this case?
In this paper, we propose a method for auto-
matic response completion. Our approach models
the generic language used in responses, as well as
the contextual information provided by the stim-
ulus. We construct a large-scale dataset of user-
generated textual exchanges, and our experimental
results show that both components help reduce typ-
ing effort. In addition, to better understand pre-
dictability of user generated English, we perform
an information-theoretic study in this conversational
setting to investigate the entropy of user-generated
content.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999889">
There has been previous work in the area of human-
computer interaction that examined text entry for
mobile devices (MacKenzie and Soukoreff, 2002).
In particular, one line of work looked into predic-
tive text input, which examined input effort reduc-
tion by language prediction. Previous work in pre-
dictive text input had very different focus from our
study. Oftentimes, the focus was to model actual
typing efforts using mobile device keypads, examine
the speed and cognitive load of different input meth-
ods, and evaluate with emprical user studies in lab
settings (James and Reischel, 2001; How and Kan,
2005), where the underlying technique for language
prediction can be as simple as unigram frequency
(James and Reischel, 2001), or restricted to narrow
domains such as grocery shopping lists (Nurmi et
al., 2009). In addition, to the best of our knowledge,
no previous work in predictive text input addressed
the conversationl setting.
As discussed in Section 1, the response genera-
tion task (Ritter et al., 2011) also considered the con-
verstational setting, but the MT-based technique was
not well-suited to produce responses as intended by
the user. There has been extensive previous research
in language modeling (Rosenfeld, 2000). While pre-
vious work has explored Web text sources that are
“better matched to a conversational speaking style”
(Bulyko et al., 2003), we are not aware of much pre-
vious work that has taken advantage of information
in the stimulus for word predictions in responses.
</bodyText>
<page confidence="0.982675">
1490
</page>
<bodyText confidence="0.999923333333333">
Previous work on entropy of language stems from
the field of information theory (Shannon, 1948),
starting with Shannon (1951). An extensive bibliog-
raphy covering early related work (e.g., insights into
the structure of language via information theory, en-
tropy estimates via other techniques and/or for dif-
ferent languages, as well as a broad range of applica-
tions of such estimates) can be found in (Cover and
King, 1978). More recently, Brown et. al (1992)
computed an upper bound for the entropy of En-
glish with a trigram model, using the Brown corpus.
Some other related works on this topic include (Tea-
han and Cleary, 1996; Moradi et al., 1998). There
was also a recent study using entropy in the context
of Web search (Mei and Church, 2008). In other set-
tings, entropy has also been employed as a tool for
studying the linguistic properties of ancient scripts
(e.g., Indus Script) (Rao et al., 2009). While this
seems like an interesting application of information
theory for linguistic studies, it has also generated
some controversies (Farmer et al., 2004).
In contrast, our work departs from traditional sce-
narios significantly. We perform entropy studies
over texts generated in online settings which are
more informal in nature. Additionally, we utilize the
properties of language predictability within a novel
application for automatically completing responses
in conversational settings. Also, in our case we do
not have to worry about issues like “is this a lan-
guage or not?” because we work with real English
news data which include articles written by pro-
fessional editors and comments generated by users
reading those articles.
</bodyText>
<sectionHeader confidence="0.992724" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999738">
In this section, we first state our problem more for-
mally, followed by descriptions of the basic N-gram
language model we use, as well as two approaches
that model both stimulus and preceding words in
response as the context for the next-word genera-
tion. Given the intended application, we hope to
achieve better prediction without incurring signifi-
cant increase in model size.
</bodyText>
<subsectionHeader confidence="0.998678">
3.1 Problem definition
</subsectionHeader>
<bodyText confidence="0.98346775">
Consider a stimulus-response pair, where the stim-
ulus is a sequence of tokens s = (s1, s2, ..., sm),
and the response is a sequence of tokens r =
(r1, r2, ..., rn). Let r1..i = (r1, r2, ..., ri), our task
is to generate and rank candidates for ri+1 given s
and r1..i.
Note the models described in this section do not
assume any knowledge of partial input for ri+1. For
the setting where the first c characters of ri+1 were
also entered, we can restrict the candidate list to the
subset with the matching prefix, and use the same
ranking function.
</bodyText>
<subsectionHeader confidence="0.999225">
3.2 Generic Response Language Model
</subsectionHeader>
<bodyText confidence="0.998594625">
First, we consider an N-gram language model
trained on all responses in the training data as our
generic response language model. Here we consider
N = 3. Normally, trigram models use back-off to
both bigrams and unigrams; in order to compare the
effectiveness of trigram models vs. bigram models
under comparable model size, we use back-off only
to unigrams in both cases:
</bodyText>
<equation confidence="0.999059">
trigram: P(ri+1  |r1..i) = A1 * P3(ri+1  |ri, ri−1)
+(1 − A1) * P1(ri+1)
bigram: P(ri+1 |r1..i) = A1 * P2(ri+1  |ri)
+(1 − A1) * P1(ri+1)
</equation>
<bodyText confidence="0.9951413">
If we ignore the context provided by texts in the
stimulus, we can simply generate and rank candidate
words from the dictionary according to the generic
response LM: P(ri+1  |r1..i).
As we will discuss in more detail in Section 6.2,
modeling s and r1..i jointly in the prediction of ri+1
would be rather expensive. In the following sec-
tions, we follow two main approaches to break this
down into separate components: P(ri+1  |r1..i) and
P(ri+1  |s), and model each one separately.
</bodyText>
<subsectionHeader confidence="0.999648">
3.3 Translating Stimuli to Responses
</subsectionHeader>
<bodyText confidence="0.9868237">
As mentioned in Section 1, Ritter et. al (2011) have
considered a related task of generating a response in
its entirety given only the text in the stimulus. They
cast the problem as a translation task, where the
stimulus is considered as the source language and
the response is considered as the target language.
We can adapt this approach for our response com-
pletion task.
Consider the noisy channel model used in statisti-
cal machine translation: P(r|s) a P(r)*P(s|r). In
</bodyText>
<page confidence="0.95507">
1491
</page>
<bodyText confidence="0.999873">
order to predict ri+1 given r1..i and s, for each can-
didate ri+1, in principle one can marginalize over
all possible completions of r1..i+1, and rank candi-
date ri+1 by that. That is, let P(n) be the distribution
of response length, let r0 be a possible completion
of r1..i+1 (i.e., a response whose first i + 1 tokens
match r1..i+1). For each possible n &gt; i, we need to
marginalize over all possible r0 of length n, and rank
ri+1 according to
</bodyText>
<equation confidence="0.919958">
�P(r1..i+1  |s) = P(n) - � P(r0  |s)
n&gt;i |r1|=n
</equation>
<bodyText confidence="0.99733225">
Clearly this will be computationally expensive. In-
stead, we take a greedy approach, and choose ri+1
which yields the optimal partial response (without
looking ahead):
</bodyText>
<equation confidence="0.984261666666667">
P(r1..i+1  |s) a P(r1..i+1) * P(s  |r1..i+1)
which is equivalent to ranking candidate ri+1 by
P(ri+1  |r1..i) * P(s  |r1..i+1) (1)
</equation>
<bodyText confidence="0.999543285714286">
Since the first component is our LM model, and the
second component is a translation model, we denote
this as the LM+TM model. We use IBM Model-1 to
learn the translation table on the training data. At
test time, equal number of candidates are generated
by each component, and combined to be ranked by
Eq. 1.
</bodyText>
<subsectionHeader confidence="0.904295">
3.4 Mixture Model
</subsectionHeader>
<bodyText confidence="0.999739833333333">
One potential concern over applying the translation
model is that the response can often contain novel in-
formation not implied by the stimulus. While tech-
nically this could be generated from the so-called
null token used in machine translation (added to the
source text to account for target text with no clear
alignment in the source text), significant amount of
text corresponding to new information not in the
source text is not what null tokens are meant to be
capturing. In general, our problem here is a lack of
clear word-to-word or phrase-to-phrase mapping in
a stimulus-response pair, at least not what one would
expect in clean parallel data.
Alternatively, one can model the response genera-
tion process with a mixture model: with probability
As, we generate a word according to a distribution
over s (P(w  |s)), and with probability 1 − As, we
generate a word using the response language model:
</bodyText>
<equation confidence="0.980342666666667">
P(ri+1  |s, r1..i) = As - P(ri+1 |(S)
+ (1 − As) - P(ri+1  |r1..i)
(2)
</equation>
<bodyText confidence="0.999753076923077">
We examine two concrete ways of exploiting the
context provided by s.
Model 1 — LM + Selection model First, we ex-
amine a very simple instantiation of P(w  |s) where
we select a token in s uniformly at random. This
is based on the intuition that to be semantically co-
herent, a reply often needs to repeat certain content
words in the stimulus. (Similar intuition has been
explored in the context of text coherency (Barzilay
and Lapata, 2005).) This is particularly useful for
words that are less frequently used: they may not
be able to receive enough statistics to be promoted
otherwise. More specifically,
</bodyText>
<equation confidence="0.962059333333333">
1ri+1∈s
P(ri+1  |s) =
|s|
</equation>
<bodyText confidence="0.9999385">
We can take As to be a constant Aselect, which can be
estimated in the training data as the probability of a
response token being a repetition of a token in the
corresponding stimulus.
Model 2 — LM + Topic model Another way to
incorporate information provided in s is to use it to
constrict the topic in r. We can learn a topic model
over conversations in the training data using Latent
Dirchlet Allocation (LDA) (Blei et al., 2003). At test
time, we identify the most likely topic of the conver-
sation based on s, and expect ri+1 to be generated
from this topic. That is,
</bodyText>
<equation confidence="0.938119">
P(ri+1  |s) = P(ri+1  |t∗)
</equation>
<bodyText confidence="0.994677222222222">
where t∗ = argmaxtP(topic = t  |s)
More specifically, we first train a topic model on (s,
r) pairs from the training data. Given a new stim-
ulus s, we then select the highest ranked topic as
being representative of s. Note that alternatively we
could consider all possible topic assignments; in that
case we would have had to sum probabilities over
all topics, and that could also introduce noise. A
similar strategy has been previously employed for
</bodyText>
<page confidence="0.971547">
1492
</page>
<bodyText confidence="0.999904">
other topic modeling applications in information re-
trieval, where documents are smoothed with their
highest ranked topic (Yi and Allan, 2009). We lower
the weight As if P(t*  |s) is low. That is, we use
As = Atopic * P(t*  |s) in Eq. 2.
</bodyText>
<sectionHeader confidence="0.995919" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.980881609756098">
In order to investigate text completion in a conver-
sational setting, we need to construct a large-scale
dataset with textual exchanges among users. An
ideal dataset would have been a collection of in-
stant messages, but these type of datasets are diffi-
cult to obtain given privacy concerns. To the best
of our knowledge, existing SMS (short message ser-
vice) datasets only contain isolated text spans and do
not provide enough information to reconstruct the
conversations. There are, however, a high volume
of textual exchanges taking places in public forums.
Many sites with a user comment environment allow
other users to reply to existing comments, where the
original comment and its reply can form a (stimulus,
response) pair for our purposes.
To this end, we extracted (comment, reply) pairs
from Yahoo! News2, where under each news article,
a user can post a new comment or reply to an exist-
ing comment. In fact, a popular comment can have a
long thread of replies where multi-party discussions
take place. To ensure the reply is a direct response to
the original comment, we took only the first reply to
a comment, and consider the resulting pair as a tex-
tual exchange in the form of a (stimulus, response)
pair. We gathered data from a period of 14 weeks
between March and May, 2011. A random sample
yielded a total of 1,487,995 exchanges, representing
237,040 unique users posting responses to stimuli
comments authored by 357,811 users. In the raw
dataset (i.e., before tokenization), stimuli average at
59 tokens (332 characters), and responses average at
26 tokens (144 characters).
We took the first 12 weeks of data as training data
(1,269,732 exchanges) and the rest 2 weeks of data
as test data (218,263 exchanges).
2Note that previous work has used a dataset with 1 million
Twitter conversations extracted from a scraping of the site (Rit-
ter et al., 2011), where a status update and its replies in Twitter
form “conversations”. This dataset is no longer publicly avail-
able. At the time of this writing, we were not able to identify a
data source to re-construct a dataset like that.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995419">
5.1 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.947174976190476">
Recall@k : Here, we follow a standard evaluation
strategy used to assess ranking quality in informa-
tion retrieval applications. For each word, we check
if the correct answer is one of the top-k tokens being
suggested. We then compute the recall at different
values of k. While this is a straight-forward mea-
sure to assess the overall quality of different top-k
lists, it is not tailored to suit our specific task of re-
sponse completion. In particular, this measure (a)
does not distinguish between (typing) savings for a
short word versus a long one, and (b) does not dis-
tinguish between the correct answer being higher up
in the list versus lower as long as the word is present
in the top-k list.
TypRed : Our main evaluation measure is based on
“reduction in typing effort for a user of the system”,
which is a more informative measure for our task.
We estimate the typing reduction via a hypothetical
typing model3 in the following manner:
Suppose we show top k predictions for a given
setting. Now, there are two possible scenarios:
1. if the user does not find the correct answer in
the top-k list, he/she gives up on this word and
will have to type the entire word. The typing
cost is then estimated to be the number of char-
acters in the word l,,,;
2. if the user spots the correct answer in the list,
the cost for choosing the word is proportional
to the rank of the word rank,,,, with a fixed cost
ratio co. Suppose the user scrolls down the list
using the down-arrow (1) to reach the intended
word (instead of typing), then rank,,, · co re-
flects the scrolling effort required, where co is
the relative cost of scrolling down versus typing
a character.
In general, pressing a fixed key can have a lower
cost than typing a new one, in addition, we can
imagine a virtual keyboard where navigational keys
occupy bigger real-estate, and thus incur less cost
to press. As a result, it’s reasonable to assume co
value that is smaller than 1. In all our experiments,
co = 0.5 unless otherwise noted. Note that if the
</bodyText>
<footnote confidence="0.981338">
3More accurate measures can be be developed by observing
user behavior in a lab setting. We leave this as future work.
</footnote>
<page confidence="0.663294">
1493
</page>
<table confidence="0.999234142857143">
System TypRed (c = 0) TypRed (c = 1) TypRed (c = 2)
1. Generic Response LM (trigram) 15.10 22.57 14.29
2. Generic Response LM (trigram) + TM 9.03 17.53 11.56
3. Mixture Model 1: 15.18∗ 23.43∗ 15.13∗
Generic Response LM (trigram) + Selection
4. Mixture Model 2: 15.10 22.57 14.33
Generic Response LM (trigram) + Topic Model
</table>
<tableCaption confidence="0.97742">
Table 1: Comparison of various prediction models (in terms of TypRed score @ rank 5) on all (stimulus,response)
pairs from a large test collection (218,263 exchanges) when the first c characters of each word are typed in by the user.
A higher score indicates better performance. * indicates statistical significance (p &lt; 0.05) over the baseline score.
</tableCaption>
<bodyText confidence="0.999930666666667">
typing model assumes a user selects the intended
word using an interface that is similar to a mousing
device, the cost may increase with rank,,, at a sub-
linear rate; in that case, our measure will be over-
estimating the cost.
In order to have a consistent measure that al-
ways improves as the ranking improves, we assume
a clever user who will choose to finish the word by
typing or by selecting, depending on which cost is
lower. Combining these two cases under the clever-
user model, we estimate the reduction in typing cost
for every word as follows:
</bodyText>
<equation confidence="0.9350845">
min(l,,,, rank,,, · c0)
rp �p���(w, rank,,,) = 100·[1−
</equation>
<bodyText confidence="0.9998758">
where w is the correct word, l,,, is the length of
w, and rank,,, is the rank of w in the top-k list.
A higher value of TypRed implies higher savings
achieved in typing cost and thereby better prediction
performance.
</bodyText>
<subsectionHeader confidence="0.989392">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99995225">
We run experiments using the models described in
Section 3 under two different settings: (1) previous
words from the response are provided, and (2) pre-
vious words from response + first c characters of the
current word are provided.
During the candidate generation phase, for every
position in the response message we present the top
1,000 candidates (as scored by the generic response
language model or mixture models). We reserve a
small subset of (�1,000) exchanges as development
data for tuning parameters from our models.
For the generic response language models, we
set the interpolation weight Al = 0.9. For the
selection-based mixture model, we estimate the mix-
ture weights on the training data and set A
(0.09). For the topic-based mixture model, we ran
a grid search with different parameter settings for
At,,pic on the held-out development set and chose the
value (0.01) that gave the best performance (in terms
of TypRed).
</bodyText>
<sectionHeader confidence="0.664455" genericHeader="method">
5.3 Results
</sectionHeader>
<bodyText confidence="0.999688533333333">
Previous words from response observed: We first
present results for the setting where only previous
words from the response are provided as context.
We use TypRed scores as our evaluation measure
here (higher TypRed implies more savings in typ-
ing effort). Even with a unigram LM we achieve
a small but non-negligible reduction (TypRed=2.15)
in the typing cost. But a bigram LM significantly
improves performance (TypRed=11.91), and with
trigram LM we observe even better performance
(TypRed=15.10). Since the trigram LM yields a
high performance, we set this as our default LM for
all other models.
Recall that in all experiments, we set co, the cost
ratio of selecting a candidate from the ranked top-
k list (via scrolling) versus typing a character to a
value of 0.5. But we also experimented with a hy-
pothetical setting where co = 1 and noticed that the
trigram LM achieves a slightly lower but still signif-
icant typing reduction (TypRed score of 9.58 versus
15.10 for the earlier case).
The first column of Table 1 (c = 0) compares the
performance of other models for this setting. We
find that adding a translation model (LM+TM) does
not help for this task; in fact, it results in lower
scores than using the LM alone. This suggests that
a translation-based generative approach may not be
suitable, if the goal is to predict text as intended by
the user. This is consistent with previous observa-
tions on a related task (Ritter et al., 2011), as we
</bodyText>
<equation confidence="0.536535">
�
l,,,
select
</equation>
<page confidence="0.968943">
1494
</page>
<bodyText confidence="0.995515319148936">
discussed in Section 1.
In contrast, the mixture models do much better.
In fact, LM+Selection model produces better results
than trigram LM alone. We also note that estimat-
ing the mixture parameter on the training data rather
than using a fixed value increases TypRed scores:
14.02 with a fixed Aselect = 0.5 versus 15.18 with
A* select = 0.09. This comparison also holds for
c &gt; 0 — that is, a naive version of LM+Selection
that selects a word from the stimulus whenever the
prefix allows would not have worked well.
In principle the LM+Topic model is potentially
more powerful in that P(w I s) is not limited to
the words in s. However, in our experiments it
does not yield any considerable improvement over
the original LM. We postulate that this could be due
to the following reason: once the context provided
by s is reduced to the topic level, it is not specific
enough to provide additional information over pro-
ceding words in the response.
Previous words from response + first c characters
of current word observed: Table 1 also compares
the TypRed performance of all the models under set-
tings where c &gt; 0. We notice striking improve-
ments in performance for c = 1 which is consistent
across all models. Our best model is able to save
the user approximately 23% in terms of typing ef-
fort (according to TypRed scores). Interestingly a lot
less reduction was observed for c = 2: the second
character, on average, does not improve the ranking
enough to justify the cost of typing this extra char-
acter.
Next, we pick our best model (Mixture Model 1)
and perform some further analysis. We examine the
effect of providing longer list (shown in Table 2) and
notice little further improvement beyond k = 10.
We also note that the TypRed improvement achieved
over the baseline (LM) model at rank 10 is more than
twice the gain achieved at rank 5.
We also evaluated its performance using a stan-
dard measure (Recall@k). Figure 2 plots the recall
achieved by the system at different ranks k. An in-
creasing recall at even high ranks (k = 100) sug-
gests that the quality of the candidate list retrieved
by this model is good. This also suggests that there
is still room for improvements, and we leave that as
interesting future work.
</bodyText>
<figure confidence="0.886339714285714">
Rank (k) TypRed score
1 9.02
5 15.18
10 16.14
15 16.28
20 16.29
25 16.29
</figure>
<tableCaption confidence="0.707397">
Table 2: Comparison of typing reductions achieved over
the entire test data when top k list is provided to the user.
</tableCaption>
<figure confidence="0.992368555555556">
70
60
50
40
30
20
10
0 10 20 30 40 50 60 70 8
Rank (k)
</figure>
<figureCaption confidence="0.959218">
Figure 2: Recall @ rank k for Mixture Model 1 on the
entire test data.
</figureCaption>
<figure confidence="0.9315425">
0 2 4 6 8 10 12
Word length (# of characters)
</figure>
<figureCaption confidence="0.924173">
Figure 3: Average TypRed score versus Word length (#
of characters) for Mixture Model 1 when the first c char-
acters of the word is typed in by the user.
</figureCaption>
<bodyText confidence="0.948473083333333">
Rcall @ k
Finally, in Figure 3, we plot the average TypRed
scores against individual token (word) length. Fig-
ure 3 indicates that the model is able to achieve a
higher reduction for shorter words compared to very
long ones. This demonstrates the utility of such a re-
sponse completion system, especially since shorter
words are predominant in conversational settings.
We also compared the average reduction achieved
on messages of different lengths (number of words).
Overall we observe consistent reduction for differ-
ent message lengths. This suggests our system can
</bodyText>
<figure confidence="0.949691076923077">
35
30
25
20
15
10
5
0
no letter provided (c=0)
first letter provided (c=1)
first 2 letters provided (c=2)
0
R
</figure>
<page confidence="0.951512">
1495
</page>
<table confidence="0.994734166666667">
Source Top translations
:) :) ! you ? :D
lmao lol lmao u ... i
feeling feeling feel better ! you
question . question the , to
Are I . , are yes
</table>
<tableCaption confidence="0.9961815">
Table 3: Examples of a few stimulus/response transla-
tions learned using IBM Model-1.
</tableCaption>
<table confidence="0.9914845">
OBAMA, USA, Fact, Meghan, GIVE, PRESIDENT, Canadian,
Mitch, Jon, Kerry, TODAY, Justice, Liberalism,...
President, Notice, Tax, LMAO, Hmmm, Trump, people,
OBAMA, common, Aren, WAIT, Bachman, mon, McCain,...
Great, Cut, Release, Ummm, Rest, Mark, isnt, YAHOO, Sad,
END, RON, jesus, Ugh, TRUMP, ...
Nice, Navy, Make, Interesting, Remember, Excuse, WAKE,
Hooray, Birth, mon, Yeah, Dumb, Michael, geronimo, ...
</table>
<bodyText confidence="0.9999447">
English as how well can the next token be predicted
when the preceding N tokens are known. How much
does the immediate context in the response help re-
duce the uncertainty? How does user-generated con-
tent compare with more formal English in this re-
spect? And how about the corresponding stimuli —
given the preceding N tokens, does the knowledge
of stimulus further reduce the uncertainty? These
questions motivated a series of studies over entropy
in different datasets.4
</bodyText>
<subsectionHeader confidence="0.999952">
6.1 Comparison of N-gram entropy
</subsectionHeader>
<bodyText confidence="0.999663666666667">
Following Shannon (1951), we consider the follow-
ing function FN, which can be called the N-gram
entropy, as the measure of predictability:
</bodyText>
<tableCaption confidence="0.759683333333333">
Table 4: Examples of top representative words for a few �FN = − p(bi,j) lo92 p(j  |bi)
topics generated by the LDA topic model trained on news i,j
comment data.
</tableCaption>
<bodyText confidence="0.990196">
be useful for both Tweet-like short messages as well
as more lengthy exchanges in detailed discussions.
</bodyText>
<subsectionHeader confidence="0.946418">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99991575">
Table 3 displays some sample translations learned
using the TM model described in Section 3.3. In-
terestingly, emoticons and informal expressions like
:) or lmao in the stimulus tend to evoke similar
type of expressions in the response (as seen in Ta-
ble 3). Some translations (e.g., feeling —* better) are
indicative of question/answer type of exchanges in
our data. But most of the other translations are noisy
or uninformative (e.g., Are —* .). This provides fur-
ther evidence as to why a translation-based approach
is not well suited for this particular task and hence
does not perform as well as other methods.
Finally, in Table 4, we provide a few sample top-
ics generated by the LDA topic model (which is used
by Mixture Model 2 described in Section 3.4). We
find that while a few topics display some seman-
tic coherence (e.g., political figures), many of them
are noisy (or too generic) which further supports our
earlier observation that they are not useful enough to
help in the prediction task.
</bodyText>
<sectionHeader confidence="0.929481" genericHeader="method">
6 Entropy of user comments
</sectionHeader>
<bodyText confidence="0.999301038461539">
We adapt the notion of predictability of English as
examined by Shannon (1951) from letter-prediction
to token-prediction, and define the predictability of
where bi is a block of N −1 tokens, j is an arbitrary
token following bi, and p(j  |bi) is the conditional
probability of j given bi. This conditional entropy
reflects how much is the uncertainty of the next to-
ken reduced by knowing the preceding N−1 tokens.
Under this measure, is user-generated content
more predictable or less predictable than the more
formal “printed” English examined by Shannon?
Maybe it is more predictable, since most users in
informal settings use simpler English, which may
contain fewer variations than the complex structures
observed in more formal English. Or perhaps it is
less predictable — variations among different users
(who may not follow proper grammar) may lead
to more uncertainty in the prediction of “the next
word”. Which would be the case?
To answer this question empirically, we construct
a reference dataset written in more formal English
(Df) to be compared against the user comments
dataset described in Section 4 (Dj. If Df covers
very different topics from D,,, then even if we do ob-
serve differences in entropy, it could be due to topi-
cal differences. A standard mixed-topic dataset like
</bodyText>
<footnote confidence="0.738167571428571">
4Note that our findings are not to be interpreted as prediction
performance over unseen texts. For that, one needs to compute
cross-entropy between training and test corpora. Since Section
5 is already addressing this question with proper training / test
split, in this section, we focus on the variability of language
usage in a corpus. This also avoids having to control for “com-
parable” training/test splits in different types of datasets.
</footnote>
<page confidence="0.990293">
1496
</page>
<bodyText confidence="0.999880731707317">
the Brown Corpus (Kucera and Francis, 1967) may
not be ideal in this sense (e.g., it contains fiction cat-
egories such as “Romance and Love Story”, which
may not be represented in our Du). Instead, we ob-
tained a sample of news articles on Yahoo! News
during March - May, 2011, and extracted unique
sentences from these articles. This yields a Df with
more comparable subject matters to Du.5
Next, we compare both the entropy over unigrams
and N-gram entropy in three datasets: the news ar-
ticle dataset described above, and comments data
(Section 4) separated into stimuli and responses. We
also report corresponding numbers computed on the
Brown Corpus as references. Note that datasets with
different vocabulary size can lead to different en-
tropy: the entropy of picking a word from the vo-
cabulary uniformly at random would have been dif-
ferent. Thus, we sample each dataset at different
rates, and plot the (conditional) entropy in the sam-
ple against the corresponding vocabulary size.
As shown in Figure 4(a), the entropy of unigrams
in Du (both stimuli and responses) is consistently
lower than in Df.6 On the other hand, both stimuli
and responses exhibit higher uncertainty in bigram
entropy (Figure 4(b)) and trigram entropy (Figure
4(c)). That is, when no contexts are provided, word
choices (from similarly-sized vocabularies) in Df is
more evenly distributed than in Du; but once the
proceeding words are given, the next word is more
predictable in Df than in Du. We postulate that
the difference in unigram entropy could be due to
(a) more balanced topic coverage in Df vs. more
skewed topic coverage in Du, or (b) professional re-
porters mastering a more balanced use of the vocab-
ulary. If (b) is the main reason, however, the lower
trigram entropy in Df would seem unexpected —
shouldn’t professional journalists also have a more
balanced use of different phrases? Upon further
contemplation, what we hypothesized earlier could
be true: professional writers use the “proper” En-
glish expected in news coverage, which could limit
</bodyText>
<footnote confidence="0.880792571428571">
5We note that this does not guarantee the exact same topic
distribution as in the comment data.
6For reference, Shannon (1951) estimated the entropy of En-
glish to be 11.82 bits per word, due to an incorrect calculation
of a 8727-word vocabulary given Zipf distribution. The correct
number should be 9.27 bits per word for a vocabulary size of
12,366 (Yavuz, 1978).
</footnote>
<figure confidence="0.992745444444444">
10000 100000 1e+06
vocabulary size
(a) Entropy of unigrams
10000 100000 1e+06
vocabulary size
(b) Bigram entropy (F2)
10000 100000 1e+06
vocabulary size
(c) Trigram entropy (F3)
</figure>
<figureCaption confidence="0.999988">
Figure 4: Entropy of unigrams and N-gram entropy.
</figureCaption>
<bodyText confidence="0.999909777777778">
their trigram uncertainties; on the other hand, users
are not bound by conventions (or even grammars),
which could lead to higher variations.
Interestingly, distributions in the stimulus dataset
are closer to news articles: they have a higher uni-
gram entropy than responses, but a lower trigram en-
tropy at comparable vocabulary sizes. In particular,
recall from Section 4 that our comments dataset con-
tains roughly 237K repliers and 357K original com-
</bodyText>
<figure confidence="0.993942275">
10.5
9.5
11
10
9
brown corpus
news articles
stimulus
response
conditional entropy
4.5
7.5
6.5
5.5
4
7
6
5
brown corpus
news article
stimulus
response
conditional entropy
4.5
3.5
2.5
1.5
4
5
3
2
1
brown corpus
news article
stimulus
response
entropy
1497
10000 100000 1e+06
vocabulary size
</figure>
<figureCaption confidence="0.9996055">
Figure 5: Predicting the next word in responses: bigram
entropy vs. bigram+stimulus entropy vs. trigram entropy.
</figureCaption>
<bodyText confidence="0.9999425">
menters. If higher trigram entropy is due to variance
among different users, the stimulus dataset should
have had a higher trigram entropy. We leave an ex-
planation of this interesting behavior as future work.
</bodyText>
<subsectionHeader confidence="0.938508">
6.2 Information in stimuli
</subsectionHeader>
<bodyText confidence="0.9999304">
We now examine the next question: does knowing
words in the stimulus further reduce the uncertainty
of the next word in the response? For simplicity,
we model the stimulus as a collection of unigrams.
Consider the following conditional entropy:
</bodyText>
<equation confidence="0.8494575">
1: GN = − p(bi, j, sk) lo92 p(j  |bi, sk)
i,k,j
</equation>
<bodyText confidence="0.99958176">
where bi is a block of N − 1 tokens in a response
r, j is an arbitrary token following bi, and sk is an
arbitrary token in the corresponding stimulus s for r.
Note that for each bi, we consider every token in the
corresponding s. That is, a (stimulus, response) pair
with m and n tokens respectively generates m*(n−
N + 1) observations of (bi, j, sk) tuples. We refer to
this as the N-gram+stimulus entropy. If knowing sk
in addition to bi does not provide extra information,
then p(j  |bi, sk) = p(j  |bi), and GN = FN.
Figure 5 plots GN for N = 2. Interestingly, we
observe F2 &gt; G2 &gt; F3 (this trend holds for larger
values of N, omitted here for clarity). That is, know-
ing both the preceding N − 1 tokens and tokens in
the stimulus results lowers the uncertainty over the
next token in response (bigram+stimulus entropy &lt;
bigram entropy); on the other hand, this is not as ef-
fective as knowing one more token in the preceding
block (trigram entropy &lt; bigram+stimulus entropy).
Note that from the model size perspective, mod-
eling p(j  |bi, sk) as in GN would have been much
more expensive than p(j  |bi) in FN+1. Take the
case of G2 vs. F3. Let V be the vocabulary of
user comments (ignore for now differences in re-
sponses and stimuli). While both seem to require
computations over V x V x V , the number of
unique observed (bi, j, sk) tuples for G2 (i.e., num-
ber of unique bigrams in responses paired up with
unigrams in corresponding stimuli) is 725,458,892,
whereas the number of unique observed (bi, j) pairs
for F3 (i.e., number of unique trigrams) is only
14,692,952. This means modeling trigrams would
result in a model 2% the size of bigram+stimulus,
yet it could achieve better reduction in uncertainty.
Note that in order to reduce model complex-
ity, the models proposed in Section 3 all broke
down P(ri+1  |s, r1..i) into independent components
P(ri+1  |s) and P(ri+1  |r1..i), rather than model-
ing the effect of s and r1..i jointly as the underlying
model corresponding to GN. Indeed, it would have
been impractical to model p(j  |bi, sk) directly. Our
studies confirmed the validity of this choice: even if
we look at the performance on the training data itself
(i.e., igoring data sparseness issues), the smaller tri-
gram model would have yielded better results than
the significantly more expensive bigram+stimulus
model. Still, since GN shows a consistent improve-
ment over FN, there could be more information in
the stimulus that we are not yet fully utilizing, which
can be interesting future work.
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999860625">
In this paper, we examined a novel application: au-
tomatic response completion in conversational set-
tings. We investigated the effectiveness of several
models that incorporate contextual information pro-
vided by the partially typed response as well as the
stimulus. We found that the partially typed response
provides strong signals. In addition, using a mix-
ture model which also incorporates stimulus content
yielded the best overall result. We also performed
empirical studies to examine the predictability of
user-generated content. Our analysis (entropy es-
timates along with upper-bound numbers observed
from experiments) suggest that there can be interest-
ing future work to explore the contextual informa-
tion provided by the stimulus more effectively and
further improve the response completion task.
</bodyText>
<figure confidence="0.988957384615385">
4
2
8
7
6
5
3
0
1
bigram (F2)
bigram+stimulus (G2)
trigram (F3)
conditional entropy
</figure>
<page confidence="0.975649">
1498
</page>
<sectionHeader confidence="0.997029" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901588888889">
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL’05.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,
Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An
estimate of an upper bound for the entropy of English.
Comput. Linguist., 18:31–40.
Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.
2003. Getting more mileage from web text sources for
conversational speech language modeling using class-
dependent mixtures. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology: companion volume of the Proceed-
ings of HLT-NAACL 2003–short papers - Volume 2,
NAACL-Short ’03, pages 7–9.
Thomas M. Cover and Roger C. King. 1978. A con-
vergent gambling estimate of the entropy of English.
IEEE Transactions on Information Theory, 24:413–
421.
Steve Farmer, Richard Sproat, and Michael Witzel. 2004.
The collapse of the Indus-script thesis: The myth of a
literate Harappan civilization. Electronic Journal of
Vedic Studies, 11:379–423 and 623–656.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of the Human Computer In-
terfaces International (HCII).
Christina L. James and Kelly M. Reischel. 2001. Text
input for mobile devices: comparing model prediction
to actual performance. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
CHI ’01, pages 365–371, New York, NY, USA. ACM.
Henry Kucera and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press.
I. Scott MacKenzie and R. William Soukoreff. 2002.
Text entry for mobile computing: Models and meth-
ods,theory and practice. Human-Computer Interac-
tion, 17(2-3):147–198.
Qiaozhu Mei and Kenneth Church. 2008. Entropy of
search logs: how hard is search? with personalization?
with backoff? In Proceedings of the international con-
ference on Web search and web data mining, WSDM
’8, pages 45–54.
Hamid Moradi, Jerzy W. Grzymala-busse, and James A.
Roberts. 1998. Entropy of english text: experiments
with humans and a machine learning system based on
rough sets. Information Sciences, 104:31–47.
Petteri Nurmi, Andreas Forsblom, Patrik Flor´een, Peter
Peltonen, and Petri Saarikko. 2009. Predictive text
input in a mobile shopping assistant: methods and in-
terface design. In Proceedings of the 14th interna-
tional conference on Intelligent user interfaces, IUI
’9, pages 435–438, New York, NY, USA. ACM.
Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia,
Hrishikesh Joglekar, R. Adhikari, and Iravatham Ma-
hadevan. 2009. Entropic evidence for linguistic struc-
ture in the Indus script. Science.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 583–593.
Ronald Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379–423 and 623–656.
Claude E. Shannon. 1951. Prediction and entropy
of printed English. Bell System Technical Journal,
30:50–64.
W. J. Teahan and John G. Cleary. 1996. The entropy of
English using PPM-based models. In In Data Com-
pression Conference, pages 53–62. IEEE Computer
Society Press.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36–45.
D. Yavuz. 1978. Zipf’s law and entropy (Corresp.).
IEEE Transactions on Information Theory, 20:650.
Xing Yi and James Allan. 2009. A comparative study
of utilizing topic models for information retrieval. In
Proceedings of the European Conference on IR Re-
search on Advances in Information Retrieval, pages
29–41.
</reference>
<page confidence="0.997593">
1499
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884613">
<title confidence="0.9965875">Revisiting the Predictability of Response Completion in Social Media</title>
<author confidence="0.999135">Bo Pang Sujith Ravi</author>
<affiliation confidence="0.998466">Yahoo! Research</affiliation>
<address confidence="0.990257">4401 Great America Santa Clara, CA 95054, USA</address>
<email confidence="0.994118">bopang42@gmail.comsujithravi@yahoo.com</email>
<abstract confidence="0.995889454545455">The question “how predictable is English?” has long fascinated researchers. While prior work has focused on formal English typically used in news articles, we turn to texts generated by users in online settings that are more informal in nature. We are motivated by a novel application scenario: given the difficulty of typing on mobile devices, can we help reduce typing effort with message completion, especially in conversational settings? We propose a method for automatic response completion. Our approach models both the language used in responses and the specific context provided by the original message. Our experimental results on a large-scale dataset show that both components help reduce typing effort. We also perform an information-theoretic study in this setting and examine the entropy of user-generated content, especially in conversational scenarios, to better understand predictability of user generated English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL’05.</booktitle>
<contexts>
<context position="14764" citStr="Barzilay and Lapata, 2005" startWordPosition="2449" endWordPosition="2452">o a distribution over s (P(w |s)), and with probability 1 − As, we generate a word using the response language model: P(ri+1 |s, r1..i) = As - P(ri+1 |(S) + (1 − As) - P(ri+1 |r1..i) (2) We examine two concrete ways of exploiting the context provided by s. Model 1 — LM + Selection model First, we examine a very simple instantiation of P(w |s) where we select a token in s uniformly at random. This is based on the intuition that to be semantically coherent, a reply often needs to repeat certain content words in the stimulus. (Similar intuition has been explored in the context of text coherency (Barzilay and Lapata, 2005).) This is particularly useful for words that are less frequently used: they may not be able to receive enough statistics to be promoted otherwise. More specifically, 1ri+1∈s P(ri+1 |s) = |s| We can take As to be a constant Aselect, which can be estimated in the training data as the probability of a response token being a repetition of a token in the corresponding stimulus. Model 2 — LM + Topic model Another way to incorporate information provided in s is to use it to constrict the topic in r. We can learn a topic model over conversations in the training data using Latent Dirchlet Allocation (</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="15388" citStr="Blei et al., 2003" startWordPosition="2560" endWordPosition="2563">s is particularly useful for words that are less frequently used: they may not be able to receive enough statistics to be promoted otherwise. More specifically, 1ri+1∈s P(ri+1 |s) = |s| We can take As to be a constant Aselect, which can be estimated in the training data as the probability of a response token being a repetition of a token in the corresponding stimulus. Model 2 — LM + Topic model Another way to incorporate information provided in s is to use it to constrict the topic in r. We can learn a topic model over conversations in the training data using Latent Dirchlet Allocation (LDA) (Blei et al., 2003). At test time, we identify the most likely topic of the conversation based on s, and expect ri+1 to be generated from this topic. That is, P(ri+1 |s) = P(ri+1 |t∗) where t∗ = argmaxtP(topic = t |s) More specifically, we first train a topic model on (s, r) pairs from the training data. Given a new stimulus s, we then select the highest ranked topic as being representative of s. Note that alternatively we could consider all possible topic assignments; in that case we would have had to sum probabilities over all topics, and that could also introduce noise. A similar strategy has been previously </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
<author>Stephen A Della Pietra</author>
<author>Jennifer C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<pages>18--31</pages>
<marker>Brown, Pietra, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of English. Comput. Linguist., 18:31–40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
<author>Andreas Stolcke</author>
</authors>
<title>Getting more mileage from web text sources for conversational speech language modeling using classdependent mixtures.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short papers -</booktitle>
<volume>2</volume>
<pages>7--9</pages>
<contexts>
<context position="7967" citStr="Bulyko et al., 2003" startWordPosition="1284" endWordPosition="1287">arrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) compu</context>
</contexts>
<marker>Bulyko, Ostendorf, Stolcke, 2003</marker>
<rawString>Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke. 2003. Getting more mileage from web text sources for conversational speech language modeling using classdependent mixtures. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short papers - Volume 2, NAACL-Short ’03, pages 7–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Roger C King</author>
</authors>
<title>A convergent gambling estimate of the entropy of English.</title>
<date>1978</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>24</volume>
<pages>421</pages>
<contexts>
<context position="8525" citStr="Cover and King, 1978" startWordPosition="1376" endWordPosition="1379">tched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generate</context>
</contexts>
<marker>Cover, King, 1978</marker>
<rawString>Thomas M. Cover and Roger C. King. 1978. A convergent gambling estimate of the entropy of English. IEEE Transactions on Information Theory, 24:413– 421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Farmer</author>
<author>Richard Sproat</author>
<author>Michael Witzel</author>
</authors>
<title>The collapse of the Indus-script thesis: The myth of a literate Harappan civilization.</title>
<date>2004</date>
<journal>Electronic Journal of Vedic Studies,</journal>
<volume>11</volume>
<pages>623--656</pages>
<contexts>
<context position="9167" citStr="Farmer et al., 2004" startWordPosition="1485" endWordPosition="1488"> et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generated some controversies (Farmer et al., 2004). In contrast, our work departs from traditional scenarios significantly. We perform entropy studies over texts generated in online settings which are more informal in nature. Additionally, we utilize the properties of language predictability within a novel application for automatically completing responses in conversational settings. Also, in our case we do not have to worry about issues like “is this a language or not?” because we work with real English news data which include articles written by professional editors and comments generated by users reading those articles. 3 Model In this sec</context>
</contexts>
<marker>Farmer, Sproat, Witzel, 2004</marker>
<rawString>Steve Farmer, Richard Sproat, and Michael Witzel. 2004. The collapse of the Indus-script thesis: The myth of a literate Harappan civilization. Electronic Journal of Vedic Studies, 11:379–423 and 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Computer Interfaces International (HCII).</booktitle>
<contexts>
<context position="7206" citStr="How and Kan, 2005" startWordPosition="1165" endWordPosition="1168">ated work There has been previous work in the area of humancomputer interaction that examined text entry for mobile devices (MacKenzie and Soukoreff, 2002). In particular, one line of work looked into predictive text input, which examined input effort reduction by language prediction. Previous work in predictive text input had very different focus from our study. Oftentimes, the focus was to model actual typing efforts using mobile device keypads, examine the speed and cognitive load of different input methods, and evaluate with emprical user studies in lab settings (James and Reischel, 2001; How and Kan, 2005), where the underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted to narrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language</context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of the Human Computer Interfaces International (HCII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L James</author>
<author>Kelly M Reischel</author>
</authors>
<title>Text input for mobile devices: comparing model prediction to actual performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’01,</booktitle>
<pages>365--371</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7186" citStr="James and Reischel, 2001" startWordPosition="1161" endWordPosition="1164">r-generated content. 2 Related work There has been previous work in the area of humancomputer interaction that examined text entry for mobile devices (MacKenzie and Soukoreff, 2002). In particular, one line of work looked into predictive text input, which examined input effort reduction by language prediction. Previous work in predictive text input had very different focus from our study. Oftentimes, the focus was to model actual typing efforts using mobile device keypads, examine the speed and cognitive load of different input methods, and evaluate with emprical user studies in lab settings (James and Reischel, 2001; How and Kan, 2005), where the underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted to narrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous </context>
</contexts>
<marker>James, Reischel, 2001</marker>
<rawString>Christina L. James and Kelly M. Reischel. 2001. Text input for mobile devices: comparing model prediction to actual performance. In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’01, pages 365–371, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kucera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational analysis of present-day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<contexts>
<context position="32259" citStr="Kucera and Francis, 1967" startWordPosition="5486" endWordPosition="5489">m D,,, then even if we do observe differences in entropy, it could be due to topical differences. A standard mixed-topic dataset like 4Note that our findings are not to be interpreted as prediction performance over unseen texts. For that, one needs to compute cross-entropy between training and test corpora. Since Section 5 is already addressing this question with proper training / test split, in this section, we focus on the variability of language usage in a corpus. This also avoids having to control for “comparable” training/test splits in different types of datasets. 1496 the Brown Corpus (Kucera and Francis, 1967) may not be ideal in this sense (e.g., it contains fiction categories such as “Romance and Love Story”, which may not be represented in our Du). Instead, we obtained a sample of news articles on Yahoo! News during March - May, 2011, and extracted unique sentences from these articles. This yields a Df with more comparable subject matters to Du.5 Next, we compare both the entropy over unigrams and N-gram entropy in three datasets: the news article dataset described above, and comments data (Section 4) separated into stimuli and responses. We also report corresponding numbers computed on the Brow</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Henry Kucera and W. Nelson Francis. 1967. Computational analysis of present-day American English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Scott MacKenzie</author>
<author>R William Soukoreff</author>
</authors>
<title>Text entry for mobile computing: Models and methods,theory and practice. Human-Computer Interaction,</title>
<date>2002</date>
<pages>17--2</pages>
<contexts>
<context position="6743" citStr="MacKenzie and Soukoreff, 2002" startWordPosition="1089" endWordPosition="1092">ch models the generic language used in responses, as well as the contextual information provided by the stimulus. We construct a large-scale dataset of usergenerated textual exchanges, and our experimental results show that both components help reduce typing effort. In addition, to better understand predictability of user generated English, we perform an information-theoretic study in this conversational setting to investigate the entropy of user-generated content. 2 Related work There has been previous work in the area of humancomputer interaction that examined text entry for mobile devices (MacKenzie and Soukoreff, 2002). In particular, one line of work looked into predictive text input, which examined input effort reduction by language prediction. Previous work in predictive text input had very different focus from our study. Oftentimes, the focus was to model actual typing efforts using mobile device keypads, examine the speed and cognitive load of different input methods, and evaluate with emprical user studies in lab settings (James and Reischel, 2001; How and Kan, 2005), where the underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted </context>
</contexts>
<marker>MacKenzie, Soukoreff, 2002</marker>
<rawString>I. Scott MacKenzie and R. William Soukoreff. 2002. Text entry for mobile computing: Models and methods,theory and practice. Human-Computer Interaction, 17(2-3):147–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Kenneth Church</author>
</authors>
<title>Entropy of search logs: how hard is search? with personalization? with backoff?</title>
<date>2008</date>
<journal>WSDM</journal>
<booktitle>In Proceedings of the international conference on Web</booktitle>
<volume>8</volume>
<pages>45--54</pages>
<contexts>
<context position="8849" citStr="Mei and Church, 2008" startWordPosition="1435" endWordPosition="1438">). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generated some controversies (Farmer et al., 2004). In contrast, our work departs from traditional scenarios significantly. We perform entropy studies over texts generated in online settings which are more informal in nature. Additionally, we utilize the properties of language predictability within a novel application for automati</context>
</contexts>
<marker>Mei, Church, 2008</marker>
<rawString>Qiaozhu Mei and Kenneth Church. 2008. Entropy of search logs: how hard is search? with personalization? with backoff? In Proceedings of the international conference on Web search and web data mining, WSDM ’8, pages 45–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamid Moradi</author>
<author>Jerzy W Grzymala-busse</author>
<author>James A Roberts</author>
</authors>
<title>Entropy of english text: experiments with humans and a machine learning system based on rough sets. Information Sciences,</title>
<date>1998</date>
<pages>104--31</pages>
<contexts>
<context position="8752" citStr="Moradi et al., 1998" startWordPosition="1417" endWordPosition="1420">language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generated some controversies (Farmer et al., 2004). In contrast, our work departs from traditional scenarios significantly. We perform entropy studies over texts generated in online settings which are more informal in nature. Additiona</context>
</contexts>
<marker>Moradi, Grzymala-busse, Roberts, 1998</marker>
<rawString>Hamid Moradi, Jerzy W. Grzymala-busse, and James A. Roberts. 1998. Entropy of english text: experiments with humans and a machine learning system based on rough sets. Information Sciences, 104:31–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petteri Nurmi</author>
<author>Andreas Forsblom</author>
<author>Patrik Flor´een</author>
<author>Peter Peltonen</author>
<author>Petri Saarikko</author>
</authors>
<title>Predictive text input in a mobile shopping assistant: methods and interface design.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th international conference on Intelligent user interfaces, IUI ’9,</booktitle>
<pages>435--438</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Nurmi, Forsblom, Flor´een, Peltonen, Saarikko, 2009</marker>
<rawString>Petteri Nurmi, Andreas Forsblom, Patrik Flor´een, Peter Peltonen, and Petri Saarikko. 2009. Predictive text input in a mobile shopping assistant: methods and interface design. In Proceedings of the 14th international conference on Intelligent user interfaces, IUI ’9, pages 435–438, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh P N Rao</author>
<author>Nisha Yadav</author>
<author>Mayank N Vahia</author>
<author>Hrishikesh Joglekar</author>
<author>R Adhikari</author>
<author>Iravatham Mahadevan</author>
</authors>
<title>Entropic evidence for linguistic structure in the Indus script.</title>
<date>2009</date>
<publisher>Science.</publisher>
<contexts>
<context position="9008" citStr="Rao et al., 2009" startWordPosition="1462" endWordPosition="1465">ues and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generated some controversies (Farmer et al., 2004). In contrast, our work departs from traditional scenarios significantly. We perform entropy studies over texts generated in online settings which are more informal in nature. Additionally, we utilize the properties of language predictability within a novel application for automatically completing responses in conversational settings. Also, in our case we do not have to worry about issues like “is this a language or not?” because we work</context>
</contexts>
<marker>Rao, Yadav, Vahia, Joglekar, Adhikari, Mahadevan, 2009</marker>
<rawString>Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia, Hrishikesh Joglekar, R. Adhikari, and Iravatham Mahadevan. 2009. Entropic evidence for linguistic structure in the Indus script. Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>William B Dolan</author>
</authors>
<title>Data-driven response generation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>583--593</pages>
<contexts>
<context position="7613" citStr="Ritter et al., 2011" startWordPosition="1230" endWordPosition="1233">al typing efforts using mobile device keypads, examine the speed and cognitive load of different input methods, and evaluate with emprical user studies in lab settings (James and Reischel, 2001; How and Kan, 2005), where the underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted to narrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting wit</context>
<context position="18118" citStr="Ritter et al., 2011" startWordPosition="3031" endWordPosition="3035">f 14 weeks between March and May, 2011. A random sample yielded a total of 1,487,995 exchanges, representing 237,040 unique users posting responses to stimuli comments authored by 357,811 users. In the raw dataset (i.e., before tokenization), stimuli average at 59 tokens (332 characters), and responses average at 26 tokens (144 characters). We took the first 12 weeks of data as training data (1,269,732 exchanges) and the rest 2 weeks of data as test data (218,263 exchanges). 2Note that previous work has used a dataset with 1 million Twitter conversations extracted from a scraping of the site (Ritter et al., 2011), where a status update and its replies in Twitter form “conversations”. This dataset is no longer publicly available. At the time of this writing, we were not able to identify a data source to re-construct a dataset like that. 5 Experiments 5.1 Evaluation measures Recall@k : Here, we follow a standard evaluation strategy used to assess ranking quality in information retrieval applications. For each word, we check if the correct answer is one of the top-k tokens being suggested. We then compute the recall at different values of k. While this is a straight-forward measure to assess the overall </context>
<context position="24438" citStr="Ritter et al., 2011" startWordPosition="4134" endWordPosition="4137">ere co = 1 and noticed that the trigram LM achieves a slightly lower but still significant typing reduction (TypRed score of 9.58 versus 15.10 for the earlier case). The first column of Table 1 (c = 0) compares the performance of other models for this setting. We find that adding a translation model (LM+TM) does not help for this task; in fact, it results in lower scores than using the LM alone. This suggests that a translation-based generative approach may not be suitable, if the goal is to predict text as intended by the user. This is consistent with previous observations on a related task (Ritter et al., 2011), as we � l,,, select 1494 discussed in Section 1. In contrast, the mixture models do much better. In fact, LM+Selection model produces better results than trigram LM alone. We also note that estimating the mixture parameter on the training data rather than using a fixed value increases TypRed scores: 14.02 with a fixed Aselect = 0.5 versus 15.18 with A* select = 0.09. This comparison also holds for c &gt; 0 — that is, a naive version of LM+Selection that selects a word from the stimulus whenever the prefix allows would not have worked well. In principle the LM+Topic model is potentially more pow</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2011</marker>
<rawString>Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583–593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<contexts>
<context position="7833" citStr="Rosenfeld, 2000" startWordPosition="1264" endWordPosition="1265"> underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted to narrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as wel</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="8199" citStr="Shannon, 1948" startWordPosition="1325" endWordPosition="1326">n task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy i</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423 and 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>Prediction and entropy of printed English.</title>
<date>1951</date>
<journal>Bell System Technical Journal,</journal>
<pages>30--50</pages>
<contexts>
<context position="1571" citStr="Shannon, 1951" startWordPosition="246" endWordPosition="247">rmation-theoretic study in this setting and examine the entropy of user-generated content, especially in conversational scenarios, to better understand predictability of user generated English. 1 Introduction How predictable is language? As early as 1951, long before large quantities of texts (or the means to process them) were easily available, Shannon had raised this question and proceeded to answer it with a set of clever analytical estimations. He studied the predictability of printed English, or “how well can the next letter of a text be predicted when the preceding N letters are known” (Shannon, 1951). This was quantified as the conditional entropy, which measures the amount of information conveyed from statistics over the preceding context. In this paper, we discuss a novel application setting which mirrors the predictability study as defined by Shannon. (a) Google (b) Amazon (c) Netflix Figure 1: Query completion as users type into the “Search using Google” box on a browser, as well as the search box in Amazon and Netflix. Text completion for user-generated texts: Consider a user who is chatting with her contact or posting to a social media site using a mobile device. If we can predict t</context>
<context position="4218" citStr="Shannon (1951)" startWordPosition="677" endWordPosition="678">losed database of available objects, and ranking can be done by overall popularity. In contrast, our scenario requires generation of unseen texts. Given the difficulty of generating full-length text, we consider a more realistic setting, where we perform completion on a word-by-word basis. Each time, we propose candidate completions at the wordlevel when the user is about to start a new word, or has partially entered the first few letters; once this word is successfully completed, we move on to the next one. This predict-verify-predict process exactly mirrors the human experiment described by Shannon (1951), except we do this at the word-level rather than the letter-level: having the user examine and verify predictions at the letter level would not be a practical solution for the intended application. The response completion task: In addition, our task has another interesting difference from Shannon’s human experiment. Consider the mobiledevice user mentioned previously. If the user is replying to a piece of text (e.g., an instant message sent by a contact), we have an additional source of contextual information in the stimulus, or the text which triggered the response that the user is trying to</context>
<context position="8229" citStr="Shannon (1951)" startWordPosition="1329" endWordPosition="1330">lso considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. 1490 Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (M</context>
<context position="29052" citStr="Shannon (1951)" startWordPosition="4953" endWordPosition="4954">ting, Remember, Excuse, WAKE, Hooray, Birth, mon, Yeah, Dumb, Michael, geronimo, ... English as how well can the next token be predicted when the preceding N tokens are known. How much does the immediate context in the response help reduce the uncertainty? How does user-generated content compare with more formal English in this respect? And how about the corresponding stimuli — given the preceding N tokens, does the knowledge of stimulus further reduce the uncertainty? These questions motivated a series of studies over entropy in different datasets.4 6.1 Comparison of N-gram entropy Following Shannon (1951), we consider the following function FN, which can be called the N-gram entropy, as the measure of predictability: Table 4: Examples of top representative words for a few �FN = − p(bi,j) lo92 p(j |bi) topics generated by the LDA topic model trained on news i,j comment data. be useful for both Tweet-like short messages as well as more lengthy exchanges in detailed discussions. 5.4 Discussion Table 3 displays some sample translations learned using the TM model described in Section 3.3. Interestingly, emoticons and informal expressions like :) or lmao in the stimulus tend to evoke similar type of</context>
<context position="30552" citStr="Shannon (1951)" startWordPosition="5210" endWordPosition="5211">nslation-based approach is not well suited for this particular task and hence does not perform as well as other methods. Finally, in Table 4, we provide a few sample topics generated by the LDA topic model (which is used by Mixture Model 2 described in Section 3.4). We find that while a few topics display some semantic coherence (e.g., political figures), many of them are noisy (or too generic) which further supports our earlier observation that they are not useful enough to help in the prediction task. 6 Entropy of user comments We adapt the notion of predictability of English as examined by Shannon (1951) from letter-prediction to token-prediction, and define the predictability of where bi is a block of N −1 tokens, j is an arbitrary token following bi, and p(j |bi) is the conditional probability of j given bi. This conditional entropy reflects how much is the uncertainty of the next token reduced by knowing the preceding N−1 tokens. Under this measure, is user-generated content more predictable or less predictable than the more formal “printed” English examined by Shannon? Maybe it is more predictable, since most users in informal settings use simpler English, which may contain fewer variatio</context>
<context position="34380" citStr="Shannon (1951)" startWordPosition="5841" endWordPosition="5842">alanced topic coverage in Df vs. more skewed topic coverage in Du, or (b) professional reporters mastering a more balanced use of the vocabulary. If (b) is the main reason, however, the lower trigram entropy in Df would seem unexpected — shouldn’t professional journalists also have a more balanced use of different phrases? Upon further contemplation, what we hypothesized earlier could be true: professional writers use the “proper” English expected in news coverage, which could limit 5We note that this does not guarantee the exact same topic distribution as in the comment data. 6For reference, Shannon (1951) estimated the entropy of English to be 11.82 bits per word, due to an incorrect calculation of a 8727-word vocabulary given Zipf distribution. The correct number should be 9.27 bits per word for a vocabulary size of 12,366 (Yavuz, 1978). 10000 100000 1e+06 vocabulary size (a) Entropy of unigrams 10000 100000 1e+06 vocabulary size (b) Bigram entropy (F2) 10000 100000 1e+06 vocabulary size (c) Trigram entropy (F3) Figure 4: Entropy of unigrams and N-gram entropy. their trigram uncertainties; on the other hand, users are not bound by conventions (or even grammars), which could lead to higher var</context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>Claude E. Shannon. 1951. Prediction and entropy of printed English. Bell System Technical Journal, 30:50–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>John G Cleary</author>
</authors>
<title>The entropy of English using PPM-based models.</title>
<date>1996</date>
<booktitle>In In Data Compression Conference,</booktitle>
<pages>53--62</pages>
<publisher>IEEE Computer Society Press.</publisher>
<contexts>
<context position="8730" citStr="Teahan and Cleary, 1996" startWordPosition="1412" endWordPosition="1416">vious work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed an upper bound for the entropy of English with a trigram model, using the Brown corpus. Some other related works on this topic include (Teahan and Cleary, 1996; Moradi et al., 1998). There was also a recent study using entropy in the context of Web search (Mei and Church, 2008). In other settings, entropy has also been employed as a tool for studying the linguistic properties of ancient scripts (e.g., Indus Script) (Rao et al., 2009). While this seems like an interesting application of information theory for linguistic studies, it has also generated some controversies (Farmer et al., 2004). In contrast, our work departs from traditional scenarios significantly. We perform entropy studies over texts generated in online settings which are more informa</context>
</contexts>
<marker>Teahan, Cleary, 1996</marker>
<rawString>W. J. Teahan and John G. Cleary. 1996. The entropy of English using PPM-based models. In In Data Compression Conference, pages 53–62. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>Eliza: a computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Commun. ACM,</journal>
<pages>9--36</pages>
<contexts>
<context position="5209" citStr="Weizenbaum, 1966" startWordPosition="839" endWordPosition="841"> the user is replying to a piece of text (e.g., an instant message sent by a contact), we have an additional source of contextual information in the stimulus, or the text which triggered the response that the user is trying to type. Can we learn from previously observed stimulus-response pairs (which we will refer to as exchanges)? That is, can we take advantage of this conversational setting and effectively use the information provided by stimulus to better predict the next word in the response? We refer to this task as the response completion task. Our task is different from “chatter-bots” (Weizenbaum, 1966), where the goal is to generate a response to an input that would resemble a human conversation partner. Instead, we want to complete a response as the replier intends to. Recently, Ritter et. al (2011) experimented with automatic response generation in social media. They had a similar conversational setting, but instead of completion based on partial input, they attempted to generate a response in its entirety given only the stimulus. While many of the generated responses are deemed possible replies to the stimulus, they have a low chance of actually matching the real response given by the us</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Joseph Weizenbaum. 1966. Eliza: a computer program for the study of natural language communication between man and machine. Commun. ACM, 9:36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yavuz</author>
</authors>
<title>Zipf’s law and entropy (Corresp.).</title>
<date>1978</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>20--650</pages>
<contexts>
<context position="34617" citStr="Yavuz, 1978" startWordPosition="5882" endWordPosition="5883">houldn’t professional journalists also have a more balanced use of different phrases? Upon further contemplation, what we hypothesized earlier could be true: professional writers use the “proper” English expected in news coverage, which could limit 5We note that this does not guarantee the exact same topic distribution as in the comment data. 6For reference, Shannon (1951) estimated the entropy of English to be 11.82 bits per word, due to an incorrect calculation of a 8727-word vocabulary given Zipf distribution. The correct number should be 9.27 bits per word for a vocabulary size of 12,366 (Yavuz, 1978). 10000 100000 1e+06 vocabulary size (a) Entropy of unigrams 10000 100000 1e+06 vocabulary size (b) Bigram entropy (F2) 10000 100000 1e+06 vocabulary size (c) Trigram entropy (F3) Figure 4: Entropy of unigrams and N-gram entropy. their trigram uncertainties; on the other hand, users are not bound by conventions (or even grammars), which could lead to higher variations. Interestingly, distributions in the stimulus dataset are closer to news articles: they have a higher unigram entropy than responses, but a lower trigram entropy at comparable vocabulary sizes. In particular, recall from Section </context>
</contexts>
<marker>Yavuz, 1978</marker>
<rawString>D. Yavuz. 1978. Zipf’s law and entropy (Corresp.). IEEE Transactions on Information Theory, 20:650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Yi</author>
<author>James Allan</author>
</authors>
<title>A comparative study of utilizing topic models for information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Conference on IR Research on Advances in Information Retrieval,</booktitle>
<pages>29--41</pages>
<contexts>
<context position="16147" citStr="Yi and Allan, 2009" startWordPosition="2692" endWordPosition="2695">P(ri+1 |s) = P(ri+1 |t∗) where t∗ = argmaxtP(topic = t |s) More specifically, we first train a topic model on (s, r) pairs from the training data. Given a new stimulus s, we then select the highest ranked topic as being representative of s. Note that alternatively we could consider all possible topic assignments; in that case we would have had to sum probabilities over all topics, and that could also introduce noise. A similar strategy has been previously employed for 1492 other topic modeling applications in information retrieval, where documents are smoothed with their highest ranked topic (Yi and Allan, 2009). We lower the weight As if P(t* |s) is low. That is, we use As = Atopic * P(t* |s) in Eq. 2. 4 Data In order to investigate text completion in a conversational setting, we need to construct a large-scale dataset with textual exchanges among users. An ideal dataset would have been a collection of instant messages, but these type of datasets are difficult to obtain given privacy concerns. To the best of our knowledge, existing SMS (short message service) datasets only contain isolated text spans and do not provide enough information to reconstruct the conversations. There are, however, a high v</context>
</contexts>
<marker>Yi, Allan, 2009</marker>
<rawString>Xing Yi and James Allan. 2009. A comparative study of utilizing topic models for information retrieval. In Proceedings of the European Conference on IR Research on Advances in Information Retrieval, pages 29–41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
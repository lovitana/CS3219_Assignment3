<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.947794">
Answering Opinion Questions on Products by Exploiting Hierarchical
Organization of Consumer Reviews
</title>
<author confidence="0.998439">
Jianxing Yu, Zheng-Jun Zha, Tat-Seng Chua
</author>
<affiliation confidence="0.999627">
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.98965">
{jianxing, zhazj, chuats}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.99664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966714285714">
This paper proposes to generate appropriate
answers for opinion questions about prod-
ucts by exploiting the hierarchical organiza-
tion of consumer reviews. The hierarchy orga-
nizes product aspects as nodes following their
parent-child relations. For each aspect, the re-
views and corresponding opinions on this as-
pect are stored. We develop a new framework
for opinion Questions Answering, which en-
ables accurate question analysis and effective
answer generation by making use the hierar-
chy. In particular, we first identify the (ex-
plicit/implicit) product aspects asked in the
questions and their sub-aspects by referring
to the hierarchy. We then retrieve the corre-
sponding review fragments relevant to the as-
pects from the hierarchy. In order to gener-
ate appropriate answers from the review frag-
ments, we develop a multi-criteria optimiza-
tion approach for answer generation by simul-
taneously taking into account review salience,
coherence, diversity, and parent-child rela-
tions among the aspects. We conduct eval-
uations on 11 popular products in four do-
mains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these
products. Experimental results demonstrate
the effectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999777">
With the rapid development of E-commerce, most
retail websites encourage consumers to post reviews
to express their opinions on the products. For exam-
ple, the review “The battery of Nokia N95 is amaz-
ing.” reveals positive opinion on the aspect “bat-
</bodyText>
<figureCaption confidence="0.999519">
Figure 1: Overview of product opinion-QA framework
</figureCaption>
<bodyText confidence="0.999637">
tery” of product Nokia N95. An aspect here refers
to a component or an attribute of a certain prod-
uct. Numerous consumer reviews are now available
online, and these reviews contain rich opinionated
information on various aspects of products. They
are naturally a valuable resource for answering opin-
ion questions about products, such as “How do peo-
ple think about the battery of Nokia N95?” Opin-
ion Question Answering (opinion-QA) on products
seeks to uncover consumers’ thinking and feeling
about the products or aspects of products. It is dif-
ferent from traditional factual QA, where the ques-
tions ask for the fact, such as “Where is the capital
of United States?” and the answer is “Washington,
D.C.”
For a product opinionated question, the answer
should not be just a best answer. It should reflect the
opinions of various segments of users, and incorpo-
</bodyText>
<page confidence="0.979615">
391
</page>
<note confidence="0.8766225">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 391–401, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998152">
rate both positive and negative viewpoints. Hence
the answer should be a summarization of public
opinions and comments on the product or specific
aspect asked in the question (Jiang et al., 2010).
In addition, it should also include public opinions
and comments on the sub-aspects. Such answers
would help users to understand the inherent reasons
of the opinions on the asked aspect. For exam-
ple, the question “What do people think the cam-
era of Nokia 5800?” asks for public positive and
negative opinions on the aspect “camera” of prod-
uct “Nokia 5800.” The summarization of opinions
on the sub-aspects such as “lens” and “resolution”
would help users better understand that the public
complaints on the aspect “camera” are due to the
poor “lens” and/or low “resolution.” Moreover, the
answer should be presented following the general-
to-specific logic, i.e., from general aspects to spe-
cific sub-aspects. This makes the answer easier to
understand by the users (Ouyang et al., 2009).
Current Opinion-QA methods mainly include
three components, including question analysis that
identifies aspects and opinions asked in the ques-
tions, answer fragment retrieval, and answer gen-
eration which summarizes the retrieved fragments
(Lloret et al., 2011). Although existing methods
show encouraging performance, they are usually not
able to generate satisfactory answers due to the fol-
lowing drawbacks. First, current methods often
identify aspects as the noun phrases in the questions.
However, noun phrases contain noises that are not
aspects. This gives rise to imprecise aspect identifi-
cation. For example, in the question “What reasons
can I persuade my wife that people prefer the battery
of Nokia N95?” noun phrases “wife” and “people”
are not aspects. Moreover, current methods relied
on noun phrases are not able to reveal the implicit
aspects, which are not explicitly asked in the ques-
tions. For example, the question “Is iPhone 4 expen-
sive?” asks about the aspect “price”, but the term
“price” does not appear in the question. Second,
current methods cannot discover sub-aspects of the
asked aspect due to its ignorance of parent-child re-
lations among aspects. Third, the answers generated
by the existing methods do not follow the general-to-
specific logic, leading to difficulty in understanding
the answers.
To overcome these problems, we can resort to
the hierarchical organization of consumer reviews
on products. As illustrated in Figure 2, the hier-
archy organizes product aspects as nodes, follow-
ing their parent-child relations. For each aspect, the
reviews and corresponding opinions on this aspect
are stored. Such hierarchy can naturally facilitate to
identify aspects asked in questions. While explicit
aspects can be recognized by referring to the hier-
archy, implicit aspects can be inferred based on the
associations between sentiment terms and aspects in
the hierarchy (Yu et al., 2011). The sentiment terms
are discovered from the reviews on corresponding
aspects. Moreover, by following the parent-child re-
lations in the hierarchy, sub-aspects of the asked as-
pect can be directly acquired, and the answers can
present aspects from general to specific.
Motivated by the above observations, we propose
to exploit the hierarchical organization of consumer
reviews for product opinion-QA. As illustrated in
Figure 1, our framework first organizes consumer
reviews of a certain product into a hierarchical or-
ganization. The resulting hierarchy is in turn used
to help question analysis and relevant review frag-
ments retrieval. In order to generate appropriate
answers from the retrieved fragments, we develop
a multi-criteria optimization approach by simulta-
neously taking into account review salience, co-
herence, and diversity. The parent-child relations
among aspects are also incorporated into the ap-
proach to ensure the answers be general-to-specific.
We conduct evaluations on 11 popular products in
four domains. The evaluated corpus contains 70,359
consumer reviews and 220 questions on these prod-
ucts. More details of the dataset are discussed in
Section 4. Experimental results to demonstrate the
effectiveness of our approach.
The main contributions of this paper include,
</bodyText>
<listItem confidence="0.9753426">
• We propose to exploit the hierarchical organi-
zation of consumer reviews for answering opin-
ion questions on products.
• With the help of the hierarchy, our pro-
posed framework can accurately identify (ex-
plicit/implicit) aspects asked in the questions,
and the corresponding sub-aspects.
• We develop a multi-criteria optimization ap-
proach to generate informative, coherent, di-
verse and general-to-specific answers.
</listItem>
<page confidence="0.99808">
392
</page>
<figureCaption confidence="0.998858">
Figure 2: Hierarchical organization for Nokia N95
</figureCaption>
<bodyText confidence="0.999866875">
The rest of this paper is organized as follows. Sec-
tion 2 introduces the components of hierarchical or-
ganization of reviews, question analysis, and answer
fragment retrieval. Section 3 elaborates the multi-
criteria optimization approach for answer generation
. Section 4 presents experimental details, while Sec-
tion 5 reviews related works. Finally, Section 6 con-
cludes this paper with future works.
</bodyText>
<sectionHeader confidence="0.988638333333333" genericHeader="introduction">
2 Hierarchical Organization, Question
Analysis, and Answer Fragment
Retrieval
</sectionHeader>
<bodyText confidence="0.999987">
Let R = {r1, · · · , rjj,j} denote a collection of con-
sumer reviews of a certain product. Each review re-
flects consumer opinions on the product and/or prod-
uct aspects. Let q denote an opinion question, which
asks for public opinions on a product or some as-
pects of the product. The task is to retrieve the opin-
ionated review fragments relevant to the asked prod-
uct/product aspects, and summarize these fragments
to form an appropriate answer to question q.
Next, we introduce the components of hierarchi-
cal organization that organizes consumer reviews
into a hierarchy, question analysis which identifies
the products/aspects and opinions asked in the ques-
tions, and answer fragment retrieval that retrieves re-
view fragments relevant to the questions.
</bodyText>
<subsectionHeader confidence="0.991725">
2.1 Hierarchical Organization of Reviews
</subsectionHeader>
<bodyText confidence="0.999968631578947">
We employ the method proposed by Yu et al. (2011)
to organize consumer reviews of a product into a hi-
erarchical organization. As shown in Figure 2, the
hierarchy organizes product aspects as nodes, fol-
lowing their parent-child relations. In particular, this
method first automatically acquires an initial aspect
hierarchy from the domain knowledge and identifies
aspects commented in the reviews. It then incremen-
tally inserts the identified aspects into appropriate
positions in the initial hierarchy, and finally obtains
an aspect hierarchy that allocates all the newly iden-
tified aspects. The consumer reviews are then orga-
nized to their corresponding aspect nodes in the hi-
erarchy. Sentiment classification is then performed
to determine consumer opinions on the reviews.
The reported performance of Yu et al. (2011)
on aspect identification, aspect hierarchy generation
and sentiment classification are 0.731, 0.705, 0.787
in terms of average F1-measure, respectively.
</bodyText>
<subsectionHeader confidence="0.99425">
2.2 Question Analysis and Answer Fragment
Retrieval
</subsectionHeader>
<bodyText confidence="0.999701363636364">
Question analysis consists of five sub-tasks: recog-
nizing product asked in the question; identifying as-
pects in the question; classifying opinions that the
question asks for (the asked opinion could be posi-
tive, negative or both); identifying the question type
(e.g. asking for public opinions, or the reason of
the opinions, etc.); and identifying the question form
(i.e. comparative question or single form question).
Recognizing the product: A name entity recog-
nizer 1 is trained to recognize the product name. In
particular, we collect 420 auxiliary questions from
Yahoo!Answer 2, and manually annotate the prod-
uct names (submitted as supplementary material in
Appendix A). A name entity recognizer for product
is learned on these data, with unigrams and POS tags
as features. Given a testing question, the recognizer
predicts each word as B, I, E or O, where B, I, E de-
note the begin, internal, and end of a product name
respectively, and O corresponds to other words.
Identifying aspects: As aforementioned, simply
extracting the noun phrases as aspects would import
noises. Also, some “implicit” aspects do not ex-
</bodyText>
<footnote confidence="0.9555665">
lhttp://nlp.stanford.edu/software/CRF-NER.shtml
zhttp://answers.yahoo.com
</footnote>
<page confidence="0.999269">
393
</page>
<bodyText confidence="0.99813906122449">
plicitly appear in the reviews. One simple solution
for these problems can resort to the review hierar-
chy. The hierarchy has organized product aspects,
which can be used to filter the noise noun phrases
for accurately identifying the explicit aspects. For
the implicit aspects, we observe they are usually
modified by some peculiar sentiment terms (Su et
al., 2008). For example, the aspect “size” is often
modified by the sentiment terms such as “large”,
but seldom by the terms such as “expensive.” Thus,
there are some associations between the aspects and
sentiment terms. Such associations can be learned
from the hierarchy and leveraged to infer the im-
plicit aspects (Yu et al., 2011). In order to simul-
taneously identify the (explicit/implicit) aspects, we
adopt a hierarchical classification technique. The
technique simultaneously learns to identify explicit
aspects, and discovers the associations between as-
pects and sentiment terms by multiple classifiers. In
particular, given a testing question, we identify its
aspect by hierarchically classify (Silla et al., 2011) it
into the appropriate aspect node of a particular prod-
uct hierarchy. The classification greedily searches a
path in the hierarchy from top to down. The search
begins at the root node, and stops at the leaf node
or a specific node where the relevance score is lower
than a pre-defined threshold. The relevance score on
each node is determined by a SVM classifier. Mul-
tiple SVM classifiers are learned on the hierarchy,
one distinct classifier for a node. The reviews that
are stored in the node and its child-nodes are used
as training samples. We employ the features of noun
terms, and sentiment terms in the sentiment lexicon
provided by MPQA project (Wilson et al., 2005).
Classifying the opinions: Given a set of testing
questions, we first distinguish the opinion questions
from the factual ones (Yu et al., 2003). Since the
opinion questions often contain one or more senti-
ment terms, we classify them by employing the sen-
timent terms in the sentiment lexicon provided from
MPQA project (Wilson et al., 2005). Subsequently,
we learn a SVM sentiment classifier to determine the
opinion polarity of the opinion questions. In partic-
ular, the reviews and corresponding opinions stored
in the hierarchy are used as training samples, which
are represented by the unigram features.
Identifying the question type: Opinion questions
are often categorized into four types (Ku et al.,
2007),
</bodyText>
<listItem confidence="0.996570454545455">
• Attitude question, asking for public opinion on
a product or product aspect, such as “What do
people think iPhone 3gs?”
• Reason question, asking for the reason of pub-
lic opinion on a product or product aspect, such
as “Why do people like iPhone 3gs?”
• Target question, asking for the object in the
public opinion, such as “Which phone is better
than Nokia N95?”
• Yes/No question, asking for whether a state-
ment is correct, such as “Is Nokia N95 bad?”
</listItem>
<bodyText confidence="0.99987306060606">
We formulate the question type identification as a
multi-class classification problem. A multi-class
SVM classifier 3 is learned for the classification. We
collect 420 auxiliary questions from Yahoo!Answer
and manually annotate their types (submitted as sup-
plementary material in Appendix B). These ques-
tions are used for training, with POS tags and ques-
tion words (i.e. why, what, how, do, is) as features.
Identifying the question form: Question form in-
cludes single and comparative. A question is viewed
as comparative if it contains comparative adjectives
and adverbs (e.g. cheaper, etc.), otherwise as the sin-
gle form (Moghaddam et al., 2011). The POS tags
are exploited to detect comparative adjectives (i.e.
tag “JJR”) and adverbs (i.e. tag “RBR”).
After analyzing the question, we retrieve all re-
view sentences on the asked aspect and all its sub-
aspects from a certain product hierarchy, and choose
the ones relevant to the opinion asked in the ques-
tion. For the single form question, we view the
retrieved sentences as the answer fragments. For
the comparative questions, we select comparative
sentences on the compared products from the re-
trieved sentences, and treat them as the answer frag-
ments. Subsequently, question type is used to define
the template for the answers. In particular, for the
questions asking for reason and attitude, we gener-
ate the answers by summarizing corresponding an-
swer fragments. For questions seeking for a target
as the answer, we output the product names based
on the majority voting of the opinions in the re-
trieved answer fragments. For the yes/no questions,
we first generate the “yes/no” answer based on the
</bodyText>
<footnote confidence="0.992192">
3http://svmlight.joachims.org/svm multiclass.html
</footnote>
<page confidence="0.998063">
394
</page>
<bodyText confidence="0.999925333333333">
consistency between the asked opinions and the ma-
jor opinions in the answer fragments, and then sum-
marize these fragments to form the answers.
</bodyText>
<sectionHeader confidence="0.9542" genericHeader="method">
3 Answer Generation
</sectionHeader>
<bodyText confidence="0.999965333333333">
Answer generation aims to generate an appropriate
answer for a given opinion question based on the
retrieved answer fragments, i.e., review sentences.
An answer is essentially a sequence of sentences.
Hence, the task of answer generation is to select sen-
tences from the retrieved answer fragments and or-
der them appropriately. We formulate this task into
a multi-criteria optimization problem. We incorpo-
rate multiple criteria in the answer generation pro-
cess, including answer salience, coherence, and di-
versity. The parent-child relations between aspects
is also incorporated to ensure the answer follow the
general-to-specific logic. In the next subsections, we
will introduce details of the proposed multi-criteria
optimization approach.
</bodyText>
<subsectionHeader confidence="0.997003">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.99982140625">
We first introduce the multiple criteria and then
present the optimization problem.
Salience is used to measure the representative-
ness of the answer. A good answer should consist
of salient review sentences. Let S denote the set
of retrieved sentences. We define a binary variable
si E 10, 11 to indicate the selection of sentence i
for the answer, i.e. si = 1 (or 0) indicates that si is
selected (or not). Let Wi denote the salience of sen-
tence i. The estimation of Wi will be described in
Section 3.2. The salience score of the answer (i.e.,
a set of sentences) is computed by summing up the
scores of all its constituent sentences, as EiES Wisi.
Coherence is used to quantify the readability of
an answer. To make the answer readable, the con-
stituent sentences in the answer should be ordered
properly. That is, the adjacent sentences should
be coherent. We define ei,j E 10, 11 to indicate
whether the sentences i and j are adjacent in the an-
swer; where ei,j = 1 (or 0) means they are (or not)
adjacent. The coherence between two adjacent sen-
tences is measured by cij. The estimation of cij will
be described in Section 3.3. As aforementioned, the
answer is expected to be presented in a general-to-
specific manner, i.e. from general aspects to specific
sub-aspects. We define hi,j in Eq.1 to measure the
general-to-specific coherence of sentences i and j.
where leveli denotes level position of the aspect
commented in sentence i by referring to the hi-
erarchy, with the root level being 0. The coher-
ence score of the answer is computed by sum-
ming up the scores of all its adjacent sentences as,
</bodyText>
<subsubsectionHeader confidence="0.465858">
EjES EiES hi,jci,jei,j.
</subsubsectionHeader>
<bodyText confidence="0.999467818181818">
Diversity. A good answer should diversely cover
all the important information. We introduce a ma-
trix M in Eq.2 to measure the pairwise diversities
among sentences. Mij corresponds to the diversity
between sentences i and j. When sentences i and
j comment on the same aspects, Mij will favor to
select the pair of sentences that discusses on diverse
content (i.e. low similarity). Otherwise, the pair of
sentences commented on different aspects is viewed
to be diverse, and Mij is set as a constant bigger
than one.
</bodyText>
<equation confidence="0.5165">
r 1 − similar(i, j) if i, j commented on same aspect
</equation>
<bodyText confidence="0.9556636">
Sl W otherwise,
where co is a constant 4.
Multi-Criteria Optimization We integrate the
above criteria into the multi-criteria optimization
formulation,
</bodyText>
<equation confidence="0.9871815">
max{A1 • EiES Wisi + A2 • EjES EiES hi,jci,jei,j
+ A3 • EjES EiES siMij;
si, ei j E 10, 11, Vi, j;
A1 + A2 + A3 = 1, 0 :5 A1, A2, A3 :5 1,
</equation>
<bodyText confidence="0.9470185">
where A1, A2, A3 are the trade-off parameters.
We further incorporate the following constrains
into the optimization framework, so as to derive ap-
propriate answers.
</bodyText>
<listItem confidence="0.915269">
• The length of the answer is up to K,
</listItem>
<equation confidence="0.407285">
EiES lisi ≤ K, (4)
</equation>
<bodyText confidence="0.977251">
where li is the length of sentence i.
</bodyText>
<listItem confidence="0.8924655">
• When sentence i is not selected (i.e. si = 0),
the adjacency between any sentence to i is set
</listItem>
<footnote confidence="0.459076">
4Empirically set to 10 in the experiment.
</footnote>
<equation confidence="0.9873115">
�
h e `e9eli−levelj ; if leveli ≠ levelj;
i j, =
(1)
1; otherwise,
Mij =
</equation>
<page confidence="0.987194">
395
</page>
<bodyText confidence="0.9701945">
to zero (i.e. EicS ei,j = EicS ej,i = 0).
When sentence i is selected, there are two sen-
tences adjacent to sentence i in the answer, one
before i and another after i. (i.e. EicS ei,j =
</bodyText>
<equation confidence="0.99770825">
E
icS ej,i = 1).
∑ ∑
iES ei,j = iES ej,i = sj, dj. (5)
</equation>
<listItem confidence="0.438908333333333">
• In order to avoid falling into a cycle in sentence
selection, we employ the following constraints
(Deshpande et al., 2009).
</listItem>
<table confidence="0.5734045">
∑iES ki = n + 1; (6)
EKES fi,n+1 &gt; 1;
KES fi,j − ∑iES fj,i = sj, dj;
0 &lt; fi,j &lt; (n + 1) - ei,j, di, j,
</table>
<bodyText confidence="0.999941166666667">
where the variable fi,j is an integer to number
the selected adjacent sentences from 1 to n+1,
and the first selected sentence is numbered f0,i
= n + 1. If the last selected sentence obtains a
number fi,n+1 which is bigger then 1, then the
selection has no cycle.
</bodyText>
<subsubsectionHeader confidence="0.633254">
Solution
</subsubsectionHeader>
<bodyText confidence="0.982850333333333">
Given the salience weights wi|S i=1, and coherence
weights ci,j|Si,j=1, the above multi-criteria optimiza-
tion problem can be solved by Integer Linear Pro-
gramming (Schrijver et al., 1998). The optimal so-
lutions si|Si=1 and ei,j|S =1 indicate the selected sen-
i,j
tences and the order of them. In the next subsec-
tions, we will introduce the estimations of wi|S i=1
and ci,j|Si,j=1.
</bodyText>
<subsectionHeader confidence="0.999001">
3.2 Salience Weight Estimation
</subsectionHeader>
<bodyText confidence="0.995396458333333">
The salience weight of sentence i is formulated as
wi = EGg=1 cpg(i)/G, where cp(i) denotes the mea-
surement for the importance of sentence i. We de-
fine seven measurements (i.e. G = 7) below.
Helpfulness: Many forum websites provide a
helpfulness score, which is used to rate the quality
of a review. The sentences that come from helpful
reviews are often representative (Mizil et al., 2009).
We compute cp(i) of sentence i by using helpfulness
score from its host review.
Timeliness: The new coming sentence often con-
tains more updated and useful information (Liu et
al., 2008). cp(i) is the post time of sentence i. We
normalize it to [0, 1].
Grammaticality: The grammatical sentence is
often more readable. We employ the method in
Agichtein et al. (2008) to calculate the grammar
score. In particular, cp(i) is calculated by the KL-
divergence between language models of sentence i
to Wikipedia articles.
Position: The first sentence in a review is usu-
ally informative (He et al., 2011). cp(i) is computed
based on the position of the sentence in the review,
i.e. cp(i) = 1/positioni.
Aspect Frequency: The sentence that contains the
frequent aspects is often salient (Nishikawa et al.,
2010). Hence, cp(i) is computed as the sum of the
frequency for aspects in sentence i.
Centroid Distance: As aforementioned, review
sentences are stored in the corresponding aspect
nodes in the hierarchy. The sentence that is close to
the centroid of the reviews stored in an aspect node
is more likely to be salient (Erkan et al., 2004). cp(i)
is computed as the Cosine similarity between sen-
tence i to the corresponding review cluster centroid
based on the unigram features.
Local Density: The sentence would be informa-
tive when it is in the dense part of the aspect node
in the feature space (Scott et al., 1992). We em-
ploy Multivariate Kernel Density Estimation to es-
timate the density. We first represent all the sen-
tences stored in each node into feature vectors, with
unigram as features. The density of a sentence is
then calculated as cp(x) = Eni=1 KH(x − xi)/n,
where x denotes the feature vector of sentence i,
n is the size of sentences stored in the node, and
KH(x) = (2π)_1/2 exp(−1/2(xTx)) represents
the Gaussian kernel.
</bodyText>
<subsectionHeader confidence="0.998598">
3.3 Coherence Weight Estimation
</subsectionHeader>
<bodyText confidence="0.999656583333333">
The coherence ci,j between sentences i and j is
formulated as ci,j = µ · 0(i, j), where µ is a
weight vector, and 0(i, j) denotes the feature func-
tion. 0(i, j) takes two sentences i and j as input,
and outputs a vector with each dimension indicating
the present/absent of a feature. In order to capture
the sequential relations among sentences, we utilize
features as the Cartesian product over the terms of
N-gram (N=1,2) and POS tags generated from sen-
tences i and j (Lapata et al., 2003).
To learn the weight vector µ, we employ
the Passive-Aggressive algorithm (Crammer et al.,
</bodyText>
<page confidence="0.997987">
396
</page>
<bodyText confidence="0.999954272727273">
2006). It is an online learning algorithm, so that we
can update the weight when more consumer reviews
are available. The algorithm takes up one training
sample and outputs the solution that has the highest
score under the current weight. If the output differs
from training samples, the weight vector is updated
according to Eq.7. Since the consumer reviews often
include multiple sentences, we can directly use the
adjacency of these sentences as training samples. In
particular, we treat the adjacent sentence pairs in the
reviews as training samples (i.e. cij = 1).
</bodyText>
<equation confidence="0.989665333333333">
�
min �µ&apos;+1 − µ&apos; �
(7)
</equation>
<bodyText confidence="0.999812888888889">
where µi is the current weight vector and µi�1 is
the updated vector, q* and q� are the gold standard
and predicted sequence of sentences, respectively, p
denotes a set of sentences, *(·) is the feature func-
tion on the whole feature space (i.e. E ii(·)), τ(·, ·)
is a Kendall’s tau lost function (Lapata et al., 2006),
T(·, ·) represents the number of inversion operations
that needs to bring q� to q*, and m denotes the num-
ber of sentences.
</bodyText>
<sectionHeader confidence="0.998383" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.999934333333333">
In this section, we evaluate the effectiveness of the
proposed approach, in terms of question analysis
and answer generation.
</bodyText>
<subsectionHeader confidence="0.999454">
4.1 Data Set and Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999685566666667">
We employed the product review dataset used in Yu
et al. (2011) as corpus. As illustrated in Table 1, the
dataset contained 70,359 reviews about 11 popular
products in four domains. In addition, we created
220 questions for these products by referring to real
questions in Yahoo!Anwser service. We corrected
the typos and grammar errors for these real ques-
tions. Each product contains 15 opinion questions
and 5 factual questions, respectively. All questions
were shown in Appendix C in supplementary mate-
rial. Three annotators were invited to generate the
gold standard. Each question was labeled by two
annotators. The labels include product name, prod-
uct aspect, opinion, question type and question form.
The average inter-rater agreement in terms of Kappa
statistics is 89%. These annotators were then invited
to read the reviews, and create the ground truth an-
swers by selecting and ordering some review sen-
tences. Such process is time consuming and labor-
intensive. We speed up the annotation process as fol-
lows. We first collected all the review sentences in
the answers generated by three evaluated methods to
be discussed in Section 4.3.1. In addition, we sam-
pled the top-N (N=20) sentences on each asked as-
pect and its sub-aspects respectively, where the sen-
tences were ranked based on their salient weights in
Section 3.2. We then provided such subset of review
sentences to the three annotators, and let them indi-
vidually create an answer of up to 100 words (i.e.
K=100) for each question.
</bodyText>
<table confidence="0.999873166666667">
Product Name Domain Review# Sentence#
Canon EOS 450D (Canon EOS) camera 440 628
Fujifilm Finepix AX245W (Fujifilm) camera 541 839
Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546
Apple MacBook Pro (MacBook) laptop 552 4,221
Samsung NC10 (Samsung) laptop 2,712 4,946
Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846
Sony NWZ-S63916GB (Sony NWZ) MP3 341 773
BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008
iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527
Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001
Nokia N95 phone 15,939 44,379
</table>
<tableCaption confidence="0.931185">
Table 1: Statistics of the product review dataset, # denotes
the number of the reviews/sentences.
</tableCaption>
<bodyText confidence="0.999611166666667">
We employed precision (P), recall (R) and F1-
measure (F1) as the evaluation metric for question
analysis, and utilized ROUGE (Lin et al., 2003) as
the metric to evaluate the quality of answer gener-
ation. ROUGE is a widely accepted standard for
summarization, which measures the quality of the
summarized answers by counting the overlapping N-
grams between the answers generated by machine
and human, respectively. In the experiment, we
reported the F1-measure of ROUGE-1, ROUGE-2
and ROUGE-SU4, which count the overlapping un-
igrams, bigrams and skip-4 bigrams respectively.
ROUGE-1 can measure informativeness of the an-
swers, while higher order ROUGE-N (N=2,4) cap-
tures the matching of subsequences, which can mea-
sure the fluency and readability of the answers. For
the trade-off parameters, we empirically set λ1 =
0.4, λ2 = 0.3 and λ3 = 0.3.
</bodyText>
<subsectionHeader confidence="0.998645">
4.2 Evaluations on Question Analysis
</subsectionHeader>
<bodyText confidence="0.999991">
We first evaluated the performance of product recog-
nition, opinion/factual question classification, opin-
ion classification, question type and question form
identification. The experimental results are shown
</bodyText>
<equation confidence="0.743669666666667">
J µ&apos;+1 · Ψ(p, q*) − µ&apos;+1 · Ψ(p, ˆq) &gt; τ(ˆq, q*);
llτ(ˆq, q*) �2,T($,q*)
�(��1)�2,
</equation>
<page confidence="0.993494">
397
</page>
<bodyText confidence="0.998168">
in Table 2. The results show that traditional methods
achieve encouraging performance on the aforemen-
tioned tasks.
</bodyText>
<table confidence="0.998728">
Evaluated Topics P R Fl
Product recognition 0.755 0.618 0.680
Opinion/factual 0.897 0.895 0.893
Opinion classification 0.755 0.745 0.748
Question type 0.800 0.775 0.783
Question form 0.910 0.903 0.905
</table>
<tableCaption confidence="0.979197">
Table 2: Performance of question analysis.
</tableCaption>
<table confidence="0.999711666666667">
Methods P R Fl
Our method 0.851* 0.763* 0.805*
Balahur’s method 0.825 0.400 0.538
</table>
<tableCaption confidence="0.886315">
Table 3: Performance of aspect identification for question
analysis. * denotes the results (i.e. P, R, Fl) are tested
for statistical significance using T-Test, p-values&lt;0.05.
</tableCaption>
<table confidence="0.999552333333333">
Methods P R Fl
Our method 0.726* 0.643* 0.682*
Su’s method 0.689 0.571 0.625
</table>
<tableCaption confidence="0.9900855">
Table 4: Performance of implicit aspect identification for
question analysis. T-Test, p-values&lt;0.05
</tableCaption>
<bodyText confidence="0.999928636363637">
We next examined the performance of our ap-
proach on aspect identification. The method pro-
posed by Balahur et al. (2008) was reimplemented
as the baseline, which identifies aspects based on
noun phrase extraction. This method achieved good
performance on the opinion QA task in TAC 2008
and was employed in subsequent works. As demon-
strated in Table 3, our approach significantly outper-
forms Balahur’s method by over 49.4% in terms of
average F1-measure. A probable reason is that Bal-
ahur’s method relies on noun phrases, which may
mis-identify some noise noun phrases as aspects,
while our approach performs hierarchical classifica-
tion based on the hierarchy, which can leverage the
prior knowledge encoded in the hierarchy to filter
out the noise and obtain accurate aspects.
Moreover, we evaluated the effectiveness of our
approach on implicit aspect identification. The 70
implicit aspect questions in our question corpus
were used here. The method proposed by Su et al.
(2008) was reimplemented as the baseline. It identi-
fies implicit aspects by mutual clustering, and it was
</bodyText>
<figureCaption confidence="0.980705333333333">
Figure 3: Evaluations on multiple optimization criteria
in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, re-
spectively.
</figureCaption>
<bodyText confidence="0.999605166666667">
evaluated in Yu et al. (2011). As shown in Table 4,
our approach significantly outperforms Su’s method
by over 9.1% in terms of average F1-measure. The
results show that the hierarchy can help to identify
implicit aspects by exploiting the underlying associ-
ations among sentiment terms and aspects.
</bodyText>
<table confidence="0.9579415">
Methods ROUGE1 ROUGE2 ROUGE-SU4
Our method 0.364* 0.137* 0.138*
Li’s method 0.127 0.043 0.049
Lloret’s method 0.149 0.058 0.065
</table>
<tableCaption confidence="0.9962735">
Table 5: Performance of answer generation. T-Test, p-
values&lt;0.05.
</tableCaption>
<subsectionHeader confidence="0.985174">
4.3 Evaluations on Answer Generation
4.3.1 Comparisons to the State-of-the-Arts
</subsectionHeader>
<bodyText confidence="0.9997865">
We compared our multi-criteria optimization ap-
proach against two state-of-the-arts methods: a) the
</bodyText>
<page confidence="0.99718">
398
</page>
<bodyText confidence="0.999946227272727">
method presented in Li et al. (2009), which selects
some retrieved sentences to generate the answers
based on a graph-based algorithm; b) the method
proposed by Lloret et al. (2011) that forms the an-
swers by re-ranking the retrieved sentences.
As shown in Table 5, our approach outperforms
Li’s method and Lloret’s method by the significant
absolute gains of over 23.7%, and 21.5% respec-
tively, in terms of average ROUGE-1. It improves
the performance over these two methods in terms
of average ROUGE-2 by the absolute gains of over
9.41% and 7.87%, respectively; and in terms of
ROUGE-SU4 by the absolute gains of over 8.86%
and 7.31%, respectively. By analyzing the results,
we find that the improvements come from the use
of the hierarchical organization and the answer gen-
eration algorithm which exploits multiple criteria,
especially the parent-child relation among aspects.
In addition, our approach can generate the answers
by following the general-to-specific logic, while Li’s
and Lloret’s methods fail to do so due to their igno-
rance of parent-child relations among aspects.
</bodyText>
<subsectionHeader confidence="0.9191165">
4.3.2 Evaluations on the Effectiveness of
Multiple Criteria
</subsectionHeader>
<bodyText confidence="0.999958461538461">
We further evaluated the effectiveness of each op-
timization criterion by tuning the trade-off parame-
ters (i.e. A1, A2, and A3). We fixed A1 as a con-
stant in [0, 1] with 0.1 as an interval, and updated A2
from 0 to 1 − A1, A3 = 1 − A1 − A2, correspond-
ingly. The performance change is shown in Figure
3 in terms of ROUGE-1, ROUGE-2, and ROUGE-
SU4, respectively. The best performance is achieved
at A1 = 0.4, A2 = 0.3, A3 = 0.3. We observe the
performance drops dramatically when any parame-
ter (i.e. A1, A2, A3) is close to 0 (i.e. remove any of
the corresponding criterion). Thus, we can conclude
that all the criteria are useful in answer generation.
We also find that the performance change is sharp
when A1 changes. This indicates that the salience
criterion is crucial for answer generation.
Table 6 shows the exemplar answers generated by
our approach. Each answer first gives the statis-
tic of positive and negative reviews. This helps
user to quickly get an overview of public opin-
ions. The summary of relevant review sentences
is then presented in the answer. The answer di-
versely comments the asked aspect and all its avail-
able sub-aspects following the general-to-specific
logic. Moreover, we feel that the answers are in-
formative and readable.
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="method">
5 Related Works
</sectionHeader>
<bodyText confidence="0.9999945">
In this section, we review existing works related
to the four components of our approach, including
organization of reviews, question analysis, answer
fragment retrieval, and answer generation.
For organization of reviews, Carenini et al. (2006)
proposed to organize the reviews by a hand-crafted
taxonomy, which was not scalable. Yu et al. (2011)
exploited the domain knowledge and consumer re-
views to automatically generate a hierarchy for or-
ganizing consumer reviews.
Question analysis often has to distinguish the
opinion question from the factual one, and find the
key points asked in the questions, such as the prod-
uct aspect and product name. For example, Yu et
al. (2003) proposed to separate opinions from facts
at both document and sentence level, and determine
the polarity on the opinionated sentences in the an-
swer documents. Similarly, Somasundaran et al.
(2007) utilized a SVM classifier to recognize opin-
ionated sentences. The paper argued that the sub-
jective types (i.e. sentiment and arguing) can im-
prove the performance of opinion-QA. Later, Ku et
al. (2007) proposed a two-layered classifier for ques-
tion analysis, and retrieved the answer-fragments by
keyword matching. In particular, they first identified
the opinion questions, and classified them into six
predefined question types, including holder, target,
attitude, reason, majority, and yes/no. These ques-
tion types and corresponding polarity on the ques-
tions were used to filter non-relevant sentences in
the answer fragments. F1-measure was employed as
the evaluation metric.
For the topic of answer generation in opinion-QA,
Li et al. (2009) formulated it as a sentence ranking
task. They argued that the answers should be simul-
taneously relevant to topics and opinions asked in
the questions. They thus designed the graph-based
methods (i.e. PageRank and HITS) to select some
high-ranked sentences to form answers. They first
built a graph on the retrieved sentences, with each
sentence as the node, and the similarity (i.e. Co-
sine similarity) between each sentences pair as the
</bodyText>
<page confidence="0.986149">
399
</page>
<bodyText confidence="0.9316784375">
Question 1: What reasons do people give for preferring iPhone 3gs?
There are 9,928 opinionated reviews about product “iphone 3gs”, with 5,717 positive and 4,221 negative reviews.
This phone is amazing and I would recommend it to anyone. It looks funky and cool. It is worth the money. It’s great
organiser, simple easy to use software. It is superfast, excellent connection via wifi or 3G. It is able to instantly access email.
It’s amazing and has so many free apps. The design is so simple and global. The hardware is good and reliable. The camera is
a good and colors are vibrant. The touch screen is user friendly and the aesthetics are top notch. Battery is charged quickly,
and power save right after stop using.
Question 2: Does anyone think it is expensive to get a iPhone 3GS?
Yes.
There are 2,645 opinionated reviews on aspect “price” about product “iphone 3gs”, with 889 positive and 1,756 negative
reviews.
Throw the costly phone, apple only knows to sell stupid stuff expensively. Don’t fool yourself with iPhone 3gs, believing that it
costs much by Apple luxurious advertising. Apple is so greedy and it just wants to earn easy &amp; fast money by selling its
techless product expensively. The phone will charge once you insert any sim card. iPhone 3gs is high-priced due to the
capacitive and Apple license. You need to pay every application at the end it costs too much. The network provider will make
up some of the cost of the phone on your call charges.
</bodyText>
<tableCaption confidence="0.932436">
Table 6: Sample answers of our approach.
</tableCaption>
<bodyText confidence="0.99800628125">
weight of the corresponding edge. Given a question,
its similarity to each sentence in the graph was com-
puted. Such similarity was viewed as the relevant
score to the corresponding sentence. The sentences
then were ranked based on three metric, i.e. relevant
score to the query, similarity score obtained from the
graph algorithm over sentences, and degree of opin-
ion matching to the query. Respectively, Lloret et
al. (2011) proposed to form answers by re-ranking
the retrieved sentences based on the metric of word
frequency, non-redundancy and the number of noun
phrases. Their method includes three components,
including information retrieval, opinion mining and
text summarization. Evaluations were conducted on
the TAC 2008 Opinion Summarization track. After-
wards, Moghaddam et al. (2011) developed a system
called AQA to generate answers for questions about
products (i.e. opinion QA on products). It classi-
fies the questions into five types, including target,
attitude, reason, majority and yes/no. As compared
to Ku et al. (2007), the question types of holder
and majority are not included. They argued that
product questions were seldom asked for the hold-
ers, since the holders (i.e. reviewers) were com-
monly shown in the reviews. Also, product ques-
tions mainly asked for majority opinions, and ma-
jority type was thus not considered. The AQA sys-
tem includes five components, including question
analysis, question expansion, high quality review re-
trieval, subjective sentence extraction, and answer
grouping. The answers are generated by aggregat-
ing opinions in the retrieved fragments.
</bodyText>
<sectionHeader confidence="0.998741" genericHeader="conclusions">
6 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.999959190476191">
In this paper, we have developed a new product
opinion-QA framework, which exploits the hierar-
chical organization of consumer reviews on prod-
ucts. With the help of the hierarchical organization,
our framework can accurately identify the aspects
asked in the questions and also discover their sub-
aspects. We have further formulated the answer gen-
eration from retrieved review sentences as a multi-
criteria optimization problem. The multiple criteria
used include answer salience, diversity, and coher-
ence. The parent-child relations between the aspects
are incorporated into the approach to ensure that the
answers follow the general-to-specific logic. The
proposed framework has been evaluated on 11 pop-
ular products in four domains using 220 questions
on the products. Significant performance improve-
ments were obtained. In the future, we will explore
the more sophisticated NLP features to improve the
proposed framework. This will be done by incorpo-
rating more NLP features in salience and coherence
weights estimation.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982092">
This work is supported in part by NUS-Tsinghua Ex-
treme Search (NExT) project under the grant num-
ber: R-252-300-001-490. We give warm thanks to
the project and anonymous reviewers for their com-
ments.
</bodyText>
<page confidence="0.995246">
400
</page>
<sectionHeader confidence="0.993884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999593349056603">
E. Agichtein, C. Castillo, and D. Donato. Finding High-
Quality Content in Social Media. WSDM, 2008.
A. Balahur, E. Boldrini, O. Ferrandez, A. Montoyo,
M. Palomar, and R. Munoz. The DLSIUAES Team’s
Participation in the TAC 2008 Tracks. TAC, 2008.
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. Com-
bining Low-level and Summary Representations of
Opinions for Multi-Perspective Question Answering.
AAAI, 2003.
G. Carenini, R. Ng, and E. Zwart. Multi-document Sum-
marization of Evaluative Text. ACL, 2006.
P. Cimiano. Ontology Learning and Population from
Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc. Secaucus, NJ, USA,
2006.
K. Crammer, O. Dekel, J. Keshet, S.S. Shwartz, and
Y. Singer. Online Passive Aggressive Algorithms.
Journal of Machine Learning Research, 2006.
P. Deshpande, R. Barzilay, and D.R. Karger. Random-
ized Decoding for Selection-and-Ordering Problems.
NAACL, 2007.
G. Erkan and D.R. Radev. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. AAAI,
2004.
T. Givon. Syntax: A functional-typological Introduction.
Benjamins Pub, 1990.
J. He and D. Dai. Summarization of Yes/No Questions
Using a Feature Function Model. JMLR, 2011.
P. Jiang, H. Fu, C. Zhang, and Z. Niu. A Framework for
Opinion Question Answering. IMS, 2010.
H.D. Kim, D.H. Park, V.G.V. Vydiswaran, and
C.X. Zhai. Opinion Summarization using Entity Fea-
tures and Probabilistic Sentence Coherence Optimiza-
tion: UIUC at TAC 2008 Opinion Summarization Pi-
lot. TAC, 2008.
D. Koller and M. Sahami. Hierarchically Classifying
Documents Using Very Few Words. ICML, 1997.
L.W. Ku, Y.T. Liang, and H.H. Chen. Question Analysis
and Answer Passage Retrieval for Opinion Question
Answering Systems. International Journal of Compu-
tational Linguistics &amp; Chinese Language Processing,
2007.
M. Lapata. Probabilistic Text Structuring: Experiments
with Sentence Ordering. ACL, 2003.
M. Lapata. Automatic Evaluation of Information Order-
ing: Kendall´s Tau. Computational Linguistics, 2006.
F. Li, Y. Tang, M. Huang, and X. Zhu. Answering
Opinion Questions with Random Walks on Graphs.
ACL/AFNLP, 2009.
C.Y. Lin and E.Hovy. Automatic Evaluation of Sum-
maries Using N-gram Co-Occurrence Statistics. HLT-
NAACL, 2003.
Y. Liu, X. Huang, A. An, and X. Yu. Modeling and Pre-
dicting the Helpfulness of Online Reviews. ICDM,
2008.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo. To-
wards a Unified Approach for Opinion Question An-
swering and Summarization. ACL-HLT, 2011.
H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.
Opinion Summarization with Integer Linear Program-
ming Formulation for Sentence Extraction and Order-
ing. COLING, 2010.
C.D. Mizil and G. Kossinets and J. Kleinberg and L. Lee.
How Opinions are Received by Online Communities:
A Case Study on Amazon.com Helpfulness Votes.
WWW, 2009.
S. Moghaddam and M. Ester. AQA: Aspect-based Opin-
ion Question Answering. IEEE-ICDMW, 2011.
Y. Ouyang, W. Li, and Q. Lu. An Integrated Multi-
document Summarization Approach based on Word
Hierarchical Representation. ACL-IJCNLP, 2009.
A. Schrijver. Theory of Linear and Integer Programming.
John Wiley &amp; Sons, 1998.
D.W. Scott. Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley &amp; Sons, Inc.,
1992.
C. Silla and A. Freitas. A Survey of Hierarchical Classi-
fication Across Different Application Domains. Data
Mining and Knowledge Discovery, 2011.
S. Somasundaran, T. Wilson, J. Wiebe and V. Stoyanov.
QA with Attitude: Exploiting Opinion Type Analysis
for Improving Question Answering in Online Discus-
sions and the News. ICWSM, 2007.
V. Stoyanov, C. Cardie and J. Wiebe. Multi-
Perspective Question Answering Using the OpQA
Corpus. EMNLP, 2005.
Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and
Z. Su. Hidden Sentiment Association in Chinese Web
Opinion Mining. WWW, 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
Contextual Polarity in Phrase-level Sentiment Analy-
sis. HLT/EMNLP, 2005.
J. Yu, Z.J. Zha, M. Wang, K. Wang and T.S. Chua.
Domain-Assisted Product Aspect Hierarchy Genera-
tion: Towards Hierarchical Organization of Unstruc-
tured Consumer Reviews. EMNLP, 2011.
J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. Hierarchi-
cal Organization of Unstructured Consumer Reviews.
WWW, 2011.
J. Yu, Z.J. Zha, M. Wang and T.S. Chua. Aspect Rank-
ing: Identifying Important Product Aspects from On-
line Consumer Reviews. ACL, 2011.
H. Yu and V. Hatzivassiloglou. Towards Answering
Opinion Questions: Separating Facts from Opinions
and Identifying the Polarity of Opinion Sentences.
EMNLP, 2003.
</reference>
<page confidence="0.998564">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854530">
<title confidence="0.9994185">Answering Opinion Questions on Products by Exploiting Organization of Consumer Reviews</title>
<author confidence="0.995681">Jianxing Yu</author>
<author confidence="0.995681">Zheng-Jun Zha</author>
<author confidence="0.995681">Tat-Seng</author>
<affiliation confidence="0.998337">School of National University of</affiliation>
<email confidence="0.887266">zhazj,</email>
<abstract confidence="0.99896075862069">This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to generate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
</authors>
<title>Finding HighQuality Content in Social Media.</title>
<date>2008</date>
<publisher>WSDM,</publisher>
<contexts>
<context position="21459" citStr="Agichtein et al. (2008)" startWordPosition="3460" endWordPosition="3463">tence i. We define seven measurements (i.e. G = 7) below. Helpfulness: Many forum websites provide a helpfulness score, which is used to rate the quality of a review. The sentences that come from helpful reviews are often representative (Mizil et al., 2009). We compute cp(i) of sentence i by using helpfulness score from its host review. Timeliness: The new coming sentence often contains more updated and useful information (Liu et al., 2008). cp(i) is the post time of sentence i. We normalize it to [0, 1]. Grammaticality: The grammatical sentence is often more readable. We employ the method in Agichtein et al. (2008) to calculate the grammar score. In particular, cp(i) is calculated by the KLdivergence between language models of sentence i to Wikipedia articles. Position: The first sentence in a review is usually informative (He et al., 2011). cp(i) is computed based on the position of the sentence in the review, i.e. cp(i) = 1/positioni. Aspect Frequency: The sentence that contains the frequent aspects is often salient (Nishikawa et al., 2010). Hence, cp(i) is computed as the sum of the frequency for aspects in sentence i. Centroid Distance: As aforementioned, review sentences are stored in the correspon</context>
</contexts>
<marker>Agichtein, Castillo, Donato, 2008</marker>
<rawString>E. Agichtein, C. Castillo, and D. Donato. Finding HighQuality Content in Social Media. WSDM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>E Boldrini</author>
<author>O Ferrandez</author>
<author>A Montoyo</author>
<author>M Palomar</author>
<author>R Munoz</author>
</authors>
<title>Tracks.</title>
<date>2008</date>
<booktitle>The DLSIUAES Team’s Participation in the TAC</booktitle>
<location>TAC,</location>
<contexts>
<context position="28995" citStr="Balahur et al. (2008)" startWordPosition="4696" endWordPosition="4699"> form 0.910 0.903 0.905 Table 2: Performance of question analysis. Methods P R Fl Our method 0.851* 0.763* 0.805* Balahur’s method 0.825 0.400 0.538 Table 3: Performance of aspect identification for question analysis. * denotes the results (i.e. P, R, Fl) are tested for statistical significance using T-Test, p-values&lt;0.05. Methods P R Fl Our method 0.726* 0.643* 0.682* Su’s method 0.689 0.571 0.625 Table 4: Performance of implicit aspect identification for question analysis. T-Test, p-values&lt;0.05 We next examined the performance of our approach on aspect identification. The method proposed by Balahur et al. (2008) was reimplemented as the baseline, which identifies aspects based on noun phrase extraction. This method achieved good performance on the opinion QA task in TAC 2008 and was employed in subsequent works. As demonstrated in Table 3, our approach significantly outperforms Balahur’s method by over 49.4% in terms of average F1-measure. A probable reason is that Balahur’s method relies on noun phrases, which may mis-identify some noise noun phrases as aspects, while our approach performs hierarchical classification based on the hierarchy, which can leverage the prior knowledge encoded in the hiera</context>
</contexts>
<marker>Balahur, Boldrini, Ferrandez, Montoyo, Palomar, Munoz, 2008</marker>
<rawString>A. Balahur, E. Boldrini, O. Ferrandez, A. Montoyo, M. Palomar, and R. Munoz. The DLSIUAES Team’s Participation in the TAC 2008 Tracks. TAC, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>D Litman</author>
</authors>
<title>Combining Low-level and Summary Representations of Opinions for Multi-Perspective Question Answering.</title>
<date>2003</date>
<publisher>AAAI,</publisher>
<marker>Cardie, Wiebe, Wilson, Litman, 2003</marker>
<rawString>C. Cardie, J. Wiebe, T. Wilson, and D. Litman. Combining Low-level and Summary Representations of Opinions for Multi-Perspective Question Answering. AAAI, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>E Zwart</author>
</authors>
<title>Multi-document Summarization of Evaluative Text.</title>
<date>2006</date>
<publisher>ACL,</publisher>
<contexts>
<context position="33403" citStr="Carenini et al. (2006)" startWordPosition="5405" endWordPosition="5408">ic of positive and negative reviews. This helps user to quickly get an overview of public opinions. The summary of relevant review sentences is then presented in the answer. The answer diversely comments the asked aspect and all its available sub-aspects following the general-to-specific logic. Moreover, we feel that the answers are informative and readable. 5 Related Works In this section, we review existing works related to the four components of our approach, including organization of reviews, question analysis, answer fragment retrieval, and answer generation. For organization of reviews, Carenini et al. (2006) proposed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Simila</context>
</contexts>
<marker>Carenini, Ng, Zwart, 2006</marker>
<rawString>G. Carenini, R. Ng, and E. Zwart. Multi-document Summarization of Evaluative Text. ACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
</authors>
<title>Ontology Learning and Population from Text: Algorithms, Evaluation and Applications.</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc. Secaucus, NJ, USA,</location>
<marker>Cimiano, 2006</marker>
<rawString>P. Cimiano. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications. Springer-Verlag New York, Inc. Secaucus, NJ, USA, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S S Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<marker>Crammer, Dekel, Keshet, Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S.S. Shwartz, and Y. Singer. Online Passive Aggressive Algorithms. Journal of Machine Learning Research, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Deshpande</author>
<author>R Barzilay</author>
<author>D R Karger</author>
</authors>
<title>Randomized Decoding for Selection-and-Ordering Problems.</title>
<date>2007</date>
<location>NAACL,</location>
<marker>Deshpande, Barzilay, Karger, 2007</marker>
<rawString>P. Deshpande, R. Barzilay, and D.R. Karger. Randomized Decoding for Selection-and-Ordering Problems. NAACL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<publisher>AAAI,</publisher>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D.R. Radev. LexRank: Graph-based lexical centrality as salience in text summarization. AAAI, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Givon</author>
</authors>
<title>Syntax: A functional-typological Introduction. Benjamins Pub,</title>
<date>1990</date>
<marker>Givon, 1990</marker>
<rawString>T. Givon. Syntax: A functional-typological Introduction. Benjamins Pub, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J He</author>
<author>D Dai</author>
</authors>
<title>Summarization of Yes/No Questions Using a Feature Function Model.</title>
<date>2011</date>
<location>JMLR,</location>
<marker>He, Dai, 2011</marker>
<rawString>J. He and D. Dai. Summarization of Yes/No Questions Using a Feature Function Model. JMLR, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jiang</author>
<author>H Fu</author>
<author>C Zhang</author>
<author>Z Niu</author>
</authors>
<title>A Framework for Opinion Question Answering.</title>
<date>2010</date>
<location>IMS,</location>
<contexts>
<context position="3073" citStr="Jiang et al., 2010" startWordPosition="466" endWordPosition="469">nswer is “Washington, D.C.” For a product opinionated question, the answer should not be just a best answer. It should reflect the opinions of various segments of users, and incorpo391 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 391–401, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics rate both positive and negative viewpoints. Hence the answer should be a summarization of public opinions and comments on the product or specific aspect asked in the question (Jiang et al., 2010). In addition, it should also include public opinions and comments on the sub-aspects. Such answers would help users to understand the inherent reasons of the opinions on the asked aspect. For example, the question “What do people think the camera of Nokia 5800?” asks for public positive and negative opinions on the aspect “camera” of product “Nokia 5800.” The summarization of opinions on the sub-aspects such as “lens” and “resolution” would help users better understand that the public complaints on the aspect “camera” are due to the poor “lens” and/or low “resolution.” Moreover, the answer sh</context>
</contexts>
<marker>Jiang, Fu, Zhang, Niu, 2010</marker>
<rawString>P. Jiang, H. Fu, C. Zhang, and Z. Niu. A Framework for Opinion Question Answering. IMS, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D Kim</author>
<author>D H Park</author>
<author>V G V Vydiswaran</author>
<author>C X Zhai</author>
</authors>
<title>Opinion Summarization using Entity Features and Probabilistic Sentence Coherence Optimization: UIUC at TAC</title>
<date>2008</date>
<location>TAC,</location>
<marker>Kim, Park, Vydiswaran, Zhai, 2008</marker>
<rawString>H.D. Kim, D.H. Park, V.G.V. Vydiswaran, and C.X. Zhai. Opinion Summarization using Entity Features and Probabilistic Sentence Coherence Optimization: UIUC at TAC 2008 Opinion Summarization Pilot. TAC, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically Classifying Documents Using Very Few Words.</title>
<date>1997</date>
<location>ICML,</location>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M. Sahami. Hierarchically Classifying Documents Using Very Few Words. ICML, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Ku</author>
<author>Y T Liang</author>
<author>H H Chen</author>
</authors>
<title>Question Analysis and Answer Passage Retrieval for Opinion Question Answering Systems.</title>
<date>2007</date>
<booktitle>International Journal of Computational Linguistics &amp; Chinese Language Processing,</booktitle>
<contexts>
<context position="13503" citStr="Ku et al., 2007" startWordPosition="2081" endWordPosition="2084">questions from the factual ones (Yu et al., 2003). Since the opinion questions often contain one or more sentiment terms, we classify them by employing the sentiment terms in the sentiment lexicon provided from MPQA project (Wilson et al., 2005). Subsequently, we learn a SVM sentiment classifier to determine the opinion polarity of the opinion questions. In particular, the reviews and corresponding opinions stored in the hierarchy are used as training samples, which are represented by the unigram features. Identifying the question type: Opinion questions are often categorized into four types (Ku et al., 2007), • Attitude question, asking for public opinion on a product or product aspect, such as “What do people think iPhone 3gs?” • Reason question, asking for the reason of public opinion on a product or product aspect, such as “Why do people like iPhone 3gs?” • Target question, asking for the object in the public opinion, such as “Which phone is better than Nokia N95?” • Yes/No question, asking for whether a statement is correct, such as “Is Nokia N95 bad?” We formulate the question type identification as a multi-class classification problem. A multi-class SVM classifier 3 is learned for the class</context>
<context position="34235" citStr="Ku et al. (2007)" startWordPosition="5540" endWordPosition="5543"> reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The paper argued that the subjective types (i.e. sentiment and arguing) can improve the performance of opinion-QA. Later, Ku et al. (2007) proposed a two-layered classifier for question analysis, and retrieved the answer-fragments by keyword matching. In particular, they first identified the opinion questions, and classified them into six predefined question types, including holder, target, attitude, reason, majority, and yes/no. These question types and corresponding polarity on the questions were used to filter non-relevant sentences in the answer fragments. F1-measure was employed as the evaluation metric. For the topic of answer generation in opinion-QA, Li et al. (2009) formulated it as a sentence ranking task. They argued </context>
<context position="37771" citStr="Ku et al. (2007)" startWordPosition="6114" endWordPosition="6117">11) proposed to form answers by re-ranking the retrieved sentences based on the metric of word frequency, non-redundancy and the number of noun phrases. Their method includes three components, including information retrieval, opinion mining and text summarization. Evaluations were conducted on the TAC 2008 Opinion Summarization track. Afterwards, Moghaddam et al. (2011) developed a system called AQA to generate answers for questions about products (i.e. opinion QA on products). It classifies the questions into five types, including target, attitude, reason, majority and yes/no. As compared to Ku et al. (2007), the question types of holder and majority are not included. They argued that product questions were seldom asked for the holders, since the holders (i.e. reviewers) were commonly shown in the reviews. Also, product questions mainly asked for majority opinions, and majority type was thus not considered. The AQA system includes five components, including question analysis, question expansion, high quality review retrieval, subjective sentence extraction, and answer grouping. The answers are generated by aggregating opinions in the retrieved fragments. 6 Conclusions and Future Works In this pap</context>
</contexts>
<marker>Ku, Liang, Chen, 2007</marker>
<rawString>L.W. Ku, Y.T. Liang, and H.H. Chen. Question Analysis and Answer Passage Retrieval for Opinion Question Answering Systems. International Journal of Computational Linguistics &amp; Chinese Language Processing, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic Text Structuring: Experiments with Sentence Ordering.</title>
<date>2003</date>
<publisher>ACL,</publisher>
<marker>Lapata, 2003</marker>
<rawString>M. Lapata. Probabilistic Text Structuring: Experiments with Sentence Ordering. ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Automatic Evaluation of Information Ordering: Kendall´s Tau. Computational Linguistics,</title>
<date>2006</date>
<marker>Lapata, 2006</marker>
<rawString>M. Lapata. Automatic Evaluation of Information Ordering: Kendall´s Tau. Computational Linguistics, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>Y Tang</author>
<author>M Huang</author>
<author>X Zhu</author>
</authors>
<title>Answering Opinion Questions with Random Walks on Graphs.</title>
<date>2009</date>
<publisher>ACL/AFNLP,</publisher>
<contexts>
<context position="30782" citStr="Li et al. (2009)" startWordPosition="4971" endWordPosition="4974"> method by over 9.1% in terms of average F1-measure. The results show that the hierarchy can help to identify implicit aspects by exploiting the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.043 0.049 Lloret’s method 0.149 0.058 0.065 Table 5: Performance of answer generation. T-Test, pvalues&lt;0.05. 4.3 Evaluations on Answer Generation 4.3.1 Comparisons to the State-of-the-Arts We compared our multi-criteria optimization approach against two state-of-the-arts methods: a) the 398 method presented in Li et al. (2009), which selects some retrieved sentences to generate the answers based on a graph-based algorithm; b) the method proposed by Lloret et al. (2011) that forms the answers by re-ranking the retrieved sentences. As shown in Table 5, our approach outperforms Li’s method and Lloret’s method by the significant absolute gains of over 23.7%, and 21.5% respectively, in terms of average ROUGE-1. It improves the performance over these two methods in terms of average ROUGE-2 by the absolute gains of over 9.41% and 7.87%, respectively; and in terms of ROUGE-SU4 by the absolute gains of over 8.86% and 7.31%,</context>
<context position="34780" citStr="Li et al. (2009)" startWordPosition="5619" endWordPosition="5622">ing) can improve the performance of opinion-QA. Later, Ku et al. (2007) proposed a two-layered classifier for question analysis, and retrieved the answer-fragments by keyword matching. In particular, they first identified the opinion questions, and classified them into six predefined question types, including holder, target, attitude, reason, majority, and yes/no. These question types and corresponding polarity on the questions were used to filter non-relevant sentences in the answer fragments. F1-measure was employed as the evaluation metric. For the topic of answer generation in opinion-QA, Li et al. (2009) formulated it as a sentence ranking task. They argued that the answers should be simultaneously relevant to topics and opinions asked in the questions. They thus designed the graph-based methods (i.e. PageRank and HITS) to select some high-ranked sentences to form answers. They first built a graph on the retrieved sentences, with each sentence as the node, and the similarity (i.e. Cosine similarity) between each sentences pair as the 399 Question 1: What reasons do people give for preferring iPhone 3gs? There are 9,928 opinionated reviews about product “iphone 3gs”, with 5,717 positive and 4,</context>
</contexts>
<marker>Li, Tang, Huang, Zhu, 2009</marker>
<rawString>F. Li, Y. Tang, M. Huang, and X. Zhu. Answering Opinion Questions with Random Walks on Graphs. ACL/AFNLP, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>and E.Hovy. Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics.</title>
<date>2003</date>
<location>HLTNAACL,</location>
<marker>Lin, 2003</marker>
<rawString>C.Y. Lin and E.Hovy. Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics. HLTNAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>X Huang</author>
<author>A An</author>
<author>X Yu</author>
</authors>
<title>Modeling and Predicting the Helpfulness of Online Reviews.</title>
<date>2008</date>
<publisher>ICDM,</publisher>
<contexts>
<context position="21280" citStr="Liu et al., 2008" startWordPosition="3429" endWordPosition="3432">|Si,j=1. 3.2 Salience Weight Estimation The salience weight of sentence i is formulated as wi = EGg=1 cpg(i)/G, where cp(i) denotes the measurement for the importance of sentence i. We define seven measurements (i.e. G = 7) below. Helpfulness: Many forum websites provide a helpfulness score, which is used to rate the quality of a review. The sentences that come from helpful reviews are often representative (Mizil et al., 2009). We compute cp(i) of sentence i by using helpfulness score from its host review. Timeliness: The new coming sentence often contains more updated and useful information (Liu et al., 2008). cp(i) is the post time of sentence i. We normalize it to [0, 1]. Grammaticality: The grammatical sentence is often more readable. We employ the method in Agichtein et al. (2008) to calculate the grammar score. In particular, cp(i) is calculated by the KLdivergence between language models of sentence i to Wikipedia articles. Position: The first sentence in a review is usually informative (He et al., 2011). cp(i) is computed based on the position of the sentence in the review, i.e. cp(i) = 1/positioni. Aspect Frequency: The sentence that contains the frequent aspects is often salient (Nishikaw</context>
</contexts>
<marker>Liu, Huang, An, Yu, 2008</marker>
<rawString>Y. Liu, X. Huang, A. An, and X. Yu. Modeling and Predicting the Helpfulness of Online Reviews. ICDM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lloret</author>
<author>A Balahur</author>
<author>M Palomar</author>
<author>A Montoyo</author>
</authors>
<title>Towards a Unified Approach for Opinion Question Answering and Summarization.</title>
<date>2011</date>
<publisher>ACL-HLT,</publisher>
<contexts>
<context position="4122" citStr="Lloret et al., 2011" startWordPosition="630" endWordPosition="633">esolution” would help users better understand that the public complaints on the aspect “camera” are due to the poor “lens” and/or low “resolution.” Moreover, the answer should be presented following the generalto-specific logic, i.e., from general aspects to specific sub-aspects. This makes the answer easier to understand by the users (Ouyang et al., 2009). Current Opinion-QA methods mainly include three components, including question analysis that identifies aspects and opinions asked in the questions, answer fragment retrieval, and answer generation which summarizes the retrieved fragments (Lloret et al., 2011). Although existing methods show encouraging performance, they are usually not able to generate satisfactory answers due to the following drawbacks. First, current methods often identify aspects as the noun phrases in the questions. However, noun phrases contain noises that are not aspects. This gives rise to imprecise aspect identification. For example, in the question “What reasons can I persuade my wife that people prefer the battery of Nokia N95?” noun phrases “wife” and “people” are not aspects. Moreover, current methods relied on noun phrases are not able to reveal the implicit aspects, </context>
<context position="30927" citStr="Lloret et al. (2011)" startWordPosition="4994" endWordPosition="4997">the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.043 0.049 Lloret’s method 0.149 0.058 0.065 Table 5: Performance of answer generation. T-Test, pvalues&lt;0.05. 4.3 Evaluations on Answer Generation 4.3.1 Comparisons to the State-of-the-Arts We compared our multi-criteria optimization approach against two state-of-the-arts methods: a) the 398 method presented in Li et al. (2009), which selects some retrieved sentences to generate the answers based on a graph-based algorithm; b) the method proposed by Lloret et al. (2011) that forms the answers by re-ranking the retrieved sentences. As shown in Table 5, our approach outperforms Li’s method and Lloret’s method by the significant absolute gains of over 23.7%, and 21.5% respectively, in terms of average ROUGE-1. It improves the performance over these two methods in terms of average ROUGE-2 by the absolute gains of over 9.41% and 7.87%, respectively; and in terms of ROUGE-SU4 by the absolute gains of over 8.86% and 7.31%, respectively. By analyzing the results, we find that the improvements come from the use of the hierarchical organization and the answer generati</context>
<context position="37158" citStr="Lloret et al. (2011)" startWordPosition="6023" endWordPosition="6026">se. You need to pay every application at the end it costs too much. The network provider will make up some of the cost of the phone on your call charges. Table 6: Sample answers of our approach. weight of the corresponding edge. Given a question, its similarity to each sentence in the graph was computed. Such similarity was viewed as the relevant score to the corresponding sentence. The sentences then were ranked based on three metric, i.e. relevant score to the query, similarity score obtained from the graph algorithm over sentences, and degree of opinion matching to the query. Respectively, Lloret et al. (2011) proposed to form answers by re-ranking the retrieved sentences based on the metric of word frequency, non-redundancy and the number of noun phrases. Their method includes three components, including information retrieval, opinion mining and text summarization. Evaluations were conducted on the TAC 2008 Opinion Summarization track. Afterwards, Moghaddam et al. (2011) developed a system called AQA to generate answers for questions about products (i.e. opinion QA on products). It classifies the questions into five types, including target, attitude, reason, majority and yes/no. As compared to Ku </context>
</contexts>
<marker>Lloret, Balahur, Palomar, Montoyo, 2011</marker>
<rawString>E. Lloret, A. Balahur, M. Palomar, and A. Montoyo. Towards a Unified Approach for Opinion Question Answering and Summarization. ACL-HLT, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nishikawa</author>
<author>T Hasegawa</author>
<author>Y Matsuo</author>
<author>G Kikui</author>
</authors>
<title>Opinion Summarization with Integer Linear Programming Formulation for Sentence Extraction and Ordering.</title>
<date>2010</date>
<location>COLING,</location>
<contexts>
<context position="21895" citStr="Nishikawa et al., 2010" startWordPosition="3531" endWordPosition="3534">., 2008). cp(i) is the post time of sentence i. We normalize it to [0, 1]. Grammaticality: The grammatical sentence is often more readable. We employ the method in Agichtein et al. (2008) to calculate the grammar score. In particular, cp(i) is calculated by the KLdivergence between language models of sentence i to Wikipedia articles. Position: The first sentence in a review is usually informative (He et al., 2011). cp(i) is computed based on the position of the sentence in the review, i.e. cp(i) = 1/positioni. Aspect Frequency: The sentence that contains the frequent aspects is often salient (Nishikawa et al., 2010). Hence, cp(i) is computed as the sum of the frequency for aspects in sentence i. Centroid Distance: As aforementioned, review sentences are stored in the corresponding aspect nodes in the hierarchy. The sentence that is close to the centroid of the reviews stored in an aspect node is more likely to be salient (Erkan et al., 2004). cp(i) is computed as the Cosine similarity between sentence i to the corresponding review cluster centroid based on the unigram features. Local Density: The sentence would be informative when it is in the dense part of the aspect node in the feature space (Scott et </context>
</contexts>
<marker>Nishikawa, Hasegawa, Matsuo, Kikui, 2010</marker>
<rawString>H. Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui. Opinion Summarization with Integer Linear Programming Formulation for Sentence Extraction and Ordering. COLING, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Mizil</author>
<author>G Kossinets</author>
<author>J Kleinberg</author>
<author>L Lee</author>
</authors>
<title>How Opinions are Received by Online Communities: A Case Study on Amazon.com Helpfulness Votes.</title>
<date>2009</date>
<location>WWW,</location>
<contexts>
<context position="21093" citStr="Mizil et al., 2009" startWordPosition="3398" endWordPosition="3401">1998). The optimal solutions si|Si=1 and ei,j|S =1 indicate the selected seni,j tences and the order of them. In the next subsections, we will introduce the estimations of wi|S i=1 and ci,j|Si,j=1. 3.2 Salience Weight Estimation The salience weight of sentence i is formulated as wi = EGg=1 cpg(i)/G, where cp(i) denotes the measurement for the importance of sentence i. We define seven measurements (i.e. G = 7) below. Helpfulness: Many forum websites provide a helpfulness score, which is used to rate the quality of a review. The sentences that come from helpful reviews are often representative (Mizil et al., 2009). We compute cp(i) of sentence i by using helpfulness score from its host review. Timeliness: The new coming sentence often contains more updated and useful information (Liu et al., 2008). cp(i) is the post time of sentence i. We normalize it to [0, 1]. Grammaticality: The grammatical sentence is often more readable. We employ the method in Agichtein et al. (2008) to calculate the grammar score. In particular, cp(i) is calculated by the KLdivergence between language models of sentence i to Wikipedia articles. Position: The first sentence in a review is usually informative (He et al., 2011). cp</context>
</contexts>
<marker>Mizil, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>C.D. Mizil and G. Kossinets and J. Kleinberg and L. Lee. How Opinions are Received by Online Communities: A Case Study on Amazon.com Helpfulness Votes. WWW, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moghaddam</author>
<author>M Ester</author>
</authors>
<title>AQA: Aspect-based Opinion Question Answering.</title>
<date>2011</date>
<publisher>IEEE-ICDMW,</publisher>
<marker>Moghaddam, Ester, 2011</marker>
<rawString>S. Moghaddam and M. Ester. AQA: Aspect-based Opinion Question Answering. IEEE-ICDMW, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ouyang</author>
<author>W Li</author>
<author>Q Lu</author>
</authors>
<title>An Integrated Multidocument Summarization Approach based on Word Hierarchical Representation.</title>
<date>2009</date>
<publisher>ACL-IJCNLP,</publisher>
<contexts>
<context position="3860" citStr="Ouyang et al., 2009" startWordPosition="594" endWordPosition="597"> on the asked aspect. For example, the question “What do people think the camera of Nokia 5800?” asks for public positive and negative opinions on the aspect “camera” of product “Nokia 5800.” The summarization of opinions on the sub-aspects such as “lens” and “resolution” would help users better understand that the public complaints on the aspect “camera” are due to the poor “lens” and/or low “resolution.” Moreover, the answer should be presented following the generalto-specific logic, i.e., from general aspects to specific sub-aspects. This makes the answer easier to understand by the users (Ouyang et al., 2009). Current Opinion-QA methods mainly include three components, including question analysis that identifies aspects and opinions asked in the questions, answer fragment retrieval, and answer generation which summarizes the retrieved fragments (Lloret et al., 2011). Although existing methods show encouraging performance, they are usually not able to generate satisfactory answers due to the following drawbacks. First, current methods often identify aspects as the noun phrases in the questions. However, noun phrases contain noises that are not aspects. This gives rise to imprecise aspect identifica</context>
</contexts>
<marker>Ouyang, Li, Lu, 2009</marker>
<rawString>Y. Ouyang, W. Li, and Q. Lu. An Integrated Multidocument Summarization Approach based on Word Hierarchical Representation. ACL-IJCNLP, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schrijver</author>
</authors>
<title>Theory of Linear and Integer Programming.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons,</publisher>
<marker>Schrijver, 1998</marker>
<rawString>A. Schrijver. Theory of Linear and Integer Programming. John Wiley &amp; Sons, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Scott</author>
</authors>
<title>Multivariate Density Estimation: Theory, Practice, and Visualization.</title>
<date>1992</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<marker>Scott, 1992</marker>
<rawString>D.W. Scott. Multivariate Density Estimation: Theory, Practice, and Visualization. John Wiley &amp; Sons, Inc., 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Silla</author>
<author>A Freitas</author>
</authors>
<title>A Survey of Hierarchical Classification Across Different Application Domains. Data Mining and Knowledge Discovery,</title>
<date>2011</date>
<marker>Silla, Freitas, 2011</marker>
<rawString>C. Silla and A. Freitas. A Survey of Hierarchical Classification Across Different Application Domains. Data Mining and Knowledge Discovery, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>V Stoyanov</author>
</authors>
<title>QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering</title>
<date>2007</date>
<booktitle>in Online Discussions and the News. ICWSM,</booktitle>
<contexts>
<context position="34034" citStr="Somasundaran et al. (2007)" startWordPosition="5507" endWordPosition="5510">osed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The paper argued that the subjective types (i.e. sentiment and arguing) can improve the performance of opinion-QA. Later, Ku et al. (2007) proposed a two-layered classifier for question analysis, and retrieved the answer-fragments by keyword matching. In particular, they first identified the opinion questions, and classified them into six predefined question types, including holder, target, attitude, reason, majority, and yes/no. These question types and corresponding polarity on the questions were used to filter non-relevant sente</context>
</contexts>
<marker>Somasundaran, Wilson, Wiebe, Stoyanov, 2007</marker>
<rawString>S. Somasundaran, T. Wilson, J. Wiebe and V. Stoyanov. QA with Attitude: Exploiting Opinion Type Analysis for Improving Question Answering in Online Discussions and the News. ICWSM, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>J Wiebe</author>
</authors>
<title>MultiPerspective Question Answering Using the OpQA Corpus.</title>
<date>2005</date>
<location>EMNLP,</location>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>V. Stoyanov, C. Cardie and J. Wiebe. MultiPerspective Question Answering Using the OpQA Corpus. EMNLP, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Su</author>
<author>X Xu</author>
<author>H Guo</author>
<author>X Wu</author>
<author>X Zhang</author>
<author>B Swen</author>
<author>Z Su</author>
</authors>
<title>Hidden Sentiment Association in Chinese Web Opinion Mining.</title>
<date>2008</date>
<location>WWW,</location>
<contexts>
<context position="11402" citStr="Su et al., 2008" startWordPosition="1745" endWordPosition="1748">orresponds to other words. Identifying aspects: As aforementioned, simply extracting the noun phrases as aspects would import noises. Also, some “implicit” aspects do not exlhttp://nlp.stanford.edu/software/CRF-NER.shtml zhttp://answers.yahoo.com 393 plicitly appear in the reviews. One simple solution for these problems can resort to the review hierarchy. The hierarchy has organized product aspects, which can be used to filter the noise noun phrases for accurately identifying the explicit aspects. For the implicit aspects, we observe they are usually modified by some peculiar sentiment terms (Su et al., 2008). For example, the aspect “size” is often modified by the sentiment terms such as “large”, but seldom by the terms such as “expensive.” Thus, there are some associations between the aspects and sentiment terms. Such associations can be learned from the hierarchy and leveraged to infer the implicit aspects (Yu et al., 2011). In order to simultaneously identify the (explicit/implicit) aspects, we adopt a hierarchical classification technique. The technique simultaneously learns to identify explicit aspects, and discovers the associations between aspects and sentiment terms by multiple classifier</context>
<context position="29856" citStr="Su et al. (2008)" startWordPosition="4832" endWordPosition="4835"> significantly outperforms Balahur’s method by over 49.4% in terms of average F1-measure. A probable reason is that Balahur’s method relies on noun phrases, which may mis-identify some noise noun phrases as aspects, while our approach performs hierarchical classification based on the hierarchy, which can leverage the prior knowledge encoded in the hierarchy to filter out the noise and obtain accurate aspects. Moreover, we evaluated the effectiveness of our approach on implicit aspect identification. The 70 implicit aspect questions in our question corpus were used here. The method proposed by Su et al. (2008) was reimplemented as the baseline. It identifies implicit aspects by mutual clustering, and it was Figure 3: Evaluations on multiple optimization criteria in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, respectively. evaluated in Yu et al. (2011). As shown in Table 4, our approach significantly outperforms Su’s method by over 9.1% in terms of average F1-measure. The results show that the hierarchy can help to identify implicit aspects by exploiting the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.04</context>
</contexts>
<marker>Su, Xu, Guo, Wu, Zhang, Swen, Su, 2008</marker>
<rawString>Q. Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, and Z. Su. Hidden Sentiment Association in Chinese Web Opinion Mining. WWW, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.</title>
<date>2005</date>
<location>HLT/EMNLP,</location>
<contexts>
<context position="12792" citStr="Wilson et al., 2005" startWordPosition="1969" endWordPosition="1972">duct hierarchy. The classification greedily searches a path in the hierarchy from top to down. The search begins at the root node, and stops at the leaf node or a specific node where the relevance score is lower than a pre-defined threshold. The relevance score on each node is determined by a SVM classifier. Multiple SVM classifiers are learned on the hierarchy, one distinct classifier for a node. The reviews that are stored in the node and its child-nodes are used as training samples. We employ the features of noun terms, and sentiment terms in the sentiment lexicon provided by MPQA project (Wilson et al., 2005). Classifying the opinions: Given a set of testing questions, we first distinguish the opinion questions from the factual ones (Yu et al., 2003). Since the opinion questions often contain one or more sentiment terms, we classify them by employing the sentiment terms in the sentiment lexicon provided from MPQA project (Wilson et al., 2005). Subsequently, we learn a SVM sentiment classifier to determine the opinion polarity of the opinion questions. In particular, the reviews and corresponding opinions stored in the hierarchy are used as training samples, which are represented by the unigram fea</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing Contextual Polarity in Phrase-level Sentiment Analysis. HLT/EMNLP, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z J Zha</author>
<author>M Wang</author>
<author>K Wang</author>
<author>T S Chua</author>
</authors>
<title>Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews.</title>
<date>2011</date>
<publisher>EMNLP,</publisher>
<contexts>
<context position="5780" citStr="Yu et al., 2011" startWordPosition="890" endWordPosition="893">nderstanding the answers. To overcome these problems, we can resort to the hierarchical organization of consumer reviews on products. As illustrated in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. Such hierarchy can naturally facilitate to identify aspects asked in questions. While explicit aspects can be recognized by referring to the hierarchy, implicit aspects can be inferred based on the associations between sentiment terms and aspects in the hierarchy (Yu et al., 2011). The sentiment terms are discovered from the reviews on corresponding aspects. Moreover, by following the parent-child relations in the hierarchy, sub-aspects of the asked aspect can be directly acquired, and the answers can present aspects from general to specific. Motivated by the above observations, we propose to exploit the hierarchical organization of consumer reviews for product opinion-QA. As illustrated in Figure 1, our framework first organizes consumer reviews of a certain product into a hierarchical organization. The resulting hierarchy is in turn used to help question analysis and</context>
<context position="8862" citStr="Yu et al. (2011)" startWordPosition="1361" endWordPosition="1364">inions on a product or some aspects of the product. The task is to retrieve the opinionated review fragments relevant to the asked product/product aspects, and summarize these fragments to form an appropriate answer to question q. Next, we introduce the components of hierarchical organization that organizes consumer reviews into a hierarchy, question analysis which identifies the products/aspects and opinions asked in the questions, and answer fragment retrieval that retrieves review fragments relevant to the questions. 2.1 Hierarchical Organization of Reviews We employ the method proposed by Yu et al. (2011) to organize consumer reviews of a product into a hierarchical organization. As shown in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. In particular, this method first automatically acquires an initial aspect hierarchy from the domain knowledge and identifies aspects commented in the reviews. It then incrementally inserts the identified aspects into appropriate positions in the initial hierarchy, and finally obtains an aspect hierarchy that allocates all the newly identified aspects. The consumer reviews are then organized to their correspo</context>
<context position="11726" citStr="Yu et al., 2011" startWordPosition="1799" endWordPosition="1802">n resort to the review hierarchy. The hierarchy has organized product aspects, which can be used to filter the noise noun phrases for accurately identifying the explicit aspects. For the implicit aspects, we observe they are usually modified by some peculiar sentiment terms (Su et al., 2008). For example, the aspect “size” is often modified by the sentiment terms such as “large”, but seldom by the terms such as “expensive.” Thus, there are some associations between the aspects and sentiment terms. Such associations can be learned from the hierarchy and leveraged to infer the implicit aspects (Yu et al., 2011). In order to simultaneously identify the (explicit/implicit) aspects, we adopt a hierarchical classification technique. The technique simultaneously learns to identify explicit aspects, and discovers the associations between aspects and sentiment terms by multiple classifiers. In particular, given a testing question, we identify its aspect by hierarchically classify (Silla et al., 2011) it into the appropriate aspect node of a particular product hierarchy. The classification greedily searches a path in the hierarchy from top to down. The search begins at the root node, and stops at the leaf n</context>
<context position="24822" citStr="Yu et al. (2011)" startWordPosition="4039" endWordPosition="4042">, q* and q� are the gold standard and predicted sequence of sentences, respectively, p denotes a set of sentences, *(·) is the feature function on the whole feature space (i.e. E ii(·)), τ(·, ·) is a Kendall’s tau lost function (Lapata et al., 2006), T(·, ·) represents the number of inversion operations that needs to bring q� to q*, and m denotes the number of sentences. 4 Evaluations In this section, we evaluate the effectiveness of the proposed approach, in terms of question analysis and answer generation. 4.1 Data Set and Experimental Settings We employed the product review dataset used in Yu et al. (2011) as corpus. As illustrated in Table 1, the dataset contained 70,359 reviews about 11 popular products in four domains. In addition, we created 220 questions for these products by referring to real questions in Yahoo!Anwser service. We corrected the typos and grammar errors for these real questions. Each product contains 15 opinion questions and 5 factual questions, respectively. All questions were shown in Appendix C in supplementary material. Three annotators were invited to generate the gold standard. Each question was labeled by two annotators. The labels include product name, product aspec</context>
<context position="30100" citStr="Yu et al. (2011)" startWordPosition="4870" endWordPosition="4873">hierarchical classification based on the hierarchy, which can leverage the prior knowledge encoded in the hierarchy to filter out the noise and obtain accurate aspects. Moreover, we evaluated the effectiveness of our approach on implicit aspect identification. The 70 implicit aspect questions in our question corpus were used here. The method proposed by Su et al. (2008) was reimplemented as the baseline. It identifies implicit aspects by mutual clustering, and it was Figure 3: Evaluations on multiple optimization criteria in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, respectively. evaluated in Yu et al. (2011). As shown in Table 4, our approach significantly outperforms Su’s method by over 9.1% in terms of average F1-measure. The results show that the hierarchy can help to identify implicit aspects by exploiting the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.043 0.049 Lloret’s method 0.149 0.058 0.065 Table 5: Performance of answer generation. T-Test, pvalues&lt;0.05. 4.3 Evaluations on Answer Generation 4.3.1 Comparisons to the State-of-the-Arts We compared our multi-criteria optimization approach agai</context>
<context position="33505" citStr="Yu et al. (2011)" startWordPosition="5422" endWordPosition="5425">ary of relevant review sentences is then presented in the answer. The answer diversely comments the asked aspect and all its available sub-aspects following the general-to-specific logic. Moreover, we feel that the answers are informative and readable. 5 Related Works In this section, we review existing works related to the four components of our approach, including organization of reviews, question analysis, answer fragment retrieval, and answer generation. For organization of reviews, Carenini et al. (2006) proposed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The pape</context>
</contexts>
<marker>Yu, Zha, Wang, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z.J. Zha, M. Wang, K. Wang and T.S. Chua. Domain-Assisted Product Aspect Hierarchy Generation: Towards Hierarchical Organization of Unstructured Consumer Reviews. EMNLP, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z J Zha</author>
<author>M Wang</author>
<author>T S Chua</author>
</authors>
<title>Hierarchical Organization of Unstructured Consumer Reviews.</title>
<date>2011</date>
<location>WWW,</location>
<contexts>
<context position="5780" citStr="Yu et al., 2011" startWordPosition="890" endWordPosition="893">nderstanding the answers. To overcome these problems, we can resort to the hierarchical organization of consumer reviews on products. As illustrated in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. Such hierarchy can naturally facilitate to identify aspects asked in questions. While explicit aspects can be recognized by referring to the hierarchy, implicit aspects can be inferred based on the associations between sentiment terms and aspects in the hierarchy (Yu et al., 2011). The sentiment terms are discovered from the reviews on corresponding aspects. Moreover, by following the parent-child relations in the hierarchy, sub-aspects of the asked aspect can be directly acquired, and the answers can present aspects from general to specific. Motivated by the above observations, we propose to exploit the hierarchical organization of consumer reviews for product opinion-QA. As illustrated in Figure 1, our framework first organizes consumer reviews of a certain product into a hierarchical organization. The resulting hierarchy is in turn used to help question analysis and</context>
<context position="8862" citStr="Yu et al. (2011)" startWordPosition="1361" endWordPosition="1364">inions on a product or some aspects of the product. The task is to retrieve the opinionated review fragments relevant to the asked product/product aspects, and summarize these fragments to form an appropriate answer to question q. Next, we introduce the components of hierarchical organization that organizes consumer reviews into a hierarchy, question analysis which identifies the products/aspects and opinions asked in the questions, and answer fragment retrieval that retrieves review fragments relevant to the questions. 2.1 Hierarchical Organization of Reviews We employ the method proposed by Yu et al. (2011) to organize consumer reviews of a product into a hierarchical organization. As shown in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. In particular, this method first automatically acquires an initial aspect hierarchy from the domain knowledge and identifies aspects commented in the reviews. It then incrementally inserts the identified aspects into appropriate positions in the initial hierarchy, and finally obtains an aspect hierarchy that allocates all the newly identified aspects. The consumer reviews are then organized to their correspo</context>
<context position="11726" citStr="Yu et al., 2011" startWordPosition="1799" endWordPosition="1802">n resort to the review hierarchy. The hierarchy has organized product aspects, which can be used to filter the noise noun phrases for accurately identifying the explicit aspects. For the implicit aspects, we observe they are usually modified by some peculiar sentiment terms (Su et al., 2008). For example, the aspect “size” is often modified by the sentiment terms such as “large”, but seldom by the terms such as “expensive.” Thus, there are some associations between the aspects and sentiment terms. Such associations can be learned from the hierarchy and leveraged to infer the implicit aspects (Yu et al., 2011). In order to simultaneously identify the (explicit/implicit) aspects, we adopt a hierarchical classification technique. The technique simultaneously learns to identify explicit aspects, and discovers the associations between aspects and sentiment terms by multiple classifiers. In particular, given a testing question, we identify its aspect by hierarchically classify (Silla et al., 2011) it into the appropriate aspect node of a particular product hierarchy. The classification greedily searches a path in the hierarchy from top to down. The search begins at the root node, and stops at the leaf n</context>
<context position="24822" citStr="Yu et al. (2011)" startWordPosition="4039" endWordPosition="4042">, q* and q� are the gold standard and predicted sequence of sentences, respectively, p denotes a set of sentences, *(·) is the feature function on the whole feature space (i.e. E ii(·)), τ(·, ·) is a Kendall’s tau lost function (Lapata et al., 2006), T(·, ·) represents the number of inversion operations that needs to bring q� to q*, and m denotes the number of sentences. 4 Evaluations In this section, we evaluate the effectiveness of the proposed approach, in terms of question analysis and answer generation. 4.1 Data Set and Experimental Settings We employed the product review dataset used in Yu et al. (2011) as corpus. As illustrated in Table 1, the dataset contained 70,359 reviews about 11 popular products in four domains. In addition, we created 220 questions for these products by referring to real questions in Yahoo!Anwser service. We corrected the typos and grammar errors for these real questions. Each product contains 15 opinion questions and 5 factual questions, respectively. All questions were shown in Appendix C in supplementary material. Three annotators were invited to generate the gold standard. Each question was labeled by two annotators. The labels include product name, product aspec</context>
<context position="30100" citStr="Yu et al. (2011)" startWordPosition="4870" endWordPosition="4873">hierarchical classification based on the hierarchy, which can leverage the prior knowledge encoded in the hierarchy to filter out the noise and obtain accurate aspects. Moreover, we evaluated the effectiveness of our approach on implicit aspect identification. The 70 implicit aspect questions in our question corpus were used here. The method proposed by Su et al. (2008) was reimplemented as the baseline. It identifies implicit aspects by mutual clustering, and it was Figure 3: Evaluations on multiple optimization criteria in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, respectively. evaluated in Yu et al. (2011). As shown in Table 4, our approach significantly outperforms Su’s method by over 9.1% in terms of average F1-measure. The results show that the hierarchy can help to identify implicit aspects by exploiting the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.043 0.049 Lloret’s method 0.149 0.058 0.065 Table 5: Performance of answer generation. T-Test, pvalues&lt;0.05. 4.3 Evaluations on Answer Generation 4.3.1 Comparisons to the State-of-the-Arts We compared our multi-criteria optimization approach agai</context>
<context position="33505" citStr="Yu et al. (2011)" startWordPosition="5422" endWordPosition="5425">ary of relevant review sentences is then presented in the answer. The answer diversely comments the asked aspect and all its available sub-aspects following the general-to-specific logic. Moreover, we feel that the answers are informative and readable. 5 Related Works In this section, we review existing works related to the four components of our approach, including organization of reviews, question analysis, answer fragment retrieval, and answer generation. For organization of reviews, Carenini et al. (2006) proposed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The pape</context>
</contexts>
<marker>Yu, Zha, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. Hierarchical Organization of Unstructured Consumer Reviews. WWW, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yu</author>
<author>Z J Zha</author>
<author>M Wang</author>
<author>T S Chua</author>
</authors>
<title>Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews.</title>
<date>2011</date>
<publisher>ACL,</publisher>
<contexts>
<context position="5780" citStr="Yu et al., 2011" startWordPosition="890" endWordPosition="893">nderstanding the answers. To overcome these problems, we can resort to the hierarchical organization of consumer reviews on products. As illustrated in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. Such hierarchy can naturally facilitate to identify aspects asked in questions. While explicit aspects can be recognized by referring to the hierarchy, implicit aspects can be inferred based on the associations between sentiment terms and aspects in the hierarchy (Yu et al., 2011). The sentiment terms are discovered from the reviews on corresponding aspects. Moreover, by following the parent-child relations in the hierarchy, sub-aspects of the asked aspect can be directly acquired, and the answers can present aspects from general to specific. Motivated by the above observations, we propose to exploit the hierarchical organization of consumer reviews for product opinion-QA. As illustrated in Figure 1, our framework first organizes consumer reviews of a certain product into a hierarchical organization. The resulting hierarchy is in turn used to help question analysis and</context>
<context position="8862" citStr="Yu et al. (2011)" startWordPosition="1361" endWordPosition="1364">inions on a product or some aspects of the product. The task is to retrieve the opinionated review fragments relevant to the asked product/product aspects, and summarize these fragments to form an appropriate answer to question q. Next, we introduce the components of hierarchical organization that organizes consumer reviews into a hierarchy, question analysis which identifies the products/aspects and opinions asked in the questions, and answer fragment retrieval that retrieves review fragments relevant to the questions. 2.1 Hierarchical Organization of Reviews We employ the method proposed by Yu et al. (2011) to organize consumer reviews of a product into a hierarchical organization. As shown in Figure 2, the hierarchy organizes product aspects as nodes, following their parent-child relations. In particular, this method first automatically acquires an initial aspect hierarchy from the domain knowledge and identifies aspects commented in the reviews. It then incrementally inserts the identified aspects into appropriate positions in the initial hierarchy, and finally obtains an aspect hierarchy that allocates all the newly identified aspects. The consumer reviews are then organized to their correspo</context>
<context position="11726" citStr="Yu et al., 2011" startWordPosition="1799" endWordPosition="1802">n resort to the review hierarchy. The hierarchy has organized product aspects, which can be used to filter the noise noun phrases for accurately identifying the explicit aspects. For the implicit aspects, we observe they are usually modified by some peculiar sentiment terms (Su et al., 2008). For example, the aspect “size” is often modified by the sentiment terms such as “large”, but seldom by the terms such as “expensive.” Thus, there are some associations between the aspects and sentiment terms. Such associations can be learned from the hierarchy and leveraged to infer the implicit aspects (Yu et al., 2011). In order to simultaneously identify the (explicit/implicit) aspects, we adopt a hierarchical classification technique. The technique simultaneously learns to identify explicit aspects, and discovers the associations between aspects and sentiment terms by multiple classifiers. In particular, given a testing question, we identify its aspect by hierarchically classify (Silla et al., 2011) it into the appropriate aspect node of a particular product hierarchy. The classification greedily searches a path in the hierarchy from top to down. The search begins at the root node, and stops at the leaf n</context>
<context position="24822" citStr="Yu et al. (2011)" startWordPosition="4039" endWordPosition="4042">, q* and q� are the gold standard and predicted sequence of sentences, respectively, p denotes a set of sentences, *(·) is the feature function on the whole feature space (i.e. E ii(·)), τ(·, ·) is a Kendall’s tau lost function (Lapata et al., 2006), T(·, ·) represents the number of inversion operations that needs to bring q� to q*, and m denotes the number of sentences. 4 Evaluations In this section, we evaluate the effectiveness of the proposed approach, in terms of question analysis and answer generation. 4.1 Data Set and Experimental Settings We employed the product review dataset used in Yu et al. (2011) as corpus. As illustrated in Table 1, the dataset contained 70,359 reviews about 11 popular products in four domains. In addition, we created 220 questions for these products by referring to real questions in Yahoo!Anwser service. We corrected the typos and grammar errors for these real questions. Each product contains 15 opinion questions and 5 factual questions, respectively. All questions were shown in Appendix C in supplementary material. Three annotators were invited to generate the gold standard. Each question was labeled by two annotators. The labels include product name, product aspec</context>
<context position="30100" citStr="Yu et al. (2011)" startWordPosition="4870" endWordPosition="4873">hierarchical classification based on the hierarchy, which can leverage the prior knowledge encoded in the hierarchy to filter out the noise and obtain accurate aspects. Moreover, we evaluated the effectiveness of our approach on implicit aspect identification. The 70 implicit aspect questions in our question corpus were used here. The method proposed by Su et al. (2008) was reimplemented as the baseline. It identifies implicit aspects by mutual clustering, and it was Figure 3: Evaluations on multiple optimization criteria in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, respectively. evaluated in Yu et al. (2011). As shown in Table 4, our approach significantly outperforms Su’s method by over 9.1% in terms of average F1-measure. The results show that the hierarchy can help to identify implicit aspects by exploiting the underlying associations among sentiment terms and aspects. Methods ROUGE1 ROUGE2 ROUGE-SU4 Our method 0.364* 0.137* 0.138* Li’s method 0.127 0.043 0.049 Lloret’s method 0.149 0.058 0.065 Table 5: Performance of answer generation. T-Test, pvalues&lt;0.05. 4.3 Evaluations on Answer Generation 4.3.1 Comparisons to the State-of-the-Arts We compared our multi-criteria optimization approach agai</context>
<context position="33505" citStr="Yu et al. (2011)" startWordPosition="5422" endWordPosition="5425">ary of relevant review sentences is then presented in the answer. The answer diversely comments the asked aspect and all its available sub-aspects following the general-to-specific logic. Moreover, we feel that the answers are informative and readable. 5 Related Works In this section, we review existing works related to the four components of our approach, including organization of reviews, question analysis, answer fragment retrieval, and answer generation. For organization of reviews, Carenini et al. (2006) proposed to organize the reviews by a hand-crafted taxonomy, which was not scalable. Yu et al. (2011) exploited the domain knowledge and consumer reviews to automatically generate a hierarchy for organizing consumer reviews. Question analysis often has to distinguish the opinion question from the factual one, and find the key points asked in the questions, such as the product aspect and product name. For example, Yu et al. (2003) proposed to separate opinions from facts at both document and sentence level, and determine the polarity on the opinionated sentences in the answer documents. Similarly, Somasundaran et al. (2007) utilized a SVM classifier to recognize opinionated sentences. The pape</context>
</contexts>
<marker>Yu, Zha, Wang, Chua, 2011</marker>
<rawString>J. Yu, Z.J. Zha, M. Wang and T.S. Chua. Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews. ACL, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<publisher>EMNLP,</publisher>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. EMNLP, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
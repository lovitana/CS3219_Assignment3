<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000134">
<title confidence="0.972811">
A Systematic Comparison of Phrase Table Pruning Techniques
</title>
<author confidence="0.94132">
Richard Zens and Daisy Stanton and Peng Xu
</author>
<affiliation confidence="0.893031">
Google Inc.
</affiliation>
<email confidence="0.99695">
{zens,daisy,xp}@google.com
</email>
<sectionHeader confidence="0.996627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999598">
When trained on very large parallel corpora,
the phrase table component of a machine
translation system grows to consume vast
computational resources. In this paper, we in-
troduce a novel pruning criterion that places
phrase table pruning on a sound theoretical
foundation. Systematic experiments on four
language pairs under various data conditions
show that our principled approach is superior
to existing ad hoc pruning methods.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998896714285714">
Over the last years, statistical machine translation
has become the dominant approach to machine
translation. This is not only due to improved mod-
eling, but also due to a significant increase in the
availability of monolingual and bilingual data. Here
are just two examples of very large data resources
that are publicly available:
</bodyText>
<listItem confidence="0.975640857142857">
• The Google Web 1T 5-gram corpus available
from the Linguistic Data Consortium consist-
ing of the 5-gram counts of about one trillion
words of web data.1
• The 109-French-English bilingual corpus with
about one billion tokens from the Workshop on
Statistical Machine Translation (WMT).2
</listItem>
<bodyText confidence="0.9683905">
These enormous data sets yield translation models
that are expensive to store and process. Even with
</bodyText>
<footnote confidence="0.988323">
1LDC catalog No. LDC2006T13
2http://www.statmt.org/wmt11/translation-task.html
</footnote>
<bodyText confidence="0.999673793103448">
modern computers, these large models lead to a long
experiment cycle that hinders progress. The situa-
tion is even more severe if computational resources
are limited, for instance when translating on hand-
held devices. Then, reducing the model size is of
the utmost importance.
The most resource-intensive components of a sta-
tistical machine translation system are the language
model and the phrase table. Recently, compact rep-
resentations of the language model have attracted
the attention of the research community, for instance
in Talbot and Osborne (2007), Brants et al. (2007),
Pauls and Klein (2011) or Heafield (2011), to name
a few. In this paper, we address the other problem
of any statistical machine translation system: large
phrase tables.
Johnson et al. (2007) has shown that large por-
tions of the phrase table can be removed without loss
in translation quality. This motivated us to perform
a systematic comparison of different pruning meth-
ods. However, we found that many existing methods
employ ad-hoc heuristics without theoretical foun-
dation.
The pruning criterion introduced in this work is
inspired by the very successful and still state-of-the-
art language model pruning criterion based on en-
tropy measures (Stolcke, 1998). We motivate its
derivation by stating the desiderata for a good phrase
table pruning criterion:
</bodyText>
<listItem confidence="0.99497">
• Soundness: The criterion should optimize
some well-understood information-theoretic
measure of translation model quality.
</listItem>
<page confidence="0.959096">
972
</page>
<note confidence="0.798137">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 972–983, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.968045666666667">
• Efficiency: Pruning should be fast, i. e., run lin-
early in the size of the phrase table.
• Self-containedness: As a practical considera-
tion, we want to prune phrases from an existing
phrase table. This means pruning should use
only information contained in the model itself.
• Good empirical behavior: We would like to
be able to prune large parts of the phrase table
without significant loss in translation quality.
</listItem>
<bodyText confidence="0.996859857142857">
Analyzing existing pruning techniques based on
these objectives, we found that they are commonly
deficient in at least one of them. We thus designed
a novel pruning criterion that not only meets these
objectives, it also performs very well in empirical
evaluations.
The novel contributions of this paper are:
</bodyText>
<listItem confidence="0.988146333333333">
1. a systematic description of existing phrase table
pruning methods.
2. a new, theoretically sound phrase table pruning
criterion.
3. an experimental comparison of several pruning
methods for several language pairs.
</listItem>
<sectionHeader confidence="0.999176" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999979270270271">
The most basic pruning methods rely on probabil-
ity and count cutoffs. We will cover the techniques
that are implemented in the Moses toolkit (Koehn et
al., 2007) and the Pharaoh decoder (Koehn, 2004) in
Section 3. We are not aware of any work that ana-
lyzes their efficacy in a systematic way. It is thus not
surprising that some of them perform poorly, as our
experimental results will show.
The work of Johnson et al. (2007) is promis-
ing as it shows that large parts of the phrase ta-
ble can be removed without affecting translation
quality. Their pruning criterion relies on statisti-
cal significance tests. However, it is unclear how
this significance-based pruning criterion is related to
translation model quality. Furthermore, a compari-
son to other methods is missing. Here we close this
gap and perform a systematic comparison. The same
idea of significance-based pruning was exploited in
(Yang and Zheng, 2009; Tomeh et al., 2009) for hi-
erarchical statistical machine translation.
A different approach to phrase table pruning was
undertaken by Eck et al. (2007a; 2007b). They rely
on usage statistics from translating sample data, so it
is not self-contained. However, it could be combined
with the methods proposed here.
Another approach to phrase table pruning is trian-
gulation (Chen et al., 2008; Chen et al., 2009). This
requires additional bilingual corpora, namely from
the source language as well as from the target lan-
guage to a third bridge language. In many situations
this does not exist or would be costly to generate.
Duan et al. (2011), Sanchis-Trilles et al. (2011)
and Tomeh et al. (2011) modify the phrase extrac-
tion methods in order to reduce the phrase table size.
The work in this paper is independent of the way the
phrase extraction is done, so those approaches are
complementary to our work.
</bodyText>
<sectionHeader confidence="0.984275" genericHeader="method">
3 Pruning Using Simple Statistics
</sectionHeader>
<bodyText confidence="0.9997668">
In this section, we will review existing pruning
methods based on simple phrase table statistics.
There are two common classes of these methods: ab-
solute phrase table pruning and relative phrase table
pruning.
</bodyText>
<subsectionHeader confidence="0.999765">
3.1 Absolute pruning
</subsectionHeader>
<bodyText confidence="0.9994395">
Absolute pruning methods rely only on the statistics
of a single phrase pair (f, e). Hence, they are in-
dependent of other phrases in the phrase table. As
opposed to relative pruning methods (Section 3.2),
they may prune all translations of a source phrase.
Their application is easy and efficient.
</bodyText>
<listItem confidence="0.9968165">
• Count-based pruning. This method prunes
a phrase pair ( f, e) if its observation count
</listItem>
<equation confidence="0.677318">
N(�f, e) is below a threshold τ,:
N(f, e) &lt; τ� (1)
</equation>
<listItem confidence="0.997392">
• Probability-based pruning. This method
prunes a phrase pair (�f, e) if its probability is
below a threshold τp:
</listItem>
<equation confidence="0.985955">
p(E|�f) &lt; τp (2)
</equation>
<bodyText confidence="0.819255">
Here the probability p(e |f) is estimated via rel-
ative frequencies.
</bodyText>
<page confidence="0.99813">
973
</page>
<subsectionHeader confidence="0.997661">
3.2 Relative pruning
</subsectionHeader>
<bodyText confidence="0.999504">
A potential problem with the absolute pruning meth-
ods is that it can prune all occurrences of a source
phrase f.3 Relative pruning methods avoid this by
considering the full set of target phrases for a spe-
cific source phrase �f.
</bodyText>
<listItem confidence="0.9946882">
• Threshold pruning. This method discards
those phrases that are far worse than the best
target phrase for a given source phrase f. Given
a pruning threshold τt, a phrase pair ( f, e) is
discarded if:
</listItem>
<equation confidence="0.959079">
{ j
p(�e |�f) (3)
</equation>
<listItem confidence="0.9224774">
• Histogram pruning. An alternative to thresh-
old pruning is histogram pruning. For each
source phrase f, this method preserves the K
target phrases with highest probability p(e |f)
or, equivalently, their count N( �f, �e).
</listItem>
<bodyText confidence="0.999951666666667">
Note that, except for count-based pruning, none of
the methods take the frequency of the source phrase
into account. As we will confirm in the empirical
evaluation, this will likely cause drops in translation
quality, since frequent source phrases are more use-
ful than the infrequent ones.
</bodyText>
<sectionHeader confidence="0.974731" genericHeader="method">
4 Significance Pruning
</sectionHeader>
<bodyText confidence="0.996122714285714">
In this section, we briefly review significance prun-
ing following Johnson et al. (2007). The idea of sig-
nificance pruning is to test whether a source phrase
f and a target phrase e� co-occur more frequently in
a bilingual corpus than they should just by chance.
Using some simple statistics derived from the bilin-
gual corpus, namely
</bodyText>
<listItem confidence="0.998680333333333">
• N(f) the count of the source phrase f
• N(e) the count of the target phrase e�
• N(f, e) the co-occurence count of the source
phrase f and the target phrase e�
• N the number of sentences in the bilingual cor-
pus
</listItem>
<footnote confidence="0.5597865">
3Note that it has never been systematically investigated
whether this is a real problem or just speculation.
</footnote>
<bodyText confidence="0.9395222">
we can compute the two-by-two contingency table
in Table 1.
Following Fisher’s exact test, we can calculate the
probability of the contingency table via the hyperge-
ometric distribution:
</bodyText>
<equation confidence="0.9968856">
ph(N( �f, �e)) _ (4)
( N(˜f) l( N−N( ˜f) l
N( ˜f,˜e) N(˜e)−N( ˜f,˜e)
( N(˜e) l
·
</equation>
<bodyText confidence="0.9993688">
The p-value is then calculated as the sum of all
probabilities that are at least as extreme. The lower
the p-value, the less likely this phrase pair occurred
with the observed frequency by chance; we thus
prune a phrase pair ( f, e) if:
</bodyText>
<equation confidence="0.99975">
�ph(k) �&gt; τF (5)
</equation>
<bodyText confidence="0.99864125">
for some pruning threshold τF. More details of this
approach can be found in Johnson et al. (2007). The
idea of using Fisher’s exact test was first explored by
Moore (2004) in the context of word alignment.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="method">
5 Entropy-based Pruning
</sectionHeader>
<bodyText confidence="0.9993375">
In this section, we will derive a novel entropy-based
pruning criterion.
</bodyText>
<subsectionHeader confidence="0.997187">
5.1 Motivational Example
</subsectionHeader>
<bodyText confidence="0.999971473684211">
In general, pruning the phrase table can be consid-
ered as selecting a subset of the original phrase table.
When doing so, we would like to alter the original
translation model distribution as little as possible.
This is a key difference to previous approaches: Our
goal is to remove redundant phrases, whereas previ-
ous approaches usually try to remove low-quality or
unreliable phrases. We believe this to be an advan-
tage of our method as it is certainly easier to measure
the redundancy of phrases than it is to estimate their
quality.
In Table 2, we show some example phrases
from the learned French-English WMT phrase table,
along with their counts and probabilities. For the
French phrase le gouvernement franc¸ais, we have,
among others, two translations: the French govern-
ment and the government of France. If we have
to prune one of those translations, we can ask our-
selves: how would the translation cost change if the
</bodyText>
<equation confidence="0.969298222222222">
�f) &lt; τt · max
e˜
p(�e|
�
�
00
E
˜f,˜e)
k=N(
</equation>
<page confidence="0.990725">
974
</page>
<table confidence="0.981992333333333">
N(�f, e) N(�f) − N(�f, e) N(f)
N(e) − N( f, e) N − N(f) − N(e) + N( f, e) N − N(�f)
N(e) N − N(e) N
</table>
<tableCaption confidence="0.998913">
Table 1: Two-by-two contingency table for a phrase pair ( 1, g).
</tableCaption>
<table confidence="0.998898714285714">
Source Phrase f Target Phrase e� N(f, e) p(�e|�f)
le the 7.6M 0.7189
gouvernement government 245 K 0.4106
franc¸ais French 51 K 0.6440
of France 695 0.0046
le gouvernement franc¸ais the French government 148 0.1686
the government of France 11 0.0128
</table>
<tableCaption confidence="0.999237">
Table 2: Example phrases from the French-English phrase table (K=thousands, M=millions).
</tableCaption>
<bodyText confidence="0.999870347826087">
same translation were generated from the remain-
ing, shorter, phrases? Removing the phrase the gov-
ernment of France would increase this cost dramat-
ically. Given the shorter phrases from the table, the
probability would be 0.7189 · 0.4106 · 0.0046 =
0.0014*, which is about an order of a magnitude
smaller than the original probability of 0.0128.
On the other hand, composing the phrase the
French government out of shorter phrases has prob-
ability 0.7189 · 0.4106 · 0.6440 = 0.1901, which is
very close to the original probability of 0.1686. This
means it is safe to discard the phrase the French gov-
ernment, since the translation cost remains essen-
tially unchanged. By contrast, discarding the phrase
the government of France does not have this effect:
it leads to a large change in translation cost.
Note that here the pruning criterion only considers
redundancy of the phrases, not the quality. Thus, we
are not saying that the government of France is a
better translation than the French government, only
that it is less redundant.
*We use the assumption that we can simply multiply the
probabilities of the shorter phrases.
</bodyText>
<subsectionHeader confidence="0.993839">
5.2 Entropy Criterion
</subsectionHeader>
<bodyText confidence="0.999846857142857">
Now, we are going to formalize the notion of re-
dundancy. We would like the pruned model p&apos;(e |f)
to be as similar as possible to the original model
p(�e |�f). We use conditional Kullback-Leibler di-
vergence, also called conditional relative entropy
(Cover and Thomas, 2006), to measure the model
similarity:
</bodyText>
<equation confidence="0.988576">
D(p(�e |h|p�(�e|�f))
� f) log pM f) 1 (6)
p�(�e|�f)
l ]
�f) log p(�e |�f) − log p�(�e |�f) (7)
</equation>
<bodyText confidence="0.9999373">
Computing the best pruned model of a given size
would require optimizing over all subsets with that
size. Since that is computationally infeasible, we in-
stead apply the equivalent approximation that Stol-
cke (1998) uses for language modeling. This as-
sumes that phrase pairs affect the relative entropy
roughly independently.
We can then choose a pruning threshold τE and
prune those phrase pairs with a contribution to the
relative entropy below that threshold. Thus, we
</bodyText>
<figure confidence="0.747075833333333">
E= E
f p( �f) p(�e|
e
=E
f,e
p(e,
</figure>
<page confidence="0.986721">
975
</page>
<bodyText confidence="0.982587">
prune a phrase pair ( f, e), if
</bodyText>
<equation confidence="0.99175">
p(e, �f) log p(�e |�f) − log p0(�e |�f) &lt; τE (8)
[ ]
</equation>
<bodyText confidence="0.984953090909091">
We now address how to assign the probability
p0(�e |f) under the pruned model. A phrase-based
system selects among different segmentations of the
source language sentence into phrases. If a segmen-
tation into longer phrases does not exist, the system
has to compose a translation out of shorter phrases.
Thus, if a phrase pair ( f, e) is no longer available,
the decoder has to use shorter phrases to produce
the same translation. We can therefore decompose
the pruned model score p0(e |f) by summing over all
segmentations sK1 and all reorderings πK1 :
</bodyText>
<equation confidence="0.964666">
p(sK1 ,πK1 |�f) · p(�e|sK1 ,πK1 ,�f) (9)
</equation>
<bodyText confidence="0.9998895">
Here the segmentation sK 1 divides both the source
and target phrases into K sub-phrases:
</bodyText>
<equation confidence="0.861959">
f = �fπ1... �fπK and e� = 61...�eK (10)
</equation>
<bodyText confidence="0.998644">
The permutation πK 1 describes the alignment of
those sub-phrases, such that the sub-phrase ek is
aligned to �fπk. Using the normal phrase translation
model, we obtain:
</bodyText>
<equation confidence="0.9844285">
1:
p0(�e |�f) =
K K
s1 π1
</equation>
<bodyText confidence="0.999110727272727">
Virtually all phrase-based decoders use the so-
called maximum-approximation, i. e. the sum is re-
placed with the maximum. As we would like the
pruning criterion to be similar to the search criterion
used during decoding, we do the same and obtain:
model. However, here the target side is constrained
to the given phrase e.
It can happen that a phrase is not compositional,
i. e., we cannot find a segmentation into shorter
phrases. In these cases, we assign a small, constant
probability:
</bodyText>
<equation confidence="0.998173">
p0(Wf) = pc (13)
</equation>
<bodyText confidence="0.9998335">
We found that the value pc = e−10 works well for
many language pairs.
</bodyText>
<subsectionHeader confidence="0.998253">
5.3 Computation
</subsectionHeader>
<bodyText confidence="0.999974913043478">
In our experiments, it was more efficient to vary the
pruning threshold τE without having to re-compute
the entire phrase table. Therefore, we computed the
entropy criterion in Equation (8) once for the whole
phrase table. This introduces an approximation for
the pruned model score p0(e |f). It might happen
that we prune short phrases that were used as part
of the best segmentation of longer phrases. As these
shorter phrases should not be available, the pruned
model score might be inaccurate. Although we be-
lieve this effect is minor, we leave a detailed experi-
mental analysis for future work.
One way to avoid this approximation would be
to perform entropy pruning with increasing phrase
length. Starting with one-word phrases, which are
trivially non-compositional, the entropy criterion
would be straightforward to compute. Proceed-
ing to two-word phrases, one would decompose the
phrases into sub-phrases by looking up the proba-
bilities of some of the unpruned one-word phrases.
Once the set of unpruned two-word phrases was ob-
tained, one would continue with three-word phrases,
etc.
</bodyText>
<sectionHeader confidence="0.989793" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<equation confidence="0.985463375">
1:
p0(�e |�f) =
K K
s1 π1
K
p(sK1 , πK1  |f) H p(4 |fπk) (11)
k=1
p(�ek |fπk) (12)
</equation>
<bodyText confidence="0.999959">
Note that we also drop the segmentation probabil-
ity, as this is not used at decoding time. This leaves
the pruning criterion a function only of the model
p(�e |�f) as stored in the phrase table. There is no need
for a special development or adaptation set. We can
determine the best segmentation using dynamic pro-
gramming, similar to decoding with a phrase-based
</bodyText>
<subsectionHeader confidence="0.999505">
6.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999975">
In this section, we describe the data sets used for the
experiments. We perform experiments on the pub-
licly available WMT shared translation task for the
following four language pairs:
</bodyText>
<listItem confidence="0.999846333333333">
• German-English
• Czech-English
• Spanish-English
</listItem>
<equation confidence="0.985112875">
�
f) ^ max
K K
s1 π1
p0(�e|
K
H
k=1
</equation>
<page confidence="0.991394">
976
</page>
<table confidence="0.999736833333333">
Number of Words
Language Pair Foreign English
German - English 42 M 45M
Czech - English 56M 65M
Spanish - English 232 M 210M
French - English 962 M 827 M
</table>
<tableCaption confidence="0.979505">
Table 3: Training data statistics. Number of words in the
training data (M=millions).
</tableCaption>
<listItem confidence="0.865575">
• French-English
</listItem>
<bodyText confidence="0.9943346">
For each pair, we train two separate system, one for
each direction. Thus it can happen that a phrase is
pruned for X-to-Y, but not for Y-to-X.
These four language pairs represent a nice range
of training corpora sizes, as shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.99753">
6.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.999921896551724">
Pruning experiments were performed on top of the
following baseline system. We used a phrase-
based statistical machine translation system similar
to (Zens et al., 2002; Koehn et al., 2003; Och and
Ney, 2004; Zens and Ney, 2008). We trained a 4-
gram language model on the target side of the bilin-
gual corpora and a second 4-gram language model
on the provided monolingual news data. All lan-
guage models used Kneser-Ney smoothing.
The baseline system uses the common phrase
translation models, such as p(El�f) and p( 1|E), lex-
ical models, word and phrase penalty, distortion
penalty as well as a lexicalized reordering model
(Zens and Ney, 2006).
The word alignment was trained with six itera-
tions of IBM model 1 (Brown et al., 1993) and 6 it-
erations of the HMM alignment model (Vogel et al.,
1996) using a symmetric lexicon (Zens et al., 2004).
The feature weights were tuned on a development
set by applying minimum error rate training (MERT)
under the Bleu criterion (Och, 2003; Macherey et al.,
2008). We ran MERT once with the full phrase table
and then kept the feature weights fixed, i. e., we did
not rerun MERT after pruning to avoid adding un-
necessary noise. We extract phrases up to a length
of six words. The baseline system already includes
phrase table pruning by removing singletons and
keeping up to 30 target language phrases per source
phrase. We found that this does not affect transla-
</bodyText>
<figure confidence="0.995428">
1 2 4 8
Number of Phrases [millions]
</figure>
<figureCaption confidence="0.962165">
Figure 1: Comparison of probability-based pruning
methods for German-English.
</figureCaption>
<bodyText confidence="0.9889855">
tion quality significantly4. All pruning experiments
are done on top of this.
</bodyText>
<subsectionHeader confidence="0.619926">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999922">
In this section, we present the experimental results.
Translation results are reported on the WMT’07
news commentary blind set.
We will show translation quality measured with
the Bleu score (Papineni et al., 2002) as a function
of the phrase table size (number of phrases). Being
in the upper left corner of these figures is desirable.
First, we show a comparison of several
probability-based pruning methods in Figure 1.
We compare
</bodyText>
<listItem confidence="0.9646755">
• Prob. Absolute pruning based on Eq. (2).
• Thres. Threshold pruning based on Eq. (3).
• Hist. Histogram pruning as described in Sec-
tion 3.2.5
</listItem>
<bodyText confidence="0.9596">
We observe that these three methods perform
equally well. There is no difference between abso-
lute and relative pruning methods, except that the
two relative methods (Thres and Hist) are limited by
</bodyText>
<footnote confidence="0.936721125">
4The Bleu score drops are as follows: English-French 0.3%,
French-English 0.4%, Czech-English 0.3%, all other are less
than 0.1%.
5Instead of using p(e |f) one could use the weighted model
score including p(�f|e), lexical weightings etc.; however, we
found that this does not give significantly different results; but
it does introduce a undesirable dependance between feature
weights and phrase table pruning.
</footnote>
<figure confidence="0.996887642857143">
Prob
Thres
Hist
BLEU[%] 28
26
24
22
20
18
16
14
12
10
8
</figure>
<page confidence="0.977572">
977
</page>
<bodyText confidence="0.97593925">
the number of source phrases. Thus, they reach a
point where they cannot prune the phrase table any
further. The results shown are for German-English;
the results for the other languages are very similar.
The results that follow use only the absolute prun-
ing method as a representative for probability-based
pruning.
In Figures 2 through 5, we show the transla-
tion quality as a function of the phrase table size.
We vary the pruning thresholds to obtain different
phrase table sizes. We compare four pruning meth-
ods:
</bodyText>
<listItem confidence="0.998582875">
• Count. Pruning based on the frequency of a
phrase pair, c.f. Equation (1).
• Prob. Pruning based on the absolute probabil-
ity of a phrase pair, c.f. Equation (2).
• Fisher. Pruning using significance tests, c.f.
Equation (5).
• Entropy. Pruning using the novel entropy cri-
terion, c.f. Equation (8).
</listItem>
<bodyText confidence="0.990503888888889">
Note that the x-axis of these figures is on a logarith-
mic scale, so the differences between the methods
can be quite dramatic. For instance, entropy pruning
requires less than a quarter of the number of phrases
needed by count- or significance-based pruning to
achieve a Spanish-English Bleu score of 34 (0.4 mil-
lion phrases compared to 1.7 million phrases).
These results clearly show how the pruning meth-
ods compare:
</bodyText>
<listItem confidence="0.994734333333333">
1. Probability-based pruning performs poorly. It
should be used only to prune small fractions of
the phrase table.
2. Count-based pruning and significance-based
pruning perform equally well. They are much
better than probability-based pruning.
3. Entropy pruning consistently outperforms the
other methods across translation directions and
language pairs.
</listItem>
<bodyText confidence="0.883585666666667">
Figures 6 and 7 show compositionality statistics
for the pruned Spanish-English phrase table (we ob-
served similar results for the other language pairs).
</bodyText>
<table confidence="0.9976734">
Total number of phrases 4137M
Compositional 3 970 M
Non-compositional 167 M
of those: one-word phrases 85M
no segmentation 82M
</table>
<tableCaption confidence="0.9820355">
Table 4: Statistics of phrase compositionality
(M=millions).
</tableCaption>
<bodyText confidence="0.999899756756757">
Each figure shows the composition of the phrase ta-
ble for a type of pruning for different phrase tables
sizes. Along the x-axis, we plotted the phrase ta-
ble size. These are the same phrase tables used to
obtain the Bleu scores in Figure 2 (left). The dif-
ferent shades of grey correspond to different phrase
lengths. For instance, in case of the smallest phrase
table for count-based pruning, the 1-word phrases
account for about 30% of all phrases, the 2-word
phrases account for about 35% of all phrases, etc.
With the exception of the probability-based prun-
ing, the plots look comparable. The more aggres-
sive the pruning, the larger the percentage of short
phrases. We observe that entropy-based pruning re-
moves many more long phrases than any of the other
methods. The plot for probability-based pruning is
different in that the percentage of long phrases ac-
tually increases with more aggressive pruning (i. e.
smaller phrase tables). A possible explanation is
that probability-based pruning does not take the fre-
quency of the source phrase into account. This
difference might explain the poor performance of
probability-based pruning.
To analyze how many phrases are compositional,
we collect statistics during the computation of the
entropy criterion. These are shown in Table 4, ac-
cumulated across all language pairs and all phrases,
i. e., including singleton phrases. We see that 96%
of all phrases are compositional (3 970 million out
of 4137 million phrases). Furthermore, out of
the 167 million non-compositional phrases, more
than half (85 million phrases), are trivially non-
compositional: they consist only of a single source
or target language word. The number of non-trivial
non-compositional phrases is, with 82 million or 2%
of the total number of phrases, very small.
In Figure 8, we show the effect of the constant
</bodyText>
<page confidence="0.997514">
978
</page>
<figureCaption confidence="0.999853">
Figure 5: Translation quality as a function of the phrase table size for German-English (left) and English-German
</figureCaption>
<figure confidence="0.9763854">
(right). 979
40
35
30
BLEU[%]
25
20
15
10
Prob
Count
Fisher
Entropy
0.01 0.1 1 10 100
Number of Phrases [M]
</figure>
<figureCaption confidence="0.94319">
Figure 2: Translation quality as a function of the phrase table size for Spanish-English (left) and English-Spanish
(right).
</figureCaption>
<figure confidence="0.998757021276596">
40
35
30
BLEU[%]
25
20
15
10
Prob
Count
Fisher
Entropy
40
35
30
BLEU[%]
25
20
15
Prob
Count
Fisher
Entropy
10
5
0.01 0.1 1 10 100
Number of Phrases [M]
38
36
34
32
BLEU[%]
30
28
26
24
22
20
18
Prob
Count
Fisher
Entropy
0.1 1 10 100 1000
Number of Phrases [M]
0.1 1 10 100 1000
Number of Phrases [M]
</figure>
<figureCaption confidence="0.99194">
Figure 3: Translation quality as a function of the phrase table size for French-English (left) and English-French (right).
</figureCaption>
<figure confidence="0.99850953125">
0.001 0.01 0.1 1 10 100
Number of Phrases [M]
26
24
22
20
18
16
14
12
10
8
6
Prob
Count
Fisher
Entropy
16
14
12
BLEU[%]
10
8
Prob
Count
Fisher
Entropy
6
4
0.001 0.01 0.1 1 10 100
Number of Phrases [M]
BLEU[%]
</figure>
<figureCaption confidence="0.998169">
Figure 4: Translation quality as a function of the phrase table size for Czech-English (left) and English-Czech (right).
</figureCaption>
<figure confidence="0.993527227272727">
0.001 0.01 0.1 1 10
Number of Phrases [M]
28
26
24
22
20
18
16
14
12
10
8
Prob
Count
Fisher
Entropy
20
18
16
14
12
10
8
6
4
2
0.001 0.01 0.1 1 10
Number of Phrases [M]
Prob
Count
Fisher
Entropy
BLEU[%]
BLEU[%]
Prob Count
1-word
2-word
3-word
4-word
5-word
6-word
100
80
60
40
20
0
0.01 0.1 1 10 100
Number of Phrases [millions]
Percentage of Phrases [%]
100
2-word
3-word
4-word
5-word
6-word
40
20
0
10 100
Number of Phrases [millions]
Percentage of Phrases [%]
1-word
80
60
</figure>
<figureCaption confidence="0.992845">
Figure 6: Phrase length statistics for Spanish-English for probability-based (left) and count-based pruning (right).
</figureCaption>
<figure confidence="0.662871666666667">
Fisher Entropy
100 1-word 100 1-word
Percentage of Phrases [%] 2-word Percentage of Phrases [%] 2-word
80 3-word 80 3-word
4-word 4-word
5-word 5-word
60 6-word 60 6-word
40 40
20 20
0 0
0.01 0.1 1 10 100 0.01 0.1 1 10 100
Number of Phrases [millions] Number of Phrases [millions]
</figure>
<figureCaption confidence="0.999821">
Figure 7: Phrase length statistics for Spanish-English for significance-based (left) and entropy-based pruning (right).
</figureCaption>
<bodyText confidence="0.999977888888889">
p, for non-compositional phrases.6 The results
shown are for Spanish-English; additional experi-
ments for the other languages and translation direc-
tions showed very similar results. Overall, there is
no big difference between the values. Hence, we
chose a value of 10 for all experiments.
The results in Figure 2 to Figure 5 show that
entropy-based pruning clearly outperforms the al-
ternative pruning methods. However, it is a bit
hard to see from the graphs exactly how much ad-
ditional savings it offers over other methods. In Ta-
ble 5, we show how much of the phrase table we
have to retain under various pruning criteria with-
out losing more than one Bleu point in translation
quality. We see that probability-based pruning al-
lows only for marginal savings. Count-based and
significance-based pruning results in larger savings
between 70% and 90%, albeit with fairly high vari-
</bodyText>
<footnote confidence="0.8366735">
6The values are in neg-log-space, i. e., a value of 10 corre-
spondstop = e−10.
</footnote>
<bodyText confidence="0.999347">
ability. Entropy-based pruning achieves consistently
high savings between 85% and 95% of the phrase ta-
ble. It always outperforms the other pruning meth-
ods and yields significant savings on top of count-
based or significance-based pruning methods. Of-
ten, we can cut the required phrase table size in half
compared to count or significance based pruning.
As a last experiment, we want to confirm that
phrase-table pruning methods are actually better
than simply reducing the maximum phrase length.
In Figure 9, we show a comparison of different
pruning methods and a length-based approach for
Spanish-English. For the ’Length’ curve, we first
drop all 6-word phrases, then all 5-word phrases, etc.
until we are left with only single-word phrases; the
phrase length is measured as the number of source
language words. We observe that entropy-based,
count-based and significance-based pruning indeed
outperform the length-based approach. We obtained
similar results for the other languages.
</bodyText>
<page confidence="0.990566">
980
</page>
<table confidence="0.9988922">
Method ES-EN EN-ES DE-EN EN-DE FR-EN EN-FR CS-EN EN-CS
Prob 77.3 % 82.7 % 61.2 % 67.3 % 84.8 % 94.1 % 85.6 % 86.3 %
Count 24.9 % 11.9 % 19.9 % 14.3 % 11.4 % 9.0 % 20.2 % 10.4 %
Fisher 23.5 % 12.6 % 21.7 % 14.0 % 14.5 % 13.6 % 31.9 % 9.9 %
Entropy 7.2 % 6.0 % 10.2 % 11.1 % 7.1 % 8.1 % 14.8 % 6.4 %
</table>
<tableCaption confidence="0.9834635">
Table 5: To what degree can we prune the phrase table without losing more than 1 Bleu point? The table shows
percentage of phrases that we have to retain. ES=Spanish, EN=English, FR=French, CS=Czech, DE=German.
</tableCaption>
<figure confidence="0.8772395">
0.01 0.1 1 10 100
Number of Phrases [M]
</figure>
<figureCaption confidence="0.977689">
Figure 8: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English for entropy pruning
with different constants p,.
</figureCaption>
<sectionHeader confidence="0.998606" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999967285714286">
Phrase table pruning is often addressed in an ad-hoc
way using the heuristics described in Section 3. We
have shown that some of those do not work well.
Choosing the wrong technique can result in sig-
nificant drops in translation quality without saving
much in terms of phrase table size. We introduced
a novel entropy-based criterion and put phrase ta-
ble pruning on a sound theoretical foundation. Fur-
thermore, we performed a systematic experimental
comparison of existing methods and the new entropy
criterion. The experiments were carried out for four
language pairs under small, medium and large data
conditions. We can summarize our conclusions as
follows:
</bodyText>
<listItem confidence="0.7460284">
• Probability-based pruning performs poorly
when pruning large parts of the phrase table.
This might be because it does not take the fre-
quency of the source phrase into account.
• Count-based pruning performs as well as
</listItem>
<figure confidence="0.9316245">
0.01 0.1 1 10 100
Number of Phrases [M]
</figure>
<figureCaption confidence="0.9963595">
Figure 9: Translation quality (Bleu) as a function of the
phrase table size for Spanish-English.
</figureCaption>
<bodyText confidence="0.817707">
significance-based pruning.
</bodyText>
<listItem confidence="0.826896333333333">
• Entropy-based pruning gives significantly
larger savings in phrase table size than any
other pruning method.
• Compared to previous work, the novel entropy-
based pruning often achieves the same Bleu
score with only half the number of phrases.
</listItem>
<sectionHeader confidence="0.998816" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999765454545455">
Currently, we take only the model p(e |f) into ac-
count when looking for the best segmentation. We
might obtain a better estimate by also consider-
ing the distortion costs, which penalize reordering.
We could also include other phrase models such as
p( �f|e) and the language model.
The entropy pruning criterion could be applied
to hierarchical machine translation systems (Chiang,
2007). Here, we might observe even larger reduc-
tions in phrase table size as there are many more en-
tries.
</bodyText>
<figure confidence="0.998607515151515">
BLEU[%]
38
36
34
32
30
28
26
24
5
10
15
20
25
30
50
BLEU[%]
38
36
34
32
30
28
26
24
22
20
18
Length
Prob
Count
Fisher
Entropy
</figure>
<page confidence="0.993062">
981
</page>
<sectionHeader confidence="0.982962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999869971428571">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858–
867, Prague, Czech Republic, June. Association for
Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311,
June.
Yu Chen, Andreas Eisele, and Martin Kay. 2008.
Improving statistical machine translation efficiency
by triangulation. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC’08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. In-
tersecting multilingual data for faster and better sta-
tistical translations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 128–136, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, New York,
NY, USA.
Nan Duan, Mu Li, and Ming Zhou. 2011. Improving
phrase extraction via MBR phrase scoring and prun-
ing. In Proceedings of MT Summit XIII, pages 189–
197, Xiamen, China, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007a.
Estimating phrase pair relevance for machine transla-
tion pruning. In Proceedings of MT Summit XI, pages
159–165, Copenhagen, Denmark, September.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007b.
Translation model pruning via usage statistics for sta-
tistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Companion Volume, Short Papers,
pages 21–24, Rochester, New York, April. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967–
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Human
Language Technology Conf. /North American Chap-
ter of the Assoc. for Computational Linguistics An-
nual Meeting (HLT-NAACL), pages 127–133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constan-
tine, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL): Poster Session, pages 177–180, Prague,
Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In 6th Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 115–124, Washington
DC, September/October.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725–734,
Honolulu, HI, October. Association for Computational
Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and
the significance of rare events. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 333–340.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Assoc. for Computational Linguistics (ACL),
pages 160–167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of
</reference>
<page confidence="0.977654">
982
</page>
<reference confidence="0.999824915492958">
the Assoc. for Computational Linguistics (ACL), pages
311–318, Philadelphia, PA, July.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258–267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
German Sanchis-Trilles, Daniel Ortiz-Martinez, Je-
sus Gonzalez-Rubio, Jorge Gonzalez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in statistical machine translation.
In Proceedings of the 15th Conference of the European
Association for Machine Translation, pages 257–264,
Leuven, Belgium, May.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270–274.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468–476, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. In Proceedings of MT
Summit XII, Ottawa, Ontario, Canada, August.
Nadi Tomeh, Marco Turchi, Guillaume Wisniewski,
Alexandre Allauzen, and Franc¸ois Yvon. 2011. How
good are your phrases? Assessing phrase quality with
single class classification. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 261–268, San Francisco, California, December.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In 16th Int. Conf. on Computational Linguistics
(COLING), pages 836–841, Copenhagen, Denmark,
August.
Mei Yang and Jing Zheng. 2009. Toward smaller, faster,
and better hierarchical phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 237–240, Suntec, Singapore, August.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Human Language Technology Conf. /North Ameri-
can Chapter of the Assoc. for Computational Linguis-
tics Annual Meeting (HLT-NAACL): Workshop on Sta-
tistical Machine Translation, pages 55–63, New York
City, NY, June.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion, pages 195–205, Honolulu, Hawaii, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation. In
M. Jarke, J. Koehler, and G. Lakemeyer, editors, 25th
German Conf. on Artificial Intelligence (KI2002), vol-
ume 2479 of Lecture Notes in Artificial Intelligence
(LNAI), pages 18–32, Aachen, Germany, September.
Springer Verlag.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In 20th Int. Conf. on Computational
Linguistics (COLING), pages 36–42, Geneva, Switzer-
land, August.
</reference>
<page confidence="0.999101">
983
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988421">
<title confidence="0.999976">A Systematic Comparison of Phrase Table Pruning Techniques</title>
<author confidence="0.999929">Richard Zens</author>
<author confidence="0.999929">Daisy Stanton</author>
<author confidence="0.999929">Peng Xu</author>
<affiliation confidence="0.997214">Google Inc.</affiliation>
<abstract confidence="0.999201090909091">When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1974" citStr="Brants et al. (2007)" startWordPosition="292" endWordPosition="295">w.statmt.org/wmt11/translation-task.html modern computers, these large models lead to a long experiment cycle that hinders progress. The situation is even more severe if computational resources are limited, for instance when translating on handheld devices. Then, reducing the model size is of the utmost importance. The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language mod</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858– 867, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="17534" citStr="Brown et al., 1993" startWordPosition="2943" endWordPosition="2946">chine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target languag</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Chen</author>
<author>Andreas Eisele</author>
<author>Martin Kay</author>
</authors>
<title>Improving statistical machine translation efficiency by triangulation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="5365" citStr="Chen et al., 2008" startWordPosition="828" endWordPosition="831"> to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning </context>
</contexts>
<marker>Chen, Eisele, Kay, 2008</marker>
<rawString>Yu Chen, Andreas Eisele, and Martin Kay. 2008. Improving statistical machine translation efficiency by triangulation. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Chen</author>
<author>Martin Kay</author>
<author>Andreas Eisele</author>
</authors>
<title>Intersecting multilingual data for faster and better statistical translations.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>128--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5385" citStr="Chen et al., 2009" startWordPosition="832" endWordPosition="835">el quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on sim</context>
</contexts>
<marker>Chen, Kay, Eisele, 2009</marker>
<rawString>Yu Chen, Martin Kay, and Andreas Eisele. 2009. Intersecting multilingual data for faster and better statistical translations. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 128–136, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of information theory.</title>
<date>2006</date>
<publisher>Wiley-Interscience,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12200" citStr="Cover and Thomas, 2006" startWordPosition="2014" endWordPosition="2017">. Note that here the pruning criterion only considers redundancy of the phrases, not the quality. Thus, we are not saying that the government of France is a better translation than the French government, only that it is less redundant. *We use the assumption that we can simply multiply the probabilities of the shorter phrases. 5.2 Entropy Criterion Now, we are going to formalize the notion of redundancy. We would like the pruned model p&apos;(e |f) to be as similar as possible to the original model p(�e |�f). We use conditional Kullback-Leibler divergence, also called conditional relative entropy (Cover and Thomas, 2006), to measure the model similarity: D(p(�e |h|p�(�e|�f)) � f) log pM f) 1 (6) p�(�e|�f) l ] �f) log p(�e |�f) − log p�(�e |�f) (7) Computing the best pruned model of a given size would require optimizing over all subsets with that size. Since that is computationally infeasible, we instead apply the equivalent approximation that Stolcke (1998) uses for language modeling. This assumes that phrase pairs affect the relative entropy roughly independently. We can then choose a pruning threshold τE and prune those phrase pairs with a contribution to the relative entropy below that threshold. Thus, we </context>
</contexts>
<marker>Cover, Thomas, 2006</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 2006. Elements of information theory. Wiley-Interscience, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Improving phrase extraction via MBR phrase scoring and pruning.</title>
<date>2011</date>
<booktitle>In Proceedings of MT Summit XIII,</booktitle>
<pages>189--197</pages>
<location>Xiamen, China,</location>
<contexts>
<context position="5616" citStr="Duan et al. (2011)" startWordPosition="872" endWordPosition="875">for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phra</context>
</contexts>
<marker>Duan, Li, Zhou, 2011</marker>
<rawString>Nan Duan, Mu Li, and Ming Zhou. 2011. Improving phrase extraction via MBR phrase scoring and pruning. In Proceedings of MT Summit XIII, pages 189– 197, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Estimating phrase pair relevance for machine translation pruning.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>159--165</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="5127" citStr="Eck et al. (2007" startWordPosition="790" endWordPosition="793"> that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2007</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2007a. Estimating phrase pair relevance for machine translation pruning. In Proceedings of MT Summit XI, pages 159–165, Copenhagen, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Translation model pruning via usage statistics for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>21--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="5127" citStr="Eck et al. (2007" startWordPosition="790" endWordPosition="793"> that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2007</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2007b. Translation model pruning via usage statistics for statistical machine translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 21–24, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2017" citStr="Heafield (2011)" startWordPosition="301" endWordPosition="302">computers, these large models lead to a long experiment cycle that hinders progress. The situation is even more severe if computational resources are limited, for instance when translating on handheld devices. Then, reducing the model size is of the utmost importance. The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measu</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>967--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2167" citStr="Johnson et al. (2007)" startWordPosition="324" endWordPosition="327">s are limited, for instance when translating on handheld devices. Then, reducing the model size is of the utmost importance. The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measures (Stolcke, 1998). We motivate its derivation by stating the desiderata for a good phrase table pruning criterion: • Soundness: The criterion should</context>
<context position="4486" citStr="Johnson et al. (2007)" startWordPosition="689" endWordPosition="692">ing phrase table pruning methods. 2. a new, theoretically sound phrase table pruning criterion. 3. an experimental comparison of several pruning methods for several language pairs. 2 Related Work The most basic pruning methods rely on probability and count cutoffs. We will cover the techniques that are implemented in the Moses toolkit (Koehn et al., 2007) and the Pharaoh decoder (Koehn, 2004) in Section 3. We are not aware of any work that analyzes their efficacy in a systematic way. It is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table p</context>
<context position="7880" citStr="Johnson et al. (2007)" startWordPosition="1258" endWordPosition="1261">Histogram pruning. An alternative to threshold pruning is histogram pruning. For each source phrase f, this method preserves the K target phrases with highest probability p(e |f) or, equivalently, their count N( �f, �e). Note that, except for count-based pruning, none of the methods take the frequency of the source phrase into account. As we will confirm in the empirical evaluation, this will likely cause drops in translation quality, since frequent source phrases are more useful than the infrequent ones. 4 Significance Pruning In this section, we briefly review significance pruning following Johnson et al. (2007). The idea of significance pruning is to test whether a source phrase f and a target phrase e� co-occur more frequently in a bilingual corpus than they should just by chance. Using some simple statistics derived from the bilingual corpus, namely • N(f) the count of the source phrase f • N(e) the count of the target phrase e� • N(f, e) the co-occurence count of the source phrase f and the target phrase e� • N the number of sentences in the bilingual corpus 3Note that it has never been systematically investigated whether this is a real problem or just speculation. we can compute the two-by-two c</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967– 975, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="16989" citStr="Koehn et al., 2003" startWordPosition="2848" endWordPosition="2851">h 56M 65M Spanish - English 232 M 210M French - English 962 M 827 M Table 3: Training data statistics. Number of words in the training data (M=millions). • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantine</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Assoc. for Computational Linguistics (ACL): Poster Session,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4222" citStr="Koehn et al., 2007" startWordPosition="640" endWordPosition="643"> are commonly deficient in at least one of them. We thus designed a novel pruning criterion that not only meets these objectives, it also performs very well in empirical evaluations. The novel contributions of this paper are: 1. a systematic description of existing phrase table pruning methods. 2. a new, theoretically sound phrase table pruning criterion. 3. an experimental comparison of several pruning methods for several language pairs. 2 Related Work The most basic pruning methods rely on probability and count cutoffs. We will cover the techniques that are implemented in the Moses toolkit (Koehn et al., 2007) and the Pharaoh decoder (Koehn, 2004) in Section 3. We are not aware of any work that analyzes their efficacy in a systematic way. It is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantine, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantine, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In 45th Annual Meeting of the Assoc. for Computational Linguistics (ACL): Poster Session, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In 6th Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<pages>115--124</pages>
<location>Washington DC, September/October.</location>
<contexts>
<context position="4260" citStr="Koehn, 2004" startWordPosition="648" endWordPosition="649">em. We thus designed a novel pruning criterion that not only meets these objectives, it also performs very well in empirical evaluations. The novel contributions of this paper are: 1. a systematic description of existing phrase table pruning methods. 2. a new, theoretically sound phrase table pruning criterion. 3. an experimental comparison of several pruning methods for several language pairs. 2 Related Work The most basic pruning methods rely on probability and count cutoffs. We will cover the techniques that are implemented in the Moses toolkit (Koehn et al., 2007) and the Pharaoh decoder (Koehn, 2004) in Section 3. We are not aware of any work that analyzes their efficacy in a systematic way. It is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In 6th Conf. of the Assoc. for Machine Translation in the Americas (AMTA), pages 115–124, Washington DC, September/October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>725--734</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, HI,</location>
<contexts>
<context position="17805" citStr="Macherey et al., 2008" startWordPosition="2990" endWordPosition="2993">. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla1 2 4 8 Number of Phrases [millions] Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4. All pruning experiments are done on top of this. 6.3 Res</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725–734, Honolulu, HI, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>On log-likelihood-ratios and the significance of rare events.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>333--340</pages>
<contexts>
<context position="9142" citStr="Moore (2004)" startWordPosition="1491" endWordPosition="1492">act test, we can calculate the probability of the contingency table via the hypergeometric distribution: ph(N( �f, �e)) _ (4) ( N(˜f) l( N−N( ˜f) l N( ˜f,˜e) N(˜e)−N( ˜f,˜e) ( N(˜e) l · The p-value is then calculated as the sum of all probabilities that are at least as extreme. The lower the p-value, the less likely this phrase pair occurred with the observed frequency by chance; we thus prune a phrase pair ( f, e) if: �ph(k) �&gt; τF (5) for some pruning threshold τF. More details of this approach can be found in Johnson et al. (2007). The idea of using Fisher’s exact test was first explored by Moore (2004) in the context of word alignment. 5 Entropy-based Pruning In this section, we will derive a novel entropy-based pruning criterion. 5.1 Motivational Example In general, pruning the phrase table can be considered as selecting a subset of the original phrase table. When doing so, we would like to alter the original translation model distribution as little as possible. This is a key difference to previous approaches: Our goal is to remove redundant phrases, whereas previous approaches usually try to remove low-quality or unreliable phrases. We believe this to be an advantage of our method as it i</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. On log-likelihood-ratios and the significance of rare events. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 333–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="17008" citStr="Och and Ney, 2004" startWordPosition="2852" endWordPosition="2855">English 232 M 210M French - English 962 M 827 M Table 3: Training data statistics. Number of words in the training data (M=millions). • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="17781" citStr="Och, 2003" startWordPosition="2988" endWordPosition="2989">l news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla1 2 4 8 Number of Phrases [millions] Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4. All pruning experiments are done</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="18623" citStr="Papineni et al., 2002" startWordPosition="3125" endWordPosition="3128">th of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla1 2 4 8 Number of Phrases [millions] Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4. All pruning experiments are done on top of this. 6.3 Results In this section, we present the experimental results. Translation results are reported on the WMT’07 news commentary blind set. We will show translation quality measured with the Bleu score (Papineni et al., 2002) as a function of the phrase table size (number of phrases). Being in the upper left corner of these figures is desirable. First, we show a comparison of several probability-based pruning methods in Figure 1. We compare • Prob. Absolute pruning based on Eq. (2). • Thres. Threshold pruning based on Eq. (3). • Hist. Histogram pruning as described in Section 3.2.5 We observe that these three methods perform equally well. There is no difference between absolute and relative pruning methods, except that the two relative methods (Thres and Hist) are limited by 4The Bleu score drops are as follows: E</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>258--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1998" citStr="Pauls and Klein (2011)" startWordPosition="296" endWordPosition="299">nslation-task.html modern computers, these large models lead to a long experiment cycle that hinders progress. The situation is even more severe if computational resources are limited, for instance when translating on handheld devices. Then, reducing the model size is of the utmost importance. The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion bas</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>German Sanchis-Trilles</author>
<author>Daniel Ortiz-Martinez</author>
<author>Jesus Gonzalez-Rubio</author>
<author>Jorge Gonzalez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Bilingual segmentation for phrasetable pruning in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference of the European Association for Machine Translation,</booktitle>
<pages>257--264</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="5647" citStr="Sanchis-Trilles et al. (2011)" startWordPosition="876" endWordPosition="879">tistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phrase pair (f, e). Hence, they are</context>
</contexts>
<marker>Sanchis-Trilles, Ortiz-Martinez, Gonzalez-Rubio, Gonzalez, Casacuberta, 2011</marker>
<rawString>German Sanchis-Trilles, Daniel Ortiz-Martinez, Jesus Gonzalez-Rubio, Jorge Gonzalez, and Francisco Casacuberta. 2011. Bilingual segmentation for phrasetable pruning in statistical machine translation. In Proceedings of the 15th Conference of the European Association for Machine Translation, pages 257–264, Leuven, Belgium, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="2636" citStr="Stolcke, 1998" startWordPosition="399" endWordPosition="400">name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measures (Stolcke, 1998). We motivate its derivation by stating the desiderata for a good phrase table pruning criterion: • Soundness: The criterion should optimize some well-understood information-theoretic measure of translation model quality. 972 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 972–983, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics • Efficiency: Pruning should be fast, i. e., run linearly in the size of the phrase table. • Self-containedness: As a practical conside</context>
<context position="12543" citStr="Stolcke (1998)" startWordPosition="2075" endWordPosition="2077">ow, we are going to formalize the notion of redundancy. We would like the pruned model p&apos;(e |f) to be as similar as possible to the original model p(�e |�f). We use conditional Kullback-Leibler divergence, also called conditional relative entropy (Cover and Thomas, 2006), to measure the model similarity: D(p(�e |h|p�(�e|�f)) � f) log pM f) 1 (6) p�(�e|�f) l ] �f) log p(�e |�f) − log p�(�e |�f) (7) Computing the best pruned model of a given size would require optimizing over all subsets with that size. Since that is computationally infeasible, we instead apply the equivalent approximation that Stolcke (1998) uses for language modeling. This assumes that phrase pairs affect the relative entropy roughly independently. We can then choose a pruning threshold τE and prune those phrase pairs with a contribution to the relative entropy below that threshold. Thus, we E= E f p( �f) p(�e| e =E f,e p(e, 975 prune a phrase pair ( f, e), if p(e, �f) log p(�e |�f) − log p0(�e |�f) &lt; τE (8) [ ] We now address how to assign the probability p0(�e |f) under the pruned model. A phrase-based system selects among different segmentations of the source language sentence into phrases. If a segmentation into longer phras</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>468--476</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1952" citStr="Talbot and Osborne (2007)" startWordPosition="288" endWordPosition="291">g No. LDC2006T13 2http://www.statmt.org/wmt11/translation-task.html modern computers, these large models lead to a long experiment cycle that hinders progress. The situation is even more severe if computational resources are limited, for instance when translating on handheld devices. Then, reducing the model size is of the utmost importance. The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 468–476, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
</authors>
<title>Complexity-based phrase-table filtering for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII,</booktitle>
<location>Ottawa, Ontario, Canada,</location>
<contexts>
<context position="4997" citStr="Tomeh et al., 2009" startWordPosition="770" endWordPosition="773">g that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate.</context>
</contexts>
<marker>Tomeh, Cancedda, Dymetman, 2009</marker>
<rawString>Nadi Tomeh, Nicola Cancedda, and Marc Dymetman. 2009. Complexity-based phrase-table filtering for statistical machine translation. In Proceedings of MT Summit XII, Ottawa, Ontario, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Marco Turchi</author>
<author>Guillaume Wisniewski</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>How good are your phrases? Assessing phrase quality with single class classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>261--268</pages>
<location>San Francisco, California,</location>
<contexts>
<context position="5671" citStr="Tomeh et al. (2011)" startWordPosition="881" endWordPosition="884">fferent approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phrase pair (f, e). Hence, they are independent of other ph</context>
</contexts>
<marker>Tomeh, Turchi, Wisniewski, Allauzen, Yvon, 2011</marker>
<rawString>Nadi Tomeh, Marco Turchi, Guillaume Wisniewski, Alexandre Allauzen, and Franc¸ois Yvon. 2011. How good are your phrases? Assessing phrase quality with single class classification. In Proceedings of the International Workshop on Spoken Language Translation, pages 261–268, San Francisco, California, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In 16th Int. Conf. on Computational Linguistics (COLING),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="17599" citStr="Vogel et al., 1996" startWordPosition="2956" endWordPosition="2959">al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In 16th Int. Conf. on Computational Linguistics (COLING), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Jing Zheng</author>
</authors>
<title>Toward smaller, faster, and better hierarchical phrase-based SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>237--240</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4976" citStr="Yang and Zheng, 2009" startWordPosition="766" endWordPosition="769"> is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would b</context>
</contexts>
<marker>Yang, Zheng, 2009</marker>
<rawString>Mei Yang and Jing Zheng. 2009. Toward smaller, faster, and better hierarchical phrase-based SMT. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 237–240, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL): Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<location>New York City, NY,</location>
<contexts>
<context position="17446" citStr="Zens and Ney, 2006" startWordPosition="2926" endWordPosition="2929"> performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL): Workshop on Statistical Machine Translation, pages 55–63, New York City, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in dynamic programming beam search for phrase-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>195--205</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="17029" citStr="Zens and Ney, 2008" startWordPosition="2856" endWordPosition="2859">French - English 962 M 827 M Table 3: Training data statistics. Number of words in the training data (M=millions). • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Ze</context>
</contexts>
<marker>Zens, Ney, 2008</marker>
<rawString>Richard Zens and Hermann Ney. 2008. Improvements in dynamic programming beam search for phrase-based statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 195–205, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>25th German Conf. on Artificial Intelligence (KI2002),</booktitle>
<volume>2479</volume>
<pages>18--32</pages>
<editor>In M. Jarke, J. Koehler, and G. Lakemeyer, editors,</editor>
<publisher>Springer Verlag.</publisher>
<location>Aachen, Germany,</location>
<contexts>
<context position="16969" citStr="Zens et al., 2002" startWordPosition="2844" endWordPosition="2847"> 45M Czech - English 56M 65M Spanish - English 232 M 210M French - English 962 M 827 M Table 3: Training data statistics. Number of words in the training data (M=millions). • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignm</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-based statistical machine translation. In M. Jarke, J. Koehler, and G. Lakemeyer, editors, 25th German Conf. on Artificial Intelligence (KI2002), volume 2479 of Lecture Notes in Artificial Intelligence (LNAI), pages 18–32, Aachen, Germany, September. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Hermann Ney</author>
</authors>
<title>Improved word alignment using a symmetric lexicon model.</title>
<date>2004</date>
<booktitle>In 20th Int. Conf. on Computational Linguistics (COLING),</booktitle>
<pages>36--42</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="17645" citStr="Zens et al., 2004" startWordPosition="2964" endWordPosition="2967">8). We trained a 4- gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(El�f) and p( 1|E), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla1 2 4 8 Number of Phrases [millions] Fig</context>
</contexts>
<marker>Zens, Matusov, Ney, 2004</marker>
<rawString>Richard Zens, Evgeny Matusov, and Hermann Ney. 2004. Improved word alignment using a symmetric lexicon model. In 20th Int. Conf. on Computational Linguistics (COLING), pages 36–42, Geneva, Switzerland, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
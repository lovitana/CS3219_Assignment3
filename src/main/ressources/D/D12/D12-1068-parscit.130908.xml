<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99647">
Domain Adaptation for Coreference Resolution: An Adaptive Ensemble
Approach
</title>
<author confidence="0.9834425">
Jian Bo Yang†$; Qi Mao†, Qiao Liang Xiang†, Ivor W. Tsang†,
Kian Ming A. Chai§, Hai Leong Chieu§
</author>
<affiliation confidence="0.9748535">
† School of Computer Engineering, Nanyang Technological University, Singapore
t Electrical and Computer Engineering Department, Duke University, USA
</affiliation>
<address confidence="0.545496">
§ DSO National Laboratories, Singapore
</address>
<email confidence="0.976371">
jianbo.yang@duke.edu, {qmao1,qlxiang,ivortsang}@ntu.edu.sg,
{ckianmin,chaileon}@dso.org.sg
</email>
<sectionHeader confidence="0.995468" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9795113">
We propose an adaptive ensemble method to
adapt coreference resolution across domains.
This method has three features: (1) it can op-
timize for any user-specified objective mea-
sure; (2) it can make document-specific pre-
diction rather than rely on a fixed base model
or a fixed set of base models; (3) it can auto-
matically adjust the active ensemble members
during prediction. With simplification, this
method can be used in the traditional within-
domain case, while still retaining the above
features. To the best of our knowledge, this
work is the first to both (i) develop a domain
adaptation algorithm for the coreference reso-
lution problem and (ii) have the above features
as an ensemble method. Empirically, we show
the benefits of (i) on the six domains of the
ACE 2005 data set in domain adaptation set-
ting, and of (ii) on both the MUC-6 and the
ACE 2005 data sets in within-domain setting.
</bodyText>
<sectionHeader confidence="0.999031" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99542304347826">
Coreference resolution is a fundamental component
of natural language processing (NLP) and has been
widely applied in other NLP tasks (Stoyanov et al.,
2010). It gathers together noun phrases (mentions)
that refer to the same real-world entity (Ng and
Cardie, 2002). In the past decade, several corefer-
ence resolution systems have been proposed, e.g.,
(Ng and Cardie, 2002), (Denis and Baldridge, 2007)
and (Stoyanov et al., 2010). All of these focus on
the within-domain case — to use the labeled doc-
uments from a domain to predict on the unlabeled
*The work is done during postdoc in NTU, Singapore.
documents in the same domain. However, in prac-
tice, there is usually limited labeled data in a specific
domain of interest, while there may be plenty of la-
beled data in other related domains. Effective use of
data from the other domains for predicting in the do-
main of interest is therefore an important strategy in
NLP. This is called domain adaptation, and, in this
context, the former domains is called the source do-
mains, while the latter domain is called the target
domain (Blitzer et al., 2006; Jiang and Zhai, 2007).
Based on the type of the knowledge to be trans-
ferred to the target domain, domain adaptation learn-
ing can be categorized as instance-based method,
feature-based method, parameter-based method or
relational-knowledge-based method (Pan and Yang,
2010). Previously, domain adaptation learning has
been successfully used in other NLP tasks such as
relation extraction (Jiang, 2009) and POS tagging
(Jiang and Zhai, 2007), semantic detection (Tan et
al., 2008), name entity recognition (Guo et al., 2009)
and entity type classification (Jiang and Zhai, 2007).
However, to the best of our knowledge, it has yet to
be explored for coreference resolution.
In this paper, we propose an adaptive ensemble
method to adapt coreference resolution across do-
mains. This proposed method can be categorized
as both feature-based and parameter-based domain
adaptation learning methods. It has three main steps:
ensemble creation, cross-domain knowledge learn-
ing and decision inference. The first step creates
the ensemble by collecting a set of base models,
which can be any individual methods with various
features/instances/parameters settings. The second
step analyzes the collected base models from vari-
</bodyText>
<page confidence="0.970779">
744
</page>
<note confidence="0.797044">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 744–753, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998497024390244">
ous domains and learns the cross-domain knowledge fully used in coreference resolution (Pang and Fan,
between each target domain and the source domain. 2009; Munson et al., 2005; Rahman and Ng, 2011a).
The third step infers the final decision in the target However, these methods only focus on the within-
domain based on all ensemble results. domain setting.
In addition to domain adaptation, the proposed All these methods comprise of two steps: ensem-
adaptive ensemble method has the following fea- ble creation and decision inference. Ng and Cardie
tures that are absent in the other ensemble methods. (2003) and Vemulapalli et al. (2009) applied the
First, it can optimize any user-specified objective bagging and boosting techniques on the documents
measure without using a separate development set. to create the ensemble. Recently, Rahman and Ng
Second, it can provide document-specific prediction (2011a) further enriched the ensemble by consider-
instead of relying on a fixed base model or a fixed ing various feature sets and learning models. Specif-
set of base models for all documents. Third, it can ically, three types of feature sets (conventional, lex-
automatically adjust the active ensemble members ical and combined) and three learning algorithms
in decision inference so that underperforming base (mention-pair model, mention-ranking model and
models are filtered out. The proposed method can the clustering-ranking model) are employed. In de-
also be used in the traditional within-domain prob- cision inference, these methods used voting or av-
lem with some simplifications. eraging to get the final prediction. Rahman and Ng
We conduct experiments for coreference resolu- (2011a) proposed four voting strategies for predic-
tion under both the within-domain setting and the tion: applying best Per-NP-Type model, antecedent-
domain-adaptation setting. In the within-domain based voting, cluster-based voting and weighted
setting, we compare the proposed adaptive ensemble clustering-based voting. Although their approaches
method with the mention-pair methods and other en- achieved promising results in their end-to-end sys-
semble methods on the MUC-6 and ACE 2005 cor- tems, these do not consider the user-specific perfor-
pora. The results show that the proposed adaptive mance measure during the ensemble learning.
ensemble method consistently outperforms these Another branch of ensemble methods uses model
baselines. In the domain adaptation setting, we use selection (Munson et al., 2005; Ng, 2005), simi-
the ACE 2005 corpora to create six domain adap- lar to the conventional model selection method for
tation tasks to evaluate the effectiveness of our do- generic parameter-tuning. The method of (Munson
main adaptation learning. The results show that our et al., 2005) first collects a large family of base mod-
method outperforms baselines that do not use do- els. Then, a separate tuning set with ground truth
main adaptation. is used to evaluate each base model’s performance.
The paper is organized as follows. Section 2 re- Finally, an iterative approach is used to select the
views some existing ensemble methods for coref- best performed base models to form the ensemble.
erence resolution. Section 3 presents the proposed Like other methods, this method uses the average
adaptive ensemble method for domain adaptation strategy in decision inference. Similarly, the method
problems. Section 4 presents a special case of of (Ng, 2005) ranks base models according to their
the proposed method for the within-domain setting. performance on separate tuning set, and then uses
Section 5 presents the experiments under both the the highest-ranked base model for predicting on test
within-domain and the domain adaptation settings. documents. These methods require a separate set of
We conclude and discuss future work in Section 6. labeled documents to assess the generalization per-
formance.
</bodyText>
<sectionHeader confidence="0.9980795" genericHeader="method">
2 Existing Ensemble Methods
3 Adaptive Ensemble Method
</sectionHeader>
<bodyText confidence="0.988879222222222">
Many ensemble methods have been proposed in the
machine learning literature, e.g., bagging (Breiman,
1996), boosting (Freund and Schapire, 1996), ran-
dom forest (Breiman, 2001) and mixture models
(Bishop, 2007). Some of them have been success-
In this section, we give our adaptive ensemble
method for domain adaptation for coreference res-
olution. We first introduce some notations.
For a corpus of N documents, document DZ
</bodyText>
<page confidence="0.9969">
745
</page>
<bodyText confidence="0.998777375">
is the ith document, and it contains ni men-
tions mi = (m1 i , . . . , mn�
i ) with the ordering of
each mention as they appear in the document.
The index set of all mention pairs in Di is
Ei = {(a, b)  |1 ≤ a &lt; b ≤ ni}. The transpose of
vector x is x′. The performance measure function
for document D is Λ(g(D); f(D)), where g(D) and
f(D) represent the coreference ground-truth and
prediction by model f on document D respectively.
In coreference resolution, typical performance mea-
sure functions include MUC (Vilain et al., 1995),
Rand index (Rand, 1971), B-CUBED (Bagga and
Baldwin, 1998) and CEAF (Luo, 2005). In this pa-
per, Λ can either be used as part of an objective func-
tion in learning or as an evaluation measure for as-
sessing the performance of a coreference system.
We consider the typical domain adaptation prob-
lem, which has one target domain t and p (p ≥ 1)
source domains s1, ... , sp. The target domain
contains N(t) labeled documents and M unla-
beled documents, while source domains contain
N(s1), ... , N(sp) labeled documents. Unlabeled
data in the source domains are not used. We use
</bodyText>
<equation confidence="0.634499">
D(v)
</equation>
<bodyText confidence="0.99315">
i for the ith document in domain v.
</bodyText>
<subsectionHeader confidence="0.996579">
3.1 Ensemble Creation
</subsectionHeader>
<bodyText confidence="0.984988164179104">
Mention-pair methods have been widely-used for
coreference resolution due to their efficiency and
effectiveness, and they have often been taken as
base models in ensemble learning (Rahman and Ng,
2011a; Munson et al., 2005). We adopt a similar ap-
proach by using the standard mention-pair method
(Soon et al., 2001; Ng and Cardie, 2002) with var-
ious parameters to form the ensemble, though our
framework can incorporate other coreference meth-
ods in the ensemble. Mention-pair methods usu-
ally comprise of two steps. The first step classifies
every mention pair into either coreference or non-
coreference with a confidence between 0 and 1. The
second step partitions the set of mentions into clus-
ters based on the confidence values, where mentions
in each cluster are presumed to be the same under-
lying entity.
Classification We use Soon’s approach (Soon et
al., 2001) to select a portion of mention pairs to train
a binary classifier because this has better generaliza-
tion (Soon et al., 2001). The positive mention pairs
are the anaphoric mention mbi (b = 2, ... , ni) paired
with its closest antecedent mention mai (a &lt; b),
while the negative mention pairs are the mention
mbi paired with each of the intervening mentions
ma+1
i , ma+2
i ,... , mb−1
i . Following (Rahman and
Ng, 2011a), our binary classifier is SVM with the
regularization parameter C. The classifier is trained
with the software Liblinear (Fan et al., 2008), which
is also used to give probabilistic binary predictions.
Clustering We adopt closest-first clustering (Soon
et al., 2001) and best-first clustering (Ng and Cardie,
2002) to determine whether a mention pair is coref-
erent. For each mention, the closest-first method
(or best-first method) links it to the the closest (or
the best) preceding mention if the confidence value
(obtained from the first step) of this mention pair is
above a specified threshold t.
Features For each mention pair, we use the
d = 39 features proposed by Rahman and Ng
(2011b) to represent it. These features can be ex-
tracted using the Reconcile software (Stoyanov et
al., 2010). We use ˆϕa,b ∈ Rd to represent the fea-
tures of a mention pair (ma, mb). With this feature
set, we found that the linear kernel is insufficient to
fit the training data. However, using an rbf kernel
would be too computationally expensive. Hence, we
augment ˆϕa,b with a ˆd-dimensional feature vector
[ψ1 · · · ψˆd] to give a new feature vector
ϕa,b = [ ˆϕa,b ψ1 ··· ψˆd], (1)
where the dˆ augmented features [ψ1 ·· · ψˆd] are de-
termined by
ψj = exp(−∥ˆϕa,b − cj∥2
d ), ∀j = 1,. . . , ˆd. (2)
Herein, c1,...,cˆd are the dˆ centroids of the
randomly-selected subset C from all labeled men-
tion pairs { ˆϕa,b  |(a, b) ∈ E1, ... , EN}. In our ex-
periments, we use the k-means algorithm to obtain
the centroids of C.
Ensemble For domain v, we create a domain-
specified ensemble F(v) = {f1, ... , fℓ} of ℓ base
models by including the closest-first and best-first
mention-pair methods with the different C and t val-
ues. If multiple domains are provided, we gather all
</bodyText>
<page confidence="0.99083">
746
</page>
<bodyText confidence="0.996046">
the domain-specific ensembles into a grand ensem-
ble F = F(s1) ∪ ···F(sp) ∪ F(t).
</bodyText>
<subsectionHeader confidence="0.999875">
3.2 Cross-domain Knowledge Learning
</subsectionHeader>
<bodyText confidence="0.999782090909091">
Generally, the feature distributions are different in
different domains. Therefore, effective domain
adaptation requires using some knowledge of cross-
domain similarity. We now propose an approach
to learn the parametric-distances between the doc-
uments in source and target domains to characterize
this cross-domain knowledge.
Distances between documents A document Di is
represented by the sum of its new mention-pair fea-
tures (Yu and Joachims, 2009; Finley and Joachims,
2005):
</bodyText>
<equation confidence="0.992266416666666">
Φ(Di) = ∑ ϕa,b. (3)
(a,b)E£i
The distance between a source labeled document
D(su)
i in domain su and a target labeled document
D(t)
j is parameterized as
Dist(D(su)
i , D(t)
j ; µ) = µ�∆(D(su)
i , D(t)
j ), (4)
</equation>
<bodyText confidence="0.920910555555556">
where vector µ ∈ Rd+d is to be learned, and vec-
tor function ∆(D(su)
i , D(t)
j ) ∈ Rd+d is the Euclidean
distance vector between two documents given by
∆(D(su)
i ,D(t)
j ) = (Φ(D(su)
i ) − Φ(D(t)
</bodyText>
<equation confidence="0.9712285">
j ))
⊙ (Φ(D(su)
i ) −Φ(D(t)
j )). (5)
</equation>
<bodyText confidence="0.970806333333333">
The operator ⊙ is the element-wise product. Dis-
tance (4) is actually the Mahanalobis distance (Yang
and Jin, 2006) with the scaling of features:
</bodyText>
<equation confidence="0.8450224">
(Φ(D(su)
i ) − Φ(D(t)
j ))�W (Φ(D(su)
i ) − Φ(D(t)
j )),
</equation>
<bodyText confidence="0.991367666666667">
where W is a diagonal matrix with diagonal entries
µ. Matrix W is diagonal to reduce computation cost
and to increase statistical confidence in estimation
when there is limited target labeled data (as is typi-
cally the case in domain adaptation).
That µ is the vector of diagonal entries in W re-
quires that each entry in µ is non-negative. If the lth
entry of µ is non-zero, then the lth feature in ϕa,b
contribute towards (4). To ensure that at least B fea-
tures are used, we also constrain that each entry in µ
is not more than unity and that fµ ≥ B.
Matching best base models For each labeled doc-
</bodyText>
<subsectionHeader confidence="0.389098">
ument D(v)
</subsectionHeader>
<bodyText confidence="0.827845333333333">
j in domain v, we identify the best per-
forming base model f(v)�
j in F(v) with
</bodyText>
<equation confidence="0.963905">
f(v)* j= arg max Λ(g(D(v)
j ); f(D(v)
j )), (6)
fEJ7( )
</equation>
<bodyText confidence="0.980205333333333">
where Λ(· ; ·) is the the performance objective func-
tion to be instantiated in Section 3.3.
Then, for each source domain su and document
</bodyText>
<equation confidence="0.816095">
D(t)
j in the target domain, we find the set I(D(t)
j ; su)
</equation>
<bodyText confidence="0.987498">
of the documents in domain su that have the same
best performing base model as that for D(t)
</bodyText>
<equation confidence="0.991886714285714">
j :
I(D(t)
j ; su) = {D(su)
i  |f(su)�
i = f(t)�
j ,
i = 1, ... , N(su)}. (7)
</equation>
<bodyText confidence="0.911092214285714">
The key idea in I(D(t)
j ; su) is to select documents
in a source domain su that are similar to document
D(t)
j in the sense that they have the same best per-
forming base model under a specific Λ. This ensures
that optimization step (to be described next) is tar-
geted towards Λ and not confounded by document
pairs that should be disimilar anyway.
Optimization We determine the vector µ by mini-
mizing the parametric distance (4) between all target
labeled documents and their corresponding source
labeled document identified in the previous step.
That is,
</bodyText>
<equation confidence="0.971460666666667">
∑ ∆(D(su)
i , D(t)
j ). (8)
E (�u)
i ET(E (t)
� ;su)
</equation>
<bodyText confidence="0.999732333333333">
The solution µ to this linear programming problem
can be regarded as the cross-domain knowledge be-
tween source domain su and the target domain t. Re-
peating for every source domain su, u = 1, ... , p,
gives the cross-domain knowledge between every
source domain and the target domain.
The above three-steps procedure selects the effec-
tive features for each pair of source and target do-
mains. Generally, the results of feature selection
vary for different pairs of source and target domains,
due to the diversities of the feature distributions in
different domains.
</bodyText>
<figure confidence="0.481086666666667">
min µ N(t) ∑
µ �
j=1
</figure>
<page confidence="0.931706">
747
</page>
<subsectionHeader confidence="0.987189">
3.3 Decision Inference
</subsectionHeader>
<bodyText confidence="0.956144636363636">
After ensemble creation and cross-domain knowl-
edge learning, we need to provide the coreference
result on an unseen document in the target domain
based on the results of all the members in F. Un-
like the previous methods using the voting/average
or their variants (Pang and Fan, 2009; Munson et
al., 2005; Rahman and Ng, 2011a), we propose the
following nearest neighbor based approach.
Given the grand ensemble F and all labeled doc-
uments, the task is to predict on the target unlabeled
document D(t)
</bodyText>
<equation confidence="0.8340238">
j , j = 1, ... , M. The idea of the pro-
posed method is to first find the k most similar docu-
ments N(D(t)
j ) from all labeled documents for doc-
ument D(t)
j . Then, we choose the base model that
performs best on the documents in N(D(t)
j ) as the
method f t)
(t)* for document De.
</equation>
<bodyText confidence="0.952610533333333">
Firstly, we employ the parametric-distance (4) to
measure the similarity between any labeled docu-
ment D(v)
i , ∀v, i, from all source and target domains,
and the target unlabeled document D(t)
j . Here, the
cross-domain knowledge µ in (4) has already been
determined by the optimization (8) in Section 3.2.
Secondly, based on the computed distance values,
we select k nearest neighbor documents for the tar-
get unlabeled document D(t)
j from all labeled doc-
uments D(v)
i , ∀v, i. These k nearest neighbor docu-
ments for document D(t)
</bodyText>
<equation confidence="0.751801333333333">
j make up the set N(D(t)
j ).
Thirdly, the optimal base model for the unlabeled
document D(t)
j prediction is chosen by
*
f t = arg max Λ(g(Dp); f(Dp)). (9)
D�∈N(D�t)
� ), f∈F
</equation>
<bodyText confidence="0.999872">
We can instantiate the performance objective func-
tion Λ(g(·); f(·)) in expressions (6) and (9) to be
any coreference resolution measures, such as MUC,
Rand index, B-CUBED and CEAF. We have not
known of other (ensemble) coreference resolution
methods that optimize for these measures. This ab-
sence is possibly due to their complex discrete and
non-convex properties.
</bodyText>
<subsectionHeader confidence="0.64827">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.98580309375">
The above proposed adaptive ensemble approach in-
corporates the domain adaptation knowledge during
(a) the identification of similar documents between
different domains and (b) the determination of ac-
tive ensemble members. Beside these, it has the fol-
lowing features over other (ensemble) coreference
methods: (i) It can optimize any user-specified ob-
jective measure via (6) and (9). An intuitive rec-
ommendation is to directly optimize for an objective
function that matches the evaluation measure. (ii)
It can make document-specific decisions, as expres-
sions (4) and (9) deal with each testing document
separately. (iii) The prediction on the testing docu-
ment D(t)
j is not based on all members in F but only
on the active ensemble members N(D(t)
j ). This can
filter out some potentially unsuitable base models
for document D(t)
j . Moreover, the active ensemble
members N(D(t)
j ) is dynamically adjusted for each
test document.
For computational cost, the majority is by ensem-
ble creation, since a large number of base models
are usually used. This is common among all ensem-
ble methods. In contrast, the costs in (4) and (9)
are trivial as both are at the document level. The
cost of generating centroids in (2) can also be high
if the size of C is more than ten thousand, but this
is still negligible compared to the cost of ensemble
creation.
</bodyText>
<sectionHeader confidence="0.995705" genericHeader="method">
4 Special Case: Within-domain Setting
</sectionHeader>
<bodyText confidence="0.999670823529412">
The adaptive ensemble method presented in Sec-
tion 3 is for the domain adaptation setting. How-
ever, it is possible to simplify it for the special case
of within-domain setting. In the within-domain set-
ting, the adaptive ensemble method only has ensem-
ble creation and decision inference steps.
In the ensemble creation step, we still use the
closest-first and best-first mention-pair methods
with various parameters to create the ensemble. Un-
like the domain adaptation setting, here we can only
use the labeled documents in the target domain to
create the ensemble F(t). Therefore, the size of en-
semble here is reduced by p times compared to the
domain adaptation setting.
In the decision inference step, we directly use the
Euclidean distance ∆(DZ�t),D�t)) in (5) for the la-
beled document D(t)
</bodyText>
<equation confidence="0.977307">
i , i = 1, ... , N(t) and unlabeled
document D(t)
j , j = 1, ... , M. Based on these dis-
</equation>
<page confidence="0.942911">
748
</page>
<bodyText confidence="0.922729571428571">
tance values, we similarly select k nearest neighbor
documents N(D(t)
j ) for document D(t)
j , and then de-
termine the final method f(t)�
j for document D(t) jby
(9) but with F replaced by F(t).
</bodyText>
<sectionHeader confidence="0.990792" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.979420833333334">
We test the proposed adaptive method and sev-
eral baselines under both the within-domain and
the domain adaptation settings on the MUC-6 and
ACE 2005 corpora. MUC-6 contains 60 docu-
ments. ACE 2005 contains 599 documents from
six different domains: Newswire (NW), Broadcast
News (BN), Broadcast Conversations (BC), Web-
blog (WL), Usenet (UN), and Conversational Tele-
phone Speech (CTS). In all our experiments, we use
two popular performance measures, B-CUBED F-
measure (Bagga and Baldwin, 1998) and CEAF F-
measure (Luo, 2005) 1, to evaluate the coreference
resolution result. Since the focus of the paper is to
investigate the effectiveness of coreference resolu-
tion methods, we use the gold standard mentions in
all experiments.
For the proposed method, the ensemble F(v) in
every domain v has 208 members totally. They
are created by the closest-first and the best-first
mention-pair methods using SVM trained with pa-
rameter C taking values
C E [0.001, 0.01, 0.1, 1, 10, 100, 1000, 1000] (10)
and using clustering with the threshold parameters t
taking values
</bodyText>
<equation confidence="0.920360333333333">
t E [0.2, 0.25, 0.3, 0.34, 0.38, 0.4, 0.42, 0.44,
(11)
0.46, 0.48, 0.5, 0.6, 0.7].
</equation>
<bodyText confidence="0.9439787">
The size of the selected subset C is fixed to 2000,
and the number of centroids is determined by
the validation procedure from four possible values
[10, 20, 30, 40]. We use k-means algorithm to com-
pute the centroids. Due to the randomness of sub-
set C and k-means algorithm, we run the proposed
method 5 times and report the average results. For
the number of nearest neighbor k, we report three
results, each for k E {1, 3, 5}.
&apos;More exactly, we use the widely used ϕ3-CEAF F-measure.
</bodyText>
<tableCaption confidence="0.75425025">
Table 1: The settings in the experiments under within-
domain setting on MUC-6 and ACE 2005 corpora. N(O
and M(O and Total are the numbers of training, testing
and all documents respectively.
</tableCaption>
<table confidence="0.99992025">
Domain N(t) M(t) Total
MUC-6 30 30 60
BC 48 12 60
BN 181 45 226
CTS 31 8 39
NW 85 21 106
UN 39 10 49
WL 95 24 119
</table>
<subsectionHeader confidence="0.939153">
5.1 Within-domain Setting
</subsectionHeader>
<bodyText confidence="0.979766095238095">
We conduct the experiment under the within-domain
setting on seven tasks, with the per-domain setting
shown in Table 1. The validation set is created by
further splitting training data into validation train-
ing and validation testing sets with the ratio of N���
M��� ,
where N(t) and M(t) are given in Table 1. In this
experiment, we attempt to study the following three
things. First, we investigate whether the proposed
ensemble method is better than the tuned mention-
pair methods and other ensemble methods. Second,
we investigate the optimal number of active ensem-
ble members. Third, we investigate the impact to the
performance of the coreference system, when differ-
ent objective measures are used with different eval-
uation measures.
For the proposed ensemble method, we experi-
mented with nearest neighbor set of sizes k = 1, 3, 5
paired with objective function A in (9) set to Rand
Index, CEAF or B-CUBED. For baselines, the fol-
lowing four are used:
</bodyText>
<listItem confidence="0.9926002">
• Two mention-pair baselines. Two baselines are
the closest-first and the best-first mention-pair
methods with the tuned parameters C and t. In
the tuning process, the ranges of C and t are
specified in (10) and (11) respectively. These
two mention-pair methods are named as Sc and
Sb for short.
• Two existing ensemble baselines. The other
two baselines are the ensemble methods us-
ing the voting procedure in decision inference.
</listItem>
<page confidence="0.998929">
749
</page>
<tableCaption confidence="0.994685">
Table 2: B-CUBED F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.
</tableCaption>
<table confidence="0.9995188">
Baselines A = Rand A = CEAF A = B-CUBED
S, Sb E,,t E, k=1 3 5 k=1 3 5 k=1 3 5
MUC-6 66.1 66.1 61.9 57.1 67.6 67.3 68.5 65.2 64.1 65.5 68.7 66.7 67.5
BC 64.1 65.1 34.2 24.8 65.5 65.4 65.7 65.9 65.5 62.9 66.5 66.1 66.0
BN 75.9 74.8 57.7 48.0 75.7 75.1 74.9 76.3 75.9 75.3 76.4 76.3 76.7
CTS 71.0 65.1 39.6 31.5 70.6 69.3 68.3 71.3 69.9 70.4 71.7 70.6 69.1
NW 74.6 74.4 45.6 34.1 74.3 74.8 72.9 73.2 71.4 70.1 75.0 74.6 73.7
UN 69.5 70.2 44.1 27.4 70.4 69.9 69.3 69.6 67.6 66.0 70.3 71.4 70.3
WL 73.8 75.4 69.8 58.5 75.5 74.6 73.9 75.5 73.0 73.4 76.2 75.5 75.6
Average 70.7 70.2 50.4 40.2 71.4 70.9 70.5 71.0 69.6 69.1 72.1 71.6 71.3
</table>
<bodyText confidence="0.999196">
These two baselines use the same ensemble as
the proposed method for fair comparison. In
decision inference, these two baselines use the
mention-based voting and cluster-based voting
respectively, as proposed in (Rahman and Ng,
2011a). In these two baselines, all members
in the ensemble participate the voting process.
These two ensemble baselines are named as E,,t
and E, for short.
Tables 2 and 3 show the experiment results using
B-CUBED and CEAF as the evaluation measures
respectively. The best result for each of the seven
tasks is highlighted in bold. The last rows of the ta-
bles show the average performance value among all
seven tasks.
From the results, we observe that the proposed en-
semble method with objective function matching the
evaluation measure and with k = 1 generally per-
forms best among all methods and all tasks. Surpris-
ingly, the common ensemble method with mention-
based voting E,,t and cluster-based voting E, strate-
gies do not perform well. The plausible reason is
the current ensemble may incorporate some bad base
models due to inappropriate C and t values, which
would undermine the voting result. Nevertheless, it
is difficult to judge the quality of the ensemble mem-
bers in advance. Therefore, this validates the impor-
tance of choosing an active set of ensemble members
in decision inference. The better performance of the
proposed method over the mention-pair baselines S,
and Sb is probably because of the document-specific
decision. This is reasonable, as different base mod-
els in the ensemble would be good at predicting
the different documents. For the proposed ensem-
ble method with various configurations, we observe
using an objective function that matches the evalu-
ation measures is generally better. An exception is
the MUC-6 and BN tasks in CEAF F-measure. We
also observe that the ensemble method with k = 1
is generally better than that with the larger k, except
the BN and UN tasks in B-CUBED F-measure. This
suggests that the fewer the active ensemble members
the better the generalization performance. Follow-
ing (Rahman and Ng, 2011a), we also conduct the
Student’s t-test, and the results show that the pro-
posed method with the objective function matching
the evaluation measure and with k = 1 is signifi-
cantly better than the best baseline. In contrast, the
two baseline ensemble methods that use voting are
significantly worse than the best baseline. The sig-
nificance level 0.05.
</bodyText>
<subsectionHeader confidence="0.999164">
5.2 Domain-adaptation Setting
</subsectionHeader>
<bodyText confidence="0.999942454545455">
We employ ACE 2005 corpora to simulate the do-
main adaptation settings in experiments. Specifi-
cally, we create six domain adaptation tasks, BC,
BN, CTS, NW, UN, WL in total. Each task has one
target domain and five source domains. For exam-
ple, in the task UN, the target domain is UN while
the other five source domains are BC, BN, CTS, NW
and WL. The number of labeled documents in each
domain is as the same as in Table 1, except when
that domain is the target domain, in which case we
use only five labeled documents. The number of test
</bodyText>
<page confidence="0.998584">
750
</page>
<tableCaption confidence="0.999209">
Table 3: CEAF F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.
</tableCaption>
<table confidence="0.9997305">
Baselines Λ = Rand Λ = B-CUBED Λ = CEAF
S, Sb Em E, k=1 3 5 k=1 3 5 k=1 3 5
MUC-6 62.6 62.5 62.7 57.5 62.0 60.6 61.0 64.5 62.7 63.8 63.1 58.7 59.2
BC 58.8 56.5 36.6 26.6 56.7 57.1 57.0 58.3 58.8 57.2 59.3 59.2 58.4
BN 67.9 66.5 55.1 44.7 69.4 69.4 69.9 69.8 70.2 69.6 69.5 69.0 68.7
CTS 61.0 60.7 38.6 31.5 67.1 66.9 63.6 68.1 68.4 68.2 68.5 67.6 67.7
NW 66.9 66.4 41.1 31.2 68.4 68.0 64.6 69.2 68.4 66.4 69.3 66.1 66.7
UN 62.5 63.5 46.2 28.9 62.9 61.8 60.9 62.2 63.7 62.9 63.9 61.5 60.4
WL 69.7 70.3 63.5 54.3 70.7 70.2 72.5 71.5 71.4 72.3 72.4 69.4 70.0
Average 64.2 63.8 49.1 39.2 65.3 64.9 64.2 66.2 66.2 65.8 66.6 64.5 64.5
</table>
<bodyText confidence="0.997727">
(or unlabeled) documents in the target document is
also the same as in Table 1. The validation set is
created similarly as in the experiment under within-
domain setting.
For the proposed ensemble method, we heuristi-
cally determine the parameter B in µ to be the num-
ber of non-zero elements in r, where
</bodyText>
<equation confidence="0.9558765">
∆(D(su) D(t)).
i ,j
</equation>
<bodyText confidence="0.999933">
Making use of the conclusion in the experiments
for the within-domain setting, we fix the optimized
measure to be the final performance measure in (9).
We compare with the following five baselines.
</bodyText>
<listItem confidence="0.904668166666667">
• Two mention-pair baselines in within-domain
setting. Two baselines are same as S, and Sb in
the experiments under within-domain settings,
except that the labeled training documents are
reduced to 5.
• Three proposed adaptive ensemble methods
</listItem>
<bodyText confidence="0.983038470588235">
without cross-domain knowledge learning.
These three baselines uses neighborhood sizes
k = 1, 3, 5 with the grand ensemble F rather
than the target domain ensemble F(t). In an-
other words, these three baselines are the same
as the proposed method, but with µ = 1.
Tables 4 and 5 show the experimental results in
the domain adaptation settings using B-CUBED and
CEAF as the final performance measures respec-
tively. From the results, we can see that the pro-
posed method with cross-domain knowledge gener-
ally outperforms all the five baselines. Among them,
the best proposed domain adaptation method on av-
erage outperforms the best of S,, Sb by 7.2% for B-
CUBED F-measure and 3% for CEAF F-measure.
The grand-ensemble baselines are also significantly
better than the within-domain baselines. These re-
sults clearly illustrate the usefulness of making use
of the labeled documents in the source domains. For
the comparison between the proposed method with
and without cross-domain knowledge learning, all
tasks, except UN task in CEAF F-measure, show
the superiority of the proposed method with cross-
domain knowledge learning. Among them, except
tasks BN and CTS in B-CUBED F-measure, the per-
formance gains are among 1%—3% for all tasks in
both measures. These results verify the necessity
of cross-domain knowledge learning. For the com-
parison of the proposed method with different k,
unlike the results in the within-domain setting, the
results here show that choosing optimal k is task-
dependent. The reason of this observation is not
clear yet. It is plausible due to the increased uncer-
tainties from multiple domains.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99991375">
In this paper, we proposed an adaptive ensem-
ble method for coreference resolution under both
within-domain and domain adaptation settings. The
key advantage of the proposed method is incor-
</bodyText>
<equation confidence="0.5349266">
∑
D su)∈Z(D�t);su)
N(t) ∑
j=1
r=
</equation>
<page confidence="0.996053">
751
</page>
<tableCaption confidence="0.9514515">
Table 4: B-CUBED F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with Λ
set to B-CUBED. The within-domain and grand ensemble methods are the baselines.
</tableCaption>
<table confidence="0.999975555555556">
Within-domain Grand ensemble Domain-adaptation
S, Sb k=1 3 5 k=1 3 5
BC 58.0 65.1 65.0 67.1 67.0 67.5 68.2 67.7
BN 72.7 73.8 75.0 75.3 75.0 75.3 75.4 74.3
CTS 63.2 62.1 65.7 64.8 64.0 64.1 65.8 65.8
NW 54.9 54.6 73.6 73.1 74.2 73.0 74.4 74.7
UN 66.5 42.7 67.2 68.2 68.9 69.7 68.7 68.2
WL 68.6 73.2 73.0 72.6 73.4 74.8 74.5 73.6
Average 64.0 61.9 69.9 70.2 70.4 70.7 71.2 70.7
</table>
<tableCaption confidence="0.997282">
Table 5: CEAF F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with Λ set
to CEAF. The within-domain and grand ensemble methods are the baselines.
</tableCaption>
<table confidence="0.999728222222222">
Within-domain Grand ensemble Domain-adaptation
S, Sb k=1 3 5 k=1 3 5
BC 55.7 43.7 56.9 57.6 57.3 58.5 58.8 57.2
BN 65.8 67.2 65.9 64.1 65.8 63.9 62.7 67.2
CTS 56.0 51.0 56.6 54.6 53.7 58.6 57.4 55.3
NW 52.7 55.0 66.4 64.1 63.8 69.4 66.7 66.8
UN 64.0 39.1 63.6 63.7 64.4 64.3 62.9 62.7
WL 70.3 64.2 68.1 67.8 70.2 67.3 69.6 72.0
Average 60.7 53.4 62.9 62.0 62.5 63.7 63.0 63.5
</table>
<bodyText confidence="0.998649034482759">
porating the cross-domain knowledge to aid coref-
erence resolution learning. This is useful when
the labeled coreference labels are scarce. We also
demonstrate that the proposed adaptive ensemble
method can be readily applied to conventional coref-
erence tasks without cross-domain knowledge learn-
ing. Compared with existing ensemble methods, the
proposed method is simultaneously endowed with
the following three distinctive features: optimizing
any user-specified performance measure, making the
document-specific prediction and automatically ad-
justing the active ensemble members. In the exper-
iments under both within-domain settings and do-
main adaptation settings, the results evidence the
effectiveness of the proposed cross-domain knowl-
edge learning method, and also demonstrate the su-
periority of the proposed adaptive ensemble method
over other baselines.
Currently, the proposed method relies on some
limited target annotations. It would be interesting
to consider the pure unsupervised tasks that have no
any target annotations. Besides, to develop some
better ways for document-level representation, e.g.,
incorporating the domain knowledge, also deserves
our attentions. Similarly, to extend the diagonal Ma-
halanobis matrix to the general covariance matrix is
also desirable. Last but not least, to find a more sys-
tematical way to determine the optimal k in the pro-
posed method is also our possible future work.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992001">
This work is supported by DSO grant
DSOCL10021.
</bodyText>
<sectionHeader confidence="0.981854" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.8437895">
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
</bodyText>
<page confidence="0.994211">
752
</page>
<reference confidence="0.999854586538462">
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics - Volume 1, ACL’98, pages 79–85.
Christopher M. Bishop. 2007. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, 1st ed. 2006. corr. 2nd printing edition,
October.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP, pages
120–128.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140, August.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32, October.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proc HLT, pages 236–
243, Rochester, New York, April.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Thomas Finley and Thorsten Joachims. 2005. Super-
vised clustering with support vector machines. In
Proc. ICML.
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a New Boosting Algorithm. In Proc. ICML,
pages 148–156.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. NAACL ’09, pages 281–289.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proc. ACL,
pages 264–271, Prague, Czech Republic, June.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1012–1020, Suntec, Singapore, August. Association
for Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages 25–
32.
Art Munson, Claire Cardie, and Rich Caruana. 2005.
Optimizing to arbitrary NLP metrics using ensemble
selection. In Proc HLT and EMNLP, pages 539–546.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. ACL, pages 104–111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proc. HLT-NAACL.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the ACL, pages 157–164.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345–1359, October.
Wenbo Pang and Xiaozhong Fan. 2009. Chinese coref-
erence resolution with ensemble learning. In Proc.
PACIIA, pages 236–243.
Altaf Rahman and Vincent Ng. 2011a. Ensemble-
based coreference resolution. In Proceedings of IJ-
CAI, pages 1884–1889.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. JAIR, 1:469–52.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):pp. 846–850.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics,, pages 521–
544.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proc. ACL, pages
156–161.
Songbo Tan, Yuefen Wang, Gaowei Wu, and Xueqi
Cheng. 2008. Using unlabeled data to handle domain-
transfer problem of semantic detection. In Proceed-
ings of the 2008 ACM symposium on Applied comput-
ing, SAC ’08, pages 896–903.
S. Vemulapalli, X. Luo, J.F.Pitrelli, and I. Zitouni. 2009.
classifier combination applied to coreference resolu-
tion. In NAACL HLT Student Rsearch Workshop.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understanding,
MUC6 ’95, pages 45–52.
Liu Yang and Rong Jin. 2006. Distance Metric Learning:
A Comprehensive Survey. Technical report, Depart-
ment of Computer Science and Engineering, Michigan
State University.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proc. ICML, pages 1169–1176, New York, NY, USA.
</reference>
<page confidence="0.999148">
753
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.519399">
<title confidence="0.9957585">Domain Adaptation for Coreference Resolution: An Adaptive Ensemble Approach</title>
<author confidence="0.997083">Bo Qiao Liang Ivor W Ming A Hai Leong</author>
<affiliation confidence="0.999962">of Computer Engineering, Nanyang Technological University, and Computer Engineering Department, Duke University,</affiliation>
<address confidence="0.554924">National Laboratories, Singapore</address>
<abstract confidence="0.995892476190476">We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>model</author>
</authors>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>79--85</pages>
<marker>model, </marker>
<rawString>model. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL’98, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2007</date>
<booktitle>Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1st ed. 2006. corr. 2nd printing edition, October.</booktitle>
<contexts>
<context position="8101" citStr="Bishop, 2007" startWordPosition="1240" endWordPosition="1241">separate tuning set, and then uses Section 5 presents the experiments under both the the highest-ranked base model for predicting on test within-domain and the domain adaptation settings. documents. These methods require a separate set of We conclude and discuss future work in Section 6. labeled documents to assess the generalization performance. 2 Existing Ensemble Methods 3 Adaptive Ensemble Method Many ensemble methods have been proposed in the machine learning literature, e.g., bagging (Breiman, 1996), boosting (Freund and Schapire, 1996), random forest (Breiman, 2001) and mixture models (Bishop, 2007). Some of them have been successIn this section, we give our adaptive ensemble method for domain adaptation for coreference resolution. We first introduce some notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), where g(D) and f(D) represent the coreference ground-truth and pre</context>
</contexts>
<marker>Bishop, 2007</marker>
<rawString>Christopher M. Bishop. 2007. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1st ed. 2006. corr. 2nd printing edition, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="2473" citStr="Blitzer et al., 2006" startWordPosition="393" endWordPosition="396"> use the labeled documents from a domain to predict on the unlabeled *The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of labeled data in other related domains. Effective use of data from the other domains for predicting in the domain of interest is therefore an important strategy in NLP. This is called domain adaptation, and, in this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="7998" citStr="Breiman, 1996" startWordPosition="1225" endWordPosition="1226"> ranks base models according to their the proposed method for the within-domain setting. performance on separate tuning set, and then uses Section 5 presents the experiments under both the the highest-ranked base model for predicting on test within-domain and the domain adaptation settings. documents. These methods require a separate set of We conclude and discuss future work in Section 6. labeled documents to assess the generalization performance. 2 Existing Ensemble Methods 3 Adaptive Ensemble Method Many ensemble methods have been proposed in the machine learning literature, e.g., bagging (Breiman, 1996), boosting (Freund and Schapire, 1996), random forest (Breiman, 2001) and mixture models (Bishop, 2007). Some of them have been successIn this section, we give our adaptive ensemble method for domain adaptation for coreference resolution. We first introduce some notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure funct</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="8067" citStr="Breiman, 2001" startWordPosition="1235" endWordPosition="1236">hin-domain setting. performance on separate tuning set, and then uses Section 5 presents the experiments under both the the highest-ranked base model for predicting on test within-domain and the domain adaptation settings. documents. These methods require a separate set of We conclude and discuss future work in Section 6. labeled documents to assess the generalization performance. 2 Existing Ensemble Methods 3 Adaptive Ensemble Method Many ensemble methods have been proposed in the machine learning literature, e.g., bagging (Breiman, 1996), boosting (Freund and Schapire, 1996), random forest (Breiman, 2001) and mixture models (Bishop, 2007). Some of them have been successIn this section, we give our adaptive ensemble method for domain adaptation for coreference resolution. We first introduce some notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), where g(D) and f(D) represent th</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5–32, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proc HLT,</booktitle>
<pages>236--243</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="1774" citStr="Denis and Baldridge, 2007" startWordPosition="268" endWordPosition="271">le method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting. 1 Introduction Coreference resolution is a fundamental component of natural language processing (NLP) and has been widely applied in other NLP tasks (Stoyanov et al., 2010). It gathers together noun phrases (mentions) that refer to the same real-world entity (Ng and Cardie, 2002). In the past decade, several coreference resolution systems have been proposed, e.g., (Ng and Cardie, 2002), (Denis and Baldridge, 2007) and (Stoyanov et al., 2010). All of these focus on the within-domain case — to use the labeled documents from a domain to predict on the unlabeled *The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of labeled data in other related domains. Effective use of data from the other domains for predicting in the domain of interest is therefore an important strategy in NLP. This is called domain adaptation, and, in this context, the former domains is c</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proc HLT, pages 236– 243, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="10906" citStr="Fan et al., 2008" startWordPosition="1733" endWordPosition="1736">Classification We use Soon’s approach (Soon et al., 2001) to select a portion of mention pairs to train a binary classifier because this has better generalization (Soon et al., 2001). The positive mention pairs are the anaphoric mention mbi (b = 2, ... , ni) paired with its closest antecedent mention mai (a &lt; b), while the negative mention pairs are the mention mbi paired with each of the intervening mentions ma+1 i , ma+2 i ,... , mb−1 i . Following (Rahman and Ng, 2011a), our binary classifier is SVM with the regularization parameter C. The classifier is trained with the software Liblinear (Fan et al., 2008), which is also used to give probabilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extra</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="13120" citStr="Finley and Joachims, 2005" startWordPosition="2114" endWordPosition="2117">are provided, we gather all 746 the domain-specific ensembles into a grand ensemble F = F(s1) ∪ ···F(sp) ∪ F(t). 3.2 Cross-domain Knowledge Learning Generally, the feature distributions are different in different domains. Therefore, effective domain adaptation requires using some knowledge of crossdomain similarity. We now propose an approach to learn the parametric-distances between the documents in source and target domains to characterize this cross-domain knowledge. Distances between documents A document Di is represented by the sum of its new mention-pair features (Yu and Joachims, 2009; Finley and Joachims, 2005): Φ(Di) = ∑ ϕa,b. (3) (a,b)E£i The distance between a source labeled document D(su) i in domain su and a target labeled document D(t) j is parameterized as Dist(D(su) i , D(t) j ; µ) = µ�∆(D(su) i , D(t) j ), (4) where vector µ ∈ Rd+d is to be learned, and vector function ∆(D(su) i , D(t) j ) ∈ Rd+d is the Euclidean distance vector between two documents given by ∆(D(su) i ,D(t) j ) = (Φ(D(su) i ) − Φ(D(t) j )) ⊙ (Φ(D(su) i ) −Φ(D(t) j )). (5) The operator ⊙ is the element-wise product. Distance (4) is actually the Mahanalobis distance (Yang and Jin, 2006) with the scaling of features: (Φ(D(su)</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2005. Supervised clustering with support vector machines. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a New Boosting Algorithm.</title>
<date>1996</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="8036" citStr="Freund and Schapire, 1996" startWordPosition="1228" endWordPosition="1231">ng to their the proposed method for the within-domain setting. performance on separate tuning set, and then uses Section 5 presents the experiments under both the the highest-ranked base model for predicting on test within-domain and the domain adaptation settings. documents. These methods require a separate set of We conclude and discuss future work in Section 6. labeled documents to assess the generalization performance. 2 Existing Ensemble Methods 3 Adaptive Ensemble Method Many ensemble methods have been proposed in the machine learning literature, e.g., bagging (Breiman, 1996), boosting (Freund and Schapire, 1996), random forest (Breiman, 2001) and mixture models (Bishop, 2007). Some of them have been successIn this section, we give our adaptive ensemble method for domain adaptation for coreference resolution. We first introduce some notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), w</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a New Boosting Algorithm. In Proc. ICML, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>Xiaoxun Zhang</author>
<author>Xian Wu</author>
<author>Zhong Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<journal>NAACL</journal>
<volume>09</volume>
<pages>281--289</pages>
<contexts>
<context position="2997" citStr="Guo et al., 2009" startWordPosition="471" endWordPosition="474">the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best of our knowledge, it has yet to be explored for coreference resolution. In this paper, we propose an adaptive ensemble method to adapt coreference resolution across domains. This proposed method can be categorized as both feature-based and parameter-based domain adaptation learning methods. It has three main steps: ensemble creation, cross-domain knowledge learning and decision inference. The first step creates the ensemble by collecting a set of base models, which can be any individual methods with various features/i</context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, Xian Wu, and Zhong Su. 2009. Domain adaptation with latent semantic association for named entity recognition. NAACL ’09, pages 281–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>264--271</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2496" citStr="Jiang and Zhai, 2007" startWordPosition="397" endWordPosition="400">ents from a domain to predict on the unlabeled *The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of labeled data in other related domains. Effective use of data from the other domains for predicting in the domain of interest is therefore an important strategy in NLP. This is called domain adaptation, and, in this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best of our knowledge, it h</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proc. ACL, pages 264–271, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
</authors>
<title>Multi-task transfer learning for weakly-supervised relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2875" citStr="Jiang, 2009" startWordPosition="453" endWordPosition="454">e an important strategy in NLP. This is called domain adaptation, and, in this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best of our knowledge, it has yet to be explored for coreference resolution. In this paper, we propose an adaptive ensemble method to adapt coreference resolution across domains. This proposed method can be categorized as both feature-based and parameter-based domain adaptation learning methods. It has three main steps: ensemble creation, cross-domain knowledge learning and decision inference. The first</context>
</contexts>
<marker>Jiang, 2009</marker>
<rawString>Jing Jiang. 2009. Multi-task transfer learning for weakly-supervised relation extraction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1012–1020, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="8927" citStr="Luo, 2005" startWordPosition="1392" endWordPosition="1393">is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), where g(D) and f(D) represent the coreference ground-truth and prediction by model f on document D respectively. In coreference resolution, typical performance measure functions include MUC (Vilain et al., 1995), Rand index (Rand, 1971), B-CUBED (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). In this paper, Λ can either be used as part of an objective function in learning or as an evaluation measure for assessing the performance of a coreference system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used fo</context>
<context position="20969" citStr="Luo, 2005" startWordPosition="3533" endWordPosition="3534">final method f(t)� j for document D(t) jby (9) but with F replaced by F(t). 5 Experiments We test the proposed adaptive method and several baselines under both the within-domain and the domain adaptation settings on the MUC-6 and ACE 2005 corpora. MUC-6 contains 60 documents. ACE 2005 contains 599 documents from six different domains: Newswire (NW), Broadcast News (BN), Broadcast Conversations (BC), Webblog (WL), Usenet (UN), and Conversational Telephone Speech (CTS). In all our experiments, we use two popular performance measures, B-CUBED Fmeasure (Bagga and Baldwin, 1998) and CEAF Fmeasure (Luo, 2005) 1, to evaluate the coreference resolution result. Since the focus of the paper is to investigate the effectiveness of coreference resolution methods, we use the gold standard mentions in all experiments. For the proposed method, the ensemble F(v) in every domain v has 208 members totally. They are created by the closest-first and the best-first mention-pair methods using SVM trained with parameter C taking values C E [0.001, 0.01, 0.1, 1, 10, 100, 1000, 1000] (10) and using clustering with the threshold parameters t taking values t E [0.2, 0.25, 0.3, 0.34, 0.38, 0.4, 0.42, 0.44, (11) 0.46, 0.</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Art Munson</author>
<author>Claire Cardie</author>
<author>Rich Caruana</author>
</authors>
<title>Optimizing to arbitrary NLP metrics using ensemble selection.</title>
<date>2005</date>
<booktitle>In Proc HLT and EMNLP,</booktitle>
<pages>539--546</pages>
<contexts>
<context position="4107" citStr="Munson et al., 2005" startWordPosition="633" endWordPosition="636"> the ensemble by collecting a set of base models, which can be any individual methods with various features/instances/parameters settings. The second step analyzes the collected base models from vari744 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 744–753, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ous domains and learns the cross-domain knowledge fully used in coreference resolution (Pang and Fan, between each target domain and the source domain. 2009; Munson et al., 2005; Rahman and Ng, 2011a). The third step infers the final decision in the target However, these methods only focus on the withindomain based on all ensemble results. domain setting. In addition to domain adaptation, the proposed All these methods comprise of two steps: ensemadaptive ensemble method has the following fea- ble creation and decision inference. Ng and Cardie tures that are absent in the other ensemble methods. (2003) and Vemulapalli et al. (2009) applied the First, it can optimize any user-specified objective bagging and boosting techniques on the documents measure without using a </context>
<context position="6431" citStr="Munson et al., 2005" startWordPosition="979" endWordPosition="982"> based voting, cluster-based voting and weighted setting, we compare the proposed adaptive ensemble clustering-based voting. Although their approaches method with the mention-pair methods and other en- achieved promising results in their end-to-end syssemble methods on the MUC-6 and ACE 2005 cor- tems, these do not consider the user-specific perforpora. The results show that the proposed adaptive mance measure during the ensemble learning. ensemble method consistently outperforms these Another branch of ensemble methods uses model baselines. In the domain adaptation setting, we use selection (Munson et al., 2005; Ng, 2005), simithe ACE 2005 corpora to create six domain adap- lar to the conventional model selection method for tation tasks to evaluate the effectiveness of our do- generic parameter-tuning. The method of (Munson main adaptation learning. The results show that our et al., 2005) first collects a large family of base modmethod outperforms baselines that do not use do- els. Then, a separate tuning set with ground truth main adaptation. is used to evaluate each base model’s performance. The paper is organized as follows. Section 2 re- Finally, an iterative approach is used to select the views</context>
<context position="9705" citStr="Munson et al., 2005" startWordPosition="1526" endWordPosition="1529">system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al., 2005). We adopt a similar approach by using the standard mention-pair method (Soon et al., 2001; Ng and Cardie, 2002) with various parameters to form the ensemble, though our framework can incorporate other coreference methods in the ensemble. Mention-pair methods usually comprise of two steps. The first step classifies every mention pair into either coreference or noncoreference with a confidence between 0 and 1. The second step partitions the set of mentions into clusters based on the confidence values, where mentions in each cluster are presumed to be the same underlying entity. Classification W</context>
<context position="16402" citStr="Munson et al., 2005" startWordPosition="2740" endWordPosition="2743">ree-steps procedure selects the effective features for each pair of source and target domains. Generally, the results of feature selection vary for different pairs of source and target domains, due to the diversities of the feature distributions in different domains. min µ N(t) ∑ µ � j=1 747 3.3 Decision Inference After ensemble creation and cross-domain knowledge learning, we need to provide the coreference result on an unseen document in the target domain based on the results of all the members in F. Unlike the previous methods using the voting/average or their variants (Pang and Fan, 2009; Munson et al., 2005; Rahman and Ng, 2011a), we propose the following nearest neighbor based approach. Given the grand ensemble F and all labeled documents, the task is to predict on the target unlabeled document D(t) j , j = 1, ... , M. The idea of the proposed method is to first find the k most similar documents N(D(t) j ) from all labeled documents for document D(t) j . Then, we choose the base model that performs best on the documents in N(D(t) j ) as the method f t) (t)* for document De. Firstly, we employ the parametric-distance (4) to measure the similarity between any labeled document D(v) i , ∀v, i, from</context>
</contexts>
<marker>Munson, Cardie, Caruana, 2005</marker>
<rawString>Art Munson, Claire Cardie, and Rich Caruana. 2005. Optimizing to arbitrary NLP metrics using ensemble selection. In Proc HLT and EMNLP, pages 539–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1637" citStr="Ng and Cardie, 2002" startWordPosition="247" endWordPosition="250">both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting. 1 Introduction Coreference resolution is a fundamental component of natural language processing (NLP) and has been widely applied in other NLP tasks (Stoyanov et al., 2010). It gathers together noun phrases (mentions) that refer to the same real-world entity (Ng and Cardie, 2002). In the past decade, several coreference resolution systems have been proposed, e.g., (Ng and Cardie, 2002), (Denis and Baldridge, 2007) and (Stoyanov et al., 2010). All of these focus on the within-domain case — to use the labeled documents from a domain to predict on the unlabeled *The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of labeled data in other related domains. Effective use of data from the other domains for predicting in the doma</context>
<context position="9817" citStr="Ng and Cardie, 2002" startWordPosition="1546" endWordPosition="1549">mains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al., 2005). We adopt a similar approach by using the standard mention-pair method (Soon et al., 2001; Ng and Cardie, 2002) with various parameters to form the ensemble, though our framework can incorporate other coreference methods in the ensemble. Mention-pair methods usually comprise of two steps. The first step classifies every mention pair into either coreference or noncoreference with a confidence between 0 and 1. The second step partitions the set of mentions into clusters based on the confidence values, where mentions in each cluster are presumed to be the same underlying entity. Classification We use Soon’s approach (Soon et al., 2001) to select a portion of mention pairs to train a binary classifier beca</context>
<context position="11081" citStr="Ng and Cardie, 2002" startWordPosition="1758" endWordPosition="1761">al., 2001). The positive mention pairs are the anaphoric mention mbi (b = 2, ... , ni) paired with its closest antecedent mention mai (a &lt; b), while the negative mention pairs are the mention mbi paired with each of the intervening mentions ma+1 i , ma+2 i ,... , mb−1 i . Following (Rahman and Ng, 2011a), our binary classifier is SVM with the regularization parameter C. The classifier is trained with the software Liblinear (Fan et al., 2008), which is also used to give probabilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extracted using the Reconcile software (Stoyanov et al., 2010). We use ˆϕa,b ∈ Rd to represent the features of a mention pair (ma, mb). With this feature set, we found that the lin</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>157--164</pages>
<contexts>
<context position="6442" citStr="Ng, 2005" startWordPosition="983" endWordPosition="984">r-based voting and weighted setting, we compare the proposed adaptive ensemble clustering-based voting. Although their approaches method with the mention-pair methods and other en- achieved promising results in their end-to-end syssemble methods on the MUC-6 and ACE 2005 cor- tems, these do not consider the user-specific perforpora. The results show that the proposed adaptive mance measure during the ensemble learning. ensemble method consistently outperforms these Another branch of ensemble methods uses model baselines. In the domain adaptation setting, we use selection (Munson et al., 2005; Ng, 2005), simithe ACE 2005 corpora to create six domain adap- lar to the conventional model selection method for tation tasks to evaluate the effectiveness of our do- generic parameter-tuning. The method of (Munson main adaptation learning. The results show that our et al., 2005) first collects a large family of base modmethod outperforms baselines that do not use do- els. Then, a separate tuning set with ground truth main adaptation. is used to evaluate each base model’s performance. The paper is organized as follows. Section 2 re- Finally, an iterative approach is used to select the views some exist</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Vincent Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the ACL, pages 157–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="2747" citStr="Pan and Yang, 2010" startWordPosition="433" endWordPosition="436">abeled data in other related domains. Effective use of data from the other domains for predicting in the domain of interest is therefore an important strategy in NLP. This is called domain adaptation, and, in this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best of our knowledge, it has yet to be explored for coreference resolution. In this paper, we propose an adaptive ensemble method to adapt coreference resolution across domains. This proposed method can be categorized as both feature-based and parameter-based domain adaptation</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345–1359, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbo Pang</author>
<author>Xiaozhong Fan</author>
</authors>
<title>Chinese coreference resolution with ensemble learning.</title>
<date>2009</date>
<booktitle>In Proc. PACIIA,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="16381" citStr="Pang and Fan, 2009" startWordPosition="2736" endWordPosition="2739">domain. The above three-steps procedure selects the effective features for each pair of source and target domains. Generally, the results of feature selection vary for different pairs of source and target domains, due to the diversities of the feature distributions in different domains. min µ N(t) ∑ µ � j=1 747 3.3 Decision Inference After ensemble creation and cross-domain knowledge learning, we need to provide the coreference result on an unseen document in the target domain based on the results of all the members in F. Unlike the previous methods using the voting/average or their variants (Pang and Fan, 2009; Munson et al., 2005; Rahman and Ng, 2011a), we propose the following nearest neighbor based approach. Given the grand ensemble F and all labeled documents, the task is to predict on the target unlabeled document D(t) j , j = 1, ... , M. The idea of the proposed method is to first find the k most similar documents N(D(t) j ) from all labeled documents for document D(t) j . Then, we choose the base model that performs best on the documents in N(D(t) j ) as the method f t) (t)* for document De. Firstly, we employ the parametric-distance (4) to measure the similarity between any labeled document</context>
</contexts>
<marker>Pang, Fan, 2009</marker>
<rawString>Wenbo Pang and Xiaozhong Fan. 2009. Chinese coreference resolution with ensemble learning. In Proc. PACIIA, pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Ensemblebased coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1884--1889</pages>
<contexts>
<context position="4128" citStr="Rahman and Ng, 2011" startWordPosition="637" endWordPosition="640">ecting a set of base models, which can be any individual methods with various features/instances/parameters settings. The second step analyzes the collected base models from vari744 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 744–753, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ous domains and learns the cross-domain knowledge fully used in coreference resolution (Pang and Fan, between each target domain and the source domain. 2009; Munson et al., 2005; Rahman and Ng, 2011a). The third step infers the final decision in the target However, these methods only focus on the withindomain based on all ensemble results. domain setting. In addition to domain adaptation, the proposed All these methods comprise of two steps: ensemadaptive ensemble method has the following fea- ble creation and decision inference. Ng and Cardie tures that are absent in the other ensemble methods. (2003) and Vemulapalli et al. (2009) applied the First, it can optimize any user-specified objective bagging and boosting techniques on the documents measure without using a separate development </context>
<context position="9682" citStr="Rahman and Ng, 2011" startWordPosition="1522" endWordPosition="1525">ance of a coreference system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al., 2005). We adopt a similar approach by using the standard mention-pair method (Soon et al., 2001; Ng and Cardie, 2002) with various parameters to form the ensemble, though our framework can incorporate other coreference methods in the ensemble. Mention-pair methods usually comprise of two steps. The first step classifies every mention pair into either coreference or noncoreference with a confidence between 0 and 1. The second step partitions the set of mentions into clusters based on the confidence values, where mentions in each cluster are presumed to be the same underlying e</context>
<context position="11459" citStr="Rahman and Ng (2011" startWordPosition="1824" endWordPosition="1827">assifier is trained with the software Liblinear (Fan et al., 2008), which is also used to give probabilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extracted using the Reconcile software (Stoyanov et al., 2010). We use ˆϕa,b ∈ Rd to represent the features of a mention pair (ma, mb). With this feature set, we found that the linear kernel is insufficient to fit the training data. However, using an rbf kernel would be too computationally expensive. Hence, we augment ˆϕa,b with a ˆd-dimensional feature vector [ψ1 · · · ψˆd] to give a new feature vector ϕa,b = [ ˆϕa,b ψ1 ··· ψˆd], (1) where the dˆ augmented features [ψ1 ·· · ψˆd] are determined by ψj = exp(−∥ˆϕa,b − cj∥2 d ), ∀j = 1,. . . , ˆd. (2) Her</context>
<context position="16423" citStr="Rahman and Ng, 2011" startWordPosition="2744" endWordPosition="2747">elects the effective features for each pair of source and target domains. Generally, the results of feature selection vary for different pairs of source and target domains, due to the diversities of the feature distributions in different domains. min µ N(t) ∑ µ � j=1 747 3.3 Decision Inference After ensemble creation and cross-domain knowledge learning, we need to provide the coreference result on an unseen document in the target domain based on the results of all the members in F. Unlike the previous methods using the voting/average or their variants (Pang and Fan, 2009; Munson et al., 2005; Rahman and Ng, 2011a), we propose the following nearest neighbor based approach. Given the grand ensemble F and all labeled documents, the task is to predict on the target unlabeled document D(t) j , j = 1, ... , M. The idea of the proposed method is to first find the k most similar documents N(D(t) j ) from all labeled documents for document D(t) j . Then, we choose the base model that performs best on the documents in N(D(t) j ) as the method f t) (t)* for document De. Firstly, we employ the parametric-distance (4) to measure the similarity between any labeled document D(v) i , ∀v, i, from all source and targe</context>
<context position="24765" citStr="Rahman and Ng, 2011" startWordPosition="4210" endWordPosition="4213"> 75.1 74.9 76.3 75.9 75.3 76.4 76.3 76.7 CTS 71.0 65.1 39.6 31.5 70.6 69.3 68.3 71.3 69.9 70.4 71.7 70.6 69.1 NW 74.6 74.4 45.6 34.1 74.3 74.8 72.9 73.2 71.4 70.1 75.0 74.6 73.7 UN 69.5 70.2 44.1 27.4 70.4 69.9 69.3 69.6 67.6 66.0 70.3 71.4 70.3 WL 73.8 75.4 69.8 58.5 75.5 74.6 73.9 75.5 73.0 73.4 76.2 75.5 75.6 Average 70.7 70.2 50.4 40.2 71.4 70.9 70.5 71.0 69.6 69.1 72.1 71.6 71.3 These two baselines use the same ensemble as the proposed method for fair comparison. In decision inference, these two baselines use the mention-based voting and cluster-based voting respectively, as proposed in (Rahman and Ng, 2011a). In these two baselines, all members in the ensemble participate the voting process. These two ensemble baselines are named as E,,t and E, for short. Tables 2 and 3 show the experiment results using B-CUBED and CEAF as the evaluation measures respectively. The best result for each of the seven tasks is highlighted in bold. The last rows of the tables show the average performance value among all seven tasks. From the results, we observe that the proposed ensemble method with objective function matching the evaluation measure and with k = 1 generally performs best among all methods and all ta</context>
<context position="26612" citStr="Rahman and Ng, 2011" startWordPosition="4517" endWordPosition="4520">cision. This is reasonable, as different base models in the ensemble would be good at predicting the different documents. For the proposed ensemble method with various configurations, we observe using an objective function that matches the evaluation measures is generally better. An exception is the MUC-6 and BN tasks in CEAF F-measure. We also observe that the ensemble method with k = 1 is generally better than that with the larger k, except the BN and UN tasks in B-CUBED F-measure. This suggests that the fewer the active ensemble members the better the generalization performance. Following (Rahman and Ng, 2011a), we also conduct the Student’s t-test, and the results show that the proposed method with the objective function matching the evaluation measure and with k = 1 is significantly better than the best baseline. In contrast, the two baseline ensemble methods that use voting are significantly worse than the best baseline. The significance level 0.05. 5.2 Domain-adaptation Setting We employ ACE 2005 corpora to simulate the domain adaptation settings in experiments. Specifically, we create six domain adaptation tasks, BC, BN, CTS, NW, UN, WL in total. Each task has one target domain and five sourc</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011a. Ensemblebased coreference resolution. In Proceedings of IJCAI, pages 1884–1889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the modeling gap: A cluster-ranking approach to coreference resolution.</title>
<date>2011</date>
<journal>JAIR,</journal>
<pages>1--469</pages>
<contexts>
<context position="4128" citStr="Rahman and Ng, 2011" startWordPosition="637" endWordPosition="640">ecting a set of base models, which can be any individual methods with various features/instances/parameters settings. The second step analyzes the collected base models from vari744 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 744–753, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ous domains and learns the cross-domain knowledge fully used in coreference resolution (Pang and Fan, between each target domain and the source domain. 2009; Munson et al., 2005; Rahman and Ng, 2011a). The third step infers the final decision in the target However, these methods only focus on the withindomain based on all ensemble results. domain setting. In addition to domain adaptation, the proposed All these methods comprise of two steps: ensemadaptive ensemble method has the following fea- ble creation and decision inference. Ng and Cardie tures that are absent in the other ensemble methods. (2003) and Vemulapalli et al. (2009) applied the First, it can optimize any user-specified objective bagging and boosting techniques on the documents measure without using a separate development </context>
<context position="9682" citStr="Rahman and Ng, 2011" startWordPosition="1522" endWordPosition="1525">ance of a coreference system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al., 2005). We adopt a similar approach by using the standard mention-pair method (Soon et al., 2001; Ng and Cardie, 2002) with various parameters to form the ensemble, though our framework can incorporate other coreference methods in the ensemble. Mention-pair methods usually comprise of two steps. The first step classifies every mention pair into either coreference or noncoreference with a confidence between 0 and 1. The second step partitions the set of mentions into clusters based on the confidence values, where mentions in each cluster are presumed to be the same underlying e</context>
<context position="11459" citStr="Rahman and Ng (2011" startWordPosition="1824" endWordPosition="1827">assifier is trained with the software Liblinear (Fan et al., 2008), which is also used to give probabilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extracted using the Reconcile software (Stoyanov et al., 2010). We use ˆϕa,b ∈ Rd to represent the features of a mention pair (ma, mb). With this feature set, we found that the linear kernel is insufficient to fit the training data. However, using an rbf kernel would be too computationally expensive. Hence, we augment ˆϕa,b with a ˆd-dimensional feature vector [ψ1 · · · ψˆd] to give a new feature vector ϕa,b = [ ˆϕa,b ψ1 ··· ψˆd], (1) where the dˆ augmented features [ψ1 ·· · ψˆd] are determined by ψj = exp(−∥ˆϕa,b − cj∥2 d ), ∀j = 1,. . . , ˆd. (2) Her</context>
<context position="16423" citStr="Rahman and Ng, 2011" startWordPosition="2744" endWordPosition="2747">elects the effective features for each pair of source and target domains. Generally, the results of feature selection vary for different pairs of source and target domains, due to the diversities of the feature distributions in different domains. min µ N(t) ∑ µ � j=1 747 3.3 Decision Inference After ensemble creation and cross-domain knowledge learning, we need to provide the coreference result on an unseen document in the target domain based on the results of all the members in F. Unlike the previous methods using the voting/average or their variants (Pang and Fan, 2009; Munson et al., 2005; Rahman and Ng, 2011a), we propose the following nearest neighbor based approach. Given the grand ensemble F and all labeled documents, the task is to predict on the target unlabeled document D(t) j , j = 1, ... , M. The idea of the proposed method is to first find the k most similar documents N(D(t) j ) from all labeled documents for document D(t) j . Then, we choose the base model that performs best on the documents in N(D(t) j ) as the method f t) (t)* for document De. Firstly, we employ the parametric-distance (4) to measure the similarity between any labeled document D(v) i , ∀v, i, from all source and targe</context>
<context position="24765" citStr="Rahman and Ng, 2011" startWordPosition="4210" endWordPosition="4213"> 75.1 74.9 76.3 75.9 75.3 76.4 76.3 76.7 CTS 71.0 65.1 39.6 31.5 70.6 69.3 68.3 71.3 69.9 70.4 71.7 70.6 69.1 NW 74.6 74.4 45.6 34.1 74.3 74.8 72.9 73.2 71.4 70.1 75.0 74.6 73.7 UN 69.5 70.2 44.1 27.4 70.4 69.9 69.3 69.6 67.6 66.0 70.3 71.4 70.3 WL 73.8 75.4 69.8 58.5 75.5 74.6 73.9 75.5 73.0 73.4 76.2 75.5 75.6 Average 70.7 70.2 50.4 40.2 71.4 70.9 70.5 71.0 69.6 69.1 72.1 71.6 71.3 These two baselines use the same ensemble as the proposed method for fair comparison. In decision inference, these two baselines use the mention-based voting and cluster-based voting respectively, as proposed in (Rahman and Ng, 2011a). In these two baselines, all members in the ensemble participate the voting process. These two ensemble baselines are named as E,,t and E, for short. Tables 2 and 3 show the experiment results using B-CUBED and CEAF as the evaluation measures respectively. The best result for each of the seven tasks is highlighted in bold. The last rows of the tables show the average performance value among all seven tasks. From the results, we observe that the proposed ensemble method with objective function matching the evaluation measure and with k = 1 generally performs best among all methods and all ta</context>
<context position="26612" citStr="Rahman and Ng, 2011" startWordPosition="4517" endWordPosition="4520">cision. This is reasonable, as different base models in the ensemble would be good at predicting the different documents. For the proposed ensemble method with various configurations, we observe using an objective function that matches the evaluation measures is generally better. An exception is the MUC-6 and BN tasks in CEAF F-measure. We also observe that the ensemble method with k = 1 is generally better than that with the larger k, except the BN and UN tasks in B-CUBED F-measure. This suggests that the fewer the active ensemble members the better the generalization performance. Following (Rahman and Ng, 2011a), we also conduct the Student’s t-test, and the results show that the proposed method with the objective function matching the evaluation measure and with k = 1 is significantly better than the best baseline. In contrast, the two baseline ensemble methods that use voting are significantly worse than the best baseline. The significance level 0.05. 5.2 Domain-adaptation Setting We employ ACE 2005 corpora to simulate the domain adaptation settings in experiments. Specifically, we create six domain adaptation tasks, BC, BN, CTS, NW, UN, WL in total. Each task has one target domain and five sourc</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011b. Narrowing the modeling gap: A cluster-ranking approach to coreference resolution. JAIR, 1:469–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<pages>846--850</pages>
<contexts>
<context position="8871" citStr="Rand, 1971" startWordPosition="1383" endWordPosition="1384"> notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), where g(D) and f(D) represent the coreference ground-truth and prediction by model f on document D respectively. In coreference resolution, typical performance measure functions include MUC (Vilain et al., 1995), Rand index (Rand, 1971), B-CUBED (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). In this paper, Λ can either be used as part of an objective function in learning or as an evaluation measure for assessing the performance of a coreference system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensembl</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):pp. 846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases. Computational Linguistics,,</title>
<date>2001</date>
<pages>521--544</pages>
<contexts>
<context position="9795" citStr="Soon et al., 2001" startWordPosition="1542" endWordPosition="1545">p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document in domain v. 3.1 Ensemble Creation Mention-pair methods have been widely-used for coreference resolution due to their efficiency and effectiveness, and they have often been taken as base models in ensemble learning (Rahman and Ng, 2011a; Munson et al., 2005). We adopt a similar approach by using the standard mention-pair method (Soon et al., 2001; Ng and Cardie, 2002) with various parameters to form the ensemble, though our framework can incorporate other coreference methods in the ensemble. Mention-pair methods usually comprise of two steps. The first step classifies every mention pair into either coreference or noncoreference with a confidence between 0 and 1. The second step partitions the set of mentions into clusters based on the confidence values, where mentions in each cluster are presumed to be the same underlying entity. Classification We use Soon’s approach (Soon et al., 2001) to select a portion of mention pairs to train a </context>
<context position="11033" citStr="Soon et al., 2001" startWordPosition="1751" endWordPosition="1754">cause this has better generalization (Soon et al., 2001). The positive mention pairs are the anaphoric mention mbi (b = 2, ... , ni) paired with its closest antecedent mention mai (a &lt; b), while the negative mention pairs are the mention mbi paired with each of the intervening mentions ma+1 i , ma+2 i ,... , mb−1 i . Following (Rahman and Ng, 2011a), our binary classifier is SVM with the regularization parameter C. The classifier is trained with the software Liblinear (Fan et al., 2008), which is also used to give probabilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extracted using the Reconcile software (Stoyanov et al., 2010). We use ˆϕa,b ∈ Rd to represent the features of a mention pair (ma, m</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics,, pages 521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Coreference resolution with reconcile.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>156--161</pages>
<contexts>
<context position="1529" citStr="Stoyanov et al., 2010" startWordPosition="230" endWordPosition="233">omain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting. 1 Introduction Coreference resolution is a fundamental component of natural language processing (NLP) and has been widely applied in other NLP tasks (Stoyanov et al., 2010). It gathers together noun phrases (mentions) that refer to the same real-world entity (Ng and Cardie, 2002). In the past decade, several coreference resolution systems have been proposed, e.g., (Ng and Cardie, 2002), (Denis and Baldridge, 2007) and (Stoyanov et al., 2010). All of these focus on the within-domain case — to use the labeled documents from a domain to predict on the unlabeled *The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of la</context>
<context position="11563" citStr="Stoyanov et al., 2010" startWordPosition="1841" endWordPosition="1844">ilistic binary predictions. Clustering We adopt closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) to determine whether a mention pair is coreferent. For each mention, the closest-first method (or best-first method) links it to the the closest (or the best) preceding mention if the confidence value (obtained from the first step) of this mention pair is above a specified threshold t. Features For each mention pair, we use the d = 39 features proposed by Rahman and Ng (2011b) to represent it. These features can be extracted using the Reconcile software (Stoyanov et al., 2010). We use ˆϕa,b ∈ Rd to represent the features of a mention pair (ma, mb). With this feature set, we found that the linear kernel is insufficient to fit the training data. However, using an rbf kernel would be too computationally expensive. Hence, we augment ˆϕa,b with a ˆd-dimensional feature vector [ψ1 · · · ψˆd] to give a new feature vector ϕa,b = [ ˆϕa,b ψ1 ··· ψˆd], (1) where the dˆ augmented features [ψ1 ·· · ψˆd] are determined by ψj = exp(−∥ˆϕa,b − cj∥2 d ), ∀j = 1,. . . , ˆd. (2) Herein, c1,...,cˆd are the dˆ centroids of the randomly-selected subset C from all labeled mention pairs { </context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coreference resolution with reconcile. In Proc. ACL, pages 156–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songbo Tan</author>
<author>Yuefen Wang</author>
<author>Gaowei Wu</author>
<author>Xueqi Cheng</author>
</authors>
<title>Using unlabeled data to handle domaintransfer problem of semantic detection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM symposium on Applied computing, SAC ’08,</booktitle>
<pages>896--903</pages>
<contexts>
<context position="2953" citStr="Tan et al., 2008" startWordPosition="464" endWordPosition="467"> this context, the former domains is called the source domains, while the latter domain is called the target domain (Blitzer et al., 2006; Jiang and Zhai, 2007). Based on the type of the knowledge to be transferred to the target domain, domain adaptation learning can be categorized as instance-based method, feature-based method, parameter-based method or relational-knowledge-based method (Pan and Yang, 2010). Previously, domain adaptation learning has been successfully used in other NLP tasks such as relation extraction (Jiang, 2009) and POS tagging (Jiang and Zhai, 2007), semantic detection (Tan et al., 2008), name entity recognition (Guo et al., 2009) and entity type classification (Jiang and Zhai, 2007). However, to the best of our knowledge, it has yet to be explored for coreference resolution. In this paper, we propose an adaptive ensemble method to adapt coreference resolution across domains. This proposed method can be categorized as both feature-based and parameter-based domain adaptation learning methods. It has three main steps: ensemble creation, cross-domain knowledge learning and decision inference. The first step creates the ensemble by collecting a set of base models, which can be an</context>
</contexts>
<marker>Tan, Wang, Wu, Cheng, 2008</marker>
<rawString>Songbo Tan, Yuefen Wang, Gaowei Wu, and Xueqi Cheng. 2008. Using unlabeled data to handle domaintransfer problem of semantic detection. In Proceedings of the 2008 ACM symposium on Applied computing, SAC ’08, pages 896–903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vemulapalli</author>
<author>X Luo</author>
<author>J F Pitrelli</author>
<author>I Zitouni</author>
</authors>
<title>classifier combination applied to coreference resolution.</title>
<date>2009</date>
<booktitle>In NAACL HLT Student Rsearch Workshop.</booktitle>
<contexts>
<context position="4569" citStr="Vemulapalli et al. (2009)" startWordPosition="708" endWordPosition="711">d learns the cross-domain knowledge fully used in coreference resolution (Pang and Fan, between each target domain and the source domain. 2009; Munson et al., 2005; Rahman and Ng, 2011a). The third step infers the final decision in the target However, these methods only focus on the withindomain based on all ensemble results. domain setting. In addition to domain adaptation, the proposed All these methods comprise of two steps: ensemadaptive ensemble method has the following fea- ble creation and decision inference. Ng and Cardie tures that are absent in the other ensemble methods. (2003) and Vemulapalli et al. (2009) applied the First, it can optimize any user-specified objective bagging and boosting techniques on the documents measure without using a separate development set. to create the ensemble. Recently, Rahman and Ng Second, it can provide document-specific prediction (2011a) further enriched the ensemble by considerinstead of relying on a fixed base model or a fixed ing various feature sets and learning models. Specifset of base models for all documents. Third, it can ically, three types of feature sets (conventional, lexautomatically adjust the active ensemble members ical and combined) and three</context>
</contexts>
<marker>Vemulapalli, Luo, Pitrelli, Zitouni, 2009</marker>
<rawString>S. Vemulapalli, X. Luo, J.F.Pitrelli, and I. Zitouni. 2009. classifier combination applied to coreference resolution. In NAACL HLT Student Rsearch Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th conference on Message understanding, MUC6 ’95,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="8846" citStr="Vilain et al., 1995" startWordPosition="1377" endWordPosition="1380">esolution. We first introduce some notations. For a corpus of N documents, document DZ 745 is the ith document, and it contains ni mentions mi = (m1 i , . . . , mn� i ) with the ordering of each mention as they appear in the document. The index set of all mention pairs in Di is Ei = {(a, b) |1 ≤ a &lt; b ≤ ni}. The transpose of vector x is x′. The performance measure function for document D is Λ(g(D); f(D)), where g(D) and f(D) represent the coreference ground-truth and prediction by model f on document D respectively. In coreference resolution, typical performance measure functions include MUC (Vilain et al., 1995), Rand index (Rand, 1971), B-CUBED (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). In this paper, Λ can either be used as part of an objective function in learning or as an evaluation measure for assessing the performance of a coreference system. We consider the typical domain adaptation problem, which has one target domain t and p (p ≥ 1) source domains s1, ... , sp. The target domain contains N(t) labeled documents and M unlabeled documents, while source domains contain N(s1), ... , N(sp) labeled documents. Unlabeled data in the source domains are not used. We use D(v) i for the ith document</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th conference on Message understanding, MUC6 ’95, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Yang</author>
<author>Rong Jin</author>
</authors>
<title>Distance Metric Learning: A Comprehensive Survey.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science and Engineering, Michigan State University.</institution>
<contexts>
<context position="13681" citStr="Yang and Jin, 2006" startWordPosition="2228" endWordPosition="2231">eatures (Yu and Joachims, 2009; Finley and Joachims, 2005): Φ(Di) = ∑ ϕa,b. (3) (a,b)E£i The distance between a source labeled document D(su) i in domain su and a target labeled document D(t) j is parameterized as Dist(D(su) i , D(t) j ; µ) = µ�∆(D(su) i , D(t) j ), (4) where vector µ ∈ Rd+d is to be learned, and vector function ∆(D(su) i , D(t) j ) ∈ Rd+d is the Euclidean distance vector between two documents given by ∆(D(su) i ,D(t) j ) = (Φ(D(su) i ) − Φ(D(t) j )) ⊙ (Φ(D(su) i ) −Φ(D(t) j )). (5) The operator ⊙ is the element-wise product. Distance (4) is actually the Mahanalobis distance (Yang and Jin, 2006) with the scaling of features: (Φ(D(su) i ) − Φ(D(t) j ))�W (Φ(D(su) i ) − Φ(D(t) j )), where W is a diagonal matrix with diagonal entries µ. Matrix W is diagonal to reduce computation cost and to increase statistical confidence in estimation when there is limited target labeled data (as is typically the case in domain adaptation). That µ is the vector of diagonal entries in W requires that each entry in µ is non-negative. If the lth entry of µ is non-zero, then the lth feature in ϕa,b contribute towards (4). To ensure that at least B features are used, we also constrain that each entry in µ i</context>
</contexts>
<marker>Yang, Jin, 2006</marker>
<rawString>Liu Yang and Rong Jin. 2006. Distance Metric Learning: A Comprehensive Survey. Technical report, Department of Computer Science and Engineering, Michigan State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>1169--1176</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="13092" citStr="Yu and Joachims, 2009" startWordPosition="2110" endWordPosition="2113">s. If multiple domains are provided, we gather all 746 the domain-specific ensembles into a grand ensemble F = F(s1) ∪ ···F(sp) ∪ F(t). 3.2 Cross-domain Knowledge Learning Generally, the feature distributions are different in different domains. Therefore, effective domain adaptation requires using some knowledge of crossdomain similarity. We now propose an approach to learn the parametric-distances between the documents in source and target domains to characterize this cross-domain knowledge. Distances between documents A document Di is represented by the sum of its new mention-pair features (Yu and Joachims, 2009; Finley and Joachims, 2005): Φ(Di) = ∑ ϕa,b. (3) (a,b)E£i The distance between a source labeled document D(su) i in domain su and a target labeled document D(t) j is parameterized as Dist(D(su) i , D(t) j ; µ) = µ�∆(D(su) i , D(t) j ), (4) where vector µ ∈ Rd+d is to be learned, and vector function ∆(D(su) i , D(t) j ) ∈ Rd+d is the Euclidean distance vector between two documents given by ∆(D(su) i ,D(t) j ) = (Φ(D(su) i ) − Φ(D(t) j )) ⊙ (Φ(D(su) i ) −Φ(D(t) j )). (5) The operator ⊙ is the element-wise product. Distance (4) is actually the Mahanalobis distance (Yang and Jin, 2006) with the s</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proc. ICML, pages 1169–1176, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.998552">
Besting the Quiz Master:
Crowdsourcing Incremental Classification Games
</title>
<author confidence="0.994521">
Jordan Boyd-Graber Brianna Satinoff, He He, and Hal Daum´e III
</author>
<affiliation confidence="0.992575">
iSchool and UMIACS Department of Computer Science
University of Maryland University of Maryland
</affiliation>
<email confidence="0.986977">
jbg@umiacs.umd.edu {bsonrisa, hhe, hal}@cs.umd.edu
</email>
<sectionHeader confidence="0.997268" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992081">
Cost-sensitive classification, where the features
used in machine learning tasks have a cost, has
been explored as a means of balancing knowl-
edge against the expense of incrementally ob-
taining new features. We introduce a setting
where humans engage in classification with
incrementally revealed features: the collegiate
trivia circuit. By providing the community with
a web-based system to practice, we collected
tens of thousands of implicit word-by-word
ratings of how useful features are for eliciting
correct answers. Observing humans’ classifi-
cation process, we improve the performance
of a state-of-the art classifier. We also use the
dataset to evaluate a system to compete in the
incremental classification task through a reduc-
tion of reinforcement learning to classification.
Our system learns when to answer a question,
performing better than baselines and most hu-
man players.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999466583333333">
A typical machine learning task takes as input a set
of features and learns a mapping from features to a
label. In such a setting, the objective is to minimize
the error of the mapping from features to labels. We
call this traditional setting, where all of the features
are consumed, rapacious machine learning.1
This not how humans approach the same task.
They do not exhaustively consider every feature. Af-
ter a certain point, a human has made a decision
and no longer needs additional features. Even in-
defatigable computers cannot always exhaustively
consider every feature. This is because the result
</bodyText>
<footnote confidence="0.656008">
1Earlier drafts called this “batch” machine learning, which
confused the distinction between batch and online learning. We
gladly adopt “rapacious” to make this distinction clearer and
to cast traditional machine learning—that always examines all
features—as a resource hungry approach.
</footnote>
<bodyText confidence="0.999895642857143">
is time sensitive, such as in interactive systems, or
because processing time is limited by the sheer quan-
tity of data, as in sifting e-mail for spam (Pujara et
al., 2011). In such settings, often the best solution
is incremental: allow a decision to be made without
seeing all of an instance’s features. We discuss the
incremental classification framework in Section 2.
Our understanding of how humans conduct incre-
mental classification is limited. This is because com-
plicating an already difficult annotation task is often
an unwise tradeoff. Instead, we adapt a real world
setting where humans are already engaging (eagerly)
in incremental classification—trivia games—and de-
velop a cheap, easy method for capturing human
incremental classification judgments.
After qualitatively examining how humans con-
duct incremental classification (Section 3), we show
that knowledge of a human’s incremental classifi-
cation process improves state-of-the-art rapacious
classification (Section 4). Having established that
these data contain an interesting signal, we build
Bayesian models that, when embedded in a Markov
decision process, can engage in effective incremental
classification (Section 5), and develop new hierar-
chical models combining local and thematic content
to better capture the underlying content (Section 7).
Finally, we conclude in Section 8 and discuss exten-
sions to other problem areas.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="introduction">
2 Incremental Classification
</sectionHeader>
<bodyText confidence="0.9995062">
In this section, we discuss previous approaches that
explore how much effort or resources a classifier
needs to come to a decision, a problem not as thor-
oughly examined as the question of whether the de-
cision is right or not.2 Incremental classification is
</bodyText>
<footnote confidence="0.66366525">
2When have an externally interrupted feature stream, the
setting is called “any time” (Boddy and Dean, 1989; Horsch and
Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010),
these are distinct but related problems.
</footnote>
<page confidence="0.824272">
1290
</page>
<note confidence="0.84498">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936263157895">
not equivalent to missing features, which have been
studied at training time (Cesa-Bianchi et al., 2011),
test time (Saar-Tsechansky and Provost, 2007), and
in an online setting (Rostamizadeh et al., 2011). In
contrast, incremental classification allows the learner
to decide whether to acquire additional features.
A common paradigm for incremental classification
is to view the problem as a Markov decision process
(MDP) (Zubek and Dietterich, 2002). The incremen-
tal classifier can either request an additional feature
or render a classification decision (Chai et al., 2004;
Ji and Carin, 2007; Melville et al., 2005), choosing
its actions to minimize a known cost function. Here,
we assume that the environment chooses a feature
in contrast to a learner, as in some active learning
settings (Settles, 2011). In Section 5, we use a MDP
to decide whether additional features need to be pro-
cessed in our application of incremental classification
to a trivia game.
</bodyText>
<subsectionHeader confidence="0.960117">
2.1 Trivia as Incremental Classification
</subsectionHeader>
<bodyText confidence="0.999974869565218">
A real-life setting where humans classify documents
incrementally is quiz bowl, an academic competition
between schools in English-speaking countries; hun-
dreds of teams compete in dozens of tournaments
each year (Jennings, 2006). Note the distinction be-
tween quiz bowl and Jeopardy, a recent application
area (Ferrucci et al., 2010). While Jeopardy also uses
signaling devices, these are only usable after a ques-
tion is completed (interrupting Jeopardy’s questions
would make for bad television). Thus, Jeopardy is
rapacious classification followed by a race to see—
among those who know the answer—who can punch
a button first. Moreover, buzzes before the question’s
end are penalized.
Two teams listen to the same question.3 In this
context, a question is a series of clues (features) re-
ferring to the same entity (for an example question,
see Figure 1). We assume a fixed feature ordering
for a test sequence (i.e., you cannot request specific
features). Teams interrupt the question at any point
by “buzzing in”; if the answer is correct, the team
gets points and the next question is read. Otherwise,
the team loses points and the other team can answer.
</bodyText>
<footnote confidence="0.8623135">
3Called a “starter” (UK) or “tossup” (US) in the lingo, as it
often is followed by a “bonus” given to the team that answers the
starter; here we only concern ourselves with tossups answerable
by both teams.
</footnote>
<bodyText confidence="0.949131">
After losing a race for the Senate, this politician edited the Om-
aha World-Herald. This man resigned ✸ from one of his posts
when the President sent a letter to Germany protesting the Lusi-
tania ✸ sinking, and ✸ he advocated ✸ coining ✸ silver at a 16
✸ to 1 ✸✸ rate ✸ compared to ✸ gold. He was the ✸ three-time
Democratic ✸ Party ✸✸✸ nominee for ✸ President ✸ but ✸✸✸
lost to McKinley twice ✸✸ and then Taft, although he served as
Secretary of State ✸✸ under Woodrow Wilson, ✸ and he later
argued ✸ against Clarence Darrow ✸ in the Scopes ✸✸ Monkey
Trial. For ten points, name this ✸ man who famously declared
that “we shall not be crucified on a Cross of ✸ Gold”. ✸
</bodyText>
<figureCaption confidence="0.998110875">
Figure 1: Quiz bowl question on William Jennings Bryan,
a late nineteenth century American politician; obscure
clues are at the beginning while more accessible clues are
at the end. Words (excluding stop words) are shaded based
on the number of times the word triggered a buzz from any
player who answered the question (darker means more
buzzes; buzzes contribute to the shading of the previous
five words). Diamonds (0) indicate buzz positions.
</figureCaption>
<bodyText confidence="0.9995776">
The answers to quiz bowl questions are well-
known entities (e.g., scientific laws, people, battles,
books, characters, etc.), so the answer space is rel-
atively limited; there are no open-ended questions
of the form “why is the sky blue?” However, there
are no multiple choice questions—as there are in
Who Wants to Be a Millionaire (Lam et al., 2003)—
or structural constraints—as there are in crossword
puzzles (Littman et al., 2002).
Now that we introduced the concepts of questions,
answers, and buzzes, we pause briefly to define them
more formally and explicitly connect to machine
learning. In the sequel, we will refer to: questions,
sequences of words (tokens) associated with a single
answer; features, inputs used for decisions (derived
from the tokens in a question); labels, a question’s
correct response; answers, the responses (either cor-
rect or incorrect) provided; and buzzes, positions in
a question where users halted the stream of features
and gave an answer.
Quiz bowl is not a typical problem domain for natu-
ral language processing; why should we care about it?
First, it is a real-world instance of incremental classi-
fication that happens hundreds of thousands of times
most weekends. Second, it is a classification problem
intricately intertwined with core computational lin-
guistics problems such as anaphora resolution, online
sentence processing, and semantic priming. Finally,
quiz bowl’s inherent fun makes it easy to acquire
human responses, as we describe in the next section.
</bodyText>
<page confidence="0.984849">
1291
</page>
<figure confidence="0.875101">
Number of Tokens Revealed
</figure>
<figureCaption confidence="0.821065857142857">
Figure 2: Users plotted based on accuracy vs. the number
of tokens—on average—the user took to give an answer.
Dot size and colour represent the total number of ques-
tions answered. Users that answered questions later in the
question had higher accuracy. However, there were users
that were able to answer questions relatively early without
sacrificing accuracy.
</figureCaption>
<sectionHeader confidence="0.826507" genericHeader="method">
3 Getting a Buzz through Crowdsourcing
</sectionHeader>
<bodyText confidence="0.99997352">
We built a corpus with 37,225 quiz bowl questions
with 25,498 distinct labels from 121 tournaments
written for tournaments between 1999 and 2010. We
created a webapp4 that simulates the experience of
playing quiz bowl. Text is incrementally revealed
(at a pace adjustable by the user) until users press
the space bar to “buzz”. Users then answer, and the
webapp judges correctness using a string matching
algorithm. Players can override the automatic check
if the system mistakenly judged an answer incorrect.
Answers of previous users are displayed after answer-
ing a question; this enhances the sense of community
and keeps users honest (e.g., it’s okay to say that “wj
bryan” is an acceptable answer for the label “william
jennings bryan”, but “asdf” is not). We did not see
examples of nonsense answers from malicious users;
in contrast, users were stricter than we expected, per-
haps because protesting required effort.
To collect a set of labels with many buzzes, we
focused on the 1186 labels with more than four dis-
tinct questions. Thus, we shuffled the labels into a
canonical order shown to all users (e.g., everyone
saw a question on “Jonathan Swift” and then a ques-
tion on “William Jennings Bryan”, but because these
labels have many questions the specific questions
</bodyText>
<footnote confidence="0.9844565">
4Play online or download the datasets at http://umiacs.
umd.edu/˜jbg/qb.
</footnote>
<figureCaption confidence="0.86925425">
Figure 3: A screenshot of the webapp used to collect data.
Users see a question revealed one word at a time. They
signal buzzes by clicking on the answer button and input
an answer.
</figureCaption>
<bodyText confidence="0.999978833333333">
were different for each user). Participants were ea-
ger to answer questions; over 7000 questions were
answered in the first day, and over 43000 questions
were answered in two weeks by 461 users.
To represent a “buzz”, we define a function b(q, f)
(“b” for buzz) as the number of times that feature
f occurred in question q at most five tokens before
a user correctly buzzed on that question.5 Aggre-
gating buzzes across questions (summing over q)
shows different features useful for eliciting a buzz
(Figure 4(a)). Some features coarsely identify the
type of answer sought, e.g., “author”, “opera”, “city”,
“war”, or “god”. Other features are relational, con-
necting the answer to other clues, e.g., “namesake”,
“defeated”, “husband”, or “wrote”. The set of buzzes
help narrow which words are important for matching
a question to its answer; for an example, see how
the word cloud for all of the buzzes on “Wuthering
</bodyText>
<footnote confidence="0.86853625">
5This window was chosen qualitatively by examining the
patterns of buzzes; this is person-dependent, based on reading
comprehension, reaction time, and what reveal speed the user
chose. We leave explicitly modeling this for future work.
</footnote>
<figure confidence="0.9704218">
1.0
Total
500
1000
1500
2000
2500
3000
0.8
Accuracy
0.6
0.4
40 60 80 100
1292
(a) Buzzes over all Questions (b) Wuthering Heights Question Text (c) Buzzes on Wuthering Heights
</figure>
<figureCaption confidence="0.859048333333333">
Figure 4: Word clouds representing all words that were a part of a buzz (a), the original text appearing in seven questions
on the book “Wuthering Heights” by Emily Br¨onte (b), and the buzzes of users on those questions (c). The buzzes
reflect what users remember about the work and is more focused than the complete question text.
</figureCaption>
<bodyText confidence="0.995553666666667">
Heights” (Figure 4(c)) is much more focused than the
word cloud for all of the words from the questions
with that label (Figure 4(b)).
</bodyText>
<sectionHeader confidence="0.993345" genericHeader="method">
4 Buzzes Reveal Useful Features
</sectionHeader>
<bodyText confidence="0.999962608695652">
If we restrict ourselves to a finite set of labels, the
process of answering questions is a multiclass clas-
sification problem. In this section, we show that in-
formation gleaned from humans making a similar de-
cision can help improve rapacious machine learning
classification. This validates that our crowdsourcing
technique is gathering useful information.
We used a state-of-the-art maximum entropy clas-
sification model, MEGAM (Daum´e III, 2004), that
accepts a per-class mean prior for feature weights
and applied MEGAM to the 200 most frequent labels
(11,663 questions, a third of the dataset). The prior
mean of the feature weight is a convenient, simple
way to incorporate human feature utility; apart from
the mean, all default options are used.
Specifying the prior requires us to specify a weight
for each pair of label and feature. The weight com-
bines buzz information (described in Section 3) and
tf-idf (Salton, 1968). The tf-idf value is computed by
treating the training set of questions with the same
label as a single document.
Buzzes and tf-idf information were combined into
the prior µ for label a and feature f as µa,f =
</bodyText>
<equation confidence="0.991834">
[ ]
Qb(a, f) + αI [b(a, f) &gt; 0] + -y tf-idf(a, f). (1)
</equation>
<bodyText confidence="0.9998292">
We describe our weight strategies in increasing order
of human knowledge. If α, Q, and -y are zero, this
is a naive zero prior. If -y only is nonzero, this is a
linear transformation of features’ tf-idf. If only α
is nonzero, this is a linear transformation of buzzed
</bodyText>
<table confidence="0.681998333333333">
Weighting α Q 7 Error
zero - - - 0.37
tf-idf - - 3.5 0.14
buzz-binary 7.1 - - 0.10
buzz-linear - 1.5 - 0.16
buzz-tier - 1.1 0.1 0.09
</table>
<tableCaption confidence="0.980176">
Table 1: Classification error of a rapacious classifier able
to draw on human incremental classification. The best
weighting scheme for each dataset is in bold. Missing
parameter values (-) mean that the parameter is fixed to
zero for that weighting scheme.
</tableCaption>
<bodyText confidence="0.996657357142857">
words’ tf-idf weights. If only Q is non-zero, num-
ber of buzzes is now a linear multiplier of the tf-idf
weight (buzz-linear). Finally we allow unbuzzed
words to have a separate linear transformation if both
Q and -y are non-zero (buzz-tier).
Grid search (width of 0.1) on development set error
was used to set parameters. Table 1 shows test error
for weighting schemes and demonstrates that adding
human information as a prior improves classification
error, leading to a 36% error reduction over tf-idf
alone. While not directly comparable (this classifier
is rapacious, not incremental, and has a predefined
answer space), the average user had an error rate of
16.7%.
</bodyText>
<sectionHeader confidence="0.861419" genericHeader="method">
5 Building an Incremental Classifier
</sectionHeader>
<bodyText confidence="0.999948333333333">
In the previous section we improved rapacious classi-
fication using humans’ incremental classification. A
more interesting problem is how to compete against
humans in incremental classification. While in the
previous section we used human data for a training
set, here we use human data as an evaluation set.
Doing so requires us to formulate an incremental rep-
resentation of the contents of questions and to learn
a strategy to decide when to buzz.
</bodyText>
<page confidence="0.911618">
1293
</page>
<bodyText confidence="0.9999904">
Because this is the first machine learning algo-
rithm for quiz bowl, we attempt to provide reason-
able rapacious baselines and compare against our new
strategies. We believe that our attempts represent a
reasonable explanation of the problem space, but ad-
ditional improvements could improve performance,
as discussed in Section 8.
A common way to represent state-dependent strate-
gies is via a Markov decision process (MDP). The
most salient component of a MDP is the policy, i.e., a
mapping from the state space to an action. In our con-
text, a state is a sequence of (thus far revealed) tokens,
and the action is whether to buzz or not. To learn a
policy, we use a standard reinforcement learning tech-
nique (Langford and Zadrozny, 2005; Abbeel and
Ng, 2004; Syed et al., 2008): given a representation
of the state space, learn a classifier that can map from
a state to an action. This is also a common paradigm
for other incremental tasks, e.g., shift-reduce pars-
ing (Nivre, 2008).
Given examples of the correct answer given a con-
figuration of the state space, we can learn a MDP
without explicitly representing the reward function.
In this section, we define our method of defining
actions and our representation of the state space.
</bodyText>
<subsectionHeader confidence="0.999508">
5.1 Action Space
</subsectionHeader>
<bodyText confidence="0.995845156862746">
We assume that there are only two possible actions:
buzz now or wait. An alternative would be a more
expressive action space (e.g., an action for every pos-
sible answer). However, this conflates the question
of when to buzz with what to answer. Instead, we call
the distinct component that provides what to answer
the content model. We describe an initial content
model in Section 5.2, below, and improve the models
further in Section 7. For the moment, assume that
a content model maintains a posterior distribution
over labels and when needed can provide its best
guess (e.g., given the features seen, the best answer
is “William Jennings Bryan”).
Given the action space, we need to specify where
examples of state space and action come from. In
the language of classification, we need to provide
(x, y) pairs to learn a mapping x H y. The clas-
sifier attempts to learn that action y is (“buzz”) in
all states where the content model gave a correct re-
sponse given state x. Negative examples (“wait”)
are applied to states where the content model gave
a wrong answer. Every token in our training set cor-
responds to a classification example; both states are
prevalent enough that we do not to explicitly need to
address class imbalance. This resembles approaches
that merge different classifiers (Riedel et al., 2011) or
attempt to estimate confidence of models (Blatz et al.,
2004). However, here we use partial observations.
This is a simplification of the problem and corre-
sponds to a strategy of “buzz as soon as you know the
answer”, ignoring all other factors. While reasonable,
this is not always optimal. For example, if you know
your opponent is unlikely to answer a question, it is
better to wait until you are more confident. Incorrect
answers might also help your opponent, e.g., by elim-
inating an incorrect answer. Moreover, strategies in a
game setting (rather than a single question) are more
complicated. For example, if a right answer is worth
+10 points and the penalty for an incorrect question
is −5, then a team leading by 15 points on the last
question should never attempt to answer. Investigat-
ing such gameplay strategies would require a “roll
out” of game states (Tesauro and Galperin, 1996) to
explore the efficacy of such strategies. While inter-
esting, we leave these issues to future work.
We also investigated learning a policy directly
from users’ buzzes directly (Abbeel and Ng, 2004),
but this performed poorly because the content model
is incompatible with the players’ abilities and the
high variation in players’ ability and styles (compare
Figure 2).
</bodyText>
<subsectionHeader confidence="0.999424">
5.2 State Space
</subsectionHeader>
<bodyText confidence="0.999600533333333">
Recall that our goal is to learn a classifier that maps
states to actions; above, we defined the action space
(the classifier’s output) but not the state space, the
classifier’s input. The straightforward parameteriza-
tion of the state space would be all of the words that
have been revealed. However, such a feature set is
very sparse.
We use three components to form the state space:
what information has been observed, what the content
model believes is the correct answer, how confident
the content model is, and whether the content model’s
confidence is changing. We describe each in more
detail below.
Text In addition to the obvious, sparse parameter-
ization that contains all of the features thus far ob-
</bodyText>
<page confidence="0.97252">
1294
</page>
<bodyText confidence="0.999882965517241">
served, we also include the total number of tokens
revealed and whether the phrase “for ten points” has
appeared.6
Guess An additional feature that we used to repre-
sent the state space is the current guess of the content
model; i.e., the argmax of the posterior.
Posterior The posterior feature (Pos for short) cap-
tures the shape of the posterior distribution: the prob-
ability of the current guess (the max of the poste-
rior), the difference between the top two probabilities
and the probabilities associated with the fifteen most
probable labels under the posterior.
Change As features are revealed, there is often
a rapid transition from a state of confusion—when
there are many candidates with no clear best choice—
to a state of clarity with the posterior pointing to only
one probable label. To capture when this happens,
we add a binary feature to reflect when the best guess
has changed when a single feature has been revealed.
Other Features We thought that other features
would be useful. While useful on their own, no
features that we tried were useful when the content
model’s posterior was also used as a feature. Fea-
tures that we attempted to use were: a logistic re-
gression model attempting to capture the probability
that any player would answer (Silver et al., 2008), a
regression predicting how many individuals would
buzz in the next n words, the year the question was
written, the category of the question, etc.
</bodyText>
<subsectionHeader confidence="0.766611">
5.3 Naive Content Model
</subsectionHeader>
<bodyText confidence="0.999943272727273">
The action space is only deciding when to answer,
having abdicated responsibility for what to answer.
So where does do the answers come from? We as-
sume that at any point we can ask “what is the highest
probability label given my current feature observa-
tions?” We call the component of our model that
answers this question the content model.
Our first content model is a naive Bayes
model (Lewis, 1998) trained over a text collection.
This generative model assumes labels for questions
come from a multinomial distribution 0 — Dir(α)
</bodyText>
<footnote confidence="0.96299525">
6The phrase “for ten points” (abbreviated FTP) appears in
all quiz bowl questions to signal the question’s last sentence or
clause. It is a signal to answer soon, as the final “giveaway” clue
is next.
</footnote>
<bodyText confidence="0.999598909090909">
and assumes that label l has a word distribution
Bl — Dir(A). Each question n has a label zn and
its words are generated from B,;n. Given labeled ob-
servations, we use the maximum a posteriori (MAP)
estimate of Bl.
Why use a generative model when a discriminative
classifier could use a richer feature space? The most
important reason is that, by definition, it makes sense
to ask a generative model the probability of a label
given a partial observation; such a question is not
well-formed for discriminative models, which expect
a complete feature set. Another important consid-
eration is that generative models can predict future,
unrevealed features (Chai et al., 2004); however, we
do not make use of that capability here.
In addition to providing our answers, the content
model also provides an additional, critically impor-
tant feature for our state space: its posterior (pos
for short) probability. With every revealed feature,
the content model updates its posterior distribution
over labels given that t tokens have been revealed in
question n,
</bodyText>
<equation confidence="0.976151">
p(zn  |wi ... wt, 0, 0). (2)
</equation>
<bodyText confidence="0.999905529411765">
To train our naive Bayes model, we semi-
automatically associate labels with a Wikipedia page
(correcting mistakes manually) and then form the
MAP estimate of the class multinomial distribution
from the Wikipedia page’s text. We did this for the
1065 labels that had at least three human answers,
excluding ambiguous labels associated with multiple
concepts (e.g., “europa”, “steppenwolf”, “georgia”,
“paris”, and “v”).
Features were taken to be the 25,000 most frequent
tokens and bigrams7 that were not stop words; fea-
tures were extracted from the Wikipedia text in the
same manner as from the question tokens.8
After demonstrating our ability to learn an incre-
mental classifier using this simple content model, we
extend the content model to capture local context and
correlations between similar labels in Section 7.
</bodyText>
<footnote confidence="0.995211571428571">
7We used NLTK (Loper and Bird, 2002) to filter stop words
and we used a x2 test to identify bigrams with that rejected the
null hypothesis at the 0.01 level.
8The Dirichlet scaling parameter A was set to 10,000 given
our relatively large vocabulary (25,000) and to not penalize a
label’s posterior probability if there were unseen features; this
corresponds to a pseudocount of 0.4. α was set to 1.0.
</footnote>
<page confidence="0.988704">
1295
</page>
<sectionHeader confidence="0.970284" genericHeader="method">
6 Pitting the Algorithm Against Humans
</sectionHeader>
<bodyText confidence="0.999932458333333">
With a state space and a policy, we now have all the
necessary ingredients to have our algorithm compete
against humans. Classification, which allows us to
learn a policy, was done using the default settings of
LIBLINEAR (Fan et al., 2008). To determine where
the algorithm buzzes, we provide a sequence of state
spaces until the policy classifier determines that it is
time to buzz.
We simulate competition by taking the human an-
swers and buzzes as a given and ask our algorithm
(independently) to provide its decision on when to
buzz on a test set. We compare the two buzz positions.
If the algorithm buzzed earlier with the right answer,
we consider it to have “won” the question; equiva-
lently, if the algorithm buzzed later, we consider it to
have “lost” that question. Ties are rare (less than 1%
of cases), as the algorithm had significantly different
behavior from humans; in the case where there was a
tie, ties were broken in favor of the machine.
Because we have a large, diverse population an-
swering questions, we need aggregate measures of
human performance to get a comprehensive view of
algorithm performance. We use the following metrics
for each question in the test set:
</bodyText>
<listItem confidence="0.99893875">
• best: the earliest anyone buzzed correctly
• median: the first buzz after 50% of human buzzes
• mean: for each recorded buzz compute a reward and
we average over all rewards
</listItem>
<bodyText confidence="0.696792">
We compare the algorithm against baseline strategies:
</bodyText>
<listItem confidence="0.9977122">
• rap The rapacious strategy waits until the end of the
question and answers the best answer possible.
• ftp Waiting until when “for 10 points” is said, then
giving the best answer possible.
• index,,, Waiting until the first feature after the nth to-
</listItem>
<bodyText confidence="0.935312588235294">
ken has been processed, then giving the best answer
possible. The indices were chosen as the quartiles
for question length (by convention, most questions
are of similar length).
We compare these baselines against policies that de-
cide when to buzz based on the state.
Recall that the non-oracle algorithms were un-
aware of the true reward function. To best simulate
conventional quiz bowl settings, a correct answer
was +10 and the incorrect answer was −5. The full
payoff matrix for the computer is shown in Table 2.
Cases where the opponent buzzes first but is wrong
are equivalent to rapacious classification, as there is
no longer any incentive to answer early. Thus we
exclude such situations (Outcomes 3, 5, 6 in Table 2)
from the dataset to focus on the challenge of process-
ing clues incrementally.
</bodyText>
<table confidence="0.991958428571429">
Computer Human Payoff
1 first and wrong right −15
2 first and correct −10
3 first and wrong wrong −5
4 first and correct — +10
5 wrong first and wrong +5
6 right first and wrong +15
</table>
<tableCaption confidence="0.992939">
Table 2: Payoff matrix (from the computer’s perspective)
</tableCaption>
<bodyText confidence="0.956818285714286">
for when agents “buzz” during a question. To focus on
incremental classification, we exclude instances where the
human interrupts with an incorrect answer, as after an
opponent eliminates themselves, the answering reduces to
rapacious classification.
Table 3 shows the algorithm did much better when
it had access to the posterior. While incremental
algorithms outperform rapacious baselines, they lose
to humans. Against the median and average players,
they lose between three and four points per question,
and nearly twice that against the best players.
Although the content model is simple, this poor
performance is not from the content model never
producing the correct answer. To see this, we also
computed the optimal actions that could be executed.
We called this strategy the oracle strategy; it was able
to consistently win against its opponents. Thus, while
the content model was able to come up with correct
answers often enough to on average win against oppo-
nents (even the best human players), we were unable
to consistently learn winning policies.
There are two ways to solve this problem: create
deeper, more nuanced policies (or the features that
feed into them) or refine content models that provide
the signal needed for our policies to make sound
decisions. We chose to refine the content model, as
we felt we had added all of the obvious features for
learning effective policies.
</bodyText>
<sectionHeader confidence="0.993514" genericHeader="method">
7 Expanding the Content Model
</sectionHeader>
<bodyText confidence="0.9979645">
When we asked quiz bowlers how they answer ques-
tions, they said that they first determine the category
</bodyText>
<page confidence="0.973323">
1296
</page>
<table confidence="0.998591636363636">
Strategy Features Mean Best Median Index
text -8.72 -10.04 -6.50 40.36
Classify +guess -5.71 -8.40 -3.95 66.02
+pos -4.13 -7.56 -2.70 67.97
+change -4.02 -7.41 -2.63 77.33
Oracle text 3.36 0.61 4.35 49.90
all -6.61 -9.03 -4.42 100.19
ftp -5.22 -8.62 -4.23 88.65
Rapacious index30 -7.89 -8.71 -6.41 32.23
Baseline indexs0 -5.16 -7.56 -3.71 61.90
index90 -5.02 -8.62 -3.50 87.13
</table>
<tableCaption confidence="0.932492">
Table 3: Performance of strategies against users. The
human scoring columns show the average points per ques-
tion (positive means winning on average, negative means
losing on average) that the algorithm would expect to ac-
cumulate per question versus each human amalgam metric.
The index column notes the average index of the token
when the strategy chose to buzz.
</tableCaption>
<bodyText confidence="0.997940117647059">
of a question, which substantially narrows the an-
swer space. Ideally, the content model should con-
duct the same calculus—if a question seems to be
about mathematics, all answers related with mathe-
matics should be more likely in the posterior. This
was consistent with our error analysis; many errors
were non-sensical (e.g., answering “entropy” for “Jo-
hannes Brahms”, when an answer such as “Robert
Schumann”, another composer, would be better).
In addition, assuming independence between fea-
tures given a label causes us to ignore potentially
informative multiword expressions such as quota-
tions, titles, or dates. Adding a language model to
our content model allows us to capture some of these
phenomena.
To create a model that jointly models categories
and local context, we propose the following model:
</bodyText>
<listItem confidence="0.8130275">
1. Draw a distribution over labels φ - Dir(α)
2. Draw a background distribution over words θ0 -
</listItem>
<figure confidence="0.714489166666667">
Dir(λ0~1)
(a) For each category c of questions, draw a distribution
over words θc - Dir(λ1θ0).
i. For each label l in category c, draw a distribu-
tion over words θl,c - Dir(λ2θc)
A. For each type v, draw a bigram distribution
θl,c,v - Dir(λ3θl,c)
3. Draw a distribution over labels φ - Dir(α).
4. For each question with category c and N words, draw
answer l - Mult(φ):
(a) Assume w0 = START
(b) Draw wn - Mult(θl,c,w,_,) for n E {1 ... N}
</figure>
<bodyText confidence="0.996600421052632">
This creates a language model over categories, la-
bels, and observed words (we use “words” loosely, as
bigrams replace some word pairs). By constructing
the word distributions using hierarchical distributions
based on domain and ngrams (a much simpler para-
metric version of more elaborate methods (Wood and
Teh, 2009)), we can share statistical strength across
related contexts. We assume that labels are (only)
associated with their majority category as seen in our
training data and that category assignments are ob-
served. All scaling parameters A were set to 10,000,
α was 1.0, and the vocabulary was still 25,000.
We used the maximal seating assignment (Wallach,
2008) for propagating counts through the Dirichlet
hierarchy. Thus, if the word v appeared Bl,u,v times
in label l following a preceding word u, 5l,v times in
label l, Tc,v times in category c, and Gv times in total,
we estimate the probability of a word v appearing
in label k, category t, and after word u as p(wn =
</bodyText>
<equation confidence="0.9410034">
v  |lab = l, cat = c, wn_1 = u; X) =
Gv+λ0/V
Tc,v+λ1 G·+λ0
Bl,u,v + A3
Bl,u,· + A3
</equation>
<bodyText confidence="0.999973083333333">
where we use · to represent marginalization, e.g.
Tc,· = Ev, Tc,v,. As with naive Bayes, Bayes’ rule
provides posterior label probabilities (Equation 2).
We compare the naive model with models that
capture more of the content in the text in Table 4;
these results also include intermediate models be-
tween naive Bayes and the full content model: “cat”
(omit 2.a.i.A) and “bigram” (omit 2.a). These models
perform much better than the naive Bayes models
seen in Table 3. They are about even against the
mean and median players and lose four points per
question against top players.
</bodyText>
<subsectionHeader confidence="0.999274">
7.1 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.9998385">
In this section, we explore what defects are prevent-
ing the model presented here from competing with
top players, exposing challenges in reinforcement
learning, interpreting pragmatic cues, and large data.
Three examples of failures of the model are in Fig-
ure 5. This model is the best performing model of
the previous section.
Too Slow The first example is a question on Mau-
rice Ravel, a French composer known for Bol´ero. The
question leads off with Ravel’s orchestral version of
</bodyText>
<equation confidence="0.766969">
Sl,v+λ2
Tc,·+λ2
Sl,·+λ2
, (3)
</equation>
<page confidence="0.933793">
1297
</page>
<table confidence="0.99822">
Strategy Model Mean Best Median Index
Classify naive -4.02 -7.41 -2.63 77.33
cat -1.69 -5.22 0.12 67.97
bigram -3.80 -7.66 -2.51 78.69
bgrm+cat -0.86 -4.46 0.83 63.42
Oracle naive 3.36 0.61 4.35 49.90
cat 4.48 1.64 5.47 47.88
bigram 3.58 0.87 4.61 49.34
bgrm+cat 4.67 1.99 5.74 46.49
</table>
<tableCaption confidence="0.98882375">
Table 4: As in Table 3, performance of strategies against
users, but with enhanced content models. Modeling both
bigrams and label categories improves overall perfor-
mance.
</tableCaption>
<bodyText confidence="0.999428">
Mussorgsky’s piano piece “Pictures at an Exhibition”.
Based on that evidence, the algorithm considers “Pic-
tures at an Exhibition” the most likely but does not
yet buzz. When it receives enough information to be
sure about the correct answer, over half of the players
had already buzzed. Correcting this problem would
require a more aggressive strategy, perhaps incorpo-
rating the identity of the opponent or estimating the
difficulty of the question.
Mislead by the Content Model The second ex-
ample is a question on Enrico Fermi, an Italian-
American physicist. The first clues are about mag-
netic fields near a Fermi surface, which causes the
content model to view “magnetic field” as the most
likely answer. The question’s text, however, has
pragmatic cues “this man” and “this Italian” which
would have ruled out the abstract answer “magnetic
field”. Correcting this would require a model that
jointly models content and bigrams (Hardisty et
al., 2010), has a coreference system as its content
model (Haghighi and Klein, 2007), or determines the
correct question type (Moldovan et al., 2000).
Insufficient Data The third example is where our
approach had no chance. The question is a very diffi-
cult question about George Washington, America’s
first president. As a sign of its difficulty, only half
the players answered correctly, and only near the end
of the question. The question concerns lesser known
episodes from Washington’s life, including a mistress
caught in the elements. To the content model, of the
several hypotheses it considers, the closest match
it can find is “Yasunari Kawabata”, who wrote the
novel Snow Country, whose plot matches some of
these keywords. To answer these types of question,
</bodyText>
<subsectionHeader confidence="0.42786">
Tokens Revealed
</subsectionHeader>
<figureCaption confidence="0.998574785714286">
Figure 5: Three questions where our algorithm performed
poorly. It gets “Maurice Ravel” (top) right but only after
over half the humans had answered correctly (i.e., the
buzz’s hexagon appears when the cyan line is above 0.6);
on “Enrico Fermi” (middle) it confuses the correct type
of answer (person vs. concept); on “George Washington”
(bottom) it lacks information to answer correctly. Lines
represent the current estimate posterior probability of the
answer (red) and the proportion of opponents who have
answered the question correctly (cyan). The label of each
of the three questions is above each chart. Words are in
black with arrows and arrows, and the current argmax
answer is at the bottom of the graph in red. The buzz
location is the hexagon.
</figureCaption>
<figure confidence="0.999213632653061">
Posterior Opponent
maurice ravel
magnetic
magnetic field magnetic field neutrino
paradox
enrico fermi
zero
this_man
this_italian
0 20 40 60 80 100
Tokens Revealed
generals
mistress
language
chill
charlemagne yasunari kawabata
george washington
0 10 20 30 40 50 60
1.0
0.8
0.6
0.4
0.2
0.0
0.8
0.6
0.4
0.2
0.0
0.8
0.6
0.4
pictures
0.2
0.0
1.0
orchestrated
0 10 20 30 40 50 60 70
pictures at an exhibition
maurice ravel
this_french
composer
bolero
Tokens Revealed
Observation
Buzz
Prediction
answer
feature
</figure>
<page confidence="0.972005">
1298
</page>
<bodyText confidence="0.999973">
the repository used to train the content model would
have to be orders of magnitude larger to be able to
link the disparate clues in the question to a consistent
target. The content model would also benefit from
weighting later (more informative) features higher.
</bodyText>
<subsectionHeader confidence="0.994905">
7.2 Assumptions
</subsectionHeader>
<bodyText confidence="0.999935">
We have made assumptions to solve a problem that
is subtly different that the game of quiz bowl that
a human would play. Some of these were simpli-
fying assumptions, such as our assumption that the
algorithm has a closed set of possible answers (Sec-
tion 5.3). Even with this advantage, the algorithm is
unable to compete with human players, who choose
answers from an unbounded set. On the other hand,
to focus on incremental classification, we idealized
our human opponents so that they never give incor-
rect answers (Section 6). This causes our estimates
of our performance to be lower than they would be
against real players.
</bodyText>
<sectionHeader confidence="0.993805" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988238095238">
We make three contributions. First, we introduce a
new setting for exploring the problem of incremental
classification: trivia games. This problem is intrin-
sically interesting because of its varied topics and
competitive elements, has a great quantity of stan-
dardized, machine-readable data, and also has the
boon of being cheaply and easily annotated. We took
advantage of that ease and created a framework for
quickly and efficiently gathering examples of humans
doing incremental classification.
There are other potential uses for the dataset; the
progression of clues from obscure nuggets to could
help determine how “known” a particular aspect of
an entity is (e.g., that William Jennings Bryant gave
the “Cross of Gold” speech is better known his resig-
nation after the Lusitania sinking, Figure 1). Which
could be used in educational settings (Smith et al.,
2008) or summarization (Das and Martins, 2007).
The second contribution shows that humans’ incre-
mental classification improves state-of-the-art rapa-
cious classification algorithms. While other frame-
works (Zaidan et al., 2008) have been proposed to
incorporate user clues about features, the system de-
scribed here provides analogous features without the
need for explicit post-hoc reflection, has faster anno-
tation throughput, and is much cheaper.
The problem of answering quiz bowl questions is
itself a challenging task that combines issues from
language modeling, large data, coreference, and re-
inforcement learning. While we do not address all
of these problems, our third contribution is a sys-
tem that learns a policy in a MDP for incremental
classification even in very large state spaces; it can
successfully compete with skilled human players.
Incorporating richer content models is one of our
next steps. This would allow us to move beyond the
closed-set model and use a more general coreference
model (Haghighi and Klein, 2007) for identifying
answers and broader corpora for training. In addi-
tion, using larger corpora would allow us to have
more comprehensive doubly-hierarchical language
models (Wood and Teh, 2009). We are also inter-
ested in adding richer models of opponents to the
state space that would adaptively adjust strategies as
it learned more about the strengths and weaknesses
of its opponent (Waugh et al., 2011).
Further afield, our presentation of sentences
closely resembles paradigms for cognitive experi-
ments in linguistics (Thibadeau et al., 1982) but are
much cheaper to conduct. If online processing ef-
fects (Levy et al., 2008; Levy, 2011) could be ob-
served in buzzing behavior; e.g., if a confusingly
worded phrase depresses buzzing probability, it could
help validate cognitively-inspired models of online
sentence processing.
Incremental classification is a natural problem,
both for humans and resource-limited machines.
While our data set is trivial (in a good sense), learn-
ing how humans process data and make decisions
in a cheap, easy crowdsourced application can help
us apply new algorithms to improve performance in
settings where features aren’t free, either because of
computational or annotation cost.
</bodyText>
<page confidence="0.996971">
1299
</page>
<sectionHeader confidence="0.999211" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999984">
We thank the many players who played our online
quiz bowl to provide our data (and hopefully had fun
doing so) and Carlo Angiuli, Arnav Moudgil, and
Jerry Vinokurov for providing access to quiz bowl
questions. This research was supported by NSF grant
#1018625. Jordan Boyd-Graber is also supported by
the Army Research Laboratory through ARL Cooper-
ative Agreement W911NF-09-2-0072. Any opinions,
findings, conclusions, or recommendations expressed
are the authors’ and do not necessarily reflect those
of the sponsors.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995155269662921">
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of International Conference of Machine
Learning.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In Proceed-
ings of the Association for Computational Linguistics.
Mark Boddy and Thomas L. Dean. 1989. Solving time-
dependent planning problems. In International Joint
Conference on Artificial Intelligence, pages 979–984.
Morgan Kaufmann Publishers, August.
Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad
Shamir. 2011. Efficient learning with partially ob-
served attributes. Journal of Machine Learning Re-
search, 12:2857–2878.
Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X.
Ling. 2004. Test-cost sensitive naive bayes classi-
fication. In IEEE International Conference on Data
Mining.
Dipanjan Das and Andre Martins. 2007. A survey on
automatic text summarization. Engineering and Tech-
nology, 4:192–195.
Hal Daum´e III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name/
˜daume04cg-bfgs, implementation available at
http://hal3.name/megam/.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building Watson:
An Overview of the DeepQA Project. AI Magazine,
31(3).
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the Association for Computational
Linguistics.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Michael C. Horsch and David Poole. 1998. An anytime
algorithm for decision making under uncertainty. In
Proceedings of Uncertainty in Artificial Intelligence.
Ken Jennings. 2006. Brainiac: adventures in the curious,
competitive, compulsive world of trivia buffs. Villard.
Shihao Ji and Lawrence Carin. 2007. Cost-sensitive fea-
ture acquisition and classification. Pattern Recognition,
40:1474–1485, May.
Shyong K. Lam, David M. Pennock, Dan Cosley, and
Steve Lawrence. 2003. 1 billion pages = 1 million
dollars? mining the web to play ”who wants to be a mil-
lionaire?”. In Proceedings of Uncertainty in Artificial
Intelligence.
John Langford and Bianca Zadrozny. 2005. Relating
reinforcement learning performance to classification
performance. In Proceedings of International Confer-
ence of Machine Learning.
Roger P. Levy, Florencia Reali, and Thomas L. Griffiths.
2008. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings ofAdvances in Neural Information Processing
Systems.
Roger Levy. 2011. Integrating surprisal and uncertain-
input models in online sentence comprehension: formal
techniques and empirical results. In Proceedings of the
Association for Computational Linguistics.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
N´edellec and C´eline Rouveirol, editors, Proceedings
of European Conference of Machine Learning, number
1398.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artif. Intell., 134(1-2):23–55, January.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond J. Mooney. 2005. An expected utility
approach to active feature-value acquisition. In Inter-
national Conference on Data Mining, November.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
</reference>
<page confidence="0.733385">
1300
</page>
<reference confidence="0.999258657142857">
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Association for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513–553, December.
Jay Pujara, Hal Daume III, and Lise Getoor. 2011. Using
classifier cascades for scalable e-mail classification.
In Collaboration, Electronic Messaging, Anti-Abuse
and Spam Conference, ACM International Conference
Proceedings Series.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model combination for event extraction in bionlp 2011.
In Proceedings of the BioNLP Workshop.
Afshin Rostamizadeh, Alekh Agarwal, and Peter L.
Bartlett. 2011. Learning with missing features. In
Proceedings of Uncertainty in Artificial Intelligence.
Maytal Saar-Tsechansky and Foster Provost. 2007. Han-
dling missing values when applying classification mod-
els. Journal of Machine Learning Research, 8:1623–
1657, December.
Gerard. Salton. 1968. Automatic Information Organiza-
tion and Retrieval. McGraw Hill Text.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of Emperical Methods
in Natural Language Processing.
David Silver, Richard S. Sutton, and Martin M¨uller. 2008.
Sample-based learning and search with permanent and
transient memories. In International Conference on
Machine Learning.
Noah A. Smith, Michael Heilman, and Rebecca Hwa.
2008. Question generation as a competitive under-
graduate course project. In Proceedings of the NSF
Workshop on the Question Generation Shared Task and
Evaluation Challenge.
Umar Syed, Michael Bowling, and Robert E. Schapire.
2008. Apprenticeship learning using linear program-
ming. In Proceedings of International Conference of
Machine Learning.
Gerald Tesauro and Gregory R. Galperin. 1996. On-line
policy improvement using monte-carlo search. In Pro-
ceedings ofAdvances in Neural Information Processing
Systems.
Robert Thibadeau, Marcel A. Just, and Patricia A. Carpen-
ter. 1982. A model of the time course and content of
reading. Cognitive Science, 6.
Hanna M Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Lidan Wang, Donald Metzler, and Jimmy Lin. 2010.
Ranking Under Temporal Constraints. In Proceedings
of the ACM International Conference on Information
and Knowledge Management.
Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell.
2011. Computational rationalization: The inverse equi-
librium problem. In Proceedings of International Con-
ference of Machine Learning.
F. Wood and Y. W. Teh. 2009. A hierarchical nonpara-
metric Bayesian approach to statistical language model
domain adaptation. In Proceedings of Artificial Intelli-
gence and Statistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008.
Machine learning with annotator rationales to reduce
annotation cost. In Proceedings of the NIPS*2008
Workshop on Cost Sensitive Learning.
Valentina Bayer Zubek and Thomas G. Dietterich. 2002.
Pruning improves heuristic search for cost-sensitive
learning. In International Conference on Machine
Learning.
</reference>
<page confidence="0.992138">
1301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914781">
<title confidence="0.9985665">Besting the Quiz Master: Crowdsourcing Incremental Classification Games</title>
<author confidence="0.993733">Jordan Boyd-Graber Brianna Satinoff</author>
<author confidence="0.993733">He He</author>
<author confidence="0.993733">Hal Daum´e</author>
<affiliation confidence="0.991946">iSchool and UMIACS Department of Computer Science University of Maryland University of Maryland</affiliation>
<email confidence="0.967187">hhe,</email>
<abstract confidence="0.998531761904762">classification, where the used in machine learning tasks have a cost, has been explored as a means of balancing knowledge against the expense of incrementally obtaining new features. We introduce a setting where humans engage in classification with incrementally revealed features: the collegiate trivia circuit. By providing the community with a web-based system to practice, we collected tens of thousands of implicit word-by-word ratings of how useful features are for eliciting correct answers. Observing humans’ classification process, we improve the performance of a state-of-the art classifier. We also use the dataset to evaluate a system to compete in the incremental classification task through a reduction of reinforcement learning to classification. system learns answer a question, performing better than baselines and most human players.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter Abbeel</author>
<author>Andrew Y Ng</author>
</authors>
<title>Apprenticeship learning via inverse reinforcement learning.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="16788" citStr="Abbeel and Ng, 2004" startWordPosition="2715" endWordPosition="2718">ainst our new strategies. We believe that our attempts represent a reasonable explanation of the problem space, but additional improvements could improve performance, as discussed in Section 8. A common way to represent state-dependent strategies is via a Markov decision process (MDP). The most salient component of a MDP is the policy, i.e., a mapping from the state space to an action. In our context, a state is a sequence of (thus far revealed) tokens, and the action is whether to buzz or not. To learn a policy, we use a standard reinforcement learning technique (Langford and Zadrozny, 2005; Abbeel and Ng, 2004; Syed et al., 2008): given a representation of the state space, learn a classifier that can map from a state to an action. This is also a common paradigm for other incremental tasks, e.g., shift-reduce parsing (Nivre, 2008). Given examples of the correct answer given a configuration of the state space, we can learn a MDP without explicitly representing the reward function. In this section, we define our method of defining actions and our representation of the state space. 5.1 Action Space We assume that there are only two possible actions: buzz now or wait. An alternative would be a more expr</context>
<context position="19696" citStr="Abbeel and Ng, 2004" startWordPosition="3209" endWordPosition="3212">eliminating an incorrect answer. Moreover, strategies in a game setting (rather than a single question) are more complicated. For example, if a right answer is worth +10 points and the penalty for an incorrect question is −5, then a team leading by 15 points on the last question should never attempt to answer. Investigating such gameplay strategies would require a “roll out” of game states (Tesauro and Galperin, 1996) to explore the efficacy of such strategies. While interesting, we leave these issues to future work. We also investigated learning a policy directly from users’ buzzes directly (Abbeel and Ng, 2004), but this performed poorly because the content model is incompatible with the players’ abilities and the high variation in players’ ability and styles (compare Figure 2). 5.2 State Space Recall that our goal is to learn a classifier that maps states to actions; above, we defined the action space (the classifier’s output) but not the state space, the classifier’s input. The straightforward parameterization of the state space would be all of the words that have been revealed. However, such a feature set is very sparse. We use three components to form the state space: what information has been o</context>
</contexts>
<marker>Abbeel, Ng, 2004</marker>
<rawString>Pieter Abbeel and Andrew Y. Ng. 2004. Apprenticeship learning via inverse reinforcement learning. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18663" citStr="Blatz et al., 2004" startWordPosition="3035" endWordPosition="3038">f classification, we need to provide (x, y) pairs to learn a mapping x H y. The classifier attempts to learn that action y is (“buzz”) in all states where the content model gave a correct response given state x. Negative examples (“wait”) are applied to states where the content model gave a wrong answer. Every token in our training set corresponds to a classification example; both states are prevalent enough that we do not to explicitly need to address class imbalance. This resembles approaches that merge different classifiers (Riedel et al., 2011) or attempt to estimate confidence of models (Blatz et al., 2004). However, here we use partial observations. This is a simplification of the problem and corresponds to a strategy of “buzz as soon as you know the answer”, ignoring all other factors. While reasonable, this is not always optimal. For example, if you know your opponent is unlikely to answer a question, it is better to wait until you are more confident. Incorrect answers might also help your opponent, e.g., by eliminating an incorrect answer. Moreover, strategies in a game setting (rather than a single question) are more complicated. For example, if a right answer is worth +10 points and the pe</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Boddy</author>
<author>Thomas L Dean</author>
</authors>
<title>Solving timedependent planning problems.</title>
<date>1989</date>
<booktitle>In International Joint Conference on Artificial Intelligence,</booktitle>
<pages>979--984</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<contexts>
<context position="3878" citStr="Boddy and Dean, 1989" startWordPosition="578" endWordPosition="581">ation (Section 5), and develop new hierarchical models combining local and thematic content to better capture the underlying content (Section 7). Finally, we conclude in Section 8 and discuss extensions to other problem areas. 2 Incremental Classification In this section, we discuss previous approaches that explore how much effort or resources a classifier needs to come to a decision, a problem not as thoroughly examined as the question of whether the decision is right or not.2 Incremental classification is 2When have an externally interrupted feature stream, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classi</context>
</contexts>
<marker>Boddy, Dean, 1989</marker>
<rawString>Mark Boddy and Thomas L. Dean. 1989. Solving timedependent planning problems. In International Joint Conference on Artificial Intelligence, pages 979–984. Morgan Kaufmann Publishers, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Ohad Shamir</author>
</authors>
<title>Efficient learning with partially observed attributes.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2857</pages>
<contexts>
<context position="4344" citStr="Cesa-Bianchi et al., 2011" startWordPosition="643" endWordPosition="646">sion is right or not.2 Incremental classification is 2When have an externally interrupted feature stream, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the en</context>
</contexts>
<marker>Cesa-Bianchi, Shalev-Shwartz, Shamir, 2011</marker>
<rawString>Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. 2011. Efficient learning with partially observed attributes. Journal of Machine Learning Research, 12:2857–2878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Chai</author>
<author>Lin Deng</author>
<author>Qiang Yang</author>
<author>Charles X Ling</author>
</authors>
<title>Test-cost sensitive naive bayes classification.</title>
<date>2004</date>
<booktitle>In IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="4815" citStr="Chai et al., 2004" startWordPosition="712" endWordPosition="715">Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams c</context>
<context position="23457" citStr="Chai et al., 2004" startWordPosition="3843" endWordPosition="3846"> Dir(A). Each question n has a label zn and its words are generated from B,;n. Given labeled observations, we use the maximum a posteriori (MAP) estimate of Bl. Why use a generative model when a discriminative classifier could use a richer feature space? The most important reason is that, by definition, it makes sense to ask a generative model the probability of a label given a partial observation; such a question is not well-formed for discriminative models, which expect a complete feature set. Another important consideration is that generative models can predict future, unrevealed features (Chai et al., 2004); however, we do not make use of that capability here. In addition to providing our answers, the content model also provides an additional, critically important feature for our state space: its posterior (pos for short) probability. With every revealed feature, the content model updates its posterior distribution over labels given that t tokens have been revealed in question n, p(zn |wi ... wt, 0, 0). (2) To train our naive Bayes model, we semiautomatically associate labels with a Wikipedia page (correcting mistakes manually) and then form the MAP estimate of the class multinomial distribution</context>
</contexts>
<marker>Chai, Deng, Yang, Ling, 2004</marker>
<rawString>Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X. Ling. 2004. Test-cost sensitive naive bayes classification. In IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Andre Martins</author>
</authors>
<title>A survey on automatic text summarization.</title>
<date>2007</date>
<booktitle>Engineering and Technology,</booktitle>
<pages>4--192</pages>
<contexts>
<context position="38941" citStr="Das and Martins, 2007" startWordPosition="6417" endWordPosition="6420">ta, and also has the boon of being cheaply and easily annotated. We took advantage of that ease and created a framework for quickly and efficiently gathering examples of humans doing incremental classification. There are other potential uses for the dataset; the progression of clues from obscure nuggets to could help determine how “known” a particular aspect of an entity is (e.g., that William Jennings Bryant gave the “Cross of Gold” speech is better known his resignation after the Lusitania sinking, Figure 1). Which could be used in educational settings (Smith et al., 2008) or summarization (Das and Martins, 2007). The second contribution shows that humans’ incremental classification improves state-of-the-art rapacious classification algorithms. While other frameworks (Zaidan et al., 2008) have been proposed to incorporate user clues about features, the system described here provides analogous features without the need for explicit post-hoc reflection, has faster annotation throughput, and is much cheaper. The problem of answering quiz bowl questions is itself a challenging task that combines issues from language modeling, large data, coreference, and reinforcement learning. While we do not address all</context>
</contexts>
<marker>Das, Martins, 2007</marker>
<rawString>Dipanjan Das and Andre Martins. 2007. A survey on automatic text summarization. Engineering and Technology, 4:192–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Notes on CG and LMBFGS optimization of logistic regression. Paper available at http://pub.hal3.name/ ˜daume04cg-bfgs, implementation available at http://hal3.name/megam/.</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>Hal Daum´e III. 2004. Notes on CG and LMBFGS optimization of logistic regression. Paper available at http://pub.hal3.name/ ˜daume04cg-bfgs, implementation available at http://hal3.name/megam/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="25369" citStr="Fan et al., 2008" startWordPosition="4156" endWordPosition="4159">ed a x2 test to identify bigrams with that rejected the null hypothesis at the 0.01 level. 8The Dirichlet scaling parameter A was set to 10,000 given our relatively large vocabulary (25,000) and to not penalize a label’s posterior probability if there were unseen features; this corresponds to a pseudocount of 0.4. α was set to 1.0. 1295 6 Pitting the Algorithm Against Humans With a state space and a policy, we now have all the necessary ingredients to have our algorithm compete against humans. Classification, which allows us to learn a policy, was done using the default settings of LIBLINEAR (Fan et al., 2008). To determine where the algorithm buzzes, we provide a sequence of state spaces until the policy classifier determines that it is time to buzz. We simulate competition by taking the human answers and buzzes as a given and ask our algorithm (independently) to provide its decision on when to buzz on a test set. We compare the two buzz positions. If the algorithm buzzed earlier with the right answer, we consider it to have “won” the question; equivalently, if the algorithm buzzed later, we consider it to have “lost” that question. Ties are rare (less than 1% of cases), as the algorithm had signi</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="5577" citStr="Ferrucci et al., 2010" startWordPosition="832" endWordPosition="835">hooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (Jennings, 2006). Note the distinction between quiz bowl and Jeopardy, a recent application area (Ferrucci et al., 2010). While Jeopardy also uses signaling devices, these are only usable after a question is completed (interrupting Jeopardy’s questions would make for bad television). Thus, Jeopardy is rapacious classification followed by a race to see— among those who know the answer—who can punch a button first. Moreover, buzzes before the question’s end are penalized. Two teams listen to the same question.3 In this context, a question is a series of clues (features) referring to the same entity (for an example question, see Figure 1). We assume a fixed feature ordering for a test sequence (i.e., you cannot re</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building Watson: An Overview of the DeepQA Project. AI Magazine, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35129" citStr="Haghighi and Klein, 2007" startWordPosition="5796" endWordPosition="5799">ent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Co</context>
<context position="39931" citStr="Haghighi and Klein, 2007" startWordPosition="6569" endWordPosition="6572">on throughput, and is much cheaper. The problem of answering quiz bowl questions is itself a challenging task that combines issues from language modeling, large data, coreference, and reinforcement learning. While we do not address all of these problems, our third contribution is a system that learns a policy in a MDP for incremental classification even in very large state spaces; it can successfully compete with skilled human players. Incorporating richer content models is one of our next steps. This would allow us to move beyond the closed-set model and use a more general coreference model (Haghighi and Klein, 2007) for identifying answers and broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing ef</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Hardisty</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Modeling perspective using adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="35055" citStr="Hardisty et al., 2010" startWordPosition="5784" endWordPosition="5787">re aggressive strategy, perhaps incorporating the identity of the opponent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the clo</context>
</contexts>
<marker>Hardisty, Boyd-Graber, Resnik, 2010</marker>
<rawString>Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik. 2010. Modeling perspective using adaptor grammars. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Horsch</author>
<author>David Poole</author>
</authors>
<title>An anytime algorithm for decision making under uncertainty.</title>
<date>1998</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="3903" citStr="Horsch and Poole, 1998" startWordPosition="582" endWordPosition="585"> develop new hierarchical models combining local and thematic content to better capture the underlying content (Section 7). Finally, we conclude in Section 8 and discuss extensions to other problem areas. 2 Incremental Classification In this section, we discuss previous approaches that explore how much effort or resources a classifier needs to come to a decision, a problem not as thoroughly examined as the question of whether the decision is right or not.2 Incremental classification is 2When have an externally interrupted feature stream, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learn</context>
</contexts>
<marker>Horsch, Poole, 1998</marker>
<rawString>Michael C. Horsch and David Poole. 1998. An anytime algorithm for decision making under uncertainty. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Jennings</author>
</authors>
<title>Brainiac: adventures in the curious, competitive, compulsive world of trivia buffs.</title>
<date>2006</date>
<publisher>Villard.</publisher>
<contexts>
<context position="5473" citStr="Jennings, 2006" startWordPosition="817" endWordPosition="818">), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (Jennings, 2006). Note the distinction between quiz bowl and Jeopardy, a recent application area (Ferrucci et al., 2010). While Jeopardy also uses signaling devices, these are only usable after a question is completed (interrupting Jeopardy’s questions would make for bad television). Thus, Jeopardy is rapacious classification followed by a race to see— among those who know the answer—who can punch a button first. Moreover, buzzes before the question’s end are penalized. Two teams listen to the same question.3 In this context, a question is a series of clues (features) referring to the same entity (for an exam</context>
</contexts>
<marker>Jennings, 2006</marker>
<rawString>Ken Jennings. 2006. Brainiac: adventures in the curious, competitive, compulsive world of trivia buffs. Villard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shihao Ji</author>
<author>Lawrence Carin</author>
</authors>
<title>Cost-sensitive feature acquisition and classification. Pattern Recognition,</title>
<date>2007</date>
<pages>40--1474</pages>
<contexts>
<context position="4835" citStr="Ji and Carin, 2007" startWordPosition="716" endWordPosition="719">putational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of </context>
</contexts>
<marker>Ji, Carin, 2007</marker>
<rawString>Shihao Ji and Lawrence Carin. 2007. Cost-sensitive feature acquisition and classification. Pattern Recognition, 40:1474–1485, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyong K Lam</author>
<author>David M Pennock</author>
<author>Dan Cosley</author>
<author>Steve Lawrence</author>
</authors>
<title>1 billion pages = 1 million dollars? mining the web to play ”who wants to be a millionaire?”.</title>
<date>2003</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="8074" citStr="Lam et al., 2003" startWordPosition="1269" endWordPosition="1272">nd. Words (excluding stop words) are shaded based on the number of times the word triggered a buzz from any player who answered the question (darker means more buzzes; buzzes contribute to the shading of the previous five words). Diamonds (0) indicate buzz positions. The answers to quiz bowl questions are wellknown entities (e.g., scientific laws, people, battles, books, characters, etc.), so the answer space is relatively limited; there are no open-ended questions of the form “why is the sky blue?” However, there are no multiple choice questions—as there are in Who Wants to Be a Millionaire (Lam et al., 2003)— or structural constraints—as there are in crossword puzzles (Littman et al., 2002). Now that we introduced the concepts of questions, answers, and buzzes, we pause briefly to define them more formally and explicitly connect to machine learning. In the sequel, we will refer to: questions, sequences of words (tokens) associated with a single answer; features, inputs used for decisions (derived from the tokens in a question); labels, a question’s correct response; answers, the responses (either correct or incorrect) provided; and buzzes, positions in a question where users halted the stream of </context>
</contexts>
<marker>Lam, Pennock, Cosley, Lawrence, 2003</marker>
<rawString>Shyong K. Lam, David M. Pennock, Dan Cosley, and Steve Lawrence. 2003. 1 billion pages = 1 million dollars? mining the web to play ”who wants to be a millionaire?”. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langford</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Relating reinforcement learning performance to classification performance.</title>
<date>2005</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="16767" citStr="Langford and Zadrozny, 2005" startWordPosition="2711" endWordPosition="2714">ious baselines and compare against our new strategies. We believe that our attempts represent a reasonable explanation of the problem space, but additional improvements could improve performance, as discussed in Section 8. A common way to represent state-dependent strategies is via a Markov decision process (MDP). The most salient component of a MDP is the policy, i.e., a mapping from the state space to an action. In our context, a state is a sequence of (thus far revealed) tokens, and the action is whether to buzz or not. To learn a policy, we use a standard reinforcement learning technique (Langford and Zadrozny, 2005; Abbeel and Ng, 2004; Syed et al., 2008): given a representation of the state space, learn a classifier that can map from a state to an action. This is also a common paradigm for other incremental tasks, e.g., shift-reduce parsing (Nivre, 2008). Given examples of the correct answer given a configuration of the state space, we can learn a MDP without explicitly representing the reward function. In this section, we define our method of defining actions and our representation of the state space. 5.1 Action Space We assume that there are only two possible actions: buzz now or wait. An alternative</context>
</contexts>
<marker>Langford, Zadrozny, 2005</marker>
<rawString>John Langford and Bianca Zadrozny. 2005. Relating reinforcement learning performance to classification performance. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger P Levy</author>
<author>Florencia Reali</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Modeling the effects of memory on human online sentence processing with particle filters.</title>
<date>2008</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="40555" citStr="Levy et al., 2008" startWordPosition="6667" endWordPosition="6670">entifying answers and broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process data and make decisions in a cheap, easy crowdsourced application can help us apply new algorithms to improve performance in settings where features aren’t free, either because of computational or annotation cost. 1299 Ackno</context>
</contexts>
<marker>Levy, Reali, Griffiths, 2008</marker>
<rawString>Roger P. Levy, Florencia Reali, and Thomas L. Griffiths. 2008. Modeling the effects of memory on human online sentence processing with particle filters. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Integrating surprisal and uncertaininput models in online sentence comprehension: formal techniques and empirical results.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="40568" citStr="Levy, 2011" startWordPosition="6671" endWordPosition="6672">nd broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process data and make decisions in a cheap, easy crowdsourced application can help us apply new algorithms to improve performance in settings where features aren’t free, either because of computational or annotation cost. 1299 Acknowledgments We</context>
</contexts>
<marker>Levy, 2011</marker>
<rawString>Roger Levy. 2011. Integrating surprisal and uncertaininput models in online sentence comprehension: formal techniques and empirical results. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of European Conference of Machine Learning,</booktitle>
<pages>1398</pages>
<contexts>
<context position="22453" citStr="Lewis, 1998" startWordPosition="3678" endWordPosition="3679">yer would answer (Silver et al., 2008), a regression predicting how many individuals would buzz in the next n words, the year the question was written, the category of the question, etc. 5.3 Naive Content Model The action space is only deciding when to answer, having abdicated responsibility for what to answer. So where does do the answers come from? We assume that at any point we can ask “what is the highest probability label given my current feature observations?” We call the component of our model that answers this question the content model. Our first content model is a naive Bayes model (Lewis, 1998) trained over a text collection. This generative model assumes labels for questions come from a multinomial distribution 0 — Dir(α) 6The phrase “for ten points” (abbreviated FTP) appears in all quiz bowl questions to signal the question’s last sentence or clause. It is a signal to answer soon, as the final “giveaway” clue is next. and assumes that label l has a word distribution Bl — Dir(A). Each question n has a label zn and its words are generated from B,;n. Given labeled observations, we use the maximum a posteriori (MAP) estimate of Bl. Why use a generative model when a discriminative clas</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of European Conference of Machine Learning, number 1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Littman</author>
<author>Greg A Keim</author>
<author>Noam Shazeer</author>
</authors>
<title>A probabilistic approach to solving crossword puzzles.</title>
<date>2002</date>
<journal>Artif. Intell.,</journal>
<pages>134--1</pages>
<contexts>
<context position="8158" citStr="Littman et al., 2002" startWordPosition="1281" endWordPosition="1284">d triggered a buzz from any player who answered the question (darker means more buzzes; buzzes contribute to the shading of the previous five words). Diamonds (0) indicate buzz positions. The answers to quiz bowl questions are wellknown entities (e.g., scientific laws, people, battles, books, characters, etc.), so the answer space is relatively limited; there are no open-ended questions of the form “why is the sky blue?” However, there are no multiple choice questions—as there are in Who Wants to Be a Millionaire (Lam et al., 2003)— or structural constraints—as there are in crossword puzzles (Littman et al., 2002). Now that we introduced the concepts of questions, answers, and buzzes, we pause briefly to define them more formally and explicitly connect to machine learning. In the sequel, we will refer to: questions, sequences of words (tokens) associated with a single answer; features, inputs used for decisions (derived from the tokens in a question); labels, a question’s correct response; answers, the responses (either correct or incorrect) provided; and buzzes, positions in a question where users halted the stream of features and gave an answer. Quiz bowl is not a typical problem domain for natural l</context>
</contexts>
<marker>Littman, Keim, Shazeer, 2002</marker>
<rawString>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artif. Intell., 134(1-2):23–55, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit. In Tools and methodologies for teaching.</title>
<date>2002</date>
<contexts>
<context position="24721" citStr="Loper and Bird, 2002" startWordPosition="4045" endWordPosition="4048">s for the 1065 labels that had at least three human answers, excluding ambiguous labels associated with multiple concepts (e.g., “europa”, “steppenwolf”, “georgia”, “paris”, and “v”). Features were taken to be the 25,000 most frequent tokens and bigrams7 that were not stop words; features were extracted from the Wikipedia text in the same manner as from the question tokens.8 After demonstrating our ability to learn an incremental classifier using this simple content model, we extend the content model to capture local context and correlations between similar labels in Section 7. 7We used NLTK (Loper and Bird, 2002) to filter stop words and we used a x2 test to identify bigrams with that rejected the null hypothesis at the 0.01 level. 8The Dirichlet scaling parameter A was set to 10,000 given our relatively large vocabulary (25,000) and to not penalize a label’s posterior probability if there were unseen features; this corresponds to a pseudocount of 0.4. α was set to 1.0. 1295 6 Pitting the Algorithm Against Humans With a state space and a policy, we now have all the necessary ingredients to have our algorithm compete against humans. Classification, which allows us to learn a policy, was done using the </context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: the natural language toolkit. In Tools and methodologies for teaching.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Maytal Saar-Tsechansky</author>
<author>Foster Provost</author>
<author>Raymond J Mooney</author>
</authors>
<title>An expected utility approach to active feature-value acquisition.</title>
<date>2005</date>
<booktitle>In International Conference on Data Mining,</booktitle>
<contexts>
<context position="4859" citStr="Melville et al., 2005" startWordPosition="720" endWordPosition="723">cs not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (J</context>
</contexts>
<marker>Melville, Saar-Tsechansky, Provost, Mooney, 2005</marker>
<rawString>Prem Melville, Maytal Saar-Tsechansky, Foster Provost, and Raymond J. Mooney. 2005. An expected utility approach to active feature-value acquisition. In International Conference on Data Mining, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Roxana Girju</author>
<author>Richard Goodrum</author>
<author>Vasile Rus</author>
</authors>
<title>The structure and performance of an opendomain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35194" citStr="Moldovan et al., 2000" startWordPosition="5806" endWordPosition="5809">tent Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Country, whose plot matches some of these keywords. To answer these</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum, and Vasile Rus. 2000. The structure and performance of an opendomain question answering system. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="17012" citStr="Nivre, 2008" startWordPosition="2756" endWordPosition="2757">dent strategies is via a Markov decision process (MDP). The most salient component of a MDP is the policy, i.e., a mapping from the state space to an action. In our context, a state is a sequence of (thus far revealed) tokens, and the action is whether to buzz or not. To learn a policy, we use a standard reinforcement learning technique (Langford and Zadrozny, 2005; Abbeel and Ng, 2004; Syed et al., 2008): given a representation of the state space, learn a classifier that can map from a state to an action. This is also a common paradigm for other incremental tasks, e.g., shift-reduce parsing (Nivre, 2008). Given examples of the correct answer given a configuration of the state space, we can learn a MDP without explicitly representing the reward function. In this section, we define our method of defining actions and our representation of the state space. 5.1 Action Space We assume that there are only two possible actions: buzz now or wait. An alternative would be a more expressive action space (e.g., an action for every possible answer). However, this conflates the question of when to buzz with what to answer. Instead, we call the distinct component that provides what to answer the content mode</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Comput. Linguist., 34(4):513–553, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Pujara</author>
<author>Hal Daume</author>
<author>Lise Getoor</author>
</authors>
<title>Using classifier cascades for scalable e-mail classification.</title>
<date>2011</date>
<booktitle>In Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference, ACM International Conference Proceedings Series.</booktitle>
<contexts>
<context position="2257" citStr="Pujara et al., 2011" startWordPosition="338" endWordPosition="341">de a decision and no longer needs additional features. Even indefatigable computers cannot always exhaustively consider every feature. This is because the result 1Earlier drafts called this “batch” machine learning, which confused the distinction between batch and online learning. We gladly adopt “rapacious” to make this distinction clearer and to cast traditional machine learning—that always examines all features—as a resource hungry approach. is time sensitive, such as in interactive systems, or because processing time is limited by the sheer quantity of data, as in sifting e-mail for spam (Pujara et al., 2011). In such settings, often the best solution is incremental: allow a decision to be made without seeing all of an instance’s features. We discuss the incremental classification framework in Section 2. Our understanding of how humans conduct incremental classification is limited. This is because complicating an already difficult annotation task is often an unwise tradeoff. Instead, we adapt a real world setting where humans are already engaging (eagerly) in incremental classification—trivia games—and develop a cheap, easy method for capturing human incremental classification judgments. After qua</context>
</contexts>
<marker>Pujara, Daume, Getoor, 2011</marker>
<rawString>Jay Pujara, Hal Daume III, and Lise Getoor. 2011. Using classifier cascades for scalable e-mail classification. In Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference, ACM International Conference Proceedings Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Andrew McCallum</author>
<author>Christopher D Manning</author>
</authors>
<title>Model combination for event extraction in bionlp 2011.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Workshop.</booktitle>
<contexts>
<context position="18598" citStr="Riedel et al., 2011" startWordPosition="3024" endWordPosition="3027">re examples of state space and action come from. In the language of classification, we need to provide (x, y) pairs to learn a mapping x H y. The classifier attempts to learn that action y is (“buzz”) in all states where the content model gave a correct response given state x. Negative examples (“wait”) are applied to states where the content model gave a wrong answer. Every token in our training set corresponds to a classification example; both states are prevalent enough that we do not to explicitly need to address class imbalance. This resembles approaches that merge different classifiers (Riedel et al., 2011) or attempt to estimate confidence of models (Blatz et al., 2004). However, here we use partial observations. This is a simplification of the problem and corresponds to a strategy of “buzz as soon as you know the answer”, ignoring all other factors. While reasonable, this is not always optimal. For example, if you know your opponent is unlikely to answer a question, it is better to wait until you are more confident. Incorrect answers might also help your opponent, e.g., by eliminating an incorrect answer. Moreover, strategies in a game setting (rather than a single question) are more complicat</context>
</contexts>
<marker>Riedel, McClosky, Surdeanu, McCallum, Manning, 2011</marker>
<rawString>Sebastian Riedel, David McClosky, Mihai Surdeanu, Andrew McCallum, and Christopher D. Manning. 2011. Model combination for event extraction in bionlp 2011. In Proceedings of the BioNLP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afshin Rostamizadeh</author>
<author>Alekh Agarwal</author>
<author>Peter L Bartlett</author>
</authors>
<title>Learning with missing features.</title>
<date>2011</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="4445" citStr="Rostamizadeh et al., 2011" startWordPosition="658" endWordPosition="661">eam, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2</context>
</contexts>
<marker>Rostamizadeh, Agarwal, Bartlett, 2011</marker>
<rawString>Afshin Rostamizadeh, Alekh Agarwal, and Peter L. Bartlett. 2011. Learning with missing features. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maytal Saar-Tsechansky</author>
<author>Foster Provost</author>
</authors>
<title>Handling missing values when applying classification models.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>8</volume>
<pages>1657</pages>
<contexts>
<context position="4391" citStr="Saar-Tsechansky and Provost, 2007" startWordPosition="649" endWordPosition="652">sification is 2When have an externally interrupted feature stream, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a le</context>
</contexts>
<marker>Saar-Tsechansky, Provost, 2007</marker>
<rawString>Maytal Saar-Tsechansky and Foster Provost. 2007. Handling missing values when applying classification models. Journal of Machine Learning Research, 8:1623– 1657, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salton</author>
</authors>
<title>Automatic Information Organization and Retrieval.</title>
<date>1968</date>
<publisher>McGraw Hill Text.</publisher>
<contexts>
<context position="13948" citStr="Salton, 1968" startWordPosition="2226" endWordPosition="2227">echnique is gathering useful information. We used a state-of-the-art maximum entropy classification model, MEGAM (Daum´e III, 2004), that accepts a per-class mean prior for feature weights and applied MEGAM to the 200 most frequent labels (11,663 questions, a third of the dataset). The prior mean of the feature weight is a convenient, simple way to incorporate human feature utility; apart from the mean, all default options are used. Specifying the prior requires us to specify a weight for each pair of label and feature. The weight combines buzz information (described in Section 3) and tf-idf (Salton, 1968). The tf-idf value is computed by treating the training set of questions with the same label as a single document. Buzzes and tf-idf information were combined into the prior µ for label a and feature f as µa,f = [ ] Qb(a, f) + αI [b(a, f) &gt; 0] + -y tf-idf(a, f). (1) We describe our weight strategies in increasing order of human knowledge. If α, Q, and -y are zero, this is a naive zero prior. If -y only is nonzero, this is a linear transformation of features’ tf-idf. If only α is nonzero, this is a linear transformation of buzzed Weighting α Q 7 Error zero - - - 0.37 tf-idf - - 3.5 0.14 buzz-bi</context>
</contexts>
<marker>Salton, 1968</marker>
<rawString>Gerard. Salton. 1968. Automatic Information Organization and Retrieval. McGraw Hill Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances.</title>
<date>2011</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5049" citStr="Settles, 2011" startWordPosition="753" endWordPosition="754">al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (Jennings, 2006). Note the distinction between quiz bowl and Jeopardy, a recent application area (Ferrucci et al., 2010). While Jeopardy also uses signaling devices, these are only usable afte</context>
</contexts>
<marker>Settles, 2011</marker>
<rawString>Burr Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Silver</author>
<author>Richard S Sutton</author>
<author>Martin M¨uller</author>
</authors>
<title>Sample-based learning and search with permanent and transient memories.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<marker>Silver, Sutton, M¨uller, 2008</marker>
<rawString>David Silver, Richard S. Sutton, and Martin M¨uller. 2008. Sample-based learning and search with permanent and transient memories. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Michael Heilman</author>
<author>Rebecca Hwa</author>
</authors>
<title>Question generation as a competitive undergraduate course project.</title>
<date>2008</date>
<booktitle>In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge.</booktitle>
<contexts>
<context position="38900" citStr="Smith et al., 2008" startWordPosition="6411" endWordPosition="6414">y of standardized, machine-readable data, and also has the boon of being cheaply and easily annotated. We took advantage of that ease and created a framework for quickly and efficiently gathering examples of humans doing incremental classification. There are other potential uses for the dataset; the progression of clues from obscure nuggets to could help determine how “known” a particular aspect of an entity is (e.g., that William Jennings Bryant gave the “Cross of Gold” speech is better known his resignation after the Lusitania sinking, Figure 1). Which could be used in educational settings (Smith et al., 2008) or summarization (Das and Martins, 2007). The second contribution shows that humans’ incremental classification improves state-of-the-art rapacious classification algorithms. While other frameworks (Zaidan et al., 2008) have been proposed to incorporate user clues about features, the system described here provides analogous features without the need for explicit post-hoc reflection, has faster annotation throughput, and is much cheaper. The problem of answering quiz bowl questions is itself a challenging task that combines issues from language modeling, large data, coreference, and reinforcem</context>
</contexts>
<marker>Smith, Heilman, Hwa, 2008</marker>
<rawString>Noah A. Smith, Michael Heilman, and Rebecca Hwa. 2008. Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umar Syed</author>
<author>Michael Bowling</author>
<author>Robert E Schapire</author>
</authors>
<title>Apprenticeship learning using linear programming.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="16808" citStr="Syed et al., 2008" startWordPosition="2719" endWordPosition="2722">ies. We believe that our attempts represent a reasonable explanation of the problem space, but additional improvements could improve performance, as discussed in Section 8. A common way to represent state-dependent strategies is via a Markov decision process (MDP). The most salient component of a MDP is the policy, i.e., a mapping from the state space to an action. In our context, a state is a sequence of (thus far revealed) tokens, and the action is whether to buzz or not. To learn a policy, we use a standard reinforcement learning technique (Langford and Zadrozny, 2005; Abbeel and Ng, 2004; Syed et al., 2008): given a representation of the state space, learn a classifier that can map from a state to an action. This is also a common paradigm for other incremental tasks, e.g., shift-reduce parsing (Nivre, 2008). Given examples of the correct answer given a configuration of the state space, we can learn a MDP without explicitly representing the reward function. In this section, we define our method of defining actions and our representation of the state space. 5.1 Action Space We assume that there are only two possible actions: buzz now or wait. An alternative would be a more expressive action space </context>
</contexts>
<marker>Syed, Bowling, Schapire, 2008</marker>
<rawString>Umar Syed, Michael Bowling, and Robert E. Schapire. 2008. Apprenticeship learning using linear programming. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Tesauro</author>
<author>Gregory R Galperin</author>
</authors>
<title>On-line policy improvement using monte-carlo search.</title>
<date>1996</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="19497" citStr="Tesauro and Galperin, 1996" startWordPosition="3177" endWordPosition="3180"> is not always optimal. For example, if you know your opponent is unlikely to answer a question, it is better to wait until you are more confident. Incorrect answers might also help your opponent, e.g., by eliminating an incorrect answer. Moreover, strategies in a game setting (rather than a single question) are more complicated. For example, if a right answer is worth +10 points and the penalty for an incorrect question is −5, then a team leading by 15 points on the last question should never attempt to answer. Investigating such gameplay strategies would require a “roll out” of game states (Tesauro and Galperin, 1996) to explore the efficacy of such strategies. While interesting, we leave these issues to future work. We also investigated learning a policy directly from users’ buzzes directly (Abbeel and Ng, 2004), but this performed poorly because the content model is incompatible with the players’ abilities and the high variation in players’ ability and styles (compare Figure 2). 5.2 State Space Recall that our goal is to learn a classifier that maps states to actions; above, we defined the action space (the classifier’s output) but not the state space, the classifier’s input. The straightforward paramete</context>
</contexts>
<marker>Tesauro, Galperin, 1996</marker>
<rawString>Gerald Tesauro and Gregory R. Galperin. 1996. On-line policy improvement using monte-carlo search. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Thibadeau</author>
<author>Marcel A Just</author>
<author>Patricia A Carpenter</author>
</authors>
<title>A model of the time course and content of reading.</title>
<date>1982</date>
<journal>Cognitive Science,</journal>
<volume>6</volume>
<contexts>
<context position="40474" citStr="Thibadeau et al., 1982" startWordPosition="6652" endWordPosition="6655">ed-set model and use a more general coreference model (Haghighi and Klein, 2007) for identifying answers and broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process data and make decisions in a cheap, easy crowdsourced application can help us apply new algorithms to improve performance in settings where fea</context>
</contexts>
<marker>Thibadeau, Just, Carpenter, 1982</marker>
<rawString>Robert Thibadeau, Marcel A. Just, and Patricia A. Carpenter. 1982. A model of the time course and content of reading. Cognitive Science, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Structured Topic Models for Language.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="32131" citStr="Wallach, 2008" startWordPosition="5291" endWordPosition="5292">d observed words (we use “words” loosely, as bigrams replace some word pairs). By constructing the word distributions using hierarchical distributions based on domain and ngrams (a much simpler parametric version of more elaborate methods (Wood and Teh, 2009)), we can share statistical strength across related contexts. We assume that labels are (only) associated with their majority category as seen in our training data and that category assignments are observed. All scaling parameters A were set to 10,000, α was 1.0, and the vocabulary was still 25,000. We used the maximal seating assignment (Wallach, 2008) for propagating counts through the Dirichlet hierarchy. Thus, if the word v appeared Bl,u,v times in label l following a preceding word u, 5l,v times in label l, Tc,v times in category c, and Gv times in total, we estimate the probability of a word v appearing in label k, category t, and after word u as p(wn = v |lab = l, cat = c, wn_1 = u; X) = Gv+λ0/V Tc,v+λ1 G·+λ0 Bl,u,v + A3 Bl,u,· + A3 where we use · to represent marginalization, e.g. Tc,· = Ev, Tc,v,. As with naive Bayes, Bayes’ rule provides posterior label probabilities (Equation 2). We compare the naive model with models that capture</context>
</contexts>
<marker>Wallach, 2008</marker>
<rawString>Hanna M Wallach. 2008. Structured Topic Models for Language. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidan Wang</author>
<author>Donald Metzler</author>
<author>Jimmy Lin</author>
</authors>
<title>Ranking Under Temporal Constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="3951" citStr="Wang et al., 2010" startWordPosition="589" endWordPosition="592">thematic content to better capture the underlying content (Section 7). Finally, we conclude in Section 8 and discuss extensions to other problem areas. 2 Incremental Classification In this section, we discuss previous approaches that explore how much effort or resources a classifier needs to come to a decision, a problem not as thoroughly examined as the question of whether the decision is right or not.2 Incremental classification is 2When have an externally interrupted feature stream, the setting is called “any time” (Boddy and Dean, 1989; Horsch and Poole, 1998). Like “budgeted” algorithms (Wang et al., 2010), these are distinct but related problems. 1290 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional featu</context>
</contexts>
<marker>Wang, Metzler, Lin, 2010</marker>
<rawString>Lidan Wang, Donald Metzler, and Jimmy Lin. 2010. Ranking Under Temporal Constraints. In Proceedings of the ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Waugh</author>
<author>Brian D Ziebart</author>
<author>J Andrew Bagnell</author>
</authors>
<title>Computational rationalization: The inverse equilibrium problem.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="40333" citStr="Waugh et al., 2011" startWordPosition="6633" endWordPosition="6636">ete with skilled human players. Incorporating richer content models is one of our next steps. This would allow us to move beyond the closed-set model and use a more general coreference model (Haghighi and Klein, 2007) for identifying answers and broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process da</context>
</contexts>
<marker>Waugh, Ziebart, Bagnell, 2011</marker>
<rawString>Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell. 2011. Computational rationalization: The inverse equilibrium problem. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>Y W Teh</author>
</authors>
<title>A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="31776" citStr="Wood and Teh, 2009" startWordPosition="5232" endWordPosition="5235">a distribution over words θl,c - Dir(λ2θc) A. For each type v, draw a bigram distribution θl,c,v - Dir(λ3θl,c) 3. Draw a distribution over labels φ - Dir(α). 4. For each question with category c and N words, draw answer l - Mult(φ): (a) Assume w0 = START (b) Draw wn - Mult(θl,c,w,_,) for n E {1 ... N} This creates a language model over categories, labels, and observed words (we use “words” loosely, as bigrams replace some word pairs). By constructing the word distributions using hierarchical distributions based on domain and ngrams (a much simpler parametric version of more elaborate methods (Wood and Teh, 2009)), we can share statistical strength across related contexts. We assume that labels are (only) associated with their majority category as seen in our training data and that category assignments are observed. All scaling parameters A were set to 10,000, α was 1.0, and the vocabulary was still 25,000. We used the maximal seating assignment (Wallach, 2008) for propagating counts through the Dirichlet hierarchy. Thus, if the word v appeared Bl,u,v times in label l following a preceding word u, 5l,v times in label l, Tc,v times in category c, and Gv times in total, we estimate the probability of a </context>
<context position="40122" citStr="Wood and Teh, 2009" startWordPosition="6597" endWordPosition="6600"> learning. While we do not address all of these problems, our third contribution is a system that learns a policy in a MDP for incremental classification even in very large state spaces; it can successfully compete with skilled human players. Incorporating richer content models is one of our next steps. This would allow us to move beyond the closed-set model and use a more general coreference model (Haghighi and Klein, 2007) for identifying answers and broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired mo</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>F. Wood and Y. W. Teh. 2009. A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation. In Proceedings of Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Machine learning with annotator rationales to reduce annotation cost.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning.</booktitle>
<contexts>
<context position="39120" citStr="Zaidan et al., 2008" startWordPosition="6440" endWordPosition="6443">g incremental classification. There are other potential uses for the dataset; the progression of clues from obscure nuggets to could help determine how “known” a particular aspect of an entity is (e.g., that William Jennings Bryant gave the “Cross of Gold” speech is better known his resignation after the Lusitania sinking, Figure 1). Which could be used in educational settings (Smith et al., 2008) or summarization (Das and Martins, 2007). The second contribution shows that humans’ incremental classification improves state-of-the-art rapacious classification algorithms. While other frameworks (Zaidan et al., 2008) have been proposed to incorporate user clues about features, the system described here provides analogous features without the need for explicit post-hoc reflection, has faster annotation throughput, and is much cheaper. The problem of answering quiz bowl questions is itself a challenging task that combines issues from language modeling, large data, coreference, and reinforcement learning. While we do not address all of these problems, our third contribution is a system that learns a policy in a MDP for incremental classification even in very large state spaces; it can successfully compete wi</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2008</marker>
<rawString>Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008. Machine learning with annotator rationales to reduce annotation cost. In Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentina Bayer Zubek</author>
<author>Thomas G Dietterich</author>
</authors>
<title>Pruning improves heuristic search for cost-sensitive learning.</title>
<date>2002</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4691" citStr="Zubek and Dietterich, 2002" startWordPosition="693" endWordPosition="696">Natural Language Processing and Computational Natural Language Learning, pages 1290–1301, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics not equivalent to missing features, which have been studied at training time (Cesa-Bianchi et al., 2011), test time (Saar-Tsechansky and Provost, 2007), and in an online setting (Rostamizadeh et al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify docu</context>
</contexts>
<marker>Zubek, Dietterich, 2002</marker>
<rawString>Valentina Bayer Zubek and Thomas G. Dietterich. 2002. Pruning improves heuristic search for cost-sensitive learning. In International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
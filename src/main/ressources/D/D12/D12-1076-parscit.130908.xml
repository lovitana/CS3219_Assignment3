<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001262">
<title confidence="0.992495">
Explore Person Specific Evidence in Web Person Name Disambiguation
</title>
<author confidence="0.998733">
Liwei Chen, Yansong Feng, Lei Zou, Dongyan Zhao
</author>
<affiliation confidence="0.996652">
Institute of Computer Science and Technology
Peking University
</affiliation>
<address confidence="0.449776">
Beijing
</address>
<email confidence="0.996591">
{clwclw88,fengyansong,zoulei,zhaodongyan}@pku.edu.cn
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999419375">
In this paper, we investigate different usages
of feature representations in the web person
name disambiguation task which has been suf-
fering from the mismatch of vocabulary and
lack of clues in web environments. In litera-
ture, the latter receives less attention and re-
mains more challenging. We explore the fea-
ture space in this task and argue that collecting
person specific evidences from a corpus level
can provide a more reasonable and robust es-
timation for evaluating a feature’s importance
in a given web page. This can alleviate the
lack of clues where discriminative features can
be reasonably weighted by taking their corpus
level importance into account, not just relying
on the current local context. We therefore pro-
pose a topic-based model to exploit the person
specific global importance and embed it into
the person name similarity. The experimen-
tal results show that the corpus level topic in-
formation provides more stable evidences for
discriminative features and our method out-
performs the state-of-the-art systems on three
WePS datasets.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976785714286">
Resolving ambiguity associated with person names
found on the Web is a key challenge in many Internet
applications, such as information retrieval, question
answering, open information extraction, automatic
knowledge acquisition(Wu and Weld, 2008) and so
on. For example, if you want to know more about a
guy named George Foster and feed Yahoo! with his
name, the results are not satisfactory where you get
more than 40 different persons named George Fos-
ter scattering in the top 100 returned pages. None
of the dominant search engines currently helps users
group those returned pages into clusters according
to whether they refer to the same person. Users thus
have to either read those pages carefully or adjust
their queries by adding extra modifiers. This moti-
vates an intensive study in automatically resolving
person name ambiguity in various web applications.
However, resolving web person name ambiguity
is not a trivial task. Due to the difficulties in fig-
uring out or predicting the number of namesakes
in the returned pages, the task has been investi-
gated in an unsupervised learning fashion in the lit-
erature, which is apparently different from the tra-
ditional word sense disambiguation or entity link-
ing/disambiguation tasks, where the inventories of
candidate word senses or entities are usually known
given the target word or entity mention.
A general framework for this task can be formu-
lated as first extracting various features from the web
pages, and then grouping these pages into several
clusters each of which is assumed to represent one
specific person. Despite of the inevitably noisy na-
ture of web data, a key challenge is how to handle
the data sparsity problem which we mean as: mis-
match of vocabulary and lack of clues. The for-
mer refers to the case that two web pages may de-
scribe the same person but use different words thus
the word overlap between them are small. Vari-
ous features, including entities, biographical infor-
mation, URL, etc., have been introduced to bridge
the gap(Mann and Yarowsky, 2003; Kalashnikov
et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009),
</bodyText>
<page confidence="0.954969">
832
</page>
<note confidence="0.8066205">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99996153125">
and external knowledge resources are also employed
to capture the semantic relationship between enti-
ties(Han and Zhao, 2009, 2010). However, a more
challenging scenario is that there are few clues avail-
able in the web pages. For example, there is a page
mentioning a nutritionist Emily Bender in WePS2
dataset(Javier et al., 2009). Throughout the whole
page we can find only one word, nutrition, related to
her identification, while other pages about the nu-
tritionist in the dataset contain substantial materi-
als about her profession and job. In this case, cur-
rent efforts, focusing on either feature engineering
or background knowledge, are incapable to exploit-
ing these limited clues from the current page to the
whole Emily Bender document set, where nutrition,
as an important feature for recognizing a nutritionist,
should be paid more attention.
As far as we know, there is less work focusing
on exploring person specific information to relieve
the lack of clues problem. Traditional vector space
model (VSM) is most widely used to accommodate
various features, but it ignores any relations between
them(Mann and Yarowsky, 2003; Ikeda et al., 2009).
Beyond bag-of-features, two kinds of features are
explored, co-occurrences of entities and Wikipedia
based semantic relationship between entities, both
of which provide a reasonable relatedness for en-
tity pairs. More recent works adopt one of these
relationships(Jiang et al., 2009; Kalashnikov et al.,
2008a; Han and Zhao, 2009). Han and Zhao try
to model both aspects, but their co-occurrence es-
timation, estimated from held-out resources, fails to
capture the person specific importance for a feature,
which is crucial to enhance limited clues in a cor-
pus level, e.g., the significance of nutrition for Emily
Bender in WePS1 dataset.
In this paper, we explore different usages of fea-
tures and propose an approach which mines cross
document information to capture the person specific
importance for a feature. Specifically, we construct a
semantic graph from Wikipedia concepts appearing
in all documents that contain the target name (which
we refer to name observation set), then group them
into several topics and further weight each feature by
considering both the relatedness of the feature to its
corresponding topic and the importance of this topic
in the current name observation set. By incorporat-
ing both the Wikipedia and topic information into
our person name similarity, our model exploits both
Wikipedia based background knowledge and per-
son specific importance. We argue that the corpus
level importance provides more stable evidences for
discriminative features in various scenarios, espe-
cially the tough case. We compared our model with
the state of the arts on three WePS datasets (from
the First and Second Web People Search Cluster-
ing Task), and our experiments show that our model
consistently outperforms other competitive models
on all three datasets.
In the rest of this paper, we first review related
work, and in Section 3, show how we exploit the
person specific importance in our disambiguation
model. Experiment results are discussed in Sec-
tion 4. We conclude this paper in Section 5.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999948310344827">
Web person name ambiguity resolution can be for-
mally defined as follows: Given a set of web
pages {d1, d2, ..., d,}, where each page di (i =
1, ..., n) contains an ambiguous name N which may
correspond to several persons holding this name
among these pages. The disambiguation system
should group these name observations into j cluster
{c1, c2, ..., cj} each of which is expected to contain
web pages about the same person.
As mentioned before, the task is usually formu-
lated in a unsupervised fashion, including two steps:
feature extraction and person clustering. Most re-
search efforts so far have been made to the for-
mer, exploring various features according to spe-
cific applications, while the second step is currently
dominated by hierarchical agglomerative cluster-
ing (HAC). According to the reliance of extra
knowledge resources, existing works can be catego-
rized into non-resource methods and resource-based
methods. Non-resource methods extract various lo-
cal features from the context of ambiguous names,
and compute the similarity between feature vectors.
These features include plain words(Bagga and Bald-
win, 1998), biographical information(Mann and
Yarowsky, 2003; Niu et al., 2004), named enti-
ties, compound key phrases, hyperlinks(Ikeda et al.,
2009), etc. The similarity between namesakes are
usually measured by the cosine similarity(Bagga
and Baldwin, 1998), or other graph based met-
</bodyText>
<page confidence="0.997986">
833
</page>
<bodyText confidence="0.999736018867925">
rics(Iria et al., 2007; Kalashnikov et al., 2008a;
Jiang et al., 2009). Those methods pay more at-
tention to extracting informative features and their
co-occurrences, but they usually treat the features lo-
cally, and ignore the semantic relatedness of features
beyond the current document.
Resource-based approaches, on the other hand,
can leverage external resources to benefit from rich
background knowledge, which is crucial to rem-
edy the data sparsity problem. The employed re-
sources include raw texts available on the web and
online encyclopedias. Kalashnikov et al. and Yim-
ing et al. use extra web corpora to obtain co-
occurrences between named entities. Rao et al. use
Google Snippets to provide more contexts. By em-
ploying Wikipedia, the largest online encyclopedia,
rich background knowledge about the semantic re-
latedness between entities can be leveraged to im-
prove the disambiguation performance, and relieve
the coverage problem, to some extent. Bunescu
and Pasca and Cucerzan utilize Wikipedia’s cate-
gory hierarchy to disambiguate entities, while Pilz
uses Wikipedia’s link information. Han and Zhao
adopt Wikipedia semantic relatedness to compute
the similarity between name observations. They also
combine multiple knowledge sources and capture
explicit semantic relatedness between concepts and
implicit semantic relationship embedded in a seman-
tic graph simultaneously(Han and Zhao, 2010).
Most approaches discussed above explore vari-
ous features in the current page or rely on exter-
nal knowledge resources to bridge the vocabulary
gap, but pay less attention to the lack of clues since
they ignore the person specific evidence in the cur-
rent corpus level. Our model focuses on solving the
data sparsity problem by utilizing other web pages
in the same name observation set to provide a robust
but person specific weighting for discriminative fea-
tures beyond the current document alone. In terms
of extra resources, the Wikipedia based model (WS)
by Han and Zhao (2009) is close to our model. The
WS model uses Wikipedia to capture the relation-
ship between entities in the local context to bridge
the vocabulary gap, but it is incapable to evaluate
the importance of a feature with regarding to the tar-
get name, hence is unable to make use of limited
clues in the current web page. Our method captures
person specific evidences by generating topics from
all concepts in the current name observation set and
weighting a feature accordingly. In this case, dis-
criminative features that are sparse in the current
page can be globally weighted so as to provide a
more accurate and stable person name similarity.
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="method">
3 The Model
</sectionHeader>
<bodyText confidence="0.99997275">
Our model consists of three steps: feature extrac-
tion, topic generation and name disambiguation. For
an ambiguous name, we first extract three types of
features and construct a semantic graph from all
Wikipedia concepts extracted from the current name
observation set. We then collect global person spe-
cific evidences by clustering these concepts on the
graph into different topics, which in turn are used
to weight each concept by considering the impor-
tance of its corresponding topic in the current name
observation set and its highly related neighbors in
both the topic and its local context. At last, we in-
corporate the proposed topic representation into the
person name similarity functionand adopt the hierar-
chical agglomerative clustering (HAC) algorithm to
group these web pages.
</bodyText>
<subsectionHeader confidence="0.999068">
3.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.982671454545455">
We extract features from the contexts of ambiguous
names, including Wikipedia concepts, named enti-
ties and biographical information, such as email ad-
dresses, phone numbers and birth years.
Wikipedia Concept Extraction Each concept in
Wikipedia is described by an article containing hy-
perlinks to other concepts which are supposed to
related to the current one. All the linking rela-
tions in Wikipedia construct a huge semantic graph,
where we can mine rich semantic relationship be-
tween concepts(David and Ian, 2008). We col-
lect Wikipedia concepts from all web pages in the
dataset by comparing all n-grams (up to 8) from
the dataset to Wikipedia anchor text dictionary and
checking whether it is a Wikipedia concept surface
form. We further prune the extracted concepts ac-
cording to their keyphraseness(Mihalcea and Cso-
mai, 2007). Initially, each concept is weighted ac-
cording to its average semantic relateness(David and
Ian, 2008) with other concepts in the current page.
Named Entity and Biographical Information Ex-
traction Although Wikipedia concepts can pro-
</bodyText>
<page confidence="0.991838">
834
</page>
<bodyText confidence="0.999978375">
vide rich background knowledge, they suffer from
the limited coverage. It is common that some
discriminative features are not likely to be found
in Wikipedia, such as names of infamous people
or organizations, email addresses, phone numbers,
etc. We therefore extract two extra kinds of fea-
tures, named entities that do not appear in the
Wikipedia anchor text dictionary, and biographical
information. We use Stanford Named Entity Rec-
ognizer(Finkel et al., 2005) to collect named entities
which are not in the Wikipedia list. We use regular
expressions to extract email address, phone numbers
and birth years. For convenience, we will also call
concept features for Wikipedia concept features and
non-concept features for the other two in the rest of
this paper.
</bodyText>
<subsectionHeader confidence="0.999839">
3.2 Topic Generation and Weighting Scheme
</subsectionHeader>
<bodyText confidence="0.999820636363636">
Now we proceed to describe the key step of our
model, topic generation and weighting strategy. The
purpose of introducing topics into our model is to
exploit the corpus level importance of a feature for
a given name so that we will not miss any discrim-
inative features which are few in the current name
observation but have shown significant importance
over the whole name observation set.
Graph Construction In our model, we capture
the topic structure through a semantic graph. Specif-
ically, for each name observation set, we connect
all Wikipedia concepts appearing in the current ob-
servation set by their pairwise semantic relatedness-
David and Ian (2008)to form a semantic graph.
The constructed graph is usually very dense since
any pair of unrelated concepts would be connected
by a small semantic relatedness resulting in many
light-weighted or even meaningless edges. We
therefore propose to prune some light-weighted
edges to make the graph stable and easier to harvest
reasonable topics. We use the following strategies to
prune the graph:
</bodyText>
<listItem confidence="0.988561">
• If an edge’s weight is lower than a predefined
threshold, it will be pruned.
• If two vertices of an edge do not co-occur in
any web page of the current observation set,
then this edge will be pruned.
</listItem>
<figureCaption confidence="0.9962105">
Figure 1: An abridged example of the semantic graph
for George Foster. The green node Sports League is a
hub node, and the yellow node Pro Football Weekly is an
outlier.
</figureCaption>
<bodyText confidence="0.993889238095238">
The second rule is set to be strict and is proposed
to handle the following circumstance. Some gen-
eral concepts, such as swimming, football, basket-
ball and golf, will be measured highly related with
each other by Wikipedia semantic relatedness and
thus are very likely to be grouped into one topic,
however, they are discriminative on their own when
disambiguating different persons. For example, the
concept swimming is discriminative enough to dis-
tinguish Russian swimmer Popov from basketball
player Popov. So it is not a good idea to group
these concepts into one topic. The proposed co-
occurrence rule is based on the above observation
that it is rare that such kind of general concepts,
e.g., swim and basketball, often co-occur with each
other when talking about one specific person. Af-
ter the pruning step, for each ambiguous name, we
get a semantic graph from all Wikipedia concepts
extracted in this name observation set. Figure 1 il-
lustrates an abridged version of a semantic graph for
George Foster.
</bodyText>
<figure confidence="0.998915142857143">
0.3862
0.4008 0.3205
0.3296
Stolen
Base
0.4228
0.4145
Home
Run
0.3799
Shortstop
0.3467
0.2976
Major
League
Baseball
0.3628
Cincinnati
Reds
0.2445
0.2697
Tackle
0.3201
Sports
League
0.2738
0.3567
Cornerback
National
Football
League Pro
0. 3136
0.2245
Football
Weekly
</figure>
<page confidence="0.993092">
835
</page>
<bodyText confidence="0.999884827586207">
Graph Clustering Considering the graph con-
struction strategy we use, it is more suitable for us
to group the concepts on the graph into several top-
ics using a density-based clustering model.
We choose SCAN algorithm Xu et al. (2007) to
perform the clustering step. The SCAN algorithm
utilizes a neighborhood structure to measure the
similarity between two vertices. If a vertex has a
minimal of µ neighbors with a similarity larger than
ε, it is called a core. The algorithm1 starts from a
random vertex in a graph, examining whether it is a
core or not. If yes, the algorithm will expand a clus-
ter from this vertex recursively, otherwise the vertex
will be assigned either a hub node or an outlier de-
pending on the number of its neighboring clusters.
A hub node connects to more than one cluster, while
an outlier connects to one or no cluster. Take the
semantic graph in Figure 1 for example, the node
Sports League is a hub node, while the node Pro
Football Weekly is an outlier. Finally, all concepts
in the graph are grouped into K + 2 parts (K is the
number of the clusters, and is determined automat-
ically), including K clusters, the set of hub nodes
and the set of outliers.
One problem of applying SCAN in our work is
that it is originally designed for unweighted graphs.
We have to adapt it to our weighted graph by mod-
ifying the similarity function between two nodes as
follows:
</bodyText>
<equation confidence="0.994718">
simnb(c1, c2) sr(c1, c2)
sim(c1, c2) = α � + (1)
1 + α 1 + α
and simnb(c1, c2) is defined as:
3r(c1,c)+3r(c2,c)
2
simnb(c1, c2) = |N(c1) U N(c2)|
</equation>
<bodyText confidence="0.99860275">
where N(c) is the neighbor set of concept c. This
new similarity function contains two parts: the
neighborhood similarity and the semantic related-
ness between two concepts. We combine them us-
ing a linear combination, where α is a weight tuned
during training.
Topic Generation Now we will map the cluster-
ing results into different topics. Intuitively, each
</bodyText>
<footnote confidence="0.8442255">
1We omit the details of SCAN for brevity, and refer inter-
ested reader to Xu et al. (2007) for more details.
</footnote>
<bodyText confidence="0.999814042553192">
cluster will be treated as a topic. However, we found
that hub nodes usually correspond to general con-
cepts which may be related to many topics, but with
a loose relatedness. We thus distribute each general
concept into its every related topic, but with a lower
weight to distinguish from ordinary concepts in this
topic.
Outliers may be concepts which are far away from
main themes of the corpus, or noise concepts. We
calculate the average semantic relatedness of an out-
lier with its neighbor concepts that belong to one
topic. If the result is lower than a threshold, this
outlier will be discarded, otherwise it will be treated
as a non-concept feature.
Now we are able to map the clustering results
into different topics. Intuitively, each cluster will
be treated as a topic. However, we found that hub
nodes usually correspond to general concepts, e.g.,
education or public, which may be related to many
topics, but with a loose relatedness. We thus dis-
tribute each general concept into its every related
topic, but with a lower weight to distinguish from
ordinary concepts in this topic. Outliers are found
to contain concepts which are far away from main
topics of the document set and look like noise con-
cepts. We therefore calculate the average semantic
relatedness of an outlier node with its neighboring
concepts which belong to some topics. If the aver-
age relatedness is lower than a threshold, this node
will be discarded, otherwise it will be treated as a
non-concept feature.
Weighting Topics After generating all topics, we
should weight each topic according to its importance
in the current name observation set as well as the
quality of the topic (cluster). Intuitively, if most con-
cepts in the topic are considered to be discriminative
in the current name set and they are closely related
to each other, this topic should be weighted as im-
portant. By properly weighting the generated topics,
we can capture the importance of a concept reliably
in the corpus level (in the current name observation
set) rather than in the current page solely.
Before we weight a topic, we first explain how
we re-weight a hub concept in a topic since our ini-
tial feature weighting scheme(Han and Zhao, 2009)
works on individual web page, lacks cross document
information and is likely to over-estimate the impor-
</bodyText>
<equation confidence="0.959956">
E
cEN(c1)nN(c2)
</equation>
<page confidence="0.98493">
836
</page>
<bodyText confidence="0.9999315">
tance of a hub node (general concept) by by assign-
ing a higher weight. Suppose a hub node h connects
to a topic t with n neighbors, namely c1, c2, · · · , cn.
The similarity between this hub node and the topic
is computed by averaging the semantic relatedness
between this hub node and these n neighbors:
</bodyText>
<equation confidence="0.846917">
sr(h,cZ). (2)
</equation>
<bodyText confidence="0.9997875">
We then update the weight of this hub node by
considering its similarity with this topic: wt(h) =
w(h) x sim(h, t) from which we can see that the
hub node receives a lower weight than before indi-
cating that it is not as important as ordinary concepts
in a topic.
Now we proceed to weight the topic t by taking
into account the frequencies of its concepts and the
coherence between the concepts and their neighbor-
hood in topic t:
</bodyText>
<equation confidence="0.998151">
n coh(cZ, t)
w(t) =
</equation>
<bodyText confidence="0.997374285714286">
where topic t contains n concepts {c1, c2, ..., cn},
f(c) is the frequency of concept c over current name
observation set, specially, when c is a hub node con-
cept, we will distribute its frequency according to
equation (2), having ft(c) = f(c)sim(c, t). And
n coh(c, t) is the neighborhood coherence of con-
cept c with topic t, defined as:
</bodyText>
<equation confidence="0.981309">
sr(q, c)
(4)
jN(c) n tj
</equation>
<bodyText confidence="0.950938625">
where N(c) is the neighboring node set of concept
c.
By incorporating corpus level concept frequen-
cies into topic weighting, discriminative concepts
that are sparse in one document and suppressed by
conventional models can benefit from their corpus
level importance as well as their coherence in related
topics.
</bodyText>
<subsectionHeader confidence="0.999232">
3.3 Clustering Person Name Observations
</subsectionHeader>
<bodyText confidence="0.9957405">
Now the remaining key step is to compute the sim-
ilarity between two name observations. The simi-
larity proposed in GRAPE(Jiang et al., 2009) mea-
sures two documents by bridge tags (common fea-
tures) shared by two document graphs. Specifically,
Jiang et al. utilize cohesion to weight a bridge tag in
a document. The more bridge tags two documents
share, the stronger the cohesion of each bridge tag
is, and in turn the more similar the two documents
are.
However, this similarity bears a shortcoming that
the bridge tags shared by the two documents re-
quire an exact match of features, which does not
take any semantic relatedness into consideration. If
two web pages mentioning the same person but have
few features in common, the GRAPE similarity may
not work properly. We, therefore, propose a new
similarity measure combining topic similarity, topic
based connectivity strength and GRAPE’s connec-
tivity strength.
Matching Topics to Person Name Observations
We first describe how to match the generated top-
ics to different name observations. In order to avoid
unreliable estimation, we only match a topic to a
name observation when they share at least one con-
cept. To measure the relatedness between a topic
and a name observation, we formulate this similar-
ity as the weighted average of semantic relatedness
between each concept from one side and its closely
related counterpart from the other side,defined as:
</bodyText>
<equation confidence="0.9951205">
P wA(a) x wB(ba) x sr(a, ba)
aEA (5)
wA(a) x wB(ba)
sim(A, B) = (sim(A —* B) + sim(B —* A))/2,
</equation>
<bodyText confidence="0.999928777777778">
where A can be a topic and B a name observation or
vice versa, ba is a concept in B that is most related to
concept a, wA(a) represents the weight of concept a
estimated by the averaged relatedness between a and
other concepts in A.
Person Name Similarity Now we describe the
first component in our proposed measure: topic sim-
ilarity, which is calculated through the common top-
ics shared by the two name observations, o1 and o2:
</bodyText>
<equation confidence="0.983761666666667">
XT5m(o1, o2) = sim(o1, t) x sim(o2, t) (6)
tET(o1,o2)
xsim(o1 n t, o2 n t) x w(t)
</equation>
<bodyText confidence="0.9999055">
where T(o1,o2) contains all common topics of o1
and o2, w(t) is the weight of topic t estimated using
</bodyText>
<equation confidence="0.9936136">
1
sim(h, t) =
n
Xn
Z=1
f(cZ)
x
Pn
Z=1
n
n
Pn
Z=1
(3)
n coh(c, t) =
P
qEN(c)nt
sim(A —* B) =
P
aEA
</equation>
<page confidence="0.973719">
837
</page>
<bodyText confidence="0.997005826086957">
equation (3), both sim(oz, t) and sim(o1 n t, o2 n t)
measure the similarity between two concept sets and
can be estimated using equation (5). The underly-
ing idea of the equation is, if two name observations
share more and closer common topics, and also these
topics receive higher weights according to the cur-
rent name observation set, then the two observations
should be more related to each other.
Specifically, the factor sim(o1 n t, o2 n t) is de-
signed to measure the fine relatedness between o1
and o2 given the topic t. Sometimes, both o1 and
o2 are mapped to t and both close to this topic, but
in fact they depict different aspects of t since some
of our topics are more general thus include several
aspects. The comparison of their intersections will
provide a more detailed view for their similarity.
Inspired by the use of bridge tags in
GRAPE(Jiang et al., 2009), we propose to capture
the connection strength between concept sets by the
means of our topics. We consider common topics
as the bridge tags and define our topic based con-
nectivity strength between two name observations
as:
</bodyText>
<equation confidence="0.99987">
1 �
T C5(o1, o2) =
2
tET(o1,o2)
(Cohs(o1, t) + Cohs(o2, t)) (7)
</equation>
<bodyText confidence="0.9991">
Note that we still need sim(o1 n t, o2 n t) to capture
the fine differences inside a topic. Cohs(o, t) is a
cohesion measure to capture the relatedness between
non-concept features in o and concept features in t,
defined as:
</bodyText>
<equation confidence="0.996003">
Cohs(o, t) = � w(t)x � occ(c, q)fo(c)fo(q)
cEont qEEB(o)
</equation>
<bodyText confidence="0.9713771875">
(8)
where EB(o) contains all non-concept features in
o (e.g., non-Wikipedia entities and biographical in-
formation), occ(c, q) is the co-occurring number of
concept c and feature q, fo(q) is the relative fre-
quency of q in observation o. It is easy to find that
a higher cohesion can be achieved by larger overlap
between o and t, higher topic weight and more co-
occurrences of concept features in t and other fea-
tures in o.
The third part is the original connectivity strength
defined in GRAPE(Jiang et al., 2009): C5(o1, o2),
calculated using plain features without topics (we
omit the details for brevity). Finally, we linearly
combine equation (6), (7) and C5(o1, o2) into the
person name similarity function as:
</bodyText>
<equation confidence="0.9997805">
5(o1,o2)= α1 x T5m(o1,o2) + α2 x TC5(o1,o2)
+(1 − α1 − α2) x C5(o1, o2) (9)
</equation>
<bodyText confidence="0.999810833333333">
where α1 and α2 are optimized during training.
This final similarity function will then be embed-
ded into a normal HAC algorithm to group the web
pages into different namesakes where we compute
the centroid-based distance between clusters(Mann
and Yarowsky, 2003).
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998155">
We compare our model with competitive baselines
on three WePS datasets. In the following, we first
describe the experimental setup, and then discuss the
their performances.
</bodyText>
<subsectionHeader confidence="0.9929">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9998134375">
Wikipedia Data Wikipedia offers free copies of
all available data to interested users in their website.
We used the one released in March 6th, 2009 in our
experiments. We identified over 4,000,000 highly
connected concepts in this dump; each concept links
to 10 other concepts in average.
WePS Datasets We used three datasets in our
experiments, WePS1 Training and Testing (Artiles
et al., 2007), WePS2 Testing (Javier et al., 2009).
These datasets collected names from three differ-
ent resources including Wikipedia names, program
committee of a computer science conference and US
census. Each name were queried in Yahoo! Search
and top N result pages (100 pages in WePS1 and
150 pages in WePS2) were obtained and manually
labeled.
</bodyText>
<subsectionHeader confidence="0.998343">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.9699476">
We compare our model TM with four baseline meth-
ods: (1)VSM: traditional vector space model with
cosine similarity. We use features extracted in Sec-
tion 3.1 and weight them using TFIDF. The docu-
ments are grouped using standard HAC algorithm.
(2)GRAPE(Jiang et al., 2009): we re-implement the
state-of-the-art system which outperforms any mod-
els that do not use extra knowledge resources re-
ported in WePS1 and WePS2. (3)WS: the Wikipedia
sim(o1 n t, o2 n t) x
</bodyText>
<page confidence="0.97496">
838
</page>
<bodyText confidence="0.99998">
Semantic method(Han and Zhao, 2009). This sys-
tem uses Wikipedia to enhance the results of name
disambiguation. (4)SSR: the Structural Semantic re-
latedness model(Han and Zhao, 2010) creates a se-
mantic graph to re-calculate the semantic related-
ness between features, and captures both explicit
semantic relations and implicit structural semantic
knowledge. We also build two variants of TM: TM-
nTW which removes topic weighting to examine
what effect the topic weighting strategy can make
and whether it can provide a person specific evi-
dence and TM-nCP which does not use co-occurring
information to prune the semantic graph to examine
whether the pruning is effective.
</bodyText>
<subsectionHeader confidence="0.999014">
4.3 Parameters
</subsectionHeader>
<bodyText confidence="0.999978846153846">
There are several parameters to be tuned in our
model. In the SCAN algorithm, we use default pa-
rameters according to (Xu et al., 2007) with an ex-
ception: the weight α is tuned exhaustively to be 0.2.
Note that the number of topics are automatically de-
cided by SCAN. The semantic graph pruning thresh-
old is set to 0.27 tuned on a held out set. The
smoothing parameters in equation (9) are: α1 = 0.3,
α2 = 0.2 which are tuned using cross validation.
Optimization of some parameters will be addressed
in detail in the following subsection. In HAC, all
optimal merging thresholds are selected by applying
leave-one-out cross validation.
</bodyText>
<subsectionHeader confidence="0.976224">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999944823529412">
We adopt the same evaluation process as (Han and
Zhao, 2009), and evaluating these models using Pu-
rity, Inverse Purity and the F-measure (also used in
WePS Task Artiles et al. (2007)). The overall perfor-
mance is shown in Table 1, and the best scores are
in boldface.
Let us first look at our model and its variants,
TM-nTW and TM-nCP. By introducing the corpus
level topic weighting scheme, our model improves
in average 1.6% consistently over all datasets. Re-
call that our topic weightings are obtained over the
whole name observation set beyond local context,
this improvement indicates that this corpus level per-
son specific evidences render the person similarity
more reasonably than that of single document. On
the other hand, by pruning the semantic graph, our
model improves averagely 1.3% over TM-nCP. This
</bodyText>
<tableCaption confidence="0.874546">
Table 1: Web person name disambiguation results on all
three WePS datasets
</tableCaption>
<table confidence="0.99987237037037">
WePS1 Training
Method P IP FMeasure
VSM 0.86 0.86 0.85
GRAPE 0.93 0.90 0.91
WS 0.88 0.89 0.87
SSR 0.82 0.92 0.85
TM-nTW 0.91 0.89 0.90
TM-nCP 0.92 0.90 0.91
TM 0.93 0.91 0.91
WePS1 Testing
Method P IP FMeasure
VSM 0.79 0.85 0.81
GRAPE 0.93 0.83 0.87
WS 0.88 0.90 0.88
SSR 0.85 0.83 0.84
TM-nTW 0.93 0.85 0.88
TM-nCP 0.92 0.86 0.88
TM 0.94 0.86 0.90
WePS2 Testing
Method P IP FMeasure
VSM 0.82 0.87 0.83
GRAPE 0.88 0.90 0.89
WS 0.85 0.89 0.86
SSR 0.89 0.84 0.86
TM-nTW 0.92 0.87 0.89
TM-nCP 0.93 0.88 0.90
TM 0.93 0.89 0.91
</table>
<bodyText confidence="0.9999745">
shows that our co-occurrence based pruning strategy
can help render the semantic graph with less noisy
edges, thus generate more reasonable topics.
Generally, our proposed model works best con-
sistently over all three datasets. Our method gains
9.3% improvement on average in three datasets com-
pared with VSM, 1.7% improvement compared to
GRAPE, 3.8% over WS and 6.7% over SSR. We also
performed significance testing on F-measures: the
differences between our model and other models are
significant. We notice there are many noisy or short
web pages which lead to inaccurate concept extrac-
tion, but this cross document evidences, to some ex-
tent, can remedy this. In the Emily Bender exam-
ple, our system correctly groups the odd page, which
contains limited clues, into the nutritionist cluster,
</bodyText>
<page confidence="0.995341">
839
</page>
<bodyText confidence="0.999988833333333">
while the rest, excluding WS and SSR, failed. Sur-
prisingly, SSR combines both kinds of relations and
implicit structural knowledge, but performs in the
same bulk with VSM in WePS1 training set. We
think the reason may be that some name observation
sets are too small to estimate non-concept related-
ness via random walk. In WePS1 training set, many
names in this dataset contains several namesakes,
each of which corresponds to a few web pages. In
this case, our corpus level weighting scheme and
WS show no advantage over GRAPE which consid-
ers word co-occurrences solely. From the results,
we can also find that there is no clear winner be-
tween GRAPE and WS. The former does not use
Wikipedia relatedness but only includes local rela-
tionship, and performs even slightly better than WS
in WePS2, which indicates that non-Wikipedia con-
cepts are important disambiguation features as well.
</bodyText>
<subsectionHeader confidence="0.899883">
4.5 Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.999980535714286">
In this subsection, we discuss the optimization of
several parameters in the proposed method. In total
we need to set four parameters. The first one is the
edge pruning threshold during graph construction;
the second one is the weight α in SCAN algorithm;
the third one and the forth one are the combination
parameters in the final similarity function. We will
address the first two in the following. The last two
combination parameters are tuned by exhaustively
searching the space and omitted here for brevity
First, we configure the pruning threshold. Intu-
itively, larger threshold can prune more unimpor-
tant edges and improve the disambiguation perfor-
mance. However, if the threshold is too large, we
may prune important edges and harm the results.
The F-measure of our method with respect to the
pruning threshold is plotted in Figure 2.
From Figure 2, we can know that in all three
data sets, a pruning threshold of 0.27 will lead to
the best performance. Both increasing and decreas-
ing of this pruning threshold will cause a decline of
the F-Measure, because they will either leave more
noisy light-weighted edges or prune some important
edges.
Secondly, we configure the neighborhood similar-
ity weight. The larger this weight is, the more neigh-
borhood information can influence the similarity be-
tween two nodes in the semantic graph. We plot the
</bodyText>
<subsectionHeader confidence="0.36883">
Edge Pruning Threshold
</subsectionHeader>
<figureCaption confidence="0.9969885">
Figure 2: The F-Measure v.s. the edge pruning threshold
on three data sets.
</figureCaption>
<figure confidence="0.999548428571429">
0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38
0.92
0.91
0.9
0.89
0.88
0.87
0.86
WePS1 Training
WePS1 Testing
WePS2 Testing
FMeasure
FMeasure
0.925
0.915
0.905
0.895
0.885
0.93
0.92
0.91
0.89
0.880 0.2 0.4 0.6 0.8 1
0.9
WePS1 Training
WePS1 Testing
WePS2 Testing
Neighborhood Weight
</figure>
<figureCaption confidence="0.9888505">
Figure 3: The F-Measure v.s. the neighborhood similarity
weight on three data sets.
</figureCaption>
<bodyText confidence="0.999353909090909">
performance of our method regarding to the neigh-
borhood similarity weight in Figure 3.
From Figure 3, we know that for the WePS 1 Test-
ing and WePS2 Testing data sets, a neighborhood
similarity weight of 0.2 can result in the best perfor-
mance, but for WePS 1 Training set, the weight for
the best performance is 0.6. In fact, when the neigh-
borhood similarity weight varies from 0 to 1, the dif-
ference between the best and worst performance are
less than 0.01, which indicates that neighborhood
similarity is as considerable as semantic relatedness.
</bodyText>
<page confidence="0.993193">
840
</page>
<sectionHeader confidence="0.996222" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999953451612903">
In this paper, we explore the feature space in the
web person name disambiguation task and propose
a topic-based model which exploits corpus level
person specific evidences to handle the data spar-
sity challenges, especially the case that limited ev-
idences can be collected from the local context. In
particular, we harvest topics from wikipedia con-
cepts appearing in the name observation set, and
weight a concept based on both the relatedness of
the concept to its corresponding topic and the im-
portance of this topic in the current name observa-
tion set, so that some discriminative but sparse fea-
tures can obtain more reliable weights. Experimen-
tal results show that our weighting strategy does its
job and the proposed model outperforms the-state-
of-the-art systems. Our current work utilizes the
topic information shared in one name observation
set but is incapable to handle sparse name set, which
needs more accurate relation extraction inside the
name observations. Jointly modeling entity link-
ing and person (entity) disambiguation tasks will
be an interesting direction where the two tasks are
closely related and usually need to be considered at
the same time. Investigating the person name dis-
ambiguation task in different web applications will
also be of great importance, e.g., disambiguating a
name in streaming data or during knowledge base
construction. In addition, graphical model, which
has been studied in academic author disambiguation,
may be a good choice to cope with the noises and
non-standard forms in web data.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998138818181818">
We would like to thank Yidong Chen, Wei Wang
and Tinghua Wang for their useful discussions and
the anonymous reviewers for their helpful comments
which greatly improved the work and the presen-
tation. This work was supported by the National
High Technology Research and Development Pro-
gram of China (Grant No. 2012AA011101), Na-
tional Natural Science Foundation of China (Grant
No.61003009) and Research Fund for the Doc-
toral Program of Higher Education of China (Grant
No.20100001120029).
</bodyText>
<sectionHeader confidence="0.998033" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998858418604651">
Artiles, J., Gonzalo, J., and Sekine, S. (2007). The
semeval-2007 weps evaluation: establishing a
benchmark for the web people search task. In
SemEval, SemEval ’07, pages 64–69, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Bagga, A. and Baldwin, B. (1998). Entity-based
cross-document coreferencing using the vector
space model. In ACL, pages 79–85, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Bunescu, R. C. and Pasca, M. (2006). Using ency-
clopedic knowledge for named entity disambigua-
tion. In EACL. The Association for Computer
Linguistics.
Cucerzan, S. (2007). Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708–716. ACL.
David, M. and Ian, H. (2008). An effective, low-cost
measure of semantic relatedness obtained from
wikipedia links. In AAAI, AAAI ’08.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
ACL, pages 363–370, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.
Han, X. and Zhao, J. (2009). Named entity dis-
ambiguation by leveraging wikipedia semantic
knowledge. In CIKM, CIKM ’09, pages 215–224,
New York, NY, USA. ACM.
Han, X. and Zhao, J. (2010). Structural semantic
relatedness: a knowledge-based method to named
entity disambiguation. In ACL, ACL ’10, pages
50–59, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ikeda, M., Ono, S., Sato, I., Yoshida, M., and Naka-
gawa, H. (2009). Person name disambiguation on
the web by two-stage clustering. In WWW.
Iria, J., Xia, L., and Zhang, Z. (2007). Wit: web peo-
ple search disambiguation using random walks. In
SemEval, SemEval ’07, pages 480–483, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
</reference>
<page confidence="0.9788">
841
</page>
<reference confidence="0.883389448275862">
Javier, A., Julio, G., and Satoshi, S. (2009). Weps 2
evaluation campaign: Overview of the web peo-
ple search clustering task. In WWW 2009.
Jiang, L., Wang, J., An, N., Wang, S., Zhan, J.,
and Li, L. (2009). Grape: A graph-based frame-
work for disambiguating people appearances in
web search. In ICDM, ICDM ’09, pages 199–208,
Washington, DC, USA. IEEE Computer Society.
Kalashnikov, D. V., Chen, Z., Mehrotra, S., and
Nuray-Turan, R. (2008a). Web people search via
connection analysis. IEEE Trans. on Knowl. and
Data Eng., 20:1550–1565.
Kalashnikov, D. V., Nuray-Turan, R., and Mehrotra,
S. (2008b). Towards breaking the quality curse.: a
web-querying approach to web people search. In
SIGIR, SIGIR ’08, pages 27–34, New York, NY,
USA. ACM.
Mann, G. S. and Yarowsky, D. (2003). Unsuper-
vised personal name disambiguation. In CONLL,
CONLL ’03, pages 33–40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mihalcea, R. and Csomai, A. (2007). Wikify!: link-
ing documents to encyclopedic knowledge. In
Proceedings of CIKM’07, pages 233–242.
Niu, C., Li, W., and Srihari, R. K. (2004). Weakly
supervised learning for cross-document person
name disambiguation supported by information
extraction. In ACL, ACL ’04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
</reference>
<bodyText confidence="0.920154166666666">
Pilz, A. (2010). Entity disambiguation using link
based relations extracted from wikipedia. In
ICML.
Rao, D., Garera, N., and Yarowsky, D. (2007). Jhu1:
an unsupervised approach to person name dis-
ambiguation using web snippets. In SemEval,
SemEval ’07, pages 199–202, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wu, F. and Weld, D. S. (2008). Automatically re-
fining the wikipedia infobox ontology. In WWW,
WWW ’08, pages 635–644, New York, NY, USA.
ACM.
</bodyText>
<reference confidence="0.994969857142857">
Xu, X., Yuruk, N., Feng, Z., and Schweiger, T. A. J.
(2007). Scan: a structural clustering algorithm
for networks. In Proceedings of KDD, KDD ’07,
pages 824–833, New York, NY, USA. ACM.
Yiming, L., Zaiqing, N., Taoyuan, C., Ying, G., and
Ji-Rong, W. (2007). Name disambiguation using
web connection. In AAAI.
</reference>
<page confidence="0.998023">
842
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.517805">
<title confidence="0.999813">Explore Person Specific Evidence in Web Person Name Disambiguation</title>
<author confidence="0.99085">Liwei Chen</author>
<author confidence="0.99085">Yansong Feng</author>
<author confidence="0.99085">Lei Zou</author>
<author confidence="0.99085">Dongyan</author>
<affiliation confidence="0.995515">Institute of Computer Science and</affiliation>
<address confidence="0.60129">Peking</address>
<abstract confidence="0.99464808">In this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. In literature, the latter receives less attention and remains more challenging. We explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature’s importance in a given web page. This can alleviate the of clues discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. We therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity. The experimental results show that the corpus level topic information provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The semeval-2007 weps evaluation: establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In SemEval, SemEval ’07,</booktitle>
<pages>64--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27405" citStr="Artiles et al., 2007" startWordPosition="4542" endWordPosition="4545">ers(Mann and Yarowsky, 2003). 4 Experiments We compare our model with competitive baselines on three WePS datasets. In the following, we first describe the experimental setup, and then discuss the their performances. 4.1 Data Wikipedia Data Wikipedia offers free copies of all available data to interested users in their website. We used the one released in March 6th, 2009 in our experiments. We identified over 4,000,000 highly connected concepts in this dump; each concept links to 10 other concepts in average. WePS Datasets We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al., 2007), WePS2 Testing (Javier et al., 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. Each name were queried in Yahoo! Search and top N result pages (100 pages in WePS1 and 150 pages in WePS2) were obtained and manually labeled. 4.2 Baselines We compare our model TM with four baseline methods: (1)VSM: traditional vector space model with cosine similarity. We use features extracted in Section 3.1 and weight them using TFIDF. The documents are grouped using standard HAC algorithm. (2)GRAPE</context>
<context position="29743" citStr="Artiles et al. (2007)" startWordPosition="4929" endWordPosition="4932">of topics are automatically decided by SCAN. The semantic graph pruning threshold is set to 0.27 tuned on a held out set. The smoothing parameters in equation (9) are: α1 = 0.3, α2 = 0.2 which are tuned using cross validation. Optimization of some parameters will be addressed in detail in the following subsection. In HAC, all optimal merging thresholds are selected by applying leave-one-out cross validation. 4.4 Results and Discussion We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al. (2007)). The overall performance is shown in Table 1, and the best scores are in boldface. Let us first look at our model and its variants, TM-nTW and TM-nCP. By introducing the corpus level topic weighting scheme, our model improves in average 1.6% consistently over all datasets. Recall that our topic weightings are obtained over the whole name observation set beyond local context, this improvement indicates that this corpus level person specific evidences render the person similarity more reasonably than that of single document. On the other hand, by pruning the semantic graph, our model improves </context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Artiles, J., Gonzalo, J., and Sekine, S. (2007). The semeval-2007 weps evaluation: establishing a benchmark for the web people search task. In SemEval, SemEval ’07, pages 64–69, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In ACL,</booktitle>
<pages>79--85</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7977" citStr="Bagga and Baldwin, 1998" startWordPosition="1258" endWordPosition="1262">, including two steps: feature extraction and person clustering. Most research efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, A. and Baldwin, B. (1998). Entity-based cross-document coreferencing using the vector space model. In ACL, pages 79–85, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In EACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Bunescu, R. C. and Pasca, M. (2006). Using encyclopedic knowledge for named entity disambiguation. In EACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL,</booktitle>
<pages>708--716</pages>
<publisher>ACL.</publisher>
<marker>Cucerzan, 2007</marker>
<rawString>Cucerzan, S. (2007). Large-scale named entity disambiguation based on wikipedia data. In EMNLPCoNLL, pages 708–716. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M David</author>
<author>H Ian</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from wikipedia links.</title>
<date>2008</date>
<booktitle>In AAAI, AAAI ’08.</booktitle>
<contexts>
<context position="12216" citStr="David and Ian, 2008" startWordPosition="1925" endWordPosition="1928">e hierarchical agglomerative clustering (HAC) algorithm to group these web pages. 3.1 Feature Extraction We extract features from the contexts of ambiguous names, including Wikipedia concepts, named entities and biographical information, such as email addresses, phone numbers and birth years. Wikipedia Concept Extraction Each concept in Wikipedia is described by an article containing hyperlinks to other concepts which are supposed to related to the current one. All the linking relations in Wikipedia construct a huge semantic graph, where we can mine rich semantic relationship between concepts(David and Ian, 2008). We collect Wikipedia concepts from all web pages in the dataset by comparing all n-grams (up to 8) from the dataset to Wikipedia anchor text dictionary and checking whether it is a Wikipedia concept surface form. We further prune the extracted concepts according to their keyphraseness(Mihalcea and Csomai, 2007). Initially, each concept is weighted according to its average semantic relateness(David and Ian, 2008) with other concepts in the current page. Named Entity and Biographical Information Extraction Although Wikipedia concepts can pro834 vide rich background knowledge, they suffer from </context>
<context position="14230" citStr="David and Ian (2008)" startWordPosition="2247" endWordPosition="2251">p of our model, topic generation and weighting strategy. The purpose of introducing topics into our model is to exploit the corpus level importance of a feature for a given name so that we will not miss any discriminative features which are few in the current name observation but have shown significant importance over the whole name observation set. Graph Construction In our model, we capture the topic structure through a semantic graph. Specifically, for each name observation set, we connect all Wikipedia concepts appearing in the current observation set by their pairwise semantic relatednessDavid and Ian (2008)to form a semantic graph. The constructed graph is usually very dense since any pair of unrelated concepts would be connected by a small semantic relatedness resulting in many light-weighted or even meaningless edges. We therefore propose to prune some light-weighted edges to make the graph stable and easier to harvest reasonable topics. We use the following strategies to prune the graph: • If an edge’s weight is lower than a predefined threshold, it will be pruned. • If two vertices of an edge do not co-occur in any web page of the current observation set, then this edge will be pruned. Figur</context>
</contexts>
<marker>David, Ian, 2008</marker>
<rawString>David, M. and Ian, H. (2008). An effective, low-cost measure of semantic relatedness obtained from wikipedia links. In AAAI, AAAI ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="13229" citStr="Finkel et al., 2005" startWordPosition="2083" endWordPosition="2087">ess(David and Ian, 2008) with other concepts in the current page. Named Entity and Biographical Information Extraction Although Wikipedia concepts can pro834 vide rich background knowledge, they suffer from the limited coverage. It is common that some discriminative features are not likely to be found in Wikipedia, such as names of infamous people or organizations, email addresses, phone numbers, etc. We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dictionary, and biographical information. We use Stanford Named Entity Recognizer(Finkel et al., 2005) to collect named entities which are not in the Wikipedia list. We use regular expressions to extract email address, phone numbers and birth years. For convenience, we will also call concept features for Wikipedia concept features and non-concept features for the other two in the rest of this paper. 3.2 Topic Generation and Weighting Scheme Now we proceed to describe the key step of our model, topic generation and weighting strategy. The purpose of introducing topics into our model is to exploit the corpus level importance of a feature for a given name so that we will not miss any discriminati</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In ACL, pages 363–370, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>J Zhao</author>
</authors>
<title>Named entity disambiguation by leveraging wikipedia semantic knowledge.</title>
<date>2009</date>
<booktitle>In CIKM, CIKM ’09,</booktitle>
<pages>215--224</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3789" citStr="Han and Zhao, 2009" startWordPosition="591" endWordPosition="595">hus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Javier et al., 2009). Throughout the whole page we can find only one word, nutrition, related to her identification, while other pages about the nutritionist in the dataset contain substantial materials about her profession and job. In this case, current efforts, focusing on either feature engineering or background knowledge, are incapable to exploiting these limited clues from the current page to the whole Em</context>
<context position="5151" citStr="Han and Zhao, 2009" startWordPosition="806" endWordPosition="809">now, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the significance of nutrition for Emily Bender in WePS1 dataset. In this paper, we explore different usages of features and propose an approach which mines cross document information to capture the person specific importance for a feature. Specifically, we construct a semantic graph from Wikipedia concepts appearing in all documents that contain the targe</context>
<context position="10240" citStr="Han and Zhao (2009)" startWordPosition="1605" endWordPosition="1608">aph simultaneously(Han and Zhao, 2010). Most approaches discussed above explore various features in the current page or rely on external knowledge resources to bridge the vocabulary gap, but pay less attention to the lack of clues since they ignore the person specific evidence in the current corpus level. Our model focuses on solving the data sparsity problem by utilizing other web pages in the same name observation set to provide a robust but person specific weighting for discriminative features beyond the current document alone. In terms of extra resources, the Wikipedia based model (WS) by Han and Zhao (2009) is close to our model. The WS model uses Wikipedia to capture the relationship between entities in the local context to bridge the vocabulary gap, but it is incapable to evaluate the importance of a feature with regarding to the target name, hence is unable to make use of limited clues in the current web page. Our method captures person specific evidences by generating topics from all concepts in the current name observation set and weighting a feature accordingly. In this case, discriminative features that are sparse in the current page can be globally weighted so as to provide a more accura</context>
<context position="20486" citStr="Han and Zhao, 2009" startWordPosition="3324" endWordPosition="3327">nce in the current name observation set as well as the quality of the topic (cluster). Intuitively, if most concepts in the topic are considered to be discriminative in the current name set and they are closely related to each other, this topic should be weighted as important. By properly weighting the generated topics, we can capture the importance of a concept reliably in the corpus level (in the current name observation set) rather than in the current page solely. Before we weight a topic, we first explain how we re-weight a hub concept in a topic since our initial feature weighting scheme(Han and Zhao, 2009) works on individual web page, lacks cross document information and is likely to over-estimate the imporE cEN(c1)nN(c2) 836 tance of a hub node (general concept) by by assigning a higher weight. Suppose a hub node h connects to a topic t with n neighbors, namely c1, c2, · · · , cn. The similarity between this hub node and the topic is computed by averaging the semantic relatedness between this hub node and these n neighbors: sr(h,cZ). (2) We then update the weight of this hub node by considering its similarity with this topic: wt(h) = w(h) x sim(h, t) from which we can see that the hub node re</context>
<context position="28253" citStr="Han and Zhao, 2009" startWordPosition="4681" endWordPosition="4684">arch and top N result pages (100 pages in WePS1 and 150 pages in WePS2) were obtained and manually labeled. 4.2 Baselines We compare our model TM with four baseline methods: (1)VSM: traditional vector space model with cosine similarity. We use features extracted in Section 3.1 and weight them using TFIDF. The documents are grouped using standard HAC algorithm. (2)GRAPE(Jiang et al., 2009): we re-implement the state-of-the-art system which outperforms any models that do not use extra knowledge resources reported in WePS1 and WePS2. (3)WS: the Wikipedia sim(o1 n t, o2 n t) x 838 Semantic method(Han and Zhao, 2009). This system uses Wikipedia to enhance the results of name disambiguation. (4)SSR: the Structural Semantic relatedness model(Han and Zhao, 2010) creates a semantic graph to re-calculate the semantic relatedness between features, and captures both explicit semantic relations and implicit structural semantic knowledge. We also build two variants of TM: TMnTW which removes topic weighting to examine what effect the topic weighting strategy can make and whether it can provide a person specific evidence and TM-nCP which does not use co-occurring information to prune the semantic graph to examine w</context>
<context position="29621" citStr="Han and Zhao, 2009" startWordPosition="4908" endWordPosition="4911">rs according to (Xu et al., 2007) with an exception: the weight α is tuned exhaustively to be 0.2. Note that the number of topics are automatically decided by SCAN. The semantic graph pruning threshold is set to 0.27 tuned on a held out set. The smoothing parameters in equation (9) are: α1 = 0.3, α2 = 0.2 which are tuned using cross validation. Optimization of some parameters will be addressed in detail in the following subsection. In HAC, all optimal merging thresholds are selected by applying leave-one-out cross validation. 4.4 Results and Discussion We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al. (2007)). The overall performance is shown in Table 1, and the best scores are in boldface. Let us first look at our model and its variants, TM-nTW and TM-nCP. By introducing the corpus level topic weighting scheme, our model improves in average 1.6% consistently over all datasets. Recall that our topic weightings are obtained over the whole name observation set beyond local context, this improvement indicates that this corpus level person specific evidences render the person simi</context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>Han, X. and Zhao, J. (2009). Named entity disambiguation by leveraging wikipedia semantic knowledge. In CIKM, CIKM ’09, pages 215–224, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>J Zhao</author>
</authors>
<title>Structural semantic relatedness: a knowledge-based method to named entity disambiguation.</title>
<date>2010</date>
<booktitle>In ACL, ACL ’10,</booktitle>
<pages>50--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9659" citStr="Han and Zhao, 2010" startWordPosition="1507" endWordPosition="1510">und knowledge about the semantic relatedness between entities can be leveraged to improve the disambiguation performance, and relieve the coverage problem, to some extent. Bunescu and Pasca and Cucerzan utilize Wikipedia’s category hierarchy to disambiguate entities, while Pilz uses Wikipedia’s link information. Han and Zhao adopt Wikipedia semantic relatedness to compute the similarity between name observations. They also combine multiple knowledge sources and capture explicit semantic relatedness between concepts and implicit semantic relationship embedded in a semantic graph simultaneously(Han and Zhao, 2010). Most approaches discussed above explore various features in the current page or rely on external knowledge resources to bridge the vocabulary gap, but pay less attention to the lack of clues since they ignore the person specific evidence in the current corpus level. Our model focuses on solving the data sparsity problem by utilizing other web pages in the same name observation set to provide a robust but person specific weighting for discriminative features beyond the current document alone. In terms of extra resources, the Wikipedia based model (WS) by Han and Zhao (2009) is close to our mo</context>
<context position="28398" citStr="Han and Zhao, 2010" startWordPosition="4703" endWordPosition="4706"> with four baseline methods: (1)VSM: traditional vector space model with cosine similarity. We use features extracted in Section 3.1 and weight them using TFIDF. The documents are grouped using standard HAC algorithm. (2)GRAPE(Jiang et al., 2009): we re-implement the state-of-the-art system which outperforms any models that do not use extra knowledge resources reported in WePS1 and WePS2. (3)WS: the Wikipedia sim(o1 n t, o2 n t) x 838 Semantic method(Han and Zhao, 2009). This system uses Wikipedia to enhance the results of name disambiguation. (4)SSR: the Structural Semantic relatedness model(Han and Zhao, 2010) creates a semantic graph to re-calculate the semantic relatedness between features, and captures both explicit semantic relations and implicit structural semantic knowledge. We also build two variants of TM: TMnTW which removes topic weighting to examine what effect the topic weighting strategy can make and whether it can provide a person specific evidence and TM-nCP which does not use co-occurring information to prune the semantic graph to examine whether the pruning is effective. 4.3 Parameters There are several parameters to be tuned in our model. In the SCAN algorithm, we use default para</context>
</contexts>
<marker>Han, Zhao, 2010</marker>
<rawString>Han, X. and Zhao, J. (2010). Structural semantic relatedness: a knowledge-based method to named entity disambiguation. In ACL, ACL ’10, pages 50–59, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ikeda</author>
<author>S Ono</author>
<author>I Sato</author>
<author>M Yoshida</author>
<author>H Nakagawa</author>
</authors>
<title>Person name disambiguation on the web by two-stage clustering.</title>
<date>2009</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="3400" citStr="Ikeda et al., 2009" startWordPosition="538" endWordPosition="541">es, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Javier et al., 2009). Th</context>
<context position="4820" citStr="Ikeda et al., 2009" startWordPosition="758" endWordPosition="761">nd job. In this case, current efforts, focusing on either feature engineering or background knowledge, are incapable to exploiting these limited clues from the current page to the whole Emily Bender document set, where nutrition, as an important feature for recognizing a nutritionist, should be paid more attention. As far as we know, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the significance of nutrit</context>
<context position="8116" citStr="Ikeda et al., 2009" startWordPosition="1278" endWordPosition="1281">atures according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity proble</context>
</contexts>
<marker>Ikeda, Ono, Sato, Yoshida, Nakagawa, 2009</marker>
<rawString>Ikeda, M., Ono, S., Sato, I., Yoshida, M., and Nakagawa, H. (2009). Person name disambiguation on the web by two-stage clustering. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Iria</author>
<author>L Xia</author>
<author>Z Zhang</author>
</authors>
<title>Wit: web people search disambiguation using random walks.</title>
<date>2007</date>
<journal>In SemEval, SemEval</journal>
<volume>07</volume>
<pages>480--483</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8278" citStr="Iria et al., 2007" startWordPosition="1303" endWordPosition="1306"> extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain co</context>
</contexts>
<marker>Iria, Xia, Zhang, 2007</marker>
<rawString>Iria, J., Xia, L., and Zhang, Z. (2007). Wit: web people search disambiguation using random walks. In SemEval, SemEval ’07, pages 480–483, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Javier</author>
<author>G Julio</author>
<author>S Satoshi</author>
</authors>
<title>Weps 2 evaluation campaign: Overview of the web people search clustering task.</title>
<date>2009</date>
<booktitle>In WWW</booktitle>
<contexts>
<context position="3996" citStr="Javier et al., 2009" startWordPosition="627" endWordPosition="630">008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Javier et al., 2009). Throughout the whole page we can find only one word, nutrition, related to her identification, while other pages about the nutritionist in the dataset contain substantial materials about her profession and job. In this case, current efforts, focusing on either feature engineering or background knowledge, are incapable to exploiting these limited clues from the current page to the whole Emily Bender document set, where nutrition, as an important feature for recognizing a nutritionist, should be paid more attention. As far as we know, there is less work focusing on exploring person specific in</context>
<context position="27442" citStr="Javier et al., 2009" startWordPosition="4548" endWordPosition="4551">ments We compare our model with competitive baselines on three WePS datasets. In the following, we first describe the experimental setup, and then discuss the their performances. 4.1 Data Wikipedia Data Wikipedia offers free copies of all available data to interested users in their website. We used the one released in March 6th, 2009 in our experiments. We identified over 4,000,000 highly connected concepts in this dump; each concept links to 10 other concepts in average. WePS Datasets We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al., 2007), WePS2 Testing (Javier et al., 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. Each name were queried in Yahoo! Search and top N result pages (100 pages in WePS1 and 150 pages in WePS2) were obtained and manually labeled. 4.2 Baselines We compare our model TM with four baseline methods: (1)VSM: traditional vector space model with cosine similarity. We use features extracted in Section 3.1 and weight them using TFIDF. The documents are grouped using standard HAC algorithm. (2)GRAPE(Jiang et al., 2009): we re-implement</context>
</contexts>
<marker>Javier, Julio, Satoshi, 2009</marker>
<rawString>Javier, A., Julio, G., and Satoshi, S. (2009). Weps 2 evaluation campaign: Overview of the web people search clustering task. In WWW 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Jiang</author>
<author>J Wang</author>
<author>N An</author>
<author>S Wang</author>
<author>J Zhan</author>
<author>L Li</author>
</authors>
<title>Grape: A graph-based framework for disambiguating people appearances in web search.</title>
<date>2009</date>
<booktitle>In ICDM, ICDM ’09,</booktitle>
<pages>199--208</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3421" citStr="Jiang et al., 2009" startWordPosition="542" endWordPosition="545">g these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Javier et al., 2009). Throughout the whole pa</context>
<context position="5103" citStr="Jiang et al., 2009" startWordPosition="798" endWordPosition="801">, should be paid more attention. As far as we know, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the significance of nutrition for Emily Bender in WePS1 dataset. In this paper, we explore different usages of features and propose an approach which mines cross document information to capture the person specific importance for a feature. Specifically, we construct a semantic graph from Wikipedia concepts a</context>
<context position="8326" citStr="Jiang et al., 2009" startWordPosition="1311" endWordPosition="1314"> be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain cooccurrences between named entities. Rao et al. u</context>
<context position="22232" citStr="Jiang et al., 2009" startWordPosition="3635" endWordPosition="3638">) = f(c)sim(c, t). And n coh(c, t) is the neighborhood coherence of concept c with topic t, defined as: sr(q, c) (4) jN(c) n tj where N(c) is the neighboring node set of concept c. By incorporating corpus level concept frequencies into topic weighting, discriminative concepts that are sparse in one document and suppressed by conventional models can benefit from their corpus level importance as well as their coherence in related topics. 3.3 Clustering Person Name Observations Now the remaining key step is to compute the similarity between two name observations. The similarity proposed in GRAPE(Jiang et al., 2009) measures two documents by bridge tags (common features) shared by two document graphs. Specifically, Jiang et al. utilize cohesion to weight a bridge tag in a document. The more bridge tags two documents share, the stronger the cohesion of each bridge tag is, and in turn the more similar the two documents are. However, this similarity bears a shortcoming that the bridge tags shared by the two documents require an exact match of features, which does not take any semantic relatedness into consideration. If two web pages mentioning the same person but have few features in common, the GRAPE simil</context>
<context position="25189" citStr="Jiang et al., 2009" startWordPosition="4167" endWordPosition="4170">ese topics receive higher weights according to the current name observation set, then the two observations should be more related to each other. Specifically, the factor sim(o1 n t, o2 n t) is designed to measure the fine relatedness between o1 and o2 given the topic t. Sometimes, both o1 and o2 are mapped to t and both close to this topic, but in fact they depict different aspects of t since some of our topics are more general thus include several aspects. The comparison of their intersections will provide a more detailed view for their similarity. Inspired by the use of bridge tags in GRAPE(Jiang et al., 2009), we propose to capture the connection strength between concept sets by the means of our topics. We consider common topics as the bridge tags and define our topic based connectivity strength between two name observations as: 1 � T C5(o1, o2) = 2 tET(o1,o2) (Cohs(o1, t) + Cohs(o2, t)) (7) Note that we still need sim(o1 n t, o2 n t) to capture the fine differences inside a topic. Cohs(o, t) is a cohesion measure to capture the relatedness between non-concept features in o and concept features in t, defined as: Cohs(o, t) = � w(t)x � occ(c, q)fo(c)fo(q) cEont qEEB(o) (8) where EB(o) contains all </context>
<context position="28025" citStr="Jiang et al., 2009" startWordPosition="4642" endWordPosition="4645"> WePS2 Testing (Javier et al., 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. Each name were queried in Yahoo! Search and top N result pages (100 pages in WePS1 and 150 pages in WePS2) were obtained and manually labeled. 4.2 Baselines We compare our model TM with four baseline methods: (1)VSM: traditional vector space model with cosine similarity. We use features extracted in Section 3.1 and weight them using TFIDF. The documents are grouped using standard HAC algorithm. (2)GRAPE(Jiang et al., 2009): we re-implement the state-of-the-art system which outperforms any models that do not use extra knowledge resources reported in WePS1 and WePS2. (3)WS: the Wikipedia sim(o1 n t, o2 n t) x 838 Semantic method(Han and Zhao, 2009). This system uses Wikipedia to enhance the results of name disambiguation. (4)SSR: the Structural Semantic relatedness model(Han and Zhao, 2010) creates a semantic graph to re-calculate the semantic relatedness between features, and captures both explicit semantic relations and implicit structural semantic knowledge. We also build two variants of TM: TMnTW which remove</context>
</contexts>
<marker>Jiang, Wang, An, Wang, Zhan, Li, 2009</marker>
<rawString>Jiang, L., Wang, J., An, N., Wang, S., Zhan, J., and Li, L. (2009). Grape: A graph-based framework for disambiguating people appearances in web search. In ICDM, ICDM ’09, pages 199–208, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Kalashnikov</author>
<author>Z Chen</author>
<author>S Mehrotra</author>
<author>R Nuray-Turan</author>
</authors>
<title>Web people search via connection analysis.</title>
<date>2008</date>
<journal>IEEE Trans. on Knowl. and Data Eng.,</journal>
<pages>20--1550</pages>
<contexts>
<context position="3379" citStr="Kalashnikov et al., 2008" startWordPosition="534" endWordPosition="537">s features from the web pages, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Jav</context>
<context position="5129" citStr="Kalashnikov et al., 2008" startWordPosition="802" endWordPosition="805">e attention. As far as we know, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the significance of nutrition for Emily Bender in WePS1 dataset. In this paper, we explore different usages of features and propose an approach which mines cross document information to capture the person specific importance for a feature. Specifically, we construct a semantic graph from Wikipedia concepts appearing in all documents </context>
<context position="8304" citStr="Kalashnikov et al., 2008" startWordPosition="1307" endWordPosition="1310">sources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain cooccurrences between named </context>
</contexts>
<marker>Kalashnikov, Chen, Mehrotra, Nuray-Turan, 2008</marker>
<rawString>Kalashnikov, D. V., Chen, Z., Mehrotra, S., and Nuray-Turan, R. (2008a). Web people search via connection analysis. IEEE Trans. on Knowl. and Data Eng., 20:1550–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Kalashnikov</author>
<author>R Nuray-Turan</author>
<author>S Mehrotra</author>
</authors>
<title>Towards breaking the quality curse.: a web-querying approach to web people search.</title>
<date>2008</date>
<booktitle>In SIGIR, SIGIR ’08,</booktitle>
<pages>27--34</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3379" citStr="Kalashnikov et al., 2008" startWordPosition="534" endWordPosition="537">s features from the web pages, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily Bender in WePS2 dataset(Jav</context>
<context position="5129" citStr="Kalashnikov et al., 2008" startWordPosition="802" endWordPosition="805">e attention. As far as we know, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the significance of nutrition for Emily Bender in WePS1 dataset. In this paper, we explore different usages of features and propose an approach which mines cross document information to capture the person specific importance for a feature. Specifically, we construct a semantic graph from Wikipedia concepts appearing in all documents </context>
<context position="8304" citStr="Kalashnikov et al., 2008" startWordPosition="1307" endWordPosition="1310">sources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich background knowledge, which is crucial to remedy the data sparsity problem. The employed resources include raw texts available on the web and online encyclopedias. Kalashnikov et al. and Yiming et al. use extra web corpora to obtain cooccurrences between named </context>
</contexts>
<marker>Kalashnikov, Nuray-Turan, Mehrotra, 2008</marker>
<rawString>Kalashnikov, D. V., Nuray-Turan, R., and Mehrotra, S. (2008b). Towards breaking the quality curse.: a web-querying approach to web people search. In SIGIR, SIGIR ’08, pages 27–34, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In CONLL, CONLL ’03,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3353" citStr="Mann and Yarowsky, 2003" startWordPosition="530" endWordPosition="533">s first extracting various features from the web pages, and then grouping these pages into several clusters each of which is assumed to represent one specific person. Despite of the inevitably noisy nature of web data, a key challenge is how to handle the data sparsity problem which we mean as: mismatch of vocabulary and lack of clues. The former refers to the case that two web pages may describe the same person but use different words thus the word overlap between them are small. Various features, including entities, biographical information, URL, etc., have been introduced to bridge the gap(Mann and Yarowsky, 2003; Kalashnikov et al., 2008a; Ikeda et al., 2009; Jiang et al., 2009), 832 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 832–842, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and external knowledge resources are also employed to capture the semantic relationship between entities(Han and Zhao, 2009, 2010). However, a more challenging scenario is that there are few clues available in the web pages. For example, there is a page mentioning a nutritionist Emily B</context>
<context position="4799" citStr="Mann and Yarowsky, 2003" startWordPosition="754" endWordPosition="757">ls about her profession and job. In this case, current efforts, focusing on either feature engineering or background knowledge, are incapable to exploiting these limited clues from the current page to the whole Emily Bender document set, where nutrition, as an important feature for recognizing a nutritionist, should be paid more attention. As far as we know, there is less work focusing on exploring person specific information to relieve the lack of clues problem. Traditional vector space model (VSM) is most widely used to accommodate various features, but it ignores any relations between them(Mann and Yarowsky, 2003; Ikeda et al., 2009). Beyond bag-of-features, two kinds of features are explored, co-occurrences of entities and Wikipedia based semantic relationship between entities, both of which provide a reasonable relatedness for entity pairs. More recent works adopt one of these relationships(Jiang et al., 2009; Kalashnikov et al., 2008a; Han and Zhao, 2009). Han and Zhao try to model both aspects, but their co-occurrence estimation, estimated from held-out resources, fails to capture the person specific importance for a feature, which is crucial to enhance limited clues in a corpus level, e.g., the s</context>
<context position="8027" citStr="Mann and Yarowsky, 2003" startWordPosition="1264" endWordPosition="1267">n clustering. Most research efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to be</context>
<context position="26812" citStr="Mann and Yarowsky, 2003" startWordPosition="4448" endWordPosition="4451">ird part is the original connectivity strength defined in GRAPE(Jiang et al., 2009): C5(o1, o2), calculated using plain features without topics (we omit the details for brevity). Finally, we linearly combine equation (6), (7) and C5(o1, o2) into the person name similarity function as: 5(o1,o2)= α1 x T5m(o1,o2) + α2 x TC5(o1,o2) +(1 − α1 − α2) x C5(o1, o2) (9) where α1 and α2 are optimized during training. This final similarity function will then be embedded into a normal HAC algorithm to group the web pages into different namesakes where we compute the centroid-based distance between clusters(Mann and Yarowsky, 2003). 4 Experiments We compare our model with competitive baselines on three WePS datasets. In the following, we first describe the experimental setup, and then discuss the their performances. 4.1 Data Wikipedia Data Wikipedia offers free copies of all available data to interested users in their website. We used the one released in March 6th, 2009 in our experiments. We identified over 4,000,000 highly connected concepts in this dump; each concept links to 10 other concepts in average. WePS Datasets We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al., 2007), WePS2</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Mann, G. S. and Yarowsky, D. (2003). Unsupervised personal name disambiguation. In CONLL, CONLL ’03, pages 33–40, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM’07,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="12530" citStr="Mihalcea and Csomai, 2007" startWordPosition="1976" endWordPosition="1980">ept Extraction Each concept in Wikipedia is described by an article containing hyperlinks to other concepts which are supposed to related to the current one. All the linking relations in Wikipedia construct a huge semantic graph, where we can mine rich semantic relationship between concepts(David and Ian, 2008). We collect Wikipedia concepts from all web pages in the dataset by comparing all n-grams (up to 8) from the dataset to Wikipedia anchor text dictionary and checking whether it is a Wikipedia concept surface form. We further prune the extracted concepts according to their keyphraseness(Mihalcea and Csomai, 2007). Initially, each concept is weighted according to its average semantic relateness(David and Ian, 2008) with other concepts in the current page. Named Entity and Biographical Information Extraction Although Wikipedia concepts can pro834 vide rich background knowledge, they suffer from the limited coverage. It is common that some discriminative features are not likely to be found in Wikipedia, such as names of infamous people or organizations, email addresses, phone numbers, etc. We therefore extract two extra kinds of features, named entities that do not appear in the Wikipedia anchor text dic</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Mihalcea, R. and Csomai, A. (2007). Wikify!: linking documents to encyclopedic knowledge. In Proceedings of CIKM’07, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Niu</author>
<author>W Li</author>
<author>R K Srihari</author>
</authors>
<title>Weakly supervised learning for cross-document person name disambiguation supported by information extraction.</title>
<date>2004</date>
<booktitle>In ACL, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8046" citStr="Niu et al., 2004" startWordPosition="1268" endWordPosition="1271">ch efforts so far have been made to the former, exploring various features according to specific applications, while the second step is currently dominated by hierarchical agglomerative clustering (HAC). According to the reliance of extra knowledge resources, existing works can be categorized into non-resource methods and resource-based methods. Non-resource methods extract various local features from the context of ambiguous names, and compute the similarity between feature vectors. These features include plain words(Bagga and Baldwin, 1998), biographical information(Mann and Yarowsky, 2003; Niu et al., 2004), named entities, compound key phrases, hyperlinks(Ikeda et al., 2009), etc. The similarity between namesakes are usually measured by the cosine similarity(Bagga and Baldwin, 1998), or other graph based met833 rics(Iria et al., 2007; Kalashnikov et al., 2008a; Jiang et al., 2009). Those methods pay more attention to extracting informative features and their co-occurrences, but they usually treat the features locally, and ignore the semantic relatedness of features beyond the current document. Resource-based approaches, on the other hand, can leverage external resources to benefit from rich bac</context>
</contexts>
<marker>Niu, Li, Srihari, 2004</marker>
<rawString>Niu, C., Li, W., and Srihari, R. K. (2004). Weakly supervised learning for cross-document person name disambiguation supported by information extraction. In ACL, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xu</author>
<author>N Yuruk</author>
<author>Z Feng</author>
<author>T A J Schweiger</author>
</authors>
<title>Scan: a structural clustering algorithm for networks.</title>
<date>2007</date>
<booktitle>In Proceedings of KDD, KDD ’07,</booktitle>
<pages>824--833</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16508" citStr="Xu et al. (2007)" startWordPosition="2627" endWordPosition="2630">ted in this name observation set. Figure 1 illustrates an abridged version of a semantic graph for George Foster. 0.3862 0.4008 0.3205 0.3296 Stolen Base 0.4228 0.4145 Home Run 0.3799 Shortstop 0.3467 0.2976 Major League Baseball 0.3628 Cincinnati Reds 0.2445 0.2697 Tackle 0.3201 Sports League 0.2738 0.3567 Cornerback National Football League Pro 0. 3136 0.2245 Football Weekly 835 Graph Clustering Considering the graph construction strategy we use, it is more suitable for us to group the concepts on the graph into several topics using a density-based clustering model. We choose SCAN algorithm Xu et al. (2007) to perform the clustering step. The SCAN algorithm utilizes a neighborhood structure to measure the similarity between two vertices. If a vertex has a minimal of µ neighbors with a similarity larger than ε, it is called a core. The algorithm1 starts from a random vertex in a graph, examining whether it is a core or not. If yes, the algorithm will expand a cluster from this vertex recursively, otherwise the vertex will be assigned either a hub node or an outlier depending on the number of its neighboring clusters. A hub node connects to more than one cluster, while an outlier connects to one o</context>
<context position="18258" citStr="Xu et al. (2007)" startWordPosition="2945" endWordPosition="2948">n two nodes as follows: simnb(c1, c2) sr(c1, c2) sim(c1, c2) = α � + (1) 1 + α 1 + α and simnb(c1, c2) is defined as: 3r(c1,c)+3r(c2,c) 2 simnb(c1, c2) = |N(c1) U N(c2)| where N(c) is the neighbor set of concept c. This new similarity function contains two parts: the neighborhood similarity and the semantic relatedness between two concepts. We combine them using a linear combination, where α is a weight tuned during training. Topic Generation Now we will map the clustering results into different topics. Intuitively, each 1We omit the details of SCAN for brevity, and refer interested reader to Xu et al. (2007) for more details. cluster will be treated as a topic. However, we found that hub nodes usually correspond to general concepts which may be related to many topics, but with a loose relatedness. We thus distribute each general concept into its every related topic, but with a lower weight to distinguish from ordinary concepts in this topic. Outliers may be concepts which are far away from main themes of the corpus, or noise concepts. We calculate the average semantic relatedness of an outlier with its neighbor concepts that belong to one topic. If the result is lower than a threshold, this outli</context>
<context position="29035" citStr="Xu et al., 2007" startWordPosition="4807" endWordPosition="4810">raph to re-calculate the semantic relatedness between features, and captures both explicit semantic relations and implicit structural semantic knowledge. We also build two variants of TM: TMnTW which removes topic weighting to examine what effect the topic weighting strategy can make and whether it can provide a person specific evidence and TM-nCP which does not use co-occurring information to prune the semantic graph to examine whether the pruning is effective. 4.3 Parameters There are several parameters to be tuned in our model. In the SCAN algorithm, we use default parameters according to (Xu et al., 2007) with an exception: the weight α is tuned exhaustively to be 0.2. Note that the number of topics are automatically decided by SCAN. The semantic graph pruning threshold is set to 0.27 tuned on a held out set. The smoothing parameters in equation (9) are: α1 = 0.3, α2 = 0.2 which are tuned using cross validation. Optimization of some parameters will be addressed in detail in the following subsection. In HAC, all optimal merging thresholds are selected by applying leave-one-out cross validation. 4.4 Results and Discussion We adopt the same evaluation process as (Han and Zhao, 2009), and evaluati</context>
</contexts>
<marker>Xu, Yuruk, Feng, Schweiger, 2007</marker>
<rawString>Xu, X., Yuruk, N., Feng, Z., and Schweiger, T. A. J. (2007). Scan: a structural clustering algorithm for networks. In Proceedings of KDD, KDD ’07, pages 824–833, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Yiming</author>
<author>N Zaiqing</author>
<author>C Taoyuan</author>
<author>G Ying</author>
<author>W Ji-Rong</author>
</authors>
<title>Name disambiguation using web connection.</title>
<date>2007</date>
<booktitle>In AAAI.</booktitle>
<marker>Yiming, Zaiqing, Taoyuan, Ying, Ji-Rong, 2007</marker>
<rawString>Yiming, L., Zaiqing, N., Taoyuan, C., Ying, G., and Ji-Rong, W. (2007). Name disambiguation using web connection. In AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000135">
<title confidence="0.994929">
Inducing a Discriminative Parser to Optimize Machine
Translation Reordering
</title>
<author confidence="0.999815">
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1
</author>
<affiliation confidence="0.965548333333333">
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
</affiliation>
<address confidence="0.795267">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<sectionHeader confidence="0.945673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999329">
This paper proposes a method for learning
a discriminative parser for machine trans-
lation reordering using only aligned par-
allel text. This is done by treating the
parser’s derivation tree as a latent variable
in a model that is trained to maximize re-
ordering accuracy. We demonstrate that
efficient large-margin training is possible
by showing that two measures of reorder-
ing accuracy can be factored over the parse
tree. Using this model in the pre-ordering
framework results in significant gains in
translation accuracy over standard phrase-
based SMT and previously proposed unsu-
pervised syntax induction methods.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998891638297873">
Finding the appropriate word ordering in the
target language is one of the most difficult prob-
lems for statistical machine translation (SMT),
particularly for language pairs with widely di-
vergent syntax. As a result, there is a large
amount of previous research that handles the
problem of reordering through the use of im-
proved reordering models for phrase-based SMT
(Koehn et al., 2005), hierarchical phrase-based
translation (Chiang, 2007), syntax-based trans-
lation (Yamada and Knight, 2001), or pre-
ordering (Xia and McCord, 2004).
In particular, systems that use source-
language syntax allow for the handling of long-
distance reordering without large increases in
The first author is now affiliated with the Nara Institute
of Science and Technology.
decoding time. However, these require a good
syntactic parser, which is not available for many
languages. In recent work, DeNero and Uszko-
reit (2011) suggest that unsupervised grammar
induction can be used to create source-sentence
parse structure for use in translation as a part
of a pre-ordering based translation system.
In this work, we present a method for inducing
a parser for SMT by training a discriminative
model to maximize reordering accuracy while
treating the parse tree as a latent variable. As a
learning framework, we use online large-margin
methods to train the model to directly minimize
two measures of reordering accuracy. We pro-
pose a variety of features, and demonstrate that
learning can succeed when no linguistic informa-
tion (POS tags or parse structure) is available in
the source language, but also show that this lin-
guistic information can be simply incorporated
when it is available. Experiments find that the
proposed model improves both reordering and
translation accuracy, leading to average gains
of 1.2 BLEU points on English-Japanese and
Japanese-English translation without linguistic
analysis tools, or up to 1.5 BLEU points when
these tools are incorporated. In addition, we
show that our model is able to effectively max-
imize various measures of reordering accuracy,
and that the reordering measure that we choose
has a direct effect on translation results.
</bodyText>
<sectionHeader confidence="0.987122" genericHeader="introduction">
2 Preordering for SMT
</sectionHeader>
<bodyText confidence="0.997010666666667">
Machine translation is defined as transforma-
tion of source sentence F = f1 ... fJ to target
sentence E = e1 ... eI. In this paper, we take
</bodyText>
<page confidence="0.985011">
843
</page>
<note confidence="0.991548">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 843–853, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.94856125">
Figure 1: An example with a source sentence F re-
ordered into target order F&apos;, and its corresponding
target sentence E. D is one of the BTG derivations
that can produce this ordering.
</figureCaption>
<bodyText confidence="0.999951423076924">
the pre-ordering approach to machine transla-
tion (Xia and McCord, 2004), which performs
translation as a two step process of reordering
and translation (Figure 1). Reordering first de-
terministically transforms F into F&apos;, which con-
tains the same words as F but is in the order of
E. Translation then transforms F&apos; into E using
a method such as phrase-based SMT (Koehn et
al., 2003), which can produce accurate transla-
tions when only local reordering is required.
This general framework has been widely stud-
ied, with the majority of works relying on a
syntactic parser being available in the source
language. Reordering rules are defined over
this parse either through machine learning tech-
niques (Xia and McCord, 2004; Zhang et al.,
2007; Li et al., 2007; Genzel, 2010; Dyer and
Resnik, 2010; Khalilov and Sima’an, 2011) or
linguistically motivated manual rules (Collins et
al., 2005; Xu et al., 2009; Carpuat et al., 2010;
Isozaki et al., 2010b). However, as building a
parser for each source language is a resource-
intensive undertaking, there has also been some
interest in developing reordering rules without
the use of a parser (Rottmann and Vogel, 2007;
Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al., 2011), and we
will follow this thread of research in this paper.
In particular, two methods deserve mention
for being similar to our approach. First, DeNero
and Uszkoreit (2011) learn a reordering model
through a three-step process of bilingual gram-
mar induction, training a monolingual parser
to reproduce the induced trees, and training
a reordering model that selects a reordering
based on this parse structure. In contrast, our
method trains the model in a single step, treat-
ing the parse structure as a latent variable in
a discriminative reordering model. In addition
Tromble and Eisner (2009) and Visweswariah et
al. (2011) present models that use binary clas-
sification to decide whether each pair of words
should be placed in forward or reverse order. In
contrast, our method uses traditional context-
free-grammar models, which allows for simple
parsing and flexible parameterization, including
features such as those that utilize the existence
of a span in the phrase table. Our work is also
unique in that we show that it is possible to di-
rectly optimize several measures of reordering
accuracy, which proves important for achieving
good translations.1
</bodyText>
<sectionHeader confidence="0.6972685" genericHeader="method">
3 Training a Reordering Model with
Latent Derivations
</sectionHeader>
<bodyText confidence="0.99978475">
In this section, we provide a basic overview of
the proposed method for learning a reordering
model with latent derivations using online dis-
criminative learning.
</bodyText>
<subsectionHeader confidence="0.999532">
3.1 Space of Reorderings
</subsectionHeader>
<bodyText confidence="0.999942214285714">
The model we present here is based on the
bracketing transduction grammar (BTG, Wu
(1997)) framework. BTGs represent a binary
tree derivation D over the source sentence F
as shown in Figure 1. Each non-terminal node
can either be a straight (sTR) or inverted (INV)
production, and terminals (TERM) span a non-
empty substring f.2
The ordering of the sentence is determined by
the tree structure and the non-terminal labels
sTR and INV, and can be built bottom-up. Each
subtree represents a source substring f and its
reordered counterpart f&apos;. For each terminal
node, no reordering occurs and f is equal to f&apos;.
</bodyText>
<footnote confidence="0.999236857142857">
1The semi-supervised method of Katz-Brown et al.
(2011) also optimizes reordering accuracy, but requires
manually annotated parses as seed data.
2In the original BTG framework used in translation,
terminals produce a bilingual substring pair f/e, but as
we are only interested in reordering the source F, we
simplify the model by removing the target substring e.
</footnote>
<page confidence="0.998279">
844
</page>
<bodyText confidence="0.9999101">
For each non-terminal node spanning f with its
left child spanning f1 and its right child span-
ning f2, if the non-terminal symbol is STR., the
reordered strings will be concatenated in order
as f0 = f01f02, and if the non-terminal symbol is
INV, the reordered strings will be concatenated
in inverted order as f0 = f02f01.
We define the space of all reorderings that can
be produced by the BTG as P, and attempt to
find the best reordering F�0 within this space.3
</bodyText>
<subsectionHeader confidence="0.98737">
3.2 Reorderings with Latent
Derivations
</subsectionHeader>
<bodyText confidence="0.9999805">
In order to find the best reordering F�0 given only
the information in the source side sentence F, we
define a scoring function 5(F0|F), and choose
the ordering of maximal score:
</bodyText>
<equation confidence="0.984882">
F�0 = arg max 5(F0|F).
F&apos;
</equation>
<bodyText confidence="0.999932666666667">
As our model is based on reorderings licensed
by BTG derivations, we also assume that there
is an underlying derivation D that produced F0.
As we can uniquely determine F0 given F and
D, we can define a scoring function 5(D|F) over
derivations, find the derivation of maximal score
</bodyText>
<equation confidence="0.994366">
D = arg max 5(D|F)
D
</equation>
<bodyText confidence="0.999978">
and use D to transform F into F0.
Furthermore, we assume that the score
5(D|F) is the weighted sum of a number of fea-
ture functions defined over D and F
</bodyText>
<equation confidence="0.9945625">
5(D|F, w) = ∑ wiOi(D, F)
i
</equation>
<bodyText confidence="0.999819571428571">
where Oi is the ith feature function, and wi is
its corresponding weight in weight vector w.
Given this model, we must next consider how
to learn the weights w. As the final goal of our
model is to produce good reorderings F0, it is
natural to attempt to learn weights that will al-
low us to produce these high-quality reorderings.
</bodyText>
<footnote confidence="0.992484333333333">
3BTGs cannot reproduce all possible reorderings, but
can handle most reorderings occurring in natural trans-
lated text (Haghighi et al., 2009).
</footnote>
<figureCaption confidence="0.996691666666667">
Figure 2: An example of (a) the ranking function
r(fj), (b) loss according to Kendall’s T, (c) loss ac-
cording to chunk fragmentation.
</figureCaption>
<sectionHeader confidence="0.988621" genericHeader="method">
4 Evaluating Reorderings
</sectionHeader>
<bodyText confidence="0.999990833333333">
Before we explain the learning algorithm, we
must know how to distinguish whether the F0
produced by the model is good or bad. This
section explains how to calculate oracle reorder-
ings, and assign each F0 a loss and an accuracy
according to how well it reproduces the oracle.
</bodyText>
<subsectionHeader confidence="0.996979">
4.1 Calculating Oracle Orderings
</subsectionHeader>
<bodyText confidence="0.987413869565218">
In order to calculate reordering quality, we first
define a ranking function r(fj|F, A), which indi-
cates the relative position of source word fj in
the proper target order (Figure 2 (a)). In or-
der to calculate this ranking function, we define
A = a1, ... , aJ, where each aj is a set of the in-
dices of the words in E to which fj is aligned.4
Given these alignments, we define an ordering
function aj1 &lt; aj2 that indicates that the in-
dices in aj1 come before the indices in aj2. For-
mally, we define this function as “the first index
in aj1 is at most the first index in aj2, similarly
for the last index, and either the first or last
index in aj1 is less than that of aj2.”
Given this ordering, we can sort every align-
ment aj, and use its relative position in the sen-
tence to assign a rank to its word r(fj). In
4Null alignments require special treatment. To do so,
we can place unaligned brackets and quotes directly be-
fore and after the spans they surround, and attach all
other unaligned words to the word directly to the right
for head-initial languages (e.g. English), or left for head-
final languages (e.g. Japanese).
</bodyText>
<page confidence="0.990102">
845
</page>
<bodyText confidence="0.999954222222222">
the case of ties, where neither aj1 &lt; aj2 nor
aj2 &lt; aj1, both fj1 and fj2 are assigned the
same rank. We can now define measures of re-
ordering accuracy for F&apos; by how well it arranges
the words in order of ascending rank. It should
be noted that as we allow ties in rank, there
are multiple possible F&apos; where all words are in
strictly ascending order, which we will call ora-
cle orderings.
</bodyText>
<subsectionHeader confidence="0.988216">
4.2 Kendall’s τ
</subsectionHeader>
<bodyText confidence="0.999985388888889">
The first measure of reordering accuracy that
we will consider is Kendall’s τ (Kendall, 1938),
a measure of pairwise rank correlation which
has been proposed for evaluating translation re-
ordering accuracy (Isozaki et al., 2010a; Birch
et al., 2010) and pre-ordering accuracy (Talbot
et al., 2011). The fundamental idea behind the
measure lies in comparisons between each pair of
elements f&apos;j1 and f&apos;j2 of the reordered sentence,
where j1 &lt; j2. Because j1 &lt; j2, f&apos;j1 comes before
f&apos;j2 in the reordered sentence, the ranks should
be r(f&apos;j1) &lt; r(f&apos;j2) in order to produce the cor-
rect ordering.
Based on this criterion, we first define a loss
Lt(F&apos;) that will be higher for orderings that are
further from the oracle. Specifically, we take the
sum of all pairwise orderings that do not follow
the expected order
</bodyText>
<equation confidence="0.911229">
δ(r(f&apos;j1) &gt; r(f&apos; j2))
</equation>
<bodyText confidence="0.999985285714286">
where δ(·) is an indicator function that is 1 when
its condition is true, and 0 otherwise. An exam-
ple of this is given in Figure 2 (b).
To calculate an accuracy measure for ordering
F&apos;, we first calculate the maximum loss for the
sentence, which is equal to the total number of
non-equal rank comparisons in the sentences
</bodyText>
<equation confidence="0.972766">
δ(r(f&apos; j1) =� r(f&apos;j2)).
(1)
</equation>
<footnote confidence="0.713459333333333">
5The traditional formulation of Kendall’s T assumes
no ties in rank, and thus the maximum loss can be cal-
culated as J(J − 1)/2.
</footnote>
<construct confidence="0.30082">
max Lt( F�&apos;),
</construct>
<bodyText confidence="0.9066828">
F˜&apos;
which will take a value between 0 (when F&apos; has
maximal loss), and 1 (when F&apos; matches one of
the oracle orderings). In Figure 2 (b), Lt(F&apos;) =
Lt( F�&apos;) = 8, so At(F&apos;) = 0.75.
</bodyText>
<subsectionHeader confidence="0.986695">
4.3 Chunk Fragmentation
</subsectionHeader>
<bodyText confidence="0.9999842">
Another measure that has been used in eval-
uation of translation accuracy (Banerjee and
Lavie, 2005) and pre-ordering accuracy (Talbot
et al., 2011) is chunk fragmentation. This mea-
sure is based on the number of chunks that the
sentence needs to be broken into to reproduce
the correct ordering, with a motivation that the
number of continuous chunks is equal to the
number of times the reader will have to jump to
a different position in the reordered sentence to
read it in the target order. One way to measure
the number of continuous chunks is considering
whether each word pair f&apos;j and f&apos;j+1 is discon-
tinuous (the rank of f&apos;j+1 is not equal to or one
greater than f&apos;)
</bodyText>
<equation confidence="0.970699666666667">
j
DISCONT(f&apos;j, f&apos;j+1) =
δ(r(f&apos;j) =� r(f&apos;j+1) &apos; r(f&apos;j) + 1 =� r(f&apos;j+1))
</equation>
<bodyText confidence="0.9929115">
and sum over all word pairs in the sentence to
create a sentence-based loss
</bodyText>
<equation confidence="0.9941235">
L,(F&apos;) = J−1∑ DISCONT(f&apos;j, f&apos;j+1) (2)
j=1
</equation>
<bodyText confidence="0.985765083333333">
While this is the formulation taken by previ-
ous work, we found that this under-penalizes
bad reorderings of the first and last words of
the sentence, which can contribute to the loss
only once, as opposed to other words which can
contribute to the loss twice. To account for
this, when calculating the chunk fragmentation
score, we additionally add two sentence bound-
ary words f0 and fJ+1 with ranks r(f0) = 0 and
r(f&apos;j) and redefine the sum-
mation in Equation (2) to consider these words
(e.g. Figure 2 (c)).
</bodyText>
<figure confidence="0.702834">
Lt(F&apos;) = J−1∑ J
j1=1 ∑
j2=j1+1
max Lt(F&apos;) = J−1∑ J
F&apos; j1=1 ∑
j2=j1+1
</figure>
<bodyText confidence="0.995145">
Finally, we use this maximum loss to normalize
the actual loss to get an accuracy
</bodyText>
<equation confidence="0.9573335">
Lt(F &apos;)
At(F &apos;) = 1
2 and max
F˜&apos;
r(fJ+1) = 1 + max
f&apos;iEF&apos;
</equation>
<page confidence="0.970688">
846
</page>
<figure confidence="0.976202214285714">
procedure WEIGHTUPDATE(F, A, w)
D ← parse(F, w) . Create parse forest
�
D ← argmax S(D|F, w) + L(D|F, A)
DED
. Find the model parse
D� ← argmin L(D|F, A) − αS(D|F, w)
DED
. Find the oracle parse
if L( D|F, A) =6 L(�D|F, A) then
w ← β(w + γ(φ( D, F) − φ(�D, F)))
. Perform weight update
end if
end procedure
</figure>
<figureCaption confidence="0.962175">
Figure 3: An online update for sentence F, alignment
A, and weight vector w. α is a very small constant,
and β and γ are defined by the update strategy.
</figureCaption>
<bodyText confidence="0.983512142857143">
Similarly to Kendall’s τ, we can also define
an accuracy measure between 0 and 1 using the
maximum loss, which will be at most J + 1,
which corresponds to the total number of com-
parisons made in calculating the loss
In Figure 2 (c), Lc(F0) = 3 and J + 1 = 6, so
Ac(F0) = 0.5.
</bodyText>
<sectionHeader confidence="0.9437205" genericHeader="method">
5 Learning a BTG Parser for
Reordering
</sectionHeader>
<bodyText confidence="0.999930666666667">
Now that we have a definition of loss over re-
orderings produced by the model, we have a
clear learning objective: we would like to find
reorderings F0 with low loss. The learning algo-
rithm we use to achieve this goal is motivated
by discriminative training for machine transla-
tion systems (Liang et al., 2006), and extended
to use large-margin training in an online frame-
work (Watanabe et al., 2007).
</bodyText>
<subsectionHeader confidence="0.989505">
5.1 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.980510087719298">
Learning uses the general framework of large-
margin online structured prediction (Crammer
et al., 2006), which makes several passes through
the data, finding a derivation with high model
score (the model parse) and a derivation with
sIt should be noted that for sentences of length one or
sentences with tied ranks, the maximum loss may be less
than J + 1, but for simplicity we use this approximation.
minimal loss (the oracle parse), and updating w
if these two parses diverge (Figure 3).
In order to create both of these parses effi-
ciently, we first create a parse forest encoding a
large number of derivations Di according to the
model scores. Next, we find the model parse Di,
which is the parse in the forest Di that maxi-
mizes the sum of the model score and the loss
S(Dk|Fk, w)+L(Dk|Fk, Ak). It should be noted
that here we are considering not only the model
score, but also the derivation’s loss. This is
necessary for loss-driven large-margin training
(Crammer et al., 2006), and follows the basic
intuition that during training, we would like to
make it easier to select negative examples with
large loss, causing these examples to be penal-
ized more often and more heavily.
We also find an oracle parse Di, which is se-
lected solely to minimize the loss L(Dk|Fk, Ak).
One important difference between the model we
describe here and traditional parsing models is
that the target derivation Dk is a latent variable.
Because many Dk achieve a particular reorder-
ing F0, many reorderings F0 are able to mini-
mize the loss L(F0k|Fk, Ak). Thus it is necessary
to choose a single oracle derivation to treat as
the target out of many equally good reorderings.
DeNero and Uszkoreit (2011) resolve this ambi-
guity with four features with empirically tuned
scores before training a monolingual parser and
reordering model. In contrast, we follow previ-
ous work on discriminative learning with latent
variables (Yu and Joachims, 2009), and break
ties within the pool of oracle derivations by se-
lecting the derivation with the largest model
score. From an implementation point of view,
this can be done by finding the derivation that
minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where
α is a constant small enough to ensure that the
effect of the loss will always be greater than the
effect of the score.
Dk has a loss that
Finally, if the model parse
is greater than that of the oracle parse Dk, we
update the weights to increase the score of the
oracle parse and decrease the score of the model
parse. Any criterion for weight updates may be
used, such as the averaged perceptron (Collins,
2002) and MIRA (Crammer et al., 2006), but
</bodyText>
<equation confidence="0.9989155">
Ac(F0) = 1 − Lc(F0)
J + 1 .
</equation>
<page confidence="0.981737">
847
</page>
<bodyText confidence="0.999906333333333">
we opted to use Pegasos (Shalev-Shwartz et al.,
2007) as it allows for the introduction of regu-
larization and relatively stable learning.
To perform this full process, given a source
sentence Fk, alignment Ak, and model weights
w we need to be able to efficiently calculate
scores, calculate losses, and create parse forests
for derivations Dk, the details of which will be
explained in the following sections.
</bodyText>
<subsectionHeader confidence="0.996862">
5.2 Scoring Derivation Trees
</subsectionHeader>
<bodyText confidence="0.999903625">
First, we must consider how to efficiently assign
scores 5(D|F, w) to a derivation or forest during
parsing. The most standard and efficient way to
do so is to create local features that can be cal-
culated based only on the information included
in a single node d in the derivation tree. The
score of the whole tree can then be expressed as
the sum of the scores from each node:
</bodyText>
<equation confidence="0.997843333333333">
5(D|F, w) = ∑ 5(d|F, w)
dED
wi0i(d, F).
</equation>
<bodyText confidence="0.999836538461538">
Based on this restriction, we define a number of
features that can be used to score the parse tree.
To ease explanation, we represent each node in
the derivation as d = (s, l, c, c + 1, r), where s
is the node’s symbol (sTR, INV, or TERM), while
l and r are the leftmost and rightmost indices
of the span that d covers. c and c + 1 are the
rightmost index of the left child and leftmost
index of the right child for non-terminal nodes.
All features are intersected with the node la-
bel s, so each feature described below corre-
sponds to three different features (or two for
features applicable to only non-terminal nodes).
</bodyText>
<listItem confidence="0.992361541666666">
• φlex: Identities of words in positions fl, fr,
fc, fc+1, fl−1, fr+1, flfr, and fcfc+1.
• φclass: Same as φlex, but with words ab-
stracted to classes. We use the 50 classes
automatically generated by Och (1999)’s
method that are calculated during align-
ment in standard SMT systems.
• φbalance: For non-terminals, features indi-
cating whether the length of the left span
(c−l +1) is lesser than, equal to, or greater
than the length of the right span (r − c).
• φtable: Features, bucketed by length, that
indicate whether “fl ... fr” appears as a
contiguous phrase in the SMT training
data, as well as the log frequency of the
number of times the phrase appears total
and the number of times it appears as a
contiguous phrase (DeNero and Uszkoreit,
2011). Phrase length is limited to 8, and
phrases of frequency one are removed.
• φpos: Same as φlex, but with words ab-
stracted to language-dependent POS tags.
• φcfg: Features indicating the label of the
spans fl ... fr, fl ... fc, and fc+1 ... fr in a
</listItem>
<bodyText confidence="0.897510076923077">
supervised parse tree, and the intersection
of the three labels. When spans do not cor-
respond to a span in the supervised parse
tree, we indicate “no span” with the label
“X” (Zollmann and Venugopal, 2006).
Most of these features can be calculated from
only a parallel corpus, but φpos requires a POS
tagger and φcfg requires a full syntactic parser
in the source language. As it is preferable to
have a method that is applicable in languages
where these tools are not available, we perform
experiments both with and without the features
that require linguistic analysis tools.
</bodyText>
<subsectionHeader confidence="0.986712">
5.3 Finding Losses for Derivation Trees
</subsectionHeader>
<bodyText confidence="0.999813384615385">
The above features φ and their corresponding
weights w are all that are needed to calculate
scores of derivation trees at test time. However,
during training, it is also necessary to find model
parses according to the loss-augmented scoring
function 5(D|F, w)+L(D|F, A) or oracle parses
according to the loss L(D|F, A). As noted by
Taskar et al. (2003), this is possible if our losses
can be factored in the same way as the feature
space. In this section, we demonstrate that the
loss L(d|F, A) for the evaluation measures we
defined in Section 4 can (mostly) be factored
over nodes in a fashion similar to features.
</bodyText>
<equation confidence="0.806009">
∑=
dED
∑
i
</equation>
<page confidence="0.950311">
848
</page>
<subsubsectionHeader confidence="0.692466">
5.3.1 Factoring Kendall’s τ
</subsubsectionHeader>
<bodyText confidence="0.9772904">
For Kendall’s τ, in the case of terminal nodes,
Lt(d = (TERM, l, r)|F, A) can be calculated by
performing the summation in Equation (1). We
can further define this sum recursively and use
memoization for improved efficiency
</bodyText>
<equation confidence="0.99219375">
Lt(d|F, A) =Lt((TERM, l, r − 1)|F, A)
r−1
+∑ δ(r(fj) &gt; r(fr)). (3)
j=l
</equation>
<bodyText confidence="0.999406153846154">
For non-terminal nodes, we first focus on
straight non-terminals with parent node d =
(STR,l, c, c+ 1, r), and left and right child nodes
dl = (sl, l, lc, lc+1, c) and dr = (sr, c+1, rc, rc+
1, r). First, we note that the loss for the subtree
rooted at d can be expressed as
In other words, the subtree’s total loss can be
factored into the loss of its left subtree, the
loss of its right subtree, and the additional loss
contributed by comparisons between the words
spanning both subtrees. In the case of inverted
terminals, we must simply reverse the compari-
son in the final sum to be δ(r(fj1) &lt; r(fj2)).
</bodyText>
<subsubsectionHeader confidence="0.653542">
5.3.2 Factoring Chunk Fragmentation
</subsubsectionHeader>
<bodyText confidence="0.999880444444444">
Chunk fragmentation loss can be factored in a
similar fashion. First, it is clear that the loss for
the terminal nodes can be calculated efficiently
in a fashion similar to Equation (3). In order to
calculate the loss for non-terminals d, we note
that the summation in Equation (2) can be di-
vided into the sum over the internal bi-grams
in the left and right subtrees, and the bi-gram
spanning the reordered trees
</bodyText>
<equation confidence="0.674515">
Lc(d|F, A) =Lc(dl|F, A) + Lc(dr|F, A)
+ DISCONT(f0 c, f0 c+1).
</equation>
<bodyText confidence="0.986573083333333">
However, unlike Kendall’s τ, this equation re-
lies not on the ranks of fc and fc+1 in the origi-
nal sentence, but on the ranks of f0c and f0c+1 in
the reordered sentence. In order to keep track
of these values, it is necessary to augment each
node in the tree to be d = (s, l, c, c + 1, r, tl, tr)
with two additional values tl and tr that indi-
cate the position of the leftmost and rightmost
words after reordering. Thus, a straight non-
terminal parent d with children dl = (sl, l, lc, lc+
1, c, tl, tlr) and dr = (sr, c+1, rc, rc+1, r, trl, tr)
will have loss as follows
</bodyText>
<equation confidence="0.965455">
Lc(d|F, A) =Lc(dl|F, A) + Lc(dr|F, A)
+ DISCONT(ftlr, ftrl)
</equation>
<bodyText confidence="0.9882235">
with a similar calculation being possible for in-
verted non-terminals.
</bodyText>
<subsectionHeader confidence="0.973291">
5.4 Parsing Derivation Trees
</subsectionHeader>
<bodyText confidence="0.9999675">
Finally, we must be able to create a parse forest
from which we select model and oracle parses.
As all feature functions factor over single nodes,
it is possible to find the parse tree with the high-
est score in O(J3) time using the CKY algo-
rithm. However, when keeping track of target
positions for calculation of chunk fragmentation
loss, there are a total of O(J5) nodes, an unrea-
sonable burden in terms of time and memory.
To overcome this problem, we note that this set-
ting is nearly identical to translation using syn-
chronous CFGs with an integrated bigram LM,
and thus we can employ cube-pruning to reduce
our search space (Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9931719375">
Our experiments test the reordering and trans-
lation accuracy of translation systems using the
proposed method. As reordering metrics, we use
Kendall’s τ and chunk fragmentation (Talbot et
al., 2011) comparing the system F0 and oracle
F0 calculated with manually created alignments.
As translation metrics, we use BLEU (Papineni
et al., 2002), as well as RIBES (Isozaki et al.,
2010a), which is similar to Kendall’s τ, but eval-
uated on the target sentence E instead of the re-
ordered sentence F0. All scores are the average
of three training runs to control for randomness
in training (Clark et al., 2011).
For translation, we use Moses (Koehn et al.,
2007) with lexicalized reordering (Koehn et al.,
2005) in all experiments. We test three types
</bodyText>
<table confidence="0.948985142857143">
Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A)
∑c ∑r δ(r(fj1) &gt; r(fj2)).
+ j2=c+1
j1=l
849
Chunk en -ja RIBES Chunk ja -en RIBES
T BLEU T BLEU
ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36
3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42
3-STEP+opos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65
3-STEP+ocfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93
LADER 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
LADER+opos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24
LADER+ocfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12
</table>
<tableCaption confidence="0.930529333333333">
Table 2: Reordering (chunk, T) and translation (BLEU, RIBES) results for each system. Bold numbers
indicate no significant difference from the best system (bootstrap resampling with p &gt; 0.05) (Koehn, 2004).
sent. word (ja) word (en)
</tableCaption>
<table confidence="0.9998422">
RM-train 602 14.5k 14.3k
RM-test 555 11.2k 10.4k
TM/LM 329k 6.08M 5.91M
Tune 1166 26.8k 24.3k
Test 1160 28.5k 26.7k
</table>
<tableCaption confidence="0.967231333333333">
Table 1: The number of sentences and words for
training and testing the reordering model (RM),
translation model (TM), and language model (LM).
</tableCaption>
<bodyText confidence="0.999498636363636">
of pre-ordering: original order with F0 +- F
(ORIG), pre-orderings learned using the 3-step
process of DeNero and Uszkoreit (2011) (3-
STEP), and the proposed model with latent
derivations (LADER).7 Except when stated oth-
erwise, LADER was trained to minimize chunk
fragmentation loss with a cube pruning stack
pop limit of 50, and the regularization constant
of 10−3 (chosen through cross-validation).
We test our systems on Japanese-English and
English-Japanese translation using data from
the Kyoto Free Translation Task (Neubig, 2011).
We use the training set for training translation
and language models, the development set for
weight tuning, and the test set for testing (Table
1). We use the designated development and test
sets of manually created alignments as training
data for the reordering models, removing sen-
tences of more than 60 words.
As default features for LADER and the mono-
lingual parsing and reordering models in 3-STEP,
we use all the features described in Section 5.2
</bodyText>
<footnote confidence="0.95038">
7Available open-source: http://phontron.com/lader
</footnote>
<bodyText confidence="0.999331">
except opos and ocfg. In addition, we test sys-
tems with opos and ocfg added. For English,
we use the Stanford parser (Klein and Manning,
2003) for both POS tagging and CFG parsing.
For Japanese, we use the KyTea tagger (Neu-
big et al., 2011) for POS tagging,8 and the EDA
word-based dependency parser (Flannery et al.,
2011) with simple manual head-rules to convert
a dependency parse to a CFG parse.
</bodyText>
<subsectionHeader confidence="0.999823">
6.1 Effect of Pre-ordering
</subsectionHeader>
<bodyText confidence="0.999981533333333">
Table 2 shows reordering and translation results
for ORIG, 3-STEP, and LADER. It can be seen
that the proposed LADER outperforms the base-
lines in both reordering and translation.9 There
are a number of reasons why LADER outper-
forms 3-STEP. First, the pipeline of 3-STEP
suffers from error propogation, with errors in
monolingual parsing and reordering resulting
in low overall accuracy.10 Second, as Section
5.1 describes, LADER breaks ties between ora-
cle parses based on model score, allowing easy-
to-reproduce model parses to be chosen dur-
ing training. In fact, LADER generally found
trees that followed from syntactic constituency,
while 3-STEP more often used terminal nodes
</bodyText>
<footnote confidence="0.9996507">
8In addition, following the example of Sudoh et al.
(2011a)’s reordering rules, we lexicalize all particles.
9It should be noted that our results for 3-STEP are
significantly worse than those of DeNero and Uszkoreit
(2011). Likely reasons include a 20x difference in training
data size, the fact that we are using naturally translated
text as opposed to text translated specifically to create
word alignments, or differences in implementation.
10When using oracle parses, chunk accuracy was up to
81%, showing that parsing errors are highly detrimental.
</footnote>
<page confidence="0.985467">
850
</page>
<table confidence="0.9995546">
Chunk en -ja RIBES Chunk ja -en RIBES
T BLEU T BLEU
L, 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26
L, + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48
</table>
<tableCaption confidence="0.998886">
Table 3: Results for systems trained to optimize chunk fragmentation (L,) or Kendall’s T (Lt).
</tableCaption>
<bodyText confidence="0.999901565217391">
that spanned constituent boundaries (as long as
the phrase frequency was high). Finally, as Sec-
tion 6.2 shows in detail, the ability of LADER to
maximize reordering accuracy directly allows for
improved reordering and translation results.
It can also be seen that incorporating POS
tags or parse trees improves accuracy of both
LADER and 3-STEP, particularly for English-
Japanese, where syntax has proven useful for
pre-ordering, and less so for Japanese-English,
where syntactic pre-ordering has been less suc-
cessful (Sudoh et al., 2011b).
We also tested Moses’s implementation of hi-
erarchical phrase-based SMT (Chiang, 2007),
which achieved BLEU scores of 23.21 and 19.30
for English-Japanese and Japanese-English re-
spectively, approximately matching LADER in
accuracy, but with a significant decrease in de-
coding speed. Further, when pre-ordering with
LADER and hierarchical phrase-based SMT were
combined, BLEU scores rose to 23.29 and 19.69,
indicating that the two techniques can be com-
bined for further accuracy improvements.
</bodyText>
<subsectionHeader confidence="0.999511">
6.2 Effect of Training Loss
</subsectionHeader>
<bodyText confidence="0.999915357142857">
Table 3 shows results when one of three losses is
optimized during training: chunk fragmentation
(L,), Kendall’s T (Lt), or the linear interpola-
tion of the two with weights chosen so that both
losses contribute equally (Lt + L,). In general,
training successfully maximizes the criterion it is
trained on, and Lt + L, achieves good results on
both measures. We also find that L, and L,+Lt
achieve the best translation results, which is
in concert with Talbot et al. (2011), who find
chunk fragmentation is better correlated with
translation accuracy than Kendall’s T. This is
an important result, as methods such as that
of Tromble and Eisner (2009) optimize pairwise
</bodyText>
<table confidence="0.996736166666667">
en-ja ja-en
BLEU/RIBES BLEU/RIBES
ORIG 21.87 68.25 18.34 65.36
MAN-602 23.11 69.86 19.54 66.93
AUTO-602 22.39 69.19 18.58 66.07
AUTO-10K 22.53 69.68 18.79 66.89
</table>
<tableCaption confidence="0.988774">
Table 4: Results based on data size, and whether
manual or automatic alignments are used in training.
</tableCaption>
<bodyText confidence="0.991247">
word comparisons equivalent to Lt, which may
not be optimal for translation.
</bodyText>
<subsectionHeader confidence="0.999137">
6.3 Effect of Automatic Alignments
</subsectionHeader>
<bodyText confidence="0.999991">
Table 4 shows the difference between using man-
ual and automatic alignments in the training of
LADER. LADER is able to improve over the ORIG
baseline in all cases, but when equal numbers
of manual and automatic alignments are used,
the reorderer trained on manual alignments is
significantly better. However, as the number of
automatic alignments is increased, accuracy im-
proves, approaching that of the system trained
on a smaller number of manual alignments.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977125">
We presented a method for learning a discrim-
inative parser to maximize reordering accuracy
for machine translation. Future work includes
application to other language pairs, develop-
ment of more sophisticated features, investiga-
tion of probabilistic approaches to inference, and
incorporation of the learned trees directly in
tree-to-string translation.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994680666666667">
We thank Isao Goto, Tetsuo Kiso, and anony-
mous reviewers for their helpful comments, and
Daniel Flannery for helping to run his parser.
</bodyText>
<page confidence="0.997775">
851
</page>
<sectionHeader confidence="0.990145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999281524752475">
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation
with improved correlation with human judgments.
In Proc. ACL Workshop.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15–26.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving arabic-to-english statistical ma-
chine translation by reordering post-verbal sub-
jects for alignment. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proc. ACL, pages
176–181.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP, pages 1–8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
John DeNero and Jakob Uszkoreit. 2011. Induc-
ing sentence structure from parallel corpora for
reordering. In Proc. EMNLP.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Daniel Flannery, Yusuke Miyao, Graham Neubig,
and Shinsuke Mori. 2011. Training dependency
parsers from partially annotated corpora. In Proc.
IJCNLP, pages 776–784, Chiang Mai, Thailand,
November.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine
translation. In Proc. COLING.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proc. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, and Hajime Tsukada. 2010a. Auto-
matic evaluation of translation quality for distant
language pairs. In Proc. EMNLP, pages 944–952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,
and Kevin Duh. 2010b. Head finalization: A
simple reordering rule for sov languages. In Proc.
WMT and MetricsMATR.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proc. EMNLP, pages 183–192.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1/2):81–93.
Maxim Khalilov and Khalil Sima’an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proc. IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. ACL, pages
423–430.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT, pages 48–54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation eval-
uation. In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. ACL, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc.
EMNLP.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. ACL.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Proc.
ACL, pages 761–768.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529–533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proc. EACL.
</reference>
<page confidence="0.981391">
852
</page>
<reference confidence="0.999895360655738">
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
COLING, pages 311–318.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
pos-based distortion model. In Proc. of TMI-2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan
Srebro. 2007. Pegasos: Primal estimated sub-
gradient solver for SVM. In Proc. ICML, pages
807–814.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2011a. NTT-
UT statistical machine translation in NTCIR-9
PatentMT. In Proc. NTCIR.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. 2011b. Post-
ordering in statistical machine translation. In
Proc. MT Summit.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proc. WMT.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks. Proc. NIPS,
16.
Roy Tromble and Jason Eisner. 2009. Learning lin-
ear ordering problems for better translation. In
Proc. EMNLP.
Karthik Visweswariah, Rajakrishnan Rajkumar,
Ankur Gandhe, Ananthakrishnan Ramanathan,
and Jiri Navratil. 2011. A word reordering
model for improved machine translation. In Proc.
EMNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proc.
EMNLP, pages 764–773.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3).
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables.
In Proc. ICML, pages 1169–1176.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statis-
tical machine translation. In Proc. SSST.
Andreas Zollmann and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proc. WMT, pages 138–141.
</reference>
<page confidence="0.999338">
853
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728456">
<title confidence="0.998191">Inducing a Discriminative Parser to Optimize Translation Reordering</title>
<author confidence="0.987102">Taro Shinsuke</author>
<affiliation confidence="0.928840666666667">School of Informatics, Kyoto Yoshida Honmachi, Sakyo-ku, Kyoto, Japan Institute of Information and Communication</affiliation>
<address confidence="0.940951">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan</address>
<abstract confidence="0.99806675">This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop.</booktitle>
<contexts>
<context position="12607" citStr="Banerjee and Lavie, 2005" startWordPosition="2110" endWordPosition="2113">e for ordering F&apos;, we first calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentences δ(r(f&apos; j1) =� r(f&apos;j2)). (1) 5The traditional formulation of Kendall’s T assumes no ties in rank, and thus the maximum loss can be calculated as J(J − 1)/2. max Lt( F�&apos;), F˜&apos; which will take a value between 0 (when F&apos; has maximal loss), and 1 (when F&apos; matches one of the oracle orderings). In Figure 2 (b), Lt(F&apos;) = Lt( F�&apos;) = 8, so At(F&apos;) = 0.75. 4.3 Chunk Fragmentation Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation. This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order. One way to measure the number of continuous chunks is considering whether each word pair f&apos;j and f&apos;j+1 is discontinuous (the rank of f&apos;j+1 is not equal to or one greater than f&apos;) j DISCONT(f&apos;j, f&apos;j+1) = δ(r</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. ACL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT evaluation: evaluating reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="11233" citStr="Birch et al., 2010" startWordPosition="1862" endWordPosition="1865">j1 &lt; aj2 nor aj2 &lt; aj1, both fj1 and fj2 are assigned the same rank. We can now define measures of reordering accuracy for F&apos; by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F&apos; where all words are in strictly ascending order, which we will call oracle orderings. 4.2 Kendall’s τ The first measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements f&apos;j1 and f&apos;j2 of the reordered sentence, where j1 &lt; j2. Because j1 &lt; j2, f&apos;j1 comes before f&apos;j2 in the reordered sentence, the ranks should be r(f&apos;j1) &lt; r(f&apos;j2) in order to produce the correct ordering. Based on this criterion, we first define a loss Lt(F&apos;) that will be higher for orderings that are further from the oracle. Specifically, we take the sum of all pairwise orderings that do not follow the expected order δ(r(f&apos;j1) &gt; r(f&apos; j2)) where δ(·) is an i</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1):15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
</authors>
<title>Improving arabic-to-english statistical machine translation by reordering post-verbal subjects for alignment.</title>
<date>2010</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4631" citStr="Carpuat et al., 2010" startWordPosition="720" endWordPosition="723">ansforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual p</context>
</contexts>
<marker>Carpuat, Marton, Habash, 2010</marker>
<rawString>Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Improving arabic-to-english statistical machine translation by reordering post-verbal subjects for alignment. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1414" citStr="Chiang, 2007" startWordPosition="204" endWordPosition="205">framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-</context>
<context position="24487" citStr="Chiang, 2007" startWordPosition="4271" endWordPosition="4272">eate a parse forest from which we select model and oracle parses. As all feature functions factor over single nodes, it is possible to find the parse tree with the highest score in O(J3) time using the CKY algorithm. However, when keeping track of target positions for calculation of chunk fragmentation loss, there are a total of O(J5) nodes, an unreasonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (</context>
<context position="29911" citStr="Chiang, 2007" startWordPosition="5143" endWordPosition="5144"> constituent boundaries (as long as the phrase frequency was high). Finally, as Section 6.2 shows in detail, the ability of LADER to maximize reordering accuracy directly allows for improved reordering and translation results. It can also be seen that incorporating POS tags or parse trees improves accuracy of both LADER and 3-STEP, particularly for EnglishJapanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b). We also tested Moses’s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching LADER in accuracy, but with a significant decrease in decoding speed. Further, when pre-ordering with LADER and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements. 6.2 Effect of Training Loss Table 3 shows results when one of three losses is optimized during training: chunk fragmentation (L,), Kendall’s T (Lt), or the linear interpolation of the two with </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>176--181</pages>
<contexts>
<context position="25106" citStr="Clark et al., 2011" startWordPosition="4371" endWordPosition="4374">. 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) &gt; r(fj2)). + j2=c+1 j1=l 849 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42 3-STEP+opos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65 3-STEP+ocfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93 LADER 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93 LADER+opos 73.97 79.24 23.32 69.78 75.49 7</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. ACL, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4592" citStr="Collins et al., 2005" startWordPosition="712" endWordPosition="715"> in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual gram</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17873" citStr="Collins, 2002" startWordPosition="3070" endWordPosition="3071"> derivations by selecting the derivation with the largest model score. From an implementation point of view, this can be done by finding the derivation that minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where α is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score. Dk has a loss that Finally, if the model parse is greater than that of the oracle parse Dk, we update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but Ac(F0) = 1 − Lc(F0) J + 1 . 847 we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence Fk, alignment Ak, and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations Dk, the details of which will be explained in the following sections. 5.2 Scoring Derivation Trees First, we must consider how to efficiently assign scores 5(D|F, w) to a derivation or fore</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="15393" citStr="Crammer et al., 2006" startWordPosition="2636" endWordPosition="2639">igure 2 (c), Lc(F0) = 3 and J + 1 = 6, so Ac(F0) = 0.5. 5 Learning a BTG Parser for Reordering Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F0 with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007). 5.1 Learning Algorithm Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with sIt should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation. minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3). In order to create both of these parses efficiently, we first create a parse forest encoding a large number of derivations Di according to the model scores. Next, we find the model parse Di, which is the parse in</context>
<context position="17905" citStr="Crammer et al., 2006" startWordPosition="3074" endWordPosition="3077"> the derivation with the largest model score. From an implementation point of view, this can be done by finding the derivation that minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where α is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score. Dk has a loss that Finally, if the model parse is greater than that of the oracle parse Dk, we update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but Ac(F0) = 1 − Lc(F0) J + 1 . 847 we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence Fk, alignment Ak, and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations Dk, the details of which will be explained in the following sections. 5.2 Scoring Derivation Trees First, we must consider how to efficiently assign scores 5(D|F, w) to a derivation or forest during parsing. The most stan</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1873" citStr="DeNero and Uszkoreit (2011)" startWordPosition="273" endWordPosition="277"> the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable. As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy. We propose a variety of features, and demonstrate that learning can succeed when no linguistic informati</context>
<context position="4920" citStr="DeNero and Uszkoreit, 2011" startWordPosition="766" endWordPosition="770">n the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative reordering model. In addition Tro</context>
<context position="16976" citStr="DeNero and Uszkoreit (2011)" startWordPosition="2913" endWordPosition="2916">e it easier to select negative examples with large loss, causing these examples to be penalized more often and more heavily. We also find an oracle parse Di, which is selected solely to minimize the loss L(Dk|Fk, Ak). One important difference between the model we describe here and traditional parsing models is that the target derivation Dk is a latent variable. Because many Dk achieve a particular reordering F0, many reorderings F0 are able to minimize the loss L(F0k|Fk, Ak). Thus it is necessary to choose a single oracle derivation to treat as the target out of many equally good reorderings. DeNero and Uszkoreit (2011) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model. In contrast, we follow previous work on discriminative learning with latent variables (Yu and Joachims, 2009), and break ties within the pool of oracle derivations by selecting the derivation with the largest model score. From an implementation point of view, this can be done by finding the derivation that minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where α is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the sc</context>
<context position="20174" citStr="DeNero and Uszkoreit, 2011" startWordPosition="3488" endWordPosition="3491">ds abstracted to classes. We use the 50 classes automatically generated by Och (1999)’s method that are calculated during alignment in standard SMT systems. • φbalance: For non-terminals, features indicating whether the length of the left span (c−l +1) is lesser than, equal to, or greater than the length of the right span (r − c). • φtable: Features, bucketed by length, that indicate whether “fl ... fr” appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency one are removed. • φpos: Same as φlex, but with words abstracted to language-dependent POS tags. • φcfg: Features indicating the label of the spans fl ... fr, fl ... fc, and fc+1 ... fr in a supervised parse tree, and the intersection of the three labels. When spans do not correspond to a span in the supervised parse tree, we indicate “no span” with the label “X” (Zollmann and Venugopal, 2006). Most of these features can be calculated from only a parallel corpus, but φpos requires a POS tagger and φcfg requires a full syntactic parser i</context>
<context position="26405" citStr="DeNero and Uszkoreit (2011)" startWordPosition="4584" endWordPosition="4587">.12 Table 2: Reordering (chunk, T) and translation (BLEU, RIBES) results for each system. Bold numbers indicate no significant difference from the best system (bootstrap resampling with p &gt; 0.05) (Koehn, 2004). sent. word (ja) word (en) RM-train 602 14.5k 14.3k RM-test 555 11.2k 10.4k TM/LM 329k 6.08M 5.91M Tune 1166 26.8k 24.3k Test 1160 28.5k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). of pre-ordering: original order with F0 +- F (ORIG), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- STEP), and the proposed model with latent derivations (LADER).7 Except when stated otherwise, LADER was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011). We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test</context>
<context position="28645" citStr="DeNero and Uszkoreit (2011)" startWordPosition="4939" endWordPosition="4942">fers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.10 Second, as Section 5.1 describes, LADER breaks ties between oracle parses based on model score, allowing easyto-reproduce model parses to be chosen during training. In fact, LADER generally found trees that followed from syntactic constituency, while 3-STEP more often used terminal nodes 8In addition, following the example of Sudoh et al. (2011a)’s reordering rules, we lexicalize all particles. 9It should be noted that our results for 3-STEP are significantly worse than those of DeNero and Uszkoreit (2011). Likely reasons include a 20x difference in training data size, the fact that we are using naturally translated text as opposed to text translated specifically to create word alignments, or differences in implementation. 10When using oracle parses, chunk accuracy was up to 81%, showing that parsing errors are highly detrimental. 850 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU L, 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93 Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26 L, + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48 Table 3: Results for systems trained to optimize chunk</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free reordering, finite-state translation.</title>
<date>2010</date>
<booktitle>In Proc. HLTNAACL.</booktitle>
<contexts>
<context position="4500" citStr="Dyer and Resnik, 2010" startWordPosition="699" endWordPosition="702">ering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero </context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free reordering, finite-state translation. In Proc. HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Flannery</author>
<author>Yusuke Miyao</author>
<author>Graham Neubig</author>
<author>Shinsuke Mori</author>
</authors>
<title>Training dependency parsers from partially annotated corpora.</title>
<date>2011</date>
<booktitle>In Proc. IJCNLP,</booktitle>
<pages>776--784</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="27641" citStr="Flannery et al., 2011" startWordPosition="4781" endWordPosition="4784">lly created alignments as training data for the reordering models, removing sentences of more than 60 words. As default features for LADER and the monolingual parsing and reordering models in 3-STEP, we use all the features described in Section 5.2 7Available open-source: http://phontron.com/lader except opos and ocfg. In addition, we test systems with opos and ocfg added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Effect of Pre-ordering Table 2 shows reordering and translation results for ORIG, 3-STEP, and LADER. It can be seen that the proposed LADER outperforms the baselines in both reordering and translation.9 There are a number of reasons why LADER outperforms 3-STEP. First, the pipeline of 3-STEP suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.10 Second, as Section 5.1 describes, LADER breaks ties between oracle parses based on model score, allowing ea</context>
</contexts>
<marker>Flannery, Miyao, Neubig, Mori, 2011</marker>
<rawString>Daniel Flannery, Yusuke Miyao, Graham Neubig, and Shinsuke Mori. 2011. Training dependency parsers from partially annotated corpora. In Proc. IJCNLP, pages 776–784, Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning source-side reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="4477" citStr="Genzel, 2010" startWordPosition="697" endWordPosition="698">gure 1). Reordering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our a</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="8985" citStr="Haghighi et al., 2009" startWordPosition="1453" endWordPosition="1456">F0. Furthermore, we assume that the score 5(D|F) is the weighted sum of a number of feature functions defined over D and F 5(D|F, w) = ∑ wiOi(D, F) i where Oi is the ith feature function, and wi is its corresponding weight in weight vector w. Given this model, we must next consider how to learn the weights w. As the final goal of our model is to produce good reorderings F0, it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings. 3BTGs cannot reproduce all possible reorderings, but can handle most reorderings occurring in natural translated text (Haghighi et al., 2009). Figure 2: An example of (a) the ranking function r(fj), (b) loss according to Kendall’s T, (c) loss according to chunk fragmentation. 4 Evaluating Reorderings Before we explain the learning algorithm, we must know how to distinguish whether the F0 produced by the model is good or bad. This section explains how to calculate oracle reorderings, and assign each F0 a loss and an accuracy according to how well it reproduces the oracle. 4.1 Calculating Oracle Orderings In order to calculate reordering quality, we first define a ranking function r(fj|F, A), which indicates the relative position of </context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proc. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs.</title>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>944--952</pages>
<marker>Isozaki, Hirao, Duh, </marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs. In Proc. EMNLP, pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head finalization: A simple reordering rule for sov languages. In</title>
<date>2010</date>
<booktitle>Proc. WMT and MetricsMATR.</booktitle>
<contexts>
<context position="4653" citStr="Isozaki et al., 2010" startWordPosition="724" endWordPosition="727">ng a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the</context>
<context position="11211" citStr="Isozaki et al., 2010" startWordPosition="1858" endWordPosition="1861">f ties, where neither aj1 &lt; aj2 nor aj2 &lt; aj1, both fj1 and fj2 are assigned the same rank. We can now define measures of reordering accuracy for F&apos; by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F&apos; where all words are in strictly ascending order, which we will call oracle orderings. 4.2 Kendall’s τ The first measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements f&apos;j1 and f&apos;j2 of the reordered sentence, where j1 &lt; j2. Because j1 &lt; j2, f&apos;j1 comes before f&apos;j2 in the reordered sentence, the ranks should be r(f&apos;j1) &lt; r(f&apos;j2) in order to produce the correct ordering. Based on this criterion, we first define a loss Lt(F&apos;) that will be higher for orderings that are further from the oracle. Specifically, we take the sum of all pairwise orderings that do not follow the expected order δ(r(f&apos;j1) &gt; r(f&apos; j</context>
<context position="24884" citStr="Isozaki et al., 2010" startWordPosition="4331" endWordPosition="4334">mory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) &gt; r(fj2)). + j2=c+1 j1=l 849 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 </context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010b. Head finalization: A simple reordering rule for sov languages. In Proc. WMT and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Katz-Brown</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Franz Och</author>
<author>David Talbot</author>
<author>Hiroshi Ichikawa</author>
<author>Masakazu Seno</author>
<author>Hideto Kazawa</author>
</authors>
<title>Training a parser for machine translation reordering.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>183--192</pages>
<contexts>
<context position="7009" citStr="Katz-Brown et al. (2011)" startWordPosition="1103" endWordPosition="1106">he bracketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (sTR) or inverted (INV) production, and terminals (TERM) span a nonempty substring f.2 The ordering of the sentence is determined by the tree structure and the non-terminal labels sTR and INV, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f&apos;. For each terminal node, no reordering occurs and f is equal to f&apos;. 1The semi-supervised method of Katz-Brown et al. (2011) also optimizes reordering accuracy, but requires manually annotated parses as seed data. 2In the original BTG framework used in translation, terminals produce a bilingual substring pair f/e, but as we are only interested in reordering the source F, we simplify the model by removing the target substring e. 844 For each non-terminal node spanning f with its left child spanning f1 and its right child spanning f2, if the non-terminal symbol is STR., the reordered strings will be concatenated in order as f0 = f01f02, and if the non-terminal symbol is INV, the reordered strings will be concatenated</context>
</contexts>
<marker>Katz-Brown, Petrov, McDonald, Och, Talbot, Ichikawa, Seno, Kazawa, 2011</marker>
<rawString>Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno, and Hideto Kazawa. 2011. Training a parser for machine translation reordering. In Proc. EMNLP, pages 183–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--1</pages>
<contexts>
<context position="11078" citStr="Kendall, 1938" startWordPosition="1840" endWordPosition="1841">irectly to the right for head-initial languages (e.g. English), or left for headfinal languages (e.g. Japanese). 845 the case of ties, where neither aj1 &lt; aj2 nor aj2 &lt; aj1, both fj1 and fj2 are assigned the same rank. We can now define measures of reordering accuracy for F&apos; by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F&apos; where all words are in strictly ascending order, which we will call oracle orderings. 4.2 Kendall’s τ The first measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements f&apos;j1 and f&apos;j2 of the reordered sentence, where j1 &lt; j2. Because j1 &lt; j2, f&apos;j1 comes before f&apos;j2 in the reordered sentence, the ranks should be r(f&apos;j1) &lt; r(f&apos;j2) in order to produce the correct ordering. Based on this criterion, we first define a loss Lt(F&apos;) that will be higher for orderings that are furt</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G. Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Khalil Sima’an</author>
</authors>
<title>Contextsensitive syntactic source-reordering by statistical transduction.</title>
<date>2011</date>
<booktitle>In Proc. IJCNLP.</booktitle>
<marker>Khalilov, Sima’an, 2011</marker>
<rawString>Maxim Khalilov and Khalil Sima’an. 2011. Contextsensitive syntactic source-reordering by statistical transduction. In Proc. IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="27460" citStr="Klein and Manning, 2003" startWordPosition="4749" endWordPosition="4752">t for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words. As default features for LADER and the monolingual parsing and reordering models in 3-STEP, we use all the features described in Section 5.2 7Available open-source: http://phontron.com/lader except opos and ocfg. In addition, we test systems with opos and ocfg added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Effect of Pre-ordering Table 2 shows reordering and translation results for ORIG, 3-STEP, and LADER. It can be seen that the proposed LADER outperforms the baselines in both reordering and translation.9 There are a number of reasons why LADER outperforms 3-STEP. First, the pipeline of 3-STEP suffers from error propogation, with errors i</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="4090" citStr="Koehn et al., 2003" startWordPosition="632" endWordPosition="635"> 2012. c�2012 Association for Computational Linguistics Figure 1: An example with a source sentence F reordered into target order F&apos;, and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering. the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Reordering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. IWSLT.</booktitle>
<contexts>
<context position="1360" citStr="Koehn et al., 2005" startWordPosition="197" endWordPosition="200">d over the parse tree. Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence pars</context>
<context position="25207" citStr="Koehn et al., 2005" startWordPosition="4387" endWordPosition="4390">sing the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) &gt; r(fj2)). + j2=c+1 j1=l 849 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42 3-STEP+opos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65 3-STEP+ocfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93 LADER 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93 LADER+opos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24 LADER+ocfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12 Table 2: Reordering (chun</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="25158" citStr="Koehn et al., 2007" startWordPosition="4380" endWordPosition="4383">and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) &gt; r(fj2)). + j2=c+1 j1=l 849 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42 3-STEP+opos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65 3-STEP+ocfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93 LADER 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93 LADER+opos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24 LADER+ocfg 75.06 80.53 23.36 70.89 </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="25987" citStr="Koehn, 2004" startWordPosition="4518" endWordPosition="4519">T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42 3-STEP+opos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65 3-STEP+ocfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93 LADER 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93 LADER+opos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24 LADER+ocfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12 Table 2: Reordering (chunk, T) and translation (BLEU, RIBES) results for each system. Bold numbers indicate no significant difference from the best system (bootstrap resampling with p &gt; 0.05) (Koehn, 2004). sent. word (ja) word (en) RM-train 602 14.5k 14.3k RM-test 555 11.2k 10.4k TM/LM 329k 6.08M 5.91M Tune 1166 26.8k 24.3k Test 1160 28.5k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). of pre-ordering: original order with F0 +- F (ORIG), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- STEP), and the proposed model with latent derivations (LADER).7 Except when stated otherwise, LADER was trained to minimize chunk fragmentation loss with a cube pruning stack po</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation. In</title>
<date>2007</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="4463" citStr="Li et al., 2007" startWordPosition="693" endWordPosition="696">d translation (Figure 1). Reordering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being si</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>761--768</pages>
<contexts>
<context position="15176" citStr="Liang et al., 2006" startWordPosition="2603" endWordPosition="2606">arly to Kendall’s τ, we can also define an accuracy measure between 0 and 1 using the maximum loss, which will be at most J + 1, which corresponds to the total number of comparisons made in calculating the loss In Figure 2 (c), Lc(F0) = 3 and J + 1 = 6, so Ac(F0) = 0.5. 5 Learning a BTG Parser for Reordering Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F0 with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007). 5.1 Learning Algorithm Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with sIt should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation. minimal loss (the oracle parse), and updating w if these two parses diverge (Figure</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. ACL, pages 761–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>529--533</pages>
<location>Portland, USA,</location>
<contexts>
<context position="27558" citStr="Neubig et al., 2011" startWordPosition="4767" endWordPosition="4771">t for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words. As default features for LADER and the monolingual parsing and reordering models in 3-STEP, we use all the features described in Section 5.2 7Available open-source: http://phontron.com/lader except opos and ocfg. In addition, we test systems with opos and ocfg added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Effect of Pre-ordering Table 2 shows reordering and translation results for ORIG, 3-STEP, and LADER. It can be seen that the proposed LADER outperforms the baselines in both reordering and translation.9 There are a number of reasons why LADER outperforms 3-STEP. First, the pipeline of 3-STEP suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.10 Second, as Section 5.1 d</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proc. ACL, pages 529–533, Portland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>The Kyoto free translation task.</title>
<date>2011</date>
<note>http://www.phontron.com/kftt.</note>
<contexts>
<context position="26812" citStr="Neubig, 2011" startWordPosition="4646" endWordPosition="4647">the reordering model (RM), translation model (TM), and language model (LM). of pre-ordering: original order with F0 +- F (ORIG), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- STEP), and the proposed model with latent derivations (LADER).7 Except when stated otherwise, LADER was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011). We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words. As default features for LADER and the monolingual parsing and reordering models in 3-STEP, we use all the features described in Section 5.2 7Available open-source: http://phontron.com/lader except opos and ocfg. In addition, we test systems with opos and ocfg added. For English, we u</context>
</contexts>
<marker>Neubig, 2011</marker>
<rawString>Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes. In</title>
<date>1999</date>
<booktitle>Proc. EACL.</booktitle>
<contexts>
<context position="19632" citStr="Och (1999)" startWordPosition="3394" endWordPosition="3395">TR, INV, or TERM), while l and r are the leftmost and rightmost indices of the span that d covers. c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes. All features are intersected with the node label s, so each feature described below corresponds to three different features (or two for features applicable to only non-terminal nodes). • φlex: Identities of words in positions fl, fr, fc, fc+1, fl−1, fr+1, flfr, and fcfc+1. • φclass: Same as φlex, but with words abstracted to classes. We use the 50 classes automatically generated by Och (1999)’s method that are calculated during alignment in standard SMT systems. • φbalance: For non-terminals, features indicating whether the length of the left span (c−l +1) is lesser than, equal to, or greater than the length of the right span (r − c). • φtable: Features, bucketed by length, that indicate whether “fl ... fr” appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency </context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="24844" citStr="Papineni et al., 2002" startWordPosition="4323" endWordPosition="4326">nreasonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) &gt; r(fj2)). + j2=c+1 j1=l 849 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU ORIG 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36 3-STEP 63</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. COLING, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word reordering in statistical machine translation with a pos-based distortion model.</title>
<date>2007</date>
<booktitle>In Proc. of TMI-2007.</booktitle>
<contexts>
<context position="4866" citStr="Rottmann and Vogel, 2007" startWordPosition="758" endWordPosition="761">orks relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word reordering in statistical machine translation with a pos-based distortion model. In Proc. of TMI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated subgradient solver for SVM.</title>
<date>2007</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="17996" citStr="Shalev-Shwartz et al., 2007" startWordPosition="3094" endWordPosition="3097">this can be done by finding the derivation that minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where α is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score. Dk has a loss that Finally, if the model parse is greater than that of the oracle parse Dk, we update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but Ac(F0) = 1 − Lc(F0) J + 1 . 847 we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence Fk, alignment Ak, and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations Dk, the details of which will be explained in the following sections. 5.2 Scoring Derivation Trees First, we must consider how to efficiently assign scores 5(D|F, w) to a derivation or forest during parsing. The most standard and efficient way to do so is to create local features that can be calculated based on</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated subgradient solver for SVM. In Proc. ICML, pages 807–814.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
</authors>
<title>Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,</title>
<booktitle>and Jun’ichi Tsujii. 2011a. NTTUT statistical machine translation in NTCIR-9 PatentMT. In Proc. NTCIR.</booktitle>
<marker>Sudoh, Duh, </marker>
<rawString>Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2011a. NTTUT statistical machine translation in NTCIR-9 PatentMT. In Proc. NTCIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Postordering in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. MT Summit.</booktitle>
<contexts>
<context position="28480" citStr="Sudoh et al. (2011" startWordPosition="4914" endWordPosition="4917">outperforms the baselines in both reordering and translation.9 There are a number of reasons why LADER outperforms 3-STEP. First, the pipeline of 3-STEP suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.10 Second, as Section 5.1 describes, LADER breaks ties between oracle parses based on model score, allowing easyto-reproduce model parses to be chosen during training. In fact, LADER generally found trees that followed from syntactic constituency, while 3-STEP more often used terminal nodes 8In addition, following the example of Sudoh et al. (2011a)’s reordering rules, we lexicalize all particles. 9It should be noted that our results for 3-STEP are significantly worse than those of DeNero and Uszkoreit (2011). Likely reasons include a 20x difference in training data size, the fact that we are using naturally translated text as opposed to text translated specifically to create word alignments, or differences in implementation. 10When using oracle parses, chunk accuracy was up to 81%, showing that parsing errors are highly detrimental. 850 Chunk en -ja RIBES Chunk ja -en RIBES T BLEU T BLEU L, 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66</context>
<context position="29822" citStr="Sudoh et al., 2011" startWordPosition="5129" endWordPosition="5132">lts for systems trained to optimize chunk fragmentation (L,) or Kendall’s T (Lt). that spanned constituent boundaries (as long as the phrase frequency was high). Finally, as Section 6.2 shows in detail, the ability of LADER to maximize reordering accuracy directly allows for improved reordering and translation results. It can also be seen that incorporating POS tags or parse trees improves accuracy of both LADER and 3-STEP, particularly for EnglishJapanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b). We also tested Moses’s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching LADER in accuracy, but with a significant decrease in decoding speed. Further, when pre-ordering with LADER and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements. 6.2 Effect of Training Loss Table 3 shows results when one of three losses is optimized during training:</context>
</contexts>
<marker>Sudoh, Wu, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011b. Postordering in statistical machine translation. In Proc. MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Hideto Kazawa</author>
<author>Hiroshi Ichikawa</author>
<author>Jason Katz-Brown</author>
<author>Masakazu Seno</author>
<author>Franz Och</author>
</authors>
<title>A lightweight evaluation framework for machine translation reordering.</title>
<date>2011</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="11281" citStr="Talbot et al., 2011" startWordPosition="1869" endWordPosition="1872">signed the same rank. We can now define measures of reordering accuracy for F&apos; by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F&apos; where all words are in strictly ascending order, which we will call oracle orderings. 4.2 Kendall’s τ The first measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements f&apos;j1 and f&apos;j2 of the reordered sentence, where j1 &lt; j2. Because j1 &lt; j2, f&apos;j1 comes before f&apos;j2 in the reordered sentence, the ranks should be r(f&apos;j1) &lt; r(f&apos;j2) in order to produce the correct ordering. Based on this criterion, we first define a loss Lt(F&apos;) that will be higher for orderings that are further from the oracle. Specifically, we take the sum of all pairwise orderings that do not follow the expected order δ(r(f&apos;j1) &gt; r(f&apos; j2)) where δ(·) is an indicator function that is 1 when its condition i</context>
<context position="12655" citStr="Talbot et al., 2011" startWordPosition="2117" endWordPosition="2120">s for the sentence, which is equal to the total number of non-equal rank comparisons in the sentences δ(r(f&apos; j1) =� r(f&apos;j2)). (1) 5The traditional formulation of Kendall’s T assumes no ties in rank, and thus the maximum loss can be calculated as J(J − 1)/2. max Lt( F�&apos;), F˜&apos; which will take a value between 0 (when F&apos; has maximal loss), and 1 (when F&apos; matches one of the oracle orderings). In Figure 2 (b), Lt(F&apos;) = Lt( F�&apos;) = 8, so At(F&apos;) = 0.75. 4.3 Chunk Fragmentation Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation. This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order. One way to measure the number of continuous chunks is considering whether each word pair f&apos;j and f&apos;j+1 is discontinuous (the rank of f&apos;j+1 is not equal to or one greater than f&apos;) j DISCONT(f&apos;j, f&apos;j+1) = δ(r(f&apos;j) =� r(f&apos;j+1) &apos; r(f&apos;j) + 1 =� r(f&apos;j+1)) and </context>
<context position="24701" citStr="Talbot et al., 2011" startWordPosition="4301" endWordPosition="4304">lgorithm. However, when keeping track of target positions for calculation of chunk fragmentation loss, there are a total of O(J5) nodes, an unreasonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F0 and oracle F0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ, but evaluated on the target sentence E instead of the reordered sentence F0. All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types Lt(d|F, A) =Lt(dl|F, A) + Lt(dr|F, A) ∑c ∑r δ(r(fj1) </context>
<context position="30818" citStr="Talbot et al. (2011)" startWordPosition="5286" endWordPosition="5289">U scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements. 6.2 Effect of Training Loss Table 3 shows results when one of three losses is optimized during training: chunk fragmentation (L,), Kendall’s T (Lt), or the linear interpolation of the two with weights chosen so that both losses contribute equally (Lt + L,). In general, training successfully maximizes the criterion it is trained on, and Lt + L, achieves good results on both measures. We also find that L, and L,+Lt achieve the best translation results, which is in concert with Talbot et al. (2011), who find chunk fragmentation is better correlated with translation accuracy than Kendall’s T. This is an important result, as methods such as that of Tromble and Eisner (2009) optimize pairwise en-ja ja-en BLEU/RIBES BLEU/RIBES ORIG 21.87 68.25 18.34 65.36 MAN-602 23.11 69.86 19.54 66.93 AUTO-602 22.39 69.19 18.58 66.07 AUTO-10K 22.53 69.68 18.79 66.89 Table 4: Results based on data size, and whether manual or automatic alignments are used in training. word comparisons equivalent to Lt, which may not be optimal for translation. 6.3 Effect of Automatic Alignments Table 4 shows the difference </context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och. 2011. A lightweight evaluation framework for machine translation reordering. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<date>2003</date>
<booktitle>Max-margin Markov networks. Proc. NIPS,</booktitle>
<pages>16</pages>
<contexts>
<context position="21393" citStr="Taskar et al. (2003)" startWordPosition="3699" endWordPosition="3702">in the source language. As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools. 5.3 Finding Losses for Derivation Trees The above features φ and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time. However, during training, it is also necessary to find model parses according to the loss-augmented scoring function 5(D|F, w)+L(D|F, A) or oracle parses according to the loss L(D|F, A). As noted by Taskar et al. (2003), this is possible if our losses can be factored in the same way as the feature space. In this section, we demonstrate that the loss L(d|F, A) for the evaluation measures we defined in Section 4 can (mostly) be factored over nodes in a fashion similar to features. ∑= dED ∑ i 848 5.3.1 Factoring Kendall’s τ For Kendall’s τ, in the case of terminal nodes, Lt(d = (TERM, l, r)|F, A) can be calculated by performing the summation in Equation (1). We can further define this sum recursively and use memoization for improved efficiency Lt(d|F, A) =Lt((TERM, l, r − 1)|F, A) r−1 +∑ δ(r(fj) &gt; r(fr)). (3) j</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-margin Markov networks. Proc. NIPS, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation. In</title>
<date>2009</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="4892" citStr="Tromble and Eisner, 2009" startWordPosition="762" endWordPosition="765">c parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative reord</context>
<context position="30995" citStr="Tromble and Eisner (2009)" startWordPosition="5314" endWordPosition="5317">en one of three losses is optimized during training: chunk fragmentation (L,), Kendall’s T (Lt), or the linear interpolation of the two with weights chosen so that both losses contribute equally (Lt + L,). In general, training successfully maximizes the criterion it is trained on, and Lt + L, achieves good results on both measures. We also find that L, and L,+Lt achieve the best translation results, which is in concert with Talbot et al. (2011), who find chunk fragmentation is better correlated with translation accuracy than Kendall’s T. This is an important result, as methods such as that of Tromble and Eisner (2009) optimize pairwise en-ja ja-en BLEU/RIBES BLEU/RIBES ORIG 21.87 68.25 18.34 65.36 MAN-602 23.11 69.86 19.54 66.93 AUTO-602 22.39 69.19 18.58 66.07 AUTO-10K 22.53 69.68 18.79 66.89 Table 4: Results based on data size, and whether manual or automatic alignments are used in training. word comparisons equivalent to Lt, which may not be optimal for translation. 6.3 Effect of Automatic Alignments Table 4 shows the difference between using manual and automatic alignments in the training of LADER. LADER is able to improve over the ORIG baseline in all cases, but when equal numbers of manual and automa</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
</authors>
<title>Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and</title>
<date>2011</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Visweswariah, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>764--773</pages>
<contexts>
<context position="15266" citStr="Watanabe et al., 2007" startWordPosition="2618" endWordPosition="2621">aximum loss, which will be at most J + 1, which corresponds to the total number of comparisons made in calculating the loss In Figure 2 (c), Lc(F0) = 3 and J + 1 = 6, so Ac(F0) = 0.5. 5 Learning a BTG Parser for Reordering Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F0 with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007). 5.1 Learning Algorithm Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with sIt should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation. minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3). In order to create both of these parses efficiently, we first create a parse forest e</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLP, pages 764–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6435" citStr="Wu (1997)" startWordPosition="1009" endWordPosition="1010">ization, including features such as those that utilize the existence of a span in the phrase table. Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations.1 3 Training a Reordering Model with Latent Derivations In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning. 3.1 Space of Reorderings The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (sTR) or inverted (INV) production, and terminals (TERM) span a nonempty substring f.2 The ordering of the sentence is determined by the tree structure and the non-terminal labels sTR and INV, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f&apos;. For each terminal node, no reordering occurs and f is equal to f&apos;. 1The semi-supervised method of Katz-Brown et al. (2011) also optimizes reordering</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1505" citStr="Xia and McCord, 2004" startWordPosition="216" endWordPosition="219">based SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser </context>
<context position="3781" citStr="Xia and McCord, 2004" startWordPosition="578" endWordPosition="581">tion is defined as transformation of source sentence F = f1 ... fJ to target sentence E = e1 ... eI. In this paper, we take 843 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 843–853, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Figure 1: An example with a source sentence F reordered into target order F&apos;, and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering. the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Reordering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through mach</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages. In</title>
<date>2009</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="4609" citStr="Xu et al., 2009" startWordPosition="716" endWordPosition="719">anslation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, tr</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1466" citStr="Yamada and Knight, 2001" startWordPosition="209" endWordPosition="212"> translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>1169--1176</pages>
<contexts>
<context position="17217" citStr="Yu and Joachims, 2009" startWordPosition="2949" endWordPosition="2952">ween the model we describe here and traditional parsing models is that the target derivation Dk is a latent variable. Because many Dk achieve a particular reordering F0, many reorderings F0 are able to minimize the loss L(F0k|Fk, Ak). Thus it is necessary to choose a single oracle derivation to treat as the target out of many equally good reorderings. DeNero and Uszkoreit (2011) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model. In contrast, we follow previous work on discriminative learning with latent variables (Yu and Joachims, 2009), and break ties within the pool of oracle derivations by selecting the derivation with the largest model score. From an implementation point of view, this can be done by finding the derivation that minimizes L(Dk|Fk, Ak) − αS(Dk|Fk, w), where α is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score. Dk has a loss that Finally, if the model parse is greater than that of the oracle parse Dk, we update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may </context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proc. ICML, pages 1169–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In</title>
<date>2007</date>
<booktitle>Proc. SSST.</booktitle>
<contexts>
<context position="4446" citStr="Zhang et al., 2007" startWordPosition="689" endWordPosition="692">ess of reordering and translation (Figure 1). Reordering first deterministically transforms F into F&apos;, which contains the same words as F but is in the order of E. Translation then transforms F&apos; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve men</context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In Proc. SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. WMT,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="20628" citStr="Zollmann and Venugopal, 2006" startWordPosition="3572" endWordPosition="3575">ining data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency one are removed. • φpos: Same as φlex, but with words abstracted to language-dependent POS tags. • φcfg: Features indicating the label of the spans fl ... fr, fl ... fc, and fc+1 ... fr in a supervised parse tree, and the intersection of the three labels. When spans do not correspond to a span in the supervised parse tree, we indicate “no span” with the label “X” (Zollmann and Venugopal, 2006). Most of these features can be calculated from only a parallel corpus, but φpos requires a POS tagger and φcfg requires a full syntactic parser in the source language. As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools. 5.3 Finding Losses for Derivation Trees The above features φ and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time. However, during training, it is also necessary to find</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. WMT, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
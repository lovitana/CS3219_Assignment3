<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.989207">
Automatically Constructing a Normalisation Dictionary for Microblogs
</title>
<author confidence="0.987883">
Bo Han,&apos;° Paul Cook,° and Timothy Baldwin&apos;°
</author>
<affiliation confidence="0.4712955">
4 NICTA Victoria Research Laboratory
C7 Department of Computing and Information Systems, The University of Melbourne
</affiliation>
<email confidence="0.960595">
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
</email>
<sectionHeader confidence="0.998494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997939421052632">
Microblog normalisation methods often utilise
complex models and struggle to differenti-
ate between correctly-spelled unknown words
and lexical variants of known words. In this
paper, we propose a method for construct-
ing a dictionary of lexical variants of known
words that facilitates lexical normalisation via
simple string substitution (e.g. tomorrow for
tmrw). We use context information to generate
possible variant and normalisation pairs and
then rank these by string similarity. Highly-
ranked pairs are selected to populate the dic-
tionary. We show that a dictionary-based ap-
proach achieves state-of-the-art performance
for both F-score and word error rate on a stan-
dard dataset. Compared with other methods,
this approach offers a fast, lightweight and
easy-to-use solution, and is thus suitable for
high-volume microblog pre-processing.
</bodyText>
<sectionHeader confidence="0.986044" genericHeader="categories and subject descriptors">
1 Lexical Normalisation
</sectionHeader>
<bodyText confidence="0.999927829787234">
A staggering number of short text “microblog” mes-
sages are produced every day through social me-
dia such as Twitter (Twitter, 2011). The immense
volume of real-time, user-generated microblogs that
flows through sites has been shown to have utility
in applications such as disaster detection (Sakaki et
al., 2010), sentiment analysis (Jiang et al., 2011;
Gonz´alez-Ib´a˜nez et al., 2011), and event discovery
(Weng and Lee, 2011; Benson et al., 2011). How-
ever, due to the spontaneous nature of the posts,
microblogs are notoriously noisy, containing many
non-standard forms — e.g., tmrw “tomorrow” and
2day “today” — which degrade the performance of
natural language processing (NLP) tools (Ritter et
al., 2010; Han and Baldwin, 2011). To reduce this
effect, attempts have been made to adapt NLP tools
to microblog data (Gimpel et al., 2011; Foster et al.,
2011; Liu et al., 2011b; Ritter et al., 2011). An al-
ternative approach is to pre-normalise non-standard
lexical variants to their standard orthography (Liu et
al., 2011a; Han and Baldwin, 2011; Xue et al., 2011;
Gouws et al., 2011). For example, se u 2morw!!!
would be normalised to see you tomorrow! The nor-
malisation approach is especially attractive as a pre-
processing step for applications which rely on key-
word match or word frequency statistics. For ex-
ample, earthqu, eathquake, and earthquakeee — all
attested in a Twitter corpus — have the standard
form earthquake; by normalising these types to their
standard form, better coverage can be achieved for
keyword-based methods, and better word frequency
estimates can be obtained.
In this paper, we focus on the task of lexical nor-
malisation of English Twitter messages, in which
out-of-vocabulary (OOV) tokens are normalised to
their in-vocabulary (IV) standard form, i.e., a stan-
dard form that is in a dictionary. Following other re-
cent work on lexical normalisation (Liu et al., 2011a;
Han and Baldwin, 2011; Gouws et al., 2011; Liu et
al., 2012), we specifically focus on one-to-one nor-
malisation in which one OOV token is normalised to
one IV word.
Naturally, not all OOV words in microblogs are
lexical variants of IV words: named entities, e.g.,
are prevalent in microblogs, but not all named en-
tities are included in our dictionary. One chal-
lenge for lexical normalisation is therefore to dis-
</bodyText>
<page confidence="0.983136">
421
</page>
<note confidence="0.7814385">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999873487804878">
tinguish those OOV tokens that require normalisa-
tion from those that are well-formed. Recent un-
supervised approaches have not attempted to distin-
guish such tokens from other types of OOV tokens
(Cook and Stevenson, 2009; Liu et al., 2011a), lim-
iting their applicability to real-world normalisation
tasks. Other approaches (Han and Baldwin, 2011;
Gouws et al., 2011) have followed a cascaded ap-
proach in which lexical variants are first identified,
and then normalised. However, such two-step ap-
proaches suffer from poor lexical variant identifica-
tion performance, which is propagated to the nor-
malisation step. Motivated by the observation that
most lexical variants have an unambiguous standard
form (especially for longer tokens), and that a lexi-
cal variant and its standard form typically occur in
similar contexts, in this paper we propose methods
for automatically constructing a lexical normalisa-
tion dictionary — a dictionary whose entries consist
of (lexical variant, standard form) pairs — that en-
ables type-based normalisation.
Despite the simplicity of this dictionary-based
normalisation method, we show it to outperform
previously-proposed approaches. This very fast,
lightweight solution is suitable for real-time pro-
cessing of the large volume of streaming microblog
data available from Twitter, and offers a simple solu-
tion to the lexical variant detection problem that hin-
ders other normalisation methods. Furthermore, this
dictionary-based method can be easily integrated
with other more-complex normalisation approaches
(Liu et al., 2011a; Han and Baldwin, 2011; Gouws
et al., 2011) to produce hybrid systems.
After discussing related work in Section 2, we
present an overview of our dictionary-based ap-
proach to normalisation in Section 3. In Sections 4
and 5 we experimentally select the optimised con-
text similarity parameters and string similarity re-
ranking method. We present experimental results on
the unseen test data in Section 6, and offer some con-
cluding remarks in Section 7.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999987076923077">
Given a token t, lexical normalisation is the task
of finding arg maxP(s|t) a arg maxP(t|s)P(s),
where s is the standard form, i.e., an IV word. Stan-
dardly in lexical normalisation, t is assumed to be an
OOV token, relative to a fixed dictionary. In prac-
tice, not all OOV tokens should be normalised; i.e.,
only lexical variants (e.g., tmrw “tomorrow”) should
be normalised and tokens that are OOV but other-
wise not lexical variants (e.g., iPad “iPad”) should
be unchanged. Most work in this area focuses only
on the normalisation task itself, oftentimes assuming
that the task of lexical variant detection has already
been completed.
Various approaches have been proposed to esti-
mate the error model, P(t|s). For example, in work
on spell-checking, Brill and Moore (2000) improve
on a standard edit-distance approach by consider-
ing multi-character edit operations; Toutanova and
Moore (2002) build on this by incorporating phono-
logical information. Li et al. (2006) utilise distri-
butional similarity (Lin, 1998) to correct misspelled
search queries.
In text message normalisation, Choudhury et al.
(2007) model the letter transformations and emis-
sions using a hidden Markov model (Rabiner, 1989).
Cook and Stevenson (2009) and Xue et al. (2011)
propose multiple simple error models, each of which
captures a particular way in which lexical variants
are formed, such as phonetic spelling (e.g., epik
“epic”) or clipping (e.g., walkin “walking”). Never-
theless, optimally weighting the various error mod-
els in these approaches is challenging.
Without pre-categorising lexical variants into dif-
ferent types, Liu et al. (2011a) collect Google
search snippets from carefully-designed queries
from which they then extract noisy lexical variant-
standard form pairs. These pairs are used to train
a conditional random field (Lafferty et al., 2001) to
estimate P(t|s) at the character level. One short-
coming of querying a search engine to obtain train-
ing pairs is it tends to be costly in terms of time and
bandwidth. Here we exploit microblog data directly
to derive (lexical variant, standard form) pairs, in-
stead of relying on external resources. In more-
recent work, Liu et al. (2012) endeavour to improve
the accuracy of top-n normalisation candidates by
integrating human cognitive inference, character-
level transformations and spell checking in their nor-
malisation model. The encouraging results shift the
focus to reranking and promoting the correct nor-
malisation to the top-1 position. However, like much
previous work on lexical normalisation, this work
</bodyText>
<page confidence="0.997698">
422
</page>
<bodyText confidence="0.991852772727273">
assumes perfect lexical variant detection.
Aw et al. (2006) and Kaufmann and Kalita (2010)
consider normalisation as a machine translation task
from lexical variants to standard forms using off-the-
shelf tools. These methods do not assume that lexi-
cal variants have been pre-identified; however, these
methods do rely on large quantities of labelled train-
ing data, which is not available for microblogs.
Recently, Han and Baldwin (2011) and Gouws
et al. (2011) propose two-step unsupervised ap-
proaches to normalisation, in which lexical vari-
ants are first identified, and then normalised. They
approach lexical variant detection by using a con-
text fitness classifier (Han and Baldwin, 2011) or
through dictionary lookup (Gouws et al., 2011).
However, the lexical variant detection of both meth-
ods is rather unreliable, indicating the challenge
of this aspect of normalisation. Both of these
approaches incorporate a relatively small normal-
isation dictionary to capture frequent lexical vari-
ants with high precision. In particular, Gouws et
al. (2011) produce a small normalisation lexicon
based on distributional similarity and string simi-
larity (Lodhi et al., 2002). Our method adopts a
similar strategy using distributional/string similarity,
but instead of constructing a small lexicon for pre-
processing, we build a much wider-coverage nor-
malisation dictionary and opt for a fully lexicon-
based end-to-end normalisation approach. In con-
trast to the normalisation dictionaries of Han and
Baldwin (2011) and Gouws et al. (2011) which fo-
cus on very frequent lexical variants, we focus on
moderate frequency lexical variants of a minimum
character length, which tend to have unambiguous
standard forms; our intention is to produce normali-
sation lexicons that are complementary to those cur-
rently available. Furthermore, we investigate the im-
pact of a variety of contextual and string similarity
measures on the quality of the resulting lexicons.
In summary, our dictionary-based normalisation ap-
proach is a lightweight end-to-end method which
performs both lexical variant detection and normal-
isation, and thus is suitable for practical online pre-
processing, despite its simplicity.
</bodyText>
<sectionHeader confidence="0.967257" genericHeader="method">
3 A Lexical Normalisation Dictionary
</sectionHeader>
<bodyText confidence="0.999896333333333">
Before discussing our method for creating a normal-
isation dictionary, we first discuss the feasibility of
such an approach.
</bodyText>
<subsectionHeader confidence="0.994973">
3.1 Feasibility
</subsectionHeader>
<bodyText confidence="0.999966757575757">
Dictionary lookup approaches to normalisation have
been shown to have high precision but low recall
(Han and Baldwin, 2011; Gouws et al., 2011). Fre-
quent (lexical variant, standard form) pairs such as
(u, you) are typically included in the dictionaries
used by such methods, while less-frequent items
such as (g0tta, gotta) are generally omitted. Be-
cause of the degree of lexical creativity and large
number of non-standard forms observed on Twitter,
a wide-coverage normalisation dictionary would be
expensive to construct manually. Based on the as-
sumption that lexical variants occur in similar con-
texts to their standard forms, however, it should
be possible to automatically construct a normalisa-
tion dictionary with wider coverage than is currently
available.
Dictionary lookup is a type-based approach to
normalisation, i.e., every token instance of a given
type will always be normalised in the same way.
However, lexical variants can be ambiguous, e.g., y
corresponds to “you” in yeah, y r right! LOL but
“why” in AM CONFUSED!!! y you did that? Nev-
ertheless, the relative occurrence of ambiguous lex-
ical variants is small (Liu et al., 2011a), and it has
been observed that while shorter variants such as y
are often ambiguous, longer variants tend to be un-
ambiguous. For example bthday and 4eva are un-
likely to have standard forms other than “birthday”
and “forever”, respectively. Therefore, the normali-
sation lexicons we produce will only contain entries
for OOVs with character length greater than a spec-
ified threshold, which are likely to have an unam-
biguous standard form.
</bodyText>
<subsectionHeader confidence="0.999912">
3.2 Overview of approach
</subsectionHeader>
<bodyText confidence="0.916279333333333">
Our method for constructing a normalisation dictio-
nary is as follows:
Input: Tokenised English tweets
</bodyText>
<listItem confidence="0.907767">
1. Extract (OOV, IV) pairs based on distributional
similarity.
</listItem>
<page confidence="0.992802">
423
</page>
<sectionHeader confidence="0.489447" genericHeader="method">
2. Re-rank the extracted pairs by string similarity.
</sectionHeader>
<bodyText confidence="0.999844642857143">
Output: A list of (OOV, IV) pairs ordered by string
similarity; select the top-n pairs for inclusion in
the normalisation lexicon.
In Step 1, we leverage large volumes of Twitter
data to identify the most distributionally-similar IV
type for each OOV type. The result of this pro-
cess is a set of (OOV, IV) pairs, ranked by dis-
tributional similarity. The extracted pairs will in-
clude (lexical variant, standard form) pairs, such as
(tmrw, tomorrow), but will also contain false posi-
tives such as (Tusday, Sunday) — Tusday is a lexical
variant, but its standard form is not “Sunday” — and
(Youtube, web) — Youtube is an OOV named en-
tity, not a lexical variant. Nevertheless, lexical vari-
ants are typically formed from their standard forms
through regular processes (Thurlow, 2003) — e.g.,
the omission of characters — and from this per-
spective Sunday and web are not plausible standard
forms for Tusday and Youtube, respectively. In Step
2, we therefore capture this intuition to re-rank the
extracted pairs by string similarity. The top-n items
in this re-ranked list then form the normalisation lex-
icon, which is based only on development data.
Although computationally-expensive to build,
this dictionary can be created offline. Once built,
it then offers a very fast approach to normalisation.
We can only reliably compute distributional simi-
larity for types that are moderately frequent in a cor-
pus. Nevertheless, many lexical variants are suffi-
ciently frequent to be able to compute distributional
similarity, and can potentially make their way into
our normalisation lexicon. This approach is not suit-
able for normalising low-frequency lexical variants,
nor is it suitable for shorter lexical variant types
which — as discussed in Section 3.1 — are more
likely to have an ambiguous standard form. Never-
theless, previously-proposed normalisation methods
that can handle such phenomena also rely in part on
a normalisation lexicon. The normalisation lexicons
we create can therefore be easily integrated with pre-
vious approaches to form hybrid normalisation sys-
tems.
</bodyText>
<sectionHeader confidence="0.988355" genericHeader="method">
4 Contextually-similar Pair Generation
</sectionHeader>
<bodyText confidence="0.9999352">
Our objective is to extract contextually-similar
(OOV, IV) pairs from a large-scale collection of mi-
croblog data. Fundamentally, the surrounding words
define the primary context, but there are different
ways of representing context and different similar-
ity measures we can use, which may influence the
quality of generated normalisation pairs.
In representing the context, we experimentally ex-
plore the following factors: (1) context window size
(from 1 to 3 tokens on both sides); (2) n-gram or-
der of the context tokens (unigram, bigram, trigram);
(3) whether context words are indexed for relative
position or not; and (4) whether we use all context
tokens, or only IV words. Because high-accuracy
linguistic processing tools for Twitter are still under
exploration (Liu et al., 2011b; Gimpel et al., 2011;
Ritter et al., 2011; Foster et al., 2011), we do not
consider richer representations of context, for exam-
ple, incorporating information about part-of-speech
tags or syntax. We also experiment with a number
of simple but widely-used geometric and informa-
tion theoretic distance/similarity measures. In par-
ticular, we use Kullback–Leibler (KL) divergence
(Kullback and Leibler, 1951), Jensen–Shannon (JS)
divergence (Lin, 1991), Euclidean distance and Co-
sine distance.
We use a corpus of 10 million English tweets to do
parameter tuning over, and a larger corpus of tweets
in the final candidate ranking. All tweets were col-
lected from September 2010 to January 2011 via
the Twitter API.1 From the raw data we extract
English tweets using a language identification tool
(Lui and Baldwin, 2011), and then apply a simpli-
fied Twitter tokeniser (adapted from O’Connor et al.
(2010)). We use the Aspell dictionary (v6.06)2 to
determine whether a word is IV, and only include
in our normalisation dictionary OOV tokens with
at least 64 occurrences in the corpus and character
length &gt; 4, both of which were determined through
empirical observation. For each OOV word type in
the corpus, we select the most similar IV type to
form (OOV, IV) pairs. To further narrow the search
space, we only consider IV words which are mor-
phophonemically similar to the OOV type, follow-
ing settings in Han and Baldwin (2011).3
</bodyText>
<footnote confidence="0.9972068">
1https://dev.twitter.com/docs/
streaming-api/methods
2http://aspell.net/
3We only consider IV words within an edit distance of 2 or a
phonemic edit distance of 1 from the OOV type, and we further
</footnote>
<page confidence="0.998323">
424
</page>
<bodyText confidence="0.99996944">
In order to evaluate the generated pairs, we ran-
domly selected 1000 OOV words from the 10 mil-
lion tweet corpus. We set up an annotation task
on Amazon Mechanical Turk,4 presenting five in-
dependent annotators with each word type (with no
context) and asking for corrections where appropri-
ate. For instance, given tmrw, the annotators would
likely identify it as a non-standard variant of “to-
morrow”. For correct OOV words like iPad, on the
other hand, we would expect them to leave the word
unchanged. If 3 or more of the 5 annotators make
the same suggestion (in the form of either a canoni-
cal spelling or leaving the word unchanged), we in-
clude this in our gold standard for evaluation. In
total, this resulted in 351 lexical variants and 282
correct OOV words, accounting for 63.3% of the
1000 OOV words. These 633 OOV words were used
as (OOV, IV) pairs for parameter tuning. The re-
mainder of the 1000 OOV words were ignored on
the grounds that there was not sufficient consensus
amongst the annotators.5
Contextually-similar pair generation aims to in-
clude as many correct normalisation pairs as pos-
sible. We evaluate the quality of the normalisation
pairs using “Cumulative Gain” (CG):
</bodyText>
<equation confidence="0.624187">
CG =
</equation>
<bodyText confidence="0.982787642857143">
Suppose there are N&apos; correct generated pairs
(oovi, ivi), each of which is weighted by rel&apos;i, the
frequency of oovi to indicate its relative importance;
for example, (thinkin, thinking) has a higher weight
than (g0tta, gotta) because thinkin is more frequent
than g0tta in our corpus. In this evaluation we don’t
consider the position of normalisation pairs, and nor
do we penalise incorrect pairs. Instead, we push dis-
tinguishing between correct and incorrect pairs into
the downstream re-ranking step in which we incor-
porate string similarity information.
Given the development data and CG, we run an
exhaustive search of parameter combinations over
only consider the top 30% most-frequent of these IV words.
</bodyText>
<footnote confidence="0.787524">
4https://www.mturk.com/mturk/welcome
</footnote>
<bodyText confidence="0.985930344827586">
5Note that the objective of this annotation task is to identify
lexical variants that have agreed-upon standard forms irrespec-
tive of context, as a special case of the more general task of
lexical normalisation (where context may or may not play a sig-
nificant role in the determination of the normalisation).
our development corpus. The five best parameter
combinations are shown in Table 1. We notice the
CG is almost identical for the top combinations. As
a context window size of 3 incurs a heavy process-
ing and memory overhead over a size of 2, we use
the 3rd-best parameter combination for subsequent
experiments, namely: context window of f2 tokens,
token bigrams, positional index, and KL divergence
as our distance measure.
To better understand the sensitivity of the method
to each parameter, we perform a post-hoc parame-
ter analysis relative to a default setting (as under-
lined in Table 2), altering one parameter at a time.
The results in Table 2 show that bigrams outper-
form other n-gram orders by a large margin (note
that the evaluation is based on a log scale), and
information-theoretic measures are superior to the
geometric measures. Furthermore, it also indicates
using the positional indexing better captures context.
However, there is little to distinguish context mod-
elling with just IV words or all tokens. Similarly,
the context window size has relatively little impact
on the overall performance, supporting our earlier
observation from Table 1.
</bodyText>
<sectionHeader confidence="0.97664" genericHeader="method">
5 Pair Re-ranking by String Similarity
</sectionHeader>
<bodyText confidence="0.999323952380953">
Once the contextually-similar (OOV, IV) pairs are
generated using the selected parameters in Section
4, we further re-rank this set of pairs in an at-
tempt to boost morphophonemically-similar pairs
like (bananaz, bananas), and penalise noisy pairs
like (paninis, beans).
Instead of using the small 10 million tweet cor-
pus, from this step onwards, we use a larger cor-
pus of 80 million English tweets (collected over the
same period as the development corpus) to develop
a larger-scale normalisation dictionary. This is be-
cause once pairs are generated, re-ranking based on
string comparison is much faster. We only include
in the dictionary OOV words with a token frequency
&gt; 15 to include more OOV types than in Section 4,
and again apply a minimum length cutoff of 4 char-
acters.
To measure how well our re-ranking method pro-
motes correct pairs and demotes incorrect pairs (in-
cluding both OOV words that should not be nor-
malised, e.g. (Youtube, web), and incorrect normal-
</bodyText>
<equation confidence="0.5911355">
N&apos;
�
i=1
rel&apos;i
</equation>
<page confidence="0.871043">
425
</page>
<table confidence="0.999728333333333">
Rank Window size n-gram Positional index? Lex. choice Sim/distance measure log(CG)
1 ±3 2 Yes All KL divergence 19.571
2 ±3 2 No All KL divergence 19.562
3 ±2 2 Yes All KL divergence 19.562
4 ±3 2 Yes IVs KL divergence 19.561
5 ±2 2 Yes IVs JS divergence 19.554
</table>
<tableCaption confidence="0.998566">
Table 1: The five best parameter combinations in the exhaustive search of parameter combinations
</tableCaption>
<table confidence="0.935688">
Window size n-gram Positional index? Lexical choice Similarity/distance measure
±1 19.325 1 19.328 Yes 19.328 IVs 19.335 KL divergence 19.328
±2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227
±3 19.328 3 19.324 JS divergence 19.311
Cosine 19.170
</table>
<tableCaption confidence="0.981663333333333">
Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs. We tune one parameter at
a time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in each
case is indicated in bold.
</tableCaption>
<bodyText confidence="0.9515776">
isations for lexical variants, e.g. (bcuz, cause)), we
modify our evaluation metric from Section 4 to
evaluate the ranking at different points, using Dis-
counted Cumulative Gain (DCG@N: Jarvelin and
Kekalainen (2002)):
</bodyText>
<equation confidence="0.993899">
DCG@N = rel1 +
</equation>
<bodyText confidence="0.999916673076923">
where reli again represents the frequency of the
OOV, but it can be gain (a positive number) or loss
(a negative number), depending on whether the ith
pair is correct or incorrect. Because we also expect
correct pairs to be ranked higher than incorrect pairs,
DCG@N takes both factors into account.
Given the generated pairs and the evaluation met-
ric, we first consider three baselines: no re-ranking
(i.e., the final ranking is that of the contextual simi-
larity scores), and re-rankings of the pairs based on
the frequencies of the OOVs in the Twitter corpus,
and the IV unigram frequencies in the Google Web
1T corpus (Brants and Franz, 2006) to get less-noisy
frequency estimates. We also compared a variety of
re-rankings based on a number of string similarity
measures that have been previously considered in
normalisation work (reviewed in Section 2). We ex-
periment with standard edit distance (Levenshtein,
1966), edit distance over double metaphone codes
(phonetic edit distance: (Philips, 2000)), longest
common subsequence ratio over the consonant edit
distance of the paired words (hereafter, denoted as
consonant edit distance: (Contractor et al., 2010)),
and a string subsequence kernel (Lodhi et al., 2002).
In Figure 1, we present the DCG@N results for
each of our ranking methods at different rank cut-
offs. Ranking by OOV frequency is motivated by
the assumption that lexical variants are frequently
used by social media users. This is confirmed
by our findings that lexical pairs like (goin, going)
and (nite, night) are at the top of the ranking.
However, many proper nouns and named entities
are also used frequently and ranked at the top,
mixed with lexical variants like (Facebook, speech)
and (Youtube, web). In ranking by IV word fre-
quency, we assume the lexical variants are usually
derived from frequently-used IV equivalents, e.g.
(abou, about). However, many less-frequent lexical
variant types have high-frequency (IV) normalisa-
tions. For instance, the highest-frequency IV word
the has more than 40 OOV lexical variants, such as
tthe and thhe. These less-frequent types occupy the
top positions, reducing the cumulative gain. Com-
pared with these two baselines, ranking by default
contextual similarity scores delivers promising re-
sults. It successfully ranks many more intuitive nor-
malisation pairs at the top, such as (2day, today)
and (wknd, weekend), but also ranks some incorrect
pairs highly, such as (needa, gotta).
The string similarity-based methods perform bet-
ter than our baselines in general. Through man-
ual analysis, we found that standard edit dis-
</bodyText>
<equation confidence="0.9897768">
N
L
i=2
reli
log2 (i)
</equation>
<page confidence="0.99381">
426
</page>
<bodyText confidence="0.999522791666667">
tance ranking is fairly accurate for lexical vari-
ants with low edit distance to their standard forms,
but fails to identify heavily-altered variants like
(tmrw, tomorrow). Consonant edit distance is simi-
lar to standard edit distance, but places many longer
words at the top of the ranking. Edit distance
over double metaphone codes (phonetic edit dis-
tance) performs particularly well for lexical vari-
ants that include character repetitions — commonly
used for emphasis on Twitter — because such rep-
etitions do not typically alter the phonetic codes.
Compared with the other methods, the string subse-
quence kernel delivers encouraging results. It mea-
sures common character subsequences of length n
between (OOV, IV) pairs. Because it is computa-
tionally expensive to calculate similarity for larger
n, we choose n=2, following Gouws et al. (2011).
As N (the lexicon size cut-off) increases, the per-
formance drops more slowly than the other meth-
ods. Although this method fails to rank heavily-
altered variants such as (4get, forget) highly, it typi-
cally works well for longer words. Given that we fo-
cus on longer OOVs (specifically those longer than
4 characters), this ultimately isn’t a great handicap.
</bodyText>
<sectionHeader confidence="0.999003" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.99996625">
Given the re-ranked pairs from Section 5, here we
apply them to a token-level normalisation task us-
ing the normalisation dataset of Han and Baldwin
(2011).
</bodyText>
<subsectionHeader confidence="0.994221">
6.1 Metrics
</subsectionHeader>
<bodyText confidence="0.999954384615385">
We evaluate using the standard evaluation metrics of
precision (P), recall (R) and F-score (F) as detailed
below. We also consider the false alarm rate (FA)
and word error rate (WER), also as shown below.
FA measures the negative effects of applying nor-
malisation; a good approach to normalisation should
not (incorrectly) normalise tokens that are already
in their standard form and do not require normalisa-
tion.6 WER, like F-score, shows the overall benefits
of normalisation, but unlike F-score, measures how
many token-level edits are required for the output to
be the same as the ground truth data. In general, dic-
tionaries with a high F-score/low WER and low FA
</bodyText>
<equation confidence="0.899694625">
6FA + P &lt; 1 because some lexical variants might be incor-
rectly normalised.
are preferable.
# correctly normalised tokens
P=
# normalised tokens
# correctly normalised tokens
R = # tokens requiring normalisation
= 2PR
F P + R
# incorrectly normalised tokens
FA =
# normalised tokens
# token edits needed after normalisation
WER =
# all tokens
</equation>
<subsectionHeader confidence="0.772111">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999812">
We select the three best re-ranking methods, and
best cut-off N for each method, based on the
highest DCG@N value for a given method over
the development data, as presented in Figure 1.
Namely, they are string subsequence kernel (S-dict,
N=40,000), double metaphone edit distance (DM-
dict, N=10,000) and default contextual similarity
without re-ranking (C-dict, N=10,000).7
We evaluate each of the learned dictionaries in Ta-
ble 3. We also compare each dictionary with the
performance of the manually-constructed Internet
slang dictionary (HB-dict) used by Han and Bald-
win (2011), the small automatically-derived dictio-
nary of Gouws et al. (2011) (GHM-dict), and com-
binations of the different dictionaries. In addition,
the contribution of these dictionaries in hybrid nor-
malisation approaches is also presented, in which we
first normalise OOVs using a given dictionary (com-
bined or otherwise), and then apply the normalisa-
tion method of Gouws et al. (2011) based on con-
sonant edit distance (GHM-norm), or the approach
of Han and Baldwin (2011) based on the summation
of many unsupervised approaches (HB-norm), to the
remaining OOVs. Results are shown in Table 3, and
discussed below.
</bodyText>
<subsectionHeader confidence="0.618118">
6.2.1 Individual Dictionaries
</subsectionHeader>
<bodyText confidence="0.9887415">
Overall, the individual dictionaries derived by the
re-ranking methods (DM-dict, S-dict) perform bet-
</bodyText>
<footnote confidence="0.92102025">
7We also experimented with combining ranks using Mean
Reciprocal Rank. However, the combined rank didn’t improve
performance on the development data. We plan to explore other
ranking aggregation methods in future work.
</footnote>
<page confidence="0.995397">
427
</page>
<figure confidence="0.9992875">
40K
20K
0
−20K
−40K
−60K
</figure>
<figureCaption confidence="0.999923">
Figure 1: Re-ranking based on different string similarity methods.
</figureCaption>
<bodyText confidence="0.9998894">
ter than that based on contextual similarity (C-dict)
in terms of precision and false alarm rate, indicating
the importance of re-ranking. Even though C-dict
delivers higher recall — indicating that many lexi-
cal variants are correctly normalised — this is offset
by its high false alarm rate, which is particularly un-
desirable in normalisation. Because S-dict has better
performance than DM-dict in terms of both F-score
and WER, and a much lower false alarm rate than
C-dict, subsequent results are presented using S-dict
only.
Both HB-dict and GHM-dict achieve better than
90% precision with moderate recall. Compared to
these methods, S-dict is not competitive in terms of
either precision or recall. This result seems rather
discouraging. However, considering that S-dict is an
automatically-constructed dictionary targeting lexi-
cal variants of varying frequency, it is not surprising
that the precision is worse than that of HB-dict —
which is manually-constructed — and GHM-dict —
which includes entries only for more-frequent OOVs
for which distributional similarity is more accurate.
Additionally, the recall of S-dict is hampered by the
restriction on lexical variant token length of 4 char-
acters.
</bodyText>
<subsectionHeader confidence="0.689109">
6.2.2 Combined Dictionaries
</subsectionHeader>
<bodyText confidence="0.999858473684211">
Next we look to combining HB-dict, GHM-dict
and S-dict. In combining the dictionaries, a given
OOV word can be listed with different standard
forms in different dictionaries. In such cases we use
the following preferences for dictionaries — moti-
vated by our confidence in the normalisation pairs
of the dictionaries — to resolve conflicts: HB-dict
&gt; GHM-dict &gt; S-dict.
When we combine dictionaries in the second sec-
tion of Table 3, we find that they contain com-
plementary information: in each case the recall
and F-score are higher for the combined dictio-
nary than any of the individual dictionaries. The
combination of HB-dict+GHM-dict produces only
a small improvement in terms of F-score over HB-
dict (the better-performing dictionary) suggesting
that, as claimed, HB-dict and GHM-dict share many
frequent normalisation pairs. HB-dict+S-dict and
GHM-dict+S-dict, on the other hand, improve sub-
</bodyText>
<equation confidence="0.482114">
D
</equation>
<page confidence="0.940652">
428
</page>
<table confidence="0.999653642857143">
Method Precision Recall F-Score False Alarm Word Error Rate
C-dict 0.474 0.218 0.299 0.298 0.103
DM-dict 0.727 0.106 0.185 0.145 0.102
S-dict 0.700 0.179 0.285 0.162 0.097
HB-dict 0.915 0.435 0.590 0.048 0.066
GHM-dict 0.982 0.319 0.482 0.000 0.076
HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052
GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061
HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063
HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049
GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135
HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124
HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081
HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079
</table>
<tableCaption confidence="0.952417">
Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone ren-
dering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al. (2011) (GHM-dict), the
Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition,
we combine the dictionaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined
unsupervised approach of Han and Baldwin (2011) (HB-norm).
</tableCaption>
<bodyText confidence="0.999709181818182">
stantially over HB-dict and GHM-dict, respectively,
indicating that S-dict contains markedly different
entries to both HB-dict and GHM-dict. The best F-
score and WER are obtained using the combination
of all three dictionaries, HB-dict+GHM-dict+S-dict.
Furthermore, the difference between the results us-
ing HB-dict+GHM-dict+S-dict and HB-dict+GHM-
dict is statistically significant (p &lt; 0.01), based on
the computationally-intensive Monte Carlo method
of Yeh (2000), demonstrating the contribution of S-
dict.
</bodyText>
<subsectionHeader confidence="0.86905">
6.2.3 Hybrid Approaches
</subsectionHeader>
<bodyText confidence="0.988060615384615">
The methods of Gouws et al. (2011) (i.e.
GHM-dict+GHM-norm) and Han and Baldwin
(2011) (i.e. HB-dict+HB-norm) have lower preci-
sion and higher false alarm rates than the dictionary-
based approaches; this is largely caused by lex-
ical variant detection errors.8 Using all dic-
tionaries in combination with these methods —
HB-dict+GHM-dict+S-dict+GHM-norm and HB-
dict+GHM-dict+S-dict+HB-norm — gives some
improvements, but the false alarm rates remain high.
Despite the limitations of a pure dictionary-based
approach to normalisation — discussed in Section
3.1 — the current best practical approach to normal-
</bodyText>
<footnote confidence="0.800513333333333">
8Here we report results that do not assume perfect detection
of lexical variants, unlike the original published results in each
case.
</footnote>
<figure confidence="0.907437375">
Error type OOV Standard form
Dict. Gold
(a) plurals playe players player
(b) negation unlike like dislike
(c)possessives anyones anyone anyone’s
(d) correct OOVs iphone phone iphone
(e) test data errors durin during durin
(f) ambiguity siging signing singing
</figure>
<tableCaption confidence="0.952072">
Table 4: Error types in the combined dictionary (HB-
dict+GHM-dict+S-dict)
</tableCaption>
<bodyText confidence="0.942019">
isation is to use a lexicon, combining hand-built and
automatically-learned normalisation dictionaries.
</bodyText>
<subsectionHeader confidence="0.997742">
6.3 Discussion and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999832846153846">
We first manually analyse the errors in the combined
dictionary (HB-dict+GHM-dict+S-dict) and give ex-
amples of each error type in Table 4. The most fre-
quent word errors are caused by slight morphologi-
cal variations, including plural forms (a), negations
(b), possessive cases (c), and OOVs that are correct
and do not require normalisation (d). In addition, we
also notice some missing annotations where lexical
variants are skipped by human annotations but cap-
tured by our method (e). Ambiguity (f) definitely
exists in longer OOVs, however, these cases do not
appear to have a strong negative impact on the nor-
malisation performance. An example of a remain-
</bodyText>
<page confidence="0.996673">
429
</page>
<table confidence="0.9985682">
Length cut-off (N) #Variants Precision Recall (&gt; N) Recall (all) False Alarm
&gt;4 556 0.700 0.381 0.179 0.162
&gt;5 382 0.814 0.471 0.152 0.122
&gt;6 254 0.804 0.484 0.104 0.131
&gt;7 138 0.793 0.471 0.055 0.122
</table>
<tableCaption confidence="0.704852">
Table 5: S-dict normalisation results broken down according to OOV token length. Recall is presented both over the
subset of instances of length &gt; N in the data (“Recall (&gt; N)”), and over the entirety of the dataset (“Recall (all)”);
“#Variants” is the number of token instances of the indicated length in the test dataset.
</tableCaption>
<bodyText confidence="0.9998976">
ing miscellaneous error is bday “birthday”, which is
mis-normalised as day.
To further study the influence of OOV word
length relative to the normalisation performance, we
conduct a fine-grained analysis of the performance
of the derived dictionary (S-dict) in Table 5, bro-
ken down across different OOV word lengths. The
results generally support our hypothesis that our
method works better for longer OOV words. The
derived dictionary is much more reliable for longer
tokens (length 5, 6, and 7 characters) in terms of pre-
cision and false alarm. Although the recall is rela-
tively modest, in the future we intend to improve re-
call by mining more normalisation pairs from larger
collections of microblog data.
</bodyText>
<sectionHeader confidence="0.998001" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999656">
In this paper, we describe a method for automat-
ically constructing a normalisation dictionary that
supports normalisation of microblog text through di-
rect substitution of lexical variants with their stan-
dard forms. After investigating the impact of dif-
ferent distributional and string similarity methods
on the quality of the dictionary, we present ex-
perimental results on a standard dataset showing
that our proposed methods acquire high quality
(lexical variant, standard form) pairs, with reason-
able coverage, and achieve state-of-the-art end-to-
end lexical normalisation performance on a real-
world token-level task. Furthermore, this dictionary-
lookup method combines the detection and normali-
sation of lexical variants into a simple, lightweight
solution which is suitable for processing of high-
volume microblog feeds.
In the future, we intend to improve our dictionary
by leveraging the constantly-growing volume of mi-
croblog data, and considering alternative ways to
combine distributional and string similarity. In addi-
tion to direct evaluation, we also want to explore the
benefits of applying normalisation for downstream
social media text processing applications, e.g. event
detection.
</bodyText>
<sectionHeader confidence="0.996279" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999947">
We would like to thank the three anonymous re-
viewers for their insightful comments, and Stephan
Gouws for kindly sharing his data and discussing his
work.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT centre of Excel-
lence programme.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999141863636364">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING/ACL 2006, pages
33–40, Sydney, Australia.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT 2011), pages 389–398, Port-
land, Oregon, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 286–293,
Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157–174.
</reference>
<page confidence="0.989917">
430
</page>
<reference confidence="0.999720943925234">
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
pages 189–196, Beijing, China.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ’09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71–
78, Boulder, USA.
Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner,
Joseph L. Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS Tagging and Parsing the Twitterverse.
In Analyzing Microtext: Papers from the 2011 AAAI
Workshop, volume WS-11-05 of AAAI Workshops,
pages 20–25, San Francisco, CA, USA.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 42–47,
Portland, Oregon, USA.
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in Twitter:
a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-HLT
2011), pages 581–586, Portland, Oregon, USA.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82–90, Edinburgh,
Scotland, UK.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 368–378,
Portland, Oregon, USA.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
151–160, Portland, Oregon, USA.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22:49–
86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282–289, San Fran-
cisco, CA, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707–710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of COL-
ING/ACL 2006, pages 1025–1032, Sydney, Australia.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145–151.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 36th An-
nual Meeting of the ACL and 17th International Con-
ference on Computational Linguistics (COLING/ACL-
98), pages 768–774, Montreal, Quebec, Canada.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
71–76, Portland, Oregon, USA.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 359–367,
Portland, Oregon, USA.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012), Jeju, Republic of Korea.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. J. Mach. Learn. Res., 2:419–
444.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553–561, Chiang Mai, Thailand.
</reference>
<page confidence="0.984094">
431
</page>
<reference confidence="0.999725115384615">
Brendan O’Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for Twitter. In Proceedings of the 4th In-
ternational Conference on Weblogs and Social Media
(ICWSM 2010), pages 384–385, Washington, USA.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38–43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2010), pages 172–180, Los Angeles,
USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 1524–1534, Edinburgh,
Scotland, UK.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on the World Wide
Web (WWW 2010), pages 851–860, Raleigh, North
Carolina, USA.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people’s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the
ACL and 3rd Annual Meeting of the NAACL (ACL-02),
pages 144–151, Philadelphia, USA.
Official Blog Twitter. 2011. 200 million tweets per day.
Retrived at August 17th, 2011.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the 5th International
Conference on Weblogs and Social Media (ICWSM
2011), Barcelona, Spain.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74–79,
San Francisco, USA.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), pages 947–953,
Saarbr¨ucken, Germany.
</reference>
<page confidence="0.998549">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999915">Automatically Constructing a Normalisation Dictionary for Microblogs</title>
<author confidence="0.750895">Paul</author>
<author confidence="0.750895">Victoria Research</author>
<affiliation confidence="0.705896">of Computing and Information Systems, The University of</affiliation>
<email confidence="0.823374">tb@ldwin.net</email>
<abstract confidence="0.991965794117647">Microblog normalisation methods often utilise complex models and struggle to differentiate between correctly-spelled unknown words and lexical variants of known words. In this paper, we propose a method for constructing a dictionary of lexical variants of known words that facilitates lexical normalisation via string substitution (e.g. We use context information to generate possible variant and normalisation pairs and then rank these by string similarity. Highlyranked pairs are selected to populate the dictionary. We show that a dictionary-based approach achieves state-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many forms — e.g., and — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; et al., 2011). For example, u 2morw!!! be normalised to you tomorrow! normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For exand all attested in a Twitter corpus — have the standard by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-to-one normalisation in which one OOV token is normalised to one IV word. Naturally, not all OOV words in microblogs are lexical variants of IV words: named entities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One chalfor lexical normalisation is therefore to dis-</abstract>
<note confidence="0.917749333333333">421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural pages 421–432, Jeju Island, Korea, 12–14 July 2012. Association for Computational Linguistics</note>
<abstract confidence="0.979798973846155">tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, in this paper we propose methods for automatically constructing a lexical normalisation dictionary — a dictionary whose entries consist variant, standard — that enables type-based normalisation. Despite the simplicity of this dictionary-based normalisation method, we show it to outperform previously-proposed approaches. This very fast, lightweight solution is suitable for real-time processing of the large volume of streaming microblog data available from Twitter, and offers a simple solution to the lexical variant detection problem that hinders other normalisation methods. Furthermore, this dictionary-based method can be easily integrated with other more-complex normalisation approaches (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011) to produce hybrid systems. After discussing related work in Section 2, we present an overview of our dictionary-based approach to normalisation in Section 3. In Sections 4 and 5 we experimentally select the optimised context similarity parameters and string similarity reranking method. We present experimental results on the unseen test data in Section 6, and offer some concluding remarks in Section 7. 2 Related Work a token lexical normalisation is the task finding the standard form, i.e., an IV word. Stanin lexical normalisation, assumed to be an OOV token, relative to a fixed dictionary. In practice, not all OOV tokens should be normalised; i.e., lexical variants (e.g., should be normalised and tokens that are OOV but othernot lexical variants (e.g., should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estithe error model, example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants formed, such as phonetic spelling (e.g., or clipping (e.g., Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to the character level. One shortcoming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly derive variant, standard instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve accuracy of candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct norto the However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al., 2011). However, the lexical variant detection of both methods is rather unreliable, indicating the challenge of this aspect of normalisation. Both of these approaches incorporate a relatively small normalisation dictionary to capture frequent lexical variants with high precision. In particular, Gouws et al. (2011) produce a small normalisation lexicon based on distributional similarity and string similarity (Lodhi et al., 2002). Our method adopts a similar strategy using distributional/string similarity, but instead of constructing a small lexicon for preprocessing, we build a much wider-coverage normalisation dictionary and opt for a fully lexiconbased end-to-end normalisation approach. In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al. (2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available. Furthermore, we investigate the impact of a variety of contextual and string similarity measures on the quality of the resulting lexicons. In summary, our dictionary-based normalisation approach is a lightweight end-to-end method which performs both lexical variant detection and normalisation, and thus is suitable for practical online preprocessing, despite its simplicity. 3 A Lexical Normalisation Dictionary Before discussing our method for creating a normalisation dictionary, we first discuss the feasibility of such an approach. 3.1 Feasibility Dictionary lookup approaches to normalisation have been shown to have high precision but low recall (Han and Baldwin, 2011; Gouws et al., 2011). Fresuch as typically included in the dictionaries used by such methods, while less-frequent items as generally omitted. Because of the degree of lexical creativity and large number of non-standard forms observed on Twitter, a wide-coverage normalisation dictionary would be expensive to construct manually. Based on the assumption that lexical variants occur in similar contexts to their standard forms, however, it should be possible to automatically construct a normalisation dictionary with wider coverage than is currently available. Dictionary lookup is a type-based approach to normalisation, i.e., every token instance of a given type will always be normalised in the same way. lexical variants can be ambiguous, e.g., to “you” in y r right! LOL in CONFUSED!!! y you did that? Nevertheless, the relative occurrence of ambiguous lexical variants is small (Liu et al., 2011a), and it has observed that while shorter variants such as are often ambiguous, longer variants tend to be un- For example unlikely to have standard forms other than “birthday” and “forever”, respectively. Therefore, the normalisation lexicons we produce will only contain entries for OOVs with character length greater than a specified threshold, which are likely to have an unambiguous standard form. 3.2 Overview of approach Our method for constructing a normalisation dictionary is as follows: English tweets based on distributional similarity. 423 the extracted pairs by string similarity. list of ordered by string similarity; select the top-n pairs for inclusion in the normalisation lexicon. In Step 1, we leverage large volumes of Twitter data to identify the most distributionally-similar IV type for each OOV type. The result of this prois a set of ranked by distributional similarity. The extracted pairs will invariant, standard such as will also contain false posisuch as — a lexical variant, but its standard form is not “Sunday” — and — an OOV named entity, not a lexical variant. Nevertheless, lexical variants are typically formed from their standard forms through regular processes (Thurlow, 2003) — e.g., the omission of characters — and from this pernot plausible standard for respectively. In Step 2, we therefore capture this intuition to re-rank the extracted pairs by string similarity. The top-n items in this re-ranked list then form the normalisation lexicon, which is based only on development data. Although computationally-expensive to build, this dictionary can be created offline. Once built, it then offers a very fast approach to normalisation. We can only reliably compute distributional similarity for types that are moderately frequent in a corpus. Nevertheless, many lexical variants are sufficiently frequent to be able to compute distributional similarity, and can potentially make their way into our normalisation lexicon. This approach is not suitable for normalising low-frequency lexical variants, nor is it suitable for shorter lexical variant types which — as discussed in Section 3.1 — are more likely to have an ambiguous standard form. Nevertheless, previously-proposed normalisation methods that can handle such phenomena also rely in part on a normalisation lexicon. The normalisation lexicons we create can therefore be easily integrated with previous approaches to form hybrid normalisation systems. 4 Contextually-similar Pair Generation Our objective is to extract contextually-similar from a large-scale collection of microblog data. Fundamentally, the surrounding words define the primary context, but there are different ways of representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via Twitter From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. We use the Aspell dictionary to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, followsettings in Han and Baldwin streaming-api/methods only consider IV words within an edit distance of 2 or a phonemic edit distance of 1 from the OOV type, and we further 424 In order to evaluate the generated pairs, we randomly selected 1000 OOV words from the 10 million tweet corpus. We set up an annotation task Amazon Mechanical five independent annotators with each word type (with no context) and asking for corrections where appropri- For instance, given the annotators would likely identify it as a non-standard variant of “to- For correct OOV words like on the other hand, we would expect them to leave the word unchanged. If 3 or more of the 5 annotators make the same suggestion (in the form of either a canonical spelling or leaving the word unchanged), we include this in our gold standard for evaluation. In total, this resulted in 351 lexical variants and 282 correct OOV words, accounting for 63.3% of the 1000 OOV words. These 633 OOV words were used for parameter tuning. The remainder of the 1000 OOV words were ignored on the grounds that there was not sufficient consensus the Contextually-similar pair generation aims to include as many correct normalisation pairs as possible. We evaluate the quality of the normalisation pairs using “Cumulative Gain” (CG): there are generated pairs each of which is weighted by the of indicate its relative importance; example, a higher weight more frequent our corpus. In this evaluation we don’t consider the position of normalisation pairs, and nor do we penalise incorrect pairs. Instead, we push distinguishing between correct and incorrect pairs into the downstream re-ranking step in which we incorporate string similarity information. Given the development data and CG, we run an exhaustive search of parameter combinations over only consider the top 30% most-frequent of these IV words. that the objective of this annotation task is to identify lexical variants that have agreed-upon standard forms irrespective of context, as a special case of the more general task of lexical normalisation (where context may or may not play a significant role in the determination of the normalisation). our development corpus. The five best parameter combinations are shown in Table 1. We notice the CG is almost identical for the top combinations. As a context window size of 3 incurs a heavy processing and memory overhead over a size of 2, we use the 3rd-best parameter combination for subsequent namely: context window of token bigrams, positional index, and KL divergence as our distance measure. To better understand the sensitivity of the method to each parameter, we perform a post-hoc parameter analysis relative to a default setting (as underlined in Table 2), altering one parameter at a time. The results in Table 2 show that bigrams outperother orders by a large margin (note the evaluation is based on a and information-theoretic measures are superior to the geometric measures. Furthermore, it also indicates using the positional indexing better captures context. However, there is little to distinguish context modelling with just IV words or all tokens. Similarly, the context window size has relatively little impact on the overall performance, supporting our earlier observation from Table 1. 5 Pair Re-ranking by String Similarity the contextually-similar are generated using the selected parameters in Section 4, we further re-rank this set of pairs in an attempt to boost morphophonemically-similar pairs and penalise noisy pairs Instead of using the small 10 million tweet corpus, from this step onwards, we use a larger corpus of 80 million English tweets (collected over the same period as the development corpus) to develop a larger-scale normalisation dictionary. This is because once pairs are generated, re-ranking based on string comparison is much faster. We only include in the dictionary OOV words with a token frequency include more OOV types than in Section 4, and again apply a minimum length cutoff of 4 characters. To measure how well our re-ranking method promotes correct pairs and demotes incorrect pairs (including both OOV words that should not be nore.g. and incorrect normal- � 425 Rank Window size Positional index? Lex. choice Sim/distance measure 1 2 Yes All KL divergence 19.571 2 2 No All KL divergence 19.562 3 2 Yes All KL divergence 19.562 4 2 Yes IVs KL divergence 19.561 5 2 Yes IVs JS divergence 19.554 Table 1: The five best parameter combinations in the exhaustive search of parameter combinations size Positional index? Lexical choice Similarity/distance measure 19.325 1 19.328Yes 19.328IVs divergence 19.328 19.327 2 19.263 All 19.328Euclidean 19.227 19.3283 19.324 JS divergence 19.311 Cosine 19.170 2: Parameter sensitivity analysis measured as correctly-generated pairs. We tune one parameter at time, using the default (underlined)setting for other parameters; the non-exhaustive best-performing setting in each is indicated in for lexical variants, e.g. we modify our evaluation metric from Section 4 to the different points, using Dis- Cumulative Gain Jarvelin and Kekalainen (2002)): represents the frequency of the OOV, but it can be gain (a positive number) or loss negative number), depending on whether the pair is correct or incorrect. Because we also expect correct pairs to be ranked higher than incorrect pairs, both factors into account. Given the generated pairs and the evaluation metric, we first consider three baselines: no re-ranking (i.e., the final ranking is that of the contextual similarity scores), and re-rankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). Figure 1, we present the for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed our findings that lexical pairs like at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, with lexical variants like In ranking by IV word frequency, we assume the lexical variants are usually derived from frequently-used IV equivalents, e.g. However, many less-frequent lexical variant types have high-frequency (IV) normalisations. For instance, the highest-frequency IV word more than 40 OOV lexical variants, such as These less-frequent types occupy the top positions, reducing the cumulative gain. Compared with these two baselines, ranking by default contextual similarity scores delivers promising results. It successfully ranks many more intuitive norpairs at the top, such as but also ranks some incorrect highly, such as The string similarity-based methods perform better than our baselines in general. Through mananalysis, we found that standard edit dis- N L 426 tance ranking is fairly accurate for lexical variants with low edit distance to their standard forms, but fails to identify heavily-altered variants like Consonant edit distance is similar to standard edit distance, but places many longer words at the top of the ranking. Edit distance over double metaphone codes (phonetic edit distance) performs particularly well for lexical variants that include character repetitions — commonly used for emphasis on Twitter — because such repetitions do not typically alter the phonetic codes. Compared with the other methods, the string subsequence kernel delivers encouraging results. It measures common character subsequences of length n Because it is computationally expensive to calculate similarity for larger n, we choose n=2, following Gouws et al. (2011). As N (the lexicon size cut-off) increases, the performance drops more slowly than the other methods. Although this method fails to rank heavilyvariants such as it typically works well for longer words. Given that we focus on longer OOVs (specifically those longer than 4 characters), this ultimately isn’t a great handicap. 6 Evaluation Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). 6.1 Metrics We evaluate using the standard evaluation metrics of precision (P), recall (R) and F-score (F) as detailed below. We also consider the false alarm rate (FA) and word error rate (WER), also as shown below. FA measures the negative effects of applying normalisation; a good approach to normalisation should not (incorrectly) normalise tokens that are already in their standard form and do not require normalisa- WER, like F-score, shows the overall benefits of normalisation, but unlike F-score, measures how many token-level edits are required for the output to be the same as the ground truth data. In general, dictionaries with a high F-score/low WER and low FA + P because some lexical variants might be incorrectly normalised. are preferable. tokens requiring normalisation P FA WER 6.2 Results We select the three best re-ranking methods, and best cut-off N for each method, based on the highest DCG@N value for a given method over the development data, as presented in Figure 1. Namely, they are string subsequence kernel (S-dict, N=40,000), double metaphone edit distance (DMdict, N=10,000) and default contextual similarity re-ranking (C-dict, We evaluate each of the learned dictionaries in Table 3. We also compare each dictionary with the performance of the manually-constructed Internet slang dictionary (HB-dict) used by Han and Baldwin (2011), the small automatically-derived dictionary of Gouws et al. (2011) (GHM-dict), and combinations of the different dictionaries. In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al. (2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. Results are shown in Table 3, and discussed below. 6.2.1 Individual Dictionaries Overall, the individual dictionaries derived by the methods (DM-dict, S-dict) perform betalso experimented with combining ranks using Mean Reciprocal Rank. However, the combined rank didn’t improve performance on the development data. We plan to explore other ranking aggregation methods in future work. 427 40K20K0−20K−40K−60K Figure 1: Re-ranking based on different string similarity methods. ter than that based on contextual similarity (C-dict) in terms of precision and false alarm rate, indicating the importance of re-ranking. Even though C-dict delivers higher recall — indicating that many lexical variants are correctly normalised — this is offset by its high false alarm rate, which is particularly undesirable in normalisation. Because S-dict has better performance than DM-dict in terms of both F-score and WER, and a much lower false alarm rate than C-dict, subsequent results are presented using S-dict only. Both HB-dict and GHM-dict achieve better than 90% precision with moderate recall. Compared to these methods, S-dict is not competitive in terms of either precision or recall. This result seems rather discouraging. However, considering that S-dict is an automatically-constructed dictionary targeting lexical variants of varying frequency, it is not surprising that the precision is worse than that of HB-dict — which is manually-constructed — and GHM-dict — which includes entries only for more-frequent OOVs for which distributional similarity is more accurate. Additionally, the recall of S-dict is hampered by the restriction on lexical variant token length of 4 characters. 6.2.2 Combined Dictionaries Next we look to combining HB-dict, GHM-dict and S-dict. In combining the dictionaries, a given OOV word can be listed with different standard forms in different dictionaries. In such cases we use the following preferences for dictionaries — motivated by our confidence in the normalisation pairs of the dictionaries — to resolve conflicts: HB-dict When we combine dictionaries in the second section of Table 3, we find that they contain complementary information: in each case the recall and F-score are higher for the combined dictionary than any of the individual dictionaries. The combination of HB-dict+GHM-dict produces only a small improvement in terms of F-score over HBdict (the better-performing dictionary) suggesting that, as claimed, HB-dict and GHM-dict share many frequent normalisation pairs. HB-dict+S-dict and on the other hand, improve sub- D 428 Method Precision Recall F-Score False Alarm Word Error Rate C-dict 0.474 0.218 0.299 0.298 0.103 DM-dict 0.727 0.106 0.185 0.145 0.102 S-dict 0.700 0.179 0.285 0.162 0.097 HB-dict 0.915 0.435 0.590 0.048 0.066 GHM-dict 0.982 0.319 0.482 0.000 0.076 HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052 GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061 HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063 HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049 GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135 HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124 HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081 HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079 Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone rendering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al. (2011) (GHM-dict), the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition, we combine the dictionaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMis statistically significant &lt; based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. 6.2.3 Hybrid Approaches The methods of Gouws et al. (2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionaryapproaches; this is largely caused by lexvariant detection Using all tionaries in combination with these methods — HB-dict+GHM-dict+S-dict+GHM-norm and HBdict+GHM-dict+S-dict+HB-norm — gives some improvements, but the false alarm rates remain high. Despite the limitations of a pure dictionary-based approach to normalisation — discussed in Section — the current best practical approach to normalwe report results that do not assume perfect detection of lexical variants, unlike the original published results in each case. Error type OOV Standard form Dict. Gold (a) plurals playe players player (b) negation unlike like dislike (c)possessives anyones anyone anyone’s (d) correct OOVs iphone phone iphone (e) test data errors durin during durin (f) ambiguity siging signing singing Table 4: Error types in the combined dictionary (HBdict+GHM-dict+S-dict) isation is to use a lexicon, combining hand-built and automatically-learned normalisation dictionaries. 6.3 Discussion and Error Analysis We first manually analyse the errors in the combined dictionary (HB-dict+GHM-dict+S-dict) and give examples of each error type in Table 4. The most frequent word errors are caused by slight morphological variations, including plural forms (a), negations (b), possessive cases (c), and OOVs that are correct and do not require normalisation (d). In addition, we also notice some missing annotations where lexical variants are skipped by human annotations but captured by our method (e). Ambiguity (f) definitely exists in longer OOVs, however, these cases do not appear to have a strong negative impact on the norperformance. An example of a remain- 429 cut-off #Variants Precision Recall (all) False Alarm</abstract>
<address confidence="0.712157">556 0.700 0.381 0.179 0.162 382 0.814 0.471 0.152 0.122 254 0.804 0.484 0.104 0.131</address>
<phone confidence="0.698263">138 0.793 0.471 0.055 0.122</phone>
<abstract confidence="0.997544203703703">Table 5: S-dict normalisation results broken down according to OOV token length. Recall is presented both over the of instances of length the data (“Recall and over the entirety of the dataset (“Recall (all)”); “#Variants” is the number of token instances of the indicated length in the test dataset. miscellaneous error is which is as To further study the influence of OOV word length relative to the normalisation performance, we conduct a fine-grained analysis of the performance of the derived dictionary (S-dict) in Table 5, broken down across different OOV word lengths. The results generally support our hypothesis that our method works better for longer OOV words. The derived dictionary is much more reliable for longer tokens (length 5, 6, and 7 characters) in terms of precision and false alarm. Although the recall is relatively modest, in the future we intend to improve recall by mining more normalisation pairs from larger collections of microblog data. 7 Conclusions and Future Work In this paper, we describe a method for automatically constructing a normalisation dictionary that supports normalisation of microblog text through direct substitution of lexical variants with their standard forms. After investigating the impact of different distributional and string similarity methods on the quality of the dictionary, we present experimental results on a standard dataset showing that our proposed methods acquire high quality with reasonable coverage, and achieve state-of-the-art end-toend lexical normalisation performance on a realworld token-level task. Furthermore, this dictionarylookup method combines the detection and normalisation of lexical variants into a simple, lightweight solution which is suitable for processing of highvolume microblog feeds. In the future, we intend to improve our dictionary by leveraging the constantly-growing volume of microblog data, and considering alternative ways to combine distributional and string similarity. In addition to direct evaluation, we also want to explore the benefits of applying normalisation for downstream social media text processing applications, e.g. event detection. Acknowledgements We would like to thank the three anonymous reviewers for their insightful comments, and Stephan Gouws for kindly sharing his data and discussing his work. NICTA is funded by the Australian government as represented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT centre of Excellence programme.</abstract>
<note confidence="0.782680424242424">References AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normal- In of COLING/ACL pages 33–40, Sydney, Australia. Edward Benson, Aria Haghighi, and Regina Barzilay. Event discovery in social media feeds. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language (ACL-HLT pages 389–398, Portland, Oregon, USA. Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting of the Associfor Computational pages 286–293, Hong Kong. Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of language. Journal on Document and 10:157–174. 430 Danish Contractor, Tanveer A. Faruquie, and L. Venkata Subramaniam. 2010. Unsupervised cleansing of noisy In of the 23rd International Conferon Computational Linguistics (COLING pages 189–196, Beijing, China. Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In CALC ’09: Proceedings of the Workshop on Computa- Approaches to Linguistic pages 71–</note>
<address confidence="0.972885">78, Boulder, USA.</address>
<author confidence="0.967265">Joachim Wagner Foster</author>
<author confidence="0.967265">Joseph L Roux</author>
<author confidence="0.967265">Stephen Hogan</author>
<author confidence="0.967265">Joakim Nivre</author>
<address confidence="0.484406">Deirdre Hogan, and Josef van Genabith. 2011. #hard-</address>
<note confidence="0.478001">toparse: POS Tagging and Parsing the Twitterverse. Microtext: Papers from the 2011 AAAI volume WS-11-05 of pages 20–25, San Francisco, CA, USA.</note>
<author confidence="0.68196575">Part-of-speech tagging for</author>
<note confidence="0.779879733333333">Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan- Technologies (ACL-HLT pages 42–47, Portland, Oregon, USA. Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in Twitter: closer look. In of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT pages 581–586, Portland, Oregon, USA. Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011. Unsupervised mining of lexical variants from noisy In of the First workshop on Unsu- Learning in pages 82–90, Edinburgh,</note>
<address confidence="0.374094">Scotland, UK. Bo Han and Timothy Baldwin. 2011. Lexical normal-</address>
<abstract confidence="0.636814">isation of short text messages: Makn sens a #twitter. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-</abstract>
<address confidence="0.723032">Technologies (ACL-HLT pages 368–378, Portland, Oregon, USA.</address>
<abstract confidence="0.794249571428571">K. Jarvelin and J. Kekalainen. 2002. Cumulated gainevaluation of IR techniques. Transactions Information 20(4). Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment clas- In of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-</abstract>
<affiliation confidence="0.750273">Language Technologies (ACL-HLT pages</affiliation>
<address confidence="0.986339">151–160, Portland, Oregon, USA.</address>
<author confidence="0.818827">Syntactic nor-</author>
<affiliation confidence="0.605428">of Twitter messages. In Conon Natural Language Kharagpur,</affiliation>
<address confidence="0.701858">India.</address>
<note confidence="0.928851653846154">S. Kullback and R. A. Leibler. 1951. On information and of Mathematical 22:49– 86. John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. of the Eighteenth International Conferon Machine pages 282–289, San Francisco, CA, USA. Vladimir I. Levenshtein. 1966. Binary codes capable of deletions, insertions, and reversals. 10:707–710. Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006. Exploring distributional similarity based models for spelling correction. In of COLpages 1025–1032, Sydney, Australia. Jianhua Lin. 1991. Divergence measures based on the entropy. Transactions on Information 37(1):145–151. Dekang Lin. 1998. Automatic retrieval and clusterof similar words. In of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACLpages 768–774, Montreal, Quebec, Canada. Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011a. Insertion, deletion, or substitution? normal-</note>
<abstract confidence="0.611054666666667">izing text messages without pre-categorization nor su- In of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- Language Technologies (ACL-HLT pages 71–76, Portland, Oregon, USA. Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011b. Recognizing named entities in tweets. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-</abstract>
<address confidence="0.7223845">Technologies (ACL-HLT pages 359–367, Portland, Oregon, USA.</address>
<author confidence="0.638158">A broad-</author>
<abstract confidence="0.69905425">coverage normalization system for social media lan- In of the 50th Annual Meeting of the Association for Computational Linguistics (ACL Jeju, Republic of Korea.</abstract>
<note confidence="0.797982131578947">Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classificausing string kernels. Mach. Learn. 2:419– 444. Marco Lui and Timothy Baldwin. 2011. Cross-domain selection for language identification. In Proceedings of the 5th International Joint Conference on Language Processing (IJCNLP pages 553–561, Chiang Mai, Thailand. 431 Brendan O’Connor, Michel Krieger, and David Ahn. 2010. TweetMotif: Exploratory search and topic sumfor Twitter. In of the 4th International Conference on Weblogs and Social Media pages 384–385, Washington, USA. Lawrence Philips. 2000. The double metaphone search Users 18:38–43. Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech of the 77(2):257–286. Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics pages 172–180, Los Angeles, USA. Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An exstudy. In of the 2011 Conference on Empirical Methods in Natural Language Pro- (EMNLP pages 1524–1534, Edinburgh, Scotland, UK. Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: real-time detection by social sensors. In of the 19th International Conference on the World Wide (WWW pages 851–860, Raleigh, North</note>
<address confidence="0.807503">Carolina, USA.</address>
<author confidence="0.308061">Generation txt The sociolin-</author>
<abstract confidence="0.722288666666667">of young people’s text-messaging. 1(1). Kristina Toutanova and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. of the 40th Annual Meeting of the and 3rd Annual Meeting of the NAACL</abstract>
<note confidence="0.9304417">pages 144–151, Philadelphia, USA. Official Blog Twitter. 2011. 200 million tweets per day. Retrived at August 17th, 2011. Jianshu Weng and Bu-Sung Lee. 2011. Event detection Twitter. In of the 5th International Conference on Weblogs and Social Media (ICWSM Barcelona, Spain. Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011. microtext. In of the AAAI- Workshop on Analyzing pages 74–79,</note>
<address confidence="0.815528">San Francisco, USA.</address>
<author confidence="0.770884">More accurate tests for the sta-</author>
<affiliation confidence="0.6808015">significance of result differences. In Proceedings of the 23rd International Conference on Compu-</affiliation>
<address confidence="0.740184333333333">Linguistics (COLING pages 947–953, Saarbr¨ucken, Germany. 432</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>33--40</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8366" citStr="Aw et al. (2006)" startWordPosition="1281" endWordPosition="1284">dwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness clas</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proceedings of COLING/ACL 2006, pages 33–40, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Benson</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Event discovery in social media feeds.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>389--398</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1622" citStr="Benson et al., 2011" startWordPosition="226" endWordPosition="229">th other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011;</context>
</contexts>
<marker>Benson, Haghighi, Barzilay, 2011</marker>
<rawString>Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 389–398, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="23212" citStr="Brants and Franz, 2006" startWordPosition="3656" endWordPosition="3659">esents the frequency of the OOV, but it can be gain (a positive number) or loss (a negative number), depending on whether the ith pair is correct or incorrect. Because we also expect correct pairs to be ranked higher than incorrect pairs, DCG@N takes both factors into account. Given the generated pairs and the evaluation metric, we first consider three baselines: no re-ranking (i.e., the final ranking is that of the contextual similarity scores), and re-rankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we presen</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<location>Hong Kong.</location>
<contexts>
<context position="6522" citStr="Brill and Moore (2000)" startWordPosition="1000" endWordPosition="1003">ardly in lexical normalisation, t is assumed to be an OOV token, relative to a fixed dictionary. In practice, not all OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as ph</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 286–293, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--157</pages>
<contexts>
<context position="6855" citStr="Choudhury et al. (2007)" startWordPosition="1047" endWordPosition="1050">k in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they th</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10:157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
<author>Tanveer A Faruquie</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Unsupervised cleansing of noisy text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<pages>189--196</pages>
<location>Beijing, China.</location>
<contexts>
<context position="23733" citStr="Contractor et al., 2010" startWordPosition="3731" endWordPosition="3734">the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and (nite, night) are at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, mixed with lexical variants like (Facebook, speech) and (Youtube, web). In ranking by IV word frequency, w</context>
</contexts>
<marker>Contractor, Faruquie, Subramaniam, 2010</marker>
<rawString>Danish Contractor, Tanveer A. Faruquie, and L. Venkata Subramaniam. 2010. Unsupervised cleansing of noisy text. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 189–196, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="3947" citStr="Cook and Stevenson, 2009" startWordPosition="600" endWordPosition="603">ntities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, </context>
<context position="6973" citStr="Cook and Stevenson (2009)" startWordPosition="1065" endWordPosition="1068">etection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Laffert</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 71– 78, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>¨Ozlem C¸etinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph L Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: POS Tagging and Parsing the Twitterverse.</title>
<date>2011</date>
<booktitle>In Analyzing Microtext: Papers from the 2011 AAAI Workshop,</booktitle>
<volume>volume</volume>
<pages>11--05</pages>
<location>San Francisco, CA, USA.</location>
<marker>Foster, C¸etinoglu, Wagner, Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner, Joseph L. Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. #hardtoparse: POS Tagging and Parsing the Twitterverse. In Analyzing Microtext: Papers from the 2011 AAAI Workshop, volume WS-11-05 of AAAI Workshops, pages 20–25, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011),</booktitle>
<pages>42--47</pages>
<location>Portland, Oregon, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 42–47, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in Twitter: a closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>581--586</pages>
<location>Portland, Oregon, USA.</location>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in Twitter: a closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 581–586, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Dirk Hovy</author>
<author>Donald Metzler</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP,</booktitle>
<pages>82--90</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2260" citStr="Gouws et al., 2011" startWordPosition="331" endWordPosition="334">he spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. In this paper, we focus on the task of lexical normalisation of English Twitter messages, in w</context>
<context position="4093" citStr="Gouws et al., 2011" startWordPosition="622" endWordPosition="625">fore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, in this paper we propose methods for automatically constructing a lexical normalisation dictionary — a dictionary whose entries consist of (lexica</context>
<context position="5329" citStr="Gouws et al., 2011" startWordPosition="803" endWordPosition="806">rd form) pairs — that enables type-based normalisation. Despite the simplicity of this dictionary-based normalisation method, we show it to outperform previously-proposed approaches. This very fast, lightweight solution is suitable for real-time processing of the large volume of streaming microblog data available from Twitter, and offers a simple solution to the lexical variant detection problem that hinders other normalisation methods. Furthermore, this dictionary-based method can be easily integrated with other more-complex normalisation approaches (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011) to produce hybrid systems. After discussing related work in Section 2, we present an overview of our dictionary-based approach to normalisation in Section 3. In Sections 4 and 5 we experimentally select the optimised context similarity parameters and string similarity reranking method. We present experimental results on the unseen test data in Section 6, and offer some concluding remarks in Section 7. 2 Related Work Given a token t, lexical normalisation is the task of finding arg maxP(s|t) a arg maxP(t|s)P(s), where s is the standard form, i.e., an IV word. Standardly in lexical normalisatio</context>
<context position="8766" citStr="Gouws et al. (2011)" startWordPosition="1344" endWordPosition="1347"> the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al., 2011). However, the lexical variant detection of both methods is rather unreliable, indicating the challenge of this aspect of normalisation. Both of these approaches incorporate a relatively small normalisation dictionary to capture frequent lexical variants with high precision. In particular, Gouws et al. (2011) produce a</context>
<context position="10807" citStr="Gouws et al., 2011" startWordPosition="1650" endWordPosition="1653">similarity measures on the quality of the resulting lexicons. In summary, our dictionary-based normalisation approach is a lightweight end-to-end method which performs both lexical variant detection and normalisation, and thus is suitable for practical online preprocessing, despite its simplicity. 3 A Lexical Normalisation Dictionary Before discussing our method for creating a normalisation dictionary, we first discuss the feasibility of such an approach. 3.1 Feasibility Dictionary lookup approaches to normalisation have been shown to have high precision but low recall (Han and Baldwin, 2011; Gouws et al., 2011). Frequent (lexical variant, standard form) pairs such as (u, you) are typically included in the dictionaries used by such methods, while less-frequent items such as (g0tta, gotta) are generally omitted. Because of the degree of lexical creativity and large number of non-standard forms observed on Twitter, a wide-coverage normalisation dictionary would be expensive to construct manually. Based on the assumption that lexical variants occur in similar contexts to their standard forms, however, it should be possible to automatically construct a normalisation dictionary with wider coverage than is</context>
<context position="26027" citStr="Gouws et al. (2011)" startWordPosition="4097" endWordPosition="4100">istance, but places many longer words at the top of the ranking. Edit distance over double metaphone codes (phonetic edit distance) performs particularly well for lexical variants that include character repetitions — commonly used for emphasis on Twitter — because such repetitions do not typically alter the phonetic codes. Compared with the other methods, the string subsequence kernel delivers encouraging results. It measures common character subsequences of length n between (OOV, IV) pairs. Because it is computationally expensive to calculate similarity for larger n, we choose n=2, following Gouws et al. (2011). As N (the lexicon size cut-off) increases, the performance drops more slowly than the other methods. Although this method fails to rank heavilyaltered variants such as (4get, forget) highly, it typically works well for longer words. Given that we focus on longer OOVs (specifically those longer than 4 characters), this ultimately isn’t a great handicap. 6 Evaluation Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). 6.1 Metrics We evaluate using the standard evaluation metrics of precision</context>
<context position="28231" citStr="Gouws et al. (2011)" startWordPosition="4458" endWordPosition="4461">re-ranking methods, and best cut-off N for each method, based on the highest DCG@N value for a given method over the development data, as presented in Figure 1. Namely, they are string subsequence kernel (S-dict, N=40,000), double metaphone edit distance (DMdict, N=10,000) and default contextual similarity without re-ranking (C-dict, N=10,000).7 We evaluate each of the learned dictionaries in Table 3. We also compare each dictionary with the performance of the manually-constructed Internet slang dictionary (HB-dict) used by Han and Baldwin (2011), the small automatically-derived dictionary of Gouws et al. (2011) (GHM-dict), and combinations of the different dictionaries. In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al. (2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. Results are shown in Table 3, and discussed below. 6.2.1 Individual Dictionaries Overall, the individual dictio</context>
<context position="32226" citStr="Gouws et al. (2011)" startWordPosition="5052" endWordPosition="5055">t+S-dict 0.840 0.601 0.701 0.090 0.052 GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061 HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063 HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049 GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135 HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124 HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081 HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079 Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone rendering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al. (2011) (GHM-dict), the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition, we combine the dictionaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the resul</context>
</contexts>
<marker>Gouws, Hovy, Metzler, 2011</marker>
<rawString>Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011. Unsupervised mining of lexical variants from noisy text. In Proceedings of the First workshop on Unsupervised Learning in NLP, pages 82–90, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>368--378</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1906" citStr="Han and Baldwin, 2011" startWordPosition="270" endWordPosition="273">tter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eat</context>
<context position="4072" citStr="Han and Baldwin, 2011" startWordPosition="618" endWordPosition="621"> normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, in this paper we propose methods for automatically constructing a lexical normalisation dictionary — a dictionary whose entri</context>
<context position="5308" citStr="Han and Baldwin, 2011" startWordPosition="799" endWordPosition="802">lexical variant, standard form) pairs — that enables type-based normalisation. Despite the simplicity of this dictionary-based normalisation method, we show it to outperform previously-proposed approaches. This very fast, lightweight solution is suitable for real-time processing of the large volume of streaming microblog data available from Twitter, and offers a simple solution to the lexical variant detection problem that hinders other normalisation methods. Furthermore, this dictionary-based method can be easily integrated with other more-complex normalisation approaches (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011) to produce hybrid systems. After discussing related work in Section 2, we present an overview of our dictionary-based approach to normalisation in Section 3. In Sections 4 and 5 we experimentally select the optimised context similarity parameters and string similarity reranking method. We present experimental results on the unseen test data in Section 6, and offer some concluding remarks in Section 7. 2 Related Work Given a token t, lexical normalisation is the task of finding arg maxP(s|t) a arg maxP(t|s)P(s), where s is the standard form, i.e., an IV word. Standardly in</context>
<context position="8742" citStr="Han and Baldwin (2011)" startWordPosition="1339" endWordPosition="1342">e encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al., 2011). However, the lexical variant detection of both methods is rather unreliable, indicating the challenge of this aspect of normalisation. Both of these approaches incorporate a relatively small normalisation dictionary to capture frequent lexical variants with high precision. In particular, Gouws</context>
<context position="10786" citStr="Han and Baldwin, 2011" startWordPosition="1646" endWordPosition="1649"> contextual and string similarity measures on the quality of the resulting lexicons. In summary, our dictionary-based normalisation approach is a lightweight end-to-end method which performs both lexical variant detection and normalisation, and thus is suitable for practical online preprocessing, despite its simplicity. 3 A Lexical Normalisation Dictionary Before discussing our method for creating a normalisation dictionary, we first discuss the feasibility of such an approach. 3.1 Feasibility Dictionary lookup approaches to normalisation have been shown to have high precision but low recall (Han and Baldwin, 2011; Gouws et al., 2011). Frequent (lexical variant, standard form) pairs such as (u, you) are typically included in the dictionaries used by such methods, while less-frequent items such as (g0tta, gotta) are generally omitted. Because of the degree of lexical creativity and large number of non-standard forms observed on Twitter, a wide-coverage normalisation dictionary would be expensive to construct manually. Based on the assumption that lexical variants occur in similar contexts to their standard forms, however, it should be possible to automatically construct a normalisation dictionary with w</context>
<context position="16815" citStr="Han and Baldwin (2011)" startWordPosition="2608" endWordPosition="2611">Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length &gt; 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, following settings in Han and Baldwin (2011).3 1https://dev.twitter.com/docs/ streaming-api/methods 2http://aspell.net/ 3We only consider IV words within an edit distance of 2 or a phonemic edit distance of 1 from the OOV type, and we further 424 In order to evaluate the generated pairs, we randomly selected 1000 OOV words from the 10 million tweet corpus. We set up an annotation task on Amazon Mechanical Turk,4 presenting five independent annotators with each word type (with no context) and asking for corrections where appropriate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of “tomorrow”</context>
<context position="26551" citStr="Han and Baldwin (2011)" startWordPosition="4185" endWordPosition="4188">nally expensive to calculate similarity for larger n, we choose n=2, following Gouws et al. (2011). As N (the lexicon size cut-off) increases, the performance drops more slowly than the other methods. Although this method fails to rank heavilyaltered variants such as (4get, forget) highly, it typically works well for longer words. Given that we focus on longer OOVs (specifically those longer than 4 characters), this ultimately isn’t a great handicap. 6 Evaluation Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). 6.1 Metrics We evaluate using the standard evaluation metrics of precision (P), recall (R) and F-score (F) as detailed below. We also consider the false alarm rate (FA) and word error rate (WER), also as shown below. FA measures the negative effects of applying normalisation; a good approach to normalisation should not (incorrectly) normalise tokens that are already in their standard form and do not require normalisation.6 WER, like F-score, shows the overall benefits of normalisation, but unlike F-score, measures how many token-level edits are required for the output to be the same as the g</context>
<context position="28164" citStr="Han and Baldwin (2011)" startWordPosition="4447" endWordPosition="4451">normalisation WER = # all tokens 6.2 Results We select the three best re-ranking methods, and best cut-off N for each method, based on the highest DCG@N value for a given method over the development data, as presented in Figure 1. Namely, they are string subsequence kernel (S-dict, N=40,000), double metaphone edit distance (DMdict, N=10,000) and default contextual similarity without re-ranking (C-dict, N=10,000).7 We evaluate each of the learned dictionaries in Table 3. We also compare each dictionary with the performance of the manually-constructed Internet slang dictionary (HB-dict) used by Han and Baldwin (2011), the small automatically-derived dictionary of Gouws et al. (2011) (GHM-dict), and combinations of the different dictionaries. In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al. (2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. Results are shown in Table 3, and discussed </context>
<context position="32306" citStr="Han and Baldwin (2011)" startWordPosition="5063" endWordPosition="5066">2 0.061 HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063 HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049 GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135 HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124 HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081 HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079 Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone rendering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al. (2011) (GHM-dict), the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition, we combine the dictionaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significan</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 368–378, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jarvelin</author>
<author>J Kekalainen</author>
</authors>
<title>Cumulated gainbased evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="22550" citStr="Jarvelin and Kekalainen (2002)" startWordPosition="3542" endWordPosition="3545">s 19.328 IVs 19.335 KL divergence 19.328 ±2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227 ±3 19.328 3 19.324 JS divergence 19.311 Cosine 19.170 Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs. We tune one parameter at a time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in each case is indicated in bold. isations for lexical variants, e.g. (bcuz, cause)), we modify our evaluation metric from Section 4 to evaluate the ranking at different points, using Discounted Cumulative Gain (DCG@N: Jarvelin and Kekalainen (2002)): DCG@N = rel1 + where reli again represents the frequency of the OOV, but it can be gain (a positive number) or loss (a negative number), depending on whether the ith pair is correct or incorrect. Because we also expect correct pairs to be ranked higher than incorrect pairs, DCG@N takes both factors into account. Given the generated pairs and the evaluation metric, we first consider three baselines: no re-ranking (i.e., the final ranking is that of the contextual similarity scores), and re-rankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram fr</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>K. Jarvelin and J. Kekalainen. 2002. Cumulated gainbased evaluation of IR techniques. ACM Transactions on Information Systems, 20(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent Twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>151--160</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1525" citStr="Jiang et al., 2011" startWordPosition="211" endWordPosition="214">-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-s</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 151–160, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of Twitter messages.</title>
<date>2010</date>
<booktitle>In International Conference on Natural Language Processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="8397" citStr="Kaufmann and Kalita (2010)" startWordPosition="1286" endWordPosition="1289">it microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) </context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Joseph Kaufmann and Jugal Kalita. 2010. Syntactic normalization of Twitter messages. In International Conference on Natural Language Processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
<author>R A Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>Annals of Mathematical Statistics,</journal>
<volume>22</volume>
<pages>86</pages>
<contexts>
<context position="15797" citStr="Kullback and Leibler, 1951" startWordPosition="2435" endWordPosition="2438">ntext words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API.1 From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normal</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>S. Kullback and R. A. Leibler. 1951. On information and sufficiency. Annals of Mathematical Statistics, 22:49– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7588" citStr="Lafferty et al., 2001" startWordPosition="1158" endWordPosition="1161"> (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to estimate P(t|s) at the character level. One shortcoming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="23487" citStr="Levenshtein, 1966" startWordPosition="3699" endWordPosition="3700">ven the generated pairs and the evaluation metric, we first consider three baselines: no re-ranking (i.e., the final ranking is that of the contextual similarity scores), and re-rankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Yang Zhang</author>
<author>Muhua Zhu</author>
<author>Ming Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>1025--1032</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6716" citStr="Li et al. (2006)" startWordPosition="1028" endWordPosition="1031">”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising </context>
</contexts>
<marker>Li, Zhang, Zhu, Zhou, 2006</marker>
<rawString>Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of COLING/ACL 2006, pages 1025–1032, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="15841" citStr="Lin, 1991" startWordPosition="2442" endWordPosition="2443"> whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API.1 From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least </context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on Information Theory, 37(1):145–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL98),</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="6762" citStr="Lin, 1998" startWordPosition="1036" endWordPosition="1037">otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL98), pages 768–774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011),</booktitle>
<pages>71--76</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2051" citStr="Liu et al., 2011" startWordPosition="297" endWordPosition="300">s such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard for</context>
<context position="3965" citStr="Liu et al., 2011" startWordPosition="604" endWordPosition="607">nt in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, in this paper we p</context>
<context position="5284" citStr="Liu et al., 2011" startWordPosition="795" endWordPosition="798">ntries consist of (lexical variant, standard form) pairs — that enables type-based normalisation. Despite the simplicity of this dictionary-based normalisation method, we show it to outperform previously-proposed approaches. This very fast, lightweight solution is suitable for real-time processing of the large volume of streaming microblog data available from Twitter, and offers a simple solution to the lexical variant detection problem that hinders other normalisation methods. Furthermore, this dictionary-based method can be easily integrated with other more-complex normalisation approaches (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011) to produce hybrid systems. After discussing related work in Section 2, we present an overview of our dictionary-based approach to normalisation in Section 3. In Sections 4 and 5 we experimentally select the optimised context similarity parameters and string similarity reranking method. We present experimental results on the unseen test data in Section 6, and offer some concluding remarks in Section 7. 2 Related Work Given a token t, lexical normalisation is the task of finding arg maxP(s|t) a arg maxP(t|s)P(s), where s is the standard form, i.e., a</context>
<context position="7371" citStr="Liu et al. (2011" startWordPosition="1126" endWordPosition="1129">, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to estimate P(t|s) at the character level. One shortcoming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n </context>
<context position="11811" citStr="Liu et al., 2011" startWordPosition="1810" endWordPosition="1813">y. Based on the assumption that lexical variants occur in similar contexts to their standard forms, however, it should be possible to automatically construct a normalisation dictionary with wider coverage than is currently available. Dictionary lookup is a type-based approach to normalisation, i.e., every token instance of a given type will always be normalised in the same way. However, lexical variants can be ambiguous, e.g., y corresponds to “you” in yeah, y r right! LOL but “why” in AM CONFUSED!!! y you did that? Nevertheless, the relative occurrence of ambiguous lexical variants is small (Liu et al., 2011a), and it has been observed that while shorter variants such as y are often ambiguous, longer variants tend to be unambiguous. For example bthday and 4eva are unlikely to have standard forms other than “birthday” and “forever”, respectively. Therefore, the normalisation lexicons we produce will only contain entries for OOVs with character length greater than a specified threshold, which are likely to have an unambiguous standard form. 3.2 Overview of approach Our method for constructing a normalisation dictionary is as follows: Input: Tokenised English tweets 1. Extract (OOV, IV) pairs based </context>
<context position="15392" citStr="Liu et al., 2011" startWordPosition="2376" endWordPosition="2379"> context, but there are different ways of representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in th</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011a. Insertion, deletion, or substitution? normalizing text messages without pre-categorization nor supervision. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 71–76, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>359--367</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2051" citStr="Liu et al., 2011" startWordPosition="297" endWordPosition="300">s such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard for</context>
<context position="3965" citStr="Liu et al., 2011" startWordPosition="604" endWordPosition="607">nt in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts, in this paper we p</context>
<context position="5284" citStr="Liu et al., 2011" startWordPosition="795" endWordPosition="798">ntries consist of (lexical variant, standard form) pairs — that enables type-based normalisation. Despite the simplicity of this dictionary-based normalisation method, we show it to outperform previously-proposed approaches. This very fast, lightweight solution is suitable for real-time processing of the large volume of streaming microblog data available from Twitter, and offers a simple solution to the lexical variant detection problem that hinders other normalisation methods. Furthermore, this dictionary-based method can be easily integrated with other more-complex normalisation approaches (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011) to produce hybrid systems. After discussing related work in Section 2, we present an overview of our dictionary-based approach to normalisation in Section 3. In Sections 4 and 5 we experimentally select the optimised context similarity parameters and string similarity reranking method. We present experimental results on the unseen test data in Section 6, and offer some concluding remarks in Section 7. 2 Related Work Given a token t, lexical normalisation is the task of finding arg maxP(s|t) a arg maxP(t|s)P(s), where s is the standard form, i.e., a</context>
<context position="7371" citStr="Liu et al. (2011" startWordPosition="1126" endWordPosition="1129">, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to estimate P(t|s) at the character level. One shortcoming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n </context>
<context position="11811" citStr="Liu et al., 2011" startWordPosition="1810" endWordPosition="1813">y. Based on the assumption that lexical variants occur in similar contexts to their standard forms, however, it should be possible to automatically construct a normalisation dictionary with wider coverage than is currently available. Dictionary lookup is a type-based approach to normalisation, i.e., every token instance of a given type will always be normalised in the same way. However, lexical variants can be ambiguous, e.g., y corresponds to “you” in yeah, y r right! LOL but “why” in AM CONFUSED!!! y you did that? Nevertheless, the relative occurrence of ambiguous lexical variants is small (Liu et al., 2011a), and it has been observed that while shorter variants such as y are often ambiguous, longer variants tend to be unambiguous. For example bthday and 4eva are unlikely to have standard forms other than “birthday” and “forever”, respectively. Therefore, the normalisation lexicons we produce will only contain entries for OOVs with character length greater than a specified threshold, which are likely to have an unambiguous standard form. 3.2 Overview of approach Our method for constructing a normalisation dictionary is as follows: Input: Tokenised English tweets 1. Extract (OOV, IV) pairs based </context>
<context position="15392" citStr="Liu et al., 2011" startWordPosition="2376" endWordPosition="2379"> context, but there are different ways of representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in th</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011b. Recognizing named entities in tweets. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 359–367, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broadcoverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="3134" citStr="Liu et al., 2012" startWordPosition="474" endWordPosition="477"> earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-to-one normalisation in which one OOV token is normalised to one IV word. Naturally, not all OOV words in microblogs are lexical variants of IV words: named entities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics tingui</context>
<context position="7927" citStr="Liu et al. (2012)" startWordPosition="1218" endWordPosition="1221">ising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to estimate P(t|s) at the character level. One shortcoming of querying a search engine to obtain training pairs is it tends to be costly in terms of time and bandwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 422 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These meth</context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broadcoverage normalization system for social media language. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>2</volume>
<pages>444</pages>
<contexts>
<context position="9472" citStr="Lodhi et al., 2002" startWordPosition="1449" endWordPosition="1452">re first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al., 2011). However, the lexical variant detection of both methods is rather unreliable, indicating the challenge of this aspect of normalisation. Both of these approaches incorporate a relatively small normalisation dictionary to capture frequent lexical variants with high precision. In particular, Gouws et al. (2011) produce a small normalisation lexicon based on distributional similarity and string similarity (Lodhi et al., 2002). Our method adopts a similar strategy using distributional/string similarity, but instead of constructing a small lexicon for preprocessing, we build a much wider-coverage normalisation dictionary and opt for a fully lexiconbased end-to-end normalisation approach. In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al. (2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are compl</context>
<context position="23788" citStr="Lodhi et al., 2002" startWordPosition="3740" endWordPosition="3743">ogle Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and (nite, night) are at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, mixed with lexical variants like (Facebook, speech) and (Youtube, web). In ranking by IV word frequency, we assume the lexical variants are usually derived from </context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. J. Mach. Learn. Res., 2:419– 444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Cross-domain feature selection for language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011),</booktitle>
<pages>553--561</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="16207" citStr="Lui and Baldwin, 2011" startWordPosition="2503" endWordPosition="2506">ax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API.1 From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length &gt; 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, following settings in Han and Baldwi</context>
</contexts>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553–561, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>TweetMotif: Exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th International Conference on Weblogs and Social Media (ICWSM 2010),</booktitle>
<pages>384--385</pages>
<location>Washington, USA.</location>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O’Connor, Michel Krieger, and David Ahn. 2010. TweetMotif: Exploratory search and topic summarization for Twitter. In Proceedings of the 4th International Conference on Weblogs and Social Media (ICWSM 2010), pages 384–385, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>The double metaphone search algorithm.</title>
<date>2000</date>
<journal>C/C++ Users Journal,</journal>
<pages>18--38</pages>
<contexts>
<context position="23571" citStr="Philips, 2000" startWordPosition="3710" endWordPosition="3711"> re-ranking (i.e., the final ranking is that of the contextual similarity scores), and re-rankings of the pairs based on the frequencies of the OOVs in the Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and (nite, night) are at the top of the ranking. However, many proper nouns and named e</context>
</contexts>
<marker>Philips, 2000</marker>
<rawString>Lawrence Philips. 2000. The double metaphone search algorithm. C/C++ Users Journal, 18:38–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="6946" citStr="Rabiner, 1989" startWordPosition="1063" endWordPosition="1064">exical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a condit</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of Twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010),</booktitle>
<pages>172--180</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="1882" citStr="Ritter et al., 2010" startWordPosition="266" endWordPosition="269">ial media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. F</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), pages 172–180, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011),</booktitle>
<pages>1524--1534</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2074" citStr="Ritter et al., 2011" startWordPosition="301" endWordPosition="304">detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can </context>
<context position="15435" citStr="Ritter et al., 2011" startWordPosition="2384" endWordPosition="2387">f representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were </context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 1524–1534, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on the World Wide Web (WWW 2010),</booktitle>
<pages>851--860</pages>
<location>Raleigh, North Carolina, USA.</location>
<contexts>
<context position="1485" citStr="Sakaki et al., 2010" startWordPosition="205" endWordPosition="208">a dictionary-based approach achieves state-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An altern</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: real-time event detection by social sensors. In Proceedings of the 19th International Conference on the World Wide Web (WWW 2010), pages 851–860, Raleigh, North Carolina, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crispin Thurlow</author>
</authors>
<title>Generation txt? The sociolinguistics of young people’s text-messaging.</title>
<date>2003</date>
<booktitle>Discourse Analysis Online,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="13276" citStr="Thurlow, 2003" startWordPosition="2052" endWordPosition="2053">mes of Twitter data to identify the most distributionally-similar IV type for each OOV type. The result of this process is a set of (OOV, IV) pairs, ranked by distributional similarity. The extracted pairs will include (lexical variant, standard form) pairs, such as (tmrw, tomorrow), but will also contain false positives such as (Tusday, Sunday) — Tusday is a lexical variant, but its standard form is not “Sunday” — and (Youtube, web) — Youtube is an OOV named entity, not a lexical variant. Nevertheless, lexical variants are typically formed from their standard forms through regular processes (Thurlow, 2003) — e.g., the omission of characters — and from this perspective Sunday and web are not plausible standard forms for Tusday and Youtube, respectively. In Step 2, we therefore capture this intuition to re-rank the extracted pairs by string similarity. The top-n items in this re-ranked list then form the normalisation lexicon, which is based only on development data. Although computationally-expensive to build, this dictionary can be created offline. Once built, it then offers a very fast approach to normalisation. We can only reliably compute distributional similarity for types that are moderate</context>
</contexts>
<marker>Thurlow, 2003</marker>
<rawString>Crispin Thurlow. 2003. Generation txt? The sociolinguistics of young people’s text-messaging. Discourse Analysis Online, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Robert C Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-02),</booktitle>
<pages>144--151</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="6642" citStr="Toutanova and Moore (2002)" startWordPosition="1016" endWordPosition="1019">l OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various </context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Kristina Toutanova and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proceedings of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-02), pages 144–151, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Official Blog Twitter</author>
</authors>
<title>200 million tweets per day. Retrived at</title>
<date>2011</date>
<contexts>
<context position="1304" citStr="Twitter, 2011" startWordPosition="180" endWordPosition="181">mation to generate possible variant and normalisation pairs and then rank these by string similarity. Highlyranked pairs are selected to populate the dictionary. We show that a dictionary-based approach achieves state-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 201</context>
</contexts>
<marker>Twitter, 2011</marker>
<rawString>Official Blog Twitter. 2011. 200 million tweets per day. Retrived at August 17th, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Bu-Sung Lee</author>
</authors>
<title>Event detection in Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Conference on Weblogs and Social Media (ICWSM 2011),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1600" citStr="Weng and Lee, 2011" startWordPosition="222" endWordPosition="225">dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´a˜nez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; </context>
</contexts>
<marker>Weng, Lee, 2011</marker>
<rawString>Jianshu Weng and Bu-Sung Lee. 2011. Event detection in Twitter. In Proceedings of the 5th International Conference on Weblogs and Social Media (ICWSM 2011), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenzhen Xue</author>
<author>Dawei Yin</author>
<author>Brian D Davison</author>
</authors>
<title>Normalizing microtext.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI11 Workshop on Analyzing Microtext,</booktitle>
<pages>74--79</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="2239" citStr="Xue et al., 2011" startWordPosition="327" endWordPosition="330"> However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. In this paper, we focus on the task of lexical normalisation of English T</context>
<context position="6995" citStr="Xue et al. (2011)" startWordPosition="1070" endWordPosition="1073">leted. Various approaches have been proposed to estimate the error model, P(t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et al. (2011a) collect Google search snippets from carefully-designed queries from which they then extract noisy lexical variantstandard form pairs. These pairs are used to train a conditional random field (Lafferty et al., 2001) to est</context>
</contexts>
<marker>Xue, Yin, Davison, 2011</marker>
<rawString>Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011. Normalizing microtext. In Proceedings of the AAAI11 Workshop on Analyzing Microtext, pages 74–79, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="32991" citStr="Yeh (2000)" startWordPosition="5159" endWordPosition="5160">onaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p &lt; 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. 6.2.3 Hybrid Approaches The methods of Gouws et al. (2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionarybased approaches; this is largely caused by lexical variant detection errors.8 Using all dictionaries in combination with these methods — HB-dict+GHM-dict+S-dict+GHM-norm and HBdict+GHM-dict+S-dict+HB-norm — gives some improvements, but the false alarm rates remain high. Despite the limitations of a pure dictionary-based approach to normalisation — dis</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000101">
<title confidence="0.721899">
Concurrent Acquisition of Word Meaning and Lexical Categories
Afra Alishahi Grzegorz Chrupała
</title>
<author confidence="0.143496">
a.alishahi@uvt.nl gchrupala@lsv.uni-saarland.de
</author>
<affiliation confidence="0.6212365">
Communication and Information Sciences Spoken Language Systems
Tilburg University, The Netherlands Saarland University, Germany
</affiliation>
<sectionHeader confidence="0.975834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999396833333333">
Learning the meaning of words from ambigu-
ous and noisy context is a challenging task for
language learners. It has been suggested that
children draw on syntactic cues such as lexical
categories of words to constrain potential ref-
erents of words in a complex scene. Although
the acquisition of lexical categories should be
interleaved with learning word meanings, it
has not previously been modeled in that fash-
ion. In this paper, we investigate the inter-
play of word learning and category induction
by integrating an LDA-based word class learn-
ing module with a probabilistic word learning
model. Our results show that the incremen-
tally induced word classes significantly im-
prove word learning, and their contribution is
comparable to that of manually assigned part
of speech categories.
</bodyText>
<sectionHeader confidence="0.784444" genericHeader="method">
1 Learning the Meaning of Words
</sectionHeader>
<bodyText confidence="0.999964557692308">
For young learners of a natural language, mapping
each word to its correct meaning is a challenging
task. Words are often used as part of an utterance
rather than in isolation. The meaning of an utter-
ance must be inferred from among numerous pos-
sible interpretations that the (usually complex) sur-
rounding scene offers. In addition, the linguistic and
visual context in which words are heard and used
is often noisy and highly ambiguous. Particularly,
many words in a language are polysemous and have
different meanings.
Various learning mechanisms have been proposed
for word learning. One well-studied mechanism
is cross-situational learning, a bottom-up strategy
based on statistical co-occurrence of words and ref-
erents across situations (Quine 1960, Pinker 1989).
Several experimental studies have shown that adults
and children are sensitive to cross-situational evi-
dence and use this information for mapping words to
objects, actions and properties (Smith and Yu 2007,
Monaghan and Mattock 2009). A number of com-
putational models have been developed based on this
principle, demonstrating that cross-situational learn-
ing is a powerful and efficient mechanism for learn-
ing the correct mappings between words and mean-
ings from noisy input (e.g. Siskind 1996, Yu 2005,
Fazly et al. 2010).
Another potential source of information that can
help the learner to constrain the relevant aspects of a
scene is the sentential context of a word. It has been
suggested that children draw on syntactic cues pro-
vided by the linguistic context in order to guide word
learning, a hypothesis known as syntactic bootstrap-
ping (Gleitman 1990). There is substantial evidence
that children are sensitive to the structural regular-
ities of language from a very young age, and that
they use these structural cues to find the referent of
a novel word (e.g. Naigles and Hoff-Ginsberg 1995,
Gertner et al. 2006). In particular, young children
have robust knowledge of some of the abstract lexi-
cal categories such as nouns and verbs (e.g. Gelman
and Taylor 1984, Kemp et al. 2005).
Recent studies have examined the interplay of
cross-situational learning and sentence-level learn-
ing mechanisms, showing that adult learners of an
artificial language can successfully and simultane-
ously apply cues and constraints from both sources
of information when mapping words to their refer-
ents (Gillette et al. 1999, Lidz et al. 2010, Koehne
and Crocker 2010; 2011). Several computational
models have also investigated this interaction by
adding manually annotated part-of-speech tags as
</bodyText>
<page confidence="0.989082">
643
</page>
<note confidence="0.7816845">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999880275862069">
input to word learning algorithms, and suggesting
that integration of lexical categories can boost the
performance of a cross-situational model (Yu 2006,
Alishahi and Fazly 2010).
However, none of the existing experimental or
computational studies have examined the acquisition
of word meanings and lexical categories in paral-
lel. They all make the simplifying assumption that
prior to the onset of word learning, the categoriza-
tion module has already formed a relatively robust
set of lexical categories. This assumption can be jus-
tified in the case of adult learners of a second or ar-
tificial language. But children’s acquisition of cate-
gories is most probably interleaved with the acquisi-
tion of word meaning, and these two processes must
ultimately be studied simultaneously.
In this paper, we investigate concurrent acquisi-
tion of word meanings and lexical categories. We
use an online version of the LDA algorithm to
induce a set of word classes from child-directed
speech, and integrate them into an existing prob-
abilistic model of word learning which combines
cross-situational evidence with cues from lexical
categories. Through a number of simulations of a
word learning scenario, we show that our automat-
ically and incrementally induced categories signifi-
cantly improve the performance of the word learning
model, and are closely comparable to a set of gold-
standard, manually-annotated part of speech tags.
</bodyText>
<sectionHeader confidence="0.984207" genericHeader="method">
2 A Word Learning Model
</sectionHeader>
<bodyText confidence="0.999925016129032">
We want to investigate whether lexical categories
(i.e. word classes) that are incrementally induced
from child-directed speech can improve the perfor-
mance of a cross-situational word learning model.
For this purpose, we use the model of Alishahi and
Fazly (2010). This model uses a probabilistic learn-
ing algorithm for combining evidence from word–
referent co-occurrence statistics and the meanings
associated with a set of pre-defined categories. They
use child-directed utterances, manually annotated
with a small set of part of speech tags, from the
Manchester corpus (Theakston et al. 2001) in the
CHILDES database (MacWhinney 1995). Their ex-
perimental results show that integrating these gold-
standard categories into the algorithm boosts its per-
formance over a pure cross-situational version.
The model of Alishahi and Fazly (2010) has the
suitable architecture for our goal: it provides an in-
tegrated learning mechanism which combines evi-
dence from word-referent co-occurrence with cues
from the meaning representation associated with
word categories. However, the model has two ma-
jor shortcomings. First, it assumes that lexical cate-
gories are formed and finalized prior to the onset of
word learning and that a correct and unique category
for a target word can be identified at each point in
time, assumptions that are highly unlikely. Second,
it does not handle any ambiguity in the meaning of
a word. Instead, each word is assumed to have only
one correct meaning. Considering the high level of
lexical ambiguity in most natural languages, this as-
sumption unreasonably simplifies the word learning
problem.
To investigate the plausibility of integrating word
and category learning, we use an online algorithm
for automatically and incrementally inducing a set
of lexical categories. Moreover, we use each word in
its original form instead of lemmatizing them, which
implies that categories contain different morpholog-
ical forms of the same word. By applying these
changes, we are able to study the contribution of lex-
ical categories to word learning in a more realistic
scenario.
Representation of input. The input to the model
consists of a sequence of utterances, each paired
with a representation of an observed scene. We rep-
resent an utterance as a set of words, U = {w}
(e.g. {she, went, home, ...}), and the corresponding
scene as a set of semantic features, 5 = {f} (e.g.
{ANIMATE, HUMAN, FEMALE, ...}).
Word and category meaning. We represent the
meaning of a word as a time-dependent probability
distribution p(t)(·|w) over all the semantic features,
where p(t)(f|w) is the probability of feature f be-
ing associated with word w at time t. In the absence
of any prior knowledge, the model assumes a uni-
form distribution over all features as the meaning of
a novel word. Also, a function cat(t)(w) gives us
the category to which a word w in utterance U(t) be-
longs.
At each point in time, a category c contains a set
of word tokens. We assign a meaning to each cat-
</bodyText>
<page confidence="0.998292">
644
</page>
<bodyText confidence="0.999596117647059">
egory as a weighted sum of the meaning learned
so far for each of its members, or p(t)(f|c) =
(1/|c|) EwEc p(t)(f|w), where |c |is the number of
word tokens in c at the current moment.
Learning algorithm. Given an utterance-scene pair
(U(t), S(t)) received at time t, the model first calcu-
lates an alignment score a for each word w E U(t)
and each semantic feature f E S(t). A semantic fea-
ture can be aligned to a word according to the mean-
ing acquired for that word from previous observa-
tions (word-based alignment, or aw). Alternatively,
distributional clues of the word can be used to de-
termine its category, and the semantic features can
be aligned to the word according to the meaning as-
sociated to its category (category-based alignment,
or ac). We combine these two sources of evidence
when estimating an alignment score:
</bodyText>
<equation confidence="0.9983405">
a(w|f, U(t), S(t)) = λ(w) x aw(w|f, U(t), S(t)) (1)
+(1 − λ(w)) x ac(w|f, U(t), S(t))
</equation>
<bodyText confidence="0.999926666666667">
where the word-based and category-based alignment
scores are estimated based on the acquired meanings
of the word and its category, respectively:
</bodyText>
<equation confidence="0.998902625">
p(t−1)(f|w)
aw(w|f, U(t), S(t)) =
E p(t−1)(f|wk)
wkEUM
p(t−1)(f|cat(w))
ac(w|f, U(t), S(t)) =
1: p(t−1)(f|cat(wk))
wkEUM
</equation>
<bodyText confidence="0.999964777777778">
The relative contribution of the word-based versus
the category-based alignment is determined by the
weight function λ(w). Cross-situational evidence
is a reliable cue for frequent words; on the other
hand, the category-based score is most informative
when the model encounters a low-frequency word
(See Alishahi and Fazly (2010) for a full analysis of
the frequency effect). Therefore, we define λ(w) as
a function of the frequency of the word n(w):
</bodyText>
<equation confidence="0.995231">
λ(w) = n(w)/(n(w) + 1)
</equation>
<bodyText confidence="0.74814275">
Once an alignment score is calculated for each
word w E U(t) and each feature f E S(t), the model
revises the meanings of all the words in U(t) and
their corresponding categories as follows:
</bodyText>
<equation confidence="0.57507">
assoc(t)(w, f) = assoc(t−1)(w, f) + a(w|f, U(t), S(t))
</equation>
<bodyText confidence="0.9737175">
where assoc(t−1)(w, f) is zero if w and f have not
co-occurred before. These association scores are
then used to update the meaning of the words in the
current input:
</bodyText>
<equation confidence="0.995066333333333">
p(t)(f|w) = assoc(t)(f, w) (2)
assoc(t)(fj, w)
fjEF
</equation>
<bodyText confidence="0.996964153846154">
where F is the set of all features seen so far. We use
a smoothed version of this formula to accommodate
noisy or rare input. This process is repeated for all
the input pairs, one at a time.
Uniform categories. Adding the category-based
alignment as a new factor to Eqn. (1) might im-
ply that the role of categories in this model is noth-
ing more than smoothing the cross-situational-based
alignment of words and referents. In order to in-
vestigate this issue, we use the following alignment
formula as an informed baseline in our experiments,
where we replace ac( |f, U(t), S(t)) with a uniform
distribution:1
</bodyText>
<equation confidence="0.992095">
a(w|f, U(t), S(t)) = λ(w) x aw(w|f, U(t), S(t)) (3)
1
+(1 − λ(w)) x
|U(t)|
</equation>
<bodyText confidence="0.999850666666667">
where aw(w|f, U(t), S(t)) and λ(w) are estimated as
before. In our experiments in Section 4, we refer to
this baseline as the ‘uniform’ condition.
</bodyText>
<sectionHeader confidence="0.992087" genericHeader="method">
3 Online induction of word classes with
LDA
</sectionHeader>
<bodyText confidence="0.999922">
Empirical findings suggest that young children form
their knowledge of abstract categories, such as
verbs, nouns, and adjectives, gradually (e.g. Gel-
man and Taylor 1984, Kemp et al. 2005). In ad-
dition, several unsupervised computational mod-
els have been proposed for inducing categories of
words which resemble part-of-speech categories, by
</bodyText>
<footnote confidence="0.6778185">
1We thank an anonymous reviewers for suggesting this con-
dition as an informed baseline.
</footnote>
<page confidence="0.997405">
645
</page>
<bodyText confidence="0.9999412">
drawing on distributional properties of their con-
text (see for example Redington et al. 1998, Clark
2000, Mintz 2003, Parisien et al. 2008, Chrupała
and Alishahi 2010). However, explicit accounts of
how such categories can be integrated in a cross-
situational model of word learning have been rare.
Here we adopt an online version of the model pro-
posed in Chrupała (2011), a method of soft word
class learning using Latent Dirichlet Allocation. The
approach is much more efficient than the commonly
used alternative (Brown clustering, (Brown et al.
1992)) while at the same time matching or outper-
forming it when the word classes are used as au-
tomatically learned features for supervised learning
of various language understanding tasks. Here we
adopt this model as our approach to learning lexical
categories.
In Section 3.1 we describe the LDA model for
word classes; in Section 3.2 we discuss the online
Gibbs sampler we use for inference.
</bodyText>
<subsectionHeader confidence="0.999203">
3.1 Word class learning with LDA
</subsectionHeader>
<bodyText confidence="0.999685">
Latent Dirichlet Allocation (LDA) was introduced
by Blei et al. (2003) and is most commonly used
for modeling the topic structure in document collec-
tions. It is a generative, probabilistic hierarchical
Bayesian model that induces a set of latent variables,
which correspond to the topics. The topics them-
selves are multinomial distributions over words.
The generative structure of the LDA model is the
following:
</bodyText>
<table confidence="0.79127975">
φk ∼ Dirichlet(β), k ∈ [1, K] (4)
θd ∼ Dirichlet(α), d ∈ [1, D]
znd ∼ Categorical(θd), nd ∈ [1, Nd]
wnd ∼ Categorical(φznd ), nd ∈ [1, Nd]
</table>
<bodyText confidence="0.988317333333333">
Chrupała (2011) reinterprets the LDA model in
terms of word classes as follows: K is the number
of classes, D is the number of unique word types,
Nd is the number of context features (such as right or
left neighbor) associated with word type d, znd is the
class of word type d in the nthd context, and wnd is the
nthd context feature of word type d. Hyperparameters
α and β control the sparseness of the vectors θd and
φk.
</bodyText>
<table confidence="0.99509625">
Wordtype Features
How doR
do HowL youR youL
you doL doR
</table>
<tableCaption confidence="0.945078666666667">
Table 1: Matrix of context features
1.8M words (CHILDES) 100M words (BNC)
Table 2: Most similar word pairs
</tableCaption>
<bodyText confidence="0.9997021">
As an example consider the small corpus consist-
ing of the single sentence How do you do. The rows
in Table 1 show the features wi ... wNd for each
word type d if we use each word’s left and right
neighbors as features, and subscript words with L
and R to indicate left and right.
After inference, the θd parameters correspond to
word class probability distributions given a word
type while the φk correspond to feature distributions
given a word class: the model provides a probabilis-
tic representation for word types independently of
their context, and also for contexts independently of
the word type.
Probabilistic, soft word classes are more expres-
sive than hard categories. First, they make it
easy and efficient to express shared ambiguities:
Chrupała (2011) gives an example of words used
as either first names or surnames, and this shared
ambiguity is reflected in the similarity of their word
class distributions. Second, with soft word classes it
becomes easy to express graded similarity between
words: as an example, Table 2 shows a random se-
lection out of the 100 most similar word pairs ac-
cording to the Jensen-Shannon divergence between
their word class distributions, according to a word
class model with 25 classes induced from (i) 1.8 mil-
lion words of the CHILDES corpus or (ii) 100 mil-
lion word of the BNC corpus. The similarities were
measured between each of the 1000 most frequent
CHILDES or BNC words.
</bodyText>
<figure confidence="0.99882975">
train car
can will
give bring
June March
shoes clothes
man woman
book hole black white
monkey rabbit business language
</figure>
<page confidence="0.975549">
646
</page>
<subsectionHeader confidence="0.992567">
3.2 Online Gibbs sampling for LDA
</subsectionHeader>
<bodyText confidence="0.999941666666667">
There have been a number of attempts to develop
online inference algorithms for topic modeling with
LDA. A simple modification of the standard Gibbs
sampler (o-LDA) was proposed by Song et al.
(2005) and Banerjee and Basu (2007).
Canini et al. (2009) experiment with three sam-
pling algorithms for online topic inference: (i) o-
LDA, (ii) incremental Gibbs sampler, and (iii) a par-
ticle filter. Only o-LDA is truly online in the sense
that it does not revisit previously seen documents.
The other two, the incremental Gibbs sampler and
the particle filter, keep seen documents and periodi-
cally resample them. In Canini et al.’s experiments
all of the online algorithms perform worse than the
standard batch Gibbs sampler on a document clus-
tering task.
Hoffman et al. (2010) develop an online version
of the variational Bayes (VB) optimization method
for inference for topic modeling with LDA. Their
method achieves good empirical results compared
to batch VB as measured by perplexity on held-
out data, especially when used with large minibatch
sizes.
Online VB for LDA is appropriate when stream-
ing documents: with online VB documents are rep-
resented as word count tables. In our scenario where
we apply LDA to modeling word classes we need to
process context features from sentences arriving in
a stream: i.e. we need to sample entries from a ta-
ble like Table 1 in order of arrival rather than row
by row. This means that online VB is not directly
applicable to online word-class induction.
However it also means that one issue with o-LDA
identified by Canini et al. (2009) is ameliorated.
When sampling in a topic modeling setting, docu-
ments are unique and are never seen again. Thus,
the topics associated with old documents get stale
and need to be periodically rejuvenated (i.e. resam-
pled). This is the reason why the incremental Gibbs
sampler and the particle filter algorithms in Canini
et al. (2009) need to keep old documents around and
cannot run in a true online fashion. Since for word
class modeling we stream context features as they
arrive, we will continue to see features associated
with the seen word types, and will automatically re-
sample their class assignments. In exploratory ex-
periments we have seen that this narrows the per-
formance gap between the o-LDA sampler and the
batch collapsed Gibbs sampler.
We present our version of the o-LDA sampler in
Algorithm 1. For each incoming sentence t we run J
passes of sampling, updating the counts tables after
each sampling step. We sample the class assignment
zti for feature wti according to:
</bodyText>
<equation confidence="0.99791475">
(nzt,dt
t−1 + α) x (ntt,i
Et + Q)
P(zt|zt−1,wt,dt) oc
</equation>
<bodyText confidence="0.981379529411765">
(5)
where nz,d tstands for the number of times class z
co-occurred with word type d up to step t, and sim-
ilarly nz,w
t is the number of times feature w was as-
signed to class z. Vt is the number of unique features
seen up to step t, while α and Q are the LDA hyper-
parameters. There are two differences between the
original o-LDA and our version: we do not initialize
the algorithm with a batch run over a prefix of the
data, and we allow more than one sampling pass per
sentence.2 Exploratory experiments have shown that
batch initialization is unnecessary, and that multiple
passes typically improve the quality of the induced
word classes.
Algorithm 1 Online Gibbs sampler for word class
induction with LDA
</bodyText>
<equation confidence="0.7831035">
for t = 1 —* oo do
for j = 1 — *J do
for i = 1 —* It do
samplezti — P(zti|zti−1,wti,dti)
increment nzti,wti and nzti,dti
t t
</equation>
<bodyText confidence="0.831683142857143">
Figure 1 shows the top 10 words for each of the
10 word classes induced with our online Gibbs sam-
pler from 1.8 million words of CHILDES. Similarly,
Figure 2 shows the top 10 words for 5 randomly cho-
sen topics out of 50, learned online from 100 million
words of the BNC.
The topics are relatively coherent and at these lev-
els of granularity express mostly part of speech and
subcategorization frame information.
Note that for each word class we show the words
most frequently assigned to it while Gibbs sampling.
2Note that we do not allow multiple passes over the stream
of sentences. Rather, while processing the current sentence, we
allow the words in this sentence to be sampled more than once.
</bodyText>
<equation confidence="0.96195">
Vt_1 zt,wj ,
j=1 nt−1 + Q
</equation>
<page confidence="0.984567">
647
</page>
<figureCaption confidence="0.99928525">
Figure 1: Top 10 words for 10 classes learned from
CHILDES
Figure 2: Top 10 words of 5 randomly chosen classes
learned from BNC
</figureCaption>
<bodyText confidence="0.9998145">
Since we are dealing with soft classes, most word-
types have non-zero assignment probabilities for
many classes. Thus frequently occurring words such
as not will typically be listed for several classes.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.996505">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9928977625">
As training data, we extract utterances from the
Manchester corpus (Theakston et al. 2001) in the
CHILDES database (MacWhinney 1995), a corpus
that contains transcripts of conversations with chil-
dren between the ages of 1 year, 8 months and 3
years. We use the mother’s speech from transcripts
of 12 children (henceforth referred to by children’s
names).
We run word class induction while simultane-
ously outputting the highest scoring word-class la-
bel for each word: for a new sentence, we sam-
ple class assignments for each feature (doing J
passes), update the counts, and then for each word
dti output the highest scoring class label according
to argmaxz nt (where nz,dti
z,dti t stands for the num-
ber of times class z co-occurred with word type dti
up to step t).
During development we ran the online word class
induction module on data for Aran, Becky, Carl and
Anne and then started the word learning module for
the Anne portion while continuing inducing cate-
gories. We then evaluated word learning on Anne.
We chose the parameters of the word class induc-
tion module based on those development results:
EK
1=1 α = 10, Q = 0.1, K = 10 and J = 20.
We used cross-validation for the final evaluation.
For each of six data files (Anne, Aran, Becky, Carl,
Dominic and Gail), we ran word-class induction on
the whole corpus with the chosen file last, and then
started applying the word-learning algorithm on this
last chosen file (while continuing with category in-
duction). We evaluated how well word meanings
were learned in those six cases.
We follow Alishahi and Fazly (2010) in the con-
struction of the input. We need a semantic represen-
tation paired with each utterance. Such a represen-
tation is not available from the corpus and has to be
constructed. We automatically construct a gold lexi-
con for all nouns and verbs in this corpus as follows.
For each word, we extract all hypernyms for its first
sense in the appropriate (verb or noun) hierarchy in
WordNet (Fellbaum 1998), and add the first word in
the synset of each hypernym to the set of semantic
features for the target word. For verbs, we also ex-
tract features from VerbNet (Kipper et al. 2006). A
small subset of words (pronouns and frequent quan-
tifiers) are also manually added. This lexicon repre-
sents the true meaning of each word, and is used in
generating the scene representations in the input and
in evaluation.
For each utterance in the input corpus, we form
the union of the feature representations of all its
words. Words not found in the lexicon (i.e. for which
we could not extract a semantic representation from
WordNet and VerbNet) are removed from the utter-
ance (only for the word learning module).
In order to simulate the high level of noise that
children receive from their environment, we follow
Alishahi and Fazly (2010) and pair each utterance
with a combination of its own scene representation
and the scene representation for the following utter-
ance. This decision was based on the intuition that
consequent utterances are more likely to be about re-
do are have can not go put did get play
is that it what not there he was where put
you not I the we what it they your a
to you we and I will not can it on
it a that the not he this right got she
are do is have on in can want did going
one I not shall there then you are we it
is in are on oh with and of have do
the a your of that it this some not very
going want bit go have look got will at little
I you he it they we she, You He
a the more some all no The other I two
as if when that where how because If before what
was is ’s had, has are would did said
the his her their this an that its your my
</bodyText>
<page confidence="0.9875">
648
</page>
<bodyText confidence="0.8328556">
Utterance: { mommy, ate, broccoli }
Scene: { ANIMATE, HUMAN,...,
CONSUMPTION, ACTION,...
BROCCOLI, VEGETABLE,...
PLATE, OBJECT, ... }
</bodyText>
<figureCaption confidence="0.998737">
Figure 3: A sample input item to the word learning model
</figureCaption>
<bodyText confidence="0.998994961538462">
lated topics and scenes. This results in a (roughly)
200% ambiguity. In addition, we remove the mean-
ing of one random word from the scene representa-
tion of every second utterance in an attempt to sim-
ulate cases where the referent of an uttered word is
not within the perception field (such as ‘daddy is not
home yet’). A sample utterance and its correspond-
ing scene are shown in Figure 3.
As mentioned before, many words in our input
corpus are polysemous. For such words, we extract
different sets of features depending on their manu-
ally tagged part of speech and keep them in the lex-
icon (e.g. the lexicon contains two different entries
for set:N and set:V). When constructing a scene rep-
resentation for an utterance which contains an am-
biguous word, we choose the correct sense from our
lexicon according to the word’s part of speech tag in
Manchester corpus.
In the experiments reported in the next section,
we assess the performance of our model on learning
words at each point in time: for each target word,
we compare its set of features in the lexicon with
its probability distribution over the semantic fea-
tures that the model has learned. We use mean aver-
age precision (MAP) to measure how well p(t)(��w)
ranks the features of w.
</bodyText>
<subsectionHeader confidence="0.998959">
4.2 Learning curves
</subsectionHeader>
<bodyText confidence="0.999987454545454">
To understand whether our categories contribute to
learning of word–meaning mappings, we compare
the pattern of word learning over time in four con-
ditions. The first condition represents our baseline,
in which we do not use category-based alignment
in the word learning model by setting A(w) = 1
in Eqn. (1). In the second condition we use a set
of uniformly distributed categories for alignment,
as estimated by Eqn. (3) on page 3 (this condition
is introduced to examine whether categories act as
more than a simple smoothing factor in the align-
</bodyText>
<table confidence="0.9968674">
Category Avg. MAP Std. Dev.
None 0.626 0.032
Uniform 0.633 0.032
LDA 0.659 0.029
POS 0.672 0.030
</table>
<tableCaption confidence="0.999726">
Table 3: Final Mean Average Precision scores
</tableCaption>
<bodyText confidence="0.99992685">
ment process.) In the third condition we use the cat-
egories induced by online LDA in the word learning
model. The fourth condition represents the perfor-
mance ceiling, in which we use the pre-defined and
manually annotated part of speech categories from
the Manchester corpus.
Table 3 shows the average and the standard devia-
tion of the final MAP scores across the six datasets,
for the four conditions (no categories, uniform cat-
egories, LDA categories and gold part-of-speech
tags). The differences between LDA and None, and
between LDA and Uniform are statistically signif-
icant according to the paired t test (p &lt; 0.01),
while the difference between LDA and POS is not
(p = 0.16).
Figure 4 shows the learning curves in each con-
dition, averaged over the six splits explained in the
previous section. The top panel shows the average
learning curve over the minimum number of sen-
tences across the six sub-corpora (8800 sentences).
The curves show that our LDA categories signifi-
cantly improve the performance of the model over
both baselines. That means that using these cate-
gories can improve word learning compared to not
using them and relying on cross-situational evidence
alone. Moreover, LDA-induced categories are not
merely acting as a smoothing function the way the
‘uniform’ categories are. Our results show that they
are bringing relevant information to the task at hand,
that is, improving word learning by using the sen-
tential context. In fact, this improvement is compa-
rable to the improvement achieved by integrating the
‘gold-standard’ POS categories.
The middle and bottom panels of Figure 4 zoom
in on shorter time spans (5000 and 1000 sentences,
respectively). These diagrams suggest that the pat-
tern of improvement over baseline is relatively con-
stant, even at very early stages of learning. In fact,
once the model receives enough input data, cross-
situational evidence becomes stronger (since fewer
</bodyText>
<page confidence="0.998351">
649
</page>
<bodyText confidence="0.999484333333333">
words in the input are encountered for the first time)
and the contribution of the categories becomes less
significant.
</bodyText>
<subsectionHeader confidence="0.999196">
4.3 Class granularity
</subsectionHeader>
<bodyText confidence="0.999923266666667">
In Figure 5 we show the influence of the number of
word classes used on the performance in word learn-
ing. It is evident that in the range between 5 to 20
classes the performance of the word learning module
is quite stable and insensitive to the exact class gran-
ularity. Even with only 5 classes the model can still
roughly distinguish noun-like words from verb-like
words from pronoun-like words, and this will help
learn the meaning elements derived from the higher
levels of WordNet hierarchy. Notwithstanding that,
ideally we would like to avoid having to pre-specify
the number of classes for the word class induction
module: we thus plan to investigate non-parametric
models such as Hierarchical Dirichlet Process for
this purpose.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9998154">
This paper investigates the interplay between two
language learning tasks which have so far been stud-
ied in isolation: the acquisition of lexical categories
from distributional clues, and learning the mapping
between words and meanings. Previous models
have shown that lexical categories can be learned
from unannotated text, mainly drawing on distri-
butional properties of words (e.g. Redington et al.
1998, Clark 2000, Mintz 2003, Parisien et al. 2008,
Chrupała and Alishahi 2010).
Independently, several computational models
have exploited cross-situational evidence in learning
the correct mappings between words and meanings,
using rule-based inference (Siskind 1996), neural
networks (Li et al. 2004, Regier 2005), hierarchical
Bayesian models (Frank et al. 2007) and probabilis-
tic alignment inspired by machine translation mod-
els (Yu 2005, Fazly et al. 2010).
There are only a few existing computational mod-
els that explore the role of syntax in word learning.
Maurits et al. (2009) investigates the joint acquisi-
tion of word meaning and word order using a batch
model. This model is tested on an artificial language
with a simple first order predicate representation of
meaning, and limited built-in possibilities for word
</bodyText>
<figureCaption confidence="0.950002">
Figure 4: Mean average precision for all observed words
at each point in time for four conditions: with gold
POS categories, with LDA categories, with uniform cate-
gories, and without using categories. Each panel displays
a different time span.
</figureCaption>
<figure confidence="0.909787481481481">
Mean Average Precision
Mean Average Precision
Mean Average Precision
0.35 0.45 0.55 0.65
0.35 0.45 0.55 0.65
0.35 0.45 0.55 0.65
0 1000 2000 3000 4000 5000
Input Items
(b) first 5000 sentences
(c) first 1000 sentences
200 400 600 800 1000
Input Items
2000 4000 6000 8000
Input Items
(a) all sentences
POS
LDA
Uniform
None
POS
LDA
Uniform
None
POS
LDA
Uniform
None
</figure>
<page confidence="0.771689">
650
</page>
<figureCaption confidence="0.97107425">
Figure 5: Mean average precision for all observed words
at each point in time in four conditions: using online LDA
categories of varying numbers of 20, 10 and 5, and with-
out using categories.
</figureCaption>
<bodyText confidence="0.999904363636364">
order. The model of Niyogi (2002) simulates the
mutual bootstrapping effects of syntactic and seman-
tic knowledge in verb learning, that is the use of syn-
tax to aid in inducing the semantics of a verb, and the
use of semantics to narrow down possible syntactic
frames in which a verb can participate. However,
this model relies on manually assigned priors for as-
sociations between syntactic and semantic features,
and is tested on a toy language with very limited vo-
cabulary and a constrained syntax.
Yu (2006) integrates automatically induced syn-
tactic word categories into his model of cross-
situational word learning, showing that they can im-
prove the model’s performance. Yu’s model also
processes input utterances in a batch mode, and its
evaluation is limited to situations in which only a
coarse distinction between referring words (words
that could potentially refer to objects in a scene, e.g.
concrete nouns) and non-referring words (words that
cannot possibly refer to objects, e.g. function words)
is sufficient. It is thus not clear whether information
about finer-grained categories (e.g. verbs and nouns)
can indeed help word learning in a more naturalistic
incremental setting.
On the other hand, the model of Alishahi and
Fazly (2010) integrates manually annotated part-of-
speech tags into an incremental word learning al-
gorithm, and shows that these tags boost the over-
all word learning performance, especially for infre-
quent words.
In a different line of research, a number of mod-
els have been proposed which study the acquisition
of the link between syntax and semantics within the
Combinatory Categorial Grammar (CCG) frame-
work (Briscoe 1997, Villavicencio 2002, Buttery
2006, Kwiatkowski et al. 2012). These approaches
set the parameters of a semantic parser on a cor-
pus of utterances paired with a logical form as their
meaning.
These models bring in extensive and detailed prior
assumptions about the nature of the syntactic repre-
sentation (i.e. atomic categories such as S and NP,
and built-in rules which govern their combination),
as well as about the representation of meaning via
the formalism of lambda calculus.
This is fundamentally different than the approach
taken in this paper, which in comparison only as-
sumes very simple syntactic and semantic represen-
tations of syntax. We view word and category learn-
ing as stand-alone cognitive tasks with independent
representations (word meanings as probabilistic col-
lections of properties or features as opposed to sin-
gle symbols; categories as sets of word tokens with
similar context distribution) and we do not bring in
any prior knowledge of specific atomic categories.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999947">
In this paper, we show the plausibility of using
automatically and incrementally induced categories
while learning word meanings. Our results suggest
that the sentential context that a word appears in
across its different uses can be used as a complemen-
tary source of guidance for mapping it to its featural
meaning representation.
In Section 4 we show that the improvement
achieved by our categories is comparable to that
gained by integrating gold POS categories. This re-
sult is very encouraging, since manually assigned
POS tags are typically believed to set the upper
bound on the usefulness of category information.
We believe that it automatically induced cate-
gories have the potential to do even better: Chrupała
and Alishahi (2010) have shown that categories in-
duced from usage data in an unsupervised fashion
can be used more effectively than POS categories in
</bodyText>
<figure confidence="0.993544">
Mean Average Precision
0.40 0.45 0.50 0.55 0.60 0.65 0.70
0 2000 4000 6000 8000
Input Items
20 LDA classes
10 LDA classes
5 LDA classes
No categories
</figure>
<page confidence="0.993088">
651
</page>
<bodyText confidence="0.9997123125">
a number of tasks. In our experiments here on the
development data we observed some improvements
over POS categories. This advantage can result from
the fact that our categories are more fine-grained (if
also more noisy) than POS categories, which some-
times yields more accurate predictions.
One important characteristic of the category in-
duction algorithm we have used in this paper is that
it provides a soft categorization scheme, where each
word is associated with a probability distribution
over all categories. In future, we plan to exploit this
feature: when estimating the category-based align-
ment, we can interpolate predictions of multiple cat-
egories to which a word belongs, weighted by its
probabilities associated with membership in each
category.
</bodyText>
<sectionHeader confidence="0.996932" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999736">
Grzegorz Chrupała was funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under grant number 01IC10S01O as part of
the Software-Cluster project EMERGENT (www.
software-cluster.org).
</bodyText>
<sectionHeader confidence="0.990321" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.838461353846154">
Alishahi, A. and Fazly, A. (2010). Integrating
Syntactic Knowledge into a Model of Cross-
situational Word Learning. In Proceedings of the
32nd Annual Conference of the Cognitive Science
Society.
Banerjee, A. and Basu, S. (2007). Topic models over
text streams: A study of batch and online unsuper-
vised learning. In SIAM Data Mining.
Blei, D., Ng, A., and Jordan, M. (2003). La-
tent dirichlet allocation. The Journal of Machine
Learning Research, 3:993–1022.
Briscoe, T. (1997). Co-evolution of language and
of the language acquisition device. In Proceed-
ings of the eighth conference on European chap-
ter of the Association for Computational Linguis-
tics, pages 418–427. Association for Computa-
tional Linguistics.
Brown, P. F., Mercer, R. L., Della Pietra, V. J., and
Lai, J. C. (1992). Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467–479.
Buttery, P. (2006). Computational models for first
language acquisition. Computer Laboratory, Uni-
versity of Cambridge, Tech. Rep. UCAM-CLTR-
675.
Canini, K., Shi, L., and Griffiths, T. (2009). Online
inference of topics with latent dirichlet allocation.
In Proceedings of the International Conference on
Artificial Intelligence and Statistics.
Chrupała, G. (2011). Efficient induction of proba-
bilistic word classes with LDA. In International
Joint Conference on Natural Language Process-
ing.
Chrupała, G. and Alishahi, A. (2010). Online
Entropy-based Model of Lexical Category Acqui-
sition. In CoNLL 2010.
Clark, A. (2000). Inducing syntactic categories by
context distribution clustering. In Proceedings of
the 2nd workshop on Learning Language in Logic
and the 4th conference on Computational Natural
Language Learning, pages 91–94. Association for
Computational Linguistics Morristown, NJ, USA.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A Probabilistic Computational Model of Cross-
Situational Word Learning. Cognitive Science,
34(6):1017–1063.
Fellbaum, C., editor (1998). WordNet, An Electronic
Lexical Database. MIT Press.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2007). A Bayesian framework for cross-
situational word-learning. In Advances in Neural
Information Processing Systems, volume 20.
Gelman, S. and Taylor, M. (1984). How two-year-
old children interpret proper and common names
for unfamiliar objects. Child Development, pages
1535–1540.
Gertner, Y., Fisher, C., and Eisengart, J. (2006).
Learning words and rules: Abstract knowledge of
word order in early sentence comprehension. Psy-
chological Science, 17(8):684–691.
Gillette, J., Gleitman, H., Gleitman, L., and Led-
erer, A. (1999). Human simulations of vocabulary
learning. Cognition, 73(2):135–76.
Gleitman, L. (1990). The structural sources of verb
meanings. Language Acquisition, 1:135–176.
</reference>
<page confidence="0.979557">
652
</page>
<reference confidence="0.99912091011236">
Hoffman, M., Blei, D., and Bach, F. (2010). On-
line learning for latent dirichlet allocation. In
Advances in Neural Information Processing Sys-
tems.
Kemp, N., Lieven, E., and Tomasello, M. (2005).
Young Children’s Knowledge of the” Deter-
miner” and” Adjective” Categories. Journal
of Speech, Language and Hearing Research,
48(3):592–609.
Kipper, K., Korhonen, A., Ryant, N., and Palmer,
M. (2006). Extensive classifications of english
verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
Koehne, J. and Crocker, M. W. (2010). Sen-
tence processing mechanisms influence cross-
situational word learning. In Proceedings of the
Annual Conference of the Cognitive Science Soci-
ety.
Koehne, J. and Crocker, M. W. (2011). The inter-
play of multiple mechanisms in word learning. In
Proceedings of the Annual Conference of the Cog-
nitive Science Society.
Kwiatkowski, T., Goldwater, S., Zettelmoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. In Pro-
ceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics.
Li, P., Farkas, I., and MacWhinney, B. (2004). Early
lexical development in a self-organizing neural
network. Neural Networks, 17:1345–1362.
Lidz, J., Bunger, A., Leddon, E., Baier, R., and Wax-
man, S. R. (2010). When one cue is better than
two: lexical vs . syntactic cues to verb learning.
Unpublished manuscript.
MacWhinney, B. (1995). The CHILDES Project:
Tools forAnalyzing Talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Maurits, L., Perfors, A. F., and Navarro, D. J. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31st Annual Confer-
ence of the Cognitive Science Society.
Mintz, T. (2003). Frequent frames as a cue for gram-
matical categories in child directed speech. Cog-
nition, 90(1):91–117.
Monaghan, P. and Mattock, K. (2009). Cross-
situational language learning: The effects of
grammatical categories as constraints on referen-
tial labeling. In Proceedings of the 31st Annual
Conference of the Cognitive Science Society.
Naigles, L. and Hoff-Ginsberg, E. (1995). Input to
Verb Learning: Evidence for the Plausibility of
Syntactic Bootstrapping. Developmental Psychol-
ogy, 31(5):827–37.
Niyogi, S. (2002). Bayesian learning at the syntax-
semantics interface. In Proceedings of the 24th
annual conference of the Cognitive Science Soci-
ety, pages 697–702.
Parisien, C., Fazly, A., and Stevenson, S. (2008). An
incremental bayesian model for learning syntactic
categories. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learn-
ing.
Pinker, S. (1989). Learnability and Cognition: The
Acquisition of Argument Structure. Cambridge,
MA: MIT Press.
Quine, W. (1960). Word and Object. Cambridge
University Press, Cambridge, MA.
Redington, M., Crater, N., and Finch, S. (1998). Dis-
tributional information: A powerful cue for ac-
quiring syntactic categories. Cognitive Science:
A Multidisciplinary Journal, 22(4):425–469.
Regier, T. (2005). The emergence of words: Atten-
tional learning in form and meaning. Cognitive
Science, 29:819–865.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39–91.
Smith, L. and Yu, C. (2007). Infants rapidly learn
words from noisy data via cross-situational statis-
tics. In Proceedings of the 29th Annual Confer-
ence of the Cognitive Science Society.
Song, X., Lin, C., Tseng, B., and Sun, M. (2005).
Modeling and predicting personal information
dissemination behavior. In Proceedings of the
eleventh ACM SIGKDD international conference
on Knowledge discovery in data mining, pages
479–488. ACM.
</reference>
<page confidence="0.990595">
653
</page>
<reference confidence="0.999265294117647">
Theakston, A. L., Lieven, E. V., Pine, J. M., and
Rowland, C. F. (2001). The role of performance
limitations in the acquisition of verb-argument
structure: An alternative account. Journal of
Child Language, 28:127–152.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial grammar.
In Proceedings of the Third CLUK Colloquium,
pages 59–66.
Yu, C. (2005). The emergence of links between
lexical acquisition and object categorization: A
computational study. Connection Science, 17(3–
4):381–397.
Yu, C. (2006). Learning syntax–semantics mappings
to bootstrap word learning. In Proceedings of the
28th Annual Conference of the Cognitive Science
Society.
</reference>
<page confidence="0.999013">
654
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.77005375">Concurrent Acquisition of Word Meaning and Lexical Categories Afra Alishahi Grzegorz Chrupała a.alishahi@uvt.nl gchrupala@lsv.uni-saarland.de Communication and Information Sciences Spoken Language Systems</title>
<affiliation confidence="0.732827">Tilburg University, The Netherlands Saarland University, Germany</affiliation>
<abstract confidence="0.998155097222222">Learning the meaning of words from ambiguous and noisy context is a challenging task for language learners. It has been suggested that children draw on syntactic cues such as lexical categories of words to constrain potential referents of words in a complex scene. Although the acquisition of lexical categories should be interleaved with learning word meanings, it has not previously been modeled in that fashion. In this paper, we investigate the interplay of word learning and category induction by integrating an LDA-based word class learning module with a probabilistic word learning model. Our results show that the incrementally induced word classes significantly improve word learning, and their contribution is comparable to that of manually assigned part of speech categories. 1 Learning the Meaning of Words For young learners of a natural language, mapping each word to its correct meaning is a challenging task. Words are often used as part of an utterance rather than in isolation. The meaning of an utterance must be inferred from among numerous possible interpretations that the (usually complex) surrounding scene offers. In addition, the linguistic and visual context in which words are heard and used is often noisy and highly ambiguous. Particularly, many words in a language are polysemous and have different meanings. Various learning mechanisms have been proposed for word learning. One well-studied mechanism a bottom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word a hypothesis known as bootstrap- 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as</abstract>
<note confidence="0.784163">643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural pages 643–654, Jeju Island, Korea, 12–14 July 2012. Association for Computational Linguistics</note>
<abstract confidence="0.992915383392228">input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). However, none of the existing experimental or computational studies have examined the acquisition of word meanings and lexical categories in parallel. They all make the simplifying assumption that the onset of word learning, the categorization module has already formed a relatively robust set of lexical categories. This assumption can be justified in the case of adult learners of a second or artificial language. But children’s acquisition of categories is most probably interleaved with the acquisition of word meaning, and these two processes must ultimately be studied simultaneously. In this paper, we investigate concurrent acquisition of word meanings and lexical categories. We use an online version of the LDA algorithm to induce a set of word classes from child-directed speech, and integrate them into an existing probabilistic model of word learning which combines cross-situational evidence with cues from lexical categories. Through a number of simulations of a word learning scenario, we show that our automatically and incrementally induced categories significantly improve the performance of the word learning model, and are closely comparable to a set of goldstandard, manually-annotated part of speech tags. 2 A Word Learning Model We want to investigate whether lexical categories (i.e. word classes) that are incrementally induced from child-directed speech can improve the performance of a cross-situational word learning model. For this purpose, we use the model of Alishahi and Fazly (2010). This model uses a probabilistic learning algorithm for combining evidence from word– referent co-occurrence statistics and the meanings associated with a set of pre-defined categories. They use child-directed utterances, manually annotated with a small set of part of speech tags, from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995). Their experimental results show that integrating these goldstandard categories into the algorithm boosts its performance over a pure cross-situational version. The model of Alishahi and Fazly (2010) has the suitable architecture for our goal: it provides an integrated learning mechanism which combines evidence from word-referent co-occurrence with cues from the meaning representation associated with word categories. However, the model has two major shortcomings. First, it assumes that lexical categories are formed and finalized prior to the onset of word learning and that a correct and unique category for a target word can be identified at each point in time, assumptions that are highly unlikely. Second, it does not handle any ambiguity in the meaning of a word. Instead, each word is assumed to have only one correct meaning. Considering the high level of lexical ambiguity in most natural languages, this assumption unreasonably simplifies the word learning problem. To investigate the plausibility of integrating word and category learning, we use an online algorithm for automatically and incrementally inducing a set of lexical categories. Moreover, we use each word in its original form instead of lemmatizing them, which implies that categories contain different morphological forms of the same word. By applying these changes, we are able to study the contribution of lexical categories to word learning in a more realistic scenario. of input. input to the model consists of a sequence of utterances, each paired with a representation of an observed scene. We repan utterance as a set of words, went, home, and the corresponding as a set of semantic features, and category meaning. represent the meaning of a word as a time-dependent probability all the semantic features, the probability of feature beassociated with word time In the absence of any prior knowledge, the model assumes a uniform distribution over all features as the meaning of novel word. Also, a function us category to which a word utterance belongs. each point in time, a category a set word tokens. We assign a meaning to each cat- 644 egory as a weighted sum of the meaning learned far for each of its members, or = where the number of tokens in the current moment. algorithm. an utterance-scene pair at time the model first calcuan alignment score each word each semantic feature A semantic feature can be aligned to a word according to the meaning acquired for that word from previous observa- (word-based alignment, or Alternatively, distributional clues of the word can be used to determine its category, and the semantic features can be aligned to the word according to the meaning associated to its category (category-based alignment, We combine these two sources of evidence when estimating an alignment score: = where the word-based and category-based alignment scores are estimated based on the acquired meanings of the word and its category, respectively: = = The relative contribution of the word-based versus the category-based alignment is determined by the function Cross-situational evidence is a reliable cue for frequent words; on the other hand, the category-based score is most informative when the model encounters a low-frequency word (See Alishahi and Fazly (2010) for a full analysis of frequency effect). Therefore, we define function of the frequency of the word = + 1) Once an alignment score is calculated for each each feature the model the meanings of all the words in their corresponding categories as follows: = + zero if not co-occurred before. These association scores are then used to update the meaning of the words in the current input: = the set of all features seen so far. We use a smoothed version of this formula to accommodate noisy or rare input. This process is repeated for all the input pairs, one at a time. categories. the category-based alignment as a new factor to Eqn. (1) might imply that the role of categories in this model is nothing more than smoothing the cross-situational-based alignment of words and referents. In order to investigate this issue, we use the following alignment formula as an informed baseline in our experiments, we replace a uniform = 1 estimated as before. In our experiments in Section 4, we refer to this baseline as the ‘uniform’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with LDA Latent Dirichlet Allocation (LDA) was introduced by Blei et al. (2003) and is most commonly used for modeling the topic structure in document collections. It is a generative, probabilistic hierarchical Bayesian model that induces a set of latent variables, which correspond to the topics. The topics themselves are multinomial distributions over words. The generative structure of the LDA model is the following: k (4) d Chrupała (2011) reinterprets the LDA model in of word classes as follows: the number classes, the number of unique word types, the number of context features (such as right or neighbor) associated with word type the of word type the and the feature of word type Hyperparameters the sparseness of the vectors Wordtype Features How do you Table 1: Matrix of context features 1.8M words (CHILDES) 100M words (BNC) Table 2: Most similar word pairs As an example consider the small corpus consistof the single sentence do you The rows Table 1 show the features each type we use each word’s left and right as features, and subscript words with indicate left and right. inference, the correspond to word class probability distributions given a word while the to feature distributions given a word class: the model provides a probabilistic representation for word types independently of their context, and also for contexts independently of the word type. classes are more expressive than hard categories. First, they make it easy and efficient to express shared ambiguities: Chrupała (2011) gives an example of words used as either first names or surnames, and this shared ambiguity is reflected in the similarity of their word class distributions. Second, with soft word classes it becomes easy to express graded similarity between words: as an example, Table 2 shows a random selection out of the 100 most similar word pairs according to the Jensen-Shannon divergence between their word class distributions, according to a word class model with 25 classes induced from (i) 1.8 million words of the CHILDES corpus or (ii) 100 million word of the BNC corpus. The similarities were measured between each of the 1000 most frequent CHILDES or BNC words. train car can will give bring June March shoes clothes man woman book hole black white monkey rabbit business language 646 3.2 Online Gibbs sampling for LDA There have been a number of attempts to develop online inference algorithms for topic modeling with LDA. A simple modification of the standard Gibbs was proposed by Song et al. (2005) and Banerjee and Basu (2007). Canini et al. (2009) experiment with three samalgorithms for online topic inference: (i) o- (ii) incremental Gibbs sampler, and (iii) a parfilter. Only truly online in the sense that it does not revisit previously seen documents. The other two, the incremental Gibbs sampler and the particle filter, keep seen documents and periodically resample them. In Canini et al.’s experiments all of the online algorithms perform worse than the standard batch Gibbs sampler on a document clustering task. Hoffman et al. (2010) develop an online version of the variational Bayes (VB) optimization method for inference for topic modeling with LDA. Their method achieves good empirical results compared to batch VB as measured by perplexity on heldout data, especially when used with large minibatch sizes. Online VB for LDA is appropriate when streaming documents: with online VB documents are represented as word count tables. In our scenario where we apply LDA to modeling word classes we need to process context features from sentences arriving in a stream: i.e. we need to sample entries from a table like Table 1 in order of arrival rather than row by row. This means that online VB is not directly applicable to online word-class induction. it also means that one issue with identified by Canini et al. (2009) is ameliorated. When sampling in a topic modeling setting, documents are unique and are never seen again. Thus, topics associated with old documents get need to be periodically resampled). This is the reason why the incremental Gibbs sampler and the particle filter algorithms in Canini et al. (2009) need to keep old documents around and cannot run in a true online fashion. Since for word class modeling we stream context features as they arrive, we will continue to see features associated with the seen word types, and will automatically resample their class assignments. In exploratory experiments we have seen that this narrows the pergap between the and the batch collapsed Gibbs sampler. present our version of the in 1. For each incoming sentence run passes of sampling, updating the counts tables after each sampling step. We sample the class assignment feature to: (5) for the number of times class with word type to step and simthe number of times feature asto class the number of unique features up to step while the LDA hyperparameters. There are two differences between the our version: we do not initialize the algorithm with a batch run over a prefix of the data, and we allow more than one sampling pass per experiments have shown that batch initialization is unnecessary, and that multiple passes typically improve the quality of the induced word classes. 1 Gibbs sampler for word class induction with LDA 1 oo 1 1 t t Figure 1 shows the top 10 words for each of the 10 word classes induced with our online Gibbs sampler from 1.8 million words of CHILDES. Similarly, Figure 2 shows the top 10 words for 5 randomly chosen topics out of 50, learned online from 100 million words of the BNC. The topics are relatively coherent and at these levels of granularity express mostly part of speech and subcategorization frame information. Note that for each word class we show the words most frequently assigned to it while Gibbs sampling. that we do not allow multiple passes over the stream of sentences. Rather, while processing the current sentence, we allow the words in this sentence to be sampled more than once. 647 Figure 1: Top 10 words for 10 classes learned from CHILDES Figure 2: Top 10 words of 5 randomly chosen classes learned from BNC Since we are dealing with soft classes, most wordtypes have non-zero assignment probabilities for many classes. Thus frequently occurring words such typically be listed for several classes. 4 Evaluation 4.1 Experimental setup As training data, we extract utterances from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995), a corpus that contains transcripts of conversations with children between the ages of 1 year, 8 months and 3 years. We use the mother’s speech from transcripts of 12 children (henceforth referred to by children’s names). We run word class induction while simultaneously outputting the highest scoring word-class label for each word: for a new sentence, we samclass assignments for each feature (doing passes), update the counts, and then for each word the highest scoring class label according for the numof times class with word type to step During development we ran the online word class induction module on data for Aran, Becky, Carl and Anne and then started the word learning module for the Anne portion while continuing inducing categories. We then evaluated word learning on Anne. We chose the parameters of the word class induction module based on those development results: 10 We used cross-validation for the final evaluation. For each of six data files (Anne, Aran, Becky, Carl, Dominic and Gail), we ran word-class induction on the whole corpus with the chosen file last, and then started applying the word-learning algorithm on this last chosen file (while continuing with category induction). We evaluated how well word meanings were learned in those six cases. We follow Alishahi and Fazly (2010) in the construction of the input. We need a semantic representation paired with each utterance. Such a representation is not available from the corpus and has to be constructed. We automatically construct a gold lexicon for all nouns and verbs in this corpus as follows. For each word, we extract all hypernyms for its first sense in the appropriate (verb or noun) hierarchy in WordNet (Fellbaum 1998), and add the first word in the synset of each hypernym to the set of semantic features for the target word. For verbs, we also extract features from VerbNet (Kipper et al. 2006). A small subset of words (pronouns and frequent quantifiers) are also manually added. This lexicon reprethe of each word, and is used in generating the scene representations in the input and in evaluation. For each utterance in the input corpus, we form the union of the feature representations of all its words. Words not found in the lexicon (i.e. for which we could not extract a semantic representation from WordNet and VerbNet) are removed from the utterance (only for the word learning module). In order to simulate the high level of noise that children receive from their environment, we follow Alishahi and Fazly (2010) and pair each utterance with a combination of its own scene representation and the scene representation for the following utterance. This decision was based on the intuition that utterances are more likely to be about redo are have can not go put did get play is that it what not there he was where put you not I the we what it they your a to you we and I will not can it on it a that the not he this right got she are do is have on in can want did going one I not shall there then you are we it is in are on oh with and of have do the a your of that it this some not very going want bit go have look got will at little I you he it they we she, You He a the more some all no The other I two as if when that where how because If before what was is ’s had, has are would did said the his her their this an that its your my 648 ate, broccoli ... Figure 3: A sample input item to the word learning model lated topics and scenes. This results in a (roughly) 200% ambiguity. In addition, we remove the meaning of one random word from the scene representation of every second utterance in an attempt to simulate cases where the referent of an uttered word is not within the perception field (such as ‘daddy is not home yet’). A sample utterance and its corresponding scene are shown in Figure 3. As mentioned before, many words in our input corpus are polysemous. For such words, we extract different sets of features depending on their manually tagged part of speech and keep them in the lexicon (e.g. the lexicon contains two different entries When constructing a scene representation for an utterance which contains an ambiguous word, we choose the correct sense from our lexicon according to the word’s part of speech tag in Manchester corpus. In the experiments reported in the next section, we assess the performance of our model on learning words at each point in time: for each target word, we compare its set of features in the lexicon with its probability distribution over the semantic features that the model has learned. We use mean averprecision (MAP) to measure how well the features of 4.2 Learning curves To understand whether our categories contribute to learning of word–meaning mappings, we compare the pattern of word learning over time in four conditions. The first condition represents our baseline, in which we do not use category-based alignment the word learning model by setting = 1 in Eqn. (1). In the second condition we use a set of uniformly distributed categories for alignment, as estimated by Eqn. (3) on page 3 (this condition is introduced to examine whether categories act as than a simple smoothing factor in the align- Category Avg. MAP Std. Dev. None 0.626 0.032 Uniform 0.633 0.032 LDA 0.659 0.029 POS 0.672 0.030 Table 3: Final Mean Average Precision scores ment process.) In the third condition we use the categories induced by online LDA in the word learning model. The fourth condition represents the performance ceiling, in which we use the pre-defined and manually annotated part of speech categories from the Manchester corpus. Table 3 shows the average and the standard deviaof the scores across the six datasets, for the four conditions (no categories, uniform categories, LDA categories and gold part-of-speech tags). The differences between LDA and None, and between LDA and Uniform are statistically signifaccording to the paired &lt; while the difference between LDA and POS is not Figure 4 shows the learning curves in each condition, averaged over the six splits explained in the previous section. The top panel shows the average learning curve over the minimum number of sentences across the six sub-corpora (8800 sentences). The curves show that our LDA categories significantly improve the performance of the model over both baselines. That means that using these categories can improve word learning compared to not using them and relying on cross-situational evidence alone. Moreover, LDA-induced categories are not merely acting as a smoothing function the way the ‘uniform’ categories are. Our results show that they are bringing relevant information to the task at hand, that is, improving word learning by using the sentential context. In fact, this improvement is comparable to the improvement achieved by integrating the ‘gold-standard’ POS categories. The middle and bottom panels of Figure 4 zoom in on shorter time spans (5000 and 1000 sentences, respectively). These diagrams suggest that the pattern of improvement over baseline is relatively constant, even at very early stages of learning. In fact, once the model receives enough input data, crosssituational evidence becomes stronger (since fewer 649 words in the input are encountered for the first time) and the contribution of the categories becomes less significant. 4.3 Class granularity In Figure 5 we show the influence of the number of word classes used on the performance in word learning. It is evident that in the range between 5 to 20 classes the performance of the word learning module is quite stable and insensitive to the exact class granularity. Even with only 5 classes the model can still roughly distinguish noun-like words from verb-like words from pronoun-like words, and this will help learn the meaning elements derived from the higher levels of WordNet hierarchy. Notwithstanding that, ideally we would like to avoid having to pre-specify the number of classes for the word class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four conditions: with gold POS categories, with LDA categories, with uniform categories, and without using categories. Each panel displays a different time span.</abstract>
<title confidence="0.970925333333333">Mean Average Precision Mean Average Precision Mean Average Precision</title>
<phone confidence="0.637616">0.35 0.45 0.55 0.65 0.35 0.45 0.55 0.65 0.35 0.45 0.55 0.65 0 1000 2000 3000 4000 5000</phone>
<note confidence="0.816471">Input Items (b) first 5000 sentences (c) first 1000 sentences</note>
<phone confidence="0.834133">200 400 600 800 1000</phone>
<affiliation confidence="0.867651">Input Items</affiliation>
<address confidence="0.652928">2000 4000 6000 8000</address>
<title confidence="0.884433571428571">Input Items sentences POS LDA Uniform None POS LDA Uniform None POS LDA Uniform None</title>
<abstract confidence="0.896567268115942">650 Figure 5: Mean average precision for all observed words at each point in time in four conditions: using online LDA categories of varying numbers of 20, 10 and 5, and without using categories. order. The model of Niyogi (2002) simulates the mutual bootstrapping effects of syntactic and semantic knowledge in verb learning, that is the use of syntax to aid in inducing the semantics of a verb, and the use of semantics to narrow down possible syntactic frames in which a verb can participate. However, this model relies on manually assigned priors for associations between syntactic and semantic features, and is tested on a toy language with very limited vocabulary and a constrained syntax. Yu (2006) integrates automatically induced syntactic word categories into his model of crosssituational word learning, showing that they can improve the model’s performance. Yu’s model also processes input utterances in a batch mode, and its evaluation is limited to situations in which only a coarse distinction between referring words (words that could potentially refer to objects in a scene, e.g. concrete nouns) and non-referring words (words that cannot possibly refer to objects, e.g. function words) is sufficient. It is thus not clear whether information about finer-grained categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syntactic and semantic representations of syntax. We view word and category learning as stand-alone cognitive tasks with independent representations (word meanings as probabilistic collections of properties or features as opposed to single symbols; categories as sets of word tokens with similar context distribution) and we do not bring in any prior knowledge of specific atomic categories. 6 Conclusion In this paper, we show the plausibility of using automatically and incrementally induced categories while learning word meanings. Our results suggest that the sentential context that a word appears in across its different uses can be used as a complementary source of guidance for mapping it to its featural meaning representation. In Section 4 we show that the improvement achieved by our categories is comparable to that gained by integrating gold POS categories. This result is very encouraging, since manually assigned POS tags are typically believed to set the upper bound on the usefulness of category information. We believe that it automatically induced categories have the potential to do even better: Chrupała and Alishahi (2010) have shown that categories induced from usage data in an unsupervised fashion can be used more effectively than POS categories in Mean Average Precision 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0 2000 4000 6000 8000 Input Items 20 LDA classes 10 LDA classes 5 LDA classes No categories 651 a number of tasks. In our experiments here on the development data we observed some improvements over POS categories. This advantage can result from the fact that our categories are more fine-grained (if also more noisy) than POS categories, which sometimes yields more accurate predictions. One important characteristic of the category induction algorithm we have used in this paper is that provides a scheme, where each word is associated with a probability distribution over all categories. In future, we plan to exploit this feature: when estimating the category-based alignment, we can interpolate predictions of multiple categories to which a word belongs, weighted by its probabilities associated with membership in each category. Acknowledgements Grzegorz Chrupała was funded by the German Federal Ministry of Education and Research (BMBF) under grant number 01IC10S01O as part of Software-Cluster project References Alishahi, A. and Fazly, A. (2010). Integrating Syntactic Knowledge into a Model of Cross- Word Learning. In of the 32nd Annual Conference of the Cognitive Science Banerjee, A. and Basu, S. (2007). Topic models over text streams: A study of batch and online unsuperlearning. In Data Blei, D., Ng, A., and Jordan, M. (2003). Ladirichlet allocation. Journal of Machine 3:993–1022. Briscoe, T. (1997). Co-evolution of language and the language acquisition device. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguispages 418–427. Association for Computational Linguistics. Brown, P. F., Mercer, R. L., Della Pietra, V. J., and Lai, J. C. (1992). Class-based n-gram models natural language. 18(4):467–479. Buttery, P. (2006). Computational models for first acquisition. Laboratory, University of Cambridge, Tech. Rep. UCAM-CLTR- Canini, K., Shi, L., and Griffiths, T. (2009). Online inference of topics with latent dirichlet allocation. of the International Conference on Intelligence and Chrupała, G. (2011). Efficient induction of proba-</abstract>
<note confidence="0.876772111111111">word classes with LDA. In Joint Conference on Natural Language Process- Chrupała, G. and Alishahi, A. (2010). Online Entropy-based Model of Lexical Category Acqui- In Clark, A. (2000). Inducing syntactic categories by distribution clustering. In of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural pages 91–94. Association for Computational Linguistics Morristown, NJ, USA. Fazly, A., Alishahi, A., and Stevenson, S. (2010). A Probabilistic Computational Model of Cross- Word Learning. 34(6):1017–1063. C., editor (1998). An Electronic MIT Press. Frank, M. C., Goodman, N. D., and Tenenbaum,</note>
<abstract confidence="0.903402285714286">J. B. (2007). A Bayesian framework for crossword-learning. In in Neural Processing volume 20. Gelman, S. and Taylor, M. (1984). How two-yearold children interpret proper and common names unfamiliar objects. pages 1535–1540.</abstract>
<note confidence="0.881006133333333">Gertner, Y., Fisher, C., and Eisengart, J. (2006). Learning words and rules: Abstract knowledge of order in early sentence comprehension. Psy- 17(8):684–691. Gillette, J., Gleitman, H., Gleitman, L., and Lederer, A. (1999). Human simulations of vocabulary 73(2):135–76. Gleitman, L. (1990). The structural sources of verb 1:135–176. 652 Hoffman, M., Blei, D., and Bach, F. (2010). Online learning for latent dirichlet allocation. In Advances in Neural Information Processing Sys- Kemp, N., Lieven, E., and Tomasello, M. (2005). Young Children’s Knowledge of the” Deterand” Adjective” Categories. Speech, Language and Hearing 48(3):592–609. Kipper, K., Korhonen, A., Ryant, N., and Palmer, M. (2006). Extensive classifications of english In of the 12th EURALEX In- Koehne, J. and Crocker, M. W. (2010). Sentence processing mechanisms influence crossword learning. In of the Annual Conference of the Cognitive Science Soci- Koehne, J. and Crocker, M. W. (2011). The interplay of multiple mechanisms in word learning. In Proceedings of the Annual Conference of the Cog- Science Kwiatkowski, T., Goldwater, S., Zettelmoyer, L.,</note>
<abstract confidence="0.946043363636364">and Steedman, M. (2012). A probabilistic model of syntactic and semantic acquisition from childutterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa- Li, P., Farkas, I., and MacWhinney, B. (2004). Early lexical development in a self-organizing neural 17:1345–1362. Lidz, J., Bunger, A., Leddon, E., Baier, R., and Waxman, S. R. (2010). When one cue is better than two: lexical vs . syntactic cues to verb learning.</abstract>
<note confidence="0.802979851851852">Unpublished manuscript. B. (1995). CHILDES Project: forAnalyzing Hillsdale, NJ: Lawrence Erlbaum Associates, second edition. Maurits, L., Perfors, A. F., and Navarro, D. J. (2009). Joint acquisition of word order and word refer- In of the 31st Annual Conferof the Cognitive Science Mintz, T. (2003). Frequent frames as a cue for gramcategories in child directed speech. Cog- 90(1):91–117. Monaghan, P. and Mattock, K. (2009). Crosssituational language learning: The effects of grammatical categories as constraints on referenlabeling. In of the 31st Annual of the Cognitive Science Naigles, L. and Hoff-Ginsberg, E. (1995). Input to Verb Learning: Evidence for the Plausibility of Bootstrapping. Psychol- 31(5):827–37. Niyogi, S. (2002). Bayesian learning at the syntaxinterface. In of the 24th annual conference of the Cognitive Science Socipages 697–702. Parisien, C., Fazly, A., and Stevenson, S. (2008). An incremental bayesian model for learning syntactic In of the Twelfth Confer-</note>
<title confidence="0.79237">ence on Computational Natural Language Learn-</title>
<author confidence="0.81751">Cognition The</author>
<affiliation confidence="0.859603">of Argument Cambridge,</affiliation>
<address confidence="0.787388">MA: MIT Press.</address>
<note confidence="0.7096925">W. (1960). and Cambridge University Press, Cambridge, MA. Redington, M., Crater, N., and Finch, S. (1998). Distributional information: A powerful cue for acsyntactic categories. Science: Multidisciplinary 22(4):425–469. Regier, T. (2005). The emergence of words: Attenlearning in form and meaning. 29:819–865. Siskind, J. M. (1996). A computational study of</note>
<abstract confidence="0.85080975">cross-situational techniques for learning word-tomappings. 61:39–91. Smith, L. and Yu, C. (2007). Infants rapidly learn words from noisy data via cross-situational statis-</abstract>
<note confidence="0.854217">In of the 29th Annual Conferof the Cognitive Science Song, X., Lin, C., Tseng, B., and Sun, M. (2005).</note>
<title confidence="0.847607">Modeling and predicting personal information</title>
<abstract confidence="0.9592066">behavior. In of the eleventh ACM SIGKDD international conference Knowledge discovery in data pages 479–488. ACM. 653 Theakston, A. L., Lieven, E. V., Pine, J. M., and Rowland, C. F. (2001). The role of performance limitations in the acquisition of verb-argument An alternative account. of 28:127–152. Villavicencio, A. (2002). The acquisition of a unification-based generalised categorial grammar. of the Third CLUK pages 59–66. Yu, C. (2005). The emergence of links between lexical acquisition and object categorization: A study. 17(3– 4):381–397. Yu, C. (2006). Learning syntax–semantics mappings bootstrap word learning. In of the</abstract>
<note confidence="0.438847">28th Annual Conference of the Cognitive Science 654</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Alishahi</author>
<author>A Fazly</author>
</authors>
<title>Integrating Syntactic Knowledge into a Model of Crosssituational Word Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="4074" citStr="Alishahi and Fazly 2010" startWordPosition="616" endWordPosition="619">eferents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). However, none of the existing experimental or computational studies have examined the acquisition of word meanings and lexical categories in parallel. They all make the simplifying assumption that prior to the onset of word learning, the categorization module has already formed a relatively robust set of lexical categories. This assumption can be justified in the case of adult learners of a second or artificial language. But children’s acquisition of categories is most probably interleaved with the acquisition of word meaning, and these two processes must ultimately be studied simultaneously</context>
<context position="5601" citStr="Alishahi and Fazly (2010)" startWordPosition="855" endWordPosition="858">ional evidence with cues from lexical categories. Through a number of simulations of a word learning scenario, we show that our automatically and incrementally induced categories significantly improve the performance of the word learning model, and are closely comparable to a set of goldstandard, manually-annotated part of speech tags. 2 A Word Learning Model We want to investigate whether lexical categories (i.e. word classes) that are incrementally induced from child-directed speech can improve the performance of a cross-situational word learning model. For this purpose, we use the model of Alishahi and Fazly (2010). This model uses a probabilistic learning algorithm for combining evidence from word– referent co-occurrence statistics and the meanings associated with a set of pre-defined categories. They use child-directed utterances, manually annotated with a small set of part of speech tags, from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995). Their experimental results show that integrating these goldstandard categories into the algorithm boosts its performance over a pure cross-situational version. The model of Alishahi and Fazly (2010) has the suitable archite</context>
<context position="9835" citStr="Alishahi and Fazly (2010)" startWordPosition="1549" endWordPosition="1552"> λ(w)) x ac(w|f, U(t), S(t)) where the word-based and category-based alignment scores are estimated based on the acquired meanings of the word and its category, respectively: p(t−1)(f|w) aw(w|f, U(t), S(t)) = E p(t−1)(f|wk) wkEUM p(t−1)(f|cat(w)) ac(w|f, U(t), S(t)) = 1: p(t−1)(f|cat(wk)) wkEUM The relative contribution of the word-based versus the category-based alignment is determined by the weight function λ(w). Cross-situational evidence is a reliable cue for frequent words; on the other hand, the category-based score is most informative when the model encounters a low-frequency word (See Alishahi and Fazly (2010) for a full analysis of the frequency effect). Therefore, we define λ(w) as a function of the frequency of the word n(w): λ(w) = n(w)/(n(w) + 1) Once an alignment score is calculated for each word w E U(t) and each feature f E S(t), the model revises the meanings of all the words in U(t) and their corresponding categories as follows: assoc(t)(w, f) = assoc(t−1)(w, f) + a(w|f, U(t), S(t)) where assoc(t−1)(w, f) is zero if w and f have not co-occurred before. These association scores are then used to update the meaning of the words in the current input: p(t)(f|w) = assoc(t)(f, w) (2) assoc(t)(fj</context>
<context position="21562" citStr="Alishahi and Fazly (2010)" startWordPosition="3578" endWordPosition="3581">inducing categories. We then evaluated word learning on Anne. We chose the parameters of the word class induction module based on those development results: EK 1=1 α = 10, Q = 0.1, K = 10 and J = 20. We used cross-validation for the final evaluation. For each of six data files (Anne, Aran, Becky, Carl, Dominic and Gail), we ran word-class induction on the whole corpus with the chosen file last, and then started applying the word-learning algorithm on this last chosen file (while continuing with category induction). We evaluated how well word meanings were learned in those six cases. We follow Alishahi and Fazly (2010) in the construction of the input. We need a semantic representation paired with each utterance. Such a representation is not available from the corpus and has to be constructed. We automatically construct a gold lexicon for all nouns and verbs in this corpus as follows. For each word, we extract all hypernyms for its first sense in the appropriate (verb or noun) hierarchy in WordNet (Fellbaum 1998), and add the first word in the synset of each hypernym to the set of semantic features for the target word. For verbs, we also extract features from VerbNet (Kipper et al. 2006). A small subset of </context>
<context position="22789" citStr="Alishahi and Fazly (2010)" startWordPosition="3793" endWordPosition="3796">rds (pronouns and frequent quantifiers) are also manually added. This lexicon represents the true meaning of each word, and is used in generating the scene representations in the input and in evaluation. For each utterance in the input corpus, we form the union of the feature representations of all its words. Words not found in the lexicon (i.e. for which we could not extract a semantic representation from WordNet and VerbNet) are removed from the utterance (only for the word learning module). In order to simulate the high level of noise that children receive from their environment, we follow Alishahi and Fazly (2010) and pair each utterance with a combination of its own scene representation and the scene representation for the following utterance. This decision was based on the intuition that consequent utterances are more likely to be about redo are have can not go put did get play is that it what not there he was where put you not I the we what it they your a to you we and I will not can it on it a that the not he this right got she are do is have on in can want did going one I not shall there then you are we it is in are on oh with and of have do the a your of that it this some not very going want bit </context>
<context position="31866" citStr="Alishahi and Fazly (2010)" startWordPosition="5333" endWordPosition="5336">ng that they can improve the model’s performance. Yu’s model also processes input utterances in a batch mode, and its evaluation is limited to situations in which only a coarse distinction between referring words (words that could potentially refer to objects in a scene, e.g. concrete nouns) and non-referring words (words that cannot possibly refer to objects, e.g. function words) is sufficient. It is thus not clear whether information about finer-grained categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. </context>
</contexts>
<marker>Alishahi, Fazly, 2010</marker>
<rawString>Alishahi, A. and Fazly, A. (2010). Integrating Syntactic Knowledge into a Model of Crosssituational Word Learning. In Proceedings of the 32nd Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Banerjee</author>
<author>S Basu</author>
</authors>
<title>Topic models over text streams: A study of batch and online unsupervised learning.</title>
<date>2007</date>
<booktitle>In SIAM Data Mining.</booktitle>
<contexts>
<context position="15675" citStr="Banerjee and Basu (2007)" startWordPosition="2551" endWordPosition="2554">cording to a word class model with 25 classes induced from (i) 1.8 million words of the CHILDES corpus or (ii) 100 million word of the BNC corpus. The similarities were measured between each of the 1000 most frequent CHILDES or BNC words. train car can will give bring June March shoes clothes man woman book hole black white monkey rabbit business language 646 3.2 Online Gibbs sampling for LDA There have been a number of attempts to develop online inference algorithms for topic modeling with LDA. A simple modification of the standard Gibbs sampler (o-LDA) was proposed by Song et al. (2005) and Banerjee and Basu (2007). Canini et al. (2009) experiment with three sampling algorithms for online topic inference: (i) oLDA, (ii) incremental Gibbs sampler, and (iii) a particle filter. Only o-LDA is truly online in the sense that it does not revisit previously seen documents. The other two, the incremental Gibbs sampler and the particle filter, keep seen documents and periodically resample them. In Canini et al.’s experiments all of the online algorithms perform worse than the standard batch Gibbs sampler on a document clustering task. Hoffman et al. (2010) develop an online version of the variational Bayes (VB) o</context>
</contexts>
<marker>Banerjee, Basu, 2007</marker>
<rawString>Banerjee, A. and Basu, S. (2007). Topic models over text streams: A study of batch and online unsupervised learning. In SIAM Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="12797" citStr="Blei et al. (2003)" startWordPosition="2050" endWordPosition="2053">hlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with LDA Latent Dirichlet Allocation (LDA) was introduced by Blei et al. (2003) and is most commonly used for modeling the topic structure in document collections. It is a generative, probabilistic hierarchical Bayesian model that induces a set of latent variables, which correspond to the topics. The topics themselves are multinomial distributions over words. The generative structure of the LDA model is the following: φk ∼ Dirichlet(β), k ∈ [1, K] (4) θd ∼ Dirichlet(α), d ∈ [1, D] znd ∼ Categorical(θd), nd ∈ [1, Nd] wnd ∼ Categorical(φznd ), nd ∈ [1, Nd] Chrupała (2011) reinterprets the LDA model in terms of word classes as follows: K is the number of classes, D is the n</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., Ng, A., and Jordan, M. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
</authors>
<title>Co-evolution of language and of the language acquisition device.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>418--427</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32276" citStr="Briscoe 1997" startWordPosition="5400" endWordPosition="5401">ther information about finer-grained categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syn</context>
</contexts>
<marker>Briscoe, 1997</marker>
<rawString>Briscoe, T. (1997). Co-evolution of language and of the language acquisition device. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 418–427. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>R L Mercer</author>
<author>Della Pietra</author>
<author>V J</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="12305" citStr="Brown et al. 1992" startWordPosition="1968" endWordPosition="1971">ewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with LDA Latent Dirichlet Allocation (LDA) was introduced by Blei et al. (2003) and is most commonly used for modeling the topic structure in document collections. It is a generative, pro</context>
</contexts>
<marker>Brown, Mercer, Pietra, J, Lai, 1992</marker>
<rawString>Brown, P. F., Mercer, R. L., Della Pietra, V. J., and Lai, J. C. (1992). Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buttery</author>
</authors>
<title>Computational models for first language acquisition.</title>
<date>2006</date>
<tech>Tech. Rep. UCAM-CLTR675.</tech>
<institution>Computer Laboratory, University of Cambridge,</institution>
<contexts>
<context position="32310" citStr="Buttery 2006" startWordPosition="5404" endWordPosition="5405">ed categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syntactic and semantic representation</context>
</contexts>
<marker>Buttery, 2006</marker>
<rawString>Buttery, P. (2006). Computational models for first language acquisition. Computer Laboratory, University of Cambridge, Tech. Rep. UCAM-CLTR675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Canini</author>
<author>L Shi</author>
<author>T Griffiths</author>
</authors>
<title>Online inference of topics with latent dirichlet allocation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="15697" citStr="Canini et al. (2009)" startWordPosition="2555" endWordPosition="2558">del with 25 classes induced from (i) 1.8 million words of the CHILDES corpus or (ii) 100 million word of the BNC corpus. The similarities were measured between each of the 1000 most frequent CHILDES or BNC words. train car can will give bring June March shoes clothes man woman book hole black white monkey rabbit business language 646 3.2 Online Gibbs sampling for LDA There have been a number of attempts to develop online inference algorithms for topic modeling with LDA. A simple modification of the standard Gibbs sampler (o-LDA) was proposed by Song et al. (2005) and Banerjee and Basu (2007). Canini et al. (2009) experiment with three sampling algorithms for online topic inference: (i) oLDA, (ii) incremental Gibbs sampler, and (iii) a particle filter. Only o-LDA is truly online in the sense that it does not revisit previously seen documents. The other two, the incremental Gibbs sampler and the particle filter, keep seen documents and periodically resample them. In Canini et al.’s experiments all of the online algorithms perform worse than the standard batch Gibbs sampler on a document clustering task. Hoffman et al. (2010) develop an online version of the variational Bayes (VB) optimization method for</context>
<context position="17018" citStr="Canini et al. (2009)" startWordPosition="2778" endWordPosition="2781">h VB as measured by perplexity on heldout data, especially when used with large minibatch sizes. Online VB for LDA is appropriate when streaming documents: with online VB documents are represented as word count tables. In our scenario where we apply LDA to modeling word classes we need to process context features from sentences arriving in a stream: i.e. we need to sample entries from a table like Table 1 in order of arrival rather than row by row. This means that online VB is not directly applicable to online word-class induction. However it also means that one issue with o-LDA identified by Canini et al. (2009) is ameliorated. When sampling in a topic modeling setting, documents are unique and are never seen again. Thus, the topics associated with old documents get stale and need to be periodically rejuvenated (i.e. resampled). This is the reason why the incremental Gibbs sampler and the particle filter algorithms in Canini et al. (2009) need to keep old documents around and cannot run in a true online fashion. Since for word class modeling we stream context features as they arrive, we will continue to see features associated with the seen word types, and will automatically resample their class assi</context>
</contexts>
<marker>Canini, Shi, Griffiths, 2009</marker>
<rawString>Canini, K., Shi, L., and Griffiths, T. (2009). Online inference of topics with latent dirichlet allocation. In Proceedings of the International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chrupała</author>
</authors>
<title>Efficient induction of probabilistic word classes with LDA.</title>
<date>2011</date>
<booktitle>In International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="12122" citStr="Chrupała (2011)" startWordPosition="1942" endWordPosition="1943"> In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with</context>
<context position="14629" citStr="Chrupała (2011)" startWordPosition="2374" endWordPosition="2375">rd type d if we use each word’s left and right neighbors as features, and subscript words with L and R to indicate left and right. After inference, the θd parameters correspond to word class probability distributions given a word type while the φk correspond to feature distributions given a word class: the model provides a probabilistic representation for word types independently of their context, and also for contexts independently of the word type. Probabilistic, soft word classes are more expressive than hard categories. First, they make it easy and efficient to express shared ambiguities: Chrupała (2011) gives an example of words used as either first names or surnames, and this shared ambiguity is reflected in the similarity of their word class distributions. Second, with soft word classes it becomes easy to express graded similarity between words: as an example, Table 2 shows a random selection out of the 100 most similar word pairs according to the Jensen-Shannon divergence between their word class distributions, according to a word class model with 25 classes induced from (i) 1.8 million words of the CHILDES corpus or (ii) 100 million word of the BNC corpus. The similarities were measured </context>
</contexts>
<marker>Chrupała, 2011</marker>
<rawString>Chrupała, G. (2011). Efficient induction of probabilistic word classes with LDA. In International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chrupała</author>
<author>A Alishahi</author>
</authors>
<title>Online Entropy-based Model of Lexical Category Acquisition. In CoNLL</title>
<date>2010</date>
<contexts>
<context position="11919" citStr="Chrupała and Alishahi 2010" startWordPosition="1905" endWordPosition="1908">n of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as ou</context>
<context position="29064" citStr="Chrupała and Alishahi 2010" startWordPosition="4880" endWordPosition="4883">class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This </context>
<context position="34017" citStr="Chrupała and Alishahi (2010)" startWordPosition="5674" endWordPosition="5677">ing word meanings. Our results suggest that the sentential context that a word appears in across its different uses can be used as a complementary source of guidance for mapping it to its featural meaning representation. In Section 4 we show that the improvement achieved by our categories is comparable to that gained by integrating gold POS categories. This result is very encouraging, since manually assigned POS tags are typically believed to set the upper bound on the usefulness of category information. We believe that it automatically induced categories have the potential to do even better: Chrupała and Alishahi (2010) have shown that categories induced from usage data in an unsupervised fashion can be used more effectively than POS categories in Mean Average Precision 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0 2000 4000 6000 8000 Input Items 20 LDA classes 10 LDA classes 5 LDA classes No categories 651 a number of tasks. In our experiments here on the development data we observed some improvements over POS categories. This advantage can result from the fact that our categories are more fine-grained (if also more noisy) than POS categories, which sometimes yields more accurate predictions. One important character</context>
</contexts>
<marker>Chrupała, Alishahi, 2010</marker>
<rawString>Chrupała, G. and Alishahi, A. (2010). Online Entropy-based Model of Lexical Category Acquisition. In CoNLL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning,</booktitle>
<pages>91--94</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11856" citStr="Clark 2000" startWordPosition="1897" endWordPosition="1898"> as the ‘uniform’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of vario</context>
<context position="29001" citStr="Clark 2000" startWordPosition="4872" endWordPosition="4873">re-specify the number of classes for the word class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquis</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Clark, A. (2000). Inducing syntactic categories by context distribution clustering. In Proceedings of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning, pages 91–94. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<date>2010</date>
<journal>A Probabilistic Computational Model of CrossSituational Word Learning. Cognitive Science,</journal>
<volume>34</volume>
<issue>6</issue>
<contexts>
<context position="2389" citStr="Fazly et al. 2010" startWordPosition="358" endWordPosition="361">n statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006).</context>
<context position="29447" citStr="Fazly et al. 2010" startWordPosition="4933" endWordPosition="4936">models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four conditions: with gold POS categories, with LDA categories, with uniform categories, and without using categories. Each panel displays a different t</context>
</contexts>
<marker>Fazly, Alishahi, Stevenson, 2010</marker>
<rawString>Fazly, A., Alishahi, A., and Stevenson, S. (2010). A Probabilistic Computational Model of CrossSituational Word Learning. Cognitive Science, 34(6):1017–1063.</rawString>
</citation>
<citation valid="true">
<title>WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, C., editor</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Fellbaum, C., editor (1998). WordNet, An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Frank</author>
<author>N D Goodman</author>
<author>J B Tenenbaum</author>
</authors>
<title>A Bayesian framework for crosssituational word-learning.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>20</volume>
<contexts>
<context position="29351" citStr="Frank et al. 2007" startWordPosition="4917" endWordPosition="4920">gories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four conditions: with gold POS categories, with LDA cate</context>
</contexts>
<marker>Frank, Goodman, Tenenbaum, 2007</marker>
<rawString>Frank, M. C., Goodman, N. D., and Tenenbaum, J. B. (2007). A Bayesian framework for crosssituational word-learning. In Advances in Neural Information Processing Systems, volume 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gelman</author>
<author>M Taylor</author>
</authors>
<title>How two-yearold children interpret proper and common names for unfamiliar objects. Child Development,</title>
<date>1984</date>
<pages>1535--1540</pages>
<contexts>
<context position="3137" citStr="Gelman and Taylor 1984" startWordPosition="483" endWordPosition="486">tial context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Languag</context>
<context position="11487" citStr="Gelman and Taylor 1984" startWordPosition="1837" endWordPosition="1841">. In order to investigate this issue, we use the following alignment formula as an informed baseline in our experiments, where we replace ac( |f, U(t), S(t)) with a uniform distribution:1 a(w|f, U(t), S(t)) = λ(w) x aw(w|f, U(t), S(t)) (3) 1 +(1 − λ(w)) x |U(t)| where aw(w|f, U(t), S(t)) and λ(w) are estimated as before. In our experiments in Section 4, we refer to this baseline as the ‘uniform’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of th</context>
</contexts>
<marker>Gelman, Taylor, 1984</marker>
<rawString>Gelman, S. and Taylor, M. (1984). How two-yearold children interpret proper and common names for unfamiliar objects. Child Development, pages 1535–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gertner</author>
<author>C Fisher</author>
<author>J Eisengart</author>
</authors>
<title>Learning words and rules: Abstract knowledge of word order in early sentence comprehension.</title>
<date>2006</date>
<journal>Psychological Science,</journal>
<volume>17</volume>
<issue>8</issue>
<contexts>
<context position="2988" citStr="Gertner et al. 2006" startWordPosition="458" endWordPosition="461">5, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this</context>
</contexts>
<marker>Gertner, Fisher, Eisengart, 2006</marker>
<rawString>Gertner, Y., Fisher, C., and Eisengart, J. (2006). Learning words and rules: Abstract knowledge of word order in early sentence comprehension. Psychological Science, 17(8):684–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillette</author>
<author>H Gleitman</author>
<author>L Gleitman</author>
<author>A Lederer</author>
</authors>
<title>Human simulations of vocabulary learning.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>73</volume>
<issue>2</issue>
<contexts>
<context position="3480" citStr="Gillette et al. 1999" startWordPosition="534" endWordPosition="537">t they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). Howe</context>
</contexts>
<marker>Gillette, Gleitman, Gleitman, Lederer, 1999</marker>
<rawString>Gillette, J., Gleitman, H., Gleitman, L., and Lederer, A. (1999). Human simulations of vocabulary learning. Cognition, 73(2):135–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gleitman</author>
</authors>
<title>The structural sources of verb meanings.</title>
<date>1990</date>
<journal>Language Acquisition,</journal>
<pages>1--135</pages>
<contexts>
<context position="2727" citStr="Gleitman 1990" startWordPosition="416" endWordPosition="417"> computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language ca</context>
</contexts>
<marker>Gleitman, 1990</marker>
<rawString>Gleitman, L. (1990). The structural sources of verb meanings. Language Acquisition, 1:135–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hoffman</author>
<author>D Blei</author>
<author>F Bach</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="16217" citStr="Hoffman et al. (2010)" startWordPosition="2641" endWordPosition="2644">sampler (o-LDA) was proposed by Song et al. (2005) and Banerjee and Basu (2007). Canini et al. (2009) experiment with three sampling algorithms for online topic inference: (i) oLDA, (ii) incremental Gibbs sampler, and (iii) a particle filter. Only o-LDA is truly online in the sense that it does not revisit previously seen documents. The other two, the incremental Gibbs sampler and the particle filter, keep seen documents and periodically resample them. In Canini et al.’s experiments all of the online algorithms perform worse than the standard batch Gibbs sampler on a document clustering task. Hoffman et al. (2010) develop an online version of the variational Bayes (VB) optimization method for inference for topic modeling with LDA. Their method achieves good empirical results compared to batch VB as measured by perplexity on heldout data, especially when used with large minibatch sizes. Online VB for LDA is appropriate when streaming documents: with online VB documents are represented as word count tables. In our scenario where we apply LDA to modeling word classes we need to process context features from sentences arriving in a stream: i.e. we need to sample entries from a table like Table 1 in order o</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Hoffman, M., Blei, D., and Bach, F. (2010). Online learning for latent dirichlet allocation. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kemp</author>
<author>E Lieven</author>
<author>M Tomasello</author>
</authors>
<date>2005</date>
<journal>Young Children’s Knowledge of the” Determiner” and” Adjective” Categories. Journal of Speech, Language and Hearing Research,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="3156" citStr="Kemp et al. 2005" startWordPosition="487" endWordPosition="490">It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Co</context>
<context position="11506" citStr="Kemp et al. 2005" startWordPosition="1842" endWordPosition="1845">e this issue, we use the following alignment formula as an informed baseline in our experiments, where we replace ac( |f, U(t), S(t)) with a uniform distribution:1 a(w|f, U(t), S(t)) = λ(w) x aw(w|f, U(t), S(t)) (3) 1 +(1 − λ(w)) x |U(t)| where aw(w|f, U(t), S(t)) and λ(w) are estimated as before. In our experiments in Section 4, we refer to this baseline as the ‘uniform’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in</context>
</contexts>
<marker>Kemp, Lieven, Tomasello, 2005</marker>
<rawString>Kemp, N., Lieven, E., and Tomasello, M. (2005). Young Children’s Knowledge of the” Determiner” and” Adjective” Categories. Journal of Speech, Language and Hearing Research, 48(3):592–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>A Korhonen</author>
<author>N Ryant</author>
<author>M Palmer</author>
</authors>
<title>Extensive classifications of english verbs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th EURALEX International Congress.</booktitle>
<contexts>
<context position="22142" citStr="Kipper et al. 2006" startWordPosition="3683" endWordPosition="3686">es. We follow Alishahi and Fazly (2010) in the construction of the input. We need a semantic representation paired with each utterance. Such a representation is not available from the corpus and has to be constructed. We automatically construct a gold lexicon for all nouns and verbs in this corpus as follows. For each word, we extract all hypernyms for its first sense in the appropriate (verb or noun) hierarchy in WordNet (Fellbaum 1998), and add the first word in the synset of each hypernym to the set of semantic features for the target word. For verbs, we also extract features from VerbNet (Kipper et al. 2006). A small subset of words (pronouns and frequent quantifiers) are also manually added. This lexicon represents the true meaning of each word, and is used in generating the scene representations in the input and in evaluation. For each utterance in the input corpus, we form the union of the feature representations of all its words. Words not found in the lexicon (i.e. for which we could not extract a semantic representation from WordNet and VerbNet) are removed from the utterance (only for the word learning module). In order to simulate the high level of noise that children receive from their e</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2006</marker>
<rawString>Kipper, K., Korhonen, A., Ryant, N., and Palmer, M. (2006). Extensive classifications of english verbs. In Proceedings of the 12th EURALEX International Congress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koehne</author>
<author>M W Crocker</author>
</authors>
<title>Sentence processing mechanisms influence crosssituational word learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="3523" citStr="Koehne and Crocker 2010" startWordPosition="542" endWordPosition="545"> the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). However, none of the existing experimental or c</context>
</contexts>
<marker>Koehne, Crocker, 2010</marker>
<rawString>Koehne, J. and Crocker, M. W. (2010). Sentence processing mechanisms influence crosssituational word learning. In Proceedings of the Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koehne</author>
<author>M W Crocker</author>
</authors>
<title>The interplay of multiple mechanisms in word learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Conference of the Cognitive Science Society.</booktitle>
<marker>Koehne, Crocker, 2011</marker>
<rawString>Koehne, J. and Crocker, M. W. (2011). The interplay of multiple mechanisms in word learning. In Proceedings of the Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>S Goldwater</author>
<author>L Zettelmoyer</author>
<author>M Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32336" citStr="Kwiatkowski et al. 2012" startWordPosition="5406" endWordPosition="5409">(e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syntactic and semantic representations of syntax. We view word </context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettelmoyer, Steedman, 2012</marker>
<rawString>Kwiatkowski, T., Goldwater, S., Zettelmoyer, L., and Steedman, M. (2012). A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Li</author>
<author>I Farkas</author>
<author>B MacWhinney</author>
</authors>
<title>Early lexical development in a self-organizing neural network. Neural Networks,</title>
<date>2004</date>
<pages>17--1345</pages>
<contexts>
<context position="29287" citStr="Li et al. 2004" startWordPosition="4908" endWordPosition="4911">r been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in t</context>
</contexts>
<marker>Li, Farkas, MacWhinney, 2004</marker>
<rawString>Li, P., Farkas, I., and MacWhinney, B. (2004). Early lexical development in a self-organizing neural network. Neural Networks, 17:1345–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lidz</author>
<author>A Bunger</author>
<author>E Leddon</author>
<author>R Baier</author>
<author>S R Waxman</author>
</authors>
<title>When one cue is better than two: lexical vs . syntactic cues to verb learning.</title>
<date>2010</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="3498" citStr="Lidz et al. 2010" startWordPosition="538" endWordPosition="541">tural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). However, none of the e</context>
</contexts>
<marker>Lidz, Bunger, Leddon, Baier, Waxman, 2010</marker>
<rawString>Lidz, J., Bunger, A., Leddon, E., Baier, R., and Waxman, S. R. (2010). When one cue is better than two: lexical vs . syntactic cues to verb learning. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools forAnalyzing Talk.</title>
<date>1995</date>
<location>Hillsdale, NJ: Lawrence</location>
<note>second edition.</note>
<contexts>
<context position="5976" citStr="MacWhinney 1995" startWordPosition="912" endWordPosition="913"> whether lexical categories (i.e. word classes) that are incrementally induced from child-directed speech can improve the performance of a cross-situational word learning model. For this purpose, we use the model of Alishahi and Fazly (2010). This model uses a probabilistic learning algorithm for combining evidence from word– referent co-occurrence statistics and the meanings associated with a set of pre-defined categories. They use child-directed utterances, manually annotated with a small set of part of speech tags, from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995). Their experimental results show that integrating these goldstandard categories into the algorithm boosts its performance over a pure cross-situational version. The model of Alishahi and Fazly (2010) has the suitable architecture for our goal: it provides an integrated learning mechanism which combines evidence from word-referent co-occurrence with cues from the meaning representation associated with word categories. However, the model has two major shortcomings. First, it assumes that lexical categories are formed and finalized prior to the onset of word learning and that a correct and uniqu</context>
<context position="20119" citStr="MacWhinney 1995" startWordPosition="3328" endWordPosition="3329">the current sentence, we allow the words in this sentence to be sampled more than once. Vt_1 zt,wj , j=1 nt−1 + Q 647 Figure 1: Top 10 words for 10 classes learned from CHILDES Figure 2: Top 10 words of 5 randomly chosen classes learned from BNC Since we are dealing with soft classes, most wordtypes have non-zero assignment probabilities for many classes. Thus frequently occurring words such as not will typically be listed for several classes. 4 Evaluation 4.1 Experimental setup As training data, we extract utterances from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995), a corpus that contains transcripts of conversations with children between the ages of 1 year, 8 months and 3 years. We use the mother’s speech from transcripts of 12 children (henceforth referred to by children’s names). We run word class induction while simultaneously outputting the highest scoring word-class label for each word: for a new sentence, we sample class assignments for each feature (doing J passes), update the counts, and then for each word dti output the highest scoring class label according to argmaxz nt (where nz,dti z,dti t stands for the number of times class z co-occurred </context>
</contexts>
<marker>MacWhinney, 1995</marker>
<rawString>MacWhinney, B. (1995). The CHILDES Project: Tools forAnalyzing Talk. Hillsdale, NJ: Lawrence Erlbaum Associates, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Maurits</author>
<author>A F Perfors</author>
<author>D J Navarro</author>
</authors>
<title>Joint acquisition of word order and word reference.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="29571" citStr="Maurits et al. (2009)" startWordPosition="4955" endWordPosition="4958">es of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four conditions: with gold POS categories, with LDA categories, with uniform categories, and without using categories. Each panel displays a different time span. Mean Average Precision Mean Average Precision Mean Average Precision 0.35 0.45 0.55 0.65 0.35 0.45 0.55 0.65 0.35 </context>
</contexts>
<marker>Maurits, Perfors, Navarro, 2009</marker>
<rawString>Maurits, L., Perfors, A. F., and Navarro, D. J. (2009). Joint acquisition of word order and word reference. In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mintz</author>
</authors>
<title>Frequent frames as a cue for grammatical categories in child directed speech.</title>
<date>2003</date>
<journal>Cognition,</journal>
<volume>90</volume>
<issue>1</issue>
<contexts>
<context position="11868" citStr="Mintz 2003" startWordPosition="1899" endWordPosition="1900">form’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language </context>
<context position="29013" citStr="Mintz 2003" startWordPosition="4874" endWordPosition="4875">he number of classes for the word class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of wor</context>
</contexts>
<marker>Mintz, 2003</marker>
<rawString>Mintz, T. (2003). Frequent frames as a cue for grammatical categories in child directed speech. Cognition, 90(1):91–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Monaghan</author>
<author>K Mattock</author>
</authors>
<title>Crosssituational language learning: The effects of grammatical categories as constraints on referential labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2100" citStr="Monaghan and Mattock 2009" startWordPosition="311" endWordPosition="314">words are heard and used is often noisy and highly ambiguous. Particularly, many words in a language are polysemous and have different meanings. Various learning mechanisms have been proposed for word learning. One well-studied mechanism is cross-situational learning, a bottom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bo</context>
</contexts>
<marker>Monaghan, Mattock, 2009</marker>
<rawString>Monaghan, P. and Mattock, K. (2009). Crosssituational language learning: The effects of grammatical categories as constraints on referential labeling. In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Naigles</author>
<author>E Hoff-Ginsberg</author>
</authors>
<title>Input to Verb Learning: Evidence for the Plausibility of Syntactic Bootstrapping.</title>
<date>1995</date>
<pages>31--5</pages>
<publisher>Developmental Psychology,</publisher>
<contexts>
<context position="2966" citStr="Naigles and Hoff-Ginsberg 1995" startWordPosition="454" endWordPosition="457">input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, Gertner et al. 2006). In particular, young children have robust knowledge of some of the abstract lexical categories such as nouns and verbs (e.g. Gelman and Taylor 1984, Kemp et al. 2005). Recent studies have examined the interplay of cross-situational learning and sentence-level learning mechanisms, showing that adult learners of an artificial language can successfully and simultaneously apply cues and constraints from both sources of information when mapping words to their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have </context>
</contexts>
<marker>Naigles, Hoff-Ginsberg, 1995</marker>
<rawString>Naigles, L. and Hoff-Ginsberg, E. (1995). Input to Verb Learning: Evidence for the Plausibility of Syntactic Bootstrapping. Developmental Psychology, 31(5):827–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niyogi</author>
</authors>
<title>Bayesian learning at the syntaxsemantics interface.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th annual conference of the Cognitive Science Society,</booktitle>
<pages>697--702</pages>
<contexts>
<context position="30650" citStr="Niyogi (2002)" startWordPosition="5140" endWordPosition="5141">ifferent time span. Mean Average Precision Mean Average Precision Mean Average Precision 0.35 0.45 0.55 0.65 0.35 0.45 0.55 0.65 0.35 0.45 0.55 0.65 0 1000 2000 3000 4000 5000 Input Items (b) first 5000 sentences (c) first 1000 sentences 200 400 600 800 1000 Input Items 2000 4000 6000 8000 Input Items (a) all sentences POS LDA Uniform None POS LDA Uniform None POS LDA Uniform None 650 Figure 5: Mean average precision for all observed words at each point in time in four conditions: using online LDA categories of varying numbers of 20, 10 and 5, and without using categories. order. The model of Niyogi (2002) simulates the mutual bootstrapping effects of syntactic and semantic knowledge in verb learning, that is the use of syntax to aid in inducing the semantics of a verb, and the use of semantics to narrow down possible syntactic frames in which a verb can participate. However, this model relies on manually assigned priors for associations between syntactic and semantic features, and is tested on a toy language with very limited vocabulary and a constrained syntax. Yu (2006) integrates automatically induced syntactic word categories into his model of crosssituational word learning, showing that t</context>
</contexts>
<marker>Niyogi, 2002</marker>
<rawString>Niyogi, S. (2002). Bayesian learning at the syntaxsemantics interface. In Proceedings of the 24th annual conference of the Cognitive Science Society, pages 697–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Parisien</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>An incremental bayesian model for learning syntactic categories.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="11890" citStr="Parisien et al. 2008" startWordPosition="1901" endWordPosition="1904">ion. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. H</context>
<context position="29035" citStr="Parisien et al. 2008" startWordPosition="4876" endWordPosition="4879"> classes for the word class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word ord</context>
</contexts>
<marker>Parisien, Fazly, Stevenson, 2008</marker>
<rawString>Parisien, C., Fazly, A., and Stevenson, S. (2008). An incremental bayesian model for learning syntactic categories. In Proceedings of the Twelfth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Learnability and Cognition: The Acquisition of Argument Structure.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="1865" citStr="Pinker 1989" startWordPosition="278" endWordPosition="279">er than in isolation. The meaning of an utterance must be inferred from among numerous possible interpretations that the (usually complex) surrounding scene offers. In addition, the linguistic and visual context in which words are heard and used is often noisy and highly ambiguous. Particularly, many words in a language are polysemous and have different meanings. Various learning mechanisms have been proposed for word learning. One well-studied mechanism is cross-situational learning, a bottom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to const</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>Pinker, S. (1989). Learnability and Cognition: The Acquisition of Argument Structure. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Quine</author>
</authors>
<title>Word and Object.</title>
<date>1960</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1851" citStr="Quine 1960" startWordPosition="276" endWordPosition="277">terance rather than in isolation. The meaning of an utterance must be inferred from among numerous possible interpretations that the (usually complex) surrounding scene offers. In addition, the linguistic and visual context in which words are heard and used is often noisy and highly ambiguous. Particularly, many words in a language are polysemous and have different meanings. Various learning mechanisms have been proposed for word learning. One well-studied mechanism is cross-situational learning, a bottom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the le</context>
</contexts>
<marker>Quine, 1960</marker>
<rawString>Quine, W. (1960). Word and Object. Cambridge University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Redington</author>
<author>N Crater</author>
<author>S Finch</author>
</authors>
<title>Distributional information: A powerful cue for acquiring syntactic categories.</title>
<date>1998</date>
<journal>Cognitive Science: A Multidisciplinary Journal,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="11844" citStr="Redington et al. 1998" startWordPosition="1893" endWordPosition="1896"> refer to this baseline as the ‘uniform’ condition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1We thank an anonymous reviewers for suggesting this condition as an informed baseline. 645 drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learn</context>
<context position="28989" citStr="Redington et al. 1998" startWordPosition="4868" endWordPosition="4871">ke to avoid having to pre-specify the number of classes for the word class induction module: we thus plan to investigate non-parametric models such as Hierarchical Dirichlet Process for this purpose. 5 Related Work This paper investigates the interplay between two language learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the </context>
</contexts>
<marker>Redington, Crater, Finch, 1998</marker>
<rawString>Redington, M., Crater, N., and Finch, S. (1998). Distributional information: A powerful cue for acquiring syntactic categories. Cognitive Science: A Multidisciplinary Journal, 22(4):425–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Regier</author>
</authors>
<title>The emergence of words: Attentional learning in form and meaning.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<pages>29--819</pages>
<contexts>
<context position="29301" citStr="Regier 2005" startWordPosition="4912" endWordPosition="4913">n isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four c</context>
</contexts>
<marker>Regier, 2005</marker>
<rawString>Regier, T. (2005). The emergence of words: Attentional learning in form and meaning. Cognitive Science, 29:819–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--39</pages>
<contexts>
<context position="2360" citStr="Siskind 1996" startWordPosition="354" endWordPosition="355">tom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsber</context>
<context position="29254" citStr="Siskind 1996" startWordPosition="4904" endWordPosition="4905"> learning tasks which have so far been studied in isolation: the acquisition of lexical categories from distributional clues, and learning the mapping between words and meanings. Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all </context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Siskind, J. M. (1996). A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61:39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Smith</author>
<author>C Yu</author>
</authors>
<title>Infants rapidly learn words from noisy data via cross-situational statistics.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2072" citStr="Smith and Yu 2007" startWordPosition="307" endWordPosition="310">l context in which words are heard and used is often noisy and highly ambiguous. Particularly, many words in a language are polysemous and have different meanings. Various learning mechanisms have been proposed for word learning. One well-studied mechanism is cross-situational learning, a bottom-up strategy based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypo</context>
</contexts>
<marker>Smith, Yu, 2007</marker>
<rawString>Smith, L. and Yu, C. (2007). Infants rapidly learn words from noisy data via cross-situational statistics. In Proceedings of the 29th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Song</author>
<author>C Lin</author>
<author>B Tseng</author>
<author>M Sun</author>
</authors>
<title>Modeling and predicting personal information dissemination behavior.</title>
<date>2005</date>
<booktitle>In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>479--488</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15646" citStr="Song et al. (2005)" startWordPosition="2546" endWordPosition="2549">class distributions, according to a word class model with 25 classes induced from (i) 1.8 million words of the CHILDES corpus or (ii) 100 million word of the BNC corpus. The similarities were measured between each of the 1000 most frequent CHILDES or BNC words. train car can will give bring June March shoes clothes man woman book hole black white monkey rabbit business language 646 3.2 Online Gibbs sampling for LDA There have been a number of attempts to develop online inference algorithms for topic modeling with LDA. A simple modification of the standard Gibbs sampler (o-LDA) was proposed by Song et al. (2005) and Banerjee and Basu (2007). Canini et al. (2009) experiment with three sampling algorithms for online topic inference: (i) oLDA, (ii) incremental Gibbs sampler, and (iii) a particle filter. Only o-LDA is truly online in the sense that it does not revisit previously seen documents. The other two, the incremental Gibbs sampler and the particle filter, keep seen documents and periodically resample them. In Canini et al.’s experiments all of the online algorithms perform worse than the standard batch Gibbs sampler on a document clustering task. Hoffman et al. (2010) develop an online version of</context>
</contexts>
<marker>Song, Lin, Tseng, Sun, 2005</marker>
<rawString>Song, X., Lin, C., Tseng, B., and Sun, M. (2005). Modeling and predicting personal information dissemination behavior. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 479–488. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Theakston</author>
<author>E V Lieven</author>
<author>J M Pine</author>
<author>C F Rowland</author>
</authors>
<title>The role of performance limitations in the acquisition of verb-argument structure: An alternative account.</title>
<date>2001</date>
<journal>Journal of Child Language,</journal>
<pages>28--127</pages>
<contexts>
<context position="5934" citStr="Theakston et al. 2001" startWordPosition="904" endWordPosition="907">. 2 A Word Learning Model We want to investigate whether lexical categories (i.e. word classes) that are incrementally induced from child-directed speech can improve the performance of a cross-situational word learning model. For this purpose, we use the model of Alishahi and Fazly (2010). This model uses a probabilistic learning algorithm for combining evidence from word– referent co-occurrence statistics and the meanings associated with a set of pre-defined categories. They use child-directed utterances, manually annotated with a small set of part of speech tags, from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995). Their experimental results show that integrating these goldstandard categories into the algorithm boosts its performance over a pure cross-situational version. The model of Alishahi and Fazly (2010) has the suitable architecture for our goal: it provides an integrated learning mechanism which combines evidence from word-referent co-occurrence with cues from the meaning representation associated with word categories. However, the model has two major shortcomings. First, it assumes that lexical categories are formed and finalized prior to the onset of </context>
<context position="20077" citStr="Theakston et al. 2001" startWordPosition="3320" endWordPosition="3323">e stream of sentences. Rather, while processing the current sentence, we allow the words in this sentence to be sampled more than once. Vt_1 zt,wj , j=1 nt−1 + Q 647 Figure 1: Top 10 words for 10 classes learned from CHILDES Figure 2: Top 10 words of 5 randomly chosen classes learned from BNC Since we are dealing with soft classes, most wordtypes have non-zero assignment probabilities for many classes. Thus frequently occurring words such as not will typically be listed for several classes. 4 Evaluation 4.1 Experimental setup As training data, we extract utterances from the Manchester corpus (Theakston et al. 2001) in the CHILDES database (MacWhinney 1995), a corpus that contains transcripts of conversations with children between the ages of 1 year, 8 months and 3 years. We use the mother’s speech from transcripts of 12 children (henceforth referred to by children’s names). We run word class induction while simultaneously outputting the highest scoring word-class label for each word: for a new sentence, we sample class assignments for each feature (doing J passes), update the counts, and then for each word dti output the highest scoring class label according to argmaxz nt (where nz,dti z,dti t stands fo</context>
</contexts>
<marker>Theakston, Lieven, Pine, Rowland, 2001</marker>
<rawString>Theakston, A. L., Lieven, E. V., Pine, J. M., and Rowland, C. F. (2001). The role of performance limitations in the acquisition of verb-argument structure: An alternative account. Journal of Child Language, 28:127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The acquisition of a unification-based generalised categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third CLUK Colloquium,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="32296" citStr="Villavicencio 2002" startWordPosition="5402" endWordPosition="5403">on about finer-grained categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the overall word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syntactic and semantic </context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, A. (2002). The acquisition of a unification-based generalised categorial grammar. In Proceedings of the Third CLUK Colloquium, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
</authors>
<title>The emergence of links between lexical acquisition and object categorization: A computational study.</title>
<date>2005</date>
<journal>Connection Science,</journal>
<volume>17</volume>
<issue>3</issue>
<pages>4--381</pages>
<contexts>
<context position="2369" citStr="Yu 2005" startWordPosition="356" endWordPosition="357">y based on statistical co-occurrence of words and referents across situations (Quine 1960, Pinker 1989). Several experimental studies have shown that adults and children are sensitive to cross-situational evidence and use this information for mapping words to objects, actions and properties (Smith and Yu 2007, Monaghan and Mattock 2009). A number of computational models have been developed based on this principle, demonstrating that cross-situational learning is a powerful and efficient mechanism for learning the correct mappings between words and meanings from noisy input (e.g. Siskind 1996, Yu 2005, Fazly et al. 2010). Another potential source of information that can help the learner to constrain the relevant aspects of a scene is the sentential context of a word. It has been suggested that children draw on syntactic cues provided by the linguistic context in order to guide word learning, a hypothesis known as syntactic bootstrapping (Gleitman 1990). There is substantial evidence that children are sensitive to the structural regularities of language from a very young age, and that they use these structural cues to find the referent of a novel word (e.g. Naigles and Hoff-Ginsberg 1995, G</context>
<context position="29427" citStr="Yu 2005" startWordPosition="4931" endWordPosition="4932">Previous models have shown that lexical categories can be learned from unannotated text, mainly drawing on distributional properties of words (e.g. Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). Independently, several computational models have exploited cross-situational evidence in learning the correct mappings between words and meanings, using rule-based inference (Siskind 1996), neural networks (Li et al. 2004, Regier 2005), hierarchical Bayesian models (Frank et al. 2007) and probabilistic alignment inspired by machine translation models (Yu 2005, Fazly et al. 2010). There are only a few existing computational models that explore the role of syntax in word learning. Maurits et al. (2009) investigates the joint acquisition of word meaning and word order using a batch model. This model is tested on an artificial language with a simple first order predicate representation of meaning, and limited built-in possibilities for word Figure 4: Mean average precision for all observed words at each point in time for four conditions: with gold POS categories, with LDA categories, with uniform categories, and without using categories. Each panel di</context>
</contexts>
<marker>Yu, 2005</marker>
<rawString>Yu, C. (2005). The emergence of links between lexical acquisition and object categorization: A computational study. Connection Science, 17(3– 4):381–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
</authors>
<title>Learning syntax–semantics mappings to bootstrap word learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="4048" citStr="Yu 2006" startWordPosition="614" endWordPosition="615">o their referents (Gillette et al. 1999, Lidz et al. 2010, Koehne and Crocker 2010; 2011). Several computational models have also investigated this interaction by adding manually annotated part-of-speech tags as 643 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics input to word learning algorithms, and suggesting that integration of lexical categories can boost the performance of a cross-situational model (Yu 2006, Alishahi and Fazly 2010). However, none of the existing experimental or computational studies have examined the acquisition of word meanings and lexical categories in parallel. They all make the simplifying assumption that prior to the onset of word learning, the categorization module has already formed a relatively robust set of lexical categories. This assumption can be justified in the case of adult learners of a second or artificial language. But children’s acquisition of categories is most probably interleaved with the acquisition of word meaning, and these two processes must ultimately</context>
<context position="31126" citStr="Yu (2006)" startWordPosition="5221" endWordPosition="5222">ions: using online LDA categories of varying numbers of 20, 10 and 5, and without using categories. order. The model of Niyogi (2002) simulates the mutual bootstrapping effects of syntactic and semantic knowledge in verb learning, that is the use of syntax to aid in inducing the semantics of a verb, and the use of semantics to narrow down possible syntactic frames in which a verb can participate. However, this model relies on manually assigned priors for associations between syntactic and semantic features, and is tested on a toy language with very limited vocabulary and a constrained syntax. Yu (2006) integrates automatically induced syntactic word categories into his model of crosssituational word learning, showing that they can improve the model’s performance. Yu’s model also processes input utterances in a batch mode, and its evaluation is limited to situations in which only a coarse distinction between referring words (words that could potentially refer to objects in a scene, e.g. concrete nouns) and non-referring words (words that cannot possibly refer to objects, e.g. function words) is sufficient. It is thus not clear whether information about finer-grained categories (e.g. verbs an</context>
</contexts>
<marker>Yu, 2006</marker>
<rawString>Yu, C. (2006). Learning syntax–semantics mappings to bootstrap word learning. In Proceedings of the 28th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
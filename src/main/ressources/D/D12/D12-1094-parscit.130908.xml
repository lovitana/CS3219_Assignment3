<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99698">
Ensemble Semantics for Large-scale Unsupervised Relation Extraction
</title>
<author confidence="0.993299">
Bonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin2
</author>
<affiliation confidence="0.961189">
1New York University 2Microsoft Research Asia
</affiliation>
<address confidence="0.949566">
New York, NY, USA Beijing, China
</address>
<email confidence="0.999555">
{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.com
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998109375">
Discovering significant types of relations
from the web is challenging because of its
open nature. Unsupervised algorithms are
developed to extract relations from a cor-
pus without knowing the relations in ad-
vance, but most of them rely on tagging
arguments of predefined types. Recently,
a new algorithm was proposed to jointly
extract relations and their argument se-
mantic classes, taking a set of relation in-
stances extracted by an open IE algorithm
as input. However, it cannot handle poly-
semy of relation phrases and fails to
group many similar (‚Äúsynonymous‚Äù) rela-
tion instances because of the sparseness of
features. In this paper, we present a novel
unsupervised algorithm that provides a
more general treatment of the polysemy
and synonymy problems. The algorithm
incorporates various knowledge sources
which we will show to be very effective
for unsupervised extraction. Moreover, it
explicitly disambiguates polysemous rela-
tion phrases and groups synonymous
ones. While maintaining approximately
the same precision, the algorithm achieves
significant improvement on recall com-
pared to the previous method. It is also
very efficient. Experiments on a real-
world dataset show that it can handle 14.7
million relation instances and extract a
very large set of relations from the web.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998745">
Relation extraction aims at discovering semantic
relations between entities. It is an important task
that has many applications in answering factoid
questions, building knowledge bases and improv-
ing search engine relevance. The web has become
a massive potential source of such relations. How-
ever, its open nature brings an open-ended set of
relation types. To extract these relations, a system
should not assume a fixed set of relation types, nor
rely on a fixed set of relation argument types.
The past decade has seen some promising solu-
tions, unsupervised relation extraction (URE) algo-
rithms that extract relations from a corpus without
knowing the relations in advance. However, most
algorithms (Hasegawa et al., 2004, Shinyama and
Sekine, 2006, Chen et. al, 2005) rely on tagging
predefined types of entities as relation arguments,
and thus are not well-suited for the open domain.
Recently, Kok and Domingos (2008) proposed
Semantic Network Extractor (SNE), which gener-
ates argument semantic classes and sets of synon-
ymous relation phrases at the same time, thus
avoiding the requirement of tagging relation argu-
ments of predefined types. However, SNE has 2
limitations: 1) Following previous URE algo-
rithms, it only uses features from the set of input
relation instances for clustering. Empirically we
found that it fails to group many relevant relation
instances. These features, such as the surface forms
of arguments and lexical sequences in between, are
very sparse in practice. In contrast, there exist sev-
eral well-known corpus-level semantic resources
that can be automatically derived from a source
corpus and are shown to be useful for generating
the key elements of a relation: its 2 argument se-
mantic classes and a set of synonymous phrases.
For example, semantic classes can be derived from
a source corpus with contextual distributional simi-
larity and web table co-occurrences. The ‚Äúsynony-
my‚Äù 1 problem for clustering relation instances
</bodyText>
<note confidence="0.49719">
* Work done during an internship at Microsoft Research Asia
</note>
<page confidence="0.948128">
1027
</page>
<note confidence="0.784625">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1027‚Äì1037, Jeju Island, Korea, 12‚Äì14 July 2012. cÔøΩ2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9985394">
could potentially be better solved by adding these
resources. 2) SNE assumes that each entity or rela-
tion phrase belongs to exactly one cluster, thus is
not able to effectively handle polysemy of relation
phrases2. An example of a polysemous phrase is be
the currency of as in 2 triples &lt;Euro, be the cur-
rency of, Germany&gt; and &lt;authorship, be the cur-
rency of, science&gt;. As the target corpus expands
from mostly news to the open web, polysemy be-
comes more important as input covers a wider
range of domains. In practice, around 22% (section
3) of relation phrases are polysemous. Failure to
handle these cases significantly limits its effective-
ness.
To move towards a more general treatment of
the polysemy and synonymy problems, we present a
novel algorithm WEBRE for open-domain large-
scale unsupervised relation extraction without pre-
defined relation or argument types. The contribu-
tions are:
</bodyText>
<listItem confidence="0.920270692307692">
‚Ä¢ WEBRE incorporates a wide range of cor-
pus-level semantic resources for improving rela-
tion extraction. The effectiveness of each
knowledge source and their combination are stud-
ied and compared. To the best of our knowledge, it
is the first to combine and compare them for unsu-
pervised relation extraction.
‚Ä¢ WEBRE explicitly disambiguates polyse-
mous relation phrases and groups synonymous
phrases, thus fundamentally it avoids the limitation
of previous methods.
‚Ä¢ Experiments on the Clueweb09 dataset
(lemurproject.org/clueweb09.php) show that
</listItem>
<bodyText confidence="0.8111641">
WEBRE is effective and efficient. We present a
large-scale evaluation and show that WEBRE can
extract a very large set of high-quality relations.
Compared to the closest prior work, WEBRE sig-
nificantly improves recall while maintaining the
same level of precision. WEBRE is efficient. To
the best of our knowledge, it handles the largest
triple set to date (7-fold larger than largest previous
effort). Taking 14.7 million triples as input, a com-
plete run with one CPU core takes about a day.
</bodyText>
<footnote confidence="0.83992925">
1 We use the term synonymy broadly as defined in Section 3.
2 A cluster of relation phrases can, however, act as a whole as
the phrase cluster for 2 different relations in SNE. However,
this only accounts for 4.8% of the polysemous cases.
</footnote>
<sectionHeader confidence="0.998496" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999833958333333">
Unsupervised relation extraction (URE) algorithms
(Hasegawa et al., 2004; Chen et al., 2005; Shinya-
ma and Sekine, 2006) collect pairs of co-occurring
entities as relation instances, extract features for
instances and then apply unsupervised clustering
techniques to find the major relations of a corpus.
These UREs rely on tagging a predefined set of
argument types, such as Person, Organization, and
Location, in advance. Yao et al. 2011 learns fine-
grained argument classes with generative models,
but they share the similar requirement of tagging
coarse-grained argument types. Most UREs use a
quadratic clustering algorithm such as Hierarchical
Agglomerate Clustering (Hasegawa et al., 2004,
Shinyama and Sekine, 2006), K-Means (Chen et
al., 2005), or both (Rosenfeld and Feldman, 2007);
thus they are not scalable to very large corpora.
As the target domain shifts to the web, new
methods are proposed without requiring predefined
entity types. Resolver (Yates and Etzioni, 2007)
resolves objects and relation synonyms. Kok and
Domingos (2008) proposed Semantic Network Ex-
tractor (SNE) to extract concepts and relations.
Based on second-order Markov logic, SNE used a
bottom-up agglomerative clustering algorithm to
jointly cluster relation phrases and argument enti-
ties. However, both Resolver and SNE require
each entity and relation phrase to belong to exactly
one cluster. This limits their ability to handle poly-
semous relation phrases. Moreover, SNE only uses
features in the input set of relation instances for
clustering, thus it fails to group many relevant in-
stances. Resolver has the same sparseness problem
but it is not affected as much as SNE because of its
different goal (synonym resolution).
As the preprocessing instance-detection step for
the problem studied in this paper, open IE extracts
relation instances (in the form of triples) from the
open domain (Etzioni et al., 2004; Banko et al.,
2007; Fader et al., 2011; Wang et al. 2011). For
efficiency, they only use shallow features. Reverb
(Fader et al., 2011) is a state-of-the-art open do-
main extractor that targets verb-centric relations,
which have been shown in Banko and Etzioni
(2008) to cover over 70% of open domain rela-
tions. Taking their output as input, algorithms have
been proposed to resolve objects and relation syn-
onyms (Resolver), extract semantic networks
</bodyText>
<page confidence="0.991979">
1028
</page>
<bodyText confidence="0.999859454545455">
(SNE), and map extracted relations into an existing
ontology (Soderland and Mandhani, 2007).
Recent work shows that it is possible to con-
struct semantic classes and sets of similar phrases
automatically with data-driven approaches. For
generating semantic classes, previous work applies
distributional similarity (Pasca, 2007; Pantel et al.,
2009), uses a few linguistic patterns (Pasca 2004;
Sarmento et al., 2007), makes use of structure in
webpages (Wang and Cohen 2007, 2009), or com-
bines all of them (Shi et al., 2010). Pennacchiotti
and Pantel (2009) combines several sources and
features. To find similar phrases, there are 2 close-
ly related tasks: paraphrase discovery and recog-
nizing textual entailment. Data-driven paraphrase
discovery methods (Lin and Pantel, 2001; Pasca
and Dienes, 2005; Wu and Zhou, 2003; Sekine,
2005) extends the idea of distributional similarity
to phrases. The Recognizing Textual Entailment
algorithms (Berant et al. 2011) can also be used to
find related phrases since they find pairs of phrases
in which one entails the other.
To efficiently cluster high-dimensional datasets,
canopy clustering (McCallum et al., 2000) uses a
cheap, approximate distance measure to divide da-
ta into smaller subsets, and then cluster each subset
using an exact distance measure. It has been ap-
plied to reference matching. The second phase of
WEBRE applies the similar high-level idea of par-
tition-then-cluster for speeding up relation cluster-
ing. We design a graph-based partitioning
subroutine that uses various types of evidence,
such as shared hypernyms.
</bodyText>
<sectionHeader confidence="0.973637" genericHeader="method">
3 Problem Analysis
</sectionHeader>
<bodyText confidence="0.99988375">
The basic input is a collection of relation instances
(triples) of the form &lt;ent1, ctx, ent2&gt;. For each tri-
ple, ctx is a relational phrase expressing the rela-
tion between the first argument ent1 and the second
argument ent2. An example triple is &lt;Obama, win
in, NY&gt;. The triples can be generated by an open
IE extractor such as TextRunner or Reverb. Our
goal is to automatically build a list of relations
</bodyText>
<equation confidence="0.847483">
R = {&lt; ent1, ctx, ent2 &gt;} ÔøΩ 3 &lt; C1, P, C2 &gt; where P
</equation>
<bodyText confidence="0.884444333333333">
is the set of relation phrases, and C1 and C2 are
two argument classes. Examples of triples and rela-
tions R (as Type B) are shown in Figure 1.
</bodyText>
<footnote confidence="0.981957">
3 This approximately equal sign connects 2 possible represen-
tations of a relation: as a set of triple instances or a triple with
2 entity classes and a relation phrase class.
</footnote>
<bodyText confidence="0.999848833333333">
The first problem is the polysemy of relation
phrases, which means that a relation phrase ctx can
express different relations in different triples. For
example, the meaning of be the currency of in the
following two triples is quite different: &lt;Euro, be
the currency of, Germany&gt; and &lt;authorship, be
the currency of, science&gt;. It is more appropriate to
assign these 2 triples to 2 relations ‚Äúa currency is
the currency of a country‚Äù and ‚Äúa factor is im-
portant in an area‚Äù than to merge them into one.
Formally, a relation phrase ctx is polysemous if
there exist 2 different relations &lt; C1, P, C2 &gt; and
</bodyText>
<equation confidence="0.86413">
‚Ä≤
&lt; C1 , P‚Ä≤, C2 ‚Ä≤ &gt; where ctx E P n P‚Ä≤. In the previ-
</equation>
<bodyText confidence="0.997671444444444">
ous example, be the currency of is polysemous
because it appears in 2 different relations.
Polysemy of relation phrases is not uncommon.
We generate clusters from a large sample of triples
with the assistance of a soft clustering algorithm,
and found that around 22% of relation phrases can
be put into at least 2 disjoint clusters that represent
different relations. More importantly, manual in-
spection reveals that some common phrases are
polysemous. For example, be part of can be put
into a relation ‚Äúa city is located in a country‚Äù when
connecting Cities to Countries, and another rela-
tion ‚Äúa company is a subsidiary of a parent com-
pany‚Äù when connecting Companies to Companies.
Failure to handle polysemous relation phrases fun-
damentally limits the effectiveness of an algorithm.
The WEBRE algorithm described later explicitly
handles polysemy and synonymy of relation
phrases in its first and second phase respectively.
The second problem is the ‚Äúsynonymy‚Äù of rela-
tion instances. We use the term synonymy broadly
and we say 2 relation instances are synonymous if
they express the same semantic relation between
the same pair of semantic classes. For example,
both &lt;Euro, be the currency used in, Germany&gt;
and &lt;Dinar, be legal tender in, Iraq&gt; express the
relation &lt;Currencies, be currency of, Countries&gt;.
Solving this problem requires grouping synony-
mous relation phrases and identifying argument
semantic classes for the relation.
Various knowledge sources can be derived from
the source corpus for this purpose. In this paper we
pay special attention to incorporating various se-
mantic resources for relation extraction. We will
show that these semantic sources can significantly
improve the coverage of extracted relations and the
</bodyText>
<page confidence="0.996757">
1029
</page>
<figureCaption confidence="0.93981225">
Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec-
tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-
tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the
green arrows show the resources used in phase 2.
</figureCaption>
<bodyText confidence="0.715876">
best performance is achieved when various re-
sources are combined together.
</bodyText>
<sectionHeader confidence="0.870727" genericHeader="method">
4 Mining Relations from the Web
</sectionHeader>
<bodyText confidence="0.999766666666667">
We first describe relevant knowledge sources, and
then introduce the WEBRE algorithm, followed by
a briefly analysis on its computational complexity.
</bodyText>
<subsectionHeader confidence="0.996672">
4.1 Knowledge Sources
</subsectionHeader>
<bodyText confidence="0.999974773584906">
Entity similarity graph We build two similarity
graphs for entities: a distributional similarity (DS)
graph and a pattern-similarity (PS) graph. The DS
graph is based on the distributional hypothesis
(Harris, 1985), saying that terms sharing similar
contexts tend to be similar. We use a text window
of size 4 as the context of a term, use Pointwise
Mutual Information (PMI) to weight context fea-
tures, and use Jaccard similarity to measure the
similarity of term vectors. The PS graph is gener-
ated by adopting both sentence lexical patterns and
HTML tag patterns (Hearst, 1992; Kozareva et al.,
2008; Zhang et al., 2009; Shi et al., 2010). Two
terms (T) tend to be semantically similar if they co-
occur in multiple patterns. One example of sen-
tence lexical patterns is (such as  |including)
T{,T}* (and|,|.). HTML tag patterns include tables,
dropdown boxes, etc. In these two graphs, nodes
are entities and the edge weights indicate entity
similarity. In all there are about 29.6 million nodes
and 1.16 billion edges.
Hypernymy graph Hypernymy relations are
very useful for finding semantically similar term
pairs. For example, we observed that a small city
in UK and another small city in Germany share
common hypernyms such as city, location, and
place. Therefore the similarity between the two
cities is large according to the hypernymy graph,
while their similarity in the DS graph and the PS
graph may be very small. Following existing work
(Hearst, 1992, Pantel &amp; Ravichandran 2004; Snow
et al., 2005; Talukdar et al., 2008; Zhang et al.,
2011), we adopt a list of lexical patterns to extract
hypernyms. The patterns include IP {,} (such as)
{IP,}* {and|or} IP, IP (is|are|was|were|being)
(a|an|the) IP, etc. The hypernymy graph is a bi-
partite graph with two types of nodes: entity nodes
and label (hypernym) nodes. There is an edge (T,
L) with weight w if L is a hypernym of entity T
with probability w. There are about 8.2 million
nodes and 42.4 million edges in the hypernymy
graph. In this paper, we use the terms hypernym
and label interchangeably.
Relation phrase similarity: To generate the pair-
wise similarity graph for relation phrases with re-
gard to the probability of expressing the same
relation, we apply a variant of the DIRT algorithm
(Lin and Pantel, 2001). Like DIRT, the paraphrase
discovery relies on the distributional hypothesis,
but there are a few differences: 1) we use stemmed
lexical sequences (relation phrases) instead of de-
pendency paths as phrase candidates because of the
very large scale of the corpus. 2) We used ordered
</bodyText>
<page confidence="0.959309">
1030
</page>
<bodyText confidence="0.999679666666667">
pairs of arguments as features of phrases while
DIRT uses them as independent features. We em-
pirically tested both feature schemes and found
that using ordered pairs results in likely para-
phrases but using independent features the result
contains general inference rules4.
</bodyText>
<subsectionHeader confidence="0.827933">
4.2 WEBRE for Relation Extraction
</subsectionHeader>
<bodyText confidence="0.993507138888889">
WEBRE consists of two phases. In the first
phase, a set of semantic classes are discovered and
used as argument classes for each relation phrase.
This results in a large collection of relations whose
arguments are pairs of semantic classes and which
have exactly one relation phrase. We call these
relations the Type A relations. An example Type A
relation is &lt;(New York, London...}, be locate in,
(USA, England, ...}&gt;. During this phase, polyse-
mous relation phrases are disambiguated and
placed into multiple Type A relations. The second
phase is an efficient algorithm which groups simi-
lar Type A relations together. This step enriches
the argument semantic classes and groups synon-
ymous relation phrases to form relations with mul-
tiple expressions, which we called Type B
relations. Both Type A and Type B relations are
system outputs since both are valuable resources
for downstream applications such as QA and Web
Search. An overview of the algorithm is shown in
Figure 1. Here we first briefly describe a clustering
subroutine that is used in both phases, and then
describe the algorithm in detail.
To handle polysemy of objects (e.g., entities or
relations) during the clustering procedure, a key
building block is an effective Multi-Membership
Clustering algorithm (MMClustering). For simplic-
ity and effectiveness, we use a variant of Hierar-
chical Agglomerative Clustering (HAC), in which
we first cluster objects with HAC, and then reas-
sign each object to additional clusters when its
similarities with these clusters exceed a certain
threshold5. In the remainder of this paper, we use
fC} = MMClustering(fobject}, SimFunc, Œ±) to rep-
resent running MMClustering over a set of objects,
4 For example, be part of has ordered argument pairs &lt;A, B&gt;
and &lt;C, D&gt;, and be not part of has ordered argument pairs
&lt;A, D&gt; and &lt;B, C&gt;. If arguments are used as independent
features, these two phrases shared the same set of features (A,
B, C, D}. However, they are inferential (complement relation-
ship) rather than being similar phrases.
5 This threshold should be slightly greater than the clustering
threshold for HAC to avoid generating duplicated clusters.
with threshold Œ± to generate a list of clusters (C} of
the objects, given the pairwise object similarity
function SimFunc. Our implementation uses HAC
with average linkage since empirically it performs
well.
Discovering Type A Relations The first phase
of the relation extraction algorithm generates Type
A relations, which have exactly one relation phrase
and two argument entity semantic classes. For each
relation phrase, we apply a clustering algorithm on
each of its two argument sets to generate argument
semantic classes. The Phase 1 algorithm processes
relation phrases one by one. For each relation
phrase ctx, step 4 clusters the set (ent1} using
MMClustering to find left-hand-side argument se-
mantic classes (C1}. Then for each cluster C in
(C1}, it gathers the right-hand-side arguments
which appeared in some triple whose left hand-
side-side argument is in C, and puts them into
(ent2‚Äô}. Following this, it clusters (ent2‚Äô} to find
right-hand-side argument semantic classes. This
results in pairs of semantic classes which are ar-
guments of ctx. Each relation phrase can appear in
multiple non-overlapping Type A relations. For
example, &lt;Cities, be part of, Countries&gt; and
&lt;Companies, be part of, Companies&gt; are different
Type A relations which share the same relation
phrase be part of. In the pseudo code, SimEntFunc
is encoded in the entity similarity graphs.
</bodyText>
<subsectionHeader confidence="0.313091">
Algorithm Phase 1: Discovering Type A relations
</subsectionHeader>
<bodyText confidence="0.6765824">
Input: set of triples T=(&lt;ent1, ctx, ent2&gt;}
entity similarity function SimEntFunc
Similarity threshold Œ±
Output: list of Type A relations (&lt;C1, ctx, C2&gt;}
Steps:
</bodyText>
<listItem confidence="0.972385181818182">
1. For each relation phrase ctx
2. (ent1, ctx, ent2} = set of triples sharing ctx
3. (ent1} = set of ent1 in (ent1, ctx, ent2}
4. (C1} = MMClustering((ent1}, SimEntFunc, Œ±)
5. For each C in ( C1}
6. (ent2‚Äô} = set of ent2 s. t. 3&lt; ent1, ctx, ent2 &gt; E
T A ent1 E C1
7. (C2} = MMClustering((ent2‚Äô}, SimEntFunc, Œ±)
8. For each C2 in (C2}
9. Add &lt;C1, ctx, C2&gt; into (&lt;C1, ctx, C2&gt;}
10. Return (&lt;C1, ctx, C2&gt;}
</listItem>
<bodyText confidence="0.995362571428571">
Discovering Type B Relations The goal of
phase 2 is to merge similar Type A relations, such
as &lt;Cities, be locate in, Countries&gt; and &lt;Cities,
be city of, Countries&gt;, to produce Type B relations,
which have a set of synonymous relation phrases
and more complete argument entity classes. The
challenge for this phase is to cluster a very large
</bodyText>
<page confidence="0.974352">
1031
</page>
<bodyText confidence="0.99972652">
set of Type A relations, on which it is infeasible to
run a clustering algorithm that does pairwise all
pair comparison. Therefore, we designed an evi-
dence-based partition-then-cluster algorithm.
The basic idea is to heuristically partition the
large set of Type A relations into small subsets,
and run clustering algorithms on each subset. It is
based on the observation that most pairs of Type A
relations are not similar because of the sparseness
in the entity class and the relation semantic space.
If there is little or no evidence showing that two
Type A relations are similar, they can be put into
different partitions. Once partitioned, the clustering
algorithm only has to be run on each much smaller
subset, thus computation complexity is reduced.
The 2 types of evidence we used are shared
members and shared hypernyms of relation argu-
ments. For example, 2 Type A relations
r1=&lt;Cities, be city of, Countries&gt; and r2=&lt;Cities,
be locate in, Countries&gt; share a pair of arguments
&lt;Tokyo, Japan&gt;, and a pair of hypernyms &lt;‚Äùcity‚Äù,
‚Äúcountry‚Äù&gt;. These pieces of evidence give us hints
that they are likely to be similar. As shown in the
pseudo code, shared arguments and hypernyms are
used as independent evidence to reduce sparseness.
</bodyText>
<table confidence="0.843588888888889">
Algorithm Phase 2: Discovering Type B relations
Input: Set of Type A relations (r)=(&lt;C1, ctz, C2&gt;)
Relation similarity function SimRelationFunc
Map from entities to their hypernyms: Mentity2label
Similarity threshold Œ±
Edge weight threshold ¬µ
Variables G(V, E) = weighted graph in which V=(r)
Output: Set of Type B relations (&lt;C1, P, C2&gt;)
Steps:
</table>
<bodyText confidence="0.7557078">
01. (&lt;ent, (r‚Äô)&gt;) = build inverted index from argument
ent to set of Type A relations (r‚Äô) on (&lt;C1, ctz, C2&gt;)
02 (&lt;l, (r‚Äô)&gt;) = build inverted index from hypernym l
of arguments to set of Type A relations (r‚Äô) on (&lt;C1,
ctz, C2&gt;) with map Mentity2label
</bodyText>
<listItem confidence="0.911479857142857">
3. For each ent in (&lt;ent, (r‚Äô)&gt;)
4. For each pair of r1 and r2 s.t. rl E fr‚Ä≤} A r2 E fr‚Ä≤}
5. weight_edge(&lt;r1, r2&gt;) += weight (ent)
6. For each l in (&lt;l, (r‚Äô)&gt;)
7. For each pair of r1 and r2 s.t. rl E fr‚Ä≤} A r2 E fr‚Ä≤}
8. weight_edge(&lt;r1, r2&gt;) += weight (l)
9. For each edge &lt;r1, r2&gt; in G
</listItem>
<figure confidence="0.929946875">
10. If weight_edge(&lt;r1, r2&gt;) &lt; ¬µ
11. Remove edge &lt;r1, r2&gt; from G
12. (CC)= DFS(G)
13. For each connected component CC in (CC)
14. (&lt;C1, ctz, C2&gt;) = vertices in CC
15. (&lt;C1‚Äô, P‚Äô, C2‚Äô&gt;) = MMClustering((&lt;C1, ctz, C2&gt;),
SimRelationFunc, Œ±)
16. Add (&lt;C1‚Äô, P‚Äô, C2‚Äô&gt;) into (&lt;C1, P, C2&gt;)
</figure>
<page confidence="0.817076">
17. Return (&lt;C1, P, C2&gt;)
</page>
<bodyText confidence="0.942967571428572">
Steps 1 and 2 build an inverted index from evi-
dence to sets of Type A relations. On the graph G
whose vertices are Type A relations, steps 3 to 8
set the value of edge weights based on the strength
of evidence that shows the end-points are related.
The weight of evidence E is calculated as follows:
# shared tuples in which E appears in
</bodyText>
<subsubsectionHeader confidence="0.394363">
max(# classes E appears in)
</subsubsectionHeader>
<bodyText confidence="0.997967933333333">
The idea behind this weighting scheme is similar
to that of TF-IDF in that the weight of evidence is
higher if it appears more frequently and is less am-
biguous (appeared in fewer semantic classes during
clustering of phase 1). The weighting scheme is
applied to both shared arguments and labels.
After collecting evidence, we prune (steps 9 to
11) the edges with a weight less than a threshold ¬µ
to remove noise. Then a Depth-First Search (DFS)
is called on G to find all Connected Components
CC of the graph. These CCs are the partitions of
likely-similar Type A relations. We run MMClus-
tering on each CC in (CC) and generate Type B
relations (step 13 to step 16). The similarity of 2
relations (SimRelationFunc) is defined as follows:
</bodyText>
<equation confidence="0.782774333333333">
sim(&lt; Cl, P, C2 &gt;, &lt; Ci, P‚Ä≤, C2‚Ä≤ &gt;)
0, if sim(P,P‚Ä≤) &lt; LT
min(sim(Cl, Ci), sim(C2, C2‚Ä≤)), else
</equation>
<subsectionHeader confidence="0.992319">
4.3 Computational Complexity
</subsectionHeader>
<bodyText confidence="0.999969095238095">
WEBRE is very efficient since both phases de-
compose the large-clustering task into much small-
er clustering tasks over partitions. Given n objects
for clustering, a hierarchical agglomerative cluster-
ing algorithm requires 0(n2) pairwise compari-
sons. Assuming the clustering task is split into
subtasks of size nl, n2, ..., nk, thus the computa-
tional complexity is reduced to 0 (Z1 n?). Ideally
each subtask has an equal size of n/k, so the com-
putational complexity is reduced to O(n2/k), a
factor of k speed up. In practice, the sizes of parti-
tions are not equal. Taking the partition sizes ob-
served in the experiment with 0.2 million Type A
relations as input, the phase 2 algorithm achieves
around a 100-fold reduction in pairwise compari-
sons compared to the agglomerative clustering al-
gorithm. The combination of phase 1 and phase 2
achieves more than a 1000-fold reduction in pair-
wise comparison, compared to running an agglom-
erative clustering algorithm directly on 14.7
million triples. This reduction of computational
</bodyText>
<equation confidence="0.956008">
weight(E) =
=1
</equation>
<page confidence="0.872801">
1032
</page>
<bodyText confidence="0.9999689">
complexity makes the unsupervised extraction of
relations on a large dataset a reality. In the experi-
ments with 14.7 million triples as input, phase 1
finished in 22 hours, and the phase 2 algorithm
finished in 4 hours with one CPU core.
Furthermore, both phases can be run in parallel
in a distributed computing environment because
data is partitioned. Therefore it is scalable and effi-
cient for clustering a very large number of relation
instances from a large-scale corpus like the web.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999658047619048">
Data preparation We tested WEBRE on re-
sources extracted from the English subset of the
Clueweb09 Dataset, which contains 503 million
webpages. For building knowledge resources, all
webpages are cleaned and then POS tagged and
chunked with in-house tools. We implemented the
algorithms described in section 4.1 to generate the
knowledge sources, including a hypernym graph,
two entity similarity graphs and a relation phrase
similarity graph.
We used Reverb Clueweb09 Extractions 1.1
(downloaded from reverb.cs.washington.edu) as
the triple store (relation instances). It is the com-
plete extraction of Reverb over Clueweb09 after
filtering low confidence and low frequency triples.
It contains 14.7 million distinct triples with 3.3
million entities and 1.3 million relation phrases.
We choose it because 1) it is extracted by a state-
of-the-art open IE extractor from the open-domain,
and 2) to the best of our knowledge, it contains the
largest number of distinct triples extracted from the
open-domain and which is publicly available.
Evaluation setup The evaluations are organized as
follows: we evaluate Type A relation extraction
and Type B relation extraction separately, and then
we compare WEBRE to its closest prior work
SNE. Since both phases are essentially clustering
algorithms, we compare the output clusters with
human labeled gold standards and report perfor-
mance measures, following most previous work
such as Kok and Domingos (2008) and Hasegawa
et al. (2004). Three gold standards are created for
evaluating Type A relations, Type B relations and
the comparison to SNE, respectively. In the exper-
iments, we set Œ±=0.6, ¬µ=0.1 and ùúé=0.02 based on
trial runs on a small development set of 10k rela-
tion instances. We filtered out the Type A relations
and Type B relations which only contain 1 or 2
triples since most of these relations are not differ-
ent from a single relation instance and are not very
interesting. Overall, 0.2 million Type A relations
and 84,000 Type B relations are extracted.
</bodyText>
<listItem confidence="0.972365421052632">
Evaluating Type A relations To understand the
effectiveness of knowledge sources, we run Phase
1 multiple times taking entity similarity graphs
(matrices) constructed with resources listed below:
‚Ä¢ TS: Distributional similarity based on the triple
store. For each triple &lt;ent1, ctx, ent2&gt;, features
of ent1 are {ctx) and {ctx ent2); features of ent2
are {ctx) and {ent1 ctx). Features are weighted
with PMI. Cosine is used as similarity measure.
‚Ä¢ LABEL: The similarity between two entities is
computed according to the percentage of top
hypernyms they share.
‚Ä¢ SIM: The similarity between two entities is the
linear combination of their similarity scores in
the distributional similarity graph and in the
pattern similarity graph.
‚Ä¢ SIM+LABEL SIM and LABEL are combined.
Observing that SIM generates high quality but
overly fine-grained semantic classes, we modify
</listItem>
<bodyText confidence="0.929817730769231">
the entity clustering procedure to cluster argu-
ment entities based on SIM first, and then fur-
ther clustering the results based on LABEL.
The outputs of these runs are pooled and mixed
for labeling. We randomly sampled 60 relation
phrases. For each phrase, we select the 5 most fre-
quent Type A relations from each run (4x5=206
Type A relations in all). For each relation phrase,
we ask a human labeler to label the mixed pool of
Type A relations that share the phrase: 1) The la-
belers7 are asked to first determine the major se-
mantic relation of each Type A relation, and then
label the triples as good, fair or bad based on
whether they express the major relation. 2) The
labeler also reads all Type A relations and manual-
ly merges the ones that express the same relation.
These 2 steps are repeated for each phrase. After
labeling, we create a gold standard GS1, which
contains roughly 10,000 triples for 60 relation
phrases. On average, close to 200 triples are manu-
6 Here 4 means the 4 methods (the bullet items above) of
computing similarity.
7 4 human labelers perform the task. A portion of the judg-
ments were independently dual annotated; inter-annotator
agreement is 79%. Moreover, each judgment is cross-checked
by at least one more annotator, further improving quality.
</bodyText>
<page confidence="0.981772">
1033
</page>
<bodyText confidence="0.999064272727273">
ally labeled and clustered for each phrase. This
creates a large data set for evaluation.
We report micro-average of precision, recall and
F1 on the 60 relation phrases for each method. Pre-
cision (P) and Recall (R) of a given relation phrase
is defined as follows. Here RA and RA represents a
Type A relation in the algorithm output and GS1,
respectively. We use t for triples and s(t) to repre-
sent the score of the labeled triple t. s(t) is set to
1.0, 0.5 or 0 for t labeled as good, fair and bad,
respectively.
</bodyText>
<equation confidence="0.993556333333333">
E RA E ÔøΩÔøΩRA S(ÔøΩ) E RA E ÔøΩÔøΩRA S(ÔøΩ)
P _ , R _
ERA |RA |ERA‚Ä≤E t‚Ä≤ERAS(t‚Ä≤)
</equation>
<bodyText confidence="0.9977152">
The results are in table 1. Overall, LABEL per-
forms 53% better than TS in F-measure, and
SIM+LABEL performs the best, 8% better than
LABEL. Applying a simple sign test shows both
differences are clearly significant (p&lt;0.001). Sur-
prisingly, SIM, which uses the similarity matrix
extracted from full text, has a F1 of 0.277, which is
lower than TS. We also tried combining TS and
LABEL but did not find encouraging performance
compared to SIM+LABEL.
</bodyText>
<table confidence="0.9996586">
Algorithm Precision Recall F1
TS 0.842 (0.886) 0.266 0.388
LABEL 0.855 (0.870) 0.481 0.596
SIM 0.755 (0.964) 0.178 0.277
SIM+LABEL 0.843 (0.872) 0.540 0.643
</table>
<tableCaption confidence="0.9810885">
Table 1. Phase 1 performance (averaged on multiple runs) of
the 4 methods. The highest performance numbers are in bold.
(The number in parenthesis is the micro-average when empty-
result relation phrases are not considered for the method).
</tableCaption>
<bodyText confidence="0.999988948275862">
Among the 4 methods, SIM has the highest preci-
sion (0.964) when relation phrases for which it
fails to generate any Type A relations are exclud-
ed, but its recall is low. Manual checking shows
that SIM tends to generate overly fine-grained ar-
gument classes. If fine-grained argument classes or
extremely high-precision Type A relations are pre-
ferred, SIM is a good choice. LABEL performs
significantly better than TS, which shows that hy-
pernymy information is very useful for finding ar-
gument semantic classes. However, it has coverage
problems in that the hypernym finding algorithm
failed to find any hypernym from the corpus for
some entities. Following up, we found that
SIM+LABEL has similar precision and the highest
recall. This shows that the combination of semantic
spaces is very helpful. The significant recall im-
provement from TS to SIM+LABEL shows that
the corpus-based knowledge resources significant-
ly reduce the data sparseness, compared to using
features extracted from the triple store only. The
result of the phase 1 algorithm with SIM+LABEL
is used as input for phase 2.
Evaluating Type B relations The goal is 2-fold:
1) to evaluate the phase 2 algorithm. This involves
comparing system output to a gold standard con-
structed by hand, and reporting performance; 2) to
evaluate the quality of Type B relations. For this,
we will also report triple-level precision.
We construct a gold standard GS28 for evaluat-
ing Type B relations as follows: We randomly
sampled 178 Type B relations, which contain 1547
Type A relations and more than 100,000 triples.
Since the number of triples is very large, it is in-
feasible for labelers to manually cluster triples to
construct a gold standard. To report precision, we
asked the labelers to label each Type A relation
contained in this Type B relation as good, fair or
bad based on whether it expresses the same rela-
tion. For recall evaluation, we need to know how
many Type A relations are missing from each Type
B relation. We provide the full data set of Type A
relations along with three additional resources: 1) a
tool which, given a Type A relation, returns a
ranked list of similar Type A relations based on the
pairwise relation similarity metric in section 4, 2)
DIRT paraphrase collection, 3) WordNet (Fell-
baum, 1998) synsets. The labelers are asked to find
similar phrases by checking phrases which contain
synonyms of the tokens in the query phrase. Given
a Type B relation, ideally we expect the labelers to
find all missing Type A relations using these re-
sources. We report precision (P) and recall (R) as
follows. Here RB and RB represent Type B rela-
tions in the algorithm output and GS2, respective-
ly. RA and RA represent Type A relations. s(RA)
denotes the score of RA. It is set to 1.0, 0.5 and 0
for good, fair or bad respectively.
</bodyText>
<equation confidence="0.966299">
P _ ERB ERAGRB |RA |-S(RA) R _ ERB E RAF:RB |RA |-S(RA)
ERBE RAERB|RA|
ERB‚Ä≤ERA‚Ä≤ ERB‚Ä≤IR‚Ä≤AI
</equation>
<bodyText confidence="0.837154777777778">
We also ask the labeler to label at most 50 ran-
domly sampled triples from each Type B relation,
and calculate triple-level precision as the ratio of
the sum of scores of triples over the number of
8 3 human labelers performed the task. A portion of the judg-
ments were independently dual annotated; inter-annotator
agreement is 73%. Similar to labeling Type A relations, each
judgment is cross-checked by at least one more annotator,
further improving quality.
</bodyText>
<page confidence="0.965282">
1034
</page>
<table confidence="0.99614875">
Argument 1 Relation phrase Argument 2
marijuana, caffeine, nicotine... result in, be risk factor for, be major cause of... insomnia, emphysema, breast cancer,...
C# 2.0, php5, java, c++, ... allow the use of, also use, introduce the concept of... destructors, interfaces, template,...
clinton, obama, mccain, ... win, win in, take, be lead in,... ca, dc, fl, nh, pa, va, ga, il, nc,...
</table>
<tableCaption confidence="0.998788">
Table 3. Sample Type B relations extracted.
</tableCaption>
<bodyText confidence="0.988762166666667">
sampled triples. We use Pins to represent the preci-
sion calculated based on labeled triples. Moreover,
as we are interested in how many phrases are
found by our algorithm, we also include Rphrase,
which is the recall of synonymous phrases. Results
are shown in Table 2.
</bodyText>
<table confidence="0.999882714285714">
Interval P R (Rphrase) F1 Pins count
[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149
[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981
[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277
[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630
[50, +oo) 0.922 0.825 (0.594) 0.871 0.929 1089
Overall 0.897 0.684 (0.324) 0.776 0.898 84126
</table>
<tableCaption confidence="0.991773">
Table 2. Performance for Type B relation extraction. The first
</tableCaption>
<bodyText confidence="0.989108192307693">
column shows the range of the maximum sizes of Type A
relations in the Type B relation. The last column shows the
number of Type B relations that are in this range. The number
in parenthesis in the third column is the recall of phrases.
The result shows that WEBRE can extract Type B
relations at high precision (both P and Pins). The
overall recall is 0.684. Table 2 also shows a trend
that if the maximum number of Type A relation in
the target Type B relation is larger, the recall is
better. This shows that the recall of Type B rela-
tions depends on the amount of data available for
that relation. Some examples of Type B relations
extracted are shown in Table 3.
Comparison with SNE We compare WEBRE‚Äôs
extracted Type B relations to the relations extract-
ed by its closest prior work SNE9. We found SNE
is not able to handle the 14.7 million triples in a
foreseeable amount of time, so we randomly sam-
pled 1 million (1M) triples10 and test both algo-
rithms on this set. We also filtered out result
clusters which have only 1 or 2 triples from both
system outputs. For comparison purposes, we con-
structed a gold standard GS3 as follows: randomly
select 30 clusters from both system outputs, and
then find similar clusters from the other system
output, followed by manually refining the clusters
</bodyText>
<sectionHeader confidence="0.769325" genericHeader="method">
9 Obtained from alchemy.cs.washington.edu/papers/kok08
</sectionHeader>
<bodyText confidence="0.995598769230769">
10 We found that SNE‚Äôs runtime on 1M triples varies from
several hours to over a week, depending on the parameters.
The best performance is achieved with runtime of approxi-
mately 3 days. We also tried SNE with 2M triples, on which
many runs take several days and show no sign of convergence.
For fairness, the comparison was done on 1M triples.
by merging similar ones and splitting non-coherent
clusters. GS3 contains 742 triples and 135 clusters.
We report triple-level pairwise precision, recall
and F1 for both algorithms against GS3, and report
results in Table 4. We fine-tuned SNE (using grid
search, internal cross-validation, and coarse-to-fine
parameter tuning), and report its best performance.
</bodyText>
<table confidence="0.998030666666667">
Algorithm Precision Recall F1
WEBRE 0.848 0.734 0.787
SNE 0.850 0.080 0.146
</table>
<tableCaption confidence="0.999831">
Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.
</tableCaption>
<bodyText confidence="0.985656828571428">
Table 4 shows that WEBRE outperforms SNE
significantly in pairwise recall while having similar
precision. There are two reasons. First, WEBRE
makes use of several corpus-level semantic sources
extracted from the corpus for clustering entities
and phrases while SNE uses only features in the
triple store. These semantic resources significantly
reduced data sparseness. Examination of the output
shows that SNE is unable to group many triples
from the same generally-recognized fine-grained
relations. For example, SNE placed relation in-
stances &lt;Barbara, grow up in, Santa Fe&gt; and
&lt;John, be raised mostly in, Santa Barbara&gt; into 2
different clusters because the arguments and
phrases do not share features nor could be grouped
by SNE‚Äôs mutual clustering. In contrast, WEBRE
groups them together. Second, SNE assumes a re-
lation phrase to be in exactly one cluster. For ex-
ample, SNE placed be part of in the phrase cluster
be city of and failed to place it in another cluster be
subsidiary of. This limits SNE‚Äôs ability to placing
relation instances with polysemous phrases into
correct relation clusters.
It should be emphasized that we use pairwise
precision and recall in table 4 to be consistent with
the original SNE paper. Pairwise metrics are much
more sensitive than instance-level metrics, and pe-
nalize recall exponentially in the worst case11 if an
algorithm incorrectly splits a coherent cluster;
therefore the absolute pairwise recall difference
11 Pairwise precision and recall are calculated on all pairs that
are in the same cluster, thus are very sensitive. For example, if
an algorithm incorrectly split a cluster of size N to a smaller
main cluster of size N/2 and some constant-size clusters, pair-
wise recall could drop to as much as 1/4 of its original value.
</bodyText>
<page confidence="0.980504">
1035
</page>
<bodyText confidence="0.999978">
should not be interpreted as the same as the in-
stance-level recall reported in previous experi-
ments. On 1 million triples, WEBRE generates
12179 triple clusters with an average size12 of 13
while SNE generate 53270 clusters with an aver-
age size 5.1. In consequence, pairwise recall drops
significantly. Nonetheless, at above 80% pairwise
precision, it demonstrates that WEBRE can group
more related triples by adding rich semantics har-
vested from the web and employing a more general
treatment of polysemous relation phrases. On 1M
triples, WEBRE finished in 40 minutes, while the
run time of SNE varies from 3 hours to a few days.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999931875">
We present a fully unsupervised algorithm
WEBRE for large-scale open-domain relation ex-
traction. WEBRE explicitly handles polysemy rela-
tions and achieves a significant improvement on
recall by incorporating rich corpus-based semantic
resources. Experiments on a large data set show
that it can extract a very large set of high-quality
relations.
</bodyText>
<sectionHeader confidence="0.996928" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999235833333333">
Supported in part by the Intelligence Advanced
Research Projects Activity (IARPA) via Air Force
Research Laboratory (AFRL) contract number
FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either
expressed or implied, of IARPA, AFRL, or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989769048387097">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007. Open
Information Extraction from the Web. In Proceedings
of IJCAI 2007.
12 The clusters which have only 1 or 2 triples are removed and
not counted here for both algorithms.
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction.
In Proceedings of ACL 2008.
Jonathan Berant, Ido Dagan and Jacob Goldberger.
2011. Global Learning of Typed Entailment Rules. In
Proceedings of ACL 2011.
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective Information Extraction with Relational Mar-
kov Networks. In Proceedings of ACL 2004.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu
Niu. 2005. Unsupervised Feature Selection for Rela-
tion Extraction. In Proceedings of IJCNLP 2005.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in
KnowItAll (preliminary results). In Proceedings of
WWW 2004.
Oren Etzioni, Michael Cafarella, Doug Downey,
AnaMaria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An
Experimental Study. In Artificial Intelligence,
165(1):91-134.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information Ex-
traction. In Proceedings of EMNLP 2011.
Christiane Fellbaum (Ed.). 1998. WordNet: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press.
Zelig S. Harris. 1985. Distributional Structure. The Phi-
losophy of Linguistics. New York: Oxford Uni-
versity Press.
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman .
2004.Discovering Relations among Named Entities
from Large Corpora. In Proceedings of ACL 2004.
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
COLING 1992.
Stanley Kok and Pedro Domingos. 2008. Extracting
Semantic Networks from Text via Relational Cluster-
ing. In Proceedings of ECML 2008.
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008.
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. In Proceedings of ACL
2008.
Dekang Lin and Patrick Pantel. 2001. DIRT ‚Äì Discov-
ery of Inference Rules from Text. In Proceedings of
KDD 2001.
Andrew McCallum, Kamal Nigam and Lyle Ungar.
2000. Efficient Clustering of High-Dimensional Data
Sets with Application to Reference Matching. In Pro-
ceedings of KDD 2000.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale
Distributional Similarity and Entity Set Expansion. In
Proceedings of EMNLP 2009.
</reference>
<page confidence="0.805188">
1036
</page>
<reference confidence="0.999895522727273">
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of KDD2002.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings
of HLT/NAACL-2004.
Marius Pasca. 2004. Acquisition of Categorized Named
Entities for Web Search, In Proceedings of CIKM
2004.
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM 2007.
Marius Pasca and Peter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the Web.
In Proceedings of IJCNLP 2005.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
Extraction via Ensemble Semantics. In Proceedings
of EMNLP 2009.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM 2007.
Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and
Eugenio Oliveira. 2007. ‚ÄúMore like these‚Äù: growing
entity classes from seeds. In Proceedings of CIKM
2007.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs. In
Proceedings of the International Workshop on Para-
phrasing, 2005.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong
Wen. 2010. Corpus-based Semantic Class Mining:
Distributional vs. Pattern-Based Approaches. In Pro-
ceedings of COLING 2010.
Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive
Information Extraction using Unrestricted Relation
Discovery, In Proceedings of NAACL 2006.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning Syntactic Patterns for Automatic Hypernym
Discovery. In Proceedings of In NIPS 17, 2005.
Stephen Soderland and Bhushan Mandhani. 2007. Mov-
ing from Textual Relations to Ontologized Relations.
In Proceedings of the 2007 AAAI Spring Symposium
on Machine Reading.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat and Fernando
Pereira. 2008. Weakly-Supervised Acquisition of La-
beled Class Instances using Graph Random Walks. In
Proceedings of EMNLP 2008.
David Vickrey, Oscar Kipersztok and Daphne Koller.
2010. An Active Learning Approach to Finding Re-
lated Terms. In Proceedings of ACL 2010.
Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic
Entity Set Refinement. In Proceedings of
NAACL/HLT 2009.
Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009,
Helping Editors Choose Better Seed Sets for Entity
Set Expansion, In Proceedings of CIKM 2009.
Richard C. Wang and William W. Cohen. 2007. Lan-
guage- Independent Set Expansion of Named Entities
Using the Web. In Proceedings of ICDM 2007.
Richard C. Wang and William W. Cohen.
2009. Automatic Set Instance Extraction using the
Web. In Proceedings of ACL-IJCNLP 2009.
Wei Wang, Romaric Besan√ßon and Olivier Ferret. 2011.
Filtering and Clustering Relations for Unsupervised
Information Extraction in Open Domain. In Proceed-
ings of CIKM 2011.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of ACL
2010.
Hua Wu and Ming Zhou. 2003. Synonymous colloca-
tion extraction using translation information. In Pro-
ceedings of the ACL Workshop on Multiword
Expressions: Integrating Processing 2003.
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew
McCallum. 2011. Structured Relation Discovery Us-
ing Generative Models. In Proceedings of EMNLP
2011.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
Resolution of Objects and Relations on the Web. In
Proceedings of HLT-NAACL 2007.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-
Yew Lin. 2011. Nonlinear Evidence Fusion and
Propagation for Hyponymy Relation Mining. In Pro-
ceedings of ACL 2011.
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong
Wen. 2009. Employing Topic Models for Pattern-
based Semantic Class Discovery. In Proceedings of
ACL 2009.
</reference>
<page confidence="0.993922">
1037
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920164">
<title confidence="0.999545">Ensemble Semantics for Large-scale Unsupervised Relation Extraction</title>
<author confidence="0.974924">Shuming Ralph Chin-Yew</author>
<affiliation confidence="0.997099">York University Research Asia</affiliation>
<address confidence="0.95683">York, NY, USA China</address>
<email confidence="0.99949">min@microsoft.com</email>
<email confidence="0.99949">grishman}@cs.nyu.edu{shumings@microsoft.com</email>
<email confidence="0.99949">cyl@microsoft.com</email>
<abstract confidence="0.999670939393939">Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (‚Äúsynonymous‚Äù) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open Information Extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of IJCAI</booktitle>
<contexts>
<context position="7942" citStr="Banko et al., 2007" startWordPosition="1230" endWordPosition="1233"> SNE require each entity and relation phrase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. Resolver has the same sparseness problem but it is not affected as much as SNE because of its different goal (synonym resolution). As the preprocessing instance-detection step for the problem studied in this paper, open IE extracts relation instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In Proceedings of IJCAI 2007.</rawString>
</citation>
<citation valid="false">
<title>12 The clusters which have only 1 or 2 triples are removed and not counted here for both algorithms.</title>
<marker></marker>
<rawString>12 The clusters which have only 1 or 2 triples are removed and not counted here for both algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The Tradeoffs Between Open and Traditional Relation Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="8189" citStr="Banko and Etzioni (2008)" startWordPosition="1270" endWordPosition="1273">ails to group many relevant instances. Resolver has the same sparseness problem but it is not affected as much as SNE because of its different goal (synonym resolution). As the preprocessing instance-detection step for the problem studied in this paper, open IE extracts relation instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The Tradeoffs Between Open and Traditional Relation Extraction. In Proceedings of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global Learning of Typed Entailment Rules.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="9336" citStr="Berant et al. 2011" startWordPosition="1443" endWordPosition="1446">., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies the similar high-level idea of partition-then-cluster for speeding up relation clustering. We design a graph-based partitioning subroutine that uses various types of evidence, such a</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan and Jacob Goldberger. 2011. Global Learning of Typed Entailment Rules. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Collective Information Extraction with Relational Markov Networks.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2004. Collective Information Extraction with Relational Markov Networks. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxiu Chen</author>
<author>Donghong Ji</author>
<author>Chew Lim Tan</author>
<author>Zhengyu Niu</author>
</authors>
<title>Unsupervised Feature Selection for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="6113" citStr="Chen et al., 2005" startWordPosition="948" endWordPosition="951">le maintaining the same level of precision. WEBRE is efficient. To the best of our knowledge, it handles the largest triple set to date (7-fold larger than largest previous effort). Taking 14.7 million triples as input, a complete run with one CPU core takes about a day. 1 We use the term synonymy broadly as defined in Section 3. 2 A cluster of relation phrases can, however, act as a whole as the phrase cluster for 2 different relations in SNE. However, this only accounts for 4.8% of the polysemous cases. 2 Related Work Unsupervised relation extraction (URE) algorithms (Hasegawa et al., 2004; Chen et al., 2005; Shinyama and Sekine, 2006) collect pairs of co-occurring entities as relation instances, extract features for instances and then apply unsupervised clustering techniques to find the major relations of a corpus. These UREs rely on tagging a predefined set of argument types, such as Person, Organization, and Location, in advance. Yao et al. 2011 learns finegrained argument classes with generative models, but they share the similar requirement of tagging coarse-grained argument types. Most UREs use a quadratic clustering algorithm such as Hierarchical Agglomerate Clustering (Hasegawa et al., 20</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2005</marker>
<rawString>Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu Niu. 2005. Unsupervised Feature Selection for Relation Extraction. In Proceedings of IJCNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Web-scale information extraction in KnowItAll (preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of WWW</booktitle>
<contexts>
<context position="7922" citStr="Etzioni et al., 2004" startWordPosition="1226" endWordPosition="1229">ver, both Resolver and SNE require each entity and relation phrase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. Resolver has the same sparseness problem but it is not affected as much as SNE because of its different goal (synonym resolution). As the preprocessing instance-detection step for the problem studied in this paper, open IE extracts relation instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct </context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in KnowItAll (preliminary results). In Proceedings of WWW 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>In Artificial Intelligence,</journal>
<pages>165--1</pages>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld and Alexander Yates. 2005. Unsupervised named-entity extraction from the Web: An Experimental Study. In Artificial Intelligence, 165(1):91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="7962" citStr="Fader et al., 2011" startWordPosition="1234" endWordPosition="1237">tity and relation phrase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. Resolver has the same sparseness problem but it is not affected as much as SNE because of its different goal (synonym resolution). As the preprocessing instance-detection step for the problem studied in this paper, open IE extracts relation instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phr</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extraction. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="34280" citStr="Fellbaum, 1998" startWordPosition="5637" endWordPosition="5639">es to construct a gold standard. To report precision, we asked the labelers to label each Type A relation contained in this Type B relation as good, fair or bad based on whether it expresses the same relation. For recall evaluation, we need to know how many Type A relations are missing from each Type B relation. We provide the full data set of Type A relations along with three additional resources: 1) a tool which, given a Type A relation, returns a ranked list of similar Type A relations based on the pairwise relation similarity metric in section 4, 2) DIRT paraphrase collection, 3) WordNet (Fellbaum, 1998) synsets. The labelers are asked to find similar phrases by checking phrases which contain synonyms of the tokens in the query phrase. Given a Type B relation, ideally we expect the labelers to find all missing Type A relations using these resources. We report precision (P) and recall (R) as follows. Here RB and RB represent Type B relations in the algorithm output and GS2, respectively. RA and RA represent Type A relations. s(RA) denotes the score of RA. It is set to 1.0, 0.5 and 0 for good, fair or bad respectively. P _ ERB ERAGRB |RA |-S(RA) R _ ERB E RAF:RB |RA |-S(RA) ERBE RAERB|RA| ERB‚Ä≤E</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum (Ed.). 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<title>Distributional Structure. The Philosophy of Linguistics.</title>
<date>1985</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="14063" citStr="Harris, 1985" startWordPosition="2238" endWordPosition="2239"> relations. The orange arrows denote resources used in phase 1 and the green arrows show the resources used in phase 2. best performance is achieved when various resources are combined together. 4 Mining Relations from the Web We first describe relevant knowledge sources, and then introduce the WEBRE algorithm, followed by a briefly analysis on its computational complexity. 4.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML t</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Zelig S. Harris. 1985. Distributional Structure. The Philosophy of Linguistics. New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
</authors>
<title>Satoshi Sekine, Ralph Grishman . 2004.Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Hasegawa, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman . 2004.Discovering Relations among Named Entities from Large Corpora. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="14426" citStr="Hearst, 1992" startWordPosition="2301" endWordPosition="2302"> complexity. 4.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another s</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of COLING 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Extracting Semantic Networks from Text via Relational Clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of ECML</booktitle>
<contexts>
<context position="2492" citStr="Kok and Domingos (2008)" startWordPosition="373" endWordPosition="376">ts open nature brings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist sever</context>
<context position="7069" citStr="Kok and Domingos (2008)" startWordPosition="1092" endWordPosition="1095">ao et al. 2011 learns finegrained argument classes with generative models, but they share the similar requirement of tagging coarse-grained argument types. Most UREs use a quadratic clustering algorithm such as Hierarchical Agglomerate Clustering (Hasegawa et al., 2004, Shinyama and Sekine, 2006), K-Means (Chen et al., 2005), or both (Rosenfeld and Feldman, 2007); thus they are not scalable to very large corpora. As the target domain shifts to the web, new methods are proposed without requiring predefined entity types. Resolver (Yates and Etzioni, 2007) resolves objects and relation synonyms. Kok and Domingos (2008) proposed Semantic Network Extractor (SNE) to extract concepts and relations. Based on second-order Markov logic, SNE used a bottom-up agglomerative clustering algorithm to jointly cluster relation phrases and argument entities. However, both Resolver and SNE require each entity and relation phrase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. Resolver has the same sparseness problem but it is not affected a</context>
<context position="27858" citStr="Kok and Domingos (2008)" startWordPosition="4545" endWordPosition="4548">a stateof-the-art open IE extractor from the open-domain, and 2) to the best of our knowledge, it contains the largest number of distinct triples extracted from the open-domain and which is publicly available. Evaluation setup The evaluations are organized as follows: we evaluate Type A relation extraction and Type B relation extraction separately, and then we compare WEBRE to its closest prior work SNE. Since both phases are essentially clustering algorithms, we compare the output clusters with human labeled gold standards and report performance measures, following most previous work such as Kok and Domingos (2008) and Hasegawa et al. (2004). Three gold standards are created for evaluating Type A relations, Type B relations and the comparison to SNE, respectively. In the experiments, we set Œ±=0.6, ¬µ=0.1 and ùúé=0.02 based on trial runs on a small development set of 10k relation instances. We filtered out the Type A relations and Type B relations which only contain 1 or 2 triples since most of these relations are not different from a single relation instance and are not very interesting. Overall, 0.2 million Type A relations and 84,000 Type B relations are extracted. Evaluating Type A relations To understa</context>
</contexts>
<marker>Kok, Domingos, 2008</marker>
<rawString>Stanley Kok and Pedro Domingos. 2008. Extracting Semantic Networks from Text via Relational Clustering. In Proceedings of ECML 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="14449" citStr="Kozareva et al., 2008" startWordPosition="2303" endWordPosition="2306">.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany sh</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<date>2001</date>
<booktitle>DIRT ‚Äì Discovery of Inference Rules from Text. In Proceedings of KDD</booktitle>
<contexts>
<context position="9153" citStr="Lin and Pantel, 2001" startWordPosition="1416" endWordPosition="1419">asses and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies</context>
<context position="16127" citStr="Lin and Pantel, 2001" startWordPosition="2586" endWordPosition="2589">nd|or} IP, IP (is|are|was|were|being) (a|an|the) IP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph for relation phrases with regard to the probability of expressing the same relation, we apply a variant of the DIRT algorithm (Lin and Pantel, 2001). Like DIRT, the paraphrase discovery relies on the distributional hypothesis, but there are a few differences: 1) we use stemmed lexical sequences (relation phrases) instead of dependency paths as phrase candidates because of the very large scale of the corpus. 2) We used ordered 1030 pairs of arguments as features of phrases while DIRT uses them as independent features. We empirically tested both feature schemes and found that using ordered pairs results in likely paraphrases but using independent features the result contains general inference rules4. 4.2 WEBRE for Relation Extraction WEBRE </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT ‚Äì Discovery of Inference Rules from Text. In Proceedings of KDD 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Lyle Ungar</author>
</authors>
<title>Efficient Clustering of High-Dimensional Data Sets with Application to Reference Matching.</title>
<date>2000</date>
<booktitle>In Proceedings of KDD</booktitle>
<contexts>
<context position="9534" citStr="McCallum et al., 2000" startWordPosition="1474" endWordPosition="1477">otti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies the similar high-level idea of partition-then-cluster for speeding up relation clustering. We design a graph-based partitioning subroutine that uses various types of evidence, such as shared hypernyms. 3 Problem Analysis The basic input is a collection of relation instances (triples) of the form &lt;ent1, ctx, ent2&gt;. For each triple, ctx is a relational phrase expressing the relat</context>
</contexts>
<marker>McCallum, Nigam, Ungar, 2000</marker>
<rawString>Andrew McCallum, Kamal Nigam and Lyle Ungar. 2000. Efficient Clustering of High-Dimensional Data Sets with Application to Reference Matching. In Proceedings of KDD 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="8934" citStr="Pantel (2009)" startWordPosition="1387" endWordPosition="1388">tion synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000)</context>
</contexts>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu and Vishnu Vyas. 2009. Web-Scale Distributional Similarity and Entity Set Expansion. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD2002.</booktitle>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of KDD2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically Labeling Semantic Classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL-2004.</booktitle>
<contexts>
<context position="15336" citStr="Pantel &amp; Ravichandran 2004" startWordPosition="2449" endWordPosition="2452">. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel &amp; Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include IP {,} (such as) {IP,}* {and|or} IP, IP (is|are|was|were|being) (a|an|the) IP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity:</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labeling Semantic Classes. In Proceedings of HLT/NAACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of Categorized Named Entities for Web Search,</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="8769" citStr="Pasca 2004" startWordPosition="1358" endWordPosition="1359">wn in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of Categorized Named Entities for Web Search, In Proceedings of CIKM 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="8703" citStr="Pasca, 2007" startWordPosition="1347" endWordPosition="1348"> extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailmen</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proceedings of CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Peter Dienes</author>
</authors>
<title>Aligning needles in a haystack: Paraphrase acquisition across the Web. In</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP</booktitle>
<contexts>
<context position="9177" citStr="Pasca and Dienes, 2005" startWordPosition="1420" endWordPosition="1423">lar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies the similar high-level </context>
</contexts>
<marker>Pasca, Dienes, 2005</marker>
<rawString>Marius Pasca and Peter Dienes. 2005. Aligning needles in a haystack: Paraphrase acquisition across the Web. In Proceedings of IJCNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Entity Extraction via Ensemble Semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="8934" citStr="Pennacchiotti and Pantel (2009)" startWordPosition="1385" endWordPosition="1388">e objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000)</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2009. Entity Extraction via Ensemble Semantics. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Clustering for Unsupervised Relation Identification.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="6811" citStr="Rosenfeld and Feldman, 2007" startWordPosition="1052" endWordPosition="1055"> relation instances, extract features for instances and then apply unsupervised clustering techniques to find the major relations of a corpus. These UREs rely on tagging a predefined set of argument types, such as Person, Organization, and Location, in advance. Yao et al. 2011 learns finegrained argument classes with generative models, but they share the similar requirement of tagging coarse-grained argument types. Most UREs use a quadratic clustering algorithm such as Hierarchical Agglomerate Clustering (Hasegawa et al., 2004, Shinyama and Sekine, 2006), K-Means (Chen et al., 2005), or both (Rosenfeld and Feldman, 2007); thus they are not scalable to very large corpora. As the target domain shifts to the web, new methods are proposed without requiring predefined entity types. Resolver (Yates and Etzioni, 2007) resolves objects and relation synonyms. Kok and Domingos (2008) proposed Semantic Network Extractor (SNE) to extract concepts and relations. Based on second-order Markov logic, SNE used a bottom-up agglomerative clustering algorithm to jointly cluster relation phrases and argument entities. However, both Resolver and SNE require each entity and relation phrase to belong to exactly one cluster. This lim</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2007. Clustering for Unsupervised Relation Identification. In Proceedings of CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
<author>Eugenio Oliveira</author>
</authors>
<title>More like these‚Äù: growing entity classes from seeds.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM</booktitle>
<marker>Sarmento, Jijkoun, de Rijke, Oliveira, 2007</marker>
<rawString>Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and Eugenio Oliveira. 2007. ‚ÄúMore like these‚Äù: growing entity classes from seeds. In Proceedings of CIKM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between NE pairs.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Paraphrasing,</booktitle>
<contexts>
<context position="9211" citStr="Sekine, 2005" startWordPosition="1428" endWordPosition="1429">approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies the similar high-level idea of partition-then-cluster for</context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between NE pairs. In Proceedings of the International Workshop on Paraphrasing, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuming Shi</author>
<author>Huibin Zhang</author>
<author>Xiaojie Yuan</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Corpus-based Semantic Class Mining: Distributional vs. Pattern-Based Approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="8901" citStr="Shi et al., 2010" startWordPosition="1381" endWordPosition="1384"> proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy c</context>
<context position="14488" citStr="Shi et al., 2010" startWordPosition="2311" endWordPosition="2314">h We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, loca</context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong Wen. 2010. Corpus-based Semantic Class Mining: Distributional vs. Pattern-Based Approaches. In Proceedings of COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
</authors>
<title>Satoshi Sekine.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Shinyama, 2006</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive Information Extraction using Unrestricted Relation Discovery, In Proceedings of NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning Syntactic Patterns for Automatic Hypernym Discovery.</title>
<date>2005</date>
<booktitle>In Proceedings of In NIPS 17,</booktitle>
<contexts>
<context position="15355" citStr="Snow et al., 2005" startWordPosition="2453" endWordPosition="2456"> are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel &amp; Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include IP {,} (such as) {IP,}* {and|or} IP, IP (is|are|was|were|being) (a|an|the) IP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pa</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning Syntactic Patterns for Automatic Hypernym Discovery. In Proceedings of In NIPS 17, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Bhushan Mandhani</author>
</authors>
<title>Moving from Textual Relations to Ontologized Relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 AAAI Spring Symposium on Machine Reading.</booktitle>
<contexts>
<context position="8469" citStr="Soderland and Mandhani, 2007" startWordPosition="1313" endWordPosition="1316">tion instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizin</context>
</contexts>
<marker>Soderland, Mandhani, 2007</marker>
<rawString>Stephen Soderland and Bhushan Mandhani. 2007. Moving from Textual Relations to Ontologized Relations. In Proceedings of the 2007 AAAI Spring Symposium on Machine Reading.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="15378" citStr="Talukdar et al., 2008" startWordPosition="2457" endWordPosition="2460">he edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel &amp; Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include IP {,} (such as) {IP,}* {and|or} IP, IP (is|are|was|were|being) (a|an|the) IP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat and Fernando Pereira. 2008. Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Oscar Kipersztok</author>
<author>Daphne Koller</author>
</authors>
<title>An Active Learning Approach to Finding Related Terms.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Vickrey, Kipersztok, Koller, 2010</marker>
<rawString>David Vickrey, Oscar Kipersztok and Daphne Koller. 2010. An Active Learning Approach to Finding Related Terms. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishnu Vyas</author>
<author>Patrick Pantel</author>
</authors>
<title>SemiAutomatic Entity Set Refinement.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL/HLT</booktitle>
<marker>Vyas, Pantel, 2009</marker>
<rawString>Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic Entity Set Refinement. In Proceedings of NAACL/HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishnu Vyas</author>
</authors>
<title>Patrick Pantel and Eric Crestan.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM</booktitle>
<marker>Vyas, 2009</marker>
<rawString>Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009, Helping Editors Choose Better Seed Sets for Entity Set Expansion, In Proceedings of CIKM 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Language- Independent Set Expansion of Named Entities Using the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of ICDM</booktitle>
<contexts>
<context position="8850" citStr="Wang and Cohen 2007" startWordPosition="1370" endWordPosition="1373">s. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To effi</context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>Richard C. Wang and William W. Cohen. 2007. Language- Independent Set Expansion of Named Entities Using the Web. In Proceedings of ICDM 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Automatic Set Instance Extraction using the Web. In</title>
<date>2009</date>
<booktitle>Proceedings of ACL-IJCNLP</booktitle>
<marker>Wang, Cohen, 2009</marker>
<rawString>Richard C. Wang and William W. Cohen. 2009. Automatic Set Instance Extraction using the Web. In Proceedings of ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Romaric Besan√ßon</author>
<author>Olivier Ferret</author>
</authors>
<title>Filtering and Clustering Relations for Unsupervised Information Extraction in Open Domain.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM</booktitle>
<contexts>
<context position="7981" citStr="Wang et al. 2011" startWordPosition="1238" endWordPosition="1241">rase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. Resolver has the same sparseness problem but it is not affected as much as SNE because of its different goal (synonym resolution). As the preprocessing instance-detection step for the problem studied in this paper, open IE extracts relation instances (in the form of triples) from the open domain (Etzioni et al., 2004; Banko et al., 2007; Fader et al., 2011; Wang et al. 2011). For efficiency, they only use shallow features. Reverb (Fader et al., 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Taking their output as input, algorithms have been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks 1028 (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically </context>
</contexts>
<marker>Wang, Besan√ßon, Ferret, 2011</marker>
<rawString>Wei Wang, Romaric Besan√ßon and Olivier Ferret. 2011. Filtering and Clustering Relations for Unsupervised Information Extraction in Open Domain. In Proceedings of CIKM 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Ming Zhou</author>
</authors>
<title>Synonymous collocation extraction using translation information.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions: Integrating Processing</booktitle>
<contexts>
<context position="9196" citStr="Wu and Zhou, 2003" startWordPosition="1424" endWordPosition="1427">y with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure. It has been applied to reference matching. The second phase of WEBRE applies the similar high-level idea of partition-t</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>Hua Wu and Ming Zhou. 2003. Synonymous collocation extraction using translation information. In Proceedings of the ACL Workshop on Multiword Expressions: Integrating Processing 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured Relation Discovery Using Generative Models.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="6460" citStr="Yao et al. 2011" startWordPosition="1002" endWordPosition="1005">lation phrases can, however, act as a whole as the phrase cluster for 2 different relations in SNE. However, this only accounts for 4.8% of the polysemous cases. 2 Related Work Unsupervised relation extraction (URE) algorithms (Hasegawa et al., 2004; Chen et al., 2005; Shinyama and Sekine, 2006) collect pairs of co-occurring entities as relation instances, extract features for instances and then apply unsupervised clustering techniques to find the major relations of a corpus. These UREs rely on tagging a predefined set of argument types, such as Person, Organization, and Location, in advance. Yao et al. 2011 learns finegrained argument classes with generative models, but they share the similar requirement of tagging coarse-grained argument types. Most UREs use a quadratic clustering algorithm such as Hierarchical Agglomerate Clustering (Hasegawa et al., 2004, Shinyama and Sekine, 2006), K-Means (Chen et al., 2005), or both (Rosenfeld and Feldman, 2007); thus they are not scalable to very large corpora. As the target domain shifts to the web, new methods are proposed without requiring predefined entity types. Resolver (Yates and Etzioni, 2007) resolves objects and relation synonyms. Kok and Doming</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew McCallum. 2011. Structured Relation Discovery Using Generative Models. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised Resolution of Objects and Relations on the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="7005" citStr="Yates and Etzioni, 2007" startWordPosition="1083" endWordPosition="1086"> types, such as Person, Organization, and Location, in advance. Yao et al. 2011 learns finegrained argument classes with generative models, but they share the similar requirement of tagging coarse-grained argument types. Most UREs use a quadratic clustering algorithm such as Hierarchical Agglomerate Clustering (Hasegawa et al., 2004, Shinyama and Sekine, 2006), K-Means (Chen et al., 2005), or both (Rosenfeld and Feldman, 2007); thus they are not scalable to very large corpora. As the target domain shifts to the web, new methods are proposed without requiring predefined entity types. Resolver (Yates and Etzioni, 2007) resolves objects and relation synonyms. Kok and Domingos (2008) proposed Semantic Network Extractor (SNE) to extract concepts and relations. Based on second-order Markov logic, SNE used a bottom-up agglomerative clustering algorithm to jointly cluster relation phrases and argument entities. However, both Resolver and SNE require each entity and relation phrase to belong to exactly one cluster. This limits their ability to handle polysemous relation phrases. Moreover, SNE only uses features in the input set of relation instances for clustering, thus it fails to group many relevant instances. R</context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>Alexander Yates and Oren Etzioni. 2007. Unsupervised Resolution of Objects and Relations on the Web. In Proceedings of HLT-NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Zhang</author>
<author>Shuming Shi</author>
<author>Jing Liu</author>
<author>Shuqi Sun</author>
<author>ChinYew Lin</author>
</authors>
<title>Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="15399" citStr="Zhang et al., 2011" startWordPosition="2461" endWordPosition="2464">e entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel &amp; Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include IP {,} (such as) {IP,}* {and|or} IP, IP (is|are|was|were|being) (a|an|the) IP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph for relation phrases</context>
</contexts>
<marker>Zhang, Shi, Liu, Sun, Lin, 2011</marker>
<rawString>Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, ChinYew Lin. 2011. Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huibin Zhang</author>
<author>Mingjie Zhu</author>
<author>Shuming Shi</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Employing Topic Models for Patternbased Semantic Class Discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="14469" citStr="Zhang et al., 2009" startWordPosition="2307" endWordPosition="2310">tity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms</context>
</contexts>
<marker>Zhang, Zhu, Shi, Wen, 2009</marker>
<rawString>Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong Wen. 2009. Employing Topic Models for Patternbased Semantic Class Discovery. In Proceedings of ACL 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
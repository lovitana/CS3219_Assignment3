<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.554835">
Word Salad: Relating Food Prices and Descriptions
</title>
<author confidence="0.993521">
Victor Chahuneau Kevin Gimpel
</author>
<affiliation confidence="0.877408333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998882">
{vchahune,kgimpel}@cs.cmu.edu
</email>
<author confidence="0.933386">
Bryan R. Routledge
</author>
<affiliation confidence="0.848545">
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.995531">
routledge@cmu.edu
</email>
<author confidence="0.725815">
Lily Scherlis
</author>
<affiliation confidence="0.460066">
Phillips Academy
</affiliation>
<address confidence="0.706844">
Andover, MA 01810, USA
</address>
<email confidence="0.99216">
lily.scherlis@gmail.com
</email>
<author confidence="0.993386">
Noah A. Smith
</author>
<affiliation confidence="0.939676">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.995661">
nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.996617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999760666666667">
We investigate the use of language in food
writing, specifically on restaurant menus and
in customer reviews. Our approach is to build
predictive models of concrete external vari-
ables, such as restaurant menu prices. We
make use of a dataset of menus and customer
reviews for thousands of restaurants in several
U.S. cities. By focusing on prediction tasks
and doing our analysis at scale, our method-
ology allows quantitative, objective measure-
ments of the words and phrases used to de-
scribe food in restaurants. We also explore
interactions in language use between menu
prices and sentiment as expressed in user re-
views.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99690265">
What words might a menu writer use to justify the
high price of a steak? How does describing an item
as chargrilled vs. charbroiled affect its price? When
a customer writes an unfavorable review of a restau-
rant, how is her word choice affected by the restau-
rant’s prices? In this paper, we explore questions
like these that relate restaurant menus, prices, and
customer sentiment. Our goal is to understand how
language is used in the food domain, and we di-
rect our investigation using external variables such
as restaurant menu prices.
We build on a thread of NLP research that seeks
linguistic understanding by predicting real-world
quantities from text data. Recent examples include
prediction of stock volatility (Kogan et al., 2009)
and movie revenues (Joshi et al., 2010). There, pre-
diction tasks were used for quantitative evaluation
and objective model comparison, while analysis of
learned models gave insight about the social process
behind the data.
We echo this pattern here as we turn our atten-
tion to language use on restaurant menus and in user
restaurant reviews. We use data from a large cor-
pus of restaurant menus and reviews crawled from
the web and formulate several prediction tasks. In
addition to predicting menu prices, we also consider
predicting sentiment along with price.
The relationship between language and senti-
ment is an active area of investigation (Pang and
Lee, 2008). Much of this research has focused on
customer-written reviews of goods and services, and
perspectives have been gained on how sentiment is
expressed in this type of informal text. In addition
to sentiment, however, other variables are reflected
in a reviewer’s choice of words, such as the price of
the item under consideration. In this paper, we take
a step toward joint modeling of multiple variables
in review text, exploring connections between price
and sentiment in restaurant reviews.
Hence this paper contributes an exploratory data
</bodyText>
<page confidence="0.934282">
1357
</page>
<note confidence="0.771415">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1357–1367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99927875">
analysis of language used to describe food (by its
purveyors and by its consumers). While our primary
goal is to understand the language used in our cor-
pus, our findings bear relevance to economics and
hospitality research as well. This paper is a step on
the way to the eventual goal of using linguistic anal-
ysis to understand social phenomena like sales and
consumption.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978011764706">
There are several areas of related work scattered
throughout linguistics, NLP, hospitality research,
and economics.
Freedman and Jurafsky (2011) studied the use of
language in food advertising, specifically the words
on potato chip bags. They argued that, due to
the ubiquity of food writing across cultures, eth-
nic groups, and social classes, studying the use of
language for describing food can provide perspec-
tive on how different socioeconomic groups self-
identify using language and how they are linguisti-
cally targeted. In particular, they showed that price
affects how “authenticity” is realized in marketing
language, a point we return to in §5. This is an ex-
ample of how price can affect how an underlying
variable is expressed in language. Among other ex-
plorations in this paper, we consider how price inter-
acts with expression of sentiment in user reviews of
restaurants.
As mentioned above, our work is related to re-
search in predicting real-world quantities using text
data (Koppel and Shtrimberg, 2006; Ghose et al.,
2007; Lerman et al., 2008; Kogan et al., 2009; Joshi
et al., 2010; Eisenstein et al., 2010; Eisenstein et
al., 2011; Yogatama et al., 2011). Like much of
this prior work, we aim to learn how language is
used in a specific context while building models that
achieve competitive performance on a quantitative
prediction task.
Along these lines, there is recent interest in ex-
ploring the relationship between product sales and
user-generated text, particularly online product re-
views. For example, Ghose and Ipeirotis (2011)
studied the sales impact of particular properties of
review text, such as readability, the presence of
spelling errors, and the balance between subjective
and objective statements. Archak et al. (2011) had a
similar goal but decomposed user reviews into parts
describing particular aspects of the product being
reviewed (Hu and Liu, 2004). Our paper differs
from price modeling based on product reviews in
several ways. We consider a large set of weakly-
related products instead of a homogeneous selection
of a few products, and the reviews in our dataset are
not product-centered but rather describe the overall
experience of visiting a restaurant. Consequently,
menu items are not always mentioned in reviews and
rarely appear with their exact names. This makes it
difficult to directly use review features in a pricing
model for individual menu items.
Menu planning and pricing has been studied for
many years by the culinary and hospitality research
community (Kasavana and Smith, 1982; Kelly et al.,
1994), often including recommendations for writing
menu item descriptions (Miller and Pavesic, 1996;
McVety et al., 2008). Their guidelines frequently
include example menus from successful restaurants,
but typically do not use large corpora of menus or
automated analysis, as we do here. Other work
focused more specifically on particular aspects of
the language used on menus, such as the study by
Zwicky and Zwicky (1980), who made linguistic ob-
servations through manual analysis of a corpus of
200 menus.
Relatedly, Wansink et al. (2001; 2005) showed
that the way that menu items are described af-
fects customers’ perceptions and purchasing behav-
ior. When menu items are described evocatively,
customers choose them more often and report higher
satisfaction with quality and value, as compared to
when they are given the same items described with
conventional names. Wansink et al. did not use a
corpus, but rather conducted a small-scale experi-
ment in a working cafeteria with customers and col-
lected surveys to analyze consumer reaction. While
our goals are related, our experimental approach is
different, as we use automated analysis of thousands
of restaurant menus and rely on a set of one mil-
lion reviews as a surrogate for observing customer
behavior.
Finally, the connection between products and
prices is also a central issue in economics. How-
ever, the stunning heterogeneity in products makes
empirical work challenging. For example, there are
over 50,000 menu items in New York that include
</bodyText>
<page confidence="0.985507">
1358
</page>
<table confidence="0.9991045">
City # Restaurants # Menu Items test # Reviews test
train dev. test train dev. train dev.
Boston 930 107 113 63,422 8,426 8,409 80,309 10,976 11,511
Chicago 804 98 100 ſ  73,251 9,582 10,965
Los Angeles 624 80 68 51,480 6,633 6,939 ƀ 13,227 5,716
New York 3,965 473 499 ɑ ʩŜſ 75,455 35,529 37,795
Philadelphia 1,015 129 117 17,980 2,938 1,592 326,801 7,347 5,790
San Francisco 1,908 255 234 ɑɑɑ 52,275 59,378 67,010
Washington, D.C. 773 110 121 365,518 42,315 45,728 499,984 11,852 14,129
ɤƇƈ  Ƈ 71,179
 ɤɩɬɯ
83,818 11,777 9,295
ƈ
103,954 12,871 12,510
47,188 5,957 7,224 ɤƇƈ  Ƈ
 
Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916
Ş Ś
</table>
<tableCaption confidence="0.999598">
Table 1: Dataset statistics.
</tableCaption>
<equation confidence="0.8550795">

ʩ ʩ řŜ ř
the word chicken. What is the price of chicken? This
�ƃɩɰɮƄ
</equation>
<bodyText confidence="0.999632">
is an important practical and daunting matter when
measuring inflation (e.g., Consumer Price Index is
measured with a precisely-defined basket of goods).
Price dispersion across goods and the variation of
the goods is an important area of industrial organi-
zation economic theory. For example, economists
</bodyText>
<equation confidence="0.82057">
�ƃɩɰɮƄ
�ƃɩɰ
</equation>
<bodyText confidence="0.9829875">
are interested in models of search, add-on pricing,
and obfuscation (Baye et al., 2006; Ellison, 2005).
</bodyText>
<sectionHeader confidence="0.995135" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99316776">
We crawled Allmenus.com (www.allmenus.
com) to gather menus for restaurants in seven
U.S. cities: Boston, Chicago, Los Angeles, New
York, Philadelphia, San Francisco, and Washing-
ƃƄƃ
ton, D.C. Each menu includes a list of item names
with optional text descriptions and prices. Most All-
menus restaurant pages contain a link to the cor-
responding page on Yelp (www.yelp.com) with
metadata and user reviews for the restaurant, which
we also collected.
The metadata consist of many fields for each
restaurant, which can be divided into three cate-
gories: location (city, neighborhood, transit stop),
services available (take-out, delivery, wifi, parking,
etc.), and ambience (good for groups, noise level,
attire, etc.). Also, the category of food and a price
range ($ to $$$$, indicating the price of a typical
meal at the restaurant) are indicated. The user re-
views include a star rating on a scale of 1 to 5.
The distribution of prices of individual menu
items is highly skewed, with a mean of $9.22 but
a median of $6.95. On average, a restaurant has
73 items on its menu with a median price of $8.69
and 119 Yelp reviews with a median rating of 3.55
</bodyText>
<figure confidence="0.986760066666667">
ɤɩ
$$
ś
5k $ 
4k $$$

3k

2k
ƀ
1k
$$ $$
0
$ $
price range
</figure>
<figureCaption confidence="0.999542">
Figure 1:Frequency distributions of restaurant price
</figureCaption>
<equation confidence="0.97668375">
k 3k � ƃɩɰɮƄ �ɪ�ſ
ranges (left) and review ratings (right).
  ʰ
2k 2k ɥ
</equation>
<bodyText confidence="0.908594">
stars. The star rating and price range distributions
</bodyText>
<equation confidence="0.84478">
 � ʰ  ʫ ſʫƀ Ƌ 
1k 1k ɪ
are shown in Figure 1.
</equation>
<bodyText confidence="0.967385">
The set ofrestaurants was randomly split into
</bodyText>
<figure confidence="0.493862428571429">
price range
price range ɑɑř ɑɑƀ
three parts (80% for training, 10% for development,
Ŝſř �ƀ
ſɑ ɩɥƀ
10% for evaluation), independently for each city.
  ʰ ɪſƀ
</figure>
<bodyText confidence="0.8231826">
The sizes of the splits and the full set of dataset
Ŝſƃɥř ɪŜƀƄƀ
Ŝ
statistics are provided in Table 1.

</bodyText>
<sectionHeader confidence="0.98104" genericHeader="method">
4 Predictive Tasks
</sectionHeader>
<bodyText confidence="0.99737">
We consider several prediction tasks using the
</bodyText>
<equation confidence="0.635567">
Ŝſ ſ ƀ
Ŝ
</equation>
<bodyText confidence="0.8341307">
dataset just described. These include predicting
ſɑ ƀŚ
individual menu item prices (§5), predicting the
Ŝ�ſɑŜɑƀ
price range for each restaurant (§6), and finally
ŜſŜƀƀ
ſƀſɑɑƀ
jointly predicting median price and sentiment for
ſɑɑř ɑɑƀ
each restaurant (§7). To do this, we use two types
</bodyText>
<subsectionHeader confidence="0.4154405">
Ŝſř Şɨɥƀ
Ŝſ  ƀ
</subsectionHeader>
<bodyText confidence="0.722723">
of models: linear regression (§5 and §6) and logis-
ſɑɑ ɑɑ
tic regression (§7), both with �1 regularization when
</bodyText>
<equation confidence="0.787115266666667">
ſſƀ Ƈ 
sparsity is desirable. We tune the regularization co-
Ŝ�ſŜƀ
ſƀ
efficient by choosing the value that minimizes de-
ƀſɑɑƀ
velopment set loss (mean squared error and log loss,
Ŝſř ƀ
 ſ
respectively).
For evaluation, we use mean absolute error
ŜſɑŞ ɑ
(MAE) and mean relative error (MRE). Given a
Ŝſſ ƀ Ƈ 
ſ
</equation>
<bodyText confidence="0.92583">
dataset (xi7 yi)Ni_1 with inputs xi and outputs yi, and
</bodyText>
<figure confidence="0.958462">
Ŝſɑɨɑ ʫɥɬƀſɑ
500k
400k
300k
200k
100k
0
star rating
</figure>
<page confidence="0.987795">
1359
</page>
<bodyText confidence="0.9966625">
denoting predicted outputs by yi, these are defined
as follows:
</bodyText>
<equation confidence="0.9985084">
1
MAE =
N
1
MRE = N
</equation>
<bodyText confidence="0.956997111111111">
In practice, since we model log-prices but eval-
uate on real prices, the final prediction is often a
non-linear transformation of the output of the linear
classifier of weight vector w, which we denote by:
yi = f(w&apos;xi).
We also frequently report the total number of fea-
tures available in the training set for each model (nf)
as well as the number of non-zero feature weights
following learning (nnz).
</bodyText>
<sectionHeader confidence="0.976322" genericHeader="method">
5 Menu Item Price Prediction
</sectionHeader>
<bodyText confidence="0.99868848">
We first consider the problem of predicting the price
of each item on a menu. In this case, every in-
stance xi corresponds to a single item in the menu
parametrized by the features detailed below and yi is
the item’s price. In this section, our models always
use the logarithm of the price1 as output values and
therefore: Pi = e&amp;quot;&amp;quot;&apos;Txz.
Baselines We evaluate several baselines which
make independent predictions for each distinct item
name. The first two predict the mean or the me-
dian of the prices in the training set for a given
item name, and use the overall price mean or me-
dian when a name is missing in the training set. The
third baseline is an `1-regularized linear regression
model trained with a single binary feature for each
item name in the training data. These baselines are
shown as the first three rows in Table 2.
We note that there is a wide variation of menu
item names in the dataset, with more than 400,000
distinct names. Although we address this issue later
by introducing local text features, we also performed
simple normalization of the item names for all of
the baselines described above. To do this normal-
ization, we first compiled a stop word list based on
the most frequent words in the item names.2 We
</bodyText>
<footnote confidence="0.99569">
1The price distribution is more symmetric in the log domain.
2This list can be found in the supplementary material.
</footnote>
<bodyText confidence="0.9996092">
removed stop words and then ordered the words in
each item name lexicographically, in order to col-
lapse together items such as coffee black and black
coffee. This normalization reduced the unique item
name count by 40%, strengthening the baselines.
</bodyText>
<subsectionHeader confidence="0.715074">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.955420054054054">
We use `1-regularized linear regression for feature-
rich models. We now introduce several sets of fea-
tures that we add to the normalized item names:3
I. METADATA: Binary features for each restaurant
metadata field mentioned above, excluding price
range. A separate binary feature is included for
each unique (field, value) tuple.
II. MENUNAMES: n-grams in menu item names.
We used binary features for unique unigrams, bi-
grams, and trigrams. Here, stop words were re-
tained as they can be informative (e.g., with and
large correlate with price).
III. MENUDESC: n-grams in menu item descrip-
tions, as in MENUNAMES.
Review Features In addition to these features, we
consider leveraging the large amount of text present
in user reviews to improve predictions. We at-
tempted to find mentions of menu items in the re-
views and to include features extracted from the sur-
rounding words in the model. Perfect item mentions
being relatively rare, we consider inexact matches
weighted by a coefficient measuring the degree of
resemblance: we used the Dice similarity between
the set of words in the sentence and in the item name.
We then extracted n-gram features from this sen-
tence, and tried several ways to use them for price
prediction.
Given a review sentence, one option is to add the
corresponding features to every item matching this
sentence, with a value equal to the similarity coeffi-
cient. Another option is to select the best matching
item and use the same real-valued features but only
for this single item. Binary feature values can be
used instead of the real-valued similarity coefficient.
We also experimented with the use of part-of-speech
tags in order to restrict our features to adjective and
adverb n-grams instead of the full vocabulary. All of
</bodyText>
<footnote confidence="0.95312">
3The normalized item names are present as binary features
in all of our regression models
</footnote>
<equation confidence="0.996214555555556">
N
i=1
|yi − yi|
i
yi − 9i
yi
i
N
i=1
</equation>
<page confidence="0.906481">
1360
</page>
<table confidence="0.999895272727273">
MAE MRE nf nnz
Predict mean 3.70 43.32 n/a n/a
Predict median 3.67 43.93 n/a n/a
Regression 3.66 45.64 267,945 240,139
METADATA 3.55 43.11 268,450 258,828
PR 3.47 43.11 267,946 205,176
MENUNAMES 3.23 38.33 896,631 230,840
+ MENUDESC 3.19 36.23 1,981,787 151,785
+ PR 3.08 34.51 1,981,788 140,954
+ METADATA 3.08 34.97 1,982,363 148,774
+ MENTIONS 3.06 34.37 4,959,488 458,462
</table>
<tableCaption confidence="0.80891375">
Table 2: Results for menu item price prediction. MAE =
mean absolute error ($), MRE = mean relative error (%),
nf = total number of features, nnz = number of features
with non-zero weight.
</tableCaption>
<bodyText confidence="0.999902352941177">
these attempts yielded negative or only slightly pos-
itive results, of which we include only one example
in our experiments: the MENTIONS feature set con-
sists of n-grams for the best matching item with the
Dice coefficient as the feature value.
We also tried to incorporate the reviews by using
them in aggregate via predictions from a separate
model; we found this approach to work better than
the methods described above which all use features
from the reviews directly in the regression model. In
particular, we use the review features in a separate
model that we will describe below (§6) to predict
the price range of each restaurant. The model uses
unigrams, bigrams, and trigrams extracted from the
reviews. We use the estimated price range (which we
denote PR) as a single additional real-valued feature
for individual item price prediction.
</bodyText>
<subsectionHeader confidence="0.831859">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999960214285714">
Our results are shown in Table 2. We achieve a fi-
nal reduction of 50 cents in MAE and nearly 10%
in MRE compared with the baselines. Using menu
name features (MENUNAMES) brings the bulk of
the improvement, though menu description features
(MENUDESC) and the remaining features also lead
to small gains. Interestingly, as the MENUDESC and
PR features are added to the model, the regulariza-
tion favors more general features by selecting fewer
and fewer non-zero weights.
While METADATA features improve over the
baselines when used alone, they do not lead to im-
proved performance over the MENU* + PR features,
suggesting that the text features may be able to sub-
stitute for the information in the metadata, at least
for prediction of individual item prices.
The MENTIONS features resulted in a small im-
provement in MAE and MRE, but at the cost of ex-
panding the model size significantly. A look at the
learned feature weights reveals that most of the se-
lected features seem more coincidental than generic
(rachel’s, highly negative) when not totally unintu-
itive (those suicide, highest positive). This suggests
that our method of extracting features from men-
tions is being hampered by noise. We suspect that
these features could be more effective with a better
method of linking menu items to mentions in review
text.
</bodyText>
<subsectionHeader confidence="0.996931">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9995458125">
We also inspected the feature weights of our learned
models. By comparing the weights of related fea-
tures, we can see the relative differences in terms
of contribution to menu item prices. Table 3 shows
example feature weights, manually arranged into
several categories (taken from the model with ME-
NUNAMES + MENUDESC + PR + METADATA).
Table 3(a) shows selected features for the “am-
bience” field in the Yelp restaurant metadata and
pane (b) lists some unigrams related to cooking
methods. Pane (c) shows feature weights for n-
grams often used to market menu items; we see
larger weights for words targeting those who want to
eat organically- or locally-grown food (farmhouse,
heirloom, wild, and hormone), compared to those
looking for comfort food (old time favorite, tradi-
tional, real, and fashioned). This is related to ob-
servations made by Freedman and Jurafsky (2011)
that cheaper food is marketed by appealing to tra-
dition and historicity, with more expensive food de-
scribed in terms of naturalness, quality of ingredi-
ents, and the preparation process (e.g., handpicked,
wild caught, etc.). Relatedly, in pane (e) we see
that real mashed potatoes are expected to be cheaper
than those described as creamy or smooth.
Pane (d) shows feature weights for trigrams con-
taining units of chicken; we can see an ordering in
terms of size (bits &lt; cubes &lt; strips &lt; cuts) as well
as the price increase associated with the use of the
word morsels in place of less refined units. We also
see a difference between pieces and pcs, with the
latter being frequently used to refer to entire cuts of
</bodyText>
<page confidence="0.97689">
1361
</page>
<table confidence="0.845385816901409">
(a) METADATA: (c) MENUDESC:
ambience descriptors
dive-y -0.015 old time favorite -0.112
intimate -0.013 fashioned -0.034
trendy -0.012 line caught -0.028
casual -0.005 all natural -0.028
romantic -0.004 traditional -0.009
classy -7e-6 natural 3e-4
touristy 0.058 classic 0.002
upscale 0.099 free range 0.004
real 0.004
fresh 0.006
homemade 0.010
authentic 0.012
organic 0.020
specialty 0.025
special 0.033
locally 0.037
natural grass fed 0.038
artisanal 0.064
raised 0.066
heirloom 0.083
wild 0.084
hormone 0.085
farmed 0.099
hand picked 0.101
wild caught 0.116
farmhouse 0.133
(b) MENUDESC:
cooking
panfried -0.094
chargrilled -0.029
cooked -0.012
boiled -0.006
fried -0.005
steamed 0.011
charbroiled 0.015
grilled 0.022
simmered 0.025
roasted 0.034
sauteed 0.034
broiled 0.053
seared 0.066
braised 0.068
stirfried 0.071
flamebroiled 0.106
(d) MENUDESC: (e) MENUDESC:
= “of chicken” = “potatoes”
slices -0.102 real mashed -0.028
bits -0.032 mashed -0.005
cubes -0.030 creamy mashed -5e-9
pieces -0.024 smashed 0.018
strips -0.001 smooth mashed 0.129
chunks 0.015
morsels 0.025
pcs 0.040
cuts 0.042
(f) MENUDESC:
= “potato”
mash -0.022
mashed -0.019
(g) MENUDESC: “crisp” vs. “crispy”
crisp -0.022 crispy bacon 0.008
crispy -0.011 crisp bacon 0.033
(h) MENUDESC: “roast” vs. “roasted”
roasted 0.034 roasted potatoes 0.026
roast 0.040 roast potatoes 0.110
roasted chicken -0.041 roasted salmon 0.091
roast chicken -0.012 roast salmon 0.151
roast pork -0.038 roasted tomato 0.010
roasted pork 0.055 roast tomato 0.026
</table>
<tableCaption confidence="0.988015">
Table 3: Selected features from model for menu item
price prediction. See text for details.
</tableCaption>
<bodyText confidence="0.9999274375">
chicken (e.g., wings, thighs, etc.) and the former
more often used as a synonym for chunks.
Panes (f), (g), and (h) reveal price differences due
to slight variations in word form. We find that, even
though crispy has a higher weight than crisp, crisp
bacon is more expensive than crispy bacon. We also
find that food items prefixed with roast lead to more
expensive prices than the similar roasted, except in
the case of pork, though here the different forms may
be evoking two different preparation styles.
Also of note is the slight difference between
the nonstandard mash potato and mashed potato.
We observed lower weights with other nonstandard
spellings, notably portobella having lower weight
than each of the more common spellings portabella,
portobello, and portabello.
</bodyText>
<sectionHeader confidence="0.9927" genericHeader="method">
6 Restaurant Price Range Prediction
</sectionHeader>
<bodyText confidence="0.999729464285715">
In addition to predicting the prices of individual
menu items, we also considered the task of predict-
ing the price range listed for each restaurant on its
Yelp page. The values for this field are integers from
1 to 4 and indicate the price of a typical meal from
the restaurant.
For this task, we again train an `1-regularized
linear regression model with integral price ranges
as the true output values yi. Each input xi corre-
sponds to the feature vector for an entire restaurant.
For evaluation, we round the predicted values to the
nearest integer: yi = ROUND(wTxi) and report the
corresponding mean absolute error and accuracy.
We compared this simple approach with an or-
dinal regression model (McCullagh, 1980) trained
with the same il regularizer and noted very little im-
provement (77.32% vs. 77.15% accuracy for META-
DATA). Therefore, we only report in this section re-
sults for the linear regression model.
In addition to the feature sets used for individ-
ual menu item price prediction, we used features on
reviews (REVIEWS). Specifically, we used binary
features for unigrams, bigrams, and trigrams in the
full set of reviews for each restaurant. A stopword
list was derived from the training data.4 Bigrams
and trigrams were filtered if they ended with stop-
words. Additionally, features occurring fewer than
three times in the training set were discarded.
</bodyText>
<footnote confidence="0.992237">
4This list is included in the supplementary material.
</footnote>
<page confidence="0.971821">
1362
</page>
<table confidence="0.999254833333333">
Features MAE Acc. nf nnz
Predict mode 0.5421 48.22 n/a n/a
MENU* 0.3875 66.29 1,910,622 995
METADATA 0.2372 77.15 591 219
REVIEWS 0.2172 79.76 3,027,470 1,567
+METADATA 0.2111 80.36 3,027,943 1,376
</table>
<tableCaption confidence="0.99340575">
Table 4: Results for restaurant price range prediction.
MAE = mean absolute error, Acc = classification accu-
racy (%), nf = total number of features, nnz = number of
features with non-zero weight.
</tableCaption>
<subsectionHeader confidence="0.877748">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.999997071428571">
Our results for price range prediction are shown
in Table 4. Predicting the most frequent price
range gave us an accuracy of 48.22%. Performance
improvements were obtained by separately adding
menu (MENU*), metadata (METADATA), and re-
view features (REVIEWS). Unlike individual item
price prediction, the reviews were more helpful than
the menu features for predicting overall price range.
This is not surprising, since reviewers will often gen-
erally discuss price in their reviews. We combined
metadata and review features to get our best accu-
racy, exceeding 80%.
We also wanted to perform an analysis of senti-
ment in the review text. To do this, we trained a lo-
gistic regression model predicting polarity for each
review; we used the REVIEWS feature set, but this
time considering each review as a single training in-
stance. The polarity of a review was determined by
whether or not its star rating was greater than the
average rating across all reviews in the dataset (3.7
stars). We achieved an accuracy of 87% on the test
data. We omit full details of these models because
the polarity prediction task for user reviews is well-
known in the sentiment analysis community and our
model is not an innovation over prior work (Pang
and Lee, 2008). However, our purpose in training
the model was to use the learned weights for under-
standing the text in the reviews.
</bodyText>
<subsectionHeader confidence="0.999502">
6.2 Interpreting Reviews
</subsectionHeader>
<bodyText confidence="0.999885341463415">
Given learned models for predicting a restaurant’s
price range from its set of reviews as well as polar-
ity for each review, we can turn the process around
and use the feature weights to analyze the review
text. Restricting our attention to reviews of 50–60
words, Table 5 shows sample reviews from our test
set that lead to various predictions of price range and
sentiment.5
This technique can also be useful when trying to
determine the “true” star rating for a review (if pro-
vided star ratings are noisy), or to show the most
positive and most negative reviews for a product
within a particular star rating. The 5-point scale
is merely a coarse approximation to the reviewer’s
mental state; using fitted models can provide addi-
tional clues to decode the reviewer’s sentiment.
We can also do a more fine-grained analysis of
review text by noting the contribution to the price
range prediction of each position in the text stream.
This is straightforward because our features are sim-
ply n-grams of the review text. In Figure 2, we show
the influence of each word in a review sentence on
the predicted polarity (brown) and price range (yel-
low). The height of a bar at a given position is pro-
portional to the sum of the feature weights for every
unigram, bigram, and trigram containing the token
at that position (there are at most 6 active n-grams
at a position).
The first example shows the smooth shift in ex-
pressed sentiment from the beginning of the sen-
tence to the end. The second sentence is a difficult
example for sentiment analysis, since there are sev-
eral positive words and phrases early but the senti-
ment is chiefly expressed in the final clause. Our
model noted the steady positive sentiment early in
the sentence but identified the crucial negation due
to strong negative weight on bigrams fresh but, left
me, and me yearning. In both examples, the yellow
bars show that price estimates are reflected mainly
through isolated mentions of offerings and ameni-
ties (drinks, atmosphere, security, good service).
</bodyText>
<sectionHeader confidence="0.984625" genericHeader="method">
7 Joint Prediction of Price and Sentiment
</sectionHeader>
<bodyText confidence="0.948028">
Although we observe no interesting correlation (r =
0.06) between median star rating and median item
price in our dataset, this does not imply that senti-
5To choose the 9 reviews in the table, we took the reviews
from our test set in the desired length range and computed pre-
dicted sentiment and price range for each; then we scaled the
predicted price range so that its range matched that of predicted
sentiment, and maximized various linear combinations of the
two. This accounts for the four corners. The others were found
by maximizing a linear combination of one (possibly negated)
prediction minus the absolute value of the other.
</bodyText>
<page confidence="0.867813">
1363
</page>
<figure confidence="0.918727">
+— cheap expensive →
T
®
e
l
</figure>
<bodyText confidence="0.997793105263158">
i love me a cheap vietnamese sandwich . this place is tiny ! the pork buns are so tender amazing service and desserts . nice wine
mmm , pate. this place has the best ones i and flavorful . i dream about these things . list and urban decor . i went with a girl-
’ve had in the city , and i conveniently live manila clams were awesome, not the biggest friend and we split an entree, appetizer and
a few blocks away . the ladies behind the clam fan either, but i loved it . mmm 7 spice dessert and they happily brought us separate
counter are always courteous and fast, and chips . i ca n’t wait to go back ! portions which were just the right size . the
who can beat a $ 3 sandwich ?! crazy ass bread is awesome , too . definitely a bit of a
deli . splurge, but worth it in moderation.
great place to get fast food that tastes good. had some solid thai here for lunch last week weekday evening was quiet , not every ta-
paneer and chicken are both good. i would . ordered the special of the day , a chicken ble was filled. our waiter was amicable and
prefer to go thursday thru saturday night . curry . quick service and nice interior. only friendly , which is always a plus . the co-
thats when they have their good shift work- issue was , had a bit of a stomach ache after- conut bread pudding was ok and very sweet
ing . also it stays open late until 4 am on wards ? prefer their sister restaurant, citizen . it ’s definitely a dessert plate that can be
weekends. really enjoyable! thai and the monkey, in north beach. shared with a glass of wine.
for some reason my friend wanted me to ugh ! the salt ! all 5 dishes we ordered were downhill alert ... had a decent lunch at
go here with him. it was a decent standard so unbearably salty , i ’d rather just have the dragon well this week marred by pretty
greasy slice of pizza. it was n’t bad by any msg . greasy , oily , salty - there is much spotty service. our waiter just did n’t have
means , but it was nothing special at all . better chinese food to be had in sf than here. it together, forgetting to bring bowls for our
on the plus side , cheap and fast . so in i was very disappointed and wo n’t be back. split soup , our beverages , etc. . food was
summary : cheap, fast, greasy, average. good but pretty pricey for what we got .
</bodyText>
<tableCaption confidence="0.994634">
Table 5: Reviews from the test set deemed by our model to have particular values of sentiment and price.
</tableCaption>
<figure confidence="0.416359">
s
this place gets 5 stars for food , drinks , and atmosphere ... and negative stars for the jar-headed douche bag `` security guards &apos;&apos;
and an negativ star fo th jarheaded douche g guards
</figure>
<figureCaption confidence="0.992212">
Figure 2: Local (position-level) sentiment (brown) and price (yellow) estimates for two sentences in the test corpus.
</figureCaption>
<figure confidence="0.508026333333333">
g
great dark , sexy atmosphere . good service . nice variety of tasty cocktails . sushi tasted good , food was fresh but nothing left me yearning to return
variety o s cocktail tasted e bt yearning to retu
</figure>
<bodyText confidence="0.917290157894737">
ment and price are independent of each other.6 We
try to capture this interaction by modeling at the
same time review polarity and item price: we con-
sider the task of jointly predicting aggregate senti-
ment and price for a restaurant.
For every restaurant in our dataset, we compute
its median item price p and its median star rating
r. The average of these two values for the entire
dataset ($8.69 and 3.55 stars) split the plane (p, r)
in four sections: we assign each restaurant to one of
these quadrants which we denote 1 e, 1 ®, T e and
T ®. This allows us to train a 4-class logistic regres-
6Price and sentiment are both endogenous outcomes reflect-
ing the characteristics of the restaurant. E.g., “better” restau-
rants can charge higher prices.
sion model using the REVIEWS feature set for each
restaurant. We achieve an accuracy of 65% on the
test data, but we are mainly interested in interpret-
ing the estimated feature weights.
</bodyText>
<subsectionHeader confidence="0.9906">
7.1 Analysis
</subsectionHeader>
<bodyText confidence="0.9999424">
To visualize the top feature weights learned by the
model, we have to map the four weight vectors
learned by the model back to the underlying two-
dimensional sentiment/price space. Therefore, we
compute the following values:
</bodyText>
<equation confidence="0.9884755">
wg = (wT® + wTe) − (wl® + wle)
w* = (wT® + wl®) − (wTe + wle)
</equation>
<page confidence="0.952756">
1364
</page>
<bodyText confidence="0.999990666666667">
We then select for display the features which are the
furthest from the origin (max w2 + w*) and rep-
resent the selected n-grams as points in the senti-
ment/price space to obtain Figure 3.
We notice that the spread of the sentiment values
is larger, which suggests that reviews give stronger
clues about consumer experience than about the cost
of a typical meal. However, obvious price-related
adjectives (inexpensive vs. expensive) appear in this
limited selection, as well as certain phrases indicat-
ing both sentiment and price (overpriced vs. very
reasonable). Other examples of note: gem is used in
strongly-positive reviews of cheap restaurants; for
expensive restaurants, reviewers use highly recom-
mended or amazing. Also, phrases like no flavor and
manager appear in negative reviews of more expen-
sive restaurants, while dirty appears more often in
negative reviews of cheaper restaurants.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999944636363636">
We have explored linguistic relationships between
food prices and customer sentiment through quan-
titative analysis of a large corpus of menus and re-
views. We have also proposed visualization tech-
niques to better understand what our models have
learned and to see how they can be applied to new
data. More broadly, this paper is an example of us-
ing extrinsic variables to drive model-building for
linguistic data, and future work might explore richer
extrinsic variables toward a goal of task-driven no-
tions of semantics.
</bodyText>
<sectionHeader confidence="0.998517" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.929697285714286">
We thank Julie Baron, Ric Crabbe, David Garvett, Laura
Gimpel, Chenxi Jiao, Elaine Lee, members of the ARK
research group, and the anonymous reviewers for helpful
comments that improved this paper. This research was
supported in part by the NSF through CAREER grant IIS-
1054319 and Sandia National Laboratories (fellowship to
K. Gimpel).
</bodyText>
<sectionHeader confidence="0.995194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665551724138">
N. Archak, A. Ghose, and P. G. Ipeirotis. 2011. Deriv-
ing the pricing power of product features by mining
consumer reviews. Management Science, 57(8).
M. R. Baye, J. Morgan, and P. Scholten. 2006. Eco-
nomics and information systems; handbooks in infor-
mation systems. In T. Hendershott, editor, Judgement
under Uncertainty: Heuristics and Biases. Elsevier,
Amsterdam.
J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing.
2010. A latent variable model for geographic lexical
variation. In Proc. of EMNLP.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
G. Ellison. 2005. A model of add-on pricing. Quarterly
Journal of Economics, 120(2):585–637, May.
J. Freedman and D. Jurafsky. 2011. Authenticity in
America: Class distinctions in potato chip advertising.
Gastronomica, 11(4):46–54.
A. Ghose and P. G. Ipeirotis. 2011. Estimating the help-
fulness and economic impact of product reviews: Min-
ing text and reviewer characteristics. IEEE Transac-
tions on Knowledge and Data Engineering, 23(10).
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proc. of AAAI.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proc. of NAACL.
M. L. Kasavana and D. I. Smith. 1982. Menu Engineer-
ing: A Practical Guide to Menu Analysis. Hospitality
Publications.
T. J. Kelly, N. M. Kiefer, and K. Burdett. 1994. A
demand-based approach to menu pricing. Cornell Ho-
tel and Restaurant Administrative Quarterly, 35(1).
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL.
M. Koppel and I. Shtrimberg. 2006. Good news or bad
news? let the market decide. Computing Attitude and
Affect in Text: Theory and Applications.
K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008.
Reading the markets: Forecasting public opinion of
political candidates by news analysis. In Proc. of
COLING.
P. McCullagh. 1980. Regression models for ordinal
data. Journal of the royal statistical society. Series B
(Methodological), pages 109–142.
P. J. McVety, B. J. Ware, and C. L. Ware. 2008. Funda-
mentals of Menu Planning. John Wiley &amp; Sons.
J. E. Miller and D. V. Pavesic. 1996. Menu: Pricing &amp;
Strategy. Hospitality, Travel, and Tourism Series. John
Wiley &amp; Sons.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
</reference>
<page confidence="0.97212">
1365
</page>
<figure confidence="0.993476666666667">
positive
cheap expensive
negative
</figure>
<figureCaption confidence="0.988006">
Figure 3: Top 50 features from joint prediction of price and sentiment. The black circle is the origin. See text for
details on how the coordinates for each feature were computed. Insets show enlargements of dense areas of the graph.
</figureCaption>
<page confidence="0.979">
1366
</page>
<reference confidence="0.988610230769231">
B. Wansink, J. E. Painter, and K. van Ittersum. 2001.
Descriptive menu labels’ effect on sales. Cornell Ho-
tel and Restaurant Administrative Quarterly, 42(6).
B. Wansink, K. van Ittersum, and J. E. Painter. 2005.
How descriptive food names bias sensory perceptions
in restaurants. Food Quality and Preference, 16(5).
D. Yogatama, M. Heilman, B. O’Connor, C. Dyer, B. R.
Routledge, and N. A. Smith. 2011. Predicting a sci-
entific community’s response to an article. In Proc. of
EMNLP.
A. D. Zwicky and A. M. Zwicky. 1980. America’s na-
tional dish: The style of restaurant menus. American
Speech, 55(2):83–92.
</reference>
<page confidence="0.993764">
1367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.390701">
<title confidence="0.999305">Word Salad: Relating Food Prices and Descriptions</title>
<author confidence="0.993095">Victor Chahuneau Kevin</author>
<affiliation confidence="0.9656965">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.994966">Pittsburgh, PA 15213,</address>
<author confidence="0.997951">Bryan R Routledge</author>
<affiliation confidence="0.9918695">Tepper School of Business Carnegie Mellon University</affiliation>
<address confidence="0.998063">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999049">routledge@cmu.edu</email>
<author confidence="0.847929">Lily</author>
<affiliation confidence="0.664549">Phillips</affiliation>
<address confidence="0.992438">Andover, MA 01810,</address>
<email confidence="0.999865">lily.scherlis@gmail.com</email>
<author confidence="0.999888">Noah A Smith</author>
<affiliation confidence="0.9991245">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.997887">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999656">nasmith@cs.cmu.edu</email>
<abstract confidence="0.98396375">We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external varisuch as restaurant menu We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to describe food in restaurants. We also explore interactions in language use between menu and expressed in user reviews.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Archak</author>
<author>A Ghose</author>
<author>P G Ipeirotis</author>
</authors>
<title>Deriving the pricing power of product features by mining consumer reviews.</title>
<date>2011</date>
<journal>Management Science,</journal>
<volume>57</volume>
<issue>8</issue>
<contexts>
<context position="5486" citStr="Archak et al. (2011)" startWordPosition="855" endWordPosition="858">et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al. (2011) had a similar goal but decomposed user reviews into parts describing particular aspects of the product being reviewed (Hu and Liu, 2004). Our paper differs from price modeling based on product reviews in several ways. We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review fe</context>
</contexts>
<marker>Archak, Ghose, Ipeirotis, 2011</marker>
<rawString>N. Archak, A. Ghose, and P. G. Ipeirotis. 2011. Deriving the pricing power of product features by mining consumer reviews. Management Science, 57(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Baye</author>
<author>J Morgan</author>
<author>P Scholten</author>
</authors>
<title>Economics and information systems; handbooks in information systems.</title>
<date>2006</date>
<booktitle>Judgement under Uncertainty: Heuristics and Biases.</booktitle>
<editor>In T. Hendershott, editor,</editor>
<publisher>Elsevier,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="9013" citStr="Baye et al., 2006" startWordPosition="1425" endWordPosition="1428">224 ɤƇƈ  Ƈ   Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916 Ş Ś Table 1: Dataset statistics.  ʩ ʩ řŜ ř the word chicken. What is the price of chicken? This �ƃɩɰɮƄ is an important practical and daunting matter when measuring inflation (e.g., Consumer Price Index is measured with a precisely-defined basket of goods). Price dispersion across goods and the variation of the goods is an important area of industrial organization economic theory. For example, economists �ƃɩɰɮƄ �ƃɩɰ are interested in models of search, add-on pricing, and obfuscation (Baye et al., 2006; Ellison, 2005). 3 Data We crawled Allmenus.com (www.allmenus. com) to gather menus for restaurants in seven U.S. cities: Boston, Chicago, Los Angeles, New York, Philadelphia, San Francisco, and WashingƃƄƃ ton, D.C. Each menu includes a list of item names with optional text descriptions and prices. Most Allmenus restaurant pages contain a link to the corresponding page on Yelp (www.yelp.com) with metadata and user reviews for the restaurant, which we also collected. The metadata consist of many fields for each restaurant, which can be divided into three categories: location (city, neighborhoo</context>
</contexts>
<marker>Baye, Morgan, Scholten, 2006</marker>
<rawString>M. R. Baye, J. Morgan, and P. Scholten. 2006. Economics and information systems; handbooks in information systems. In T. Hendershott, editor, Judgement under Uncertainty: Heuristics and Biases. Elsevier, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>B O’Connor</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. 2010. A latent variable model for geographic lexical variation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4878" citStr="Eisenstein et al., 2011" startWordPosition="762" endWordPosition="765">d. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ellison</author>
</authors>
<title>A model of add-on pricing.</title>
<date>2005</date>
<journal>Quarterly Journal of Economics,</journal>
<volume>120</volume>
<issue>2</issue>
<contexts>
<context position="9029" citStr="Ellison, 2005" startWordPosition="1429" endWordPosition="1430">Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916 Ş Ś Table 1: Dataset statistics.  ʩ ʩ řŜ ř the word chicken. What is the price of chicken? This �ƃɩɰɮƄ is an important practical and daunting matter when measuring inflation (e.g., Consumer Price Index is measured with a precisely-defined basket of goods). Price dispersion across goods and the variation of the goods is an important area of industrial organization economic theory. For example, economists �ƃɩɰɮƄ �ƃɩɰ are interested in models of search, add-on pricing, and obfuscation (Baye et al., 2006; Ellison, 2005). 3 Data We crawled Allmenus.com (www.allmenus. com) to gather menus for restaurants in seven U.S. cities: Boston, Chicago, Los Angeles, New York, Philadelphia, San Francisco, and WashingƃƄƃ ton, D.C. Each menu includes a list of item names with optional text descriptions and prices. Most Allmenus restaurant pages contain a link to the corresponding page on Yelp (www.yelp.com) with metadata and user reviews for the restaurant, which we also collected. The metadata consist of many fields for each restaurant, which can be divided into three categories: location (city, neighborhood, transit stop)</context>
</contexts>
<marker>Ellison, 2005</marker>
<rawString>G. Ellison. 2005. A model of add-on pricing. Quarterly Journal of Economics, 120(2):585–637, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Freedman</author>
<author>D Jurafsky</author>
</authors>
<title>Authenticity in America: Class distinctions in potato chip advertising.</title>
<date>2011</date>
<journal>Gastronomica,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="3877" citStr="Freedman and Jurafsky (2011)" startWordPosition="595" endWordPosition="598">357–1367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics analysis of language used to describe food (by its purveyors and by its consumers). While our primary goal is to understand the language used in our corpus, our findings bear relevance to economics and hospitality research as well. This paper is a step on the way to the eventual goal of using linguistic analysis to understand social phenomena like sales and consumption. 2 Related Work There are several areas of related work scattered throughout linguistics, NLP, hospitality research, and economics. Freedman and Jurafsky (2011) studied the use of language in food advertising, specifically the words on potato chip bags. They argued that, due to the ubiquity of food writing across cultures, ethnic groups, and social classes, studying the use of language for describing food can provide perspective on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in langu</context>
<context position="19617" citStr="Freedman and Jurafsky (2011)" startWordPosition="3205" endWordPosition="3208">ed into several categories (taken from the model with MENUNAMES + MENUDESC + PR + METADATA). Table 3(a) shows selected features for the “ambience” field in the Yelp restaurant metadata and pane (b) lists some unigrams related to cooking methods. Pane (c) shows feature weights for ngrams often used to market menu items; we see larger weights for words targeting those who want to eat organically- or locally-grown food (farmhouse, heirloom, wild, and hormone), compared to those looking for comfort food (old time favorite, traditional, real, and fashioned). This is related to observations made by Freedman and Jurafsky (2011) that cheaper food is marketed by appealing to tradition and historicity, with more expensive food described in terms of naturalness, quality of ingredients, and the preparation process (e.g., handpicked, wild caught, etc.). Relatedly, in pane (e) we see that real mashed potatoes are expected to be cheaper than those described as creamy or smooth. Pane (d) shows feature weights for trigrams containing units of chicken; we can see an ordering in terms of size (bits &lt; cubes &lt; strips &lt; cuts) as well as the price increase associated with the use of the word morsels in place of less refined units. </context>
</contexts>
<marker>Freedman, Jurafsky, 2011</marker>
<rawString>J. Freedman and D. Jurafsky. 2011. Authenticity in America: Class distinctions in potato chip advertising. Gastronomica, 11(4):46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghose</author>
<author>P G Ipeirotis</author>
</authors>
<title>Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics.</title>
<date>2011</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>23</volume>
<issue>10</issue>
<contexts>
<context position="5284" citStr="Ghose and Ipeirotis (2011)" startWordPosition="825" endWordPosition="828">research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al. (2011) had a similar goal but decomposed user reviews into parts describing particular aspects of the product being reviewed (Hu and Liu, 2004). Our paper differs from price modeling based on product reviews in several ways. We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe</context>
</contexts>
<marker>Ghose, Ipeirotis, 2011</marker>
<rawString>A. Ghose and P. G. Ipeirotis. 2011. Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics. IEEE Transactions on Knowledge and Data Engineering, 23(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghose</author>
<author>P G Ipeirotis</author>
<author>A Sundararajan</author>
</authors>
<title>Opinion mining using econometrics: A case study on reputation systems.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4767" citStr="Ghose et al., 2007" startWordPosition="742" endWordPosition="745"> on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readabil</context>
</contexts>
<marker>Ghose, Ipeirotis, Sundararajan, 2007</marker>
<rawString>A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007. Opinion mining using econometrics: A case study on reputation systems. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="5623" citStr="Hu and Liu, 2004" startWordPosition="877" endWordPosition="880">g models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al. (2011) had a similar goal but decomposed user reviews into parts describing particular aspects of the product being reviewed (Hu and Liu, 2004). Our paper differs from price modeling based on product reviews in several ways. We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospit</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining opinion features in customer reviews. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Joshi</author>
<author>D Das</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Movie reviews and revenues: An experiment in text regression.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1930" citStr="Joshi et al., 2010" startWordPosition="290" endWordPosition="293">ustomer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and sentiment is an active area of investig</context>
<context position="4828" citStr="Joshi et al., 2010" startWordPosition="754" endWordPosition="757">guage and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between</context>
</contexts>
<marker>Joshi, Das, Gimpel, Smith, 2010</marker>
<rawString>M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Kasavana</author>
<author>D I Smith</author>
</authors>
<title>Menu Engineering: A Practical Guide to Menu Analysis.</title>
<date>1982</date>
<publisher>Hospitality Publications.</publisher>
<contexts>
<context position="6273" citStr="Kasavana and Smith, 1982" startWordPosition="981" endWordPosition="984">rice modeling based on product reviews in several ways. We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospitality research community (Kasavana and Smith, 1982; Kelly et al., 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al., 2008). Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by Zwicky and Zwicky (1980), who made linguistic observations through manual analysis of a corpus of 200 menus. Relatedly, Wansink et al. (2001; 2005) showed that the way that menu items a</context>
</contexts>
<marker>Kasavana, Smith, 1982</marker>
<rawString>M. L. Kasavana and D. I. Smith. 1982. Menu Engineering: A Practical Guide to Menu Analysis. Hospitality Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Kelly</author>
<author>N M Kiefer</author>
<author>K Burdett</author>
</authors>
<title>A demand-based approach to menu pricing. Cornell Hotel and Restaurant Administrative Quarterly,</title>
<date>1994</date>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="6294" citStr="Kelly et al., 1994" startWordPosition="985" endWordPosition="988">duct reviews in several ways. We consider a large set of weaklyrelated products instead of a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospitality research community (Kasavana and Smith, 1982; Kelly et al., 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al., 2008). Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by Zwicky and Zwicky (1980), who made linguistic observations through manual analysis of a corpus of 200 menus. Relatedly, Wansink et al. (2001; 2005) showed that the way that menu items are described affects </context>
</contexts>
<marker>Kelly, Kiefer, Burdett, 1994</marker>
<rawString>T. J. Kelly, N. M. Kiefer, and K. Burdett. 1994. A demand-based approach to menu pricing. Cornell Hotel and Restaurant Administrative Quarterly, 35(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kogan</author>
<author>D Levin</author>
<author>B R Routledge</author>
<author>J Sagi</author>
<author>N A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1890" citStr="Kogan et al., 2009" startWordPosition="283" endWordPosition="286">. charbroiled affect its price? When a customer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and</context>
<context position="4808" citStr="Kogan et al., 2009" startWordPosition="750" endWordPosition="753">lfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>I Shtrimberg</author>
</authors>
<title>Good news or bad news? let the market decide. Computing Attitude and Affect in Text: Theory and Applications.</title>
<date>2006</date>
<contexts>
<context position="4747" citStr="Koppel and Shtrimberg, 2006" startWordPosition="738" endWordPosition="741"> food can provide perspective on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review te</context>
</contexts>
<marker>Koppel, Shtrimberg, 2006</marker>
<rawString>M. Koppel and I. Shtrimberg. 2006. Good news or bad news? let the market decide. Computing Attitude and Affect in Text: Theory and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lerman</author>
<author>A Gilder</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Reading the markets: Forecasting public opinion of political candidates by news analysis.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="4788" citStr="Lerman et al., 2008" startWordPosition="746" endWordPosition="749">cioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of </context>
</contexts>
<marker>Lerman, Gilder, Dredze, Pereira, 2008</marker>
<rawString>K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008. Reading the markets: Forecasting public opinion of political candidates by news analysis. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McCullagh</author>
</authors>
<title>Regression models for ordinal data.</title>
<date>1980</date>
<journal>Journal of the royal statistical society. Series B (Methodological),</journal>
<pages>109--142</pages>
<contexts>
<context position="23474" citStr="McCullagh, 1980" startWordPosition="3821" endWordPosition="3822">cting the price range listed for each restaurant on its Yelp page. The values for this field are integers from 1 to 4 and indicate the price of a typical meal from the restaurant. For this task, we again train an `1-regularized linear regression model with integral price ranges as the true output values yi. Each input xi corresponds to the feature vector for an entire restaurant. For evaluation, we round the predicted values to the nearest integer: yi = ROUND(wTxi) and report the corresponding mean absolute error and accuracy. We compared this simple approach with an ordinal regression model (McCullagh, 1980) trained with the same il regularizer and noted very little improvement (77.32% vs. 77.15% accuracy for METADATA). Therefore, we only report in this section results for the linear regression model. In addition to the feature sets used for individual menu item price prediction, we used features on reviews (REVIEWS). Specifically, we used binary features for unigrams, bigrams, and trigrams in the full set of reviews for each restaurant. A stopword list was derived from the training data.4 Bigrams and trigrams were filtered if they ended with stopwords. Additionally, features occurring fewer than</context>
</contexts>
<marker>McCullagh, 1980</marker>
<rawString>P. McCullagh. 1980. Regression models for ordinal data. Journal of the royal statistical society. Series B (Methodological), pages 109–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J McVety</author>
<author>B J Ware</author>
<author>C L Ware</author>
</authors>
<title>Fundamentals of Menu Planning.</title>
<date>2008</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="6410" citStr="McVety et al., 2008" startWordPosition="1001" endWordPosition="1004">of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospitality research community (Kasavana and Smith, 1982; Kelly et al., 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al., 2008). Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by Zwicky and Zwicky (1980), who made linguistic observations through manual analysis of a corpus of 200 menus. Relatedly, Wansink et al. (2001; 2005) showed that the way that menu items are described affects customers’ perceptions and purchasing behavior. When menu items are described evocatively, customers choose them mor</context>
</contexts>
<marker>McVety, Ware, Ware, 2008</marker>
<rawString>P. J. McVety, B. J. Ware, and C. L. Ware. 2008. Fundamentals of Menu Planning. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Miller</author>
<author>D V Pavesic</author>
</authors>
<title>Menu: Pricing &amp; Strategy. Hospitality, Travel, and Tourism Series.</title>
<date>1996</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="6388" citStr="Miller and Pavesic, 1996" startWordPosition="997" endWordPosition="1000">f a homogeneous selection of a few products, and the reviews in our dataset are not product-centered but rather describe the overall experience of visiting a restaurant. Consequently, menu items are not always mentioned in reviews and rarely appear with their exact names. This makes it difficult to directly use review features in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospitality research community (Kasavana and Smith, 1982; Kelly et al., 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al., 2008). Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by Zwicky and Zwicky (1980), who made linguistic observations through manual analysis of a corpus of 200 menus. Relatedly, Wansink et al. (2001; 2005) showed that the way that menu items are described affects customers’ perceptions and purchasing behavior. When menu items are described evocatively, cus</context>
</contexts>
<marker>Miller, Pavesic, 1996</marker>
<rawString>J. E. Miller and D. V. Pavesic. 1996. Menu: Pricing &amp; Strategy. Hospitality, Travel, and Tourism Series. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2556" citStr="Pang and Lee, 2008" startWordPosition="392" endWordPosition="395">, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and sentiment is an active area of investigation (Pang and Lee, 2008). Much of this research has focused on customer-written reviews of goods and services, and perspectives have been gained on how sentiment is expressed in this type of informal text. In addition to sentiment, however, other variables are reflected in a reviewer’s choice of words, such as the price of the item under consideration. In this paper, we take a step toward joint modeling of multiple variables in review text, exploring connections between price and sentiment in restaurant reviews. Hence this paper contributes an exploratory data 1357 Proceedings of the 2012 Joint Conference on Empirica</context>
<context position="25830" citStr="Pang and Lee, 2008" startWordPosition="4208" endWordPosition="4211">eview text. To do this, we trained a logistic regression model predicting polarity for each review; we used the REVIEWS feature set, but this time considering each review as a single training instance. The polarity of a review was determined by whether or not its star rating was greater than the average rating across all reviews in the dataset (3.7 stars). We achieved an accuracy of 87% on the test data. We omit full details of these models because the polarity prediction task for user reviews is wellknown in the sentiment analysis community and our model is not an innovation over prior work (Pang and Lee, 2008). However, our purpose in training the model was to use the learned weights for understanding the text in the reviews. 6.2 Interpreting Reviews Given learned models for predicting a restaurant’s price range from its set of reviews as well as polarity for each review, we can turn the process around and use the feature weights to analyze the review text. Restricting our attention to reviews of 50–60 words, Table 5 shows sample reviews from our test set that lead to various predictions of price range and sentiment.5 This technique can also be useful when trying to determine the “true” star rating</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wansink</author>
<author>J E Painter</author>
<author>K van Ittersum</author>
</authors>
<title>Descriptive menu labels’ effect on sales. Cornell Hotel and Restaurant Administrative Quarterly,</title>
<date>2001</date>
<volume>42</volume>
<issue>6</issue>
<marker>Wansink, Painter, van Ittersum, 2001</marker>
<rawString>B. Wansink, J. E. Painter, and K. van Ittersum. 2001. Descriptive menu labels’ effect on sales. Cornell Hotel and Restaurant Administrative Quarterly, 42(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wansink</author>
<author>K van Ittersum</author>
<author>J E Painter</author>
</authors>
<title>How descriptive food names bias sensory perceptions in restaurants. Food Quality and Preference,</title>
<date>2005</date>
<volume>16</volume>
<issue>5</issue>
<marker>Wansink, van Ittersum, Painter, 2005</marker>
<rawString>B. Wansink, K. van Ittersum, and J. E. Painter. 2005. How descriptive food names bias sensory perceptions in restaurants. Food Quality and Preference, 16(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yogatama</author>
<author>M Heilman</author>
<author>B O’Connor</author>
<author>C Dyer</author>
<author>B R Routledge</author>
<author>N A Smith</author>
</authors>
<title>Predicting a scientific community’s response to an article.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Yogatama, Heilman, O’Connor, Dyer, Routledge, Smith, 2011</marker>
<rawString>D. Yogatama, M. Heilman, B. O’Connor, C. Dyer, B. R. Routledge, and N. A. Smith. 2011. Predicting a scientific community’s response to an article. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Zwicky</author>
<author>A M Zwicky</author>
</authors>
<title>America’s national dish: The style of restaurant menus.</title>
<date>1980</date>
<journal>American Speech,</journal>
<volume>55</volume>
<issue>2</issue>
<contexts>
<context position="6712" citStr="Zwicky and Zwicky (1980)" startWordPosition="1049" endWordPosition="1052">ures in a pricing model for individual menu items. Menu planning and pricing has been studied for many years by the culinary and hospitality research community (Kasavana and Smith, 1982; Kelly et al., 1994), often including recommendations for writing menu item descriptions (Miller and Pavesic, 1996; McVety et al., 2008). Their guidelines frequently include example menus from successful restaurants, but typically do not use large corpora of menus or automated analysis, as we do here. Other work focused more specifically on particular aspects of the language used on menus, such as the study by Zwicky and Zwicky (1980), who made linguistic observations through manual analysis of a corpus of 200 menus. Relatedly, Wansink et al. (2001; 2005) showed that the way that menu items are described affects customers’ perceptions and purchasing behavior. When menu items are described evocatively, customers choose them more often and report higher satisfaction with quality and value, as compared to when they are given the same items described with conventional names. Wansink et al. did not use a corpus, but rather conducted a small-scale experiment in a working cafeteria with customers and collected surveys to analyze </context>
</contexts>
<marker>Zwicky, Zwicky, 1980</marker>
<rawString>A. D. Zwicky and A. M. Zwicky. 1980. America’s national dish: The style of restaurant menus. American Speech, 55(2):83–92.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.943905">
Joint Entity and Event Coreference Resolution across Documents
</title>
<author confidence="0.916155">
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan Jurafsky
</author>
<affiliation confidence="0.46876">
Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.998584">
{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999743647058824">
We introduce a novel coreference resolution
system that models entities and events jointly.
Our iterative method cautiously constructs
clusters of entity and event mentions using lin-
ear regression to model cluster merge opera-
tions. As clusters are built, information flows
between entity and event clusters through fea-
tures that model semantic role dependencies.
Our system handles nominal and verbal events
as well as entities, and our joint formulation
allows information from event coreference to
help entity coreference, and vice versa. In a
cross-document domain with comparable doc-
uments, joint coreference resolution performs
significantly better (over 3 CoNLL F1 points)
than two strong baselines that resolve entities
and events separately.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999555714285714">
Most coreference resolution systems focus on enti-
ties and tacitly assume a correspondence between
entities and noun phrases (NPs). Focusing on NPs
is a way to restrict the challenging problem of coref-
erence resolution, but misses coreference relations
like the one between hanged and his suicide in (1),
and between placed and put in (2).
</bodyText>
<listItem confidence="0.984361714285714">
1. (a) One of the key suspected Mafia bosses ar-
rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.
2. (a) The New Orleans Saints placed Reggie Bush
on the injured list on Wednesday.
(b) Saints put Bush on I.R.
</listItem>
<bodyText confidence="0.99997178125">
As (1c) shows, NPs can also refer to events, and
so corefer with phrases other than NPs (Webber,
1988). By being anchored in spatio-temporal dimen-
sions, events represent the most frequent referent of
verbal elements. In addition to time and location,
events are characterized by their participants or ar-
guments, which often correspond with discourse en-
tities. This two-way feedback between events and
their arguments (or entities) is the core of our ap-
proach. Since arguments play a key role in describ-
ing an event, knowing that two arguments corefer
is useful for finding coreference relations between
events, and knowing that two events corefer is use-
ful for finding coreference relations between enti-
ties. In (1), the coreference relation between One
of the key suspected Mafia bosses arrested yesterday
and Lo Presti can be found by knowing that their
predicates (i.e., has hanged and had hanged) core-
fer. On the other hand, the coreference relations be-
tween the arguments Saints and Bush in (2) helps
to determine the coreference relation between their
predicates placed and put.
In this paper, we take a holistic approach to coref-
erence. We annotate a corpus with cross-document
coreference relations for nominal and verbal men-
tions. We focus on both intra and inter-document
coreference because this scenario is at the same time
more challenging and more relevant to real-world
applications such as news aggregation. We use this
corpus to train a model that jointly addresses refer-
ences to both entities and events across documents.
The contributions of this work are the following:
</bodyText>
<listItem confidence="0.95776">
• We introduce a novel approach for entity and
event coreference resolution. At the core of
</listItem>
<page confidence="0.988056">
489
</page>
<note confidence="0.7944375">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 489–500, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.873007166666667">
our approach is an iterative algorithm that cau-
tiously constructs clusters of entity and event
mentions using linear regression to model clus-
ter merge operations. Importantly, our model
allows information to flow between clusters of
both types through features that model context
using semantic role dependencies.
• We annotate and release a new corpus with
coreference relations between both entities and
events across documents. The relations anno-
tated are both intra and inter-document, which
more accurately models real-world scenarios.
</bodyText>
<listItem confidence="0.8130982">
• We evaluate our cross-document coreference
resolution system on this corpus and show that
our joint approach significantly outperforms
two strong baselines that resolve entities and
events separately.
</listItem>
<sectionHeader confidence="0.999707" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999964145454546">
Entity coreference resolution is a well studied prob-
lem with many successful techniques for identify-
ing mention clusters (Ponzetto and Strube, 2006;
Haghighi and Klein, 2009; Stoyanov et al., 2009;
Haghighi and Klein, 2010; Raghunathan et al., 2010;
Rahman and Ng, 2011, inter alia). Most of these
techniques focus on matching compatible noun pairs
using various syntactic and semantic features, with
efforts targeted toward improving features and clus-
tering models.
Prior work showed that models that jointly resolve
mentions across multiple entities result in better per-
formance than simply resolving mentions in a pair-
wise fashion (Denis and Baldridge, 2007; Poon and
Domingos, 2008; Wick et al., 2008; Lee et al., 2011,
inter alia). A natural extension is to perform coref-
erence jointly across both entities and events. Yet
there has been little attempt in this direction.
We know of only limited work that incorporates
event-related information in entity coreference, typ-
ically by incorporating the verbs in context as fea-
tures. For instance, Haghighi and Klein (2010) in-
clude the governor of the head of nominal mentions
as features in their model. Rahman and Ng (2011)
also used event-related information by looking at
which semantic role the entity mentions can have
and the verb pairs of their predicates. We confirm
that such features are useful but also show that the
complementary features for verbal mentions lead to
even better performance, especially when event and
entity clusters are jointly modeled.
Compared to the extensive work on entity coref-
erence, the related problem of event coreference re-
mains relatively under-explored, with minimal work
on how entity and event coreference can be con-
sidered jointly on an open domain. Early work on
event coreference for MUC (Humphreys et al., 1997;
Bagga and Baldwin, 1999) focused on scenario-
specific events. More recently, there have been
approaches that looked at event coreference for
wider domains. Chen and Ji (2009) proposed us-
ing spectral graph clustering to cluster events. Be-
jan and Harabagiu (2010) proposed a nonparamet-
ric Bayesian model for open-domain event resolu-
tion. However, most of this prior work focused only
on event coreference, whereas we address both en-
tities and events with a single model. Humphreys
et al. (1997) considered entities as well as events,
but due to the lack of a corpus annotated with event
coreference, their approach was only evaluated im-
plicitly in the MUC-6 template filling task. To our
knowledge, the only previous work that considered
entity and event coreference resolution jointly is
He (2007), but limited to the medical domain and
focused on just five semantic categories.
</bodyText>
<sectionHeader confidence="0.989998" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.999985117647059">
Following the intuition introduced in Section 1, our
approach iteratively builds clusters of event and en-
tity mentions jointly. As more information becomes
available (e.g., finding out that two verbal mentions
have arguments that belong to the same entity clus-
ter), the features of both entity and event mentions
are re-generated, which prompts future clustering
operations. Our model follows a cautious (or “baby
steps”) approach, which we previously showed to be
successful for entity coreference resolution (Raghu-
nathan et al., 2010; Lee et al., 2011). However,
unlike our previous work, which used deterministic
rules, in this paper we learn a coreference resolution
model using linear regression. Algorithm 1 summa-
rizes the flow of the proposed algorithm. We detail
its steps next. We describe the training procedure in
Section 4 and the features used in Section 5.
</bodyText>
<page confidence="0.991475">
490
</page>
<construct confidence="0.2988275">
Algorithm 1: Joint Coreference Resolution
input : set of documents D
input : coreference model O
// clusters of mentions:
</construct>
<equation confidence="0.998206421052632">
1 E= ��
//clusters of documents:
2 C = clusterDocuments(D)
3 foreach document cluster c in C do
// all mentions in one doc cluster:
4 M = extractMentions(c)
// singleton mention clusters:
E&apos; = buildSingletonClusters(M)
// high-precision deterministic sieves:
E&apos; = applyHighPrecisionSieves(E&apos;)
// iterative event/entity coreference:
while 3 e1, e2 E E&apos;s.t. score(e1, e2, O) &gt; 0.5 do
(e1, e2) = arg max e1,e2Es, score(e1, e2, O)
E&apos; = merge(e1, e2, E&apos;)
// pronoun sieve:
E&apos; = applyPronounSieve(E&apos;)
// append to global output:
E = E + E&apos;
output : E
</equation>
<subsectionHeader confidence="0.998594">
3.1 Document Clustering
</subsectionHeader>
<bodyText confidence="0.999978772727273">
Our approach starts with several steps that reduce
the search space for the actual coreference resolution
task. The first is document clustering, which clusters
the set of input documents (D) into a set of docu-
ment clusters (C). In the subsequent steps we only
cluster mentions that appear in the same document
cluster. We found this to be very useful in practice
because, in addition to reducing the search space, it
provides a word sense disambiguation mechanism
based on corpus-wide topics. For example, with-
out document clustering, our algorithm may decide
to cluster two mentions of the verb hit, but know-
ing that one belongs to a cluster containing earth-
quake reports and the other to a cluster with reports
on criminal activities, this decision can be avoided.1
Any non-parametric clustering algorithm can be
used in this step. In this paper, we used the algo-
rithm proposed by Surdeanu et al. (2005). This algo-
rithm is an Expectation Maximization (EM) variant
where the initial points (and the number of clusters)
are selected from the clusters generated by a hierar-
chical agglomerative clustering algorithm using ge-
</bodyText>
<footnote confidence="0.929061333333333">
1Since different mentions of the verb say in the same topic
might refer to different events, they are only merged if they have
coreferent arguments.
</footnote>
<bodyText confidence="0.9994615">
ometric heuristics. This algorithm performs well on
our data. For example, in the training dataset, only
two topics (handling different earthquake events) are
incorrectly merged into the same cluster.
</bodyText>
<subsectionHeader confidence="0.999882">
3.2 Mention Extraction
</subsectionHeader>
<bodyText confidence="0.999988954545455">
In this step (4 in Algorithm 1) we extract nominal,
pronominal, and verbal mentions. We extract nom-
inal and pronominal mentions using the mention
identification component in the publicly download-
able Stanford coreference resolution system (Raghu-
nathan et al., 2010; Lee et al., 2011). We consider
as verbal mentions all words whose part of speech
starts with VB, with the exception of some auxil-
iary/copulative verbs (have, be and seem). For each
of the identified mentions we build a singleton clus-
ter (step 5 in Algorithm 1).
Crucially, we do not make a formal distinction be-
tween entity and event mentions. This distinction is
not trivial to implement (e.g., is the noun earthquake
an entity or an event mention?) and an imperfect
classification would negatively affect the following
coreference resolution. Instead, we simply classify
mentions into verbal or nominal, and use this dis-
tinction later during feature generation (Section 5).
To compare event nouns (e.g., development) with
verbal mentions, the “derivationally related form”
relation in WordNet is used.
</bodyText>
<subsectionHeader confidence="0.999318">
3.3 High-precision Entity Resolution Sieves
</subsectionHeader>
<bodyText confidence="0.999989777777778">
To further reduce the problem’s search space, in
step 6 of Algorithm 1 we apply a set of high-
precision filters from the Stanford coreference res-
olution system. This system is a collection of deter-
ministic models (or “sieves”) for entity coreference
resolution that incorporate lexical, syntactic, seman-
tic, and discourse information. These sieves are ap-
plied from higher to lower precision. As clusters are
built, information such as mention gender and num-
ber is propagated across mentions in the same clus-
ter, which helps subsequent decisions. The Stanford
system obtained the highest score at the CoNLL-
2011 shared task on English coreference resolution.
For this step, we selected all the sieves from the
Stanford system with the exception of the pronoun
resolution sieve. All the remaining sieves (listed
in Table 1) have high precision because they em-
ploy linguistic heuristics with little ambiguity, e.g.,
</bodyText>
<figure confidence="0.983317142857143">
5
6
7
8
9
10
11
</figure>
<page confidence="0.650825">
491
</page>
<table confidence="0.44123425">
High-precision sieves
Discourse processing sieve
Exact string match sieve
Relaxed string match sieve
Precise constructs sieve (e.g., appositives)
Strict head match sieves
Proper head noun match sieve
Relaxed head matching sieve
</table>
<tableCaption confidence="0.996445">
Table 1: Deterministic sieves in step 6 of Algorithm 1.
</tableCaption>
<bodyText confidence="0.999976071428571">
one sieve clusters together two entity mentions only
when they have the same head word. Note that all
these heuristics were designed for within-document
coreference. They work well in our context be-
cause we apply them in individual document clus-
ters, where the one-sense-per-discourse principle
still holds (Yarowsky, 1995).
Importantly, these sieves do not address verbal
mentions. That is, all verbal mentions are still in sin-
gleton clusters after this step. Furthermore, none of
these sieves use features that facilitate the joint reso-
lution of nominal and verbal mentions (e.g., features
from semantic role frames). All these limitations are
addressed next.
</bodyText>
<subsectionHeader confidence="0.98787">
3.4 Iterative Entity/Event Resolution
</subsectionHeader>
<bodyText confidence="0.999974967741936">
In this stage (steps 7 – 9 in Algorithm 1), we con-
struct entity and event clusters using a cautious or
“baby steps” approach. We use a single linear re-
gressor (0) to model cluster merge operations be-
tween both verbal and nominal clusters. Intuitively,
the linear regressor models the quality of the merge
operation, i.e., a score larger than 0.5 indicates that
more than half of the mention pairs introduced by
this merge are correct. We discuss the training pro-
cedure that yields this scoring function in Section 4.
In each iteration, we perform the merge operation
that has the highest score. Once two clusters are
merged (step 9) we regenerate all the mention fea-
tures to reflect the current clusters. We stop when no
merging operation with an overall benefit is found.
This iterative procedure is the core of our joint
coreference resolution approach. This algorithm
transparently merges both entity and event men-
tions and, importantly, allows information to flow
between clusters of both types as merge operations
take place. For example, assume that during iter-
ation i we merge the two hanged verbs in the first
example in Section 1 (because they have the same
lemma). Because of this merge, in iteration i + 1 the
nominal mentions Lo Presti and One of the key sus-
pected Mafia bosses have the same semantic role for
verbs assigned to the same cluster. This is a strong
hint that these two nominal mentions belong to the
same cluster. Indeed, the feature that models this
structure received one of the highest weights in our
linear regression model (see Section 7).
</bodyText>
<subsectionHeader confidence="0.993311">
3.5 Pronoun Sieve
</subsectionHeader>
<bodyText confidence="0.9999792">
Our approach concludes with the pronominal coref-
erence resolution sieve from the Stanford system.
This sieve is necessary because our current reso-
lution algorithm ignores mention ordering and dis-
tance (i.e., in step 7 we compare all clusters regard-
less of where their mentions appear in the text). As
previous work has proved, the structure of the text is
crucial for pronominal coreference (Hobbs, 1978).
For this reason, we handle pronouns outside of the
main algorithm block.
</bodyText>
<sectionHeader confidence="0.946072" genericHeader="method">
4 Training the Cluster Merging Model
</sectionHeader>
<bodyText confidence="0.999925090909091">
Two observations drove our choice of model and
training algorithm. First, modeling the merge op-
eration as a classification task is not ideal, because
only a few of the resulting clusters are entirely cor-
rect or incorrect. In practice, most of the clusters
will contain some mention pairs that are correct and
some that are not. Second, generating training data
for the merging model is not trivial: a brute force
approach that looks at all the possible combinations
is exponential in the number of mentions. This is
both impractical and unnecessary, as some of these
combinations are unlikely to be seen in practice.
We address these observations with Algorithm 2.
The algorithm uses gold coreference labels to train a
linear regressor that models the quality of the clus-
ters produced by merge operations. We define the
quality score q of a new cluster as the percentage of
new mention pairs (i.e., not present in either one of
the clusters to be merged) that are correct:
where links(in)correct is the number of newly intro-
duced (in)correct pairwise mention links when two
clusters are merged.
</bodyText>
<equation confidence="0.893556774193549">
q=
linkscorrect + linksincorrect
linkscorrect (1)
492
Algorithm 2: Training Procedure
input : set of documents D
input : correct mention clusters 9
1 C = clusterDocuments(D)
// linear regression coreference model:
2 O = assignInitialWeights(C, 9)
// repeat for T epochs:
3 fort= 1 to T do
// training data for linear regressor:
4 r={}
foreach document cluster c in C do
M = extractMentions(c)
E = buildSingletonClusters(M)
E = applyHighPrecisionSieves(E)
// gather training examples
// as clusters are built:
while 3 e1, e2 E Es.t. sco(e1, e2, O) &gt; 0.5 do
forall e&apos;1, e&apos;2 E E do
q = qualityOfMerge(e&apos;1, e&apos;2, 9)
r = append(e&apos;1, e&apos;2, q, r)
(e1, e2) = arg max e1,e2Es sco(e1, e2, O)
E = merge(e1, e2, E)
// train using data from last epoch:
O&apos; = trainLinearRegressor(r)
// interpolate with older model:
O = AO + (1 − A)O&apos;
output : O
</equation>
<bodyText confidence="0.992538237288136">
We address the potential explosion in training data
size by considering only merge operations that are
likely to be inspected by the algorithm as it runs.
To achieve this, Algorithm 2 repeatedly runs the ac-
tual clustering algorithm (as given by the current
model O) over the training dataset (steps 5 – 14).2
When the algorithm iteratively constructs its clus-
ters (steps 9 – 14), we generate training data from
all possible cluster pairs available during a particular
iteration (steps 10 – 12). For each pair, we compute
its score using Equation 1 (step 11) and add it to the
training corpus F (step 12). Note that this avoids in-
specting many of the possible cluster combinations:
once a cluster is built (e.g., during the previous iter-
ations or by the deterministic sieves in step 8), we
do not generate training data from its members, but
rather treat it as an atomic unit. On the other hand,
our approach generates more training data than on-
line learning, which trains using only the actual de-
cisions taken during inference in each iteration (i.e.,
2We skip the pronoun sieve here because it does not affect
the decisions taken during the iterative resolution steps.
the pair (e1, e2) in step 13).
After each epoch we have a new training cor-
pus F, which we use to train the new linear regres-
sion model O’ (step 15), which is then interpolated
with the old one (step 16).
Our training procedure is similar in spirit to trans-
formation based learning (TBL) (Brill, 1995). Sim-
ilarly to TBL, our approach repeatedly applies the
model over the training data and attempts to mini-
mize the error rate of the current model. However,
while TBL learns rules that directly minimize the
current error rate, our approach achieves this indi-
rectly, by incorporating the reduction in error rate in
the score of the generated datums. This allows us
to fit a linear regression to this task, which, as dis-
cussed before, is a better model for this task.
Just like any hill-climbing algorithm, our ap-
proach has the risk of converging to a local max-
imum. To mitigate this risk, we do not initialize
our model O with random weights, but rather use
hints from the deterministic sieves. This procedure
(listed in step 2) runs the high-precision sieves in-
troduced in Section 3.3 and, just like the data gen-
eration loop in Algorithm 2, creates training exam-
ples from the clusters available after every merge
operation. Since these deterministic models address
only nominal clusters, at the end we generate train-
ing data for events by inspecting all the pairs of sin-
gleton verbal clusters. Using this data, we train the
initial linear regression model.
We trained our model using L2 regularized linear
regression with a regularization coefficient of 1.0.
We did not tune the regularization coefficient. We
ran the training algorithm for 10 epochs, although
we observed minimal changes after three epochs.
We tuned the interpolation weight (A) to a value
of 0.7 using our development corpus.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.998839142857143">
We list in Table 2 the features used by the lin-
ear regression model. As the table indicates, our
feature set relies heavily on semantic roles, which
were extracted using the SwiRL semantic role la-
beling (SRL) system (Surdeanu et al., 2007).3 Be-
cause SwiRL addresses only verbal predicates, we
extended it to handle nominal predicates. In this
</bodyText>
<footnote confidence="0.529828">
3http://www.surdeanu.name/mihai/swirl/
</footnote>
<figure confidence="0.993116333333333">
5
6
7
8
9
10
11
12
13
14
15
16
</figure>
<page confidence="0.992586">
493
</page>
<table confidence="0.907921722222222">
Applies to
Feature Name Entities (E) Description and Example
or Events (V)
Entity Heads E Cosine similarity of the head-word vectors of two clusters. The head-word vector
stores the head words of all mentions in a cluster and their frequencies. For example,
the vector for the three-mention cluster {Barack Obama, President Obama, US
president}, is {Obama:2, president:1}.
Cosine similarity of the lemma vectors of two clusters. For example, the lemma
Event Lemmas V
vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.
Links between E, V
Synonyms
Coreferent Arguments E, V
in a Specific Role?
Coreferent Predicate in E
a Specific Role?
Number; Animacy; E
Gender; NE Label
</table>
<bodyText confidence="0.984395727272727">
The percentage of newly-introduced mention links after the merge that are WordNet
synonyms (Fellbaum, 1998). For example, when merging the following two clus-
ters, {hit, strike} and {strike, join, say}, two out of the six new links are between
words that belong to the same WordNet synset: (hit – strike) and (strike – strike).
The total number of shared arguments and predicates between mentions in the
two clusters. We use the cluster IDs of the corresponding arguments/predicates
to check for identity. For example, when comparing the event clusters {bought}
and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and
[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-
tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,
AMD} and {ATI, ATI} were previously created). For entity clusters, this feature
counts the number of coreferent predicates. In addition to PropBank-style roles, for
event mentions we also include the closest left and right entity mentions in order to
capture any arguments missed by the SRL system.
Indicator feature set to 1 if the two clusters have at least one coreferent argument in
a given role. We generate one variant of this feature for each argument label, e.g.,
Arg0, Arg1, etc. For example, the value of this feature for Arg0 for the clusters
{bought} and {acquired} in the above example is 1.
Indicator feature set to 1 if the two clusters have at least one coreferent predicate for
a given role. For example, for the clusters {the man} and {the person}, extracted
from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of
this feature is 1 if the two helped verbs were previously clustered together.
Cosine similarity of vectors containing words that are distributionally similar to
words in the cluster mentions. We built these vectors by extracting the top-ten
most-similar words in Dekang Lin’s similarity thesaurus (Lin, 1998) for all the
nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new
home}, we construct this vector by expanding new and home to: {new:1, original:1,
old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,
small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,
mansion:1, school:1, restaurant:1, hospital:1 }.
Cosine similarity of number, gender, animacy, and NE label vectors. For example,
the number and gender vectors for the two-mention cluster {systems, a pen} are
Number = {singular:1, plural:1}, Gender = {neutral:2}.
</bodyText>
<table confidence="0.5886414">
Number of Coreferent
Arguments or E, V
Predicates
2nd Order Similarity of E
Mention Words
</table>
<tableCaption confidence="0.85261375">
Table 2: List of features used when comparing two clusters. If any of the two clusters contains a verbal mention we
consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We
append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in
each of the two clusters. We use the suffix Proper only if both head words are proper nouns.
</tableCaption>
<bodyText confidence="0.966742">
paper we used a single heuristic: the possessor of
a nominal event’s predicate is marked as its Arg0,
e.g., Logan is the Arg0 to run in Logan’s run.4
</bodyText>
<footnote confidence="0.967322333333333">
4A principled solution to this problem is to use an SRL sys-
tem for nominal predicates trained using NomBank (Meyers et
al., 2004). We will address this in future work.
</footnote>
<page confidence="0.998812">
494
</page>
<bodyText confidence="0.9992735">
We extracted named entity labels using the named
entity recognizer from the Stanford CoreNLP suite.
</bodyText>
<sectionHeader confidence="0.997685" genericHeader="method">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.9866">
6.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99989727027027">
The training and test data sets were derived from
the EventCorefBank (ECB) corpus5 created by Be-
jan and Harabagiu (2010) to study event coreference
since standard corpora such as OntoNotes (Pradhan
et al., 2007) contain a small number of annotated
event clusters. The ECB corpus consists of 482 doc-
uments from Google News clustered into 43 topics,
where a topic is described as a seminal event. The
reason for including comparable documents was to
increase the number of cross-document coreference
relations. Bejan and Harabagiu (2010) only anno-
tated a selection of events.
For the purpose of our study, we extended the
original corpus in two directions: (i) fully anno-
tated sentences, and (ii) entity coreference relations.
In addition, we removed relations other than coref-
erence (e.g., subevent, purpose, related, etc.) that
had been originally annotated. We revised and com-
pleted the original annotation by annotating every
entity and event in the sentences that were (partially)
annotated. The annotation was performed by four
experts, using the Callisto annotation tool.6 The
annotation guidelines and the generated corpus are
available here.7
Our annotation of the ECB corpus followed the
OntoNotes (Pradhan et al., 2007) standard for coref-
erence annotation, with a few extensions to handle
events. For nouns, we annotated full NPs (with all
modifiers), excluding appositive phrases and nomi-
nal predicates. Only premodifiers that were proper
nouns or possessive phrases were annotated. For
events, we annotated the semantic head of the verb
phrase. We extended the OntoNotes guidelines by
also annotating singletons (but we do not score
them; see below), and by including all events men-
tions (not only those mentioned at least once with an
NP). This required us to be specific with respect to:
</bodyText>
<footnote confidence="0.9323886">
5http://faculty.washington.edu/bejan/
data/ECB1.0.tar.gz
6http://callisto.mitre.org
7http://nlp.stanford.edu/pubs/
jcoref-corpus.zip
</footnote>
<table confidence="0.999928285714286">
Training Dev Test Total
# Topics 12 3 28 43
# Documents 112 39 331 482
# Entities 459 46 563 1068
# Entity Mentions 1723 259 3465 5447
# Events 300 30 444 774
# Event Mentions 751 140 1642 2533
</table>
<tableCaption confidence="0.999893">
Table 3: Corpus statistics.
</tableCaption>
<figure confidence="0.8328645">
(ENTITY COREFID=“26”) A publicist (/ENTITY) (EVENT
COREFID=“4”) says (/EVENT) (ENTITY COREFID=“23”)
Tara Reid (/ENTITY) has (EVENT COREFID=“3”) checked
(/EVENT) (ENTITY COREFID=“23”) herself (/ENTITY) (EVENT
COREFID=“3*”) into (/EVENT) (ENTITY COREFID=“28”) rehab
(/ENTITY).
</figure>
<figureCaption confidence="0.999854">
Figure 1: Annotation example.
</figureCaption>
<bodyText confidence="0.9997485">
Light verbs Verbs such as give and make followed
by a noun (e.g., make an offer) were not anno-
tated, but the noun was.
Phrasal verbs We annotated the verb together with
the preposition or adverb (e.g., check in).
Idioms They were annotated with all their elements
(e.g., booze it up).
The first topic was annotated by all four anno-
tators as burn-in. Afterwards, annotation disagree-
ments were resolved between all annotators and the
next three topics were annotated again by all four an-
notators to measure agreement. Following Passon-
neau (2004), we computed an inter-annotator agree-
ment of α = 0.55 (Krippendorff, 2004) on these
three topics, indicating moderate agreement among
the annotators. Given the complexity of the task, we
consider this to be a good score. For example, the
average of the CoNLL F1 between any two annota-
tors is 73.58, which is much higher than the system
scores reported in the literature.
After annotating the four topics, disagreements
were resolved again and all the documents in the
four topics were corrected to match the consensus.
The rest of the corpus was split between the four an-
notators, and each document was annotated by a sin-
gle annotator. Figure 1 shows an example. Table 3
shows the corpus statistics, including the training,
development (dev) and test set splits. The dev topics
were used for tuning the interpolation parameter A
from Section 4.
</bodyText>
<page confidence="0.997348">
495
</page>
<table confidence="0.999774">
System R MUC F1 R B3 F1 R CEAF-04 F1 R BLANC F1 CoNLL F1
P P P P
Baseline 1 Entity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9
Wo/ SRL Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2
Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8
Baseline 2 Entity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8
With SRL Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2
Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6
This paper Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2
Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8
Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9
</table>
<tableCaption confidence="0.9988035">
Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the
complete task using five metrics.
</tableCaption>
<subsectionHeader confidence="0.995051">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998937829787234">
We use five coreference evaluation metrics widely
used in the literature:
MUC (Vilain et al., 1995) Link-based metric which
measures how many predicted and gold clus-
ters need to be merged to cover the gold and
predicted clusters, respectively.
B3 (Bagga and Baldwin, 1998) Mention-based
metric which measures the proportion of over-
lap between predicted and gold clusters for a
given mention.
CEAF (Luo, 2005) Entity-based metric that, unlike
B3, enforces a one-to-one alignment between
gold and predicted clusters. We employ the
entity-based version of CEAF.
BLANC (Recasens and Hovy, 2011) Metric based
on the Rand index (Rand, 1971) that consid-
ers both coreference and non-coreference links
to address the imbalance between singleton and
coreferent mentions.
CoNLL F1 Average of MUC, B3, and CEAF-04.
This was the official metric in the CoNLL-2011
shared task (Pradhan et al., 2011).
We followed the CoNLL-2011 evaluation methodol-
ogy, that is, we removed all singleton clusters, and
apposition/copular relations before scoring.
We evaluated the systems on three different set-
tings: only on entity clusters, only on event clus-
ters, and on the complete task, i.e., both entities and
events. Note that the gold corpus separates clusters
into entity and event clusters (see Table 3), but our
system does not make this distinction at runtime.
In order to compute the entity-only and event-only
scores in Table 4, we implemented the following
procedure: (a) when scoring entity clusters, we re-
moved all mentions that were found to be coreferent
with at least one gold event mention and not coref-
erent with any gold entity mentions; and (b) we per-
formed the opposite action when scoring event clus-
ters. This procedure is necessary because our men-
tion identification component is not perfect, i.e., it
generates mentions that do not exist in the gold an-
notation. Furthermore, this procedure is conserva-
tive with respect to the clustering errors of our sys-
tem, e.g., all spurious mentions that our system in-
cludes in a cluster with a gold entity mention are
considered for the entity score, regardless of their
gold type (event or entity).
</bodyText>
<subsectionHeader confidence="0.876209">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999811333333333">
Table 4 compares the performance of our system
against two strong baselines that resolve entities and
events separately. Baseline 1 uses a modified Stan-
ford coreference resolution system after our doc-
ument clustering and mention identification steps.
Because the original Stanford system implements
only entity coreference, we extended it with an extra
sieve that implements lemma matching for events.
This additional sieve merges two verbal clusters
(i.e., clusters that contain at least one verbal men-
tion) or a verbal and a nominal cluster when at least
two lemmas of mention head words are the same be-
tween clusters, e.g., helped and the help.
The second baseline adds two more sieves to
Baseline 1. Both these sieves model entity and event
</bodyText>
<page confidence="0.998155">
496
</page>
<bodyText confidence="0.999900388888889">
contextual information using semantic roles. The
first sieve merges two nominal clusters when two
mentions in the respective clusters have the same
head words and two mentions (possibly with dif-
ferent heads) modify with the same role label two
predicates that have the same lemma. For exam-
ple, this sieve merges the clusters {Obama, the pres-
ident} (seen in the text [Obama]Arg0 attended and
[the president]Arg1 was elected) and {Obama} (seen
in the text [Obama]Arg1 was elected), because they
share a mention with the same head word (Obama)
and two mentions modify with the same role (Arg1)
predicates with the same lemma (elect). The sec-
ond sieve implements the complementary action for
event clusters. That is, it merges two verbal clusters
when at least two mentions have the same lemma
and at least two mentions have semantic arguments
with the same role label and the same lemma.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999990535714286">
The first block in Table 4 indicates that lemma
matching is a strong baseline for event resolution.
Most of the event scores for Baseline 1 are actually
higher than the corresponding entity scores, which
were obtained using the highest ranked system at the
CoNLL-2011 shared task (Lee et al., 2011). Adding
contextual information using semantic roles (Base-
line 2) helps both entities and events. The CoNLL
F1 for Baseline 2 increases almost 3 points for enti-
ties and 1 point for events. This demonstrates that
local syntactico-semantic context is important for
coreference resolution even in a cross-document set-
ting and that the current state-of-the-art in SRL can
model this context accurately.
The best scores (almost unanimously) are ob-
tained by the model proposed in this paper, which
scores 3.4 CoNLL F1 points higher than Baseline 2
for entities, and 2.6 points higher for events. For the
complete task, our approach scores 3.3 CoNLL F1
points higher than Baseline 2, and 6.1 points higher
than Baseline 1. This demonstrates that a holistic
approach to coreference resolution improves the res-
olution of both entities and events more than models
that address aspects of the task separately. To fur-
ther understand our experiments, we listed the top
five entity/event features with the highest weights in
our model in Table 5. The table indicates that six out
of the ten features serve the purpose of passing infor-
</bodyText>
<table confidence="0.999532083333333">
Entity Feature Weight
Entity Heads – Proper 1.10
Coreferent Predicate for ArgM-LOC – Common 0.45
Entity Heads – Common 0.36
Coreferent Predicate for Arg0 – Proper 0.29
Coreferent Predicate for Arg2 – Common 0.28
Event Feature Weight
Event Lemmas 0.45
Coreferent Argument for Arg1 0.19
Links between Synonym 0.16
Coreferent Argument for Arg2 0.13
Number of Coreferent Arguments 0.07
</table>
<tableCaption confidence="0.999847">
Table 5: Top five features with the highest weights.
</tableCaption>
<bodyText confidence="0.999883958333333">
mation between entity and event clusters. For exam-
ple, the “Coreferent Argument for Arg1” feature is
triggered when two event clusters have Arg1 argu-
ments that already belong to the same entity cluster.
This allows information from previous entity coref-
erence operations to impact future merges of event
clusters. This is the crux of our iterative approach to
joint coreference resolution.
Finally, we performed an error analysis by man-
ually evaluating 100 errors. We distinguished nine
major types of errors. Their ratios together with a
description and an example are given in Table 6.
This work demonstrates that an approach that
jointly models entities and events is better for cross-
document coreference resolution. However, our
model can be improved. For example, document
clustering and coreference resolution can be solved
jointly, which we expect would improve both tasks.
Furthermore, our iterative coreference resolution
procedure (Algorithm 1) could be modified to ac-
count for mention ordering and distance, which
would allow us to include pronominal resolution in
our joint model, rather than addressing it with a sep-
arate deterministic sieve.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999891">
We have presented a holistic model for cross-
document coreference resolution that jointly solves
references to events and entities by handling both
nominal and verbal mentions. Our joint resolution
algorithm allows event coreference to help improve
entity coreference, and vice versa. In addition, our
iterative procedure, based on a linear regressor that
models the quality of cluster merges, allows each
</bodyText>
<page confidence="0.987624">
497
</page>
<figure confidence="0.899949857142857">
Description
Example
Pronoun resolution
(36%)
The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity
system. These errors include (only a small number of) event pronouns.
Error Type (Ratio)
He said Timmons aimed and missed his target.
Phrasal verbs The meaning of a phrasal verb is not captured.
(6%)
A relative unknown will take over the title role of Doctor Who ... But the casting of Smith is
a stroke of genius.
Linear regression Recall error made by the regression model when the features are otherwise correct.
(4%)
</figure>
<table confidence="0.632447375">
The Interior Department on Thursday issued “revised” regulations ...Interior Secretary Dirk
Kempthorne announced major changes ...
Mention detection The mention detection module detects a spurious mention.
(3%)
Police have arrested a man ... in the parking lot crosswalk at Sam’s Club in Bloomington.
SRL The SRL system fails to label the semantic role. In this example, jail is detected as the ArgM-
(1%) MNR of hanged instead of ArgM-LOC.
A Mafia boss in Palermo hanged himself in jail.
</table>
<tableCaption confidence="0.9783035">
Table 6: Error analysis. Mentions to be resolved are in bold face, correct antecedents are in italics, and our system’s
predictions are underlined.
</tableCaption>
<bodyText confidence="0.787400076923077">
merging state to benefit from the previous merged world applications such as multi-document summa-
entity and event mentions. This approach allows us rization and cross-document information extraction.
to start with a set of high-precision coreference rela- We also release our labeled corpus to facilitate ex-
tions and gradually add new ones to increase recall. tensions and comparisons to our work.
Semantics beyond
role frames
(20%)
The semantics of the coreference relation cannot be captured by role frames or WordNet.
Israeli forces on Tuesday killed at least 40 people ... The Israeli army said the UN school in the
Jabaliya refugee camp was hit ... and that the dead included a number of Hamas militants.
The arguments of two nominal events are not detected and thus not coreferred.
The attack on the school has caused widespread shock across Israel ... while Israeli forces on
Tuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.
</bodyText>
<figure confidence="0.954177">
Arguments of
nominal events
(17%)
Cascaded errors Entities or events are not coreferred due to errors in a previous merge iteration in the same
(7%) semantic frame. In the example below, we failed to link the two die verbs, which leads to the
listed entity error.
An Australian climber who survived two nights stuck on Mount Cook after seeing his brother
die ... Dr Mark Vinar, 43, is presumed dead ...
Initial high-precision
sieves
(6%)
An error made by the initial high-precision entity resolution sieves is propagated to our model.
Timmons told police he fired when he thought he saw someone in the other group reach for
a gun ...15-year-old Timmons was at the scene of the shooting and had a gun.
</figure>
<bodyText confidence="0.999800769230769">
The experimental evaluation shows that our coref-
erence algorithm gives markedly better F1 for both
entities and events, outperforming two strong base-
lines that handle entities and events separately, mea-
sured by all the standard measures: MUC, B3,
CEAF-04, BLANC and the official CoNLL-2011
metric. This is noteworthy since each measure has
been shown to place primary emphasis in evaluating
a different aspect of the coreference resolution task.
Our system is tailored for cross-document coref-
erence resolution on a corpus that contains news ar-
ticles that repeatedly report on a smaller number of
topics. This makes it particularly suitable for real-
</bodyText>
<sectionHeader confidence="0.996492" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999002583333333">
We acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of the DARPA, AFRL, or the US
government. MR is supported by a Beatriu de Pin´os post-
doctoral scholarship (2010 BP-A 00149) from Generali-
tat de Catalunya. AC is supported by a SAP Stanford
Graduate Fellowship. We also gratefully thank Cosmin
Bejan for sharing his code and the useful discussions.
</bodyText>
<page confidence="0.998475">
498
</page>
<sectionHeader confidence="0.998293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999633401960784">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563–566.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the ACL 1999 Workshop
on Coreference and Its Applications, pages 1–8.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412–1422.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part of speech tagging. Computational Linguistics,
21(4):543–565.
Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the ACL-
IJCNLP 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54–57.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP 2009, pages 1152–1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL 2010, pages 385–393.
Tian He. 2007. Coreference Resolution on Entities and
Events for Hospital Discharge Summaries. Thesis,
Massachusetts Institute of Technology.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311–338.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop On Operational
Factors In Practical Robust Anaphora Resolution For
Unrestricted Texts, pages 75–81.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to its Methodology. Sage, Thousand Oaks,
CA, second edition.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of CoNLL 2011: Shared Task, pages 28–34.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
pages 768–774.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25–32.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: an interim report. In Proceedings of the
HLT-NAACL 2004 Workshop on Frontiers in Corpus
Annotation, pages 24–31.
Rebecca Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of LREC
2004, pages 1503–1506.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL 2006, pages 192–199.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of EMNLP 2008, pages 650–659.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in OntoNotes. In Proceedings of ICSC 2007, pages
446–453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011: Shared Task, pages 1–27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Chris Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
EMNLP 2010, pages 492–501.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings of
ACL 2011, pages 814–824.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846–850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485–510.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings ofACL-IJCNLP 2009, pages 656–664.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005.
A hybrid unsupervised approach for document cluster-
ing. In Proceedings of KDD 2005, pages 685–690.
</reference>
<page confidence="0.989476">
499
</page>
<reference confidence="0.999707789473684">
Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105–151.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45–52.
Bonnie Lynn Webber. 1988. Discourse deixis: reference
to discourse segments. In Proceedings of ACL 1988,
pages 113–122.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of KDD 2008, pages 722–
730.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL 1995, pages 189–196.
</reference>
<page confidence="0.995599">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865995">
<title confidence="0.99907">Joint Entity and Event Coreference Resolution across Documents</title>
<author confidence="0.953891">Heeyoung Lee</author>
<author confidence="0.953891">Marta Recasens</author>
<author confidence="0.953891">Angel Chang</author>
<author confidence="0.953891">Mihai Surdeanu</author>
<author confidence="0.953891">Dan</author>
<affiliation confidence="0.893427">Stanford University, Stanford, CA 94305</affiliation>
<abstract confidence="0.999645611111111">We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC 1998 Workshop on Linguistic Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="29828" citStr="Bagga and Baldwin, 1998" startWordPosition="4834" endWordPosition="4837">4.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2 Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8 Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B3, and CEAF-04. This was the official metric in the CoNLL-2011 shared task (Pradhan e</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the LREC 1998 Workshop on Linguistic Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Cross-document event coreference: Annotations, experiments, and observations.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL 1999 Workshop on Coreference and Its Applications,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6159" citStr="Bagga and Baldwin, 1999" startWordPosition="947" endWordPosition="950">by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only eva</context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Amit Bagga and Breck Baldwin. 1999. Cross-document event coreference: Annotations, experiments, and observations. In Proceedings of the ACL 1999 Workshop on Coreference and Its Applications, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised Event Coreference Resolution with Rich Linguistic Features.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>1412--1422</pages>
<contexts>
<context position="6395" citStr="Bejan and Harabagiu (2010)" startWordPosition="983" endWordPosition="987">ormance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolution jointly is He (2007), but limited to the medical domain and focused on just five sema</context>
<context position="24901" citStr="Bejan and Harabagiu (2010)" startWordPosition="4022" endWordPosition="4026">the suffix Proper only if both head words are proper nouns. paper we used a single heuristic: the possessor of a nominal event’s predicate is marked as its Arg0, e.g., Logan is the Arg0 to run in Logan’s run.4 4A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004). We will address this in future work. 494 We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite. 6 Evaluation 6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al., 2007) contain a small number of annotated event clusters. The ECB corpus consists of 482 documents from Google News clustered into 43 topics, where a topic is described as a seminal event. The reason for including comparable documents was to increase the number of cross-document coreference relations. Bejan and Harabagiu (2010) only annotated a selection of events. For the purpose of our study, we extended the original corpus in two directions: (i) fully annotated sentences, and (ii) entity coreference relati</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Bejan and Sanda Harabagiu. 2010. Unsupervised Event Coreference Resolution with Rich Linguistic Features. In Proceedings of ACL 2010, pages 1412–1422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="18618" citStr="Brill, 1995" startWordPosition="3000" endWordPosition="3001">s an atomic unit. On the other hand, our approach generates more training data than online learning, which trains using only the actual decisions taken during inference in each iteration (i.e., 2We skip the pronoun sieve here because it does not affect the decisions taken during the iterative resolution steps. the pair (e1, e2) in step 13). After each epoch we have a new training corpus F, which we use to train the new linear regression model O’ (step 15), which is then interpolated with the old one (step 16). Our training procedure is similar in spirit to transformation based learning (TBL) (Brill, 1995). Similarly to TBL, our approach repeatedly applies the model over the training data and attempts to minimize the error rate of the current model. However, while TBL learns rules that directly minimize the current error rate, our approach achieves this indirectly, by incorporating the reduction in error rate in the score of the generated datums. This allows us to fit a linear regression to this task, which, as discussed before, is a better model for this task. Just like any hill-climbing algorithm, our approach has the risk of converging to a local maximum. To mitigate this risk, we do not ini</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Graph-based event coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>54--57</pages>
<contexts>
<context position="6308" citStr="Chen and Ji (2009)" startWordPosition="970" endWordPosition="973">ow that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolutio</context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Zheng Chen and Heng Ji. 2009. Graph-based event coreference resolution. In Proceedings of the ACLIJCNLP 2009 Workshop on Graph-based Methods for Natural Language Processing, pages 54–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACLHLT</booktitle>
<contexts>
<context position="4983" citStr="Denis and Baldridge, 2007" startWordPosition="756" endWordPosition="759">well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity men</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proceedings of NAACLHLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="21341" citStr="Fellbaum, 1998" startWordPosition="3448" endWordPosition="3449"> words of all mentions in a cluster and their frequencies. For example, the vector for the three-mention cluster {Barack Obama, President Obama, US president}, is {Obama:2, president:1}. Cosine similarity of the lemma vectors of two clusters. For example, the lemma Event Lemmas V vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}. Links between E, V Synonyms Coreferent Arguments E, V in a Specific Role? Coreferent Predicate in E a Specific Role? Number; Animacy; E Gender; NE Label The percentage of newly-introduced mention links after the merge that are WordNet synonyms (Fellbaum, 1998). For example, when merging the following two clusters, {hit, strike} and {strike, join, say}, two out of the six new links are between words that belong to the same WordNet synset: (hit – strike) and (strike – strike). The total number of shared arguments and predicates between mentions in the two clusters. We use the cluster IDs of the corresponding arguments/predicates to check for identity. For example, when comparing the event clusters {bought} and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and [AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because t</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1152--1161</pages>
<contexts>
<context position="4496" citStr="Haghighi and Klein, 2009" startWordPosition="682" endWordPosition="685">cies. • We annotate and release a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform co</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP 2009, pages 1152–1161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>385--393</pages>
<contexts>
<context position="4545" citStr="Haghighi and Klein, 2010" startWordPosition="690" endWordPosition="693"> coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proceedings of HLT-NAACL 2010, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian He</author>
</authors>
<title>Coreference Resolution on Entities and Events for Hospital Discharge Summaries. Thesis,</title>
<date>2007</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="6930" citStr="He (2007)" startWordPosition="1073" endWordPosition="1074">ng spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolution jointly is He (2007), but limited to the medical domain and focused on just five semantic categories. 3 Architecture Following the intuition introduced in Section 1, our approach iteratively builds clusters of event and entity mentions jointly. As more information becomes available (e.g., finding out that two verbal mentions have arguments that belong to the same entity cluster), the features of both entity and event mentions are re-generated, which prompts future clustering operations. Our model follows a cautious (or “baby steps”) approach, which we previously showed to be successful for entity coreference reso</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Tian He. 2007. Coreference Resolution on Entities and Events for Hospital Discharge Summaries. Thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="15111" citStr="Hobbs, 1978" startWordPosition="2392" endWordPosition="2393">hat these two nominal mentions belong to the same cluster. Indeed, the feature that models this structure received one of the highest weights in our linear regression model (see Section 7). 3.5 Pronoun Sieve Our approach concludes with the pronominal coreference resolution sieve from the Stanford system. This sieve is necessary because our current resolution algorithm ignores mention ordering and distance (i.e., in step 7 we compare all clusters regardless of where their mentions appear in the text). As previous work has proved, the structure of the text is crucial for pronominal coreference (Hobbs, 1978). For this reason, we handle pronouns outside of the main algorithm block. 4 Training the Cluster Merging Model Two observations drove our choice of model and training algorithm. First, modeling the merge operation as a classification task is not ideal, because only a few of the resulting clusters are entirely correct or incorrect. In practice, most of the clusters will contain some mention pairs that are correct and some that are not. Second, generating training data for the merging model is not trivial: a brute force approach that looks at all the possible combinations is exponential in the </context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Jerry R. Hobbs. 1978. Resolving pronoun references. Lingua, 44(4):311–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Humphreys</author>
<author>Robert Gaizauskas</author>
<author>Saliha Azzam</author>
</authors>
<title>Event coreference for information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop On Operational Factors In Practical Robust Anaphora Resolution For Unrestricted Texts,</booktitle>
<pages>75--81</pages>
<contexts>
<context position="6133" citStr="Humphreys et al., 1997" startWordPosition="943" endWordPosition="946">ent-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, t</context>
</contexts>
<marker>Humphreys, Gaizauskas, Azzam, 1997</marker>
<rawString>Kevin Humphreys, Robert Gaizauskas, and Saliha Azzam. 1997. Event coreference for information extraction. In Proceedings of the Workshop On Operational Factors In Practical Robust Anaphora Resolution For Unrestricted Texts, pages 75–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology.</title>
<date>2004</date>
<location>Sage, Thousand Oaks, CA,</location>
<note>second edition.</note>
<contexts>
<context position="27860" citStr="Krippendorff, 2004" startWordPosition="4479" endWordPosition="4480"> Light verbs Verbs such as give and make followed by a noun (e.g., make an offer) were not annotated, but the noun was. Phrasal verbs We annotated the verb together with the preposition or adverb (e.g., check in). Idioms They were annotated with all their elements (e.g., booze it up). The first topic was annotated by all four annotators as burn-in. Afterwards, annotation disagreements were resolved between all annotators and the next three topics were annotated again by all four annotators to measure agreement. Following Passonneau (2004), we computed an inter-annotator agreement of α = 0.55 (Krippendorff, 2004) on these three topics, indicating moderate agreement among the annotators. Given the complexity of the task, we consider this to be a good score. For example, the average of the CoNLL F1 between any two annotators is 73.58, which is much higher than the system scores reported in the literature. After annotating the four topics, disagreements were resolved again and all the documents in the four topics were corrected to match the consensus. The rest of the corpus was split between the four annotators, and each document was annotated by a single annotator. Figure 1 shows an example. Table 3 sho</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to its Methodology. Sage, Thousand Oaks, CA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL 2011: Shared Task,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="5045" citStr="Lee et al., 2011" startWordPosition="768" endWordPosition="771">ntion clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. We conf</context>
<context position="7581" citStr="Lee et al., 2011" startWordPosition="1171" endWordPosition="1174">n and focused on just five semantic categories. 3 Architecture Following the intuition introduced in Section 1, our approach iteratively builds clusters of event and entity mentions jointly. As more information becomes available (e.g., finding out that two verbal mentions have arguments that belong to the same entity cluster), the features of both entity and event mentions are re-generated, which prompts future clustering operations. Our model follows a cautious (or “baby steps”) approach, which we previously showed to be successful for entity coreference resolution (Raghunathan et al., 2010; Lee et al., 2011). However, unlike our previous work, which used deterministic rules, in this paper we learn a coreference resolution model using linear regression. Algorithm 1 summarizes the flow of the proposed algorithm. We detail its steps next. We describe the training procedure in Section 4 and the features used in Section 5. 490 Algorithm 1: Joint Coreference Resolution input : set of documents D input : coreference model O // clusters of mentions: 1 E= �� //clusters of documents: 2 C = clusterDocuments(D) 3 foreach document cluster c in C do // all mentions in one doc cluster: 4 M = extractMentions(c) </context>
<context position="10372" citStr="Lee et al., 2011" startWordPosition="1624" endWordPosition="1627">ay in the same topic might refer to different events, they are only merged if they have coreferent arguments. ometric heuristics. This algorithm performs well on our data. For example, in the training dataset, only two topics (handling different earthquake events) are incorrectly merged into the same cluster. 3.2 Mention Extraction In this step (4 in Algorithm 1) we extract nominal, pronominal, and verbal mentions. We extract nominal and pronominal mentions using the mention identification component in the publicly downloadable Stanford coreference resolution system (Raghunathan et al., 2010; Lee et al., 2011). We consider as verbal mentions all words whose part of speech starts with VB, with the exception of some auxiliary/copulative verbs (have, be and seem). For each of the identified mentions we build a singleton cluster (step 5 in Algorithm 1). Crucially, we do not make a formal distinction between entity and event mentions. This distinction is not trivial to implement (e.g., is the noun earthquake an entity or an event mention?) and an imperfect classification would negatively affect the following coreference resolution. Instead, we simply classify mentions into verbal or nominal, and use thi</context>
<context position="33646" citStr="Lee et al., 2011" startWordPosition="5457" endWordPosition="5460">role (Arg1) predicates with the same lemma (elect). The second sieve implements the complementary action for event clusters. That is, it merges two verbal clusters when at least two mentions have the same lemma and at least two mentions have semantic arguments with the same role label and the same lemma. 7 Discussion The first block in Table 4 indicates that lemma matching is a strong baseline for event resolution. Most of the event scores for Baseline 1 are actually higher than the corresponding entity scores, which were obtained using the highest ranked system at the CoNLL-2011 shared task (Lee et al., 2011). Adding contextual information using semantic roles (Baseline 2) helps both entities and events. The CoNLL F1 for Baseline 2 increases almost 3 points for entities and 1 point for events. This demonstrates that local syntactico-semantic context is important for coreference resolution even in a cross-document setting and that the current state-of-the-art in SRL can model this context accurately. The best scores (almost unanimously) are obtained by the model proposed in this paper, which scores 3.4 CoNLL F1 points higher than Baseline 2 for entities, and 2.6 points higher for events. For the co</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of CoNLL 2011: Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>768--774</pages>
<contexts>
<context position="23195" citStr="Lin, 1998" startWordPosition="3750" endWordPosition="3751">ught} and {acquired} in the above example is 1. Indicator feature set to 1 if the two clusters have at least one coreferent predicate for a given role. For example, for the clusters {the man} and {the person}, extracted from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of this feature is 1 if the two helped verbs were previously clustered together. Cosine similarity of vectors containing words that are distributionally similar to words in the cluster mentions. We built these vectors by extracting the top-ten most-similar words in Dekang Lin’s similarity thesaurus (Lin, 1998) for all the nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new home}, we construct this vector by expanding new and home to: {new:1, original:1, old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1, small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1, mansion:1, school:1, restaurant:1, hospital:1 }. Cosine similarity of number, gender, animacy, and NE label vectors. For example, the number and gender vectors for the two-mention cluster {systems, a pen} are Number = {singular:1, plural:1}, Gender = {neut</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 1998, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>25--32</pages>
<contexts>
<context position="29964" citStr="Luo, 2005" startWordPosition="4857" endWordPosition="4858">45.2 30.0 35.8 67.1 82.2 72.3 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B3, and CEAF-04. This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular rela</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT-EMNLP 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank project: an interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="24614" citStr="Meyers et al., 2004" startWordPosition="3976" endWordPosition="3979">bal mention we consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in each of the two clusters. We use the suffix Proper only if both head words are proper nouns. paper we used a single heuristic: the possessor of a nominal event’s predicate is marked as its Arg0, e.g., Logan is the Arg0 to run in Logan’s run.4 4A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004). We will address this in future work. 494 We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite. 6 Evaluation 6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al., 2007) contain a small number of annotated event clusters. The ECB corpus consists of 482 documents from Google News clustered into 43 topics, where a topic is described as a seminal event. The reason for including comparable do</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: an interim report. In Proceedings of the HLT-NAACL 2004 Workshop on Frontiers in Corpus Annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
</authors>
<title>Computing reliability for coreference annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1503--1506</pages>
<contexts>
<context position="27785" citStr="Passonneau (2004)" startWordPosition="4466" endWordPosition="4468">ENT) (ENTITY COREFID=“28”) rehab (/ENTITY). Figure 1: Annotation example. Light verbs Verbs such as give and make followed by a noun (e.g., make an offer) were not annotated, but the noun was. Phrasal verbs We annotated the verb together with the preposition or adverb (e.g., check in). Idioms They were annotated with all their elements (e.g., booze it up). The first topic was annotated by all four annotators as burn-in. Afterwards, annotation disagreements were resolved between all annotators and the next three topics were annotated again by all four annotators to measure agreement. Following Passonneau (2004), we computed an inter-annotator agreement of α = 0.55 (Krippendorff, 2004) on these three topics, indicating moderate agreement among the annotators. Given the complexity of the task, we consider this to be a good score. For example, the average of the CoNLL F1 between any two annotators is 73.58, which is much higher than the system scores reported in the literature. After annotating the four topics, disagreements were resolved again and all the documents in the four topics were corrected to match the consensus. The rest of the corpus was split between the four annotators, and each document </context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>Rebecca Passonneau. 2004. Computing reliability for coreference annotation. In Proceedings of LREC 2004, pages 1503–1506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>192--199</pages>
<contexts>
<context position="4470" citStr="Ponzetto and Strube, 2006" startWordPosition="678" endWordPosition="681">sing semantic role dependencies. • We annotate and release a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural </context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of HLT-NAACL 2006, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>650--659</pages>
<contexts>
<context position="5008" citStr="Poon and Domingos, 2008" startWordPosition="760" endWordPosition="763">any successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity mentions can have and the ve</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of EMNLP 2008, pages 650–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
<author>Jessica MacBride</author>
<author>Linnea Micciulla</author>
</authors>
<title>Unrestricted coreference: Identifying entities and events in OntoNotes.</title>
<date>2007</date>
<booktitle>In Proceedings of ICSC</booktitle>
<pages>446--453</pages>
<contexts>
<context position="24992" citStr="Pradhan et al., 2007" startWordPosition="4037" endWordPosition="4040">e possessor of a nominal event’s predicate is marked as its Arg0, e.g., Logan is the Arg0 to run in Logan’s run.4 4A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004). We will address this in future work. 494 We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite. 6 Evaluation 6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al., 2007) contain a small number of annotated event clusters. The ECB corpus consists of 482 documents from Google News clustered into 43 topics, where a topic is described as a seminal event. The reason for including comparable documents was to increase the number of cross-document coreference relations. Bejan and Harabagiu (2010) only annotated a selection of events. For the purpose of our study, we extended the original corpus in two directions: (i) fully annotated sentences, and (ii) entity coreference relations. In addition, we removed relations other than coreference (e.g., subevent, purpose, rel</context>
</contexts>
<marker>Pradhan, Ramshaw, Weischedel, MacBride, Micciulla, 2007</marker>
<rawString>Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel, Jessica MacBride, and Linnea Micciulla. 2007. Unrestricted coreference: Identifying entities and events in OntoNotes. In Proceedings of ICSC 2007, pages 446–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL 2011: Shared Task,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="30440" citStr="Pradhan et al., 2011" startWordPosition="4928" endWordPosition="4931">in, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B3, and CEAF-04. This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. Note that the gold corpus separates clusters into entity and event clusters (see Table 3), but our system does not make this distinction at runtime. In order to compute the entity-only and event-only scores in Table 4, we implemented the following procedure: (a) when scoring entity clusters, </context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of CoNLL 2011: Shared Task, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Chris Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>492--501</pages>
<contexts>
<context position="4571" citStr="Raghunathan et al., 2010" startWordPosition="694" endWordPosition="697">ween both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been littl</context>
<context position="7562" citStr="Raghunathan et al., 2010" startWordPosition="1166" endWordPosition="1170">mited to the medical domain and focused on just five semantic categories. 3 Architecture Following the intuition introduced in Section 1, our approach iteratively builds clusters of event and entity mentions jointly. As more information becomes available (e.g., finding out that two verbal mentions have arguments that belong to the same entity cluster), the features of both entity and event mentions are re-generated, which prompts future clustering operations. Our model follows a cautious (or “baby steps”) approach, which we previously showed to be successful for entity coreference resolution (Raghunathan et al., 2010; Lee et al., 2011). However, unlike our previous work, which used deterministic rules, in this paper we learn a coreference resolution model using linear regression. Algorithm 1 summarizes the flow of the proposed algorithm. We detail its steps next. We describe the training procedure in Section 4 and the features used in Section 5. 490 Algorithm 1: Joint Coreference Resolution input : set of documents D input : coreference model O // clusters of mentions: 1 E= �� //clusters of documents: 2 C = clusterDocuments(D) 3 foreach document cluster c in C do // all mentions in one doc cluster: 4 M = </context>
<context position="10353" citStr="Raghunathan et al., 2010" startWordPosition="1619" endWordPosition="1623">ent mentions of the verb say in the same topic might refer to different events, they are only merged if they have coreferent arguments. ometric heuristics. This algorithm performs well on our data. For example, in the training dataset, only two topics (handling different earthquake events) are incorrectly merged into the same cluster. 3.2 Mention Extraction In this step (4 in Algorithm 1) we extract nominal, pronominal, and verbal mentions. We extract nominal and pronominal mentions using the mention identification component in the publicly downloadable Stanford coreference resolution system (Raghunathan et al., 2010; Lee et al., 2011). We consider as verbal mentions all words whose part of speech starts with VB, with the exception of some auxiliary/copulative verbs (have, be and seem). For each of the identified mentions we build a singleton cluster (step 5 in Algorithm 1). Crucially, we do not make a formal distinction between entity and event mentions. This distinction is not trivial to implement (e.g., is the noun earthquake an entity or an event mention?) and an imperfect classification would negatively affect the following coreference resolution. Instead, we simply classify mentions into verbal or n</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Chris Manning. 2010. A multi-pass sieve for coreference resolution. In Proceedings of EMNLP 2010, pages 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>814--824</pages>
<contexts>
<context position="4592" citStr="Rahman and Ng, 2011" startWordPosition="698" endWordPosition="701">nts across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this dir</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of ACL 2011, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="30190" citStr="Rand, 1971" startWordPosition="4891" endWordPosition="4892"> evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B3, and CEAF-04. This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. Note that the gold corpus separates cluster</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="30146" citStr="Recasens and Hovy, 2011" startWordPosition="4881" endWordPosition="4884">sing five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B3, and CEAF-04. This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events.</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(4):485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP 2009,</booktitle>
<pages>656--664</pages>
<contexts>
<context position="4519" citStr="Stoyanov et al., 2009" startWordPosition="686" endWordPosition="689">lease a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly acros</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proceedings ofACL-IJCNLP 2009, pages 656–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Alicia Ageno</author>
</authors>
<title>A hybrid unsupervised approach for document clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of KDD</booktitle>
<pages>685--690</pages>
<contexts>
<context position="9499" citStr="Surdeanu et al. (2005)" startWordPosition="1490" endWordPosition="1493">ppear in the same document cluster. We found this to be very useful in practice because, in addition to reducing the search space, it provides a word sense disambiguation mechanism based on corpus-wide topics. For example, without document clustering, our algorithm may decide to cluster two mentions of the verb hit, but knowing that one belongs to a cluster containing earthquake reports and the other to a cluster with reports on criminal activities, this decision can be avoided.1 Any non-parametric clustering algorithm can be used in this step. In this paper, we used the algorithm proposed by Surdeanu et al. (2005). This algorithm is an Expectation Maximization (EM) variant where the initial points (and the number of clusters) are selected from the clusters generated by a hierarchical agglomerative clustering algorithm using ge1Since different mentions of the verb say in the same topic might refer to different events, they are only merged if they have coreferent arguments. ometric heuristics. This algorithm performs well on our data. For example, in the training dataset, only two topics (handling different earthquake events) are incorrectly merged into the same cluster. 3.2 Mention Extraction In this st</context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2005</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005. A hybrid unsupervised approach for document clustering. In Proceedings of KDD 2005, pages 685–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>29--105</pages>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and Pere R. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, 29:105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="29655" citStr="Vilain et al., 1995" startWordPosition="4806" endWordPosition="4809">57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2 Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6 This paper Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2 Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8 Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to add</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Lynn Webber</author>
</authors>
<title>Discourse deixis: reference to discourse segments.</title>
<date>1988</date>
<booktitle>In Proceedings of ACL 1988,</booktitle>
<pages>113--122</pages>
<contexts>
<context position="1741" citStr="Webber, 1988" startWordPosition="262" endWordPosition="263"> (NPs). Focusing on NPs is a way to restrict the challenging problem of coreference resolution, but misses coreference relations like the one between hanged and his suicide in (1), and between placed and put in (2). 1. (a) One of the key suspected Mafia bosses arrested yesterday has hanged himself. (b) Police said Lo Presti had hanged himself. (c) His suicide appeared to be related to clan feuds. 2. (a) The New Orleans Saints placed Reggie Bush on the injured list on Wednesday. (b) Saints put Bush on I.R. As (1c) shows, NPs can also refer to events, and so corefer with phrases other than NPs (Webber, 1988). By being anchored in spatio-temporal dimensions, events represent the most frequent referent of verbal elements. In addition to time and location, events are characterized by their participants or arguments, which often correspond with discourse entities. This two-way feedback between events and their arguments (or entities) is the core of our approach. Since arguments play a key role in describing an event, knowing that two arguments corefer is useful for finding coreference relations between events, and knowing that two events corefer is useful for finding coreference relations between ent</context>
</contexts>
<marker>Webber, 1988</marker>
<rawString>Bonnie Lynn Webber. 1988. Discourse deixis: reference to discourse segments. In Proceedings of ACL 1988, pages 113–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Karl Schultz</author>
<author>Andrew McCallum</author>
</authors>
<title>A unified approach for schema matching, coreference and canonicalization.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD</booktitle>
<pages>722--730</pages>
<contexts>
<context position="5027" citStr="Wick et al., 2008" startWordPosition="764" endWordPosition="767"> for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their p</context>
</contexts>
<marker>Wick, Rohanimanesh, Schultz, McCallum, 2008</marker>
<rawString>Michael L. Wick, Khashayar Rohanimanesh, Karl Schultz, and Andrew McCallum. 2008. A unified approach for schema matching, coreference and canonicalization. In Proceedings of KDD 2008, pages 722– 730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>189--196</pages>
<contexts>
<context position="12741" citStr="Yarowsky, 1995" startWordPosition="1998" endWordPosition="1999"> 11 491 High-precision sieves Discourse processing sieve Exact string match sieve Relaxed string match sieve Precise constructs sieve (e.g., appositives) Strict head match sieves Proper head noun match sieve Relaxed head matching sieve Table 1: Deterministic sieves in step 6 of Algorithm 1. one sieve clusters together two entity mentions only when they have the same head word. Note that all these heuristics were designed for within-document coreference. They work well in our context because we apply them in individual document clusters, where the one-sense-per-discourse principle still holds (Yarowsky, 1995). Importantly, these sieves do not address verbal mentions. That is, all verbal mentions are still in singleton clusters after this step. Furthermore, none of these sieves use features that facilitate the joint resolution of nominal and verbal mentions (e.g., features from semantic role frames). All these limitations are addressed next. 3.4 Iterative Entity/Event Resolution In this stage (steps 7 – 9 in Algorithm 1), we construct entity and event clusters using a cautious or “baby steps” approach. We use a single linear regressor (0) to model cluster merge operations between both verbal and no</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of ACL 1995, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
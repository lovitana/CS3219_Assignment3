<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.8557185">
Hierarchical Latent Words Language Models for Robust Modeling
to Out-Of Domain Tasks
</title>
<author confidence="0.561488">
Ryo Masumura†‡, Taichi Asami†, Takanobu Oba†,
</author>
<affiliation confidence="0.4418395">
Hirokazu Masataki†, Sumitaka Sakauchi†, Akinori Ito‡† NTT Media Intelligence Laboratories, NTT Corporation, Japan
‡ Graduate School of Engineering, Tohoku University, Japan
</affiliation>
<address confidence="0.500375">
t{masumura.ryo, asami.taichi, oba.takanobu, masataki.hirokazu,
</address>
<email confidence="0.74316">
sakauchi.sumitakal @lab.ntt.co.jp, t aito@spcom.ecei.tohoku.ac.jp
</email>
<sectionHeader confidence="0.993733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818333333333">
This paper focuses on language modeling
with adequate robustness to support differ-
ent domain tasks. To this end, we propose
a hierarchical latent word language model
(h-LWLM). The proposed model can be
regarded as a generalized form of the stan-
dard LWLMs. The key advance is in-
troducing a multiple latent variable space
with hierarchical structure. The structure
can flexibly take account of linguistic phe-
nomena not present in the training data.
This paper details the definition as well
as a training method based on layer-wise
inference and a practical usage in natural
language processing tasks with an approx-
imation technique. Experiments on speech
recognition show the effectiveness of h-
LWLM in out-of domain tasks.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951967213115">
Language models (LMs) are essential for auto-
matic speech recognition or statistical machine
translation (Rosenfeld, 2000). The performance of
LMs strongly depends on quality and quantity of
their training data. Superior performance is usu-
ally obtained by using enormous domain-matched
training data sets to construct LMs (Brants et
al., 2007). Unfortunately, in many cases, large
amounts of domain-matched training data sets are
not available. Therefore, LM technology that can
robustly work for domains that differ from that of
the training data is needed (Goodman, 2001).
For robust language modeling, several tech-
nologies have been proposed. Fundamental tech-
niques are smoothing (Chen and Goodman, 1999)
and clustering (Brown et al., 1992). Other solu-
tions are Bayesian modeling (Teh, 2006) and en-
semble modeling (Xu and Jelinek, 2004; Emami
and Jelinek, 2005). Moreover, continuous rep-
resentation of words in neural network LMs can
also support robust modeling (Bengio et al., 2003;
Mikolov et al., 2010). However, previous works
are focused on maximizing performance in the
same domain as that of the training data. In other
words, it is uncertain that these technologies ro-
bustly support out-of domain tasks.
In contrast, latent words LMs (LWLMs) (De-
schacht et al., 2012) are clearly effective for out-
of domain tasks. We employed the LWLM to
speech recognition and the resulting performance
was significantly superior in out-of domain tasks
while the performance was comparable in domain-
matched task to conventional LMs (Masumura et
al., 2013a; Masumura et al., 2013b). LWLMs
are generative models that employ a latent word
space. The latent space can flexibly take into ac-
count relationships between words and the model-
ing helps to efficiently increase the robustness to
out-of domain tasks (Sec. 2).
In this paper, we focus on LWLMs and aim to
make them more flexible for greater robustness to
out-of domain tasks. To this end, this paper takes
note of a fact that standard LWLM simply repre-
sents the latent space as n-gram model of latent
words. However, function and meaning of words
are essentially hierarchical and upper layers ought
to be useful to increase the robustness to out-of
domain tasks. The conventional LWLMs do not
model the hierarchy, while the latent words are
used to represent function and meaning of words.
Thus, we tried to model the hierarchy in the latent
space by estimating a latent word of a latent word
recursively.
This paper proposes a novel LWLM with mul-
tiple latent word spaces that are hierarchically
structured; we call it the hierarchical LWLM (h-
LWLM). The proposed model can be regarded
as a generalized form of the standard LWLMs.
The hierarchical structure can take into account
the abstraction process of function and meaning
of words. Therefore, it can be expected that h-
</bodyText>
<page confidence="0.924691">
1896
</page>
<note confidence="0.651115">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1896–1901,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999964294117647">
LWLMs flexibly calculate generative probability
for unseen words unlike non-hierarchical LWLMs.
To create the hierarchical latent word structure
from training data sets, we also propose a layer-
wise inference. The inference is inspired by a
deep Boltzmann machine (Salakhutdinov and Hin-
ton, 2009) that stacks up restricted Boltzmann ma-
chines (Hinton et al., 2006). In addition, we detail
an n-gram approximation technique to apply the
proposed model to practical natural language pro-
cessing tasks (see Sec. 3).
In experiments, we construct LMs from sponta-
neous lecture task data and apply them to a contact
center dialogue task and a voice mail task as out-
of domain tasks. The effectiveness of the proposed
method is shown by perplexity and speech recog-
nition evaluation (Sec. 4).
</bodyText>
<sectionHeader confidence="0.951645" genericHeader="method">
2 Latent Words Language Models
</sectionHeader>
<bodyText confidence="0.998660125">
LWLMs are generative models with single latent
word space (Deschacht et al., 2012). The latent
word is represented as a specific word that is se-
lected from the entire vocabulary. Thus, the num-
ber of latent words equals the number of observed
words.
Bayesian modeling of LWLM produces the gen-
erative probability of observed word sequence
</bodyText>
<equation confidence="0.9936848">
w = w1,···,wK as:
9 K
P(w) = f Y XP(wk  |hk, θ)
k=1 hk
P(hk|lk, θ)P(θ)dθ, (1)
</equation>
<bodyText confidence="0.976720272727273">
where θ indicates a model parameter of the
LWLM, h = h1, · · · , hK denotes a latent
word sequence and lk denotes context latent
words hk−n+1, · · · , hk−1. P(hk|lk, θ) repre-
sents the transition probability which can be ex-
pressed by an n-gram model for latent words, and
P(wk|hk, θ) represents the emission probability
that models the dependency between the observed
word and the latent word. More details are shown
in previous works (Deschacht et al., 2012; Ma-
sumura et al., 2013a; Masumura et al., 2013b).
</bodyText>
<sectionHeader confidence="0.963999" genericHeader="method">
3 Hierarchical LWLMs
</sectionHeader>
<subsectionHeader confidence="0.992827">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.999357333333333">
This paper introduces h-LWLM. The proposed
model has multiple latent word spaces in a hier-
archical structure. Thus, it assumes that there is
</bodyText>
<figureCaption confidence="0.997021">
Figure 1: Graphical representation of h-LWLM.
</figureCaption>
<bodyText confidence="0.9999440625">
a latent word behind a latent word. The proposed
model can be regarded as a generalized form of
the standard LWLM. Thus, standard LWLMs cor-
respond to h-LWLMs with just one layer. The la-
tent words in all layers are represented as a specific
word that is selected from the entire vocabulary.
A graphic rendering of h-LWLM is shown in
Figure 1. In a generative process of the h-LWLM,
a latent word in the highest layer is first generated
depending on its context latent words. Next, a la-
tent word in a lower layer is recursively generated
depending on the latent word in the upper layer.
Finally, an observed word is generated depending
on the latent word in the lowest layer.
Bayesian modeling of h-LWLM produces the
following generative probability:
</bodyText>
<equation confidence="0.99294">
X···XP (wk |hk1),Θ)···
k=1 h(1) h(�)
k k
P(h(M−1)
k |h(M)
k , Θ)P(h(M)
k |l(M)
k , Θ)P(Θ)dΘ,
(2)
</equation>
<bodyText confidence="0.935075333333333">
where M is the number of layers and Θ indi-
cates a model parameter of h-LWLM. h(m) =
h(m)
</bodyText>
<equation confidence="0.938238">
1 , · · · , h(m)
</equation>
<bodyText confidence="0.707451">
K denotes a latent word sequence
in the m-th layer. P(h(M)
</bodyText>
<equation confidence="0.759195">
k |l(M)
</equation>
<bodyText confidence="0.841751">
k , Θ) represents
the transition probability which is expressed by n-
gram model for latent words in the highest layer.
</bodyText>
<equation confidence="0.936890666666667">
P(h(m)
k |h(m+1)
k ,Θ) and P(wk|h(1)
</equation>
<bodyText confidence="0.921345">
k , Θ) represent
the emission probabilities that respectively model
the dependency between latent words in two layers
and the dependency between the observed word
and the latent word in the lowest layer.
As the integral with respect to Θ is analytically
</bodyText>
<equation confidence="0.977984">
Z YK
P(w) =
�
</equation>
<page confidence="0.965515">
1897
</page>
<figureCaption confidence="0.998627">
Figure 2: Layer-wise inference procedure.
</figureCaption>
<bodyText confidence="0.7958836">
Algorithm 1 :
Inference procedure for h-LWLM.
Input: Training data w, number of instances T,
number of layers M
Output: Model parameters Θ1, · · · , ΘT
</bodyText>
<listItem confidence="0.904702">
1: for t = 1 to T do
2: h(0) = w
3: form = 1 to M do
θ(m), h(m) — P(h(m)|h(m−1), θ(m))
5: end for
6: Θt = θ(1), ··· , θ(M)
7: end for
8: return Θ1, · · · , ΘT
</listItem>
<bodyText confidence="0.895392">
intractable, the equation can be approximated as:
</bodyText>
<equation confidence="0.992084857142857">
11K
1
P(w) = T
k=1
··· P(h(M−1) k|h(M)
k , Θt)P(h(M) k|l(M)
k , Θt). (3)
</equation>
<bodyText confidence="0.999928333333333">
The probability distribution can be approximated
using T instances of point estimated parameter;
Θt indicates the t-th point estimated parameter.
</bodyText>
<subsectionHeader confidence="0.997042">
3.2 Parameter Inference
</subsectionHeader>
<bodyText confidence="0.99995475">
This paper proposes a layer-wise inference pro-
cedure for constructing h-LWLMs from training
data. The detailed procedure is shown in Algo-
rithm 1, and Figure 2 shows an image representa-
tion of the procedure as increased with the number
of layers. In the procedure, LWLM structure is re-
cursively accumulated by estimating a latent word
sequence in an upper layer from a latent word se-
quence in the lower layer.
Line 4 in Algorithm 1 denotes the key proce-
dure of estimating a latent word sequence in an up-
per layer from a latent word sequence in the lower
layer. θ(m) denotes model parameter of LWLM
structure in m-th layer; it can be defined from both
h(m) and h(m−1). For the inference of h(m) from
h(m−1), Gibbs sampling is suitable (Casella and
George, 1992; Robert et al., 1993; Scott, 2002).
Gibbs sampling picks a new value for h(m) kac-
cording to its probability distribution which is es-
timated from both h(m)
</bodyText>
<equation confidence="0.982001307692308">
−k and h(m−1). h(m)
−k repre-
sents all latent words in the m-th layer except for
h(m)
k . The probability distribution is given by:
P(h(m) k|h(m)
−k , h(m−1), θ(m))
oc P(h(km−1)|h(km), θ(m))
k+n−1
11 P(h(m)
j |l(m)
j ,θ(m)). (4)
j=k
</equation>
<bodyText confidence="0.999618909090909">
For the inference, the prior distribution is neces-
sary for each probability distribution. Usually, a
hierarchical Pitman-Yor prior (Teh, 2006) is used
for each transition probability and a Dirichlet prior
(MacKay and Peto, 1994) is used for each emis-
sion probability.
As shown in line 6, t-th point estimated param-
eter Θt indicates parameters of each LWLM for
all layers in t-th iteration. The transition proba-
bilities except for M-th layer are only used in the
layer-wise inference procedure.
</bodyText>
<subsectionHeader confidence="0.997224">
3.3 Usage
</subsectionHeader>
<bodyText confidence="0.9991385">
It is impractical to directly apply the h-LWLM to
natural language processing tasks since the pro-
posed model has multiple latent word spaces and
we have to consider all possible latent word as-
signment for calculating generative probabilities.
Therefore, this paper introduces an n-gram ap-
proximation technique as well as that for standard
LWLM (Masumura et al., 2013a).
</bodyText>
<equation confidence="0.955652333333333">
T h(1) �· · · P(wk|h(1)
t=1 � h(m) k , Θt)
�
</equation>
<page confidence="0.835059">
1898
</page>
<construct confidence="0.2708625">
Algorithm 2 :
Random sampling for trained h-LWLM.
</construct>
<listItem confidence="0.8377456">
Input: Model parameters Θ1, · · · , ΘT,
number of sampled words K
Output: Sampled data w
1: fork= 1toKdo
2: Θt — P(Θt) = T 1
3: h(m) k— P(h(m)
k |l(m)
k , Θt)
4: for m = M − 1 to 1 do
5: h(m) — P(h(km) |h(km+1), Θt)
6: end for
7: wk — P (wk|h(1)
k , Θt)
8: end for
9: return w = w1, · · · , wK
</listItem>
<bodyText confidence="0.99877225">
The n-gram approximation is conducted as fol-
lowing steps. First, a lot of text data that permit h-
LWLMs to be approximated by n-gram structure
is generated by random sampling using trained
h-LWLM. Next, an n-gram model is constructed
from the generated data. The random sampling is
based on Algorithm 2. The sampled data w in
line 9 is only used for n-gram model estimation.
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99364">
4.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999973704545455">
Our basic assumption is domain-matched train-
ing data is not available. Thus, for LM train-
ing, we used the Corpus of Spontaneous Japanese
(CSJ) whose domain is a spontaneous lecture task
(Maekawa et al., 2000). We divided CSJ into a
training set and a small validation set (Valid). The
validation set was used for optimizing several hy-
per parameters of LMs. For evaluation, a contact
center dialogue task (Test 1) and a voice mail task
(Test 2) were prepared. In contact center dialogue
task, two speakers, an operator and a customer,
talked to each other as in call center dialogues. 24
phone calls (24 operator channels and 24 customer
channels) were used in the evaluation. In the voice
mail task, a person spoke small voice messages us-
ing a smart phone. 237 messages are used in the
evaluation. The training data had about 7M words,
the vocabulary size was about 80K. The validation
data size and test data size (both tasks) were about
20K words.
For speech recognition evaluation, we prepared
an acoustic model based on hidden Markov mod-
els with deep neural networks (DNN-HMM) (Hin-
ton et al., 2012). The DNN-HMM had 8 hidden
layers with 2048 nodes. The speech recognizer
was a weighted finite state transducer (WFST) de-
coder (Mohri et al., 2001; Hori et al., 2007).
As a baseline, 3-gram LM with interpolated
Kneser-Ney smoothing (MKN) (Kneser and Ney,
1995) and 3-gram hierarchical Pitman-Yor LM
(HPY) (Huang and Yor, 2007) were constructed
from the training data. We also trained a class-
based recurrent neural network LM with 500 hid-
den nodes and 500 classes (RNN) for comparison
to state-of-the art language modeling (Mikolov et
al., 2011). In addition, we constructed 3-gram
standard LWLM and 3-gram h-LWLMs (LW). LW
with 1 layer represents standard LWLM, and LW
with 2-5 layers represent proposed h-LWLMs.
The number of instances was set to 10 for each LW.
For their n-gram approximation, we generated one
billion words and approximated each as a 3-gram
HPYLM. Moreover, we constructed interpolated
model with LW and HPY (LW+HPY).
</bodyText>
<subsectionHeader confidence="0.647544">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999968482758621">
Figure 3 shows the relation between number of
layers in h-LWLM and perplexity (PPL) reduc-
tion for each condition. In addition, Table 1 shows
speech recognition results in terms of word error
rate (WER) for each condition. RNN was only
tested in PPL evaluation as RNN cannot be con-
verted into WFST format.
For the validation set (same domain as that of
training set), PPL was not improved by the hier-
archical structure in LW. LW is comparable to MKN
and HPY, and inferior to RNN in terms of PPL. On
the other hand, in test sets (out-of domain tasks),
PPL improved with the increase in the number of
layers in LW. LW with 5 layers was superior to
1 layer in terms of PPL and WER. The best re-
sults were obtained by LW+HPY with 5 layers. In
fact, when we generated one billion words using
a trained LWLM or trained h-LWLM, the num-
ber of observed trigrams in h-LWLM with 5 lay-
ers was 101M while the number of observed tri-
grams in non-hierarchical LWLM was 94M. Thus,
h-LWLM can generate unseen words unlike non-
hierarchical LWLM. Moreover, trigram coverage
in each test data slightly increased with number
of layers. These results show that h-LWLM with
multiple layers offers robust performance not pos-
sible with other models while its performance in
the same domain as that of training data was not
improved. As a result, LW+HPY with 5 layers
</bodyText>
<page confidence="0.998445">
1899
</page>
<figureCaption confidence="0.98907">
Figure 3: Perplexity (PPL) results.
</figureCaption>
<table confidence="0.999177571428571">
Setup Layer Valid Test 1 Test 2
MKN - 24.79 38.67 32.31
HPY - 24.67 38.29 32.00
LW 1 24.54 36.93 30.26
LW 5 24.60 36.49 29.57
LW+HPY 1 23.62 36.49 29.76
LW+HPY 5 23.68 36.03 29.21
</table>
<tableCaption confidence="0.99985">
Table 1: Word error rate (WER) results (%).
</tableCaption>
<bodyText confidence="0.9889655">
performed significantly better than MKN, HPY and
RNN in the out-of domain tasks.
</bodyText>
<sectionHeader confidence="0.996122" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99996425">
This paper proposed h-LWLM for robust model-
ing and detailed its definition, inference proce-
dure, and approximation method. The proposed
model has a hierarchical latent word space and
it can flexibly handle linguistic phenomena not
present in the training data. Our experiments
showed that h-LWLM offers improved robustness
to out-of domain tasks; h-LWLM is also superior
to standard LWLM in terms of PPL and WER.
Furthermore, our approach is significantly supe-
rior to the conventional n-gram models or the re-
current neural network LM in out-of domain tasks.
</bodyText>
<sectionHeader confidence="0.998483" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985645355555556">
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Thorsten Brants, AShok C. Popat, Peng Xu, Ftanz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proc. Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 858–867.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
George Casella and Edward I George. 1992. Explain-
ing the Gibbs sampler. The American Statistician,
46:167–174.
Stanley F. Chen and Joshua Goodman. 1999. An em-
pirical study of smoothing techniques for language
modeling. Computer Speech &amp; Language, 13:359–
383.
Koen Deschacht, Jan De Belder, and Marie-Francine
Moens. 2012. The latent words language model.
Computer Speech &amp; Language, 26:384–409.
Ahmad Emami and Frederick Jelinek. 2005. Random
clusterings for language modeling. In Proc. IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 1:581–584.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech &amp; Language,
15:403–434.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep bilief
nets. Neural Computation, 18:1527–1554.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine, pages 1–27.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring
in extremely large vocabulary continuous speech
recognition. IEEE Transactions on Audio, Speech
and Language Processing, 15(4):1352–1365.
</reference>
<page confidence="0.929637">
1900
</page>
<reference confidence="0.995734602941177">
Songfang Huang and Marc Yor. 2007. Hierarchical
Pitman-Yor language models for ASR in meetings.
In Proc IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU), pages 124–129.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proc. IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 1:181–
184.
David J. C. MacKay and Linda C. Peto. 1994. A hi-
erarchical Dirichlet language model. Natural lan-
guage engineering, 1:289–308.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus
of Japanese. In Proc. International Conference on
Language Resources and Evaluation (LREC), pages
947–952.
Ryo Masumura, Hirokazu Masataki, Takanobu Oba,
Osamu Yoshioka, and Satoshi Takahashi. 2013a.
Use of latent words language models in ASR: a
sampling-based implementation. In Proc. IEEE In-
ternational Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 8445–8449.
Ryo Masumura, Takanobu Oba, Hirokazu Masataki,
Osamu Yoshioka, and Satoshi Takahashi. 2013b.
Viterbi decoding for latent words language models
using Gibbs sampling. In Proc. Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH), pages 3429–3433.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model.
In Proc. Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 1045–1048.
Tomas Mikolov, Stefan Kombrink Stefan, Lukas Bur-
get, Jan Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 5528–5531.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2001. Weighted finite-state transducers in speech
recognition. Computer Speech &amp; Language, 16:69–
88.
Christian P. Robert, Gilles Celeux, and Jean Diebolt.
1993. Bayesian estimation of hidden Markov
chains: A stochastic implementation. Statistics &amp;
Probability Letters, 16:77–83.
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here? In
Proc. IEEE, 88:1270–1278.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Deep Boltzmann machines. In Proc. the Inter-
national Conference on Artificial Intelligence and
Statistics, 5:448–455.
Steven L. Scott. 2002. Bayesian methods for hidden
Markov models: Recursive computing in the 21st
century. Journal of the American Statistical Associ-
ation, 97:337–351.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 985–992.
Peng Xu and Frederick Jelinek. 2004. Random forests
in language modeling. In Proc. Empirical Methods
on Natural Language Processing (EMNLP), pages
325–332.
</reference>
<page confidence="0.994113">
1901
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829537">
<title confidence="0.990279">Hierarchical Latent Words Language Models for Robust to Out-Of Domain Tasks</title>
<author confidence="0.968382">Taichi Takanobu</author>
<affiliation confidence="0.974374">Sumitaka Akinori Media Intelligence Laboratories, NTT Corporation, School of Engineering, Tohoku University, Japan</affiliation>
<email confidence="0.973737">asami.taichi,oba.takanobu,</email>
<abstract confidence="0.996293368421053">This paper focuses on language modeling with adequate robustness to support different domain tasks. To this end, we propose a hierarchical latent word language model (h-LWLM). The proposed model can be regarded as a generalized form of the standard LWLMs. The key advance is introducing a multiple latent variable space with hierarchical structure. The structure can flexibly take account of linguistic phenomena not present in the training data. This paper details the definition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique. Experiments on speech recognition show the effectiveness of h- LWLM in out-of domain tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2164" citStr="Bengio et al., 2003" startWordPosition="310" endWordPosition="313"> cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domainmatched task to conventional LMs (Masumura et al., 2013a; Masumura et al., 2013b). LWLMs are</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>AShok C Popat</author>
<author>Peng Xu</author>
<author>Ftanz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1521" citStr="Brants et al., 2007" startWordPosition="211" endWordPosition="214">etails the definition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique. Experiments on speech recognition show the effectiveness of hLWLM in out-of domain tasks. 1 Introduction Language models (LMs) are essential for automatic speech recognition or statistical machine translation (Rosenfeld, 2000). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also s</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, AShok C. Popat, Peng Xu, Ftanz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="1921" citStr="Brown et al., 1992" startWordPosition="271" endWordPosition="274">The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LW</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Casella</author>
<author>Edward I George</author>
</authors>
<title>Explaining the Gibbs sampler.</title>
<date>1992</date>
<journal>The American Statistician,</journal>
<pages>46--167</pages>
<contexts>
<context position="9006" citStr="Casella and George, 1992" startWordPosition="1493" endWordPosition="1496">lgorithm 1, and Figure 2 shows an image representation of the procedure as increased with the number of layers. In the procedure, LWLM structure is recursively accumulated by estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. Line 4 in Algorithm 1 denotes the key procedure of estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. θ(m) denotes model parameter of LWLM structure in m-th layer; it can be defined from both h(m) and h(m−1). For the inference of h(m) from h(m−1), Gibbs sampling is suitable (Casella and George, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h(m) kaccording to its probability distribution which is estimated from both h(m) −k and h(m−1). h(m) −k represents all latent words in the m-th layer except for h(m) k . The probability distribution is given by: P(h(m) k|h(m) −k , h(m−1), θ(m)) oc P(h(km−1)|h(km), θ(m)) k+n−1 11 P(h(m) j |l(m) j ,θ(m)). (4) j=k For the inference, the prior distribution is necessary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay</context>
</contexts>
<marker>Casella, George, 1992</marker>
<rawString>George Casella and Edward I George. 1992. Explaining the Gibbs sampler. The American Statistician, 46:167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<pages>383</pages>
<contexts>
<context position="1885" citStr="Chen and Goodman, 1999" startWordPosition="265" endWordPosition="268"> machine translation (Rosenfeld, 2000). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for ou</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13:359– 383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Jan De Belder</author>
<author>Marie-Francine Moens</author>
</authors>
<title>The latent words language model.</title>
<date>2012</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>26--384</pages>
<marker>Deschacht, De Belder, Moens, 2012</marker>
<rawString>Koen Deschacht, Jan De Belder, and Marie-Francine Moens. 2012. The latent words language model. Computer Speech &amp; Language, 26:384–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random clusterings for language modeling. In</title>
<date>2005</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>1--581</pages>
<contexts>
<context position="2042" citStr="Emami and Jelinek, 2005" startWordPosition="291" endWordPosition="294">lly obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the perfor</context>
</contexts>
<marker>Emami, Jelinek, 2005</marker>
<rawString>Ahmad Emami and Frederick Jelinek. 2005. Random clusterings for language modeling. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1:581–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>15--403</pages>
<contexts>
<context position="1751" citStr="Goodman, 2001" startWordPosition="248" endWordPosition="249">LM in out-of domain tasks. 1 Introduction Language models (LMs) are essential for automatic speech recognition or statistical machine translation (Rosenfeld, 2000). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technolo</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. Computer Speech &amp; Language, 15:403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep bilief nets.</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<pages>18--1527</pages>
<contexts>
<context position="4561" citStr="Hinton et al., 2006" startWordPosition="694" endWordPosition="697">and meaning of words. Therefore, it can be expected that h1896 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1896–1901, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. LWLMs flexibly calculate generative probability for unseen words unlike non-hierarchical LWLMs. To create the hierarchical latent word structure from training data sets, we also propose a layerwise inference. The inference is inspired by a deep Boltzmann machine (Salakhutdinov and Hinton, 2009) that stacks up restricted Boltzmann machines (Hinton et al., 2006). In addition, we detail an n-gram approximation technique to apply the proposed model to practical natural language processing tasks (see Sec. 3). In experiments, we construct LMs from spontaneous lecture task data and apply them to a contact center dialogue task and a voice mail task as outof domain tasks. The effectiveness of the proposed method is shown by perplexity and speech recognition evaluation (Sec. 4). 2 Latent Words Language Models LWLMs are generative models with single latent word space (Deschacht et al., 2012). The latent word is represented as a specific word that is selected </context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep bilief nets. Neural Computation, 18:1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Li Deng</author>
<author>Dong Yu</author>
<author>George Dahl</author>
<author>Abdel rahman Mohamed</author>
<author>Navdeep Jaitly</author>
<author>Andrew Senior</author>
<author>Vincent Vanhoucke</author>
<author>Patrick Nguyen</author>
<author>Tara Sainath</author>
<author>Brian Kingsbury</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine,</title>
<date>2012</date>
<pages>1--27</pages>
<contexts>
<context position="12180" citStr="Hinton et al., 2012" startWordPosition="2060" endWordPosition="2064">gue task, two speakers, an operator and a customer, talked to each other as in call center dialogues. 24 phone calls (24 operator channels and 24 customer channels) were used in the evaluation. In the voice mail task, a person spoke small voice messages using a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words. For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3</context>
</contexts>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, Kingsbury, 2012</marker>
<rawString>Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hori</author>
<author>Chiori Hori</author>
<author>Yasuhiro Minami</author>
<author>Atsushi Nakamura</author>
</authors>
<title>Efficient WFST-based onepass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="12346" citStr="Hori et al., 2007" startWordPosition="2090" endWordPosition="2093">e used in the evaluation. In the voice mail task, a person spoke small voice messages using a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words. For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW. Fo</context>
</contexts>
<marker>Hori, Hori, Minami, Nakamura, 2007</marker>
<rawString>Takaaki Hori, Chiori Hori, Yasuhiro Minami, and Atsushi Nakamura. 2007. Efficient WFST-based onepass decoding with on-the-fly hypothesis rescoring in extremely large vocabulary continuous speech recognition. IEEE Transactions on Audio, Speech and Language Processing, 15(4):1352–1365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songfang Huang</author>
<author>Marc Yor</author>
</authors>
<title>Hierarchical Pitman-Yor language models for ASR in meetings.</title>
<date>2007</date>
<booktitle>In Proc IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>124--129</pages>
<contexts>
<context position="12506" citStr="Huang and Yor, 2007" startWordPosition="2114" endWordPosition="2117">ng data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words. For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW. For their n-gram approximation, we generated one billion words and approximated each as a 3-gram HPYLM. Moreover, we constructed interpolated model with LW and HP</context>
</contexts>
<marker>Huang, Yor, 2007</marker>
<rawString>Songfang Huang and Marc Yor. 2007. Hierarchical Pitman-Yor language models for ASR in meetings. In Proc IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 124–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>1--181</pages>
<contexts>
<context position="12440" citStr="Kneser and Ney, 1995" startWordPosition="2104" endWordPosition="2107"> a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words. For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW. For their n-gram approximation, we generated one billion words and approximated each as a 3-gram</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1:181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
<author>Linda C Peto</author>
</authors>
<title>A hierarchical Dirichlet language model. Natural language engineering,</title>
<date>1994</date>
<pages>1--289</pages>
<contexts>
<context position="9622" citStr="MacKay and Peto, 1994" startWordPosition="1597" endWordPosition="1600">, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h(m) kaccording to its probability distribution which is estimated from both h(m) −k and h(m−1). h(m) −k represents all latent words in the m-th layer except for h(m) k . The probability distribution is given by: P(h(m) k|h(m) −k , h(m−1), θ(m)) oc P(h(km−1)|h(km), θ(m)) k+n−1 11 P(h(m) j |l(m) j ,θ(m)). (4) j=k For the inference, the prior distribution is necessary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay and Peto, 1994) is used for each emission probability. As shown in line 6, t-th point estimated parameter Θt indicates parameters of each LWLM for all layers in t-th iteration. The transition probabilities except for M-th layer are only used in the layer-wise inference procedure. 3.3 Usage It is impractical to directly apply the h-LWLM to natural language processing tasks since the proposed model has multiple latent word spaces and we have to consider all possible latent word assignment for calculating generative probabilities. Therefore, this paper introduces an n-gram approximation technique as well as tha</context>
</contexts>
<marker>MacKay, Peto, 1994</marker>
<rawString>David J. C. MacKay and Linda C. Peto. 1994. A hierarchical Dirichlet language model. Natural language engineering, 1:289–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
<author>Hanae Koiso</author>
<author>Sadaoki Furui</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Spontaneous speech corpus of Japanese.</title>
<date>2000</date>
<booktitle>In Proc. International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>947--952</pages>
<contexts>
<context position="11286" citStr="Maekawa et al., 2000" startWordPosition="1906" endWordPosition="1909"> n-gram approximation is conducted as following steps. First, a lot of text data that permit hLWLMs to be approximated by n-gram structure is generated by random sampling using trained h-LWLM. Next, an n-gram model is constructed from the generated data. The random sampling is based on Algorithm 2. The sampled data w in line 9 is only used for n-gram model estimation. 4 Experiments 4.1 Experimental Conditions Our basic assumption is domain-matched training data is not available. Thus, for LM training, we used the Corpus of Spontaneous Japanese (CSJ) whose domain is a spontaneous lecture task (Maekawa et al., 2000). We divided CSJ into a training set and a small validation set (Valid). The validation set was used for optimizing several hyper parameters of LMs. For evaluation, a contact center dialogue task (Test 1) and a voice mail task (Test 2) were prepared. In contact center dialogue task, two speakers, an operator and a customer, talked to each other as in call center dialogues. 24 phone calls (24 operator channels and 24 customer channels) were used in the evaluation. In the voice mail task, a person spoke small voice messages using a smart phone. 237 messages are used in the evaluation. The traini</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of Japanese. In Proc. International Conference on Language Resources and Evaluation (LREC), pages 947–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Masumura</author>
<author>Hirokazu Masataki</author>
<author>Takanobu Oba</author>
<author>Osamu Yoshioka</author>
<author>Satoshi Takahashi</author>
</authors>
<title>Use of latent words language models in ASR: a sampling-based implementation. In</title>
<date>2013</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>8445--8449</pages>
<contexts>
<context position="2727" citStr="Masumura et al., 2013" startWordPosition="400" endWordPosition="403"> LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domainmatched task to conventional LMs (Masumura et al., 2013a; Masumura et al., 2013b). LWLMs are generative models that employ a latent word space. The latent space can flexibly take into account relationships between words and the modeling helps to efficiently increase the robustness to out-of domain tasks (Sec. 2). In this paper, we focus on LWLMs and aim to make them more flexible for greater robustness to out-of domain tasks. To this end, this paper takes note of a fact that standard LWLM simply represents the latent space as n-gram model of latent words. However, function and meaning of words are essentially hierarchical and upper layers ought to</context>
<context position="5904" citStr="Masumura et al., 2013" startWordPosition="930" endWordPosition="934">M produces the generative probability of observed word sequence w = w1,···,wK as: 9 K P(w) = f Y XP(wk |hk, θ) k=1 hk P(hk|lk, θ)P(θ)dθ, (1) where θ indicates a model parameter of the LWLM, h = h1, · · · , hK denotes a latent word sequence and lk denotes context latent words hk−n+1, · · · , hk−1. P(hk|lk, θ) represents the transition probability which can be expressed by an n-gram model for latent words, and P(wk|hk, θ) represents the emission probability that models the dependency between the observed word and the latent word. More details are shown in previous works (Deschacht et al., 2012; Masumura et al., 2013a; Masumura et al., 2013b). 3 Hierarchical LWLMs 3.1 Definition This paper introduces h-LWLM. The proposed model has multiple latent word spaces in a hierarchical structure. Thus, it assumes that there is Figure 1: Graphical representation of h-LWLM. a latent word behind a latent word. The proposed model can be regarded as a generalized form of the standard LWLM. Thus, standard LWLMs correspond to h-LWLMs with just one layer. The latent words in all layers are represented as a specific word that is selected from the entire vocabulary. A graphic rendering of h-LWLM is shown in Figure 1. In a ge</context>
<context position="10264" citStr="Masumura et al., 2013" startWordPosition="1702" endWordPosition="1705">ssion probability. As shown in line 6, t-th point estimated parameter Θt indicates parameters of each LWLM for all layers in t-th iteration. The transition probabilities except for M-th layer are only used in the layer-wise inference procedure. 3.3 Usage It is impractical to directly apply the h-LWLM to natural language processing tasks since the proposed model has multiple latent word spaces and we have to consider all possible latent word assignment for calculating generative probabilities. Therefore, this paper introduces an n-gram approximation technique as well as that for standard LWLM (Masumura et al., 2013a). T h(1) �· · · P(wk|h(1) t=1 � h(m) k , Θt) � 1898 Algorithm 2 : Random sampling for trained h-LWLM. Input: Model parameters Θ1, · · · , ΘT, number of sampled words K Output: Sampled data w 1: fork= 1toKdo 2: Θt — P(Θt) = T 1 3: h(m) k— P(h(m) k |l(m) k , Θt) 4: for m = M − 1 to 1 do 5: h(m) — P(h(km) |h(km+1), Θt) 6: end for 7: wk — P (wk|h(1) k , Θt) 8: end for 9: return w = w1, · · · , wK The n-gram approximation is conducted as following steps. First, a lot of text data that permit hLWLMs to be approximated by n-gram structure is generated by random sampling using trained h-LWLM. Next, </context>
</contexts>
<marker>Masumura, Masataki, Oba, Yoshioka, Takahashi, 2013</marker>
<rawString>Ryo Masumura, Hirokazu Masataki, Takanobu Oba, Osamu Yoshioka, and Satoshi Takahashi. 2013a. Use of latent words language models in ASR: a sampling-based implementation. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8445–8449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Masumura</author>
<author>Takanobu Oba</author>
<author>Hirokazu Masataki</author>
<author>Osamu Yoshioka</author>
<author>Satoshi Takahashi</author>
</authors>
<title>Viterbi decoding for latent words language models using Gibbs sampling.</title>
<date>2013</date>
<booktitle>In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH),</booktitle>
<pages>3429--3433</pages>
<contexts>
<context position="2727" citStr="Masumura et al., 2013" startWordPosition="400" endWordPosition="403"> LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domainmatched task to conventional LMs (Masumura et al., 2013a; Masumura et al., 2013b). LWLMs are generative models that employ a latent word space. The latent space can flexibly take into account relationships between words and the modeling helps to efficiently increase the robustness to out-of domain tasks (Sec. 2). In this paper, we focus on LWLMs and aim to make them more flexible for greater robustness to out-of domain tasks. To this end, this paper takes note of a fact that standard LWLM simply represents the latent space as n-gram model of latent words. However, function and meaning of words are essentially hierarchical and upper layers ought to</context>
<context position="5904" citStr="Masumura et al., 2013" startWordPosition="930" endWordPosition="934">M produces the generative probability of observed word sequence w = w1,···,wK as: 9 K P(w) = f Y XP(wk |hk, θ) k=1 hk P(hk|lk, θ)P(θ)dθ, (1) where θ indicates a model parameter of the LWLM, h = h1, · · · , hK denotes a latent word sequence and lk denotes context latent words hk−n+1, · · · , hk−1. P(hk|lk, θ) represents the transition probability which can be expressed by an n-gram model for latent words, and P(wk|hk, θ) represents the emission probability that models the dependency between the observed word and the latent word. More details are shown in previous works (Deschacht et al., 2012; Masumura et al., 2013a; Masumura et al., 2013b). 3 Hierarchical LWLMs 3.1 Definition This paper introduces h-LWLM. The proposed model has multiple latent word spaces in a hierarchical structure. Thus, it assumes that there is Figure 1: Graphical representation of h-LWLM. a latent word behind a latent word. The proposed model can be regarded as a generalized form of the standard LWLM. Thus, standard LWLMs correspond to h-LWLMs with just one layer. The latent words in all layers are represented as a specific word that is selected from the entire vocabulary. A graphic rendering of h-LWLM is shown in Figure 1. In a ge</context>
<context position="10264" citStr="Masumura et al., 2013" startWordPosition="1702" endWordPosition="1705">ssion probability. As shown in line 6, t-th point estimated parameter Θt indicates parameters of each LWLM for all layers in t-th iteration. The transition probabilities except for M-th layer are only used in the layer-wise inference procedure. 3.3 Usage It is impractical to directly apply the h-LWLM to natural language processing tasks since the proposed model has multiple latent word spaces and we have to consider all possible latent word assignment for calculating generative probabilities. Therefore, this paper introduces an n-gram approximation technique as well as that for standard LWLM (Masumura et al., 2013a). T h(1) �· · · P(wk|h(1) t=1 � h(m) k , Θt) � 1898 Algorithm 2 : Random sampling for trained h-LWLM. Input: Model parameters Θ1, · · · , ΘT, number of sampled words K Output: Sampled data w 1: fork= 1toKdo 2: Θt — P(Θt) = T 1 3: h(m) k— P(h(m) k |l(m) k , Θt) 4: for m = M − 1 to 1 do 5: h(m) — P(h(km) |h(km+1), Θt) 6: end for 7: wk — P (wk|h(1) k , Θt) 8: end for 9: return w = w1, · · · , wK The n-gram approximation is conducted as following steps. First, a lot of text data that permit hLWLMs to be approximated by n-gram structure is generated by random sampling using trained h-LWLM. Next, </context>
</contexts>
<marker>Masumura, Oba, Masataki, Yoshioka, Takahashi, 2013</marker>
<rawString>Ryo Masumura, Takanobu Oba, Hirokazu Masataki, Osamu Yoshioka, and Satoshi Takahashi. 2013b. Viterbi decoding for latent words language models using Gibbs sampling. In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), pages 3429–3433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiat</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH),</booktitle>
<pages>1045--1048</pages>
<contexts>
<context position="2187" citStr="Mikolov et al., 2010" startWordPosition="314" endWordPosition="317"> of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domainmatched task to conventional LMs (Masumura et al., 2013a; Masumura et al., 2013b). LWLMs are generative models that</context>
</contexts>
<marker>Mikolov, Karafiat, Burget, Cernocky, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink Stefan</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>5528--5531</pages>
<contexts>
<context position="12724" citStr="Mikolov et al., 2011" startWordPosition="2150" endWordPosition="2153">idden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW. For their n-gram approximation, we generated one billion words and approximated each as a 3-gram HPYLM. Moreover, we constructed interpolated model with LW and HPY (LW+HPY). 4.2 Results Figure 3 shows the relation between number of layers in h-LWLM and perplexity (PPL) reduction for each condition. In addition, Table 1 shows speech recognition results in terms of word error rat</context>
</contexts>
<marker>Mikolov, Stefan, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink Stefan, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5528–5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>16</volume>
<pages>88</pages>
<contexts>
<context position="12326" citStr="Mohri et al., 2001" startWordPosition="2086" endWordPosition="2089">stomer channels) were used in the evaluation. In the voice mail task, a person spoke small voice messages using a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words. For speech recognition evaluation, we prepared an acoustic model based on hidden Markov models with deep neural networks (DNN-HMM) (Hinton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) decoder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing (MKN) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM (HPY) (Huang and Yor, 2007) were constructed from the training data. We also trained a classbased recurrent neural network LM with 500 hidden nodes and 500 classes (RNN) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs (LW). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set t</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2001</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2001. Weighted finite-state transducers in speech recognition. Computer Speech &amp; Language, 16:69– 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian P Robert</author>
<author>Gilles Celeux</author>
<author>Jean Diebolt</author>
</authors>
<title>Bayesian estimation of hidden Markov chains: A stochastic implementation.</title>
<date>1993</date>
<booktitle>Statistics &amp; Probability Letters,</booktitle>
<pages>16--77</pages>
<contexts>
<context position="9027" citStr="Robert et al., 1993" startWordPosition="1497" endWordPosition="1500">hows an image representation of the procedure as increased with the number of layers. In the procedure, LWLM structure is recursively accumulated by estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. Line 4 in Algorithm 1 denotes the key procedure of estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. θ(m) denotes model parameter of LWLM structure in m-th layer; it can be defined from both h(m) and h(m−1). For the inference of h(m) from h(m−1), Gibbs sampling is suitable (Casella and George, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h(m) kaccording to its probability distribution which is estimated from both h(m) −k and h(m−1). h(m) −k represents all latent words in the m-th layer except for h(m) k . The probability distribution is given by: P(h(m) k|h(m) −k , h(m−1), θ(m)) oc P(h(km−1)|h(km), θ(m)) k+n−1 11 P(h(m) j |l(m) j ,θ(m)). (4) j=k For the inference, the prior distribution is necessary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay and Peto, 1994) is u</context>
</contexts>
<marker>Robert, Celeux, Diebolt, 1993</marker>
<rawString>Christian P. Robert, Gilles Celeux, and Jean Diebolt. 1993. Bayesian estimation of hidden Markov chains: A stochastic implementation. Statistics &amp; Probability Letters, 16:77–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>In Proc. IEEE,</booktitle>
<pages>88--1270</pages>
<contexts>
<context position="1300" citStr="Rosenfeld, 2000" startWordPosition="179" endWordPosition="180">andard LWLMs. The key advance is introducing a multiple latent variable space with hierarchical structure. The structure can flexibly take account of linguistic phenomena not present in the training data. This paper details the definition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique. Experiments on speech recognition show the effectiveness of hLWLM in out-of domain tasks. 1 Introduction Language models (LMs) are essential for automatic speech recognition or statistical machine translation (Rosenfeld, 2000). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? In Proc. IEEE, 88:1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Deep Boltzmann machines.</title>
<date>2009</date>
<booktitle>In Proc. the International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>5--448</pages>
<contexts>
<context position="4494" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="682" endWordPosition="686">rarchical structure can take into account the abstraction process of function and meaning of words. Therefore, it can be expected that h1896 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1896–1901, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. LWLMs flexibly calculate generative probability for unseen words unlike non-hierarchical LWLMs. To create the hierarchical latent word structure from training data sets, we also propose a layerwise inference. The inference is inspired by a deep Boltzmann machine (Salakhutdinov and Hinton, 2009) that stacks up restricted Boltzmann machines (Hinton et al., 2006). In addition, we detail an n-gram approximation technique to apply the proposed model to practical natural language processing tasks (see Sec. 3). In experiments, we construct LMs from spontaneous lecture task data and apply them to a contact center dialogue task and a voice mail task as outof domain tasks. The effectiveness of the proposed method is shown by perplexity and speech recognition evaluation (Sec. 4). 2 Latent Words Language Models LWLMs are generative models with single latent word space (Deschacht et al., 2012). </context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Deep Boltzmann machines. In Proc. the International Conference on Artificial Intelligence and Statistics, 5:448–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven L Scott</author>
</authors>
<title>Bayesian methods for hidden Markov models: Recursive computing in the 21st century.</title>
<date>2002</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>97--337</pages>
<contexts>
<context position="9041" citStr="Scott, 2002" startWordPosition="1501" endWordPosition="1502">ntation of the procedure as increased with the number of layers. In the procedure, LWLM structure is recursively accumulated by estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. Line 4 in Algorithm 1 denotes the key procedure of estimating a latent word sequence in an upper layer from a latent word sequence in the lower layer. θ(m) denotes model parameter of LWLM structure in m-th layer; it can be defined from both h(m) and h(m−1). For the inference of h(m) from h(m−1), Gibbs sampling is suitable (Casella and George, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h(m) kaccording to its probability distribution which is estimated from both h(m) −k and h(m−1). h(m) −k represents all latent words in the m-th layer except for h(m) k . The probability distribution is given by: P(h(m) k|h(m) −k , h(m−1), θ(m)) oc P(h(km−1)|h(km), θ(m)) k+n−1 11 P(h(m) j |l(m) j ,θ(m)). (4) j=k For the inference, the prior distribution is necessary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay and Peto, 1994) is used for each e</context>
</contexts>
<marker>Scott, 2002</marker>
<rawString>Steven L. Scott. 2002. Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Association, 97:337–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>985--992</pages>
<contexts>
<context position="1972" citStr="Teh, 2006" startWordPosition="281" endWordPosition="282">ity of their training data. Superior performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performa</context>
<context position="9536" citStr="Teh, 2006" startWordPosition="1585" endWordPosition="1586">erence of h(m) from h(m−1), Gibbs sampling is suitable (Casella and George, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h(m) kaccording to its probability distribution which is estimated from both h(m) −k and h(m−1). h(m) −k represents all latent words in the m-th layer except for h(m) k . The probability distribution is given by: P(h(m) k|h(m) −k , h(m−1), θ(m)) oc P(h(km−1)|h(km), θ(m)) k+n−1 11 P(h(m) j |l(m) j ,θ(m)). (4) j=k For the inference, the prior distribution is necessary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay and Peto, 1994) is used for each emission probability. As shown in line 6, t-th point estimated parameter Θt indicates parameters of each LWLM for all layers in t-th iteration. The transition probabilities except for M-th layer are only used in the layer-wise inference procedure. 3.3 Usage It is impractical to directly apply the h-LWLM to natural language processing tasks since the proposed model has multiple latent word spaces and we have to consider all possible latent word assignment for calculating generative probabilit</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on Pitman-Yor processes. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In Proc. Empirical Methods on Natural Language Processing (EMNLP),</booktitle>
<pages>325--332</pages>
<contexts>
<context position="2016" citStr="Xu and Jelinek, 2004" startWordPosition="287" endWordPosition="290">or performance is usually obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001). For robust language modeling, several technologies have been proposed. Fundamental techniques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solutions are Bayesian modeling (Teh, 2006) and ensemble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous representation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies robustly support out-of domain tasks. In contrast, latent words LMs (LWLMs) (Deschacht et al., 2012) are clearly effective for outof domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of dom</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In Proc. Empirical Methods on Natural Language Processing (EMNLP), pages 325–332.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
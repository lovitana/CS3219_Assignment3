<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000315">
<title confidence="0.9988455">
A Comparative Study on Regularization Strategies
for Embedding-based Neural Networks
</title>
<author confidence="0.997742">
Hao Peng∗,1 Lili Mou,∗1 Ge Li,†1 Yunchuan Chen,2 Yangyang Lu,1 Zhi Jin1
</author>
<affiliation confidence="0.987005">
1Software Institute, Peking University, 100871, P. R. China
</affiliation>
<email confidence="0.669213">
{penghao.pku, doublepower.mou}@gmail.com,{lige, luyy11, zhijin}@sei.pku.edu.cn
2University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn
</email>
<sectionHeader confidence="0.985535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999068125">
This paper aims to compare different reg-
ularization strategies to address a com-
mon phenomenon, severe overfitting, in
embedding-based neural networks for
NLP. We chose two widely studied neu-
ral models and tasks as our testbed.
We tried several frequently applied or
newly proposed regularization strategies,
including penalizing weights (embeddings
excluded), penalizing embeddings, re-
embedding words, and dropout. We also
emphasized on incremental hyperparame-
ter tuning, and combining different regu-
larizations. The results provide a picture
on tuning hyperparameters for neural NLP
models.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996091333333334">
Neural networks have exhibited considerable po-
tential in various fields (Krizhevsky et al., 2012;
Graves et al., 2013). In early years on neural
NLP research, neural networks were used in lan-
guage modeling (Bengio et al., 2003; Morin and
Bengio, 2005; Mnih and Hinton, 2009); recently,
they have been applied to various supervised tasks,
such as named entity recognition (Collobert and
Weston, 2008), sentiment analysis (Socher et al.,
2011; Mou et al., 2015), relation classification
(Zeng et al., 2014; Xu et al., 2015), etc. In the field
of NLP, neural networks are typically combined
with word embeddings, which are usually first pre-
trained by unsupervised algorithms like Mikolov
et al. (2013); then they are fed forward to standard
neural models, fine-tuned during supervised learn-
ing. However, embedding-based neural networks
usually suffer from severe overfitting because of
the high dimensionality of parameters.
∗Equal contribution. †Corresponding author.
A curious question is whether we can regular-
ize embedding-based NLP neural models to im-
prove generalization. Although existing and newly
proposed regularization methods might alleviate
the problem, their inherent performance in neural
NLP models is not clear: the use of embeddings
is sparse; the behaviors may be different from
those in other scenarios like image recognition.
Further, selecting hyperparameters to pursue the
best performance by validation is extremely time-
consuming, as suggested in Collobert et al. (2011).
Therefore, new studies are needed to provide a
more complete picture regarding regularization for
neural natural language processing. Specifically,
we focus on the following research questions in
this paper.
</bodyText>
<listItem confidence="0.978372125">
RQ 1: How do different regularization strategies
typically behave in embedding-based neural
networks?
RQ 2: Can regularization coefficients be tuned in-
crementally during training so as to ease the
burden of hyperparameter tuning?
RQ 3: What is the effect of combining different
regularization strategies?
</listItem>
<bodyText confidence="0.999801294117647">
In this paper, we systematically and quan-
titatively compared four different regularization
strategies, namely penalizing weights, penalizing
embeddings, newly proposed word re-embedding
(Labutov and Lipson, 2013), and dropout (Srivas-
tava et al., 2014). We analyzed these regulariza-
tion methods by two widely studied models and
tasks. We also emphasized on incremental hyper-
parameter tuning and the combination of different
regularization methods.
Our experiments provide some interesting re-
sults: (1) Regularizations do help generalization,
but their effect depends largely on the datasets’
size. (2) Penalizing E2-norm of embeddings helps
optimization as well, improving training accu-
racy unexpectedly. (3) Incremental hyperparam-
eter tuning achieves similar performance, indicat-
</bodyText>
<page confidence="0.957146">
2106
</page>
<note confidence="0.6526025">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2106–2111,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999304571428571">
ing that regularizations mainly serve as a “local”
effect. (4) Dropout performs slightly worse than
E2 penalty in our experiments; however, provided
very small E2 penalty, dropping out hidden units
and penalizing `2-norm are generally complemen-
tary. (5) The newly proposed re-embedding words
method is not effective in our experiments.
</bodyText>
<sectionHeader confidence="0.826848" genericHeader="method">
2 Tasks, Models, and Setup
</sectionHeader>
<bodyText confidence="0.9988652">
Experiment I: Relation extraction. The dataset
in this experiment comes from SemEval-2010
Task 8.1 The goal is to classify the relationship
between two marked entities in each sentence. We
refer interested readers to recent advances, e.g.,
Hashimoto et al. (2013), Zeng et al. (2014), and
Xu et al. (2015). To make our task and model
general, however, we do not consider entity tag-
ging information; we do not distinguish the order
of two entities either. In total, there are 10 labels,
i.e., 9 different relations plus a default other.
Regarding the neural model, we applied Col-
lobert’s convolutional neural network (CNN)
(Collobert and Weston, 2008) with minor modi-
fications. The model comprises a fixed-window
convolutional layer with size equal to 5, 0 padded
at the end of each sentence; a max pooling layer;
a tanh hidden layer; and a softmax output layer.
Experiment II: Sentiment analysis. This is
another testbed for neural NLP, aiming to pre-
dict the sentiment of a sentence. The dataset is
the Stanford sentiment treebank (Socher et al.,
2011)2; target labels are strongly/weakly
positive/negative, or neutral.
We used the recursive neural network (RNN),
which is proposed in Socher et al. (2011), and fur-
ther developed in Socher et al. (2012); Irsoy and
Cardie (2014). RNNs make use of binarized con-
stituency trees, and recursively encode children’s
information to their parent’s; the root vector is fi-
nally used for sentiment classification.
Experimental Setup. To setup a fair compari-
son, we set all layers to be 50-dimensional in ad-
vance (rather than by validation). Such setting has
been used in previous work like Zhao et al. (2015).
Our embeddings are pretrained on the Wikipedia
corpus using Collobert and Weston (2008). The
learning rate is 0.1 and fixed in Experiment I;
for RNN, however, we found learning rate decay
helps to prevent parameter blowup (probably due
</bodyText>
<footnote confidence="0.998795">
1http://www.aclweb.org/anthology/S10-1006
2http://nlp.stanford.edu/sentiment/
</footnote>
<bodyText confidence="0.99979852631579">
to the recursive, and thus chaotic nature). There-
fore, we applied power decay (Senior et al., 2013)
with power equal to −1. For each strategy, we
tried a large range of regularization coefficients,
10−9, · · · ,10−2, extensively from underfitting to
no effect with granularity 10x. We ran the model
5 times with different initializations. We used
mini-batch stochastic gradient descent; gradients
are computed by standard backpropagation. For
source code, please refer to our project website.3
It needs to be noticed that, the goal of this paper
is not to outperform or reproduce state-of-the-art
results. Instead, we would like to have a fair com-
parison. The testbed of our work is two widely
studied models and tasks, which were not chosen
on purpose. During the experiments, we tried to
make the comparison as fair as possible. There-
fore, we think that the results of this work can be
generalized to similar scenarios.
</bodyText>
<sectionHeader confidence="0.986725" genericHeader="method">
3 Regularization Strategies
</sectionHeader>
<bodyText confidence="0.9889365">
In this section, we describe four regularization
strategies used in our experiment.
</bodyText>
<listItem confidence="0.989705708333333">
• Penalizing `2-norm of weights. Let E be the
cross-entropy error for classification, and R
be a regularization term. The overall cost
function is J = E + AR, where A is the co-
efficient. In this case, R = 11W 112, and the
coefficient is denoted as AW.
• Penalizing `2-norm of embeddings. Some
studies do not distinguish embeddings or
connectional weights for regularization (Tai
et al., 2015). However, we would like to an-
alyze their effect separately, for embeddings
are sparse in use. Let 4&apos; denote embeddings;
then we have R = 114&apos;112.
• Re-embedding words (Labutov and Lipson,
2013). Suppose 4&apos;0 denotes the original em-
beddings trained on a large corpus, and 4&apos; de-
notes the embeddings fine-tuned during su-
pervised training. We would like to penalize
the norm of the difference between 4&apos;0 and 4&apos;,
i.e., R = 114&apos;0−4&apos;112. In the limit of penalty to
infinity, the model is mathematically equiv-
alent to “frozen embeddings,” where word
vectors are used as surface features.
• Dropout (Srivastava et al., 2014). In this
</listItem>
<bodyText confidence="0.63637575">
strategy, each neural node is set to 0 with a
predefined dropout probability p during train-
ing; when testing, all nodes are used, with ac-
tivation multiplied by 1 − p.
</bodyText>
<footnote confidence="0.948731">
3https://sites.google.com/site/regembeddingnn/
</footnote>
<page confidence="0.996781">
2107
</page>
<figure confidence="0.998012566666666">
(c) Penalizing embeddings in Experiment I.
100 5 10 15 20 25
Epochs
100
90
80
70
60
50
λb = 0
λb = 10−5
λb = 10−4
λb = 10−3
Training
Validation
100 5 10 15 20 25
Epochs
40
30
20
100
90
80
70
60
50
40
30
20
λreembed = 0
λreembed = 10−5
λreembed = 10−4
λreembed = 10−3
Training
Validation
(a) Penalizing weights in Experiment I.
100
90
80
70
60
50
λW = 0
λW = 10−5
λW = 10−4
λW = 10−3
Training
Validation
100 5 10 15 20 25
Epochs
40
30
20
(e) Re-embedding words in Experiment I.
Accuracy (%)
60
50
40
30
20
100
90
80
70
100 5 10 15 20 25
Epochs
pdrop = 0
pdrop = 0.1
pdrop = 0.2
pdrop = 0.3
pdrop = 0.4
pdrop = 0.7
Training
Validation
(g) Applying dropout in Experiment I. P = 0.5, 0.6
are omitted because they are similar to small values.
(b) Penalizing weights in Experiment II.
60
50
Accuracy (%)
40
30
20
λb = 0
λb = 10−5
λb = 10−4
λb = 10−3
Training
Validation
Accuracy (%)
55
50
45
40
35
λW = 0
λW = 10−5
λW = 10−4
λW = 10−3
Training
Validation
150 10 20 30 40 50 60 70 80
Epochs
30
25
20
0 10 20 30 40 50 60 70 80
Epochs
(d) Penalizing embeddings in Experiment II.
(f) Re-embedding words in Experiment II.
Accuracy (%)
55
50
45
40
A�«���d = 0
A�«�d = 10−5
4
A�«�d = 10
A-. d = 10−3
Training
Validation
200 10 20 30 40 50 60 70 80
Epochs
35
30
25
Accuracy (%)
55
50
45
40
pdrop = 0
pdrop = 0.1
pdrop = 0.2
pdrop = 0.3
pdrop = 0.4
pdrop = 0.5
Training
Validation
150 10 20 30 40 50 60 70 80
Epochs
35
30
25
20
(h) Applying dropout in Experiment II.
Accuracy (%)
Accuracy (%)
Accuracy (%)
</figure>
<sectionHeader confidence="0.91359" genericHeader="method">
4 Individual Regularization Behaviors
</sectionHeader>
<bodyText confidence="0.999637875">
This section compares the behavior of each strat-
egy. We first conducted both experiments with-
out regularization, achieving accuracies of 54.02±
0.84%, 41.47± 2.85%, respectively. Then we plot
in Figure 1 learning curves when each regulariza-
tion strategy is applied individually. We report
training and validation accuracies through out this
paper. The main findings are as follows.
</bodyText>
<listItem confidence="0.989028956521739">
• Penalizing E2-norm of weights helps gener-
alization; the effect depends largely on the
size of training set. Experiment I contains
7,000 training samples and the improvement
is 6.98%; Experiment II contains more than
150k samples, and the improvement is only
2.07%. Such results are consistent with other
machine learning models.
• Penalizing E2-norm of embeddings unexpect-
edly helps optimization (improves training
accuracy). One plausible explanation is that
since embeddings are trained on a large cor-
pus by unsupervised methods, they tend to
settle down to large values and may not per-
fectly agree with the tasks of interest. E2
penalty pushes the embeddings towards small
values and thus helps optimization. Regard-
ing validation accuracy, Experiment I is im-
proved by 6.89%, whereas Experiment II has
no significant difference.
• Re-embedding words does not improve gen-
eralization. Particularly, in Experiment II,
the ultimate accuracy is improved by 0.44,
</listItem>
<bodyText confidence="0.7664815">
which is not large. Further, too much penalty
hurts the models in both experiments. In the
limit Areembed to infinity, re-embedding words
is mathematically equivalent to using embed-
dings as surface features, that is, freezing em-
beddings. Such strategy is sometimes applied
in the literature like Hu et al. (2014), but is
not favorable as suggested by the experiment.
</bodyText>
<listItem confidence="0.9846065">
• Dropout helps generalization. Under the best
settings, the eventual accuracy is improved
by 3.12% and 1.76%, respectively. In our ex-
periments, dropout alone is not as useful as
E2 penalty. However, other studies report that
dropout is very effective (Irsoy and Cardie,
2014). Our results are not consistent; differ-
ent dimensionality may contribute to this dis-
agreement, but more experiments are needed
to confirm the hypothesis.
</listItem>
<sectionHeader confidence="0.980308" genericHeader="method">
5 Incremental Hyperparameter Tuning
</sectionHeader>
<bodyText confidence="0.999696578947369">
The above experiments show that regularization
generally helps prevent overfitting. To pursue the
best performance, we need to try out different hy-
perparameters through validation. Unfortunately,
training deep neural networks is time-consuming,
preventing full grid search from being a practical
technique. Things will get easier if we can incre-
mentally tune hyperparameters, that is, to train the
model without regularization first, and then add
penalty.
In this section, we study whether E2 penalty of
weights and embeddings can be tuned incremen-
tally. We exclude the dropout strategy because its
does not make much sense to incrementally drop
out hidden units. Besides, from this section, we
only focus on Experiment I due to time and space
limit.
Before continuing, we may envision several
possibilities on how regularization works.
</bodyText>
<listItem confidence="0.986069909090909">
• (On initial effects) As E2-norm prevents pa-
rameters from growing large, adding it at
early stages may cause parameters settling
down to local optima. If this is the case, de-
layed penalty would help parameters get over
local optima, leading to better performance.
• (On eventual effects) E2 penalty lifts er-
ror surface of large weights. Adding such
penalty may cause parameters settling down
to (a) almost the same catchment basin, or (b)
different basins. In case (a), when the penalty
</listItem>
<bodyText confidence="0.89380735">
is added does not matter much. In case (b),
however, it makes difference, because param-
eters would have already gravitated to catch-
ment basins of larger values before regular-
ization is added, which means incremental
hyperparameter tuning would be ineffective.
To verify the above conjectures, we design four
settings: adding penalty (1) at the beginning, (2)
before overfitting at epoch 2, (3) at peak perfor-
mance (epoch 5), and (4) after overfitting (valida-
tion accuracy drops) at epoch 10.
Figure 2 plots the learning curves regarding pe-
nalizing weights and embeddings, respectively;
baseline (without regularization) is also included.
For both weights and embeddings, all settings
yield similar ultimate validation accuracies. This
shows E2 regularization mainly serves as a “local”
effect—it changes the error surface, but parame-
ters tend to settle down to a same catchment basin.
We notice a recent report also shows local optima
</bodyText>
<page confidence="0.998816">
2109
</page>
<figure confidence="0.99837375">
100
90
80
70
60
50
40
30
20
100 10 20 30 40 50 60 70 80
Epochs
(a) Incrementally penalizing `2-norm of weights. (b) Incrementally penalizing `2-norm of biases.
</figure>
<figureCaption confidence="0.938661">
Figure 2: Tuning hyperparameters incrementally in Experiment I. Penalty is added at epochs 0, 2, 5, 10,
respectively. We chose the coefficients yielding the best performance in Figure 1. The controlled trial
(no regularization) is early stopped because the accuracy has already decreased.
</figureCaption>
<figure confidence="0.882196485714286">
Accuracy (%)
Reg at epoch 0
Reg at epoch 2
Reg at epoch 5
Reg at epoch 10
No regularization
Training
Validation
Accuracy (%)
100
90
80
60
70
50
100 10 20 30 40 50 60 70 80
Epochs
Reg at epoch 0
Reg at epoch 2
Reg at epoch 5
Reg at epoch 10
No regularization
Training
Validation
40
30
20
Aembed
0
10−5
3·10−5
10−4
3·10−4
10−3
AW
</figure>
<table confidence="0.965588714285714">
0 10−4 3·10−4 10−3
54.02 57.88 59.96 61.00
54.94 57.82 60.68 62.05
55.68 61.02 64.00 63.15
60.91 64.00 63.07 60.56
58.92 61.33 59.85 42.93
54.77 56.43 54.05 16.50
</table>
<tableCaption confidence="0.6431928">
Table 1: Accuracy in percentage when we com-
bine E2-norm of weights and embeddings (Exper-
iment I). Bold numbers are among highest accu-
racies (greater than peak performance minus 1.5
times standard deviation, i.e., 1.26 in percentage).
</tableCaption>
<table confidence="0.99787325">
AW Aembed
p 10–4 3·10–4 10–3 10–5 3·10–5 10–4
0 57.88 59.96 61.00 54.94 55.68 60.91
1/6 58.36 59.36 43.42 58.49 59.59 60.00
2/6 58.22 60.00 16.60 59.34 60.08 59.61
3/6 58.63 59.73 16.60 59.59 59.98 58.82
4/6 56.43 54.63 16.60 56.76 59.19 56.64
5/6 38.07 16.60 16.60 49.79 53.63 49.75
</table>
<tableCaption confidence="0.996289">
Table 2: Combining E2 regularization and dropout.
</tableCaption>
<bodyText confidence="0.951942666666667">
Left: connectional weights. Right: embeddings.
(p refers to the dropout rate.)
may not play an important role in training neural
networks, if the effect of parameter symmetry is
ruled out (Breuel, 2015).
We also observe that regularization helps gener-
alization as soon as it is added (Figure 2a), and that
regularizing embeddings helps optimization also
right after the penalty is applied (Figure 2b).
</bodyText>
<sectionHeader confidence="0.969804" genericHeader="method">
6 Combination of Regularizations
</sectionHeader>
<bodyText confidence="0.9997798">
We are further curious about the behaviors when
different regularization methods are combined.
Table 1 shows that combining E2-norm of
weights and embeddings results in a further accu-
racy improvement of 3–4 percents from applying
either single one of them. In a certain range of
coefficients, weights and embeddings are comple-
mentary: given one hyperparameter, we can tune
the other to achieve a result among highest ones.
Such compensation is also observed in penal-
izing E2-norm versus dropout (Table 2)—although
the peak performance is obtained by pure E2 regu-
larization, applying dropout with small E2 penalty
also achieves a similar accuracy. The dropout rate
is not very sensitive, provided it is small.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999951">
In this paper, we systematically compared four
regularization strategies for embedding-based
neural networks in NLP. Based on the experimen-
tal results, we answer our research questions as
follows. (1) Regularization methods (except re-
embedding words) basically help generalization.
Penalizing E2-norm of embeddings unexpectedly
helps optimization as well. Regularization perfor-
mance depends largely on the dataset’s size. (2)
E2 penalty mainly acts as a local effect; hyperpa-
rameters can be tuned incrementally. (3) Combin-
ing E2-norm of weights and biases (dropout and E2
penalty) further improves generalization; their co-
efficients are mostly complementary within a cer-
tain range. These empirical results of regulariza-
tion strategies shed some light on tuning neural
models for NLP.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.746789666666667">
This research is supported by the National Basic
Research Program of China (the 973 Program) un-
der Grant No. 2015CB352201 and the National
Natural Science Foundation of China under Grant
No. 61232015. We would also like to thank Hao
Jia and Ran Jia.
</bodyText>
<page confidence="0.988452">
2110
</page>
<sectionHeader confidence="0.990006" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606252427185">
Yoshua Bengio, R´ejean Ducharme, P. Vincent, and
C. Jauvin. 2003. A neural probabilistic language
model. Journal of Machine Learning Research,
3:1137–1155.
Thomas M. Breuel. 2015. The effects of hyperpa-
rameters on sgd training of neural networks. arXiv
preprint arXiv:1508.02788.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine learning.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Proceedings of 2013 IEEE
International Conference on Acoustics, Speech and
Signal Processing.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple cus-
tomization of recursive neural networks for semantic
relation classification. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems.
Alex Krizhevsky, Ilyz Sutskever, and Geoffrey Hinton.
2012. ImageNet classification with deep convolu-
tional neural networks. In Advances in Neural In-
formation Processing Systems.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems.
Andriy Mnih and Geoffrey Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of International Conference on Artifi-
cial Intelligence and Statistics.
Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Tree-based convolution: A new neu-
ral architecture for sentence modeling. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (to appear).
Andrew Senior, Georg Heigold, Marc’aurelio Ranzato,
and Ke Yang. 2013. An empirical study of learning
rates in deep neural networks for speech recognition.
In Proceedings of 2013 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing.
Richard Socher, Jeffrey Pennington, Eric Huang, An-
drew Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, pages 1929–1958.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics.
Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing (to
appear).
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
Computational Linguistics.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In Pro-
ceedings of Intenational Joint Conference in Artifi-
cial Intelligence.
</reference>
<page confidence="0.993803">
2111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744040">
<title confidence="0.932263">A Comparative Study on Regularization for Embedding-based Neural Networks</title>
<note confidence="0.953149666666667">Institute, Peking University, 100871, P. R. luyy11, of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn</note>
<abstract confidence="0.996650176470588">This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1214" citStr="Bengio et al., 2003" startWordPosition="160" endWordPosition="163">our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural ne</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Breuel</author>
</authors>
<title>The effects of hyperparameters on sgd training of neural networks. arXiv preprint arXiv:1508.02788.</title>
<date>2015</date>
<contexts>
<context position="16252" citStr="Breuel, 2015" startWordPosition="2620" endWordPosition="2621">k performance minus 1.5 times standard deviation, i.e., 1.26 in percentage). AW Aembed p 10–4 3·10–4 10–3 10–5 3·10–5 10–4 0 57.88 59.96 61.00 54.94 55.68 60.91 1/6 58.36 59.36 43.42 58.49 59.59 60.00 2/6 58.22 60.00 16.60 59.34 60.08 59.61 3/6 58.63 59.73 16.60 59.59 59.98 58.82 4/6 56.43 54.63 16.60 56.76 59.19 56.64 5/6 38.07 16.60 16.60 49.79 53.63 49.75 Table 2: Combining E2 regularization and dropout. Left: connectional weights. Right: embeddings. (p refers to the dropout rate.) may not play an important role in training neural networks, if the effect of parameter symmetry is ruled out (Breuel, 2015). We also observe that regularization helps generalization as soon as it is added (Figure 2a), and that regularizing embeddings helps optimization also right after the penalty is applied (Figure 2b). 6 Combination of Regularizations We are further curious about the behaviors when different regularization methods are combined. Table 1 shows that combining E2-norm of weights and embeddings results in a further accuracy improvement of 3–4 percents from applying either single one of them. In a certain range of coefficients, weights and embeddings are complementary: given one hyperparameter, we can</context>
</contexts>
<marker>Breuel, 2015</marker>
<rawString>Thomas M. Breuel. 2015. The effects of hyperparameters on sgd training of neural networks. arXiv preprint arXiv:1508.02788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine learning.</booktitle>
<contexts>
<context position="1387" citStr="Collobert and Weston, 2008" startWordPosition="186" endWordPosition="189">gs, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we </context>
<context position="4986" citStr="Collobert and Weston, 2008" startWordPosition="709" endWordPosition="712">extraction. The dataset in this experiment comes from SemEval-2010 Task 8.1 The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other. Regarding the neural model, we applied Collobert’s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral. We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (201</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2479" citStr="Collobert et al. (2011)" startWordPosition="346" endWordPosition="349">ing because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from those in other scenarios like image recognition. Further, selecting hyperparameters to pursue the best performance by validation is extremely timeconsuming, as suggested in Collobert et al. (2011). Therefore, new studies are needed to provide a more complete picture regarding regularization for neural natural language processing. Specifically, we focus on the following research questions in this paper. RQ 1: How do different regularization strategies typically behave in embedding-based neural networks? RQ 2: Can regularization coefficients be tuned incrementally during training so as to ease the burden of hyperparameter tuning? RQ 3: What is the effect of combining different regularization strategies? In this paper, we systematically and quantitatively compared four different regulariz</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="1106" citStr="Graves et al., 2013" startWordPosition="141" endWordPosition="144">rfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed </context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple customization of recursive neural networks for semantic relation classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4599" citStr="Hashimoto et al. (2013)" startWordPosition="644" endWordPosition="647"> that regularizations mainly serve as a “local” effect. (4) Dropout performs slightly worse than E2 penalty in our experiments; however, provided very small E2 penalty, dropping out hidden units and penalizing `2-norm are generally complementary. (5) The newly proposed re-embedding words method is not effective in our experiments. 2 Tasks, Models, and Setup Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8.1 The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other. Regarding the neural model, we applied Collobert’s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. E</context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="11755" citStr="Hu et al. (2014)" startWordPosition="1883" endWordPosition="1886">dings towards small values and thus helps optimization. Regarding validation accuracy, Experiment I is improved by 6.89%, whereas Experiment II has no significant difference. • Re-embedding words does not improve generalization. Particularly, in Experiment II, the ultimate accuracy is improved by 0.44, which is not large. Further, too much penalty hurts the models in both experiments. In the limit Areembed to infinity, re-embedding words is mathematically equivalent to using embeddings as surface features, that is, freezing embeddings. Such strategy is sometimes applied in the literature like Hu et al. (2014), but is not favorable as suggested by the experiment. • Dropout helps generalization. Under the best settings, the eventual accuracy is improved by 3.12% and 1.76%, respectively. In our experiments, dropout alone is not as useful as E2 penalty. However, other studies report that dropout is very effective (Irsoy and Cardie, 2014). Our results are not consistent; different dimensionality may contribute to this disagreement, but more experiments are needed to confirm the hypothesis. 5 Incremental Hyperparameter Tuning The above experiments show that regularization generally helps prevent overfit</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5613" citStr="Irsoy and Cardie (2014)" startWordPosition="812" endWordPosition="815">h minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral. We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children’s information to their parent’s; the root vector is finally used for sentiment classification. Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (proba</context>
<context position="12086" citStr="Irsoy and Cardie, 2014" startWordPosition="1936" endWordPosition="1939">Further, too much penalty hurts the models in both experiments. In the limit Areembed to infinity, re-embedding words is mathematically equivalent to using embeddings as surface features, that is, freezing embeddings. Such strategy is sometimes applied in the literature like Hu et al. (2014), but is not favorable as suggested by the experiment. • Dropout helps generalization. Under the best settings, the eventual accuracy is improved by 3.12% and 1.76%, respectively. In our experiments, dropout alone is not as useful as E2 penalty. However, other studies report that dropout is very effective (Irsoy and Cardie, 2014). Our results are not consistent; different dimensionality may contribute to this disagreement, but more experiments are needed to confirm the hypothesis. 5 Incremental Hyperparameter Tuning The above experiments show that regularization generally helps prevent overfitting. To pursue the best performance, we need to try out different hyperparameters through validation. Unfortunately, training deep neural networks is time-consuming, preventing full grid search from being a practical technique. Things will get easier if we can incrementally tune hyperparameters, that is, to train the model witho</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilyz Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>ImageNet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1084" citStr="Krizhevsky et al., 2012" startWordPosition="137" endWordPosition="140">on phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (201</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilyz Sutskever, and Geoffrey Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>2</volume>
<contexts>
<context position="3206" citStr="Labutov and Lipson, 2013" startWordPosition="445" endWordPosition="448">ural natural language processing. Specifically, we focus on the following research questions in this paper. RQ 1: How do different regularization strategies typically behave in embedding-based neural networks? RQ 2: Can regularization coefficients be tuned incrementally during training so as to ease the burden of hyperparameter tuning? RQ 3: What is the effect of combining different regularization strategies? In this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivastava et al., 2014). We analyzed these regularization methods by two widely studied models and tasks. We also emphasized on incremental hyperparameter tuning and the combination of different regularization methods. Our experiments provide some interesting results: (1) Regularizations do help generalization, but their effect depends largely on the datasets’ size. (2) Penalizing E2-norm of embeddings helps optimization as well, improving training accuracy unexpectedly. (3) Incremental hyperparameter tuning achieves similar performance, indicat2106 Proceedings of the 2015 Conf</context>
<context position="7919" citStr="Labutov and Lipson, 2013" startWordPosition="1183" endWordPosition="1186">ion strategies used in our experiment. • Penalizing `2-norm of weights. Let E be the cross-entropy error for classification, and R be a regularization term. The overall cost function is J = E + AR, where A is the coefficient. In this case, R = 11W 112, and the coefficient is denoted as AW. • Penalizing `2-norm of embeddings. Some studies do not distinguish embeddings or connectional weights for regularization (Tai et al., 2015). However, we would like to analyze their effect separately, for embeddings are sparse in use. Let 4&apos; denote embeddings; then we have R = 114&apos;112. • Re-embedding words (Labutov and Lipson, 2013). Suppose 4&apos;0 denotes the original embeddings trained on a large corpus, and 4&apos; denotes the embeddings fine-tuned during supervised training. We would like to penalize the norm of the difference between 4&apos;0 and 4&apos;, i.e., R = 114&apos;0−4&apos;112. In the limit of penalty to infinity, the model is mathematically equivalent to “frozen embeddings,” where word vectors are used as surface features. • Dropout (Srivastava et al., 2014). In this strategy, each neural node is set to 0 with a predefined dropout probability p during training; when testing, all nodes are used, with activation multiplied by 1 − p. 3</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1686" citStr="Mikolov et al. (2013)" startWordPosition="234" endWordPosition="237">hevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly proposed regularization methods might alleviate the problem, their inherent performance in neural NLP models is not clear: the use of embeddings is sparse; the behaviors may be different from tho</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1262" citStr="Mnih and Hinton, 2009" startWordPosition="168" endWordPosition="171">ied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting be</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2009. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="1238" citStr="Morin and Bengio, 2005" startWordPosition="164" endWordPosition="167"> several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer fr</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Mou</author>
<author>Hao Peng</author>
<author>Ge Li</author>
<author>Yan Xu</author>
<author>Lu Zhang</author>
<author>Zhi Jin</author>
</authors>
<title>Tree-based convolution: A new neural architecture for sentence modeling.</title>
<date>2015</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing</booktitle>
<note>(to appear).</note>
<contexts>
<context position="1447" citStr="Mou et al., 2015" startWordPosition="196" endWordPosition="199">hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural models to improve </context>
</contexts>
<marker>Mou, Peng, Li, Xu, Zhang, Jin, 2015</marker>
<rawString>Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015. Tree-based convolution: A new neural architecture for sentence modeling. In Proceedings of Conference on Empirical Methods in Natural Language Processing (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Senior</author>
<author>Georg Heigold</author>
<author>Marc’aurelio Ranzato</author>
<author>Ke Yang</author>
</authors>
<title>An empirical study of learning rates in deep neural networks for speech recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="6398" citStr="Senior et al., 2013" startWordPosition="930" endWordPosition="933">ification. Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due 1http://www.aclweb.org/anthology/S10-1006 2http://nlp.stanford.edu/sentiment/ to the recursive, and thus chaotic nature). Therefore, we applied power decay (Senior et al., 2013) with power equal to −1. For each strategy, we tried a large range of regularization coefficients, 10−9, · · · ,10−2, extensively from underfitting to no effect with granularity 10x. We ran the model 5 times with different initializations. We used mini-batch stochastic gradient descent; gradients are computed by standard backpropagation. For source code, please refer to our project website.3 It needs to be noticed that, the goal of this paper is not to outperform or reproduce state-of-the-art results. Instead, we would like to have a fair comparison. The testbed of our work is two widely studi</context>
</contexts>
<marker>Senior, Heigold, Ranzato, Yang, 2013</marker>
<rawString>Andrew Senior, Georg Heigold, Marc’aurelio Ranzato, and Ke Yang. 2013. An empirical study of learning rates in deep neural networks for speech recognition. In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric Huang</author>
<author>Andrew Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1428" citStr="Socher et al., 2011" startWordPosition="192" endWordPosition="195">sized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural</context>
<context position="5388" citStr="Socher et al., 2011" startWordPosition="777" endWordPosition="780">f two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other. Regarding the neural model, we applied Collobert’s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral. We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children’s information to their parent’s; the root vector is finally used for sentiment classification. Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric Huang, Andrew Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5588" citStr="Socher et al. (2012)" startWordPosition="808" endWordPosition="811"> and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentiment analysis. This is another testbed for neural NLP, aiming to predict the sentiment of a sentence. The dataset is the Stanford sentiment treebank (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral. We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children’s information to their parent’s; the root vector is finally used for sentiment classification. Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to preven</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Dropout: A simple way to prevent neural networks from overfitting.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>1929--1958</pages>
<contexts>
<context position="3245" citStr="Srivastava et al., 2014" startWordPosition="451" endWordPosition="455">ically, we focus on the following research questions in this paper. RQ 1: How do different regularization strategies typically behave in embedding-based neural networks? RQ 2: Can regularization coefficients be tuned incrementally during training so as to ease the burden of hyperparameter tuning? RQ 3: What is the effect of combining different regularization strategies? In this paper, we systematically and quantitatively compared four different regularization strategies, namely penalizing weights, penalizing embeddings, newly proposed word re-embedding (Labutov and Lipson, 2013), and dropout (Srivastava et al., 2014). We analyzed these regularization methods by two widely studied models and tasks. We also emphasized on incremental hyperparameter tuning and the combination of different regularization methods. Our experiments provide some interesting results: (1) Regularizations do help generalization, but their effect depends largely on the datasets’ size. (2) Penalizing E2-norm of embeddings helps optimization as well, improving training accuracy unexpectedly. (3) Incremental hyperparameter tuning achieves similar performance, indicat2106 Proceedings of the 2015 Conference on Empirical Methods in Natural </context>
<context position="8341" citStr="Srivastava et al., 2014" startWordPosition="1254" endWordPosition="1257">., 2015). However, we would like to analyze their effect separately, for embeddings are sparse in use. Let 4&apos; denote embeddings; then we have R = 114&apos;112. • Re-embedding words (Labutov and Lipson, 2013). Suppose 4&apos;0 denotes the original embeddings trained on a large corpus, and 4&apos; denotes the embeddings fine-tuned during supervised training. We would like to penalize the norm of the difference between 4&apos;0 and 4&apos;, i.e., R = 114&apos;0−4&apos;112. In the limit of penalty to infinity, the model is mathematically equivalent to “frozen embeddings,” where word vectors are used as surface features. • Dropout (Srivastava et al., 2014). In this strategy, each neural node is set to 0 with a predefined dropout probability p during training; when testing, all nodes are used, with activation multiplied by 1 − p. 3https://sites.google.com/site/regembeddingnn/ 2107 (c) Penalizing embeddings in Experiment I. 100 5 10 15 20 25 Epochs 100 90 80 70 60 50 λb = 0 λb = 10−5 λb = 10−4 λb = 10−3 Training Validation 100 5 10 15 20 25 Epochs 40 30 20 100 90 80 70 60 50 40 30 20 λreembed = 0 λreembed = 10−5 λreembed = 10−4 λreembed = 10−3 Training Validation (a) Penalizing weights in Experiment I. 100 90 80 70 60 50 λW = 0 λW = 10−5 λW = 10−</context>
</contexts>
<marker>Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, pages 1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7725" citStr="Tai et al., 2015" startWordPosition="1150" endWordPosition="1153"> as fair as possible. Therefore, we think that the results of this work can be generalized to similar scenarios. 3 Regularization Strategies In this section, we describe four regularization strategies used in our experiment. • Penalizing `2-norm of weights. Let E be the cross-entropy error for classification, and R be a regularization term. The overall cost function is J = E + AR, where A is the coefficient. In this case, R = 11W 112, and the coefficient is denoted as AW. • Penalizing `2-norm of embeddings. Some studies do not distinguish embeddings or connectional weights for regularization (Tai et al., 2015). However, we would like to analyze their effect separately, for embeddings are sparse in use. Let 4&apos; denote embeddings; then we have R = 114&apos;112. • Re-embedding words (Labutov and Lipson, 2013). Suppose 4&apos;0 denotes the original embeddings trained on a large corpus, and 4&apos; denotes the embeddings fine-tuned during supervised training. We would like to penalize the norm of the difference between 4&apos;0 and 4&apos;, i.e., R = 114&apos;0−4&apos;112. In the limit of penalty to infinity, the model is mathematically equivalent to “frozen embeddings,” where word vectors are used as surface features. • Dropout (Srivasta</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Xu</author>
<author>Lili Mou</author>
<author>Ge Li</author>
<author>Yunchuan Chen</author>
<author>Hao Peng</author>
<author>Zhi Jin</author>
</authors>
<title>Classifying relations via long short term memory networks along shortest dependency paths.</title>
<date>2015</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing</booktitle>
<note>(to appear).</note>
<contexts>
<context position="1509" citStr="Xu et al., 2015" startWordPosition="206" endWordPosition="209"> The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly proposed regulariz</context>
<context position="4641" citStr="Xu et al. (2015)" startWordPosition="653" endWordPosition="656">ffect. (4) Dropout performs slightly worse than E2 penalty in our experiments; however, provided very small E2 penalty, dropping out hidden units and penalizing `2-norm are generally complementary. (5) The newly proposed re-embedding words method is not effective in our experiments. 2 Tasks, Models, and Setup Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8.1 The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other. Regarding the neural model, we applied Collobert’s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentiment analysis. This is </context>
</contexts>
<marker>Xu, Mou, Li, Chen, Peng, Jin, 2015</marker>
<rawString>Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of Conference on Empirical Methods in Natural Language Processing (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In Proceedings of Computational Linguistics.</booktitle>
<contexts>
<context position="1491" citStr="Zeng et al., 2014" startWordPosition="202" endWordPosition="205">nt regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 1 Introduction Neural networks have exhibited considerable potential in various fields (Krizhevsky et al., 2012; Graves et al., 2013). In early years on neural NLP research, neural networks were used in language modeling (Bengio et al., 2003; Morin and Bengio, 2005; Mnih and Hinton, 2009); recently, they have been applied to various supervised tasks, such as named entity recognition (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2011; Mou et al., 2015), relation classification (Zeng et al., 2014; Xu et al., 2015), etc. In the field of NLP, neural networks are typically combined with word embeddings, which are usually first pretrained by unsupervised algorithms like Mikolov et al. (2013); then they are fed forward to standard neural models, fine-tuned during supervised learning. However, embedding-based neural networks usually suffer from severe overfitting because of the high dimensionality of parameters. ∗Equal contribution. †Corresponding author. A curious question is whether we can regularize embedding-based NLP neural models to improve generalization. Although existing and newly </context>
<context position="4619" citStr="Zeng et al. (2014)" startWordPosition="648" endWordPosition="651">nly serve as a “local” effect. (4) Dropout performs slightly worse than E2 penalty in our experiments; however, provided very small E2 penalty, dropping out hidden units and penalizing `2-norm are generally complementary. (5) The newly proposed re-embedding words method is not effective in our experiments. 2 Tasks, Models, and Setup Experiment I: Relation extraction. The dataset in this experiment comes from SemEval-2010 Task 8.1 The goal is to classify the relationship between two marked entities in each sentence. We refer interested readers to recent advances, e.g., Hashimoto et al. (2013), Zeng et al. (2014), and Xu et al. (2015). To make our task and model general, however, we do not consider entity tagging information; we do not distinguish the order of two entities either. In total, there are 10 labels, i.e., 9 different relations plus a default other. Regarding the neural model, we applied Collobert’s convolutional neural network (CNN) (Collobert and Weston, 2008) with minor modifications. The model comprises a fixed-window convolutional layer with size equal to 5, 0 padded at the end of each sentence; a max pooling layer; a tanh hidden layer; and a softmax output layer. Experiment II: Sentim</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhao</author>
<author>Zhengdong Lu</author>
<author>Pascal Poupart</author>
</authors>
<title>Self-adaptive hierarchical sentence model.</title>
<date>2015</date>
<booktitle>In Proceedings of Intenational Joint Conference in Artificial Intelligence.</booktitle>
<contexts>
<context position="5983" citStr="Zhao et al. (2015)" startWordPosition="873" endWordPosition="876">k (Socher et al., 2011)2; target labels are strongly/weakly positive/negative, or neutral. We used the recursive neural network (RNN), which is proposed in Socher et al. (2011), and further developed in Socher et al. (2012); Irsoy and Cardie (2014). RNNs make use of binarized constituency trees, and recursively encode children’s information to their parent’s; the root vector is finally used for sentiment classification. Experimental Setup. To setup a fair comparison, we set all layers to be 50-dimensional in advance (rather than by validation). Such setting has been used in previous work like Zhao et al. (2015). Our embeddings are pretrained on the Wikipedia corpus using Collobert and Weston (2008). The learning rate is 0.1 and fixed in Experiment I; for RNN, however, we found learning rate decay helps to prevent parameter blowup (probably due 1http://www.aclweb.org/anthology/S10-1006 2http://nlp.stanford.edu/sentiment/ to the recursive, and thus chaotic nature). Therefore, we applied power decay (Senior et al., 2013) with power equal to −1. For each strategy, we tried a large range of regularization coefficients, 10−9, · · · ,10−2, extensively from underfitting to no effect with granularity 10x. We</context>
</contexts>
<marker>Zhao, Lu, Poupart, 2015</marker>
<rawString>Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. In Proceedings of Intenational Joint Conference in Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040802">
<title confidence="0.970996">
Aligning Knowledge and Text Embeddings by Entity Descriptions
</title>
<author confidence="0.977716">
Huaping Zhong§ , Jianwen Zhang† , Zhen Wang§ , Hai Wan§ , Zheng Chen†
</author>
<affiliation confidence="0.81628">
§{zhonghp@mail2,wangzh56@mail2,wanhai@mail}.sysu.edu.cn
</affiliation>
<email confidence="0.948377">
†{jiazhan,zhengc}@microsoft.com
</email>
<note confidence="0.583587">
§Sun Yat-sen University †Microsoft Research
</note>
<sectionHeader confidence="0.983554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993785">
We study the problem of jointly em-
bedding a knowledge base and a text
corpus. The key issue is the alignment
model making sure the vectors of entities,
relations and words are in the same space.
Wang et al. (2014a) rely on Wikipedia an-
chors, making the applicable scope quite
limited. In this paper we propose a new
alignment model based on text descrip-
tions of entities, without dependency on
anchors. We require the embedding vector
of an entity not only to fit the structured
constraints in KBs but also to be equal to
the embedding vector computed from the
text description. Extensive experiments
show that, the proposed approach consis-
tently performs comparably or even better
than the method of Wang et al. (2014a),
which is encouraging as we do not use
any anchor information.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999673875">
Knowledge base embedding has attracted surging
interest recently. The aim is to learn continuous
vector representations (embeddings) for entities
and relations of a structured knowledge base (KB)
such as Freebase. Typically it optimizes a global
objective function over all the facts in the KB and
hence the embedding vector of an entity / relation
is expected to encode global information in the
KB. It is capable of reasoning missing facts in
a KB and helping facts extraction (Bordes et al.,
2011; Bordes et al., 2012; Bordes et al., 2013;
Socher et al., 2013; Chang et al., 2013; Wang et
al., 2014b; Lin et al., 2015).
Although seeming encouraging, the approaches
in the aforementioned literature suffer from two
common issues: (1) Embeddings are exclusive
to entities/relations within KBs. Computation
between KBs and text cannot be handled, which
are prevalent in practice. For example, in fact
extraction, a candidate value may be just a phrase
in text. (2) KB sparsity. The above approaches are
only based on structured facts of KBs, and thus
cannot work well on entities with few facts.
An important milestone, the approach of Wang
et al. (2014a) solves issue (1) by jointly embed-
ding entities, relations, and words into the same
vector space and hence is able to deal with word-
s/phrases beyond entities in KBs. The key com-
ponent is the so-called alignment model, which
makes sure the embeddings of entities, relations,
and words are in the same space. Two alignment
models are introduced there: one uses entity
names and another uses Wikipedia anchors. How-
ever, both of them have drawbacks. As reported in
the paper, using entity names severely pollutes the
embeddings of words. Thus it is not recommended
in practice. Using Wikipedia anchors completely
relies on the special data source and hence the
approach cannot be applied to other customer data.
To fully address the two issues, this paper pro-
poses a new alignment method, aligning by entity
descriptions. We only assume some entities in
KBs have text descriptions, which almost always
holds in practice. We require the embedding of
an entity not only fits the structured constraints
in KBs but also equals the vector computed from
the text description. Meanwhile, if an entity has
few facts, the description will provide information
for embedding, thus the issue of KB sparsity
is also well handled. We conduct extensive
experiments on the tasks of triplet classification,
link prediction, relational fact extraction, and
analogical reasoning to compare with the previous
approach (Wang et al., 2014a). Results show
that our approach consistently achieves better or
comparable performance.
</bodyText>
<page confidence="0.958328">
267
</page>
<note confidence="0.856004">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267–272,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.999587" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999492826923077">
TransE This is a representative knowledge em-
bedding model proposed by Bordes et al. (2013).
For a fact (h, r, t) in KBs, where h is the head en-
tity, r is the relation, and t is the tail entity, TransE
models the relation r as a translation vector r con-
necting the embeddings h and t of the two entities,
i.e., h + r is close to t. The model is simple, ef-
fective and efficient. Most knowledge embedding
models thereafter including this paper are variants
of this model (Wang et al., 2014b; Wang et al.,
2014a; Lin et al., 2015).
Skip-gram This is an efficient word embedding
method proposed by Mikolov et al. (2013a),
which learns word embeddings from word
concurrencies in text windows. Without any
supervision, it amazingly recovers the semantic
relations between words in a vector space such as
’King’ −’Queen’ ≈ ’Man’ − ’Women’. However,
as it is unsupervised, it cannot tell the exact
relation between two words.
Knowledge and Text Jointly Embedding
Wang et al. (2014a) combines knowledge embed-
ding and word embedding in a joint framework
so that the entities/relations and words are in the
same vector space and hence operators like inner
product (similarity) between them are meaning-
ful. This brings convenience to tasks requiring
computation between knowledge bases and text.
Meanwhile, jointly embedding utilizes informa-
tion from both structured KBs and unstructured
text and hence the knowledge embedding and
word embedding can be enhanced by each other.
Their model is composed of three components: a
knowledge model to embed entities and relations,
a text model to embed words, and an alignment
model to make sure entities/relations and words
are in the same vector space. The knowledge
model and text model are variants of TransE
and Skip-gram respectively. The key component
is the alignment model. They introduced two:
alignment by entity names and alignment by
Wikipedia anchors. (1) Alignment by Entity
Names makes a replicate of KB facts but replaces
each entity ID with its name string, i.e., the
vector of a name phrase is encouraged to equal
to the vector of the entity (identified by ID). It
has problems with ambiguous entity names and
observed polluting word embeddings thus it is not
recommended by the authors. (2) Alignment by
Wikipedia Anchors replaces the surface phrase
v of a Wikipedia anchor with its corresponding
Freebase entity ev and defines the likelihood
</bodyText>
<equation confidence="0.9902675">
�LAA = log Pr(w|ev) (1)
(w,v)EC,vEA
</equation>
<bodyText confidence="0.9999569">
where C is the collection of observed word and
context pairs and A refers to the set of all anchors
in Wikipedia. Pr(w|ev) is the probability of the
anchor predicting its context word, which takes a
form similar to Skip-gram for word embedding.
Alignment by anchors works well in both improv-
ing knowledge embedding and word embeddings.
However, it completely relies on the special data
source of Wikipedia anchors and cannot be applied
to other general data settings.
</bodyText>
<sectionHeader confidence="0.97878" genericHeader="method">
3 Alignment by Entity Descriptions
</sectionHeader>
<bodyText confidence="0.9999338">
We first describe the settings and notations. Giv-
en a knowledge base, i.e., a set of facts (h, r, t),
where h, t ∈ E (the set of entities) and r ∈ R (the
set of relations). Some entities have text descrip-
tions. The description of entity e is denoted as De.
wi,n is the nth word in the description of ei. Ni is
the length (in words) of the description of ei. We
try to learn embeddings ei, rj and wl for each en-
tity ei, relation rj and word wl respectively. The
vocabulary of words is V. The union vocabulary
of entities and words together is I = E ∪ V. In
this paper “word(s)” refers to “word(s)/phrase(s)”.
We follow the jointly embedding framework of
(Wang et al., 2014a), i.e., learning optimal embed-
dings by minimizing the following loss
</bodyText>
<equation confidence="0.944284">
L ({ei}, {rj}, {wl}) = LK + LT + LA, (2)
</equation>
<bodyText confidence="0.9990928">
where LK, LT and LA are the component loss
functions of the knowledge model, text model and
alignment model respectively. Our focus is on
a new alignment model LA while the knowledge
model LK and text model LT are the same as the
counterparts in (Wang et al., 2014a). However, to
make the content self-contained, we still need to
briefly explain LK and LT.
Knowledge Model Describes the plausibility of
a triplet (h, r, t) by defining
</bodyText>
<equation confidence="0.993989">
Pr(h|r, t) =
E˜hE_, ex{z(˜h, r, t)}p
exp{z(h, r, t)} (3)
</equation>
<bodyText confidence="0.98605">
where z(h, r, t) = b−0.5· kh+r−tk22, b = 7 as
suggested by Wang et al. (2014a). Pr(r|h, t) and
</bodyText>
<page confidence="0.984944">
268
</page>
<bodyText confidence="0.9968535">
Pr(t|h, r) are defined in the same way. The loss
function of knowledge model is then defined as
</bodyText>
<equation confidence="0.999402333333333">
�LK = − [log Pr(h|r, t)
(h,r,t)
+ log Pr(t|h, r) + log Pr(r|h, t)] (4)
</equation>
<bodyText confidence="0.9540535">
Text Model Defines the probability of a pair of
words w and v co-occurring in a text window:
</bodyText>
<equation confidence="0.975219">
exp{z(w,v)}
Pr(w|v) = (5)
&apos;˜wEV exp{z( ˜w, v)}
</equation>
<bodyText confidence="0.997309">
where z(w,v) = b−0.5·kw−vk22. Then the loss
function of text model is
</bodyText>
<equation confidence="0.9955855">
�LT = − log Pr(w|v) (6)
(w,v)
</equation>
<bodyText confidence="0.9994885">
Alignment Model This part is different from
Wang et al. (2014a). For each word w in the
description of entity e, we define Pr(w|e), the
conditional probability of predicting w given e:
</bodyText>
<equation confidence="0.9150345">
exp{z(e, w)}
Pr(w|e) = E˜wEV exp{z(e, ˜w)}, (7)
</equation>
<bodyText confidence="0.9994816">
where z(e, w) = b − 0.5 · ke − wk22. Notice that
e is the same vector of entity e appearing in the
knowledge model of Eq. (3).
We also define Pr(e|w) in the same way by re-
vising the normalization term
</bodyText>
<equation confidence="0.963387142857143">
exp{z(e, w)}
Pr(e|w) = (8)
,˜eE£ exp{z(˜e, w)}
Then the loss function of alignment model is
�LA = − � [log Pr(w|e) + log Pr(e|w)]
eE£ wED,
(9)
</equation>
<bodyText confidence="0.999955857142857">
Training We use stochastic gradient descent (S-
GD) to minimize the overall loss of Eq. (2), which
sequentially updates the embeddings. Negative
sampling is used to calculate the normalization
items over large vocabularies. We implement a
multi-threading version to deal with large data set-
s, where memory is shared and lock-free.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.988032">
We conduct experiments on the following tasks:
link prediction (Bordes et al., 2013), triplet clas-
sification (Socher et al., 2013), relational fact ex-
traction (Weston et al., 2013), and analogical rea-
soning (Mikolov et al., 2013b). The last one e-
valuates quality of word embeddings. We try
</bodyText>
<tableCaption confidence="0.993192">
Table 1: Link prediction results.
</tableCaption>
<table confidence="0.7500072">
HITS@10
Raw Filtered
TransE
Jointly(anchor)
Jointly(desp) 167 39 51.7 77.3
</table>
<tableCaption confidence="0.779351">
Table 2: Triplet classification results.
</tableCaption>
<table confidence="0.999176">
Type e - e w - e e - w w - w all
Separately 94.0 51.7 51.0 69.0 73.6
Jointly(anchor) 95.2 65.3 65.1 76.2 79.9
Jointly(desp) 96.1 66.7 66.1 76.4 80.9
</table>
<bodyText confidence="0.999288861111111">
to study whether the proposed alignment mod-
el, without using any anchor information, is able
to achieve comparable or better performance than
alignment by anchors. As to the methods, “Sep-
arately” denotes the method of separately embed-
ding knowledge bases and text. “Jointly(anchor)”
and “Jointly(name)” denote the jointly embedding
methods based on Alignment by Wikipedia An-
chors and Alignment by Entity Names in (Wang
et al., 2014a) respectively. “Jointly(desp)” is the
joint embedding method based on alignment by
entity descriptions.
Data For link prediction, FB15K from (Bordes
et al., 2013) is used as the knowledge base. For
triplet classification, a large dataset provided by
(Wang et al., 2014a) is used as the knowledge
base. Both sets are subsets of Freebase. For all
tasks, Wikipedia articles are used as the text cor-
pus. As many Wikipedia articles can be mapped
to Freebase entities, we regard a Wikipedia arti-
cle as the description for the corresponding entity
in Freebase. Following the settings in (Wang et
al., 2014a), we apply the same preprocessing step-
s, including sentence segmentation, tokenization,
and named entity recognition. We combine the
consecutive tokens covered by an anchor or iden-
tically tagged as “Location/Person/Organization”
and regard them as phrases.
Link Prediction This task aims to complete a
fact (h, r, t) in absence of h or t, simply based on
kh + r − tk. We follow the same protocol in (Bor-
des et al., 2013). We directly copy the results of
the baseline (TransE) from (Bordes et al., 2013)
and implement “Jointly(anchor)”. The results are
in Table 1. “MEAN” is the average rank of the true
absent entity. “HITS@10” is accuracy of the top
</bodyText>
<figure confidence="0.988470725">
MEAN
Metric
Raw Filtered
243 125 34.9 47.1
166 47 49.9 72.0
269
precision
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0 0.2 0.4 0.6 0.8 1.0
Recall
(a)
0.2
0.0 0.2 0.4 0.6 0.8 1.0
Recall
(b)
precision 1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
Mintz+Separately (0.518)
Mintz+Jointly(anchor) (0.584)
Mintz+Jointly(desp) (0.614)
Mintz (0.378)
MIML+Separately (0.634)
MIML+Jointly(anchor) (0.729)
MIML+Jointly(desp) (0.737)
MIML (0.564)
</figure>
<figureCaption confidence="0.9806765">
Figure 1: Precision-recall curves for relation extraction. (a) Mintz (Mintz et al., 2009) as base extractor
(b) MIML (Surdeanu et al., 2012) as base extractor.
</figureCaption>
<bodyText confidence="0.997770046875">
10 predictions containing the true entity. Lower
“MEAN” and higher “HITS@10” is better. “Raw”
and “Filtered” are two settings on processing can-
didates (Bordes et al., 2013).
We train “Jointly(anchor)” and “Joint-
ly(desp)” with the embedding dimension k
among {50, 100,150}, the learning rate α in
{0.01, 0.025}, the number of negative examples
per positive example c in {5,10}, the max skip-
range s in {5,10} and traverse the text corpus
with only 1 epoch. The best configurations of
“Jointly(anchor)” and “Jointly(desp)” are exactly
the same: k = 100, α = 0.025, c = 10, s = 5.
From the results, we observe that: (1) Both
jointly embedding methods are much better than
the baseline TransE, which demonstrates that ex-
ternal textual resources make entity embeddings
become more discriminative. Intuitively, “Joint-
ly(anchor)” indicates “how to use an entity in tex-
t”, while “Jointly(desp)” shows “what is the def-
inition/meaning of an entity”. Both are helpful
to distinguish an entity from others. (2) Under
the setting of “Raw”, “Jointly(desp)” and “Joint-
ly(anchor)” are comparable. In other settings
“Jointly(desp)” wins.
Triplet Classification This is a binary classifi-
cation task, predicting whether a candidate triplet
(h, r, t) is a correct fact or not. It is used in (Socher
et al., 2013; Wang et al., 2014b; Wang et al.,
2014a). We follow the same protocol in (Wang
et al., 2014a).
We train their models via our own implemen-
tation on our dataset. The results are in Table 2.
“e-e” means both sides of a triplet (h, r, t) are en-
tities in KB, “e-w” means the tail side is a word
out of KB entity vocabulary, similarly for “w-e”
and “w-w”. The best configurations of the mod-
els are: k = 150, α = 0.025, c = 10, s = 5 and
traversing the text corpus with 6 epochs.
The results reveal that: (1) Jointly embedding is
indeed effective. Both jointly embedding methods
can well handle the cases of “e-w”, “w-e” and “w-
w”, which means the vector computation between
entities/relations and words are really meaning-
ful. Meanwhile, even the case of “e-e” is also
improved. (2) Our method, “Jointly(desp)”, out-
performs “Jointly(anchor)” on all types of triplets.
We believe that the good performance of “Joint-
ly(desp)” is due to the appropriate design of the
alignment mechanism. Using entity’s description
information is a more straightforward and effec-
tive way to align entity embeddings and word em-
beddings.
Relational Fact Extraction This task is to ex-
tract facts (h, r, t) from plain text. Weston et al.
(2013) show that combing scores from TransE and
some text side base extractor achieved much bet-
ter precision-recall curve compared to the base
extractor. Wang et al. (2014a) confirm this ob-
servation and show that jointly embedding brings
further encouraging improvement over TransE. In
this experiment, we follow the same settings as
(Wang et al., 2014a) to investigate the perfor-
mance of our new alignment model. We use the
</bodyText>
<page confidence="0.998072">
270
</page>
<tableCaption confidence="0.992034">
Table 3: Analogical reasoning results
</tableCaption>
<table confidence="0.999302166666666">
Metric Acc. Words Acc. Phrases
Hits@10 Hits@10
Skip-gram 67.4 86.7 22.0 63.6
Jointly(anchor) 69.4 87.7 26.2 68.1
Jointly(name) 44.5 69.7 11.5 46.0
Jointly(desp) 69.3 88.3 49.0 86.5
</table>
<bodyText confidence="0.999492883333333">
same public dataset NYT+FB, released by Riedel
et al. (2010) and used in (Weston et al., 2013) and
(Wang et al., 2014a). We use Mintz (Mintz et al.,
2009) and MIML (Surdeanu et al., 2012) as our
base extractors.
In order to combine the score of a base extrac-
tor and the score from embeddings, we only re-
serve the testing triplets whose entitites and rela-
tions can be mapped to the embeddings learned
from the triplet classification experiment. Since
both Mintz and MIML are probabilistic models,
we use the same method in (Wang et al., 2014a) to
linearly combine the scores.
The precision-recall curves are plot in Fig. (1).
On both base extractors, the jointly embedding
methods outperform separate embedding. More-
over, “Jointly(desp)” is slightly better than “Joint-
ly(anchor)”, which is in accordance with the re-
sults from the link prediction experiment and the
triplet classification experiment.
Analogical Reasoning This task evaluates the
quality of word embeddings (Mikolov et al.,
2013b). We use the original dataset released
by (Mikolov et al., 2013b) and follow the same
evaluation protocol of (Wang et al., 2014a). For
a true analogical pair like (“France”, “Paris”) and
(“China”, “Beijing”), we hide “Beijing” and pre-
dict it by selecting the word from the vocabu-
lary whose vector has highest similarity with the
vector of “China” + “Paris” - “France”. We
use the word embeddings learned for the triplet
classification experiment and conduct the analogi-
cal reasoning experiment for “Skip-gram”, “Joint-
ly(anchor)”, “Jointly(name)” and “Jointly(desp)”.
Results are presented in Table 3. “Acc” is the
accuracy of the predicted word. “HITS@10” is the
accuracy of the top 10 candidates containing the
ground truth. The evaluation analogical pairs are
organized into two groups, “Words” and “Phras-
es”, by whether an analogical pair contains phras-
es (i.e., multiple words). From the table we ob-
serve that: (1) Both “Jointly(anchor)” and “Joint-
ly(desp)” outperform “Skip-gram”. (2) “Joint-
ly(desp)” achieves the best results, especially for
the case of “Phrases”. Both “Jointly(anchor)” and
“Skip-gram” only consider the context of words,
while “Jointly(desp)” not only consider the con-
text but also use the whole document to disam-
biguate words. Intuitively, the whole document
is also a valuable resource to disambiguate word-
s. (3) We further verify that “Jointly(name)”, i.e.,
using entity names for alignment, indeed pollutes
word embeddings, which is consistent with the re-
ports in (Wang et al., 2014a).
The above four experiments are consisten-
t in results: without using any anchor informa-
tion, alignment by entity description is able to
achieve better or comparable performance, com-
pared to alignment by Wikipedia anchors pro-
posed by Wang et al. (2014a).
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985083333333">
We propose a new alignment model based on enti-
ty descriptions for jointly embedding a knowledge
base and a text corpus. Compared to the method
of alignment using Wikipedia anchors Wang et al.
(2014a), our method has no dependency on special
data sources of anchors and hence can be applied
to any knowledge bases with text descriptions for
entities. Extensive experiments on four prevalen-
t tasks to evaluate the quality of knowledge and
word embeddings produce very consistent results:
our alignment model achieves better or compara-
ble performance.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999504157894737">
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proceedings of the
25th AAAI Conference on Artificial Intelligence.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1–27.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602–1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
</reference>
<page confidence="0.961284">
271
</page>
<reference confidence="0.996884981481482">
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Zheng Chen. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of the Twenty-Eighth AAAI Conference on
Artificial Intelligence, pages 2181–2187.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their mention-
s without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148–163.
Springer.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465. Association for Computational Linguistics.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014a. Knowledge graph and text jointly em-
bedding. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics, pages 1591–1601.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of the
Twenty-Eighth AAAI Conference on Artificial Intel-
ligence, pages 1112–1119.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. arXiv preprint arXiv:1307.7973.
</reference>
<page confidence="0.997151">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973398">
<title confidence="0.999935">Aligning Knowledge and Text Embeddings by Entity Descriptions</title>
<author confidence="0.996825">Jianwen Zhen Hai Zheng</author>
<affiliation confidence="0.999131">Yat-sen University Research</affiliation>
<abstract confidence="0.998883523809524">We study the problem of jointly embedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1571" citStr="Bordes et al., 2011" startWordPosition="244" endWordPosition="247">er than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An </context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the 25th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>A semantic matching energy function for learning with multi-relational data.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="1592" citStr="Bordes et al., 2012" startWordPosition="248" endWordPosition="251"> Wang et al. (2014a), which is encouraging as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, </context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. A semantic matching energy function for learning with multi-relational data. Machine Learning, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2787--2795</pages>
<contexts>
<context position="1613" citStr="Bordes et al., 2013" startWordPosition="252" endWordPosition="255"> which is encouraging as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang </context>
<context position="4032" citStr="Bordes et al. (2013)" startWordPosition="634" endWordPosition="637">rsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach (Wang et al., 2014a). Results show that our approach consistently achieves better or comparable performance. 267 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267–272, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Wit</context>
<context position="9658" citStr="Bordes et al., 2013" startWordPosition="1624" endWordPosition="1627">n the same way by revising the normalization term exp{z(e, w)} Pr(e|w) = (8) ,˜eE£ exp{z(˜e, w)} Then the loss function of alignment model is �LA = − � [log Pr(w|e) + log Pr(e|w)] eE£ wED, (9) Training We use stochastic gradient descent (SGD) to minimize the overall loss of Eq. (2), which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data sets, where memory is shared and lock-free. 4 Experiments We conduct experiments on the following tasks: link prediction (Bordes et al., 2013), triplet classification (Socher et al., 2013), relational fact extraction (Weston et al., 2013), and analogical reasoning (Mikolov et al., 2013b). The last one evaluates quality of word embeddings. We try Table 1: Link prediction results. HITS@10 Raw Filtered TransE Jointly(anchor) Jointly(desp) 167 39 51.7 77.3 Table 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to</context>
<context position="11616" citStr="Bordes et al., 2013" startWordPosition="1946" endWordPosition="1950">As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in (Wang et al., 2014a), we apply the same preprocessing steps, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or identically tagged as “Location/Person/Organization” and regard them as phrases. Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on kh + r − tk. We follow the same protocol in (Bordes et al., 2013). We directly copy the results of the baseline (TransE) from (Bordes et al., 2013) and implement “Jointly(anchor)”. The results are in Table 1. “MEAN” is the average rank of the true absent entity. “HITS@10” is accuracy of the top MEAN Metric Raw Filtered 243 125 34.9 47.1 166 47 49.9 72.0 269 precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Recall (a) 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Recall (b) precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Mintz+Separately (0.518) Mintz+Jointly(anchor) (0.584) Mintz+Jointly(desp) (0.614) Mintz (0.378) MIML+Separately (0.634) MIML+Jointly(ancho</context>
</contexts>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-relational latent semantic analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1602--1612</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1654" citStr="Chang et al., 2013" startWordPosition="260" endWordPosition="263"> anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang et al. (2014a) solves issue (1) by jointl</context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yankai Lin</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Yang Liu</author>
<author>Zheng Chen</author>
</authors>
<title>Learning entity and relation embeddings for knowledge graph completion.</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>2181--2187</pages>
<contexts>
<context position="1693" citStr="Lin et al., 2015" startWordPosition="268" endWordPosition="271">ledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang et al. (2014a) solves issue (1) by jointly embedding entities, relations, and wo</context>
<context position="4468" citStr="Lin et al., 2015" startWordPosition="721" endWordPosition="724">al, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as ’King’ −’Queen’ ≈ ’Man’ − ’Women’. However, as it is unsupervised, it cannot tell the exact relation between two words. Knowledge and Text Jointly Embedding Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence ope</context>
</contexts>
<marker>Lin, Liu, Sun, Liu, Chen, 2015</marker>
<rawString>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Zheng Chen. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 2181–2187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="4555" citStr="Mikolov et al. (2013" startWordPosition="735" endWordPosition="738">d Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as ’King’ −’Queen’ ≈ ’Man’ − ’Women’. However, as it is unsupervised, it cannot tell the exact relation between two words. Knowledge and Text Jointly Embedding Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings conveni</context>
<context position="9802" citStr="Mikolov et al., 2013" startWordPosition="1647" endWordPosition="1650">A = − � [log Pr(w|e) + log Pr(e|w)] eE£ wED, (9) Training We use stochastic gradient descent (SGD) to minimize the overall loss of Eq. (2), which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data sets, where memory is shared and lock-free. 4 Experiments We conduct experiments on the following tasks: link prediction (Bordes et al., 2013), triplet classification (Socher et al., 2013), relational fact extraction (Weston et al., 2013), and analogical reasoning (Mikolov et al., 2013b). The last one evaluates quality of word embeddings. We try Table 1: Link prediction results. HITS@10 Raw Filtered TransE Jointly(anchor) Jointly(desp) 167 39 51.7 77.3 Table 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, “Separately” denotes the method of separately embedding </context>
<context position="16564" citStr="Mikolov et al., 2013" startWordPosition="2764" endWordPosition="2767">d to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly better than “Jointly(anchor)”, which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b). We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of (Wang et al., 2014a). For a true analogical pair like (“France”, “Paris”) and (“China”, “Beijing”), we hide “Beijing” and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of “China” + “Paris” - “France”. We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for “Skip-gram”, “Jointly(anchor)”, “Jointly(name)” and “Jointly(desp)”. Results are presented in Table </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4555" citStr="Mikolov et al. (2013" startWordPosition="735" endWordPosition="738">d Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, effective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as ’King’ −’Queen’ ≈ ’Man’ − ’Women’. However, as it is unsupervised, it cannot tell the exact relation between two words. Knowledge and Text Jointly Embedding Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings conveni</context>
<context position="9802" citStr="Mikolov et al., 2013" startWordPosition="1647" endWordPosition="1650">A = − � [log Pr(w|e) + log Pr(e|w)] eE£ wED, (9) Training We use stochastic gradient descent (SGD) to minimize the overall loss of Eq. (2), which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data sets, where memory is shared and lock-free. 4 Experiments We conduct experiments on the following tasks: link prediction (Bordes et al., 2013), triplet classification (Socher et al., 2013), relational fact extraction (Weston et al., 2013), and analogical reasoning (Mikolov et al., 2013b). The last one evaluates quality of word embeddings. We try Table 1: Link prediction results. HITS@10 Raw Filtered TransE Jointly(anchor) Jointly(desp) 167 39 51.7 77.3 Table 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, “Separately” denotes the method of separately embedding </context>
<context position="16564" citStr="Mikolov et al., 2013" startWordPosition="2764" endWordPosition="2767">d to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly better than “Jointly(anchor)”, which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b). We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of (Wang et al., 2014a). For a true analogical pair like (“France”, “Paris”) and (“China”, “Beijing”), we hide “Beijing” and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of “China” + “Paris” - “France”. We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for “Skip-gram”, “Jointly(anchor)”, “Jointly(name)” and “Jointly(desp)”. Results are presented in Table </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12356" citStr="Mintz et al., 2009" startWordPosition="2065" endWordPosition="2068">esults are in Table 1. “MEAN” is the average rank of the true absent entity. “HITS@10” is accuracy of the top MEAN Metric Raw Filtered 243 125 34.9 47.1 166 47 49.9 72.0 269 precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Recall (a) 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Recall (b) precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Mintz+Separately (0.518) Mintz+Jointly(anchor) (0.584) Mintz+Jointly(desp) (0.614) Mintz (0.378) MIML+Separately (0.634) MIML+Jointly(anchor) (0.729) MIML+Jointly(desp) (0.737) MIML (0.564) Figure 1: Precision-recall curves for relation extraction. (a) Mintz (Mintz et al., 2009) as base extractor (b) MIML (Surdeanu et al., 2012) as base extractor. 10 predictions containing the true entity. Lower “MEAN” and higher “HITS@10” is better. “Raw” and “Filtered” are two settings on processing candidates (Bordes et al., 2013). We train “Jointly(anchor)” and “Jointly(desp)” with the embedding dimension k among {50, 100,150}, the learning rate α in {0.01, 0.025}, the number of negative examples per positive example c in {5,10}, the max skiprange s in {5,10} and traverse the text corpus with only 1 epoch. The best configurations of “Jointly(anchor)” and “Jointly(desp)” are exact</context>
<context position="15726" citStr="Mintz et al., 2009" startWordPosition="2631" endWordPosition="2634">his observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11.5 46.0 Jointly(desp) 69.3 88.3 49.0 86.5 same public dataset NYT+FB, released by Riedel et al. (2010) and used in (Weston et al., 2013) and (Wang et al., 2014a). We use Mintz (Mintz et al., 2009) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly bette</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>148--163</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="15632" citStr="Riedel et al. (2010)" startWordPosition="2612" endWordPosition="2615">uch better precision-recall curve compared to the base extractor. Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11.5 46.0 Jointly(desp) 69.3 88.3 49.0 86.5 same public dataset NYT+FB, released by Riedel et al. (2010) and used in (Weston et al., 2013) and (Wang et al., 2014a). We use Mintz (Mintz et al., 2009) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointl</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>926--934</pages>
<contexts>
<context position="1634" citStr="Socher et al., 2013" startWordPosition="256" endWordPosition="259"> as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang et al. (2014a) solves</context>
<context position="9704" citStr="Socher et al., 2013" startWordPosition="1631" endWordPosition="1634">erm exp{z(e, w)} Pr(e|w) = (8) ,˜eE£ exp{z(˜e, w)} Then the loss function of alignment model is �LA = − � [log Pr(w|e) + log Pr(e|w)] eE£ wED, (9) Training We use stochastic gradient descent (SGD) to minimize the overall loss of Eq. (2), which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data sets, where memory is shared and lock-free. 4 Experiments We conduct experiments on the following tasks: link prediction (Bordes et al., 2013), triplet classification (Socher et al., 2013), relational fact extraction (Weston et al., 2013), and analogical reasoning (Mikolov et al., 2013b). The last one evaluates quality of word embeddings. We try Table 1: Link prediction results. HITS@10 Raw Filtered TransE Jointly(anchor) Jointly(desp) 167 39 51.7 77.3 Table 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than</context>
<context position="13717" citStr="Socher et al., 2013" startWordPosition="2286" endWordPosition="2289"> baseline TransE, which demonstrates that external textual resources make entity embeddings become more discriminative. Intuitively, “Jointly(anchor)” indicates “how to use an entity in text”, while “Jointly(desp)” shows “what is the definition/meaning of an entity”. Both are helpful to distinguish an entity from others. (2) Under the setting of “Raw”, “Jointly(desp)” and “Jointly(anchor)” are comparable. In other settings “Jointly(desp)” wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; Wang et al., 2014a). We follow the same protocol in (Wang et al., 2014a). We train their models via our own implementation on our dataset. The results are in Table 2. “e-e” means both sides of a triplet (h, r, t) are entities in KB, “e-w” means the tail side is a word out of KB entity vocabulary, similarly for “w-e” and “w-w”. The best configurations of the models are: k = 150, α = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs. The results reveal that: (1) Jointly embedding is indeed effective. Both jointly embedding methods can well handle the cases o</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12407" citStr="Surdeanu et al., 2012" startWordPosition="2074" endWordPosition="2077">k of the true absent entity. “HITS@10” is accuracy of the top MEAN Metric Raw Filtered 243 125 34.9 47.1 166 47 49.9 72.0 269 precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Recall (a) 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Recall (b) precision 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Mintz+Separately (0.518) Mintz+Jointly(anchor) (0.584) Mintz+Jointly(desp) (0.614) Mintz (0.378) MIML+Separately (0.634) MIML+Jointly(anchor) (0.729) MIML+Jointly(desp) (0.737) MIML (0.564) Figure 1: Precision-recall curves for relation extraction. (a) Mintz (Mintz et al., 2009) as base extractor (b) MIML (Surdeanu et al., 2012) as base extractor. 10 predictions containing the true entity. Lower “MEAN” and higher “HITS@10” is better. “Raw” and “Filtered” are two settings on processing candidates (Bordes et al., 2013). We train “Jointly(anchor)” and “Jointly(desp)” with the embedding dimension k among {50, 100,150}, the learning rate α in {0.01, 0.025}, the number of negative examples per positive example c in {5,10}, the max skiprange s in {5,10} and traverse the text corpus with only 1 epoch. The best configurations of “Jointly(anchor)” and “Jointly(desp)” are exactly the same: k = 100, α = 0.025, c = 10, s = 5. Fro</context>
<context position="15759" citStr="Surdeanu et al., 2012" startWordPosition="2637" endWordPosition="2640">jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11.5 46.0 Jointly(desp) 69.3 88.3 49.0 86.5 same public dataset NYT+FB, released by Riedel et al. (2010) and used in (Weston et al., 2013) and (Wang et al., 2014a). We use Mintz (Mintz et al., 2009) and MIML (Surdeanu et al., 2012) as our base extractors. In order to combine the score of a base extractor and the score from embeddings, we only reserve the testing triplets whose entitites and relations can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly better than “Jointly(anchor)”, which i</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 455– 465. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph and text jointly embedding.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics,</booktitle>
<pages>1591--1601</pages>
<contexts>
<context position="991" citStr="Wang et al. (2014" startWordPosition="150" endWordPosition="153">ignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 201</context>
<context position="2225" citStr="Wang et al. (2014" startWordPosition="352" endWordPosition="355"> 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang et al. (2014a) solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors. However, both of them have drawbacks. As reported in the paper, using entity names severely pollutes the embeddings of words. Thus it is not recommended in practice. Using Wikipedia anchors </context>
<context position="3639" citStr="Wang et al., 2014" startWordPosition="580" endWordPosition="583">y entity descriptions. We only assume some entities in KBs have text descriptions, which almost always holds in practice. We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description. Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach (Wang et al., 2014a). Results show that our approach consistently achieves better or comparable performance. 267 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267–272, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two ent</context>
<context position="4913" citStr="Wang et al. (2014" startWordPosition="791" endWordPosition="794">fective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as ’King’ −’Queen’ ≈ ’Man’ − ’Women’. However, as it is unsupervised, it cannot tell the exact relation between two words. Knowledge and Text Jointly Embedding Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings convenience to tasks requiring computation between knowledge bases and text. Meanwhile, jointly embedding utilizes information from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other. Their model is composed of three components: a knowledge model to embed entities and relations, a text mode</context>
<context position="7544" citStr="Wang et al., 2014" startWordPosition="1238" endWordPosition="1241">knowledge base, i.e., a set of facts (h, r, t), where h, t ∈ E (the set of entities) and r ∈ R (the set of relations). Some entities have text descriptions. The description of entity e is denoted as De. wi,n is the nth word in the description of ei. Ni is the length (in words) of the description of ei. We try to learn embeddings ei, rj and wl for each entity ei, relation rj and word wl respectively. The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper “word(s)” refers to “word(s)/phrase(s)”. We follow the jointly embedding framework of (Wang et al., 2014a), i.e., learning optimal embeddings by minimizing the following loss L ({ei}, {rj}, {wl}) = LK + LT + LA, (2) where LK, LT and LA are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model LA while the knowledge model LK and text model LT are the same as the counterparts in (Wang et al., 2014a). However, to make the content self-contained, we still need to briefly explain LK and LT. Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining Pr(h|r, t) = E˜hE_, ex{z(˜h, r, t)}p exp{z(h, r, t)} </context>
<context position="10594" citStr="Wang et al., 2014" startWordPosition="1777" endWordPosition="1780">e 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, “Separately” denotes the method of separately embedding knowledge bases and text. “Jointly(anchor)” and “Jointly(name)” denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in (Wang et al., 2014a) respectively. “Jointly(desp)” is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from (Bordes et al., 2013) is used as the knowledge base. For triplet classification, a large dataset provided by (Wang et al., 2014a) is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text corpus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in (Wang et al., 2014</context>
<context position="13736" citStr="Wang et al., 2014" startWordPosition="2290" endWordPosition="2293">ch demonstrates that external textual resources make entity embeddings become more discriminative. Intuitively, “Jointly(anchor)” indicates “how to use an entity in text”, while “Jointly(desp)” shows “what is the definition/meaning of an entity”. Both are helpful to distinguish an entity from others. (2) Under the setting of “Raw”, “Jointly(desp)” and “Jointly(anchor)” are comparable. In other settings “Jointly(desp)” wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; Wang et al., 2014a). We follow the same protocol in (Wang et al., 2014a). We train their models via our own implementation on our dataset. The results are in Table 2. “e-e” means both sides of a triplet (h, r, t) are entities in KB, “e-w” means the tail side is a word out of KB entity vocabulary, similarly for “w-e” and “w-w”. The best configurations of the models are: k = 150, α = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs. The results reveal that: (1) Jointly embedding is indeed effective. Both jointly embedding methods can well handle the cases of “e-w”, “w-e” and </context>
<context position="15095" citStr="Wang et al. (2014" startWordPosition="2528" endWordPosition="2531"> improved. (2) Our method, “Jointly(desp)”, outperforms “Jointly(anchor)” on all types of triplets. We believe that the good performance of “Jointly(desp)” is due to the appropriate design of the alignment mechanism. Using entity’s description information is a more straightforward and effective way to align entity embeddings and word embeddings. Relational Fact Extraction This task is to extract facts (h, r, t) from plain text. Weston et al. (2013) show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11.5 46.0 Jointly(desp) 69.3 88.3 49.0 86.5 same public dataset NYT+FB, released by Riedel et al. (2010) and used in (Weston et al., 2013) and (Wang et al., 2014a). We</context>
<context position="16693" citStr="Wang et al., 2014" startWordPosition="2786" endWordPosition="2789">he same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly better than “Jointly(anchor)”, which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b). We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of (Wang et al., 2014a). For a true analogical pair like (“France”, “Paris”) and (“China”, “Beijing”), we hide “Beijing” and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of “China” + “Paris” - “France”. We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for “Skip-gram”, “Jointly(anchor)”, “Jointly(name)” and “Jointly(desp)”. Results are presented in Table 3. “Acc” is the accuracy of the predicted word. “HITS@10” is the accuracy of the top 10 candidates containing the ground truth. T</context>
<context position="18077" citStr="Wang et al., 2014" startWordPosition="2999" endWordPosition="3002">ble we observe that: (1) Both “Jointly(anchor)” and “Jointly(desp)” outperform “Skip-gram”. (2) “Jointly(desp)” achieves the best results, especially for the case of “Phrases”. Both “Jointly(anchor)” and “Skip-gram” only consider the context of words, while “Jointly(desp)” not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that “Jointly(name)”, i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (Wang et al., 2014a). The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a). 5 Conclusion We propose a new alignment model based on entity descriptions for jointly embedding a knowledge base and a text corpus. Compared to the method of alignment using Wikipedia anchors Wang et al. (2014a), our method has no dependency on special data sources of anchors and hence can be applied to any knowledge bases with text descrip</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014a. Knowledge graph and text jointly embedding. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 1591–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph embedding by translating on hyperplanes.</title>
<date>2014</date>
<booktitle>In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1112--1119</pages>
<contexts>
<context position="991" citStr="Wang et al. (2014" startWordPosition="150" endWordPosition="153">ignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information. 1 Introduction Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction (Bordes et al., 2011; Bordes et al., 201</context>
<context position="2225" citStr="Wang et al. (2014" startWordPosition="352" endWordPosition="355"> 2013; Socher et al., 2013; Chang et al., 2013; Wang et al., 2014b; Lin et al., 2015). Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts. An important milestone, the approach of Wang et al. (2014a) solves issue (1) by jointly embedding entities, relations, and words into the same vector space and hence is able to deal with words/phrases beyond entities in KBs. The key component is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors. However, both of them have drawbacks. As reported in the paper, using entity names severely pollutes the embeddings of words. Thus it is not recommended in practice. Using Wikipedia anchors </context>
<context position="3639" citStr="Wang et al., 2014" startWordPosition="580" endWordPosition="583">y entity descriptions. We only assume some entities in KBs have text descriptions, which almost always holds in practice. We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description. Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach (Wang et al., 2014a). Results show that our approach consistently achieves better or comparable performance. 267 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267–272, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work TransE This is a representative knowledge embedding model proposed by Bordes et al. (2013). For a fact (h, r, t) in KBs, where h is the head entity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r connecting the embeddings h and t of the two ent</context>
<context position="4913" citStr="Wang et al. (2014" startWordPosition="791" endWordPosition="794">fective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model (Wang et al., 2014b; Wang et al., 2014a; Lin et al., 2015). Skip-gram This is an efficient word embedding method proposed by Mikolov et al. (2013a), which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as ’King’ −’Queen’ ≈ ’Man’ − ’Women’. However, as it is unsupervised, it cannot tell the exact relation between two words. Knowledge and Text Jointly Embedding Wang et al. (2014a) combines knowledge embedding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaningful. This brings convenience to tasks requiring computation between knowledge bases and text. Meanwhile, jointly embedding utilizes information from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other. Their model is composed of three components: a knowledge model to embed entities and relations, a text mode</context>
<context position="7544" citStr="Wang et al., 2014" startWordPosition="1238" endWordPosition="1241">knowledge base, i.e., a set of facts (h, r, t), where h, t ∈ E (the set of entities) and r ∈ R (the set of relations). Some entities have text descriptions. The description of entity e is denoted as De. wi,n is the nth word in the description of ei. Ni is the length (in words) of the description of ei. We try to learn embeddings ei, rj and wl for each entity ei, relation rj and word wl respectively. The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper “word(s)” refers to “word(s)/phrase(s)”. We follow the jointly embedding framework of (Wang et al., 2014a), i.e., learning optimal embeddings by minimizing the following loss L ({ei}, {rj}, {wl}) = LK + LT + LA, (2) where LK, LT and LA are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model LA while the knowledge model LK and text model LT are the same as the counterparts in (Wang et al., 2014a). However, to make the content self-contained, we still need to briefly explain LK and LT. Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining Pr(h|r, t) = E˜hE_, ex{z(˜h, r, t)}p exp{z(h, r, t)} </context>
<context position="10594" citStr="Wang et al., 2014" startWordPosition="1777" endWordPosition="1780">e 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, “Separately” denotes the method of separately embedding knowledge bases and text. “Jointly(anchor)” and “Jointly(name)” denote the jointly embedding methods based on Alignment by Wikipedia Anchors and Alignment by Entity Names in (Wang et al., 2014a) respectively. “Jointly(desp)” is the joint embedding method based on alignment by entity descriptions. Data For link prediction, FB15K from (Bordes et al., 2013) is used as the knowledge base. For triplet classification, a large dataset provided by (Wang et al., 2014a) is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text corpus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia article as the description for the corresponding entity in Freebase. Following the settings in (Wang et al., 2014</context>
<context position="13736" citStr="Wang et al., 2014" startWordPosition="2290" endWordPosition="2293">ch demonstrates that external textual resources make entity embeddings become more discriminative. Intuitively, “Jointly(anchor)” indicates “how to use an entity in text”, while “Jointly(desp)” shows “what is the definition/meaning of an entity”. Both are helpful to distinguish an entity from others. (2) Under the setting of “Raw”, “Jointly(desp)” and “Jointly(anchor)” are comparable. In other settings “Jointly(desp)” wins. Triplet Classification This is a binary classification task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in (Socher et al., 2013; Wang et al., 2014b; Wang et al., 2014a). We follow the same protocol in (Wang et al., 2014a). We train their models via our own implementation on our dataset. The results are in Table 2. “e-e” means both sides of a triplet (h, r, t) are entities in KB, “e-w” means the tail side is a word out of KB entity vocabulary, similarly for “w-e” and “w-w”. The best configurations of the models are: k = 150, α = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs. The results reveal that: (1) Jointly embedding is indeed effective. Both jointly embedding methods can well handle the cases of “e-w”, “w-e” and </context>
<context position="15095" citStr="Wang et al. (2014" startWordPosition="2528" endWordPosition="2531"> improved. (2) Our method, “Jointly(desp)”, outperforms “Jointly(anchor)” on all types of triplets. We believe that the good performance of “Jointly(desp)” is due to the appropriate design of the alignment mechanism. Using entity’s description information is a more straightforward and effective way to align entity embeddings and word embeddings. Relational Fact Extraction This task is to extract facts (h, r, t) from plain text. Weston et al. (2013) show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11.5 46.0 Jointly(desp) 69.3 88.3 49.0 86.5 same public dataset NYT+FB, released by Riedel et al. (2010) and used in (Weston et al., 2013) and (Wang et al., 2014a). We</context>
<context position="16693" citStr="Wang et al., 2014" startWordPosition="2786" endWordPosition="2789">he same method in (Wang et al., 2014a) to linearly combine the scores. The precision-recall curves are plot in Fig. (1). On both base extractors, the jointly embedding methods outperform separate embedding. Moreover, “Jointly(desp)” is slightly better than “Jointly(anchor)”, which is in accordance with the results from the link prediction experiment and the triplet classification experiment. Analogical Reasoning This task evaluates the quality of word embeddings (Mikolov et al., 2013b). We use the original dataset released by (Mikolov et al., 2013b) and follow the same evaluation protocol of (Wang et al., 2014a). For a true analogical pair like (“France”, “Paris”) and (“China”, “Beijing”), we hide “Beijing” and predict it by selecting the word from the vocabulary whose vector has highest similarity with the vector of “China” + “Paris” - “France”. We use the word embeddings learned for the triplet classification experiment and conduct the analogical reasoning experiment for “Skip-gram”, “Jointly(anchor)”, “Jointly(name)” and “Jointly(desp)”. Results are presented in Table 3. “Acc” is the accuracy of the predicted word. “HITS@10” is the accuracy of the top 10 candidates containing the ground truth. T</context>
<context position="18077" citStr="Wang et al., 2014" startWordPosition="2999" endWordPosition="3002">ble we observe that: (1) Both “Jointly(anchor)” and “Jointly(desp)” outperform “Skip-gram”. (2) “Jointly(desp)” achieves the best results, especially for the case of “Phrases”. Both “Jointly(anchor)” and “Skip-gram” only consider the context of words, while “Jointly(desp)” not only consider the context but also use the whole document to disambiguate words. Intuitively, the whole document is also a valuable resource to disambiguate words. (3) We further verify that “Jointly(name)”, i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the reports in (Wang et al., 2014a). The above four experiments are consistent in results: without using any anchor information, alignment by entity description is able to achieve better or comparable performance, compared to alignment by Wikipedia anchors proposed by Wang et al. (2014a). 5 Conclusion We propose a new alignment model based on entity descriptions for jointly embedding a knowledge base and a text corpus. Compared to the method of alignment using Wikipedia anchors Wang et al. (2014a), our method has no dependency on special data sources of anchors and hence can be applied to any knowledge bases with text descrip</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014b. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction. arXiv preprint arXiv:1307.7973.</title>
<date>2013</date>
<contexts>
<context position="9754" citStr="Weston et al., 2013" startWordPosition="1639" endWordPosition="1642"> Then the loss function of alignment model is �LA = − � [log Pr(w|e) + log Pr(e|w)] eE£ wED, (9) Training We use stochastic gradient descent (SGD) to minimize the overall loss of Eq. (2), which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data sets, where memory is shared and lock-free. 4 Experiments We conduct experiments on the following tasks: link prediction (Bordes et al., 2013), triplet classification (Socher et al., 2013), relational fact extraction (Weston et al., 2013), and analogical reasoning (Mikolov et al., 2013b). The last one evaluates quality of word embeddings. We try Table 1: Link prediction results. HITS@10 Raw Filtered TransE Jointly(anchor) Jointly(desp) 167 39 51.7 77.3 Table 2: Triplet classification results. Type e - e w - e e - w w - w all Separately 94.0 51.7 51.0 69.0 73.6 Jointly(anchor) 95.2 65.3 65.1 76.2 79.9 Jointly(desp) 96.1 66.7 66.1 76.4 80.9 to study whether the proposed alignment model, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, “Separat</context>
<context position="14930" citStr="Weston et al. (2013)" startWordPosition="2501" endWordPosition="2504">cases of “e-w”, “w-e” and “ww”, which means the vector computation between entities/relations and words are really meaningful. Meanwhile, even the case of “e-e” is also improved. (2) Our method, “Jointly(desp)”, outperforms “Jointly(anchor)” on all types of triplets. We believe that the good performance of “Jointly(desp)” is due to the appropriate design of the alignment mechanism. Using entity’s description information is a more straightforward and effective way to align entity embeddings and word embeddings. Relational Fact Extraction This task is to extract facts (h, r, t) from plain text. Weston et al. (2013) show that combing scores from TransE and some text side base extractor achieved much better precision-recall curve compared to the base extractor. Wang et al. (2014a) confirm this observation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as (Wang et al., 2014a) to investigate the performance of our new alignment model. We use the 270 Table 3: Analogical reasoning results Metric Acc. Words Acc. Phrases Hits@10 Hits@10 Skip-gram 67.4 86.7 22.0 63.6 Jointly(anchor) 69.4 87.7 26.2 68.1 Jointly(name) 44.5 69.7 11</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. arXiv preprint arXiv:1307.7973.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
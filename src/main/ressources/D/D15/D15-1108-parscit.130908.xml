<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.989497">
Dual Decomposition Inference for Graphical Models over Strings∗
</title>
<author confidence="0.996802">
Nanyun Peng and Ryan Cotterell and Jason Eisner
</author>
<affiliation confidence="0.999439">
Department of Computer Science, Johns Hopkins University
</affiliation>
<email confidence="0.993948">
{npeng1,ryan.cotterell,eisner}@jhu.edu
</email>
<sectionHeader confidence="0.997317" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929708333333">
We investigate dual decomposition for
joint MAP inference of many strings.
Given an arbitrary graphical model, we de-
compose it into small acyclic sub-models,
whose MAP configurations can be found
by finite-state composition and dynamic
programming. We force the solutions of
these subproblems to agree on overlap-
ping variables, by tuning Lagrange multi-
pliers for an adaptively expanding set of
variable-length n-gram count features.
This is the first inference method for ar-
bitrary graphical models over strings that
does not require approximations such as
random sampling, message simplification,
or a bound on string length. Provided that
the inference method terminates, it gives
a certificate of global optimality (though
MAP inference in our setting is undecid-
able in general). On our global phonolog-
ical inference problems, it always termi-
nates, and achieves more accurate results
than max-product and sum-product loopy
belief propagation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864571428571">
Graphical models allow expert modeling of com-
plex relations and interactions between random
variables. Since a graphical model with given pa-
rameters defines a probability distribution, it can
be used to reconstruct values for unobserved vari-
ables. The marginal inference problem is to com-
pute the posterior marginal distributions of these
variables. The MAP inference (or MPE) prob-
lem is to compute the single highest-probability
joint assignment to all the unobserved variables.
Inference in general graphical models is NP-
hard even when the variables’ values are finite dis-
crete values such as categories, tags or domains. In
this paper, we address the more challenging setting
</bodyText>
<footnote confidence="0.777832">
∗This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1423276.
</footnote>
<bodyText confidence="0.99998878125">
where the variables in the graphical models range
over strings. Thus, the domain of the variables is
an infinite space of discrete structures.
In NLP, such graphical models can deal with
large, incompletely observed lexicons. They could
be used to model diverse relationships among
strings that represent spellings or pronunciations;
morphemes, words, phrases (such as named enti-
ties and URLs), or utterances; standard or variant
forms; clean or noisy forms; contemporary or his-
torical forms; underlying or surface forms; source
or target language forms. Such relationships arise
in domains such as morphology, phonology, his-
torical linguistics, translation between related lan-
guages, and social media text analysis.
In this paper, we assume a given graphical
model, whose factors evaluate the relationships
among observed and unobserved strings.1 We
present a dual decomposition algorithm for MAP
inference, which returns a certifiably optimal so-
lution when it converges. We demonstrate our
method on a graphical model for phonology pro-
posed by Cotterell et al. (2015). We show that the
method generally converges and that it achieves
better results than alternatives.
The rest of the paper is arranged as follows: We
will review graphical models over strings in sec-
tion 2, and briefly introduce our sample problem
in section 3. Section 4 develops dual decompo-
sition inference for graphical models over strings.
Then our experimental setup and results are pre-
sented in sections 5 and 6, with some discussion.
</bodyText>
<sectionHeader confidence="0.997408" genericHeader="method">
2 Graphical Models Over Strings
</sectionHeader>
<subsectionHeader confidence="0.991227">
2.1 Factor Graphs and MAP Inference
</subsectionHeader>
<bodyText confidence="0.99198725">
To perform inference on a graphical model (di-
rected or undirected), one first converts the model
to a factor graph representation (Kschischang et
al., 2001). A factor graph is a finite bipartite
</bodyText>
<footnote confidence="0.938545666666667">
1In some task settings, it is also necessary to discover the
model topology along with the model parameters. In this pa-
per we do not treat that structure learning problem. However,
both structure learning and parameter learning need to call
inference—such as the method presented here—in order to
evaluate proposed topologies or improve their parameters.
</footnote>
<page confidence="0.856155">
917
</page>
<note confidence="0.9917245">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.9899778">
1) Underlying morphemes
Concatenation
2) Underlying words
Phonology
3) Surface words
</figure>
<figureCaption confidence="0.998397">
Figure 1: A fragment of the factor graph for the directed graphical model of Cotterell et al. (2015), displaying a possible
assignment to the variables (ellipses). The model explains each observed surface word as the result of applying phonology
to a concatenation of underlying morphemes. Shaded variables show the observed surface forms for four words: resignation,
resigns, damns, and damnation. The underlying pronunciations of these words are assumed to be more similar than their surface
pronunciations, because the words are known to share latent morphemes. The factor graph encodes what is shared. Each
observed word at layer 3 has a latent underlying form at layer 2, which is a deterministic concatenation of latent morphemes at
layer 1. The binary factors between layers 2 and 3 score each (underlying,surface) pair for its phonological plausibility. The
unary factors at layer 1 score each morpheme for its lexical plausibility. See Cotterell et al. (2015) for discussion of alternatives.
</figureCaption>
<table confidence="0.7189445">
rizajgn z e:Jən dæmn
rεz:gn#e:Jən rizajn#z dæmn#z dæmn#e:Jən
r,εz:gn’e:Jn riz’ajnz d’æmz d,æmn’e:Jn
resignation resigns damns damnation
</table>
<bodyText confidence="0.998437625">
graph over a set X = {X1, X2,...} of variables
and a set F of factors. An assignment to the vari-
ables is a vector of values x = (x1, x2, ...). Each
factor F E F is a real-valued function of x, but it
depends on a given xi only if F is connected to Xi
in the graph. Thus, a degree d-factor scores some
length-d subtuple of x. The score of the whole
joint assignment simply sums over all factors:
</bodyText>
<equation confidence="0.971594">
�score(x) def = F(x). (1)
F∈F
</equation>
<bodyText confidence="0.998285888888889">
We seek the x of maximum score that is con-
sistent with our partial observation of x. This
is a generic constraint satisfaction problem with
soft constraints. While our algorithm does not de-
pend on a probabilistic interpretation of the fac-
tor graph,2 it can be regarded as peforming max-
imum a posteriori (MAP) inference of the unob-
served variables, under the probability distribution
p(x) def = (1/Z) exp score(x).
</bodyText>
<subsectionHeader confidence="0.999542">
2.2 The String Case
</subsectionHeader>
<bodyText confidence="0.982280676470588">
Graphical models over strings have enjoyed some
attention in the NLP community. Tree-shaped
graphical models naturally model the evolution-
ary tree of word forms (Bouchard-Cˆot´e et al.,
2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein,
2010; Hall and Klein, 2011). Cyclic graphical
2E.g., it could be used for exactly computing the separa-
tion oracle when training a structural SVM (Tsochantaridis et
al., 2005; Finley and Joachims, 2007). Another use is mini-
mum Bayes risk decoding—computing the joint assignment
having minimum expected loss—if the loss function does not
decompose over the variables, but a factor graph can be con-
structed that evaluates the expected loss of any assignment.
models have been used to model morphological
paradigms (Dreyer and Eisner, 2009; Dreyer and
Eisner, 2011) and to reconstruct phonological un-
derlying forms of words (Cotterell et al., 2015).
The variables in such a model are strings of un-
bounded length: each variable Xi is permitted to
range over E∗ where E is some fixed, finite al-
phabet. As in previous work, we assume that a
degree-d factor is a d-way rational relation, i.e.,
a function of d strings that can be computed by
a d-tape weighted finite-state machine (WFSM)
(Mohri et al., 2002; Kempe et al., 2004). Such a
machine is called an acceptor (WFSA) if d = 1
or a transducer (WFST) if d = 2.3
Past work has shown how to approximately
sample from the distribution over x defined by
such a model (Bouchard-Cˆot´e et al., 2007), or ap-
proximately compute the distribution’s marginals
using variants of sum-product belief propaga-
tion (BP) (Dreyer and Eisner, 2009) and expecta-
tion propagation (EP) (Cotterell and Eisner, 2015).
</bodyText>
<subsectionHeader confidence="0.999796">
2.3 Finite-State Belief Propagation
</subsectionHeader>
<bodyText confidence="0.999389555555556">
BP iteratively updates messages between factors
and variables. Each message is a vector whose el-
ements score the possible values of a variable.
Murphy et al. (1999) discusses BP on cyclic
(“loopy”) graphs. For pedagogical reasons, sup-
pose momentarily that all factors have degree ≤ 2
(this loses no power). Then BP manipulates only
vectors and matrices—whose dimensionality de-
pends on the number of possible values of the vari-
</bodyText>
<footnote confidence="0.995228666666667">
3Finite-state software libraries often support only these
cases. Accordingly, Cotterell and Eisner (2015, Appendix
B.10) explain how to eliminate factors of degree d &gt; 2.
</footnote>
<page confidence="0.993547">
918
</page>
<bodyText confidence="0.999851735294118">
ables. In the string case, they have infinitely many
rows and columns, indexed by possible strings.
Dreyer and Eisner (2009) represented these in-
finite vectors and matrices by WFSAs and WF-
STs, respectively. They observed that the simple
linear-algebra operations used by BP can be im-
plemented by finite-state constructions. The point-
wise product of two vectors is the intersection of
their WFSAs; the marginalization of a matrix is
the projection of its WFST; a vector-matrix prod-
uct is computed by composing the WFSA with the
WFST and then projecting onto the output tape.
For degree &gt; 2, BP’s rank-d tensors become d-
tape WFSMs, and these constructions generalize.
Unfortunately, except in small acyclic models,
the BP messages—which are WFSAs—usually
become impractically large. Each intersection
or composition involves a cross-product construc-
tion. For example, when finding the marginal
distribution at a degree-d variable, intersecting d
WFSA messages having m states each may yield
a WFSA with up to md states. (Our models in
section 6 include variables with d up to 156.)
Combining many cross products, as BP iteratively
passes messages along a path in the factor graph,
leads to blowup that is exponential in the length of
the path—which in turn is unbounded if the graph
has cycles (Dreyer and Eisner, 2009), as ours do.
The usual solution is to prune or otherwise ap-
proximate the messages at each step. In particu-
lar, Cotterell and Eisner (2015) gave a principled
way to approximate the messages using variable-
length n-gram models, using an adaptive variant
of Expectation Propagation (Minka, 2001).
</bodyText>
<subsectionHeader confidence="0.994047">
2.4 Dual Decomposition Inference
</subsectionHeader>
<bodyText confidence="0.999416454545455">
In section 4, we will present a dual decomposition
(DD) method that decomposes the original com-
plex problem into many small subproblems that
are free of cycles and high degree nodes. BP can
solve each subproblem without approximation.4
The subproblems “communicate” through La-
grange multipliers that guide them towards agree-
ment on a single global solution. This information
is encoded in WFSAs that score possible values
of a string variable. DD incrementally adjusts the
WFSAs so as to encourage values that agree with
</bodyText>
<footnote confidence="0.64903">
4Such small BP problems commonly arise in NLP. In par-
ticular, using finite-state methods to decode a composition of
several finite-state noisy channels (Pereira and Riley, 1997;
Knight and Graehl, 1998) can be regarded as BP on a graph-
ical model over strings that has a linear-chain topology.
</footnote>
<bodyText confidence="0.995669125">
the variable’s average value across subproblems.
Unlike BP messages, the WFSAs in our DD
method will be restricted to be variable-length
n-gram models, similar to Cotterell and Eisner
(2015). They may still grow over time; but DD of-
ten halts while the WFSAs are still small. It halts
when its strings agree exactly, rather than when it
has converged up to a numerical tolerance, like BP.
</bodyText>
<subsectionHeader confidence="0.999627">
2.5 Switching Between Semirings
</subsectionHeader>
<bodyText confidence="0.999740470588235">
Our factors may be nondeterministic WFSMs. So
when F ∈ F scores a given d-tuple of string val-
ues, it may accept that d-tuple along multiple dif-
ferent WFSM paths with different scores, corre-
sponding to different alignments of the strings.
For purposes of MAP inference, we define F to
return the maximum of these path scores. That is,
we take the WFSMs to be defined with weights
in the (max, +) semiring (Mohri et al., 2002).
Equivalently, we are seeking the “best global solu-
tion” in the sense of choosing not only the strings
xi but also the alignments of the d-tuples.5
To do so, we must solve each DD subprob-
lem in the same sense. We use max-product BP.
This still applies the Dreyer-Eisner method of sec-
tion 2.3. Since these WFSMs are defined in the
(max, +) semiring, the method’s finite-state oper-
ations will combine weights using max and +.
MAP inference in our setting is in general com-
putationally undecidable.6 However, if DD con-
verges (as in our experiments), then its solution is
guaranteed to be the true MAP assignment.
In section 6, we will compare DD with (loopy)
max-product BP and (loopy) sum-product BP.
These respectively approximate MAP inference
and marginal inference over the entire factor
graph. Marginal inference computes marginal
string probabilities that sum (rather than maxi-
mize) over the choices of other strings and the
choices of paths. Thus, for sum-product BP, we
re-interpret the factor WFSMs as defined over the
(logadd, +) semiring. This means that the expo-
nentiated score assigned by a WFSM is the sum of
the exponentiated scores of the accepting paths.
</bodyText>
<footnote confidence="0.917014">
5This problem is more specifically called MPE inference.
6The trouble is that we cannot bound the length of the la-
</footnote>
<bodyText confidence="0.956305833333333">
tent strings. If we could, then we could encode them using a
finite set of boolean variables, and solve as an ILP problem.
But that would allow us to determine whether there exists a
MAP assignment with score ≥ 0. That is impossible in gen-
eral, because it would solve Post’s Correspondence Problem
as a simple special case (see Dreyer and Eisner (2009)).
</bodyText>
<page confidence="0.997209">
919
</page>
<figureCaption confidence="0.983130375">
Figure 2: To apply dual decomposition, we choose to decom-
pose 1 into one subproblem per surface word. Dashed lines
connect two or more variables from different subproblems
that correspond to the same variable in the original graph.
The method of Lagrange multipliers is used to force these
variables to have identical values. An additional unary factor
attached to each subproblem variable (not shown) is used to
incorporate its Lagrangian term.
</figureCaption>
<sectionHeader confidence="0.980989" genericHeader="method">
3 A Sample Task: Generative Phonology
</sectionHeader>
<bodyText confidence="0.999982333333333">
Before giving the formal details of our DD
method, we give a motivating example: a recently
proposed graphical model for morphophonology.
Cotterell et al. (2015) defined a Bayesian network
to describe the generative process of phonological
words. Our Figure 1 shows a conversion of their
model to a factor graph and explains what the vari-
ables and factors mean.
Inference on this graph performs unsupervised
discovery of latent strings. Given observed surface
representations of words (SRs), inference aims to
recover the underlying representations (URs) of
the words and their shared constituent morphemes.
The latter can then be used to predict held-out SRs.
Notice that the 8 edges in the first layer of Fig-
ure 1 form a cycle; such cycles make BP inexact.
Moreover, the figure shows only a schematic frag-
ment of the graphical model. In the actual exper-
iments, the graphical models have up to 829 vari-
ables, and the variables representing morpheme
URs are connected to up to 156 factors (because
many words share the same affix).
To handle the above challenges without ap-
proximation, we want to decompose the original
problem into subproblems where each subproblem
can be solved efficiently. In particular, we want
the subproblems to be free of cycles and high-
degree nodes. In our phonology example, each
observed word along with its correspondent latent
URs forms an ideal subproblem. This decomposi-
tion is shown in Figure 2.
While the subproblems can be solved efficiently
in isolation, they may share variables, as shown
by the dashed lines in Figure 2. DD repeatedly
modifies and re-solves the subproblems until they
agree on their shared variables.
</bodyText>
<sectionHeader confidence="0.995756" genericHeader="method">
4 Dual Decomposition
</sectionHeader>
<bodyText confidence="0.999648">
Dual decomposition is a general technique for
solving constrained optimization problems. It has
been widely used for MAP inference in graphi-
cal models (Komodakis et al., 2007; Komodakis
and Paragios, 2009; Koo et al., 2010; Martins et
al., 2011; Sontag et al., 2011; Rush and Collins,
2014). However, previous work has focused on
variables Xi whose values are in R or a small fi-
nite set; we will consider the infinite set E*.
</bodyText>
<subsectionHeader confidence="0.999351">
4.1 Review of Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.99995975">
To apply dual decomposition, we must partition
the original problem into a union of K subprob-
lems, each of which can be solved exactly and ef-
ficiently (and in parallel). For example, our exper-
iments partition Figure 1 as shown in Figure 2.
Specifically, we partition the factors into K sets
F1, . . . ,FK. Each factor F E F appears in ex-
actly one of these sets. This lets us rewrite the
</bodyText>
<equation confidence="0.9706234">
score (1) as E EFEFk F(x). Instead of simply
k
seeking its maximizer x, we equivalently seek
F(xk)) s.t. x1 = · · · = xK
(2)
</equation>
<bodyText confidence="0.999948230769231">
If we dropped the equality constraint, (2)
could be solved by separately maximizing
EFEFk F(xk) for each k. This “subproblem” is
itself a MAP problem which considers only the
factors Fk and the variables Xk adjacent to them
in the original factor graph. The subproblem ob-
jective does not depend on the other variables.
We now attempt to enforce the equality con-
straint indirectly, by adding Lagrange multipli-
ers that encourage agreement among the subprob-
lems. Assume for the moment that the variables in
the factor graph are real-valued (each xki is in R).
Then consider the Lagrangian relaxation of (2),
</bodyText>
<equation confidence="0.906539">
Aki · xk) (3)
</equation>
<bodyText confidence="0.76208175">
This can still be solved by separate maximizations.
For any choices of Aki E R having (�i) Ek Aki =
0, it upper-bounds the objective of (2). Why? The
solution to (2) achieves the same value in (3), yet
(3) may do even better by considering solutions
that do not satisfy the constraint. Our goal is to
find Aki values that tighten this upper bound as
much as possible. If we can find Aki values so that
</bodyText>
<table confidence="0.9808129">
rεzɪgn
r,εzɪgn’eɪʃn riz’ajnz d,æmn’eɪʃn d’æmz
Subproblem 1 Subproblem 2 Subproblem 3 Subproblem 4
rεzɪgn#eɪʃən rizajn#z dæmn#eɪʃən dæmn#z
eɪʃən rizajn z dæmn eɪʃən dæmn z
argmax 1:K ( 1:
x1,...,xK k=1 FEFk
max 1:K ( 1: 1:
x1,...,xK k=1 FEFk F(xk) +
i
</table>
<page confidence="0.984445">
920
</page>
<bodyText confidence="0.999892727272727">
the optimum of (3) satisfies the equality constraint,
then we have a tight bound and a solution to (2).
To improve the method, recall that subproblem
k considers only variables Xk. It is indifferent to
the value of Xi if Xi E/ Xk, so we just leave xki un-
defined in the subproblem’s solution. We treat that
as automatically satisfying the equality constraint;
thus we do not need any Lagrange multiplier Aki to
force equality. Our final solution x ignores unde-
fined values, and sets xi to the value agreed on by
the subproblems that did consider Xi.7
</bodyText>
<subsectionHeader confidence="0.995315">
4.2 Substring Count Features
</subsectionHeader>
<bodyText confidence="0.999418303030303">
But what do we do if the variables are strings? The
Lagrangian term Aki ·xki in (3) is now ill-typed. We
replace it with λki · γ(xki ), where γ(·) extracts a
real-valued feature vector from a string, and λki
is a vector of Lagrange multipliers.
This corresponds to changing the constraint in
(2). Instead of requiring x1i = · · · = xKi for each
i, we are now requiring γ(x1i) = · · · = γ(xK i ),
i.e., these strings must agree in their features.
We want each possible string to have a unique
feature vector, so that matching features forces the
actual strings to match. We follow Paul and Eisner
(2012) and use a substring count feature for each
w E Σ∗. In other words, γ(x) is an infinitely long
vector, which maps each w to the number of times
that w appears in x as a substring.8
Computing λki · γ(xki ) in (3) remains possi-
ble because in practice, λki will have only finitely
many nonzeros. This is so because our feature
vector γ(x) has only finitely many nonzeros for
any string x, and the subgradient algorithm in sec-
tion 4.3 below always updates λki by adding mul-
tiples of such γ(x) vectors.
We will use a further trick below to prevent
rapid growth of this finite set of nonzeros. Each
variable Xi maintains an active set of features,
Wi. Only these features may have nonzero La-
grange multipliers. While the active set can grow
over time, it will be finite at any given step.
Given the Lagrange multipliers, subproblem k
of (3) is simply MAP inference on the factor graph
consisting of the variables Xk and factors Fk as
well as an extra unary factor Gki at each Xi E Xk:
</bodyText>
<footnote confidence="0.9326075">
7Without this optimization, the Lagrangian term λ • xi
would have driven xi to match that value anyway. %
8More precisely, the number of times that w appears in
BOS x EOS, where BOS, EOS are distinguished boundary sym-
bols. We allow w to start with BOS and/or end with EOS,
which yields prefix and suffix indicator features.
</footnote>
<equation confidence="0.540143">
Gk i (xk) def = λk i ·γ(xki ) (4)
</equation>
<bodyText confidence="0.999909714285714">
These unary factors penalize strings according to
the Lagrange multipliers. They can be encoded
as WFSAs (Allauzen et al., 2003; Cotterell and
Eisner, 2015, Appendices B.1–B.5), allowing us to
solve the subproblem by max-product BP as usual.
The topology of the WFSA for Gki depends only
on Wi, while its weights come from λki .
</bodyText>
<subsectionHeader confidence="0.994267">
4.3 Projected Subgradient Method
</subsectionHeader>
<bodyText confidence="0.999833444444444">
We aim to adjust the collection λ of Lagrange
multipliers to minimize the upper bound (3). Fol-
lowing Komodakis et al. (2007), we solve this con-
vex dual problem using a projected subgradient
method. We initialize λ = 0 and compute (3) by
solving the K subproblems. Then we take a step
to adjust λ, and repeat in hopes of eventually sat-
isfying the equality condition.
The projected subgradient step is
</bodyText>
<equation confidence="0.9882605">
( �
λk i := λk i + η · µi − γ(xk i ) (5)
</equation>
<bodyText confidence="0.998192894736842">
where η &gt; 0 is the current step size, and µi is the
mean of γ(xk,
i ) over all subproblems k&apos; that con-
sider Xi. This update modifies (3) to encourage
solutions xk such that γ(xki ) comes closer to µi.
For each i, we update all λki at once to preserve
the property that (∀i) Ek Aki = 0. However, we
are only allowed to update components of the λki
that correspond to features in the active set Wi. To
ensure that we continue to make progress even af-
ter we agree on these features, we first expand Wi
by adding the minimal strings (if any) on which the
xki do not yet all agree. For example, we will add
the abc feature only when the xki already agree on
their counts of its substrings ab and bc.9
Algorithm 1 summarizes the whole method. Ta-
ble 1 illustrates how one active set Wi (section 4.3)
evolves, in our experiments, as it tries to enforce
agreement on a particular string xi.
</bodyText>
<subsectionHeader confidence="0.985388">
4.4 Past Work: Implicit Intersection
</subsectionHeader>
<bodyText confidence="0.9999155">
Our DD algorithm is an extension of one that Paul
and Eisner (2012) developed for the simpler im-
plicit intersection problem. Given many WFSAs
F1, ... , FK, they were able to find the string x
with maximum total score EKk=1 Fk(x). (They ap-
plied this to solve instances of the NP-hard Steiner
</bodyText>
<footnote confidence="0.972964333333333">
9In principle, we should check that they also (still) agree
on a, b, and c, but we skip this check. Our active set heuristic
is almost identical to that of Paul and Eisner (2012).
</footnote>
<page confidence="0.980331">
921
</page>
<bodyText confidence="0.384462">
Algorithm 1 DD for graphical models over strings
</bodyText>
<listItem confidence="0.972518384615385">
1: initialize the active set Wi for each variable Xi E X
2: initialize Aki = 0 for each Xi and each subproblem k
3: for t = 1 to T do &gt; max number of iterations
4: for k = 1 to K do &gt; solve all primal subproblems
5: if any of the Aki have changed then
6: run max-product BP on the acyclic graph de-
fined by variables Xk and factors Fk and Gki
7: extract MAP strings: ∀i with Xi E Xk, xki
is the label of the max-scoring accepting path
in the WFSA that represents the belief at Xi
8: for each Xi E X do &gt; improve dual bound
9: if the defined strings xki are not all equal then
10: Expand active feature set Wi &gt; section 4.3
</listItem>
<figure confidence="0.757661142857143">
11: Update each Ak &gt; equation (5)
i
12: Update each Gki from Θi, Ak &gt; see (4)
i
13: if none of the Xi required updates then
14: return any defined xki (all are equal) for each i
15: return {xi , ... , xKi } for each i &gt; failed to converge
</figure>
<bodyText confidence="0.999833125">
string problem, i.e., finding the string x of mini-
mum total edit distance to a collection of K Pz� 100
given strings.) The naive solution to this problem
would be to find the highest-weighted path in the
intersection F1 n · · · n FK. Unfortunately, the in-
tersection of WFSAs takes the Cartesian product
of their state sets. Thus materializing this inter-
section would have taken time exponential in K.
To put this another way, inference is NP-hard
even on a “trivial” factor graph: a single variable
X1 attached to K factors. Recall from section 2.3
that BP would solve this via the expensive inter-
section above. Paul and Eisner (2012) instead ap-
plied DD with one subproblem per factor. We
generalize their method to handle arbitrary factor
graphs, with multiple latent variables and cycles.
</bodyText>
<subsectionHeader confidence="0.996644">
4.5 Block Coordinate Update
</subsectionHeader>
<bodyText confidence="0.9999904375">
We also explored a possible speedup for our algo-
rithm. We used a block coordinate update vari-
ant of the algorithm when performing inference on
the phonology problem and observed an empiri-
cal speedup. Block coordinate updates are widely
used in Lagrangian relaxation and have also been
explored specifically for dual decomposition.
In general, block algorithms minimize the ob-
jective by holding some variables fixed while up-
dating others. Sontag et al. (2011) proposed a so-
phisticated block method called MPLP that con-
siders all values of variable Xi instead of the ones
obtained from the best assignments for the sub-
problems. However, it is not clear how to apply
their technique to string-valued variables. Instead,
the algorithm we propose here is much simpler—it
</bodyText>
<figure confidence="0.546330454545455">
Iter# x1i x2i x3i x4 ΔWi
i
1 e e e e 0
3 g g g g 0
4 gris griz griz griz {s, z, is, iz, s$ z$ }
5 gris grizo griz griz {o, zo, o$ }
14 griz grizo griz griz 0
17 griz griz griz griz 0
18 griz griz grize griz { e, ze, e$ }
19 gris griz griz griz 0
31 griz griz griz griz 0
</figure>
<tableCaption confidence="0.803295866666667">
Table 1: One variable’s active set as DD runs. This variable is
the unobserved stem morpheme shared by the Catalan words
gris, grizos, grize, grizes. The second column shows
the current set of solutions from the 4 subproblems having
copies of this variable. The third column shows the new sub-
strings that are then added to the active set, to try to enforce
agreement via their Lagrange multipliers. The table does not
show iterations in which these columns have not changed.
However, those iterations still update the Lagrange multipli-
ers to more strongly encourage agreement (if needed). Al-
though agreement is achieved at iterations 1, 3, and 17, it
is then disrupted—the subproblems’ solutions change be-
cause of Lagrange-multiplier pressures on their other vari-
ables (suffixes that do not agree yet). At iteration 31, the vari-
able returns to agreement on griz, and never changes again.
</tableCaption>
<bodyText confidence="0.999726823529412">
divides the primal variables into groups and up-
dates each group’s associated dual variables in
turn, using a single subgradient step (5). Note that
this way of partitioning the dual variables has the
nice property that we can still use the projected
subgradient update we gave in (5) and preserve the
property that (�i) Ek λki = 0.
In the graphical model for generative phonol-
ogy, there are two types of underlying morphemes
in the first layer: word stems and word affixes. Our
block coordinate update algorithm thus alternates
between subgradient updates to the dual variables
for the stems and the dual variables for the affixes.
Note that when performing block coordinate up-
date on the dual variables, the primal variables are
not held constant, but rather are chosen by opti-
mizing the corresponding subproblem.
</bodyText>
<sectionHeader confidence="0.997635" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.836363">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9998917">
We compare DD to belief propagation, using the
graphical model for generative phonology dis-
cussed in section 3. Inference in this model aims to
reconstruct underlying morphemes. Since our fo-
cus is inference, we will evaluate these reconstruc-
tions directly (whereas Cotterell et al. (2015) eval-
uated their ability to predict novel surface forms
using the reconstructions).
Our factor graphs have a similar topology to the
pedagogical fragment shown in Figure 1. How-
</bodyText>
<page confidence="0.994117">
922
</page>
<bodyText confidence="0.999479142857143">
ever, they are actually derived from datasets con-
structed by Cotterell et al. (2015), which are avail-
able with full descriptions at http://hubal.cs.
jhu.edu/tacl2015/. Briefly:
EXERCISE Small datasets of Catalan, English,
Maori, and Tangale, drawn from phonology
textbooks. Each dataset contains 55 to 106
surface words, formed from a collection of
16 to 55 morphemes.
CELEX Larger datasets of German, English, and
Dutch, drawn from the CELEX database
(Baayen et al., 1995). Each dataset contains
1000 surface words, formed from 341 to 381
underlying morphemes.
</bodyText>
<subsectionHeader confidence="0.99732">
5.2 Evaluation Scheme
</subsectionHeader>
<bodyText confidence="0.987489647058824">
We compared three types of inference:
DD Use DD to perform exact MAP inference.
SP Perform approximate marginal inference by
sum-product loopy BP with pruning (Cot-
terell et al., 2015).
MP Perform approximate MAP inference by
max-product loopy BP with pruning. DD and
SP improve this baseline in different ways.
DD predicts a string value for each variable. For
SP and MP, we deem the prediction at a variable
to be the string that is scored most highly by the
belief at that variable.
We report the fraction of predicted morpheme
URs that exactly match the gold-standard URs
proposed by a human (Cotterell et al., 2015). We
also compare these predicted URs to one another,
to see how well the methods agree.
</bodyText>
<subsectionHeader confidence="0.998187">
5.3 Parameterization
</subsectionHeader>
<bodyText confidence="0.99992575">
The model of Cotterell et al. (2015) has two fac-
tor types whose parameters must be chosen.10
The first is a unary factor Mφ. Each underlying-
morpheme variable (layer 1 of Figure 1) is con-
nected to a copy of Mφ, which gives the prior dis-
tribution over its values. The second is a binary
factor 5θ. For each surface word (layer 3), a copy
of 5θ gives its conditional distribution given the
corresponding underlying word (layer 2). Mφ and
5θ respectively model the lexicon and the phonol-
ogy of the specific language; both are encoded as
WFSMs.
</bodyText>
<footnote confidence="0.940112333333333">
10The model also has a three-way factor, connecting layers
1 and 2 of Figure 1. This represents deterministic concatena-
tion (appropriate for these languages) and has no parameters.
</footnote>
<bodyText confidence="0.99983437037037">
Mφ is a 0-gram generative model: at each step
it emits a character chosen uniformly from the al-
phabet E with probability 0, or halts with proba-
bility 1−0. It favors shorter strings in general, but
0 determines how weak this preference is.
5θ is a sequential edit model that produces a
word’s SR by stochastically copying, inserting,
substituting, and deleting the phonemes of its UR.
We explore two ways of parameterizing it.
Model 1 is a simple model in which 0 is a scalar,
specifying the probability of copying the next
character of the underlying word as it is transduced
to the surface word. The remaining probability
mass 1−0 is apportioned equally among insertion,
substitution and deletion operations.11 This mod-
els phonology as “noisy concatenation”—the min-
imum necessary to account for the fact that surface
words cannot quite be obtained as simple concate-
nations of their shared underlying morphemes.
Model 2 is a replication of the much more
complicated parametric model of Cotterell et al.
(2015), which can handle linguistic phonology.
Here the factor 5θ is a contextual edit FST (Cot-
terell et al., 2014). The probabilities of competing
edits in a given context are determined by a log-
linear model with weight vector 0 and features that
are meant to pick up on phonological phenomena.
</bodyText>
<subsectionHeader confidence="0.989996">
5.4 Training
</subsectionHeader>
<bodyText confidence="0.985695055555556">
When evaluating an inference method from sec-
tion 5.2, we use the same inference method both
for prediction and within training.
We train Model 1 by grid search. Specifically,
we choose 0 E [0.65, 1) and 0 E [0.25, 1) such
that the predicted forms maximize the joint score
(1) (always using the (max, +) semiring).
For Model 2, we compared two methods for
training the 0 and 0 parameters (0 is a vector):
Model 2S Supervised training, which observes
the “true” (hand-constructed) values of the
URs. This idealized setting uses the best pos-
sible parameters (trained on the test data).
Model 2E Expectation maximization (EM),
whose E step imputes the unobserved URs.
EM’s E step calls for exact marginal inference,
which is intractable for our model. So we substi-
tute the same inference method that we are test-
</bodyText>
<footnote confidence="0.569098">
11That is, probability mass of (1 − θ)/3 is divided equally
among the |Σ |possible insertions; another (1 − θ)/3 is di-
vided equally among the |Σ|−1 possible substitutions; and
the final (1 − θ)/3 is allocated to deletion.
</footnote>
<page confidence="0.997469">
923
</page>
<bodyText confidence="0.999946333333333">
ing. This gives us three approximations to EM,
based on DD, SP and MP. Note that DD specif-
ically gives the Viterbi approximation to EM—
which sometimes gets better results than true EM
(Spitkovsky et al., 2010). For MP (but not SP), we
extract only the 1-best predictions for the E step,
since we study MP as an approximation to DD.
As initialization, our first E step uses the trained
version of Model 1 for the same inference method.
</bodyText>
<subsectionHeader confidence="0.931069">
5.5 Inference Details
</subsectionHeader>
<bodyText confidence="0.999975">
We run SP and MP for 20 iterations (usually the
predictions converge within 10 iterations). We run
DD to convergence (usually &lt; 600 iterations). DD
iterations are much faster since each variable con-
siders d strings, not d distributions over strings.
Hence DD does not intersect distributions, and
many parts of the graph settle down early because
discrete values can converge in finite time.12
We follow Paul and Eisner (2012, section 5.1)
fairly closely. In particular: Our stepsize in (5) is
η = α/(t + 500), where t is the iteration num-
ber; α = 1 for Model 2S and α = 10 otherwise.
We proactively include all 1-gram and 2-gram sub-
string features in the active sets Wi at initializa-
tion, rather than adding them only as needed. At it-
erations 200, 400, and 600, we proactively add all
3-, 4-, and 5-gram features (respectively) on which
the counts still disagree; this accelerates conver-
gence on the few variables that have not already
converged. We handle negative-weight cycles as
Paul and Eisner do. If we had ever failed to con-
verge within 2000 iterations, we would have used
their heuristic to extract a prediction anyway.
Model 1 suffers from a symmetry-breaking
problem. Many edits have identical probability,
and when we run inference, many assignments
will tie for highest scoring configuration. This
can prevent DD from converging and makes per-
formance hard to measure. To break these ties,
we add “jitter” separately to each copy of Mφ
in Figure 1. Specifically, if Fi is the unary fac-
tor attached to Xi, we expand our 0-gram model
</bodyText>
<equation confidence="0.941651666666667">
Fi(x) = log((p/|E|)|x |(1 — p)) to become
Fi(x) = log(�c∈Σ p|x|c
c,i (1 — p)), where |x|c
</equation>
<bodyText confidence="0.994642">
denotes the count of character c in string x, and
pc,i a (p/|E|) expεc,i where εc,i — N(0,0.01)
and we preserve Ec∈Σ pc,i = p.
</bodyText>
<footnote confidence="0.9610205">
12A variable need not update λ if its strings agree; a sub-
problem is not re-solved if none of its variables updated λ.
</footnote>
<figure confidence="0.999167">
(a) Tangale (b) Catalan
(c) Maori (d) English
</figure>
<figureCaption confidence="0.997742">
Figure 3: The primal-dual curve of NVDD v.s. BCDD on 4
EXERCISE languages. BCDD always converges faster.
</figureCaption>
<sectionHeader confidence="0.998651" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999608">
6.1 Convergence and Speed of DD
</subsectionHeader>
<bodyText confidence="0.99916035483871">
As linguists know, reconstructing an underlying
stem or suffix can be difficult. We may face insuf-
ficient evidence or linguistic irregularity—or reg-
ularity that goes unrecognized because the phono-
logical model is impoverished (Model 1) or poorly
trained (early EM iterations on Model 2). DD
may then require extensive negotiation to resolve
disagreements among subproblems. Furthermore,
DD must renegotiate as conditions change else-
where in the factor graph (Table 1).
DD converged in all of our experiments. Note
that DD (section 4.3) has converged when all the
equality constraints in (2) are satisfied. In this
case, we have found the true MAP configuration.
In section 4.5, we discussed a block coordi-
nate update variation (BCDD) of our DD algo-
rithm. Figure 3 shows the convergence behavior
of BCDD against the naive projected subgradi-
ent algorithm (NVDD) on the four EXERCISE lan-
guages under Model 1. The dual objective (3) al-
ways upper-bounds the primal score (i.e., the score
(1) of an assignment derived heuristically from
the current subproblem solutions). The dual de-
creases as the algorithm progresses. When the two
objectives meet, we have found an optimal solu-
tion to the primal problem. We can see in Figure 3
that our DD algorithm converges quickly on the
four EXERCISE languages and BCDD converges
consistently faster than NVDD. We use BCDD in
the remaining experiments.
When DD runs fast, it is competitive with the
</bodyText>
<page confidence="0.991921">
924
</page>
<table confidence="0.994854952380952">
DD SP MP Gold
DD 92.74% 90.55% 96.92%
SP 95.22% 94.63%
MP 90.63%
(a) The 4 EXERCISE languages under Model 1
DD SP MP Gold
DD 88.05% 85.19% 89.66%
SP 92.64% 85.71%
MP 83.46%
(b) The 3 CELEX languages under Model 1
DD SP MP Gold
DD 96.53% 100% 98.67%
SP 96.53% 96.05%
MP 98.67%
(c) The 3 CELEX languages under Model 2S (EXERCISE
dataset gives 100% everywhere)
DD SP MP Gold
DD 92.43% 89.39% 98.18%
SP 96.73% 95.42%
MP 90.74%
(d) The 4 EXERCISE languages under Model 2E
</table>
<tableCaption confidence="0.968549666666667">
Table 2: Pairwise agreement (on morpheme URs) of DD, SP,
MP and the gold standard, for each group of inference prob-
lems. Boldface is highest accuracy (agreement with gold).
</tableCaption>
<bodyText confidence="0.999877416666667">
other methods. It is typically faster on the EXER-
CISE data, and a few times slower on the CELEX
data. But we stop the other methods after 20 it-
erations, whereas DD runs until it gets an exact
answer. We find that this runtime is unpredictable
and sometimes quite long. In the grid search for
training Model 1, we observed that changes in the
parameters (φ, θ) could cause the runtime of DD
inference to vary by 2 orders of magnitude. Sim-
ilarly, on the CELEX data, the runtime on Model
1 (over 10 different N = 600 subsets of English)
varied from about 1 hour to nearly 2 days.13
</bodyText>
<subsectionHeader confidence="0.999919">
6.2 Comparison of Inference
</subsectionHeader>
<bodyText confidence="0.995177352941177">
For each language, we constructed several differ-
ent unsupervised prediction problems. In each
problem, we observe some size-N subset of the
words in our dataset, and we attempt to predict the
URs of the morphemes in those words. For each
CELEX language, we took N = 600, and used
three of the size-N training sets from (Cotterell
et al., 2015). For each EXERCISE language, we
took N to be one less than the dataset size, and
used all N + 1 subsets of size N, again similar to
(Cotterell et al., 2015). We report the unweighted
macro-average of all these accuracy numbers.
13Note that our implementation is not optimized; e.g., it
uses Python (not Cython).
We compare DD, SP, and MP inference on
each language under different settings. Table 2
shows aggregate results, as an unweighted aver-
age over multiple languages and training sets. We
present various additional results at http://cs.
jhu.edu/˜npeng/emnlp2015/, including a per-
language breakdown of the results, runtime num-
bers, and significance tests.
The results for Model 1 are shown in Tables 2a
and 2b. As we can see, in both datasets, dual
decomposition performed the best at recovering
the URs, while MP performed the worst. Both
DD and MP are doing MAP inference, so the dif-
ferences reflect the search error in MP. Interest-
ingly, DD agrees more with SP than with MP, even
though SP uses marginal inference.
Although the aggregate results on the EXER-
CISE dataset show a large improvement of DD
over both of the BP algorithms, the gain all comes
from the English language. SP actually does better
than DD on Catalan and Maori, and MP also gets
better results than DD on Maori, tying with SP.
For Model 2S, all inference methods achieved
100% accuracy on the EXERCISE dataset, so we
do not show a table. The results on the CELEX
dataset are shown in Table 2c. Here both DD and
MP performed equally well, and outperformed
BP—a result like (Spitkovsky et al., 2010). This
trend is consistent over all three languages: DD
and MP always achieve similar results and both
outperform SP. Of course, one advantage of DD
in the setting is that it actually finds the true MAP
prediction of the model; the errors are known to be
due to the model, not the search procedure.
For Model 2E, we show results on the EXER-
CISE dataset in Table 2d. Here the results resemble
the pattern of Model 1.
</bodyText>
<sectionHeader confidence="0.996356" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999732">
We presented a general dual decomposition algo-
rithm for MAP inference on graphical models over
strings, and applied it to an unsupervised learn-
ing task in phonology. The experiments show that
our DD algorithm converges and gets better results
than both max-product and sum-product BP.
Techniques should be explored to speed up the
DD method. Adapting the MPLP algorithm (Son-
tag et al., 2011) to the string-valued case would be
a nontrivial extension. We could also explore other
serial update schemes, which generally speed up
message-passing algorithms over parallel update.
</bodyText>
<page confidence="0.997761">
925
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888117647059">
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of ACL,
pages 40–47.
R. Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database on CD-
ROM.
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas L
Griffiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In Proceedings of
EMNLP-CoNLL, pages 887–896.
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2008. A probabilistic ap-
proach to language change. In Proceedings of NIPS.
Ryan Cotterell and Jason Eisner. 2015. Penalized
expectation propagation for graphical models over
strings. In Proceedings of NAACL-HLT, pages 932–
942, Denver, June. Supplementary material (11
pages) also available.
Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014.
Stochastic contextual edit distance and probabilistic
FSTs. In Proceedings of ACL, Baltimore, June. 6
pages.
Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2015. Modeling word forms using latent underlying
morphs and phonology. Transactions of the Associ-
ation for Computational Linguistics, 3:433–447.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of
EMNLP, pages 101–110, Singapore, August.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616–627, Edinburgh, July.
Thomas Finley and Thorsten Joachims. 2007. Param-
eter learning for loopy markov random fields with
structural support vector machines. In ICML Work-
shop on Constrained Optimization and Structured
Output Spaces.
David Hall and Dan Klein. 2010. Finding cognate
groups using phylogenies. In Proceedings of ACL.
David Hall and Dan Klein. 2011. Large-scale cognate
recovery. In Proceedings of EMNLP.
Andr´e Kempe, Jean-Marc Champarnaud, and Jason
Eisner. 2004. A note on join and auto-intersection
of n-ary rational relations. In Loek Cleophas and
Bruce Watson, editors, Proceedings of the Eind-
hoven FASTAR Days (Computer Science Technical
Report 04-40), pages 64–78. Department of Math-
ematics and Computer Science, Technische Univer-
siteit Eindhoven, Netherlands, December.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Nikos Komodakis and Nikos Paragios. 2009. Beyond
pairwise energies: Efficient optimization for higher-
order MRFs. In Proceedings of CVPR, pages 2985–
2992. IEEE.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. MRF optimization via dual decomposi-
tion: Message-passing revisited. In Proceedings of
ICCV, pages 1–8. IEEE.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP, pages 1288–
1298.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger.
2001. Factor graphs and the sum-product algo-
rithm. IEEE Transactions on Information Theory,
47(2):498–519, February.
Andr´e Martins, M´ario Figueiredo, Pedro Aguiar,
Eric P. Xing, and Noah A. Smith. 2011. An aug-
mented lagrangian approach to constrained map in-
ference. In Proceedings of ICML, pages 169–176.
Thomas P. Minka. 2001. Expectation propagation for
approximate Bayesian inference. In Proceedings of
UAI, pages 362–369.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech &amp; Language,
16(1):69–88.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of UAI,
pages 467–475.
Michael J. Paul and Jason Eisner. 2012. Implicitly in-
tersecting weighted automata using dual decompo-
sition. In Proceedings of NAACL, pages 232–242.
Fernando C. N. Pereira and Michael Riley. 1997.
Speech recognition by composition of weighted fi-
nite automata. In Emmanuel Roche and Yves
Schabes, editors, Finite-State Language Processing.
MIT Press, Cambridge, MA.
Alexander M. Rush and Michael Collins. 2014. A
tutorial on dual decomposition and Lagrangian re-
laxation for inference in natural language process-
ing. Technical report available from arXiv.org
as arXiv:1405.5208.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for infer-
ence. Optimization for Machine Learning, 1:219–
254.
</reference>
<page confidence="0.983122">
926
</page>
<reference confidence="0.999743333333333">
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of CoNLL, page 917, Uppsala, Swe-
den, July.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453–1484, September.
</reference>
<page confidence="0.997149">
927
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962392">
<title confidence="0.997147">Decomposition Inference for Graphical Models over</title>
<author confidence="0.998515">Peng Cotterell Eisner</author>
<affiliation confidence="0.999977">Department of Computer Science, Johns Hopkins University</affiliation>
<abstract confidence="0.99865104">We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of count features. This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, a bound on string length. that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="20680" citStr="Allauzen et al., 2003" startWordPosition="3425" endWordPosition="3428">y MAP inference on the factor graph consisting of the variables Xk and factors Fk as well as an extra unary factor Gki at each Xi E Xk: 7Without this optimization, the Lagrangian term λ • xi would have driven xi to match that value anyway. % 8More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary symbols. We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features. Gk i (xk) def = λk i ·γ(xki ) (4) These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only on Wi, while its weights come from λki . 4.3 Projected Subgradient Method We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition.</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of ACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1995</date>
<booktitle>The CELEX lexical database on CDROM.</booktitle>
<contexts>
<context position="28194" citStr="Baayen et al., 1995" startWordPosition="4783" endWordPosition="4786">ct novel surface forms using the reconstructions). Our factor graphs have a similar topology to the pedagogical fragment shown in Figure 1. How922 ever, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs. jhu.edu/tacl2015/. Briefly: EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995). Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes. 5.2 Evaluation Scheme We compared three types of inference: DD Use DD to perform exact MAP inference. SP Perform approximate marginal inference by sum-product loopy BP with pruning (Cotterell et al., 2015). MP Perform approximate MAP inference by max-product loopy BP with pruning. DD and SP improve this baseline in different ways. DD predicts a string value for each variable. For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable. We r</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>R. Harald Baayen, Richard Piepenbrock, and Leon Gulikers. 1995. The CELEX lexical database on CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to diachronic phonology.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>887--896</pages>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2007</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas L Griffiths, and Dan Klein. 2007. A probabilistic approach to diachronic phonology. In Proceedings of EMNLP-CoNLL, pages 887–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to language change.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2008</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths, and Dan Klein. 2008. A probabilistic approach to language change. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Cotterell</author>
<author>Jason Eisner</author>
</authors>
<title>Penalized expectation propagation for graphical models over strings.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>932--942</pages>
<location>Denver,</location>
<note>Supplementary material (11 pages) also available.</note>
<contexts>
<context position="8042" citStr="Cotterell and Eisner, 2015" startWordPosition="1262" endWordPosition="1265"> work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.3 Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari3Finite-state software libraries often support only these cases. Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to </context>
<context position="10137" citStr="Cotterell and Eisner (2015)" startWordPosition="1599" endWordPosition="1602">s-product construction. For example, when finding the marginal distribution at a degree-d variable, intersecting d WFSA messages having m states each may yield a WFSA with up to md states. (Our models in section 6 include variables with d up to 156.) Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles (Dreyer and Eisner, 2009), as ours do. The usual solution is to prune or otherwise approximate the messages at each step. In particular, Cotterell and Eisner (2015) gave a principled way to approximate the messages using variablelength n-gram models, using an adaptive variant of Expectation Propagation (Minka, 2001). 2.4 Dual Decomposition Inference In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation.4 The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution. This information is encoded in WFSAs that score possib</context>
<context position="20708" citStr="Cotterell and Eisner, 2015" startWordPosition="3429" endWordPosition="3432">factor graph consisting of the variables Xk and factors Fk as well as an extra unary factor Gki at each Xi E Xk: 7Without this optimization, the Lagrangian term λ • xi would have driven xi to match that value anyway. % 8More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary symbols. We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features. Gk i (xk) def = λk i ·γ(xki ) (4) These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only on Wi, while its weights come from λki . 4.3 Projected Subgradient Method We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition. The projected subgradient s</context>
</contexts>
<marker>Cotterell, Eisner, 2015</marker>
<rawString>Ryan Cotterell and Jason Eisner. 2015. Penalized expectation propagation for graphical models over strings. In Proceedings of NAACL-HLT, pages 932– 942, Denver, June. Supplementary material (11 pages) also available.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Cotterell</author>
<author>Nanyun Peng</author>
<author>Jason Eisner</author>
</authors>
<title>Stochastic contextual edit distance and probabilistic FSTs.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>6</volume>
<pages>pages.</pages>
<location>Baltimore,</location>
<contexts>
<context position="30873" citStr="Cotterell et al., 2014" startWordPosition="5234" endWordPosition="5238">ability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−0 is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes. Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor 5θ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector 0 and features that are meant to pick up on phonological phenomena. 5.4 Training When evaluating an inference method from section 5.2, we use the same inference method both for prediction and within training. We train Model 1 by grid search. Specifically, we choose 0 E [0.65, 1) and 0 E [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (max, +) semiring). For Model 2, we compared two methods for training the 0 and 0 parameters (0 is a vector): Mo</context>
</contexts>
<marker>Cotterell, Peng, Eisner, 2014</marker>
<rawString>Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014. Stochastic contextual edit distance and probabilistic FSTs. In Proceedings of ACL, Baltimore, June. 6 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Cotterell</author>
<author>Nanyun Peng</author>
<author>Jason Eisner</author>
</authors>
<title>Modeling word forms using latent underlying morphs and phonology.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--433</pages>
<contexts>
<context position="3030" citStr="Cotterell et al. (2015)" startWordPosition="447" endWordPosition="450">noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis. In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges. We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015). We show that the method generally converges and that it achieves better results than alternatives. The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion. 2 Graphical Models Over Strings 2.1 Factor Graphs and MAP Inference To perform inference on a graphical model (directed or undirected), one first converts the </context>
<context position="4471" citStr="Cotterell et al. (2015)" startWordPosition="672" endWordPosition="675">is paper we do not treat that structure learning problem. However, both structure learning and parameter learning need to call inference—such as the method presented here—in order to evaluate proposed topologies or improve their parameters. 917 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 1) Underlying morphemes Concatenation 2) Underlying words Phonology 3) Surface words Figure 1: A fragment of the factor graph for the directed graphical model of Cotterell et al. (2015), displaying a possible assignment to the variables (ellipses). The model explains each observed surface word as the result of applying phonology to a concatenation of underlying morphemes. Shaded variables show the observed surface forms for four words: resignation, resigns, damns, and damnation. The underlying pronunciations of these words are assumed to be more similar than their surface pronunciations, because the words are known to share latent morphemes. The factor graph encodes what is shared. Each observed word at layer 3 has a latent underlying form at layer 2, which is a deterministi</context>
<context position="7249" citStr="Cotterell et al., 2015" startWordPosition="1123" endWordPosition="1126"> 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or</context>
<context position="14272" citStr="Cotterell et al. (2015)" startWordPosition="2285" endWordPosition="2288">l decomposition, we choose to decompose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each subproblem variable (not shown) is used to incorporate its Lagrangian term. 3 A Sample Task: Generative Phonology Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphical model for morphophonology. Cotterell et al. (2015) defined a Bayesian network to describe the generative process of phonological words. Our Figure 1 shows a conversion of their model to a factor graph and explains what the variables and factors mean. Inference on this graph performs unsupervised discovery of latent strings. Given observed surface representations of words (SRs), inference aims to recover the underlying representations (URs) of the words and their shared constituent morphemes. The latter can then be used to predict held-out SRs. Notice that the 8 edges in the first layer of Figure 1 form a cycle; such cycles make BP inexact. Mo</context>
<context position="27541" citStr="Cotterell et al. (2015)" startWordPosition="4682" endWordPosition="4685">us alternates between subgradient updates to the dual variables for the stems and the dual variables for the affixes. Note that when performing block coordinate update on the dual variables, the primal variables are not held constant, but rather are chosen by optimizing the corresponding subproblem. 5 Experimental Setup 5.1 Datasets We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3. Inference in this model aims to reconstruct underlying morphemes. Since our focus is inference, we will evaluate these reconstructions directly (whereas Cotterell et al. (2015) evaluated their ability to predict novel surface forms using the reconstructions). Our factor graphs have a similar topology to the pedagogical fragment shown in Figure 1. How922 ever, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs. jhu.edu/tacl2015/. Briefly: EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch</context>
<context position="28924" citStr="Cotterell et al., 2015" startWordPosition="4905" endWordPosition="4908">eme We compared three types of inference: DD Use DD to perform exact MAP inference. SP Perform approximate marginal inference by sum-product loopy BP with pruning (Cotterell et al., 2015). MP Perform approximate MAP inference by max-product loopy BP with pruning. DD and SP improve this baseline in different ways. DD predicts a string value for each variable. For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable. We report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human (Cotterell et al., 2015). We also compare these predicted URs to one another, to see how well the methods agree. 5.3 Parameterization The model of Cotterell et al. (2015) has two factor types whose parameters must be chosen.10 The first is a unary factor Mφ. Each underlyingmorpheme variable (layer 1 of Figure 1) is connected to a copy of Mφ, which gives the prior distribution over its values. The second is a binary factor 5θ. For each surface word (layer 3), a copy of 5θ gives its conditional distribution given the corresponding underlying word (layer 2). Mφ and 5θ respectively model the lexicon and the phonology of </context>
<context position="30764" citStr="Cotterell et al. (2015)" startWordPosition="5216" endWordPosition="5219"> explore two ways of parameterizing it. Model 1 is a simple model in which 0 is a scalar, specifying the probability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−0 is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes. Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor 5θ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector 0 and features that are meant to pick up on phonological phenomena. 5.4 Training When evaluating an inference method from section 5.2, we use the same inference method both for prediction and within training. We train Model 1 by grid search. Specifically, we choose 0 E [0.65, 1) and 0 E [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (ma</context>
<context position="37671" citStr="Cotterell et al., 2015" startWordPosition="6421" endWordPosition="6424">changes in the parameters (φ, θ) could cause the runtime of DD inference to vary by 2 orders of magnitude. Similarly, on the CELEX data, the runtime on Model 1 (over 10 different N = 600 subsets of English) varied from about 1 hour to nearly 2 days.13 6.2 Comparison of Inference For each language, we constructed several different unsupervised prediction problems. In each problem, we observe some size-N subset of the words in our dataset, and we attempt to predict the URs of the morphemes in those words. For each CELEX language, we took N = 600, and used three of the size-N training sets from (Cotterell et al., 2015). For each EXERCISE language, we took N to be one less than the dataset size, and used all N + 1 subsets of size N, again similar to (Cotterell et al., 2015). We report the unweighted macro-average of all these accuracy numbers. 13Note that our implementation is not optimized; e.g., it uses Python (not Cython). We compare DD, SP, and MP inference on each language under different settings. Table 2 shows aggregate results, as an unweighted average over multiple languages and training sets. We present various additional results at http://cs. jhu.edu/˜npeng/emnlp2015/, including a perlanguage brea</context>
</contexts>
<marker>Cotterell, Peng, Eisner, 2015</marker>
<rawString>Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2015. Modeling word forms using latent underlying morphs and phonology. Transactions of the Association for Computational Linguistics, 3:433–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Graphical models over multiple strings.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>101--110</pages>
<location>Singapore,</location>
<contexts>
<context position="7140" citStr="Dreyer and Eisner, 2009" startWordPosition="1106" endWordPosition="1109">word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to</context>
<context position="8805" citStr="Dreyer and Eisner (2009)" startWordPosition="1381" endWordPosition="1384">s score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari3Finite-state software libraries often support only these cases. Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d &gt; 2. 918 ables. In the string case, they have infinitely many rows and columns, indexed by possible strings. Dreyer and Eisner (2009) represented these infinite vectors and matrices by WFSAs and WFSTs, respectively. They observed that the simple linear-algebra operations used by BP can be implemented by finite-state constructions. The pointwise product of two vectors is the intersection of their WFSAs; the marginalization of a matrix is the projection of its WFST; a vector-matrix product is computed by composing the WFSA with the WFST and then projecting onto the output tape. For degree &gt; 2, BP’s rank-d tensors become dtape WFSMs, and these constructions generalize. Unfortunately, except in small acyclic models, the BP mess</context>
<context position="13620" citStr="Dreyer and Eisner (2009)" startWordPosition="2182" endWordPosition="2185">ined over the (logadd, +) semiring. This means that the exponentiated score assigned by a WFSM is the sum of the exponentiated scores of the accepting paths. 5This problem is more specifically called MPE inference. 6The trouble is that we cannot bound the length of the latent strings. If we could, then we could encode them using a finite set of boolean variables, and solve as an ILP problem. But that would allow us to determine whether there exists a MAP assignment with score ≥ 0. That is impossible in general, because it would solve Post’s Correspondence Problem as a simple special case (see Dreyer and Eisner (2009)). 919 Figure 2: To apply dual decomposition, we choose to decompose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each subproblem variable (not shown) is used to incorporate its Lagrangian term. 3 A Sample Task: Generative Phonology Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphica</context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In Proceedings of EMNLP, pages 101–110, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a Dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>616--627</pages>
<location>Edinburgh,</location>
<contexts>
<context position="7166" citStr="Dreyer and Eisner, 2011" startWordPosition="1110" endWordPosition="1113">´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from</context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a Dirichlet process mixture model. In Proceedings of EMNLP, pages 616–627, Edinburgh, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Parameter learning for loopy markov random fields with structural support vector machines.</title>
<date>2007</date>
<booktitle>In ICML Workshop on Constrained Optimization and Structured Output Spaces.</booktitle>
<contexts>
<context position="6806" citStr="Finley and Joachims, 2007" startWordPosition="1054" endWordPosition="1057">,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def = (1/Z) exp score(x). 2.2 The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in</context>
</contexts>
<marker>Finley, Joachims, 2007</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2007. Parameter learning for loopy markov random fields with structural support vector machines. In ICML Workshop on Constrained Optimization and Structured Output Spaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Finding cognate groups using phylogenies.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6609" citStr="Hall and Klein, 2010" startWordPosition="1023" endWordPosition="1026">ur partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def = (1/Z) exp score(x). 2.2 The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying</context>
</contexts>
<marker>Hall, Klein, 2010</marker>
<rawString>David Hall and Dan Klein. 2010. Finding cognate groups using phylogenies. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Large-scale cognate recovery.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6632" citStr="Hall and Klein, 2011" startWordPosition="1027" endWordPosition="1030"> of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def = (1/Z) exp score(x). 2.2 The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotter</context>
</contexts>
<marker>Hall, Klein, 2011</marker>
<rawString>David Hall and Dan Klein. 2011. Large-scale cognate recovery. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Kempe</author>
<author>Jean-Marc Champarnaud</author>
<author>Jason Eisner</author>
</authors>
<title>A note on join and auto-intersection of n-ary rational relations.</title>
<date>2004</date>
<booktitle>In Loek Cleophas and Bruce Watson, editors, Proceedings of the Eindhoven FASTAR Days (Computer Science Technical Report 04-40),</booktitle>
<pages>64--78</pages>
<institution>Department of Mathematics and Computer Science, Technische Universiteit Eindhoven,</institution>
<location>Netherlands,</location>
<contexts>
<context position="7625" citStr="Kempe et al., 2004" startWordPosition="1192" endWordPosition="1195">ed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.3 Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. M</context>
</contexts>
<marker>Kempe, Champarnaud, Eisner, 2004</marker>
<rawString>Andr´e Kempe, Jean-Marc Champarnaud, and Jason Eisner. 2004. A note on join and auto-intersection of n-ary rational relations. In Loek Cleophas and Bruce Watson, editors, Proceedings of the Eindhoven FASTAR Days (Computer Science Technical Report 04-40), pages 64–78. Department of Mathematics and Computer Science, Technische Universiteit Eindhoven, Netherlands, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="11048" citStr="Knight and Graehl, 1998" startWordPosition="1739" endWordPosition="1742"> problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation.4 The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution. This information is encoded in WFSAs that score possible values of a string variable. DD incrementally adjusts the WFSAs so as to encourage values that agree with 4Such small BP problems commonly arise in NLP. In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology. the variable’s average value across subproblems. Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to Cotterell and Eisner (2015). They may still grow over time; but DD often halts while the WFSAs are still small. It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP. 2.5 Switching Between Semirings Our factors may be nondeterministic WFSMs. So when F ∈ F scores a given d-tuple of string</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
<author>Nikos Paragios</author>
</authors>
<title>Beyond pairwise energies: Efficient optimization for higherorder MRFs.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>2985--2992</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15992" citStr="Komodakis and Paragios, 2009" startWordPosition="2564" endWordPosition="2567"> and highdegree nodes. In our phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2. While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables. 4 Dual Decomposition Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set E*. 4.1 Review of Dual Decomposition To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2. Specifically, we partition the factors into K sets F1, . . . ,FK. Each factor F E F appears in exactly on</context>
</contexts>
<marker>Komodakis, Paragios, 2009</marker>
<rawString>Nikos Komodakis and Nikos Paragios. 2009. Beyond pairwise energies: Efficient optimization for higherorder MRFs. In Proceedings of CVPR, pages 2985– 2992. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
</authors>
<title>Nikos Paragios, and Georgios Tziritas.</title>
<date>2007</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<marker>Komodakis, 2007</marker>
<rawString>Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. MRF optimization via dual decomposition: Message-passing revisited. In Proceedings of ICCV, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="16010" citStr="Koo et al., 2010" startWordPosition="2568" endWordPosition="2571">phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2. While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables. 4 Dual Decomposition Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set E*. 4.1 Review of Dual Decomposition To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2. Specifically, we partition the factors into K sets F1, . . . ,FK. Each factor F E F appears in exactly one of these sets. T</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288– 1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Kschischang</author>
<author>B J Frey</author>
<author>H A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="3695" citStr="Kschischang et al., 2001" startWordPosition="556" endWordPosition="559">erges and that it achieves better results than alternatives. The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion. 2 Graphical Models Over Strings 2.1 Factor Graphs and MAP Inference To perform inference on a graphical model (directed or undirected), one first converts the model to a factor graph representation (Kschischang et al., 2001). A factor graph is a finite bipartite 1In some task settings, it is also necessary to discover the model topology along with the model parameters. In this paper we do not treat that structure learning problem. However, both structure learning and parameter learning need to call inference—such as the method presented here—in order to evaluate proposed topologies or improve their parameters. 917 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 1) Underl</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47(2):498–519, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Martins</author>
<author>M´ario Figueiredo</author>
<author>Pedro Aguiar</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>An augmented lagrangian approach to constrained map inference.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="16032" citStr="Martins et al., 2011" startWordPosition="2572" endWordPosition="2575"> each observed word along with its correspondent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2. While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables. 4 Dual Decomposition Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set E*. 4.1 Review of Dual Decomposition To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2. Specifically, we partition the factors into K sets F1, . . . ,FK. Each factor F E F appears in exactly one of these sets. This lets us rewrite th</context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Xing, Smith, 2011</marker>
<rawString>Andr´e Martins, M´ario Figueiredo, Pedro Aguiar, Eric P. Xing, and Noah A. Smith. 2011. An augmented lagrangian approach to constrained map inference. In Proceedings of ICML, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Expectation propagation for approximate Bayesian inference.</title>
<date>2001</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>362--369</pages>
<contexts>
<context position="10290" citStr="Minka, 2001" startWordPosition="1623" endWordPosition="1624">ith up to md states. (Our models in section 6 include variables with d up to 156.) Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles (Dreyer and Eisner, 2009), as ours do. The usual solution is to prune or otherwise approximate the messages at each step. In particular, Cotterell and Eisner (2015) gave a principled way to approximate the messages using variablelength n-gram models, using an adaptive variant of Expectation Propagation (Minka, 2001). 2.4 Dual Decomposition Inference In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation.4 The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution. This information is encoded in WFSAs that score possible values of a string variable. DD incrementally adjusts the WFSAs so as to encourage values that agree with 4Such small BP problems commonly arise in NL</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. Expectation propagation for approximate Bayesian inference. In Proceedings of UAI, pages 362–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7604" citStr="Mohri et al., 2002" startWordPosition="1188" endWordPosition="1191">aph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.3 Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible va</context>
<context position="11983" citStr="Mohri et al., 2002" startWordPosition="1903" endWordPosition="1906">t DD often halts while the WFSAs are still small. It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP. 2.5 Switching Between Semirings Our factors may be nondeterministic WFSMs. So when F ∈ F scores a given d-tuple of string values, it may accept that d-tuple along multiple different WFSM paths with different scores, corresponding to different alignments of the strings. For purposes of MAP inference, we define F to return the maximum of these path scores. That is, we take the WFSMs to be defined with weights in the (max, +) semiring (Mohri et al., 2002). Equivalently, we are seeking the “best global solution” in the sense of choosing not only the strings xi but also the alignments of the d-tuples.5 To do so, we must solve each DD subproblem in the same sense. We use max-product BP. This still applies the Dreyer-Eisner method of section 2.3. Since these WFSMs are defined in the (max, +) semiring, the method’s finite-state operations will combine weights using max and +. MAP inference in our setting is in general computationally undecidable.6 However, if DD converges (as in our experiments), then its solution is guaranteed to be the true MAP a</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech &amp; Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Yair Weiss</author>
<author>Michael I Jordan</author>
</authors>
<title>Loopy belief propagation for approximate inference: An empirical study.</title>
<date>1999</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>467--475</pages>
<contexts>
<context position="8244" citStr="Murphy et al. (1999)" startWordPosition="1293" endWordPosition="1296">). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.3 Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari3Finite-state software libraries often support only these cases. Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d &gt; 2. 918 ables. In the string case, they have infinitely many rows and columns, indexed by possible strings. Dreyer and Eisner (2009) represented these infinite vectors and</context>
</contexts>
<marker>Murphy, Weiss, Jordan, 1999</marker>
<rawString>Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 1999. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of UAI, pages 467–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Jason Eisner</author>
</authors>
<title>Implicitly intersecting weighted automata using dual decomposition.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>232--242</pages>
<contexts>
<context position="19216" citStr="Paul and Eisner (2012)" startWordPosition="3154" endWordPosition="3157">ring Count Features But what do we do if the variables are strings? The Lagrangian term Aki ·xki in (3) is now ill-typed. We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers. This corresponds to changing the constraint in (2). Instead of requiring x1i = · · · = xKi for each i, we are now requiring γ(x1i) = · · · = γ(xK i ), i.e., these strings must agree in their features. We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow Paul and Eisner (2012) and use a substring count feature for each w E Σ∗. In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8 Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors. We will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variabl</context>
<context position="22342" citStr="Paul and Eisner (2012)" startWordPosition="3741" endWordPosition="3744">o features in the active set Wi. To ensure that we continue to make progress even after we agree on these features, we first expand Wi by adding the minimal strings (if any) on which the xki do not yet all agree. For example, we will add the abc feature only when the xki already agree on their counts of its substrings ab and bc.9 Algorithm 1 summarizes the whole method. Table 1 illustrates how one active set Wi (section 4.3) evolves, in our experiments, as it tries to enforce agreement on a particular string xi. 4.4 Past Work: Implicit Intersection Our DD algorithm is an extension of one that Paul and Eisner (2012) developed for the simpler implicit intersection problem. Given many WFSAs F1, ... , FK, they were able to find the string x with maximum total score EKk=1 Fk(x). (They applied this to solve instances of the NP-hard Steiner 9In principle, we should check that they also (still) agree on a, b, and c, but we skip this check. Our active set heuristic is almost identical to that of Paul and Eisner (2012). 921 Algorithm 1 DD for graphical models over strings 1: initialize the active set Wi for each variable Xi E X 2: initialize Aki = 0 for each Xi and each subproblem k 3: for t = 1 to T do &gt; max num</context>
<context position="24294" citStr="Paul and Eisner (2012)" startWordPosition="4125" endWordPosition="4128">em, i.e., finding the string x of minimum total edit distance to a collection of K Pz� 100 given strings.) The naive solution to this problem would be to find the highest-weighted path in the intersection F1 n · · · n FK. Unfortunately, the intersection of WFSAs takes the Cartesian product of their state sets. Thus materializing this intersection would have taken time exponential in K. To put this another way, inference is NP-hard even on a “trivial” factor graph: a single variable X1 attached to K factors. Recall from section 2.3 that BP would solve this via the expensive intersection above. Paul and Eisner (2012) instead applied DD with one subproblem per factor. We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles. 4.5 Block Coordinate Update We also explored a possible speedup for our algorithm. We used a block coordinate update variant of the algorithm when performing inference on the phonology problem and observed an empirical speedup. Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition. In general, block algorithms minimize the objective by holding some variables fixed </context>
<context position="32983" citStr="Paul and Eisner (2012" startWordPosition="5599" endWordPosition="5602">edictions for the E step, since we study MP as an approximation to DD. As initialization, our first E step uses the trained version of Model 1 for the same inference method. 5.5 Inference Details We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many parts of the graph settle down early because discrete values can converge in finite time.12 We follow Paul and Eisner (2012, section 5.1) fairly closely. In particular: Our stepsize in (5) is η = α/(t + 500), where t is the iteration number; α = 1 for Model 2S and α = 10 otherwise. We proactively include all 1-gram and 2-gram substring features in the active sets Wi at initialization, rather than adding them only as needed. At iterations 200, 400, and 600, we proactively add all 3-, 4-, and 5-gram features (respectively) on which the counts still disagree; this accelerates convergence on the few variables that have not already converged. We handle negative-weight cycles as Paul and Eisner do. If we had ever failed</context>
</contexts>
<marker>Paul, Eisner, 2012</marker>
<rawString>Michael J. Paul and Jason Eisner. 2012. Implicitly intersecting weighted automata using dual decomposition. In Proceedings of NAACL, pages 232–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11022" citStr="Pereira and Riley, 1997" startWordPosition="1735" endWordPosition="1738">oses the original complex problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation.4 The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution. This information is encoded in WFSAs that score possible values of a string variable. DD incrementally adjusts the WFSAs so as to encourage values that agree with 4Such small BP problems commonly arise in NLP. In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology. the variable’s average value across subproblems. Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to Cotterell and Eisner (2015). They may still grow over time; but DD often halts while the WFSAs are still small. It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP. 2.5 Switching Between Semirings Our factors may be nondeterministic WFSMs. So when F ∈ F scores</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando C. N. Pereira and Michael Riley. 1997. Speech recognition by composition of weighted finite automata. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. Technical report available from arXiv.org as arXiv:1405.5208.</title>
<date>2014</date>
<contexts>
<context position="16078" citStr="Rush and Collins, 2014" startWordPosition="2580" endWordPosition="2583">ndent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2. While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables. 4 Dual Decomposition Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set E*. 4.1 Review of Dual Decomposition To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2. Specifically, we partition the factors into K sets F1, . . . ,FK. Each factor F E F appears in exactly one of these sets. This lets us rewrite the score (1) as E EFEFk F(x). Instead of simply</context>
</contexts>
<marker>Rush, Collins, 2014</marker>
<rawString>Alexander M. Rush and Michael Collins. 2014. A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. Technical report available from arXiv.org as arXiv:1405.5208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Introduction to dual decomposition for inference. Optimization for Machine Learning,</title>
<date>2011</date>
<pages>1--219</pages>
<contexts>
<context position="16053" citStr="Sontag et al., 2011" startWordPosition="2576" endWordPosition="2579">ong with its correspondent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2. While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables. 4 Dual Decomposition Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set E*. 4.1 Review of Dual Decomposition To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2. Specifically, we partition the factors into K sets F1, . . . ,FK. Each factor F E F appears in exactly one of these sets. This lets us rewrite the score (1) as E EFEF</context>
<context position="24937" citStr="Sontag et al. (2011)" startWordPosition="4226" endWordPosition="4229">h one subproblem per factor. We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles. 4.5 Block Coordinate Update We also explored a possible speedup for our algorithm. We used a block coordinate update variant of the algorithm when performing inference on the phonology problem and observed an empirical speedup. Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition. In general, block algorithms minimize the objective by holding some variables fixed while updating others. Sontag et al. (2011) proposed a sophisticated block method called MPLP that considers all values of variable Xi instead of the ones obtained from the best assignments for the subproblems. However, it is not clear how to apply their technique to string-valued variables. Instead, the algorithm we propose here is much simpler—it Iter# x1i x2i x3i x4 ΔWi i 1 e e e e 0 3 g g g g 0 4 gris griz griz griz {s, z, is, iz, s$ z$ } 5 gris grizo griz griz {o, zo, o$ } 14 griz grizo griz griz 0 17 griz griz griz griz 0 18 griz griz grize griz { e, ze, e$ } 19 gris griz griz griz 0 31 griz griz griz griz 0 Table 1: One variable</context>
</contexts>
<marker>Sontag, Globerson, Jaakkola, 2011</marker>
<rawString>David Sontag, Amir Globerson, and Tommi Jaakkola. 2011. Introduction to dual decomposition for inference. Optimization for Machine Learning, 1:219– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>917</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="32310" citStr="Spitkovsky et al., 2010" startWordPosition="5484" endWordPosition="5487">M), whose E step imputes the unobserved URs. EM’s E step calls for exact marginal inference, which is intractable for our model. So we substitute the same inference method that we are test11That is, probability mass of (1 − θ)/3 is divided equally among the |Σ |possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1 − θ)/3 is allocated to deletion. 923 ing. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD. As initialization, our first E step uses the trained version of Model 1 for the same inference method. 5.5 Inference Details We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many parts of the graph settle down early because discr</context>
<context position="39249" citStr="Spitkovsky et al., 2010" startWordPosition="6694" endWordPosition="6697">ees more with SP than with MP, even though SP uses marginal inference. Although the aggregate results on the EXERCISE dataset show a large improvement of DD over both of the BP algorithms, the gain all comes from the English language. SP actually does better than DD on Catalan and Maori, and MP also gets better results than DD on Maori, tying with SP. For Model 2S, all inference methods achieved 100% accuracy on the EXERCISE dataset, so we do not show a table. The results on the CELEX dataset are shown in Table 2c. Here both DD and MP performed equally well, and outperformed BP—a result like (Spitkovsky et al., 2010). This trend is consistent over all three languages: DD and MP always achieve similar results and both outperform SP. Of course, one advantage of DD in the setting is that it actually finds the true MAP prediction of the model; the errors are known to be due to the model, not the search procedure. For Model 2E, we show results on the EXERCISE dataset in Table 2d. Here the results resemble the pattern of Model 1. 7 Conclusion and Future Work We presented a general dual decomposition algorithm for MAP inference on graphical models over strings, and applied it to an unsupervised learning task in </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proceedings of CoNLL, page 917, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1453</pages>
<contexts>
<context position="6778" citStr="Tsochantaridis et al., 2005" startWordPosition="1050" endWordPosition="1053">pretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def = (1/Z) exp score(x). 2.2 The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical 2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment. models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over E∗ where E is some f</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
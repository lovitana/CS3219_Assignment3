<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001727">
<title confidence="0.981066">
Key Concept Identification for Medical Information Retrieval
</title>
<author confidence="0.997409">
Jiaping Zheng
</author>
<affiliation confidence="0.997170666666667">
College of Information
and Computer Sciences
University of Massachusetts
</affiliation>
<address confidence="0.500785">
Amherst, MA
</address>
<email confidence="0.99593">
jzheng@cs.umass.edu
</email>
<author confidence="0.979752">
Hong Yu
</author>
<affiliation confidence="0.95604725">
Bedford VA Medical Center
Bedford, MA
Department of Quantitative Health Sciences
University of Massachusetts
</affiliation>
<address confidence="0.831165">
Worcester, MA
</address>
<email confidence="0.998438">
hong.yu@umassmed.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914647058824">
The difficult language in Electronic Health
Records (EHRs) presents a challenge to
patients’ understanding of their own con-
ditions. One approach to lowering the
barrier is to provide tailored patient ed-
ucation based on their own EHR notes.
We are developing a system to retrieve
EHR note-tailored online consumer ori-
ented health education materials. We ex-
plored topic model and key concept identi-
fication methods to construct queries from
the EHR notes. Our experiments show
that queries using identified key concepts
with pseudo-relevance feedback signifi-
cantly outperform (over 10-fold improve-
ment) the baseline system of using the full
text note.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999316173913043">
Allowing patients access to their own electronic
health records (EHRs) can enhance medical un-
derstanding and provide clinical relevant bene-
fits (Wiljer et al., 2006), including increased med-
ication adherence (Delbanco et al., 2012). How-
ever, EHR notes present unique challenges to the
average patients. Since these notes are not usually
targeted at the patients (Mossanen et al., 2014),
languages that may be difficult for non-medical
professionals to comprehend are prevalent, in-
cluding medical terms, abbreviations, and medi-
cal domain-specific language patterns. The valu-
able and authoritative information contained in the
EHR is thus less accessible to the patients, who ul-
timately stand to benefit the most from the infor-
mation.
To address the challenges, we are developing a
system to link medical notes to targeted education
materials from trusted resources. The textual nar-
ratives in the EHR notes are not conducive to ef-
fective and efficient query. We therefore explored
topic model and key concept identification meth-
ods to construct short queries from the EHR notes.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999898722222222">
Patent retrieval (Fujii et al., 2007) is similar to
this work, as the queries are usually long and
complex patent documents. Several methods
have been proposed to construct shorter queries
from the documents. For example, words in the
summary section of a patent document can be
ranked by TFIDF scores and extracted to form a
query (Xue and Croft, 2009). Sentences that are
similar to pseudo-relevant documents according to
a language model are also used to reduce query
length (Ganguly et al., 2011). Other similarity
measures such as Kullback-Leibler divergence are
used to extract terms, which are expanded to gen-
erate queries in the patent retrieval domain (Mahd-
abi et al., 2012). However, the patent retrieval do-
main is recall-driven, while in our scenario, pa-
tients are generally not expected to read relevant
education documents exhaustively.
Various methods have also been proposed to re-
trieve documents relevant to passages of text or
web documents. A model extended from CRF is
proposed to identify noun phrases and named en-
tities from a user-selected passage as queries (Lee
and Croft, 2012). Similarly, noun phrases in a ver-
bose query are also used as candidates for key con-
cepts (Bendersky and Croft, 2008). Other related
work that reduces long queries includes ranking all
subsets of the original query (Kumaran and Car-
valho, 2009). However, typical EHR notes are
longer than the passages and verbose queries in
these systems, which makes the graphical model
and other learning based models less efficient.
Moreover, parsers as required by Bendersky et
al. (2009) and Named Entity Recognizers for the
medical domain are less effective than the general
domain.
</bodyText>
<page confidence="0.977828">
579
</page>
<note confidence="0.701662">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 579–584,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.963539">
TREC Clinical Decision Support Track1 is an
IR challenge to link medical cases to information
relevant for patient care. Unlike our system, this
task is designed to address the physicians’ infor-
mation needs of diagnosis, testing and treatment
for the patients.
</bodyText>
<sectionHeader confidence="0.987863" genericHeader="method">
3 Materials and Methods
</sectionHeader>
<subsectionHeader confidence="0.996894">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999969621621622">
MedlinePlus2 provides current and reliable infor-
mation about over 900 health topics pages and
1000 medication pages to users in consumer-
oriented lay language. Additionally, the medi-
cal encyclopedia section includes over 7000 arti-
cles about diseases, tests, symptoms, injuries, and
surgeries. We include in this study the textual nar-
ratives in the “health topics”, “drugs, supplements,
and herbal information”, and “medical encyclope-
dia” sections of the MedlinePlus as the collection
of educational materials. There are a total of ap-
proximately 9400 articles in this collection, which
we designate as MedlinePlus. Table 3.1 summa-
rizes the characteristics of the collection.
We index the MedlinePlus documents with
Galago, an advanced open source search engine.
Galago implements the inference network retrieval
model (Turtle and Croft, 1991). This model cal-
culates the probability of the user’s information
needs being satisfied given a document in a di-
rected acyclic graph. This framework is applied
in many information retrieval tasks, and shown to
be successful (Metzler et al., 2004).
Twenty progress notes are randomly selected
from a corpus of de-identified EHR notes as the
EHR document collection. Each note contains on
average 261 tokens, with a standard deviation of
133. A physician read each note, and manually
identified relevant education materials from the
MedlinePlus documents. Each EHR note is linked
to 22 education material documents on average.
For example, Table 3.1 shows the summary of one
EHR note and some of its relevant MedlinePlus
documents. There are approximately 30 sentences
or 360 tokens in the actual document.
We adopted Mean Average Precision (MAP)
and Precision at 10 to evaluate the IR systems.
</bodyText>
<footnote confidence="0.9983005">
1http://www.trec-cds.org/
2http://www.nlm.nih.gov/medlineplus/
</footnote>
<bodyText confidence="0.854282">
Summary of EHR Note
Patient remains in ICU with the following
problems: respiratory failure, hemodynam-
ics, renal failure, status post liver transplant,
atrial fib, infectious disease, nutrition.
</bodyText>
<figure confidence="0.524544">
Select Relevant Documents
Respiratory Failure
Deep Vein Thrombosis
Aspiration pneumonia
Pulmonary Hypertension
Kidney Failure
Atrial Fibrillation or Flutter
Liver Transplantation
Dialysis - Hemodialysis
</figure>
<tableCaption confidence="0.8093335">
Table 2: Example EHR Note and its relevant doc-
uments
</tableCaption>
<subsectionHeader confidence="0.998113">
3.2 IR Systems
</subsectionHeader>
<bodyText confidence="0.99994575">
We investigate several query generation ap-
proaches, whereby short queries are built from an
EHR note. In our queries, sequential dependence
model (Metzler and Croft, 2005) was used to cap-
ture the dependencies in a multi-word query term.
In this model, given a query, documents are ranked
based on features of documents containing a single
query term, two query terms sequentially appear-
ing in the query, and two query terms in any or-
der. This model has been shown to be effective in
many applications (Balasubramanian et al., 2007;
Cartright et al., 2011; Bendersky et al., 2009).
</bodyText>
<subsectionHeader confidence="0.842521">
3.2.1 Baseline Approach
</subsectionHeader>
<bodyText confidence="0.999971555555556">
Our baseline approach issues each of the 20 test
EHR notes as the query to the MedlinePlus doc-
ument index and retrieves top 500 relevant doc-
uments. Although queries are not generally as
long as EHR notes, an average patient without ad-
equate medical knowledge may have difficulties
constructing effective queries. Thus, this baseline
can be considered as a proxy to how a patient ac-
tually conducts his own search in the real word.
</bodyText>
<subsectionHeader confidence="0.622193">
3.2.2 LDA Models
</subsectionHeader>
<bodyText confidence="0.999898166666667">
We trained LDA topic models from over 6000 de-
identified EHR notes to identify prominent top-
ics from the test notes. The number of topics
are selected based on retrieval performance. LDA
models extract distributions over individual word
tokens for each topic. However, medical con-
</bodyText>
<page confidence="0.989941">
580
</page>
<table confidence="0.9998868">
Document Type Documents (Tokens) Average Tokens (StdDev)
Health Topics 956 (141,185) 147.7 (37.6)
Medical Encyclopedia 7078 (5,126,101) 724.2 (363.7)
Drugs, Supplements, and Herbal Information 1332 (1,726,570) 1296.2 (992.8)
Total 9366 (6,993,856) 749.1 (565.9)
</table>
<tableCaption confidence="0.999034">
Table 1: MedlinePlus Collection
</tableCaption>
<bodyText confidence="0.999923875">
cepts often contain more than one token. We em-
ploy turbo topics (Blei and Lafferty, 2009) to find
phrases from these topics. This method builds sig-
nificant n-grams based on a language model of ar-
bitrary length expressions. Queries are then gener-
ated from the n-grams. We take the top 5 phrases
as queries from the topics that has a combined
probability of over 80%.
</bodyText>
<sectionHeader confidence="0.9097035" genericHeader="method">
3.2.3 Learning-Based Key Concept
Identification
</sectionHeader>
<bodyText confidence="0.9998520625">
We also developed learning-based key concept
identification to build queries from EHR notes.
We employed Conditional Random Fields (CRF)
model (Lafferty et al., 2001) to identify key con-
cepts, which are most in need of explanation by
external education materials. These key con-
cepts can be considered in a broad sense topics,
as they also capture various aspects of the EHR
note content. We explored lexical, morpholog-
ical, UMLS (Bodenreider, 2004) semantic type,
and word embeddings as features. The word em-
beddings were induced from a combination of
Wikipedia articles in the Medicine category and
de-identified EHR progress notes.
Three CRF models are learned using differ-
ent training data. One uses Wikipedia articles.
Wikipedia, especially the Medicine category, is an
appealing resource for such information as the hu-
man curated links in them are naturally concepts
that are important. However, the number of ar-
ticles in the Medicine category outnumbers our
EHR notes substantially, we therefore restricted
the Wikipedia articles to the Diabetes category.
The paragraphs before the table of contents box
are used. Anchor texts in these paragraphs are
treated as key concepts. The second model uses
EHR notes. Training data for the model was gen-
erated from the retrieval gold standard. A phrase
in an EHR note would be annotated as a key con-
cept if it matches the title of one of the note’s
relevant MedlinePlus documents. Lastly, we aug-
mented the EHR corpus with Wikipedia articles,
</bodyText>
<table confidence="0.998585714285714">
System P@10 MAP Increase
1 Baseline 0% 0.0091 -
2 CHV 5% 0.0240 2.6
3 LDA 10% 0.0489 5.4
4 Key (Wiki) 16% 0.0851 9.4
5 Key (EHR) 16.5% 0.0879 9.7
6 Key (Wiki+EHR) 18% 0.1030 11.3
</table>
<tableCaption confidence="0.999281">
Table 3: System Performance
</tableCaption>
<bodyText confidence="0.999101">
and compared its performance to the other CRF-
based models. Leave-one-out cross validation is
used in the last two models.
</bodyText>
<subsectionHeader confidence="0.534543">
3.2.4 Query Expansion
</subsectionHeader>
<bodyText confidence="0.999952846153846">
We explored query expansion by incorporating
relevance feedback from pseudo-relevant docu-
ments. The initial queries are generated using
methods described previously. Among the top 20
retrieval results, those with a title that matches
one of the identified key concepts are considered
pseudo-relevant documents. This additional re-
quirement is to ensure that the expanded concepts
do not drift from the main topic of the medical
notes. From these documents, medical concepts
are extracted using MetaMap (Aronson and Lang,
2010). These concepts, with their synonyms pro-
vided by UMLS, are used as expansions.
</bodyText>
<sectionHeader confidence="0.989858" genericHeader="method">
4 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.926578">
4.1 LDA Models
</subsectionHeader>
<bodyText confidence="0.999589571428571">
100 topics are learned from the de-identified EHR
note collection. This level of topic granularity
shows the best performance in our experiments.
The retrieval result is shown in Table 4, row 3. The
improvement over the baseline is 5.4 folds, and is
statistically significant using a paired Student’s t-
test (p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.961151">
4.2 Key Concept Identification
</subsectionHeader>
<bodyText confidence="0.995697">
The key concept identification performance of the
three CRF models is shown in Table 4.2. Retrieval
</bodyText>
<page confidence="0.997241">
581
</page>
<table confidence="0.9764936">
Training Data
Wiki EHR Wiki+EHR
Precision 16.27% 35.92% 33.79%
Recall 26.88% 34.09% 33.18%
F1 18.74% 33.70% 32.54%
</table>
<tableCaption confidence="0.995288">
Table 4: Key concept identification performance
</tableCaption>
<table confidence="0.9997485">
Training Data P@10 MAP
Wiki 20% (16%) 0.1067 (0.0851)
EHR 16% (16.5%) 0.0951 (0.0879)
Wiki+EHR 21% (18%) 0.1169 (0.1030)
</table>
<tableCaption confidence="0.845943666666667">
Table 5: System performance with pseudo-
relevance feedback. Numbers in parentheses are
without pseudo-relevance feedback.
</tableCaption>
<bodyText confidence="0.999270142857143">
performance of these models are shown in Table 4,
rows 4 to 6. All systems showed a statistical sig-
nificant improvement over the baseline. The last
model’s improvement is also statistically signif-
icant over the LDA approach. Query expansion
methods further improved system performance, as
shown in Table 4.2.
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="method">
5 Discussions
</sectionHeader>
<bodyText confidence="0.99413026984127">
From the LDA model, Table 5 shows the top 10 n-
grams from 7 topics trained on the medical text. It
is clear that while topics like the first one capture
medical concepts, others like the second one do
not. The LDA results also highlight the noisy na-
ture of the EHR notes. Queries formed by includ-
ing the generic or noisy terms such as “continue
on” will not benefit retrieval results. Examining
the retrieval results, we found that when the promi-
nent topics include medical concepts, the top 10
results usually contain at least one relevant docu-
ment. When only generic topics are identified, rel-
evant documents are absent in the top 10 results.
The CRF models, on the other hand, are bet-
ter at capturing the concepts in the EHR notes.
All three models outperformed the LDA model.
One drawback of using models trained from title-
matching phrases is that the identified key con-
cepts fail to cover the scope of the medical con-
cepts contained in an EHR note. On average
only 23.6% of the concepts are annotated as key
phrases in each note. In the evaluation of the CRF
models for their key concept identification perfor-
Phrases with the highest probability
1 dialysis, hemodialysis, catheter, renal fail-
ure, renal, coumadin, line, picc line, dialy-
sis catheter, failure
2 job id, today, point, continue on, reason-
able, try to, continue, yesterday, left, right
3 continue, patient, job id, pain, patient has,
normal, patient s, white count, secondary
to, culture
4 liver, ascites, normal, tenderness, fluid,
stable, elevated, today, edema, chest
5 preliminary, patient, patient s, time, blood,
mmoll, patient has, routine, high, vial
6 diarrhea, abdominal, flagyl, stool, abdom-
inal pain, colitis, abdomen, difficile, fluid,
distended
7 bipap, pneumonia, year old, respiratory
failure, failure, minutes, requiring, en-
cephalopathy, ards, patient
Table 6: Top 10 n-grams from 7 topics using the
LDA model
mance, inclusion of Wikipedia data slightly de-
creased the F1 score when they were tested on
EHR notes. This can be attributed to the fact that
the Wikipedia articles outnumbered the EHR data
by 7 times. However, this data helped improve the
coverage of the key concepts. In one document,
the augmented model identified two more out of
the total eight key concepts.
Finally, to investigate the gap between medi-
cal language and lay language, we substituted the
medical concepts recognized by MetaMap with
their consumer-oriented counterparts created by
the Consumer Health Vocabulary (CHV) Initia-
tive (Zeng and Tse, 2006). Performance using the
substituted EHR notes as shown in Table 4, row 2
more than doubled. The gap highlights the issue
that patients may have difficulty finding relevant
health information without assistance due to vo-
cabulary mismatch.
</bodyText>
<sectionHeader confidence="0.911429" genericHeader="method">
6 Conclusions, Limitations and Future
Plan
</sectionHeader>
<bodyText confidence="0.999881">
We have shown that using full EHR notes is in-
effective at retrieving relevant education materi-
als. Identifying key concepts of an EHR note and
then using the key concepts as query terms re-
</bodyText>
<page confidence="0.994087">
582
</page>
<bodyText confidence="0.999972684210526">
sult in significantly improved performance (over
10-fold). Furthermore, a query expansion ap-
proach in which key concepts are complemented
by other medical concepts from pseudo-relevant
documents further improves the performance.
One limitation of our design is that only one
physician provided relevancy judgments. Addi-
tional annotators would provide a more rigorous
set of gold standard, allowing us to measure inter-
annotator agreement.
There are several directions we can explore in
our future research. Firstly, our key concept iden-
tification methods are not optimized for the re-
trieval results, but for the identification subtask
only. We hypothesize that directly optimizing the
key concept identifier for retrieval would lead to
better performance. We would also investigate do-
main adaptation techniques to learn key concept
identification models from other data sources.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998822857142857">
This work was supported in part by the Award
1I01HX001457 from the United States Depart-
ment of Veterans Affairs Health Services Research
and Development Program Investigator Initiated
Research. The contents do not represent the views
of the U.S. Department of Veterans Affairs or the
United States Government.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999371298701299">
Alan R Aronson and Franois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. JAm Med Inform Assoc, 17(3):229–
236, June.
Niranjan Balasubramanian, James Allan, and W. Bruce
Croft. 2007. A comparison of sentence retrieval
techniques. In Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’07,
pages 813–814, New York, NY, USA. ACM.
Michael Bendersky and W. Bruce Croft. 2008. Dis-
covering key concepts in verbose queries. In Pro-
ceedings of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ’08, pages 491–498,
New York, NY, USA. ACM.
Michael Bendersky, W. Bruce Croft, and David A.
Smith. 2009. Two-stage query segmentation for
information retrieval. In Proceedings of the 32Nd
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’09, pages 810–811, New York, NY, USA. ACM.
David M. Blei and John D. Lafferty. 2009.
Visualizing topics with multi-word expressions.
arXiv:0907.1013 [stat], July. arXiv: 0907.1013.
Olivier Bodenreider. 2004. The unified medi-
cal language system (UMLS): integrating biomed-
ical terminology. Nucleic Acids Res., 32(Database
issue):D267–270, January.
Marc-Allen Cartright, Henry A. Feild, and James Al-
lan. 2011. Evidence finding using a collection
of books. In Proceedings of the 4th ACM Work-
shop on Online Books, Complementary Social Me-
dia and Crowdsourcing, BooksOnline ’11, pages
11–18, New York, NY, USA. ACM.
Tom Delbanco, Jan Walker, Sigall K. Bell, Jonathan D.
Darer, Joann G. Elmore, Nadine Farag, Henry J.
Feldman, Roanne Mejilla, Long Ngo, James D.
Ralston, Stephen E. Ross, Neha Trivedi, Elisabeth
Vodicka, and Suzanne G. Leveille. 2012. Invit-
ing patients to read their doctors’ notes: a quasi-
experimental study and a look ahead. Ann. Intern.
Med., 157(7):461–470, October.
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2007. Introduction to the special issue on patent pro-
cessing. Information Processing &amp; Management,
43(5):1149–1153, September.
Debasis Ganguly, Johannes Leveling, Walid Magdy,
and Gareth J.F. Jones. 2011. Patent query reduction
using pseudo relevance feedback. In Proceedings of
the 20th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ’11, pages
1953–1956, New York, NY, USA. ACM.
Giridhar Kumaran and Vitor R. Carvalho. 2009. Re-
ducing long queries using query quality predictors.
In Proceedings of the 32Nd International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ’09, pages 564–571,
New York, NY, USA. ACM.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282–289, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Chia-Jung Lee and W. Bruce Croft. 2012. Generat-
ing queries from user-selected text. In Proceedings
of the 4th Information Interaction in Context Sym-
posium, IIIX ’12, pages 100–109, New York, NY,
USA. ACM.
Parvaz Mahdabi, Linda Andersson, Mostafa Keikha,
and Fabio Crestani. 2012. Automatic refinement
of patent queries using concept importance predic-
tors. In Proceedings of the 35th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’12, pages 505–514,
New York, NY, USA. ACM.
</reference>
<page confidence="0.98862">
583
</page>
<reference confidence="0.999520096774194">
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of the 28th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ’05, pages 472–479,
New York, NY, USA. ACM.
Donald Metzler, Trevor Strohman, Yun Zhou, and
W. B. Croft. 2004. Indri at TREC 2005: Terabyte
track.
Matthew Mossanen, Lawrence D. True, Jonathan L.
Wright, Funda Vakar-Lopez, Danielle Lavallee, and
John L. Gore. 2014. Surgical pathology and the
patient: a systematic review evaluating the primary
audience of pathology reports. Hum. Pathol., July.
Howard Turtle and W. Bruce Croft. 1991. Evaluation
of an inference network-based retrieval model. ACM
Trans. Inf. Syst., 9(3):187–222, July.
David Wiljer, Sima Bogomilsky, Pamela Catton, Cindy
Murray, Janice Stewart, and Mark Minden. 2006.
Getting results for hematology patients through ac-
cess to the electronic health record. Can Oncol Nurs
J, 16(3):154–164.
Xiaobing Xue and W. Bruce Croft. 2009. Transform-
ing patents into prior-art queries. In Proceedings
of the 32Nd International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, SIGIR ’09, pages 808–809, New York, NY,
USA. ACM.
Qing T. Zeng and Tony Tse. 2006. Exploring and de-
veloping consumer health vocabularies. J Am Med
Inform Assoc, 13(1):24–29, February.
</reference>
<page confidence="0.998598">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.206746">
<title confidence="0.879205">Key Concept Identification for Medical Information Retrieval</title>
<author confidence="0.58905">Jiaping</author>
<affiliation confidence="0.979451333333333">College of and Computer University of</affiliation>
<address confidence="0.639206">Amherst,</address>
<email confidence="0.999853">jzheng@cs.umass.edu</email>
<author confidence="0.863784">Hong</author>
<affiliation confidence="0.9278754">Bedford VA Medical Bedford, Department of Quantitative Health University of Worcester,</affiliation>
<email confidence="0.999851">hong.yu@umassmed.edu</email>
<abstract confidence="0.998718388888889">The difficult language in Electronic Health Records (EHRs) presents a challenge to patients’ understanding of their own conditions. One approach to lowering the barrier is to provide tailored patient education based on their own EHR notes. We are developing a system to retrieve EHR note-tailored online consumer oriented health education materials. We explored topic model and key concept identification methods to construct queries from the EHR notes. Our experiments show that queries using identified key concepts with pseudo-relevance feedback significantly outperform (over 10-fold improvement) the baseline system of using the full text note.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
<author>Franois-Michel Lang</author>
</authors>
<title>An overview of MetaMap: historical perspective and recent advances. JAm Med Inform Assoc,</title>
<date>2010</date>
<volume>17</volume>
<issue>3</issue>
<pages>236</pages>
<contexts>
<context position="10933" citStr="Aronson and Lang, 2010" startWordPosition="1696" endWordPosition="1699">ased models. Leave-one-out cross validation is used in the last two models. 3.2.4 Query Expansion We explored query expansion by incorporating relevance feedback from pseudo-relevant documents. The initial queries are generated using methods described previously. Among the top 20 retrieval results, those with a title that matches one of the identified key concepts are considered pseudo-relevant documents. This additional requirement is to ensure that the expanded concepts do not drift from the main topic of the medical notes. From these documents, medical concepts are extracted using MetaMap (Aronson and Lang, 2010). These concepts, with their synonyms provided by UMLS, are used as expansions. 4 Experiment Results 4.1 LDA Models 100 topics are learned from the de-identified EHR note collection. This level of topic granularity shows the best performance in our experiments. The retrieval result is shown in Table 4, row 3. The improvement over the baseline is 5.4 folds, and is statistically significant using a paired Student’s ttest (p &lt; 0.05). 4.2 Key Concept Identification The key concept identification performance of the three CRF models is shown in Table 4.2. Retrieval 581 Training Data Wiki EHR Wiki+EH</context>
</contexts>
<marker>Aronson, Lang, 2010</marker>
<rawString>Alan R Aronson and Franois-Michel Lang. 2010. An overview of MetaMap: historical perspective and recent advances. JAm Med Inform Assoc, 17(3):229– 236, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>James Allan</author>
<author>W Bruce Croft</author>
</authors>
<title>A comparison of sentence retrieval techniques.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07,</booktitle>
<pages>813--814</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7055" citStr="Balasubramanian et al., 2007" startWordPosition="1073" endWordPosition="1076">ation Dialysis - Hemodialysis Table 2: Example EHR Note and its relevant documents 3.2 IR Systems We investigate several query generation approaches, whereby short queries are built from an EHR note. In our queries, sequential dependence model (Metzler and Croft, 2005) was used to capture the dependencies in a multi-word query term. In this model, given a query, documents are ranked based on features of documents containing a single query term, two query terms sequentially appearing in the query, and two query terms in any order. This model has been shown to be effective in many applications (Balasubramanian et al., 2007; Cartright et al., 2011; Bendersky et al., 2009). 3.2.1 Baseline Approach Our baseline approach issues each of the 20 test EHR notes as the query to the MedlinePlus document index and retrieves top 500 relevant documents. Although queries are not generally as long as EHR notes, an average patient without adequate medical knowledge may have difficulties constructing effective queries. Thus, this baseline can be considered as a proxy to how a patient actually conducts his own search in the real word. 3.2.2 LDA Models We trained LDA topic models from over 6000 deidentified EHR notes to identify </context>
</contexts>
<marker>Balasubramanian, Allan, Croft, 2007</marker>
<rawString>Niranjan Balasubramanian, James Allan, and W. Bruce Croft. 2007. A comparison of sentence retrieval techniques. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages 813–814, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W Bruce Croft</author>
</authors>
<title>Discovering key concepts in verbose queries.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>491--498</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3317" citStr="Bendersky and Croft, 2008" startWordPosition="509" endWordPosition="512">ich are expanded to generate queries in the patent retrieval domain (Mahdabi et al., 2012). However, the patent retrieval domain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web documents. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 2012). Similarly, noun phrases in a verbose query are also used as candidates for key concepts (Bendersky and Croft, 2008). Other related work that reduces long queries includes ranking all subsets of the original query (Kumaran and Carvalho, 2009). However, typical EHR notes are longer than the passages and verbose queries in these systems, which makes the graphical model and other learning based models less efficient. Moreover, parsers as required by Bendersky et al. (2009) and Named Entity Recognizers for the medical domain are less effective than the general domain. 579 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 579–584, Lisbon, Portugal, 17-21 September 2015</context>
</contexts>
<marker>Bendersky, Croft, 2008</marker>
<rawString>Michael Bendersky and W. Bruce Croft. 2008. Discovering key concepts in verbose queries. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 491–498, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W Bruce Croft</author>
<author>David A Smith</author>
</authors>
<title>Two-stage query segmentation for information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09,</booktitle>
<pages>810--811</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3675" citStr="Bendersky et al. (2009)" startWordPosition="565" endWordPosition="568">nts. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 2012). Similarly, noun phrases in a verbose query are also used as candidates for key concepts (Bendersky and Croft, 2008). Other related work that reduces long queries includes ranking all subsets of the original query (Kumaran and Carvalho, 2009). However, typical EHR notes are longer than the passages and verbose queries in these systems, which makes the graphical model and other learning based models less efficient. Moreover, parsers as required by Bendersky et al. (2009) and Named Entity Recognizers for the medical domain are less effective than the general domain. 579 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 579–584, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. TREC Clinical Decision Support Track1 is an IR challenge to link medical cases to information relevant for patient care. Unlike our system, this task is designed to address the physicians’ information needs of diagnosis, testing and treatment for the patients. 3 Materials and Methods 3.1 Data MedlinePlus2</context>
<context position="7104" citStr="Bendersky et al., 2009" startWordPosition="1081" endWordPosition="1084">e and its relevant documents 3.2 IR Systems We investigate several query generation approaches, whereby short queries are built from an EHR note. In our queries, sequential dependence model (Metzler and Croft, 2005) was used to capture the dependencies in a multi-word query term. In this model, given a query, documents are ranked based on features of documents containing a single query term, two query terms sequentially appearing in the query, and two query terms in any order. This model has been shown to be effective in many applications (Balasubramanian et al., 2007; Cartright et al., 2011; Bendersky et al., 2009). 3.2.1 Baseline Approach Our baseline approach issues each of the 20 test EHR notes as the query to the MedlinePlus document index and retrieves top 500 relevant documents. Although queries are not generally as long as EHR notes, an average patient without adequate medical knowledge may have difficulties constructing effective queries. Thus, this baseline can be considered as a proxy to how a patient actually conducts his own search in the real word. 3.2.2 LDA Models We trained LDA topic models from over 6000 deidentified EHR notes to identify prominent topics from the test notes. The number </context>
</contexts>
<marker>Bendersky, Croft, Smith, 2009</marker>
<rawString>Michael Bendersky, W. Bruce Croft, and David A. Smith. 2009. Two-stage query segmentation for information retrieval. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09, pages 810–811, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Visualizing topics with multi-word expressions.</title>
<date>2009</date>
<booktitle>arXiv:0907.1013 [stat],</booktitle>
<volume>arXiv:</volume>
<pages>0907--1013</pages>
<contexts>
<context position="8243" citStr="Blei and Lafferty, 2009" startWordPosition="1260" endWordPosition="1263"> deidentified EHR notes to identify prominent topics from the test notes. The number of topics are selected based on retrieval performance. LDA models extract distributions over individual word tokens for each topic. However, medical con580 Document Type Documents (Tokens) Average Tokens (StdDev) Health Topics 956 (141,185) 147.7 (37.6) Medical Encyclopedia 7078 (5,126,101) 724.2 (363.7) Drugs, Supplements, and Herbal Information 1332 (1,726,570) 1296.2 (992.8) Total 9366 (6,993,856) 749.1 (565.9) Table 1: MedlinePlus Collection cepts often contain more than one token. We employ turbo topics (Blei and Lafferty, 2009) to find phrases from these topics. This method builds significant n-grams based on a language model of arbitrary length expressions. Queries are then generated from the n-grams. We take the top 5 phrases as queries from the topics that has a combined probability of over 80%. 3.2.3 Learning-Based Key Concept Identification We also developed learning-based key concept identification to build queries from EHR notes. We employed Conditional Random Fields (CRF) model (Lafferty et al., 2001) to identify key concepts, which are most in need of explanation by external education materials. These key c</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and John D. Lafferty. 2009. Visualizing topics with multi-word expressions. arXiv:0907.1013 [stat], July. arXiv: 0907.1013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The unified medical language system (UMLS): integrating biomedical terminology.</title>
<date>2004</date>
<booktitle>Nucleic Acids Res., 32(Database issue):D267–270,</booktitle>
<contexts>
<context position="9016" citStr="Bodenreider, 2004" startWordPosition="1387" endWordPosition="1388">rated from the n-grams. We take the top 5 phrases as queries from the topics that has a combined probability of over 80%. 3.2.3 Learning-Based Key Concept Identification We also developed learning-based key concept identification to build queries from EHR notes. We employed Conditional Random Fields (CRF) model (Lafferty et al., 2001) to identify key concepts, which are most in need of explanation by external education materials. These key concepts can be considered in a broad sense topics, as they also capture various aspects of the EHR note content. We explored lexical, morphological, UMLS (Bodenreider, 2004) semantic type, and word embeddings as features. The word embeddings were induced from a combination of Wikipedia articles in the Medicine category and de-identified EHR progress notes. Three CRF models are learned using different training data. One uses Wikipedia articles. Wikipedia, especially the Medicine category, is an appealing resource for such information as the human curated links in them are naturally concepts that are important. However, the number of articles in the Medicine category outnumbers our EHR notes substantially, we therefore restricted the Wikipedia articles to the Diabe</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Res., 32(Database issue):D267–270, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc-Allen Cartright</author>
<author>Henry A Feild</author>
<author>James Allan</author>
</authors>
<title>Evidence finding using a collection of books.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th ACM Workshop on Online Books, Complementary Social Media and Crowdsourcing, BooksOnline ’11,</booktitle>
<pages>11--18</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7079" citStr="Cartright et al., 2011" startWordPosition="1077" endWordPosition="1080">Table 2: Example EHR Note and its relevant documents 3.2 IR Systems We investigate several query generation approaches, whereby short queries are built from an EHR note. In our queries, sequential dependence model (Metzler and Croft, 2005) was used to capture the dependencies in a multi-word query term. In this model, given a query, documents are ranked based on features of documents containing a single query term, two query terms sequentially appearing in the query, and two query terms in any order. This model has been shown to be effective in many applications (Balasubramanian et al., 2007; Cartright et al., 2011; Bendersky et al., 2009). 3.2.1 Baseline Approach Our baseline approach issues each of the 20 test EHR notes as the query to the MedlinePlus document index and retrieves top 500 relevant documents. Although queries are not generally as long as EHR notes, an average patient without adequate medical knowledge may have difficulties constructing effective queries. Thus, this baseline can be considered as a proxy to how a patient actually conducts his own search in the real word. 3.2.2 LDA Models We trained LDA topic models from over 6000 deidentified EHR notes to identify prominent topics from th</context>
</contexts>
<marker>Cartright, Feild, Allan, 2011</marker>
<rawString>Marc-Allen Cartright, Henry A. Feild, and James Allan. 2011. Evidence finding using a collection of books. In Proceedings of the 4th ACM Workshop on Online Books, Complementary Social Media and Crowdsourcing, BooksOnline ’11, pages 11–18, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Delbanco</author>
<author>Jan Walker</author>
<author>Sigall K Bell</author>
<author>Jonathan D Darer</author>
<author>Joann G Elmore</author>
<author>Nadine Farag</author>
<author>Henry J Feldman</author>
</authors>
<title>Inviting patients to read their doctors’ notes: a quasiexperimental study and a look ahead.</title>
<date>2012</date>
<journal>Ann. Intern. Med.,</journal>
<volume>157</volume>
<issue>7</issue>
<location>Roanne Mejilla, Long</location>
<contexts>
<context position="1237" citStr="Delbanco et al., 2012" startWordPosition="173" endWordPosition="176">ieve EHR note-tailored online consumer oriented health education materials. We explored topic model and key concept identification methods to construct queries from the EHR notes. Our experiments show that queries using identified key concepts with pseudo-relevance feedback significantly outperform (over 10-fold improvement) the baseline system of using the full text note. 1 Introduction Allowing patients access to their own electronic health records (EHRs) can enhance medical understanding and provide clinical relevant benefits (Wiljer et al., 2006), including increased medication adherence (Delbanco et al., 2012). However, EHR notes present unique challenges to the average patients. Since these notes are not usually targeted at the patients (Mossanen et al., 2014), languages that may be difficult for non-medical professionals to comprehend are prevalent, including medical terms, abbreviations, and medical domain-specific language patterns. The valuable and authoritative information contained in the EHR is thus less accessible to the patients, who ultimately stand to benefit the most from the information. To address the challenges, we are developing a system to link medical notes to targeted education </context>
</contexts>
<marker>Delbanco, Walker, Bell, Darer, Elmore, Farag, Feldman, 2012</marker>
<rawString>Tom Delbanco, Jan Walker, Sigall K. Bell, Jonathan D. Darer, Joann G. Elmore, Nadine Farag, Henry J. Feldman, Roanne Mejilla, Long Ngo, James D. Ralston, Stephen E. Ross, Neha Trivedi, Elisabeth Vodicka, and Suzanne G. Leveille. 2012. Inviting patients to read their doctors’ notes: a quasiexperimental study and a look ahead. Ann. Intern. Med., 157(7):461–470, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>Noriko Kando</author>
</authors>
<title>Introduction to the special issue on patent processing.</title>
<date>2007</date>
<journal>Information Processing &amp; Management,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="2135" citStr="Fujii et al., 2007" startWordPosition="315" endWordPosition="318">ations, and medical domain-specific language patterns. The valuable and authoritative information contained in the EHR is thus less accessible to the patients, who ultimately stand to benefit the most from the information. To address the challenges, we are developing a system to link medical notes to targeted education materials from trusted resources. The textual narratives in the EHR notes are not conducive to effective and efficient query. We therefore explored topic model and key concept identification methods to construct short queries from the EHR notes. 2 Related Work Patent retrieval (Fujii et al., 2007) is similar to this work, as the queries are usually long and complex patent documents. Several methods have been proposed to construct shorter queries from the documents. For example, words in the summary section of a patent document can be ranked by TFIDF scores and extracted to form a query (Xue and Croft, 2009). Sentences that are similar to pseudo-relevant documents according to a language model are also used to reduce query length (Ganguly et al., 2011). Other similarity measures such as Kullback-Leibler divergence are used to extract terms, which are expanded to generate queries in the </context>
</contexts>
<marker>Fujii, Iwayama, Kando, 2007</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 2007. Introduction to the special issue on patent processing. Information Processing &amp; Management, 43(5):1149–1153, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debasis Ganguly</author>
<author>Johannes Leveling</author>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>Patent query reduction using pseudo relevance feedback.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11,</booktitle>
<pages>1953--1956</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2598" citStr="Ganguly et al., 2011" startWordPosition="392" endWordPosition="395">xplored topic model and key concept identification methods to construct short queries from the EHR notes. 2 Related Work Patent retrieval (Fujii et al., 2007) is similar to this work, as the queries are usually long and complex patent documents. Several methods have been proposed to construct shorter queries from the documents. For example, words in the summary section of a patent document can be ranked by TFIDF scores and extracted to form a query (Xue and Croft, 2009). Sentences that are similar to pseudo-relevant documents according to a language model are also used to reduce query length (Ganguly et al., 2011). Other similarity measures such as Kullback-Leibler divergence are used to extract terms, which are expanded to generate queries in the patent retrieval domain (Mahdabi et al., 2012). However, the patent retrieval domain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web documents. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 201</context>
</contexts>
<marker>Ganguly, Leveling, Magdy, Jones, 2011</marker>
<rawString>Debasis Ganguly, Johannes Leveling, Walid Magdy, and Gareth J.F. Jones. 2011. Patent query reduction using pseudo relevance feedback. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11, pages 1953–1956, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>Vitor R Carvalho</author>
</authors>
<title>Reducing long queries using query quality predictors.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09,</booktitle>
<pages>564--571</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3443" citStr="Kumaran and Carvalho, 2009" startWordPosition="528" endWordPosition="532">ain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web documents. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 2012). Similarly, noun phrases in a verbose query are also used as candidates for key concepts (Bendersky and Croft, 2008). Other related work that reduces long queries includes ranking all subsets of the original query (Kumaran and Carvalho, 2009). However, typical EHR notes are longer than the passages and verbose queries in these systems, which makes the graphical model and other learning based models less efficient. Moreover, parsers as required by Bendersky et al. (2009) and Named Entity Recognizers for the medical domain are less effective than the general domain. 579 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 579–584, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. TREC Clinical Decision Support Track1 is an IR challenge to link medical c</context>
</contexts>
<marker>Kumaran, Carvalho, 2009</marker>
<rawString>Giridhar Kumaran and Vitor R. Carvalho. 2009. Reducing long queries using query quality predictors. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09, pages 564–571, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8734" citStr="Lafferty et al., 2001" startWordPosition="1338" endWordPosition="1341">1 (565.9) Table 1: MedlinePlus Collection cepts often contain more than one token. We employ turbo topics (Blei and Lafferty, 2009) to find phrases from these topics. This method builds significant n-grams based on a language model of arbitrary length expressions. Queries are then generated from the n-grams. We take the top 5 phrases as queries from the topics that has a combined probability of over 80%. 3.2.3 Learning-Based Key Concept Identification We also developed learning-based key concept identification to build queries from EHR notes. We employed Conditional Random Fields (CRF) model (Lafferty et al., 2001) to identify key concepts, which are most in need of explanation by external education materials. These key concepts can be considered in a broad sense topics, as they also capture various aspects of the EHR note content. We explored lexical, morphological, UMLS (Bodenreider, 2004) semantic type, and word embeddings as features. The word embeddings were induced from a combination of Wikipedia articles in the Medicine category and de-identified EHR progress notes. Three CRF models are learned using different training data. One uses Wikipedia articles. Wikipedia, especially the Medicine category</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chia-Jung Lee</author>
<author>W Bruce Croft</author>
</authors>
<title>Generating queries from user-selected text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 4th Information Interaction in Context Symposium, IIIX ’12,</booktitle>
<pages>100--109</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3200" citStr="Lee and Croft, 2012" startWordPosition="488" endWordPosition="491">guly et al., 2011). Other similarity measures such as Kullback-Leibler divergence are used to extract terms, which are expanded to generate queries in the patent retrieval domain (Mahdabi et al., 2012). However, the patent retrieval domain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web documents. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 2012). Similarly, noun phrases in a verbose query are also used as candidates for key concepts (Bendersky and Croft, 2008). Other related work that reduces long queries includes ranking all subsets of the original query (Kumaran and Carvalho, 2009). However, typical EHR notes are longer than the passages and verbose queries in these systems, which makes the graphical model and other learning based models less efficient. Moreover, parsers as required by Bendersky et al. (2009) and Named Entity Recognizers for the medical domain are less effective than the general domain. 579 Proceedings of the 2015 </context>
</contexts>
<marker>Lee, Croft, 2012</marker>
<rawString>Chia-Jung Lee and W. Bruce Croft. 2012. Generating queries from user-selected text. In Proceedings of the 4th Information Interaction in Context Symposium, IIIX ’12, pages 100–109, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parvaz Mahdabi</author>
<author>Linda Andersson</author>
<author>Mostafa Keikha</author>
<author>Fabio Crestani</author>
</authors>
<title>Automatic refinement of patent queries using concept importance predictors.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>505--514</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2781" citStr="Mahdabi et al., 2012" startWordPosition="420" endWordPosition="424"> as the queries are usually long and complex patent documents. Several methods have been proposed to construct shorter queries from the documents. For example, words in the summary section of a patent document can be ranked by TFIDF scores and extracted to form a query (Xue and Croft, 2009). Sentences that are similar to pseudo-relevant documents according to a language model are also used to reduce query length (Ganguly et al., 2011). Other similarity measures such as Kullback-Leibler divergence are used to extract terms, which are expanded to generate queries in the patent retrieval domain (Mahdabi et al., 2012). However, the patent retrieval domain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web documents. A model extended from CRF is proposed to identify noun phrases and named entities from a user-selected passage as queries (Lee and Croft, 2012). Similarly, noun phrases in a verbose query are also used as candidates for key concepts (Bendersky and Croft, 2008). Other related work that reduces long queries includes ranking </context>
</contexts>
<marker>Mahdabi, Andersson, Keikha, Crestani, 2012</marker>
<rawString>Parvaz Mahdabi, Linda Andersson, Mostafa Keikha, and Fabio Crestani. 2012. Automatic refinement of patent queries using concept importance predictors. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 505–514, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05,</booktitle>
<pages>472--479</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6696" citStr="Metzler and Croft, 2005" startWordPosition="1010" endWordPosition="1013">R Note Patient remains in ICU with the following problems: respiratory failure, hemodynamics, renal failure, status post liver transplant, atrial fib, infectious disease, nutrition. Select Relevant Documents Respiratory Failure Deep Vein Thrombosis Aspiration pneumonia Pulmonary Hypertension Kidney Failure Atrial Fibrillation or Flutter Liver Transplantation Dialysis - Hemodialysis Table 2: Example EHR Note and its relevant documents 3.2 IR Systems We investigate several query generation approaches, whereby short queries are built from an EHR note. In our queries, sequential dependence model (Metzler and Croft, 2005) was used to capture the dependencies in a multi-word query term. In this model, given a query, documents are ranked based on features of documents containing a single query term, two query terms sequentially appearing in the query, and two query terms in any order. This model has been shown to be effective in many applications (Balasubramanian et al., 2007; Cartright et al., 2011; Bendersky et al., 2009). 3.2.1 Baseline Approach Our baseline approach issues each of the 20 test EHR notes as the query to the MedlinePlus document index and retrieves top 500 relevant documents. Although queries a</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2005. A markov random field model for term dependencies. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05, pages 472–479, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Trevor Strohman</author>
<author>Yun Zhou</author>
<author>W B Croft</author>
</authors>
<date>2004</date>
<note>Indri at TREC 2005: Terabyte track.</note>
<contexts>
<context position="5349" citStr="Metzler et al., 2004" startWordPosition="816" endWordPosition="819">as the collection of educational materials. There are a total of approximately 9400 articles in this collection, which we designate as MedlinePlus. Table 3.1 summarizes the characteristics of the collection. We index the MedlinePlus documents with Galago, an advanced open source search engine. Galago implements the inference network retrieval model (Turtle and Croft, 1991). This model calculates the probability of the user’s information needs being satisfied given a document in a directed acyclic graph. This framework is applied in many information retrieval tasks, and shown to be successful (Metzler et al., 2004). Twenty progress notes are randomly selected from a corpus of de-identified EHR notes as the EHR document collection. Each note contains on average 261 tokens, with a standard deviation of 133. A physician read each note, and manually identified relevant education materials from the MedlinePlus documents. Each EHR note is linked to 22 education material documents on average. For example, Table 3.1 shows the summary of one EHR note and some of its relevant MedlinePlus documents. There are approximately 30 sentences or 360 tokens in the actual document. We adopted Mean Average Precision (MAP) a</context>
</contexts>
<marker>Metzler, Strohman, Zhou, Croft, 2004</marker>
<rawString>Donald Metzler, Trevor Strohman, Yun Zhou, and W. B. Croft. 2004. Indri at TREC 2005: Terabyte track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Mossanen</author>
<author>Lawrence D True</author>
<author>Jonathan L Wright</author>
<author>Funda Vakar-Lopez</author>
<author>Danielle Lavallee</author>
<author>John L Gore</author>
</authors>
<title>Surgical pathology and the patient: a systematic review evaluating the primary audience of pathology reports.</title>
<date>2014</date>
<tech>Hum. Pathol.,</tech>
<contexts>
<context position="1391" citStr="Mossanen et al., 2014" startWordPosition="198" endWordPosition="201">eries from the EHR notes. Our experiments show that queries using identified key concepts with pseudo-relevance feedback significantly outperform (over 10-fold improvement) the baseline system of using the full text note. 1 Introduction Allowing patients access to their own electronic health records (EHRs) can enhance medical understanding and provide clinical relevant benefits (Wiljer et al., 2006), including increased medication adherence (Delbanco et al., 2012). However, EHR notes present unique challenges to the average patients. Since these notes are not usually targeted at the patients (Mossanen et al., 2014), languages that may be difficult for non-medical professionals to comprehend are prevalent, including medical terms, abbreviations, and medical domain-specific language patterns. The valuable and authoritative information contained in the EHR is thus less accessible to the patients, who ultimately stand to benefit the most from the information. To address the challenges, we are developing a system to link medical notes to targeted education materials from trusted resources. The textual narratives in the EHR notes are not conducive to effective and efficient query. We therefore explored topic </context>
</contexts>
<marker>Mossanen, True, Wright, Vakar-Lopez, Lavallee, Gore, 2014</marker>
<rawString>Matthew Mossanen, Lawrence D. True, Jonathan L. Wright, Funda Vakar-Lopez, Danielle Lavallee, and John L. Gore. 2014. Surgical pathology and the patient: a systematic review evaluating the primary audience of pathology reports. Hum. Pathol., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Turtle</author>
<author>W Bruce Croft</author>
</authors>
<title>Evaluation of an inference network-based retrieval model.</title>
<date>1991</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="5103" citStr="Turtle and Croft, 1991" startWordPosition="776" endWordPosition="779">er 7000 articles about diseases, tests, symptoms, injuries, and surgeries. We include in this study the textual narratives in the “health topics”, “drugs, supplements, and herbal information”, and “medical encyclopedia” sections of the MedlinePlus as the collection of educational materials. There are a total of approximately 9400 articles in this collection, which we designate as MedlinePlus. Table 3.1 summarizes the characteristics of the collection. We index the MedlinePlus documents with Galago, an advanced open source search engine. Galago implements the inference network retrieval model (Turtle and Croft, 1991). This model calculates the probability of the user’s information needs being satisfied given a document in a directed acyclic graph. This framework is applied in many information retrieval tasks, and shown to be successful (Metzler et al., 2004). Twenty progress notes are randomly selected from a corpus of de-identified EHR notes as the EHR document collection. Each note contains on average 261 tokens, with a standard deviation of 133. A physician read each note, and manually identified relevant education materials from the MedlinePlus documents. Each EHR note is linked to 22 education materi</context>
</contexts>
<marker>Turtle, Croft, 1991</marker>
<rawString>Howard Turtle and W. Bruce Croft. 1991. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187–222, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wiljer</author>
<author>Sima Bogomilsky</author>
<author>Pamela Catton</author>
<author>Cindy Murray</author>
<author>Janice Stewart</author>
<author>Mark Minden</author>
</authors>
<title>Getting results for hematology patients through access to the electronic health record.</title>
<date>2006</date>
<journal>Can Oncol Nurs J,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="1171" citStr="Wiljer et al., 2006" startWordPosition="164" endWordPosition="167">based on their own EHR notes. We are developing a system to retrieve EHR note-tailored online consumer oriented health education materials. We explored topic model and key concept identification methods to construct queries from the EHR notes. Our experiments show that queries using identified key concepts with pseudo-relevance feedback significantly outperform (over 10-fold improvement) the baseline system of using the full text note. 1 Introduction Allowing patients access to their own electronic health records (EHRs) can enhance medical understanding and provide clinical relevant benefits (Wiljer et al., 2006), including increased medication adherence (Delbanco et al., 2012). However, EHR notes present unique challenges to the average patients. Since these notes are not usually targeted at the patients (Mossanen et al., 2014), languages that may be difficult for non-medical professionals to comprehend are prevalent, including medical terms, abbreviations, and medical domain-specific language patterns. The valuable and authoritative information contained in the EHR is thus less accessible to the patients, who ultimately stand to benefit the most from the information. To address the challenges, we ar</context>
</contexts>
<marker>Wiljer, Bogomilsky, Catton, Murray, Stewart, Minden, 2006</marker>
<rawString>David Wiljer, Sima Bogomilsky, Pamela Catton, Cindy Murray, Janice Stewart, and Mark Minden. 2006. Getting results for hematology patients through access to the electronic health record. Can Oncol Nurs J, 16(3):154–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>W Bruce Croft</author>
</authors>
<title>Transforming patents into prior-art queries.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09,</booktitle>
<pages>808--809</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2451" citStr="Xue and Croft, 2009" startWordPosition="369" endWordPosition="372">ation materials from trusted resources. The textual narratives in the EHR notes are not conducive to effective and efficient query. We therefore explored topic model and key concept identification methods to construct short queries from the EHR notes. 2 Related Work Patent retrieval (Fujii et al., 2007) is similar to this work, as the queries are usually long and complex patent documents. Several methods have been proposed to construct shorter queries from the documents. For example, words in the summary section of a patent document can be ranked by TFIDF scores and extracted to form a query (Xue and Croft, 2009). Sentences that are similar to pseudo-relevant documents according to a language model are also used to reduce query length (Ganguly et al., 2011). Other similarity measures such as Kullback-Leibler divergence are used to extract terms, which are expanded to generate queries in the patent retrieval domain (Mahdabi et al., 2012). However, the patent retrieval domain is recall-driven, while in our scenario, patients are generally not expected to read relevant education documents exhaustively. Various methods have also been proposed to retrieve documents relevant to passages of text or web docum</context>
</contexts>
<marker>Xue, Croft, 2009</marker>
<rawString>Xiaobing Xue and W. Bruce Croft. 2009. Transforming patents into prior-art queries. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09, pages 808–809, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing T Zeng</author>
<author>Tony Tse</author>
</authors>
<title>Exploring and developing consumer health vocabularies.</title>
<date>2006</date>
<journal>J Am Med Inform Assoc,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="14743" citStr="Zeng and Tse, 2006" startWordPosition="2312" endWordPosition="2315">mance, inclusion of Wikipedia data slightly decreased the F1 score when they were tested on EHR notes. This can be attributed to the fact that the Wikipedia articles outnumbered the EHR data by 7 times. However, this data helped improve the coverage of the key concepts. In one document, the augmented model identified two more out of the total eight key concepts. Finally, to investigate the gap between medical language and lay language, we substituted the medical concepts recognized by MetaMap with their consumer-oriented counterparts created by the Consumer Health Vocabulary (CHV) Initiative (Zeng and Tse, 2006). Performance using the substituted EHR notes as shown in Table 4, row 2 more than doubled. The gap highlights the issue that patients may have difficulty finding relevant health information without assistance due to vocabulary mismatch. 6 Conclusions, Limitations and Future Plan We have shown that using full EHR notes is ineffective at retrieving relevant education materials. Identifying key concepts of an EHR note and then using the key concepts as query terms re582 sult in significantly improved performance (over 10-fold). Furthermore, a query expansion approach in which key concepts are co</context>
</contexts>
<marker>Zeng, Tse, 2006</marker>
<rawString>Qing T. Zeng and Tony Tse. 2006. Exploring and developing consumer health vocabularies. J Am Med Inform Assoc, 13(1):24–29, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
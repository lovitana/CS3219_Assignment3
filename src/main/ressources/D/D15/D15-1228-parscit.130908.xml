<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000292">
<title confidence="0.995037">
Extractive Summarization by Maximizing Semantic Volume
</title>
<author confidence="0.998996">
Dani Yogatama
</author>
<affiliation confidence="0.933009666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.976508">
dyogatama@cs.cmu.edu
</email>
<author confidence="0.958099">
Fei Liu
</author>
<affiliation confidence="0.853088">
Electrical Engineering &amp; Computer Science
University of Central Florida
Orlando, FL 32816, USA
</affiliation>
<email confidence="0.980131">
feiliu@cs.ucf.edu
</email>
<author confidence="0.99272">
Noah A. Smith
</author>
<affiliation confidence="0.9957445">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.618215">
Seattle, WA 98195, USA
</address>
<email confidence="0.998065">
nasmith@cs.washington.edu
</email>
<sectionHeader confidence="0.994782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962285714286">
The most successful approaches to extrac-
tive text summarization seek to maximize
bigram coverage subject to a budget con-
straint. In this work, we propose instead
to maximize semantic volume. We em-
bed each sentence in a semantic space and
construct a summary by choosing a sub-
set of sentences whose convex hull max-
imizes volume in that space. We provide
a greedy algorithm based on the Gram-
Schmidt process to efficiently perform
volume maximization. Our method out-
performs the state-of-the-art summariza-
tion approaches on benchmark datasets.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938375">
In artificial intelligence, changes in representation
sometimes suggest new algorithms. For example,
increased attention to distributed meaning repre-
sentations suggests that existing combinatorial al-
gorithms for NLP might be supplanted by alterna-
tives designed specifically for embeddings. In this
work, we consider summarization.
Classical approaches to extractive summariza-
tion represent each sentence as a bag of terms
(typically bigrams) and seek a subset of sentences
from the input document(s) that either (a) trade off
between high relevance and low redundancy (Car-
bonell and Goldstein, 1998; McDonald, 2007), or
(b) maximize bigram coverage (Yih et al., 2007;
Gillick et al., 2008). The sentence representa-
tion is fundamentally discrete, and a range of
greedy (Carbonell and Goldstein, 1998), approx-
imate (Almeida and Martins, 2013), and exact op-
timization algorithms (McDonald, 2007; Martins
and Smith, 2009; Berg-Kirkpatrick et al., 2011)
have been proposed.
Recent studies have explored continuous sen-
tence representations, including the paragraph
vector (Le and Mikolov, 2014), a convolutional
neural network architecture (Kalchbrenner et al.,
2014), and a dictionary learning approach (Jenat-
ton et al., 2011). If sentences are represented as
low-dimensional embeddings in a distributed se-
mantic space, then we begin to imagine a geomet-
ric relationship between a summary and a doc-
ument. We propose that the volume of a sum-
mary (i.e., the semantic subspace spanned by the
selected sentences) should ideally be large. We
therefore formalize a new objective function for
summarization based on semantic volume (§2),
and we provide a fast greedy algorithm that can
be used to maximize it (§3). We show that our
method outperforms competing extractive base-
lines under similar experimental conditions on
benchmark summarization datasets (§4).
</bodyText>
<sectionHeader confidence="0.991155" genericHeader="method">
2 Extractive Summarization Models
</sectionHeader>
<bodyText confidence="0.948161071428571">
Assume we are given a set of N sentences: D =
{s1, s2, ... , sN} from one or many documents,
and the goal is to produce a summary by choos-
ing a subset S of M sentences, where S ⊆ D and
M G N, and the length of the summary is less
than or equal to L words. In this work, we as-
sume no summaries are available as training data.
Denote a binary indicator vector y ∈ ][8N, where
sentence i is included if and only if yi = 1 and 0
otherwise. Extractive summarization can be writ-
ten as an optimization problem:
max score(S) = score(D, y)
with respect to S equivalently y
subject to length(S) G L
</bodyText>
<page confidence="0.944394">
1961
</page>
<note confidence="0.6535815">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1961–1966,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9998526">
with a scoring function score(D, y). A good scor-
ing function should assign higher scores to bet-
ter summaries. In the following, we describe two
commonly used scoring functions and our pro-
posed scoring function.
</bodyText>
<subsectionHeader confidence="0.995346">
2.1 Maximal Marginal Relevance
</subsectionHeader>
<bodyText confidence="0.996987333333333">
The Maximal Marginal Relevance (MMR) method
(Carbonell and Goldstein, 1998) considers the fol-
lowing scoring function:
</bodyText>
<equation confidence="0.988779">
N N
score(D, y) = yiRel(si) − yiyjSim(si, sj)
i=1 i,j=1
</equation>
<bodyText confidence="0.999136090909091">
where Rel(si) measures the relevancy of sentence
i and Sim(si, sj) measures the (e.g., cosine) simi-
larity between sentence i and sentence j. The in-
tuition is to choose sentences that are highly rel-
evant to the document(s) and avoid redundancy.
The above maximization problem has been shown
to be NP-hard, solvable exactly using ILP (Mc-
Donald, 2007). A greedy algorithm that approxi-
mates the global solution by adding one sentence
at a time to maximize the overall score (Lin and
Bilmes, 2010) is often used in practice.
</bodyText>
<subsectionHeader confidence="0.993483">
2.2 Coverage-Based Summarization
</subsectionHeader>
<bodyText confidence="0.999978333333333">
Another popular scoring function aims to give
higher scores for covering more diverse concepts
in the summary. Gillick et al. (2008) use bigrams
as a surrogate for concepts. Following convention,
we extract bigrams from each sentence si E D.
Denote the number of unique bigrams extracted
from all sentences by B. We introduce another
binary vector z E RB to indicate the presence or
absence of a bigram in the summary, and a binary
indicator matrix M E RN×B, where mi,j is 1 if
and only if bigram j is present in sentence i and 0
otherwise. The scoring function is:
</bodyText>
<equation confidence="0.996692666666667">
B
score(D, y, z) = bjzj
j=1
</equation>
<bodyText confidence="0.951122">
and the two additional constraints are:
</bodyText>
<equation confidence="0.96933825">
Vj E [B], Vi E [N] yimi,j G zj
N
Vj E [B] yimi,j ? zj
i=1
</equation>
<bodyText confidence="0.998696666666667">
where we use [B] as a shorthand for 11, 2, ... , B}.
The first constraint makes sure that selecting a sen-
tence implies selecting all its bigrams, whereas the
</bodyText>
<figureCaption confidence="0.869384">
Figure 1: A toy example of seven sentences
</figureCaption>
<bodyText confidence="0.9499371875">
projected into a two-dimensional semantic space.
Consider the case when the maximum summary
length is four sentences. Our scoring function is
optimized by chooseing the four sentences in red
as the summary, since they maximize the volume
(area in two dimensions).
second constraint makes sure that selecting a bi-
gram implies selecting at least one of the sentences
that contains it. In this formulation, there is no ex-
plicit penalty on redundancy. However, insofar as
redundant sentences cover fewer bigrams, they are
implicitly discouraged. Although the above scor-
ing function also results in an NP-hard problem,
an off-the-shelf ILP solver (Gillick et al., 2008)
or a dual decomposition algorithm (Almeida and
Martins, 2013) can be used to solve it in practice.
</bodyText>
<subsectionHeader confidence="0.999431">
2.3 Semantic Volume
</subsectionHeader>
<bodyText confidence="0.999964928571429">
We introduce a new scoring function for summa-
rization. The main idea is based on the notion of
coverage, but in a distributed semantic space: a
good summary should have broad semantic cover-
age with respect to document contents. For every
sentence si, i E [N], we denote its continuous se-
mantic representation in a K-dimensional seman-
tic space by Q(si) = ui E RK, where Q is a func-
tion that takes a sentence and returns its semantic
vector representation. We denote embeddings of
all sentences in D with the function Q by Q(D).
We will return to the choice of Q later. We propose
to use a scoring function that maximizes the vol-
ume of selected sentences in this semantic space:
</bodyText>
<equation confidence="0.655918">
score(D, y) = Volume(Q(D), y) = Volume(Q(S))
</equation>
<bodyText confidence="0.998432625">
In the case when K = 2, this scoring function
maximizes the area of a polytope, as illustrated in
Figure 1. In the example, there exists a maximum
number of sentences that can be selected such that
adding more sentences does not increase the score,
i.e., the set of selected sentences forms a convex
hull of the set of all sentences. The sentences
forming a convex hull may together be longer than
</bodyText>
<page confidence="0.980746">
1962
</page>
<bodyText confidence="0.999858166666667">
L words, so we seek to maximize the volume of
the summary under this constraint.
There are many choices of Q that we can use to
produce sentence embeddings. As an exploratory
study, we construct a vector of bigrams for each
sentence, that is, si ∈ RB, ∀i ∈ [N]. If bigram b
is present in si, we let si,b be the number of doc-
uments in the corpus that contain bigram b, and
zero otherwise. We stack these vectors in columns
to produce a matrix S ∈ RN×B, where N is the
number of sentences in the corpus and B is the
number of bigrams. We then perform singular
value decomposition (SVD) on S = UEV&gt;. We
use UK ∈ RN×K as the sentence representations,
where K is a parameter that specifies the number
of latent dimensions. Instead of performing SVD,
we can also take si ∈ RB as our sentence repre-
sentation, which makes our method resemble the
bigram coverage-based summarization approach.
However, this makes si a very sparse vector. Pro-
jecting to a lower dimensional space makes sense
to allow the representation to incorporate informa-
tion from (bigram) cooccurrences and share infor-
mation across bigrams.
</bodyText>
<sectionHeader confidence="0.986057" genericHeader="method">
3 Volume Maximization
</sectionHeader>
<bodyText confidence="0.99195075">
Given the semantic coverage scoring function in
§2.3, our optimization problem is:
max score(S) = Volume(Q(S))
with respect to S
subject to length(S) ≤ L
For computational considerations, we propose to
use a greedy algorithm that approximates the so-
lution by iteratively adding a sentence that max-
imizes the current semantic coverage, given that
the length constraint is still satisfied. The main
steps in our algorithm are as follows. We first
find the sentence that is farthest from the cluster
centroid and add it to S. Next, we find the sen-
tence that is farthest from the first sentence and
add it to S. Given a set of already selected sen-
tences, we choose the next one by finding the sen-
tence farthest from the subspace spanned by sen-
tences already in the set. We repeat this process
until we have gone through all sentences, break-
ing ties arbitrarily and checking whether adding
a sentence to S will result in a violation of the
length constraint. This method is summarized in
Algorithm 1. We note that related variants of our
method for maximizing volume have appeared in
Algorithm 1 Greedy algorithm for approximately
maximizing the semantic volume given a budget
constraint.
Input: Budget constraint L, sentence representa-
</bodyText>
<equation confidence="0.986233666666667">
tions R = {u1, u2, ... , uN}
S = {},B = {}
�N
Compute the cluster centroid c: 1 i�1 ui.
N
p ← index of sentence that is farthest from c.
S = S ∪ {sp}. No. add first sentence
q ← index of sentence that is farthest from sp.
S = S ∪ {sq}.
</equation>
<bodyText confidence="0.97343280952381">
other applications, such as remote sensing (Nasci-
mento and Dias, 2005; Gomez et al., 2007) and
topic modeling (Arora et al., 2012; Arora et al.,
2013).
Computing Distance to a Subspace Our algo-
rithm involves finding a point farthest from a sub-
space (except for the first and second sentences,
which can be selected by computing pointwise dis-
tances). In order for this algorithm to be efficient,
we need this operation to be fast, since it is ex-
ecuted frequently. There are several established
methods to compute the distance between a point
to a subspace spanned by sentences in S. For com-
pleteness, we describe one method based on the
Gram-Schmidt process (Laplace, 1812) here.
We maintain a set of basis vectors, denoted by
B. Our first basis vector consists of one element:
b0 =uq
kuqk, where q is the second sentence chosen
above. Next, we project each candidate sentence i
to this basis vector:
</bodyText>
<equation confidence="0.680147">
Projb0(ui) = (u&gt;i b0)b0,
</equation>
<bodyText confidence="0.8046945">
and find the distance by computing
Distance(ui, B) = kui − Projb0(ui)k. Once we
find the farthest sentence r, we add a new basis
vector B = B ∪ {br}, where br = ur
kurk and
repeat this process. When there are more than one
</bodyText>
<figure confidence="0.979589714285715">
► add second sentence
b0 = uq ,B = B ∪ {u0}
kuqk
total length = length(sp) + length(sq)
for i = 1,...,N − 2 do
r ← index of sentence that is farthest from
the subspace of Span(B). ► see text
if total length + length(sr) ≤ L then
S = S ∪ {sr}.
br = ur ,B = B ∪ {br}.
kurk
total length = total length + length(sr)
end if
end for
</figure>
<page confidence="0.816354">
1963
</page>
<bodyText confidence="0.849338">
basis vectors, we find the distance by computing:
Distance(ui, B) =
</bodyText>
<sectionHeader confidence="0.968047" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.982917">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999983148148148">
We evaluate our proposed method on the non-
update portion of TAC-2008 and TAC-2009. The
datasets contain 48 and 44 multi-document sum-
marization problems, respectively. Each problem
has 10 news articles as input; each is to be sum-
marized in a maximum of L = 100 words. There
are 4 human reference summaries for each prob-
lem, against which an automatically generated
summary is compared. We compare our method
with two baselines: Maximal Marginal Relevance
(MMR, §2.1) and the coverage-based summariza-
tion method (CBS, §2.2). ROUGE (Lin, 2004) is
used to evaluate the summarization results.
For preprocessing, we tokenize, stem with the
Porter (1980) stemmer, and split documents into
sentences. We remove bigrams consisting of only
stopwords and bigrams which appear in less than
3 sentences. As a result, we have 2,746 and 3,273
bigrams for the TAC-2008 and TAC-2009 datasets
respectively. Unlabeled data can help generate
better sentence representations. For each sum-
marization problem in each dataset, we use other
problems in the same dataset as unlabeled data.
We concatenate every problem in each dataset and
perform SVD on this matrix (§2.3). Note that this
also means we only need to do one SVD for each
dataset.
</bodyText>
<subsectionHeader confidence="0.573625">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999610705882353">
Table 1 shows results on the TAC-2008 and TAC-
2009 datasets. We report results for our method
with K = 500 (Volume 500), and K = 600 (Vol-
ume 600). We also include results for an oracle
model that has access to the human reference sum-
maries and extracts sentences that maximize bi-
gram recall as an upper bound. Similar to previous
findings, CBS is generally better than MMR. Our
method outperforms other competing methods, al-
though the optimal value of K is different in each
dataset. The improvements with our proposed ap-
proach are small in terms of R-2. This is likely
because the R-2 score computes bigram overlaps,
and the CBS method that directly maximizes bi-
gram coverage is already a resonable approach to
optimizing this metric (although still worse than
the best of our methods).
</bodyText>
<table confidence="0.995036857142857">
Methods TAC-2008 TAC-2009
R-1 R-2 R-1 R-2
MMR 34.08 9.30 31.87 7.99
CBS 35.83 9.43 32.70 8.84
Volume 500 37.40 9.17 34.08 8.91
Volume 600 37.50 9.58 34.37 8.76
Oracle 46.06 19.33 46.77 16.99
</table>
<tableCaption confidence="0.809629333333333">
Table 1: Results on the TAC-2008 and TAC-2009
datasets. “Volume” refers to our method, shown
with two embedding sizes.
</tableCaption>
<figure confidence="0.9444305">
0 200 400 600 800 1000
number of dimensions
</figure>
<figureCaption confidence="0.9975925">
Figure 2: R-SU4 scores as we vary the number of
dimensions (K) on the TAC-2008 datasets.
</figureCaption>
<sectionHeader confidence="0.995121" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.992014416666667">
Runtime comparisons In terms of inference
running time, all methods perform reasonably fast.
MMR is the slowest, on average it takes 0.38 sec-
onds per problem, followed by our method at 0.17
seconds per problem, and CBS at 0.15 seconds
per problem. However, our implementations of
MMR and Algorithm 1 are in Python, whereas we
use an optimzed solver from Gurobi for our CBS
baseline. For preprocessing, our method is the
slowest, since we need to compute sentence em-
beddings using SVD. There are about 10,000 sen-
tences and 3,000 bigrams for each dataset. SVD
takes approximately 2.5 minutes (150 seconds) us-
ing Matlab on our 12-core machine with 24GB
RAM. Our method introduces another hyperpa-
rameter, the number of latent dimensions K for
sentence embeddings. We observe that the optimal
value depends on the dataset, although a value in
the range of 400 to 800 seems best. Figure 2 shows
R-SU4 scores on the TAC-2008 dataset as we vary
K.
Other sentence projection methods We use
SVD in this study for computing sentence embed-
dings. As mentioned previously, our summariza-
</bodyText>
<figure confidence="0.994901714285714">
R-SU4
11.0 12.0 13.0
Our method
CBS
MMR
� � � � � � �uz − Projbj(ui) � � � � � � .
II bj∈B
</figure>
<page confidence="0.991429">
1964
</page>
<bodyText confidence="0.999985436363636">
tion approach can benefit from advances in neural-
network-based sentence representations (Jenatton
et al., 2011; Le and Mikolov, 2014; Kalchbrenner
et al., 2014). These models can also produce vec-
tor representations of sentences, so Algorithm 1
can be readily applied to the learned representa-
tions. Our work opens up a possibility to make
summarization a future benchmark task for evalu-
ating the quality of sentence representations.
Our method is related to determinantal point
processes (DPPs; Gillenwater et al., 2012; Kulesza
and Taskar, 2012) in that they both seek to maxi-
mize the volume spanned by sentence vectors to
produce a summary. In DPP-based approaches,
quality and selectional diversity correspond to
vector magnitude and angle respectively. In this
work, the length of a sentence vector is not tai-
lored to encode quality in terms of representative-
ness directly. In contrast, we rely on sentence em-
bedding methods to produce a semantic space and
assume that a good summary should have a large
volume in the semantic space. We show that a sim-
ple singular value decomposition embedding of
sentences—one that is not especially tuned for this
task—produces reasonably good results. We leave
exploration of other sentence embedding methods
to future work.
Future work Our method could be extended for
compressive summarization, by simply including
compressed sentences in the embedded space and
running Algorithm 1 without any change. This re-
sembles the summarization methods that jointly
extracts and compresses (Berg-Kirkpatrick et al.,
2011; Woodsend and Lapata, 2012; Almeida and
Martins, 2013). Another alternative is a pipeline
approach, where extractive summarization is fol-
lowed or preceded by a sentence compression
module, which can be built and tuned indepen-
dent of our proposed extractive method (Knight
and Marcu, 2000; Lin, 2003; Zajic et al., 2007;
Wang et al., 2013; Li et al., 2013).
We are also interested in exploring volume as
a relevance function within MMR. MMR avoids
redundancy by penalizing redundant sentences,
whereas in our method semantic redundancy is
inherently discouraged since the method chooses
sentences to maximize volume. Depending on
the method used to embed sentences, this might
not translate directly into avoiding n-gram redun-
dancy. Plugging our scoring function to an MMR
objective is a simple way to enforce diversity.
Finally, an interesting future direction is find-
ing an exact tractable solution to the volume max-
imization problem (or demonstrating that one does
not exist).
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999899571428571">
We introduced a summarization approach based
on maximizing volume in a semantic vector space.
We showed an algorithm to efficiently perform
volume maximization in this semantic space. We
demonstrated that our method outperforms exist-
ing state-of-the-art extractive methods on bench-
mark summarization datasets.
</bodyText>
<sectionHeader confidence="0.998369" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999793833333333">
We thank anonymous reviewers for helpful sug-
gestions. This work was supported by the Defense
Advanced Research Projects Agency through
grant FA87501420244 and by NSF grant SaTC-
1330596. This work was completed while the au-
thors were at CMU.
</bodyText>
<sectionHeader confidence="0.994071" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989433413793103">
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In
Proc. of ACL.
Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur
Moitra. 2012. Computing a nonnegative matrix fac-
torization – provably. In Proc. of STOC.
Sanjeev Arora, Rong Ge, Yoni Halpren, David Mimno,
Ankur Moitra, David Sontag, Yichen Wu, and
Michael Zu. 2013. A practical algorithm for topic
modeling with provable guarantees. In Proc. of
ICML.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of ACL.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proc. of EMNLP-CoNLL.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008. The ICSI summarization system at TAC 2008.
In Proc. of TAC.
C. Gomez, H. Le Borgne, P. Allemand, C. Delacourt,
and P. Ledru. 2007. N-findr method versus indepen-
dent component analysis for lithological identifica-
tion in hyperspectral imagery. International Journal
of Remote Sensing, 28(23):5315–5338.
</reference>
<page confidence="0.870427">
1965
</page>
<reference confidence="0.999622596774194">
Rodolphe Jenatton, Julien Mairal, Gullaume Obozin-
ski, and Francis Bach. 2011. Proximal methods
for hierarchical sparse coding. Journal of Machine
Learning Research, 12:2297–2334.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proc. of ACL.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization – step one: Sentence compres-
sion. In Proc. of AAAI.
Alex Kulesza and Ben Taskar. 2012. Determinantal
point processes for machine learning. Foundations
and Trends in Machine Learning, 5(2–3):123–286.
Pierre-Simon Laplace. 1812. Theorie analytique des
probabilites. Courcier, Paris.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proc. of
ICML.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proc. of EMNLP.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proc. of NAACL-HLT.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression – A pilot study.
In Proc. of Workshop on Information Retrieval with
Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proc. of the ACL Work-
shop on Text Summarization Branches Out.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proc. of the ACL Workshop on
Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proc. of ECIR.
Jose M. P. Nascimento and Jose M. Bioucas Dias.
2005. Vertex component analysis: A fast algorithm
to unmix hyperspectral data. Proc. ofIEEE Transac-
tion on Geoscience and Remote Sensing, 43(4):898–
910.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Lu Wang, Hema Raghavan Vittorio Castelli Radu Flo-
rian, and Claire Cardie. 2013. A sentence compres-
sion based framework to query-focused multidocu-
ment summarization. In Proc. of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proc. of EMNLP.
Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proc. of IJCAI.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing and Manage-
ment, 43(6):1549–1570.
</reference>
<page confidence="0.994711">
1966
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828325">
<title confidence="0.999507">Extractive Summarization by Maximizing Semantic Volume</title>
<author confidence="0.999994">Dani Yogatama</author>
<affiliation confidence="0.9999195">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.99895">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999682">dyogatama@cs.cmu.edu</email>
<author confidence="0.963605">Fei</author>
<affiliation confidence="0.998459">Electrical Engineering &amp; Computer University of Central</affiliation>
<address confidence="0.919562">Orlando, FL 32816,</address>
<email confidence="0.999873">feiliu@cs.ucf.edu</email>
<author confidence="0.961435">A Noah</author>
<affiliation confidence="0.999819">Computer Science &amp; University of</affiliation>
<address confidence="0.998572">Seattle, WA 98195,</address>
<email confidence="0.99957">nasmith@cs.washington.edu</email>
<abstract confidence="0.998617333333333">The most successful approaches to extractive text summarization seek to maximize bigram coverage subject to a budget constraint. In this work, we propose instead to maximize semantic volume. We embed each sentence in a semantic space and construct a summary by choosing a subset of sentences whose convex hull maximizes volume in that space. We provide a greedy algorithm based on the Gram- Schmidt process to efficiently perform volume maximization. Our method outperforms the state-of-the-art summarization approaches on benchmark datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel B Almeida</author>
<author>Andre F T Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1815" citStr="Almeida and Martins, 2013" startWordPosition="259" endWordPosition="262">P might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (</context>
<context position="6235" citStr="Almeida and Martins, 2013" startWordPosition="1001" endWordPosition="1004">s four sentences. Our scoring function is optimized by chooseing the four sentences in red as the summary, since they maximize the volume (area in two dimensions). second constraint makes sure that selecting a bigram implies selecting at least one of the sentences that contains it. In this formulation, there is no explicit penalty on redundancy. However, insofar as redundant sentences cover fewer bigrams, they are implicitly discouraged. Although the above scoring function also results in an NP-hard problem, an off-the-shelf ILP solver (Gillick et al., 2008) or a dual decomposition algorithm (Almeida and Martins, 2013) can be used to solve it in practice. 2.3 Semantic Volume We introduce a new scoring function for summarization. The main idea is based on the notion of coverage, but in a distributed semantic space: a good summary should have broad semantic coverage with respect to document contents. For every sentence si, i E [N], we denote its continuous semantic representation in a K-dimensional semantic space by Q(si) = ui E RK, where Q is a function that takes a sentence and returns its semantic vector representation. We denote embeddings of all sentences in D with the function Q by Q(D). We will return </context>
<context position="16831" citStr="Almeida and Martins, 2013" startWordPosition="2850" endWordPosition="2853">mmary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one that is not especially tuned for this task—produces reasonably good results. We leave exploration of other sentence embedding methods to future work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed </context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel B. Almeida and Andre F. T. Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Rong Ge</author>
<author>Ravi Kannan</author>
<author>Ankur Moitra</author>
</authors>
<title>Computing a nonnegative matrix factorization – provably.</title>
<date>2012</date>
<booktitle>In Proc. of STOC.</booktitle>
<contexts>
<context position="10138" citStr="Arora et al., 2012" startWordPosition="1702" endWordPosition="1705">d in Algorithm 1. We note that related variants of our method for maximizing volume have appeared in Algorithm 1 Greedy algorithm for approximately maximizing the semantic volume given a budget constraint. Input: Budget constraint L, sentence representations R = {u1, u2, ... , uN} S = {},B = {} �N Compute the cluster centroid c: 1 i�1 ui. N p ← index of sentence that is farthest from c. S = S ∪ {sp}. No. add first sentence q ← index of sentence that is farthest from sp. S = S ∪ {sq}. other applications, such as remote sensing (Nascimento and Dias, 2005; Gomez et al., 2007) and topic modeling (Arora et al., 2012; Arora et al., 2013). Computing Distance to a Subspace Our algorithm involves finding a point farthest from a subspace (except for the first and second sentences, which can be selected by computing pointwise distances). In order for this algorithm to be efficient, we need this operation to be fast, since it is executed frequently. There are several established methods to compute the distance between a point to a subspace spanned by sentences in S. For completeness, we describe one method based on the Gram-Schmidt process (Laplace, 1812) here. We maintain a set of basis vectors, denoted by B. </context>
</contexts>
<marker>Arora, Ge, Kannan, Moitra, 2012</marker>
<rawString>Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonnegative matrix factorization – provably. In Proc. of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Rong Ge</author>
<author>Yoni Halpren</author>
<author>David Mimno</author>
<author>Ankur Moitra</author>
<author>David Sontag</author>
<author>Yichen Wu</author>
<author>Michael Zu</author>
</authors>
<title>A practical algorithm for topic modeling with provable guarantees.</title>
<date>2013</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="10159" citStr="Arora et al., 2013" startWordPosition="1706" endWordPosition="1709"> note that related variants of our method for maximizing volume have appeared in Algorithm 1 Greedy algorithm for approximately maximizing the semantic volume given a budget constraint. Input: Budget constraint L, sentence representations R = {u1, u2, ... , uN} S = {},B = {} �N Compute the cluster centroid c: 1 i�1 ui. N p ← index of sentence that is farthest from c. S = S ∪ {sp}. No. add first sentence q ← index of sentence that is farthest from sp. S = S ∪ {sq}. other applications, such as remote sensing (Nascimento and Dias, 2005; Gomez et al., 2007) and topic modeling (Arora et al., 2012; Arora et al., 2013). Computing Distance to a Subspace Our algorithm involves finding a point farthest from a subspace (except for the first and second sentences, which can be selected by computing pointwise distances). In order for this algorithm to be efficient, we need this operation to be fast, since it is executed frequently. There are several established methods to compute the distance between a point to a subspace spanned by sentences in S. For completeness, we describe one method based on the Gram-Schmidt process (Laplace, 1812) here. We maintain a set of basis vectors, denoted by B. Our first basis vecto</context>
</contexts>
<marker>Arora, Ge, Halpren, Mimno, Moitra, Sontag, Wu, Zu, 2013</marker>
<rawString>Sanjeev Arora, Rong Ge, Yoni Halpren, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zu. 2013. A practical algorithm for topic modeling with provable guarantees. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1923" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="274" endWordPosition="277">mmarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formali</context>
<context position="16776" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="2842" endWordPosition="2845">hods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one that is not especially tuned for this task—produces reasonably good results. We leave exploration of other sentence embedding methods to future work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to </context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="1573" citStr="Carbonell and Goldstein, 1998" startWordPosition="221" endWordPosition="225">hmark datasets. 1 Introduction In artificial intelligence, changes in representation sometimes suggest new algorithms. For example, increased attention to distributed meaning representations suggests that existing combinatorial algorithms for NLP might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach </context>
<context position="3964" citStr="Carbonell and Goldstein, 1998" startWordPosition="611" endWordPosition="614">as an optimization problem: max score(S) = score(D, y) with respect to S equivalently y subject to length(S) G L 1961 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1961–1966, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. with a scoring function score(D, y). A good scoring function should assign higher scores to better summaries. In the following, we describe two commonly used scoring functions and our proposed scoring function. 2.1 Maximal Marginal Relevance The Maximal Marginal Relevance (MMR) method (Carbonell and Goldstein, 1998) considers the following scoring function: N N score(D, y) = yiRel(si) − yiyjSim(si, sj) i=1 i,j=1 where Rel(si) measures the relevancy of sentence i and Sim(si, sj) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Discovering diverse and salient threads in document collections.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="15742" citStr="Gillenwater et al., 2012" startWordPosition="2680" endWordPosition="2683">summarizaR-SU4 11.0 12.0 13.0 Our method CBS MMR � � � � � � �uz − Projbj(ui) � � � � � � . II bj∈B 1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tailored to encode quality in terms of representativeness directly. In contrast, we rely on sentence embedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one tha</context>
</contexts>
<marker>Gillenwater, Kulesza, Taskar, 2012</marker>
<rawString>Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. 2012. Discovering diverse and salient threads in document collections. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>The ICSI summarization system at TAC</title>
<date>2008</date>
<booktitle>In Proc. of TAC.</booktitle>
<contexts>
<context position="1664" citStr="Gillick et al., 2008" startWordPosition="237" endWordPosition="240">est new algorithms. For example, increased attention to distributed meaning representations suggests that existing combinatorial algorithms for NLP might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a di</context>
<context position="4748" citStr="Gillick et al. (2008)" startWordPosition="740" endWordPosition="743">j) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence si E D. Denote the number of unique bigrams extracted from all sentences by B. We introduce another binary vector z E RB to indicate the presence or absence of a bigram in the summary, and a binary indicator matrix M E RN×B, where mi,j is 1 if and only if bigram j is present in sentence i and 0 otherwise. The scoring function is: B score(D, y, z) = bjzj j=1 and the two additional constraints are: Vj E [B], Vi E [N] yimi,j G zj N Vj E [B] yimi,j ? zj i=1 where we use [B] as a shorthand for 11, </context>
<context position="6173" citStr="Gillick et al., 2008" startWordPosition="992" endWordPosition="995">pace. Consider the case when the maximum summary length is four sentences. Our scoring function is optimized by chooseing the four sentences in red as the summary, since they maximize the volume (area in two dimensions). second constraint makes sure that selecting a bigram implies selecting at least one of the sentences that contains it. In this formulation, there is no explicit penalty on redundancy. However, insofar as redundant sentences cover fewer bigrams, they are implicitly discouraged. Although the above scoring function also results in an NP-hard problem, an off-the-shelf ILP solver (Gillick et al., 2008) or a dual decomposition algorithm (Almeida and Martins, 2013) can be used to solve it in practice. 2.3 Semantic Volume We introduce a new scoring function for summarization. The main idea is based on the notion of coverage, but in a distributed semantic space: a good summary should have broad semantic coverage with respect to document contents. For every sentence si, i E [N], we denote its continuous semantic representation in a K-dimensional semantic space by Q(si) = ui E RK, where Q is a function that takes a sentence and returns its semantic vector representation. We denote embeddings of a</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008. The ICSI summarization system at TAC 2008. In Proc. of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gomez</author>
<author>H Le Borgne</author>
<author>P Allemand</author>
<author>C Delacourt</author>
<author>P Ledru</author>
</authors>
<title>N-findr method versus independent component analysis for lithological identification in hyperspectral imagery.</title>
<date>2007</date>
<journal>International Journal of Remote Sensing,</journal>
<volume>28</volume>
<issue>23</issue>
<marker>Gomez, Le Borgne, Allemand, Delacourt, Ledru, 2007</marker>
<rawString>C. Gomez, H. Le Borgne, P. Allemand, C. Delacourt, and P. Ledru. 2007. N-findr method versus independent component analysis for lithological identification in hyperspectral imagery. International Journal of Remote Sensing, 28(23):5315–5338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodolphe Jenatton</author>
<author>Julien Mairal</author>
<author>Gullaume Obozinski</author>
<author>Francis Bach</author>
</authors>
<title>Proximal methods for hierarchical sparse coding.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2297</pages>
<contexts>
<context position="2196" citStr="Jenatton et al., 2011" startWordPosition="311" endWordPosition="315"> McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (§2), and we provide a fast greedy algorithm that can be used to maximize it (§3). We show that our method outperforms competing extractive baselines under similar experimental conditions on benchmark s</context>
<context position="15333" citStr="Jenatton et al., 2011" startWordPosition="2616" endWordPosition="2619">r method introduces another hyperparameter, the number of latent dimensions K for sentence embeddings. We observe that the optimal value depends on the dataset, although a value in the range of 400 to 800 seems best. Figure 2 shows R-SU4 scores on the TAC-2008 dataset as we vary K. Other sentence projection methods We use SVD in this study for computing sentence embeddings. As mentioned previously, our summarizaR-SU4 11.0 12.0 13.0 Our method CBS MMR � � � � � � �uz − Projbj(ui) � � � � � � . II bj∈B 1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspo</context>
</contexts>
<marker>Jenatton, Mairal, Obozinski, Bach, 2011</marker>
<rawString>Rodolphe Jenatton, Julien Mairal, Gullaume Obozinski, and Francis Bach. 2011. Proximal methods for hierarchical sparse coding. Journal of Machine Learning Research, 12:2297–2334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2136" citStr="Kalchbrenner et al., 2014" startWordPosition="302" endWordPosition="305">igh relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (§2), and we provide a fast greedy algorithm that can be used to maximize it (§3). We show that our method outperforms competing extractive ba</context>
<context position="15383" citStr="Kalchbrenner et al., 2014" startWordPosition="2624" endWordPosition="2627">he number of latent dimensions K for sentence embeddings. We observe that the optimal value depends on the dataset, although a value in the range of 400 to 800 seems best. Figure 2 shows R-SU4 scores on the TAC-2008 dataset as we vary K. Other sentence projection methods We use SVD in this study for computing sentence embeddings. As mentioned previously, our summarizaR-SU4 11.0 12.0 13.0 Our method CBS MMR � � � � � � �uz − Projbj(ui) � � � � � � . II bj∈B 1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization – step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="17064" citStr="Knight and Marcu, 2000" startWordPosition="2886" endWordPosition="2889">her sentence embedding methods to future work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Finally, an interesting future direction is finding an exact tracta</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization – step one: Sentence compression. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Determinantal point processes for machine learning. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="15769" citStr="Kulesza and Taskar, 2012" startWordPosition="2684" endWordPosition="2687">3.0 Our method CBS MMR � � � � � � �uz − Projbj(ui) � � � � � � . II bj∈B 1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tailored to encode quality in terms of representativeness directly. In contrast, we rely on sentence embedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one that is not especially tuned f</context>
</contexts>
<marker>Kulesza, Taskar, 2012</marker>
<rawString>Alex Kulesza and Ben Taskar. 2012. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(2–3):123–286.</rawString>
</citation>
<citation valid="false">
<title>Theorie analytique des probabilites. Courcier,</title>
<location>Paris.</location>
<marker></marker>
<rawString>Pierre-Simon Laplace. 1812. Theorie analytique des probabilites. Courcier, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="2063" citStr="Mikolov, 2014" startWordPosition="295" endWordPosition="296">rom the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (§2), and we provide a fast greedy algorithm that can be used to maxi</context>
<context position="15355" citStr="Mikolov, 2014" startWordPosition="2622" endWordPosition="2623">perparameter, the number of latent dimensions K for sentence embeddings. We observe that the optimal value depends on the dataset, although a value in the range of 400 to 800 seems best. Figure 2 shows R-SU4 scores on the TAC-2008 dataset as we vary K. Other sentence projection methods We use SVD in this study for computing sentence embeddings. As mentioned previously, our summarizaR-SU4 11.0 12.0 13.0 Our method CBS MMR � � � � � � �uz − Projbj(ui) � � � � � � . II bj∈B 1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Yang Liu</author>
</authors>
<title>Document summarization via guided sentence compression.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="17132" citStr="Li et al., 2013" startWordPosition="2900" endWordPosition="2903">d be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Finally, an interesting future direction is finding an exact tractable solution to the volume maximization problem (or demonstrating th</context>
</contexts>
<marker>Li, Liu, Weng, Liu, 2013</marker>
<rawString>Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="4555" citStr="Lin and Bilmes, 2010" startWordPosition="711" endWordPosition="714">Carbonell and Goldstein, 1998) considers the following scoring function: N N score(D, y) = yiRel(si) − yiyjSim(si, sj) i=1 i,j=1 where Rel(si) measures the relevancy of sentence i and Sim(si, sj) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence si E D. Denote the number of unique bigrams extracted from all sentences by B. We introduce another binary vector z E RB to indicate the presence or absence of a bigram in the summary, and a binary indicator matrix M E RN×B, where mi,j is 1 if and only if bigram j is present in sentence i and 0 otherwise</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression – A pilot study.</title>
<date>2003</date>
<booktitle>In Proc. of Workshop on Information Retrieval with Asian Language.</booktitle>
<contexts>
<context position="17075" citStr="Lin, 2003" startWordPosition="2890" endWordPosition="2891">ethods to future work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Finally, an interesting future direction is finding an exact tractable solutio</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression – A pilot study. In Proc. of Workshop on Information Retrieval with Asian Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL Workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="12121" citStr="Lin, 2004" startWordPosition="2065" endWordPosition="2066"> the distance by computing: Distance(ui, B) = 4 Experiments 4.1 Setup We evaluate our proposed method on the nonupdate portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document summarization problems, respectively. Each problem has 10 news articles as input; each is to be summarized in a maximum of L = 100 words. There are 4 human reference summaries for each problem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR, §2.1) and the coverage-based summarization method (CBS, §2.2). ROUGE (Lin, 2004) is used to evaluate the summarization results. For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each summarization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proc. of the ACL Workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proc. of the ACL Workshop on Integer Linear Programming for Natural Language Processing.</booktitle>
<contexts>
<context position="1891" citStr="Martins and Smith, 2009" startWordPosition="270" endWordPosition="273">this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideall</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andre F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proc. of the ACL Workshop on Integer Linear Programming for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proc. of ECIR.</booktitle>
<contexts>
<context position="1590" citStr="McDonald, 2007" startWordPosition="226" endWordPosition="227">In artificial intelligence, changes in representation sometimes suggest new algorithms. For example, increased attention to distributed meaning representations suggests that existing combinatorial algorithms for NLP might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al.,</context>
<context position="4411" citStr="McDonald, 2007" startWordPosition="687" endWordPosition="689">nly used scoring functions and our proposed scoring function. 2.1 Maximal Marginal Relevance The Maximal Marginal Relevance (MMR) method (Carbonell and Goldstein, 1998) considers the following scoring function: N N score(D, y) = yiRel(si) − yiyjSim(si, sj) i=1 i,j=1 where Rel(si) measures the relevancy of sentence i and Sim(si, sj) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence si E D. Denote the number of unique bigrams extracted from all sentences by B. We introduce another binary vector z E RB to indicate the presence or absence of a</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proc. of ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose M P Nascimento</author>
<author>Jose M Bioucas Dias</author>
</authors>
<title>Vertex component analysis: A fast algorithm to unmix hyperspectral data.</title>
<date>2005</date>
<booktitle>Proc. ofIEEE Transaction on Geoscience and Remote Sensing,</booktitle>
<volume>43</volume>
<issue>4</issue>
<pages>910</pages>
<contexts>
<context position="10078" citStr="Nascimento and Dias, 2005" startWordPosition="1690" endWordPosition="1694">t in a violation of the length constraint. This method is summarized in Algorithm 1. We note that related variants of our method for maximizing volume have appeared in Algorithm 1 Greedy algorithm for approximately maximizing the semantic volume given a budget constraint. Input: Budget constraint L, sentence representations R = {u1, u2, ... , uN} S = {},B = {} �N Compute the cluster centroid c: 1 i�1 ui. N p ← index of sentence that is farthest from c. S = S ∪ {sp}. No. add first sentence q ← index of sentence that is farthest from sp. S = S ∪ {sq}. other applications, such as remote sensing (Nascimento and Dias, 2005; Gomez et al., 2007) and topic modeling (Arora et al., 2012; Arora et al., 2013). Computing Distance to a Subspace Our algorithm involves finding a point farthest from a subspace (except for the first and second sentences, which can be selected by computing pointwise distances). In order for this algorithm to be efficient, we need this operation to be fast, since it is executed frequently. There are several established methods to compute the distance between a point to a subspace spanned by sentences in S. For completeness, we describe one method based on the Gram-Schmidt process (Laplace, 18</context>
</contexts>
<marker>Nascimento, Dias, 2005</marker>
<rawString>Jose M. P. Nascimento and Jose M. Bioucas Dias. 2005. Vertex component analysis: A fast algorithm to unmix hyperspectral data. Proc. ofIEEE Transaction on Geoscience and Remote Sensing, 43(4):898– 910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="12228" citStr="Porter (1980)" startWordPosition="2081" endWordPosition="2082"> the nonupdate portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document summarization problems, respectively. Each problem has 10 news articles as input; each is to be summarized in a maximum of L = 100 words. There are 4 human reference summaries for each problem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR, §2.1) and the coverage-based summarization method (CBS, §2.2). ROUGE (Lin, 2004) is used to evaluate the summarization results. For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each summarization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix (§2.3). Note that this also means we only need to do one SVD for each dataset. 4.2 Results Table 1 shows re</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Hema Raghavan Vittorio Castelli Radu Florian</author>
<author>Claire Cardie</author>
</authors>
<title>A sentence compression based framework to query-focused multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="17114" citStr="Wang et al., 2013" startWordPosition="2896" endWordPosition="2899">ork Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Finally, an interesting future direction is finding an exact tractable solution to the volume maximization problem (o</context>
</contexts>
<marker>Wang, Florian, Cardie, 2013</marker>
<rawString>Lu Wang, Hema Raghavan Vittorio Castelli Radu Florian, and Claire Cardie. 2013. A sentence compression based framework to query-focused multidocument summarization. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="16803" citStr="Woodsend and Lapata, 2012" startWordPosition="2846" endWordPosition="2849">e and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one that is not especially tuned for this task—produces reasonably good results. We leave exploration of other sentence embedding methods to future work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending </context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="1641" citStr="Yih et al., 2007" startWordPosition="233" endWordPosition="236">ion sometimes suggest new algorithms. For example, increased attention to distributed meaning representations suggests that existing combinatorial algorithms for NLP might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensi</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="17095" citStr="Zajic et al., 2007" startWordPosition="2892" endWordPosition="2895">uture work. Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Finally, an interesting future direction is finding an exact tractable solution to the volume maxi</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing and Management, 43(6):1549–1570.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
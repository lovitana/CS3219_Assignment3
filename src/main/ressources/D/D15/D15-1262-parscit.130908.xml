<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.999179">
Comparing Word Representations for
Implicit Discourse Relation Classification
</title>
<author confidence="0.743755">
Chlo´e Braud
</author>
<affiliation confidence="0.516899">
ALPAGE, Univ Paris Diderot
</affiliation>
<address confidence="0.370146">
&amp; INRIA Paris-Rocquencourt
75013 Paris - France
</address>
<email confidence="0.997256">
chloe.braud@inria.fr
</email>
<sectionHeader confidence="0.99385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829888888889">
This paper presents a detailed compar-
ative framework for assessing the use-
fulness of unsupervised word representa-
tions for identifying so-called implicit dis-
course relations. Specifically, we compare
standard one-hot word pair representations
against low-dimensional ones based on
Brown clusters and word embeddings. We
also consider various word vector combi-
nation schemes for deriving discourse seg-
ment representations from word vectors,
and compare representations based either
on all words or limited to head words.
Our main finding is that denser represen-
tations systematically outperform sparser
ones and give state-of-the-art performance
or above without the need for additional
hand-crafted features.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999634928571428">
Identifying discourse relations is an important
task, either to build a discourse parser or to help
other NLP systems such as text summarization
or question-answering. This task is relatively
straightforward when a discourse connective, such
as but or because, is used (Pitler and Nenkova,
2009). The identification becomes much more
challenging when such an overt marker is lacking,
and the relation needs to be inferred through other
means. In (1), the presence of the pair of verbs
(rose,tumbled) triggers a Contrast relation. Such
relations are extremely pervasive in real text cor-
pora: they account for about 50% of all relations in
the Penn Discourse Treebank (Prasad et al., 2008).
</bodyText>
<footnote confidence="0.602919">
(1) [ Quarterly revenue rose 4.5%, to $2.3 billion
from $2.2 billion]arg1 [ For the year, net in-
come tumbled 61% to $86 million, or $1.55
a share]arg2
</footnote>
<note confidence="0.963806666666667">
Pascal Denis
MAGNET, INRIA Lille Nord-Europe
59650 Villeneuve d’Ascq - France
</note>
<email confidence="0.841431">
pascal.denis@inria.fr
</email>
<bodyText confidence="0.999949341463415">
Automatically classifying implicit relations is dif-
ficult in large part because it relies on numerous
factors, ranging from syntax, and tense and as-
pect, to lexical semantics and even world knowl-
edge (Asher and Lascarides, 2003). Consequently,
a lot of previous work on this problem have at-
tempted to incorporate some of these information
into their systems. These assume the existence
of syntactic parsers and lexical databases of var-
ious kinds, which are available but for a few lan-
guages, and they often involve heavy feature en-
gineering (Pitler et al., 2009; Park and Cardie,
2012). While acknowledging this knowledge bot-
tleneck, this paper focuses on trying to predict im-
plicit relations based on easily accessible lexical
features, targeting in particular simple word-based
features, such as pairs like (rose,tumbled) in (1).
Most previous studies on implicit relations, go-
ing back to (Marcu and Echihabi, 2002), in-
corporate word-based information in the form
of word pair features defined across the pair
of text segments to be related. Such word
pairs are often encoded in a one-hot represen-
tation, in which each possible word pair corre-
sponds to a single component of a very high-
dimensional vector. From a machine learning
perspective, this type of sparse representation
makes parameter estimation extremely difficult
and prone to overfitting. It also makes it diffi-
cult to achieve any interesting semantic general-
ization. To see this, consider the distance (e.g.,
Euclidean or cosine) induced by such representa-
tion. Assuming for simplicity that one character-
izes each pair of discourse segments via their main
verbs, the corresponding one-hot encoding for the
pair (rose,tumbled) would be at equal distance
from the synonymic pair (went up,lost) and the
antonymic pair (went down,gained), as all three
vectors are orthogonal to each others.
Various attempts have been made at reducing
sparsity of lexical features. Recently, Ruther-
</bodyText>
<page confidence="0.907008">
2201
</page>
<note confidence="0.984647">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2201–2211,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998819244444445">
ford and Xue (2014) proposed to use Brown clus-
ters (Brown et al., 1992) for this task, in effect
replacing each token by its cluster binary code.
These authors conclude that these denser, cluster-
derived representations significantly improve the
identification of implicit discourse relations and
report the best performance to date using also
additional features. Unfortunately, their claim
is somewhat weakened by the fact that they fail
to compare the use of their cluster word pairs
against other types of word representations, in-
cluding one-hot encodings of word pairs or other
low-dimensional word representations. This work
also leaves other important questions open. In par-
ticular, it is unclear whether all word pairs con-
structed over the two discourse segments are truly
informative and should be included in the model.
Given that word embeddings capture latent syn-
tactic and semantic information, yet another im-
portant question is to which extent the use of these
representations dispenses us from using additional
hand-crafted syntactic and semantic features.
This paper fills these gaps and significantly ex-
tends the work of (Rutherford and Xue, 2014)
by explicitly comparing various types of word
representations and vector composition meth-
ods. Specifically, we investigate three well-known
word embeddings, namely Collobert and We-
ston (Collobert and Weston, 2008), hierarchical
log-bilinear model (Mnih and Hinton, 2007) and
Hellinger Principal Component Analysis (Lebret
and Collobert, 2014), in addition to Brown cluster-
based and standard one-hot representations. All
these word representations are publicly available
for English and can be easily acquired for other
languages just using raw text data, thus alleviat-
ing the need for hand-crafted lexical databases.
This makes our approach easily extendable to
resource-poor languages. In addition, we also in-
vestigate the issue of which specific words need
to be fed to the model, by comparing using just
pairs of verbs against all pairs of words, and how
word representations should be combined over
discourse segments, comparing component-wise
product against simple vector concatenation.
</bodyText>
<sectionHeader confidence="0.95264" genericHeader="method">
2 Word Representations
</sectionHeader>
<bodyText confidence="0.999931714285714">
A word representation associates a word to a math-
ematical object, typically a high-dimensional vec-
tor in {0,1}|V |or R|V|, where V is a base vocabu-
lary. Each dimension of this vector corresponds to
a feature which might have a syntactic or semantic
interpretation. In the following, we review differ-
ent types of word representations used in NLP.
</bodyText>
<subsectionHeader confidence="0.996641">
2.1 One-hot Word Representations
</subsectionHeader>
<bodyText confidence="0.999902285714286">
Given a particular NLP problem, the crudest and
yet most common type of word representation
consists in mapping each word into a one-hot vec-
tor, wherein each observed word corresponds to a
distinct vector component. More formally, let V
denote the set of all words found in the texts and
w a particular word in V. The one-hot represen-
tation of w is the d-dimensional indicator vector,
noted ✶,,,, such that d = |V|: that is, all of this
vector’s components are 0’s but for one 1 compo-
nent corresponding to the word’s index in V. It
is easy to see that this representation is extremely
sparse, and makes learning difficult as it mechani-
cally blows up the parameter space of the model.
</bodyText>
<subsectionHeader confidence="0.999029">
2.2 Clustering-based Word Representations
</subsectionHeader>
<bodyText confidence="0.999949571428571">
An alternative to these very sparse representations
consists in learning word representations in an un-
supervised fashion using clustering. An example
of this approach are the so-called Brown clusters
induced using the Brown hierarchical clustering
algorithm (Brown et al., 1992) with the goal of
maximizing the mutual information of bigrams.
As a result, each word is associated to a binary
code corresponding to the cluster it belongs to.
Given the hierarchical nature of the algorithm,
one can create word classes of different levels of
granularity, corresponding to bit codes of different
sizes. The less clusters, the less fine-grained the
distinctions between words but the less sparsity.
Note that this kind of representations also yields
one-hot encodings but on a much smaller vocab-
ulary size (i.e., the number of clusters). Brown
clusters have been used for several NLP tasks, in-
cluding NER, chunking (Turian et al., 2010), pars-
ing (Koo et al., 2008) and implicit discourse rela-
tion classification (Rutherford and Xue, 2014).
</bodyText>
<subsectionHeader confidence="0.999644">
2.3 Dense Real-Valued Representations
</subsectionHeader>
<bodyText confidence="0.999628625">
Another approach to induce word representations
from raw text is to learn distributed word represen-
tations (aka word embeddings), which are dense,
low-dimensional, and real-valued vectors. These
are typically learned using neural language models
(Bengio et al., 2003). Each dimension correspond-
ing to a latent feature of the word that captures
paradigmatic information. An example of such
</bodyText>
<page confidence="0.991016">
2202
</page>
<bodyText confidence="0.999932222222222">
embeddings are the so-called Collobert and We-
ston embeddings (Collobert and Weston, 2008).
The embeddings are learned discriminatively by
minimizing a loss between the current n-gram and
a corrupted n-gram whose last word comes from
the same vocabulary but is different from the last
word of the original n-gram. Another example are
the Hierarchical log-bilinear embeddings (Mnih
and Hinton, 2007) induced using a probabilistic
and linear neural model, with a hierarchical prin-
ciple used to speed up the model evaluation. The
embeddings are obtained by concatenating the em-
beddings of the n−1 words of a n-gram and learn-
ing the embedding of the last word.
A final approach is based on the assumption
that words occurring in similar contexts tend to
have similar meanings. Building word distribu-
tional representations is done by computing the
raw cooccurrence frequencies between each word
and the |D |words that serve as context, with D
generally smaller than the overall vocabulary, then
applying some transformation (e.g. TF-IDF). As
|D |is generally too large to form a tractable rep-
resentation, a dimensionality reduction algorithm
is used to end up with p « |D |dimensions.
Like for distributed representations, we end up
with a dense low-dimensional real-valued vector
for each word. A recent example of such approach
is the Hellinger PCA embeddings of (Lebret and
Collobert, 2014) which were built using Principal
Component Analysis based on Hellinger distance
as dimensionality reduction algorithm. An impor-
tant appeal of these representations is that they are
much less time-consuming to train than the ones
based on neural language models while allowing
similar performance (Lebret and Collobert, 2014).
</bodyText>
<sectionHeader confidence="0.974374" genericHeader="method">
3 Segment Pair Representations
</sectionHeader>
<bodyText confidence="0.999967263157895">
We now turn to the issue of combining the word
representations as described in section 2 into com-
posite vectors corresponding to implicit discourse
classification instances. Schematically, the rep-
resentations employed for pairs of discourse seg-
ments differ along three main dimensions. First,
we compare the use of a single word per segment
(roughly, the two main verbs) against that of all the
words contained in the two segments. Second, we
compare the use of sparse (i.e., one-hot) vs. dense
representations for words. As discussed, Brown
cluster bit representations are a special (i.e., low-
dimensional) version of one-hot encoding. Third,
we use two different types of combinations of
word vectors to yield segment vector representa-
tions: concatenation and Kronecker product. The
proposed framework is therefore much more gen-
eral than the one given in previous work such
as (Rutherford and Xue, 2014).
</bodyText>
<subsectionHeader confidence="0.994994">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.999990133333334">
Our classification inputs are pairs of text seg-
ments, the two arguments of the relation to be pre-
dicted. Let S1 = {w11, ... , w1.} denote the n
words that make up the first segment and S2 =
{w21, ... , w2„} the m words in the second seg-
ment. That is, we regard segments as bags of
words. Let V again denote the word vocabulary,
that is the set of all words found in the segments.
Sometimes, we will find it useful to refer to a par-
ticular subset of V. Let head(·) refer to the func-
tion that extracts the head word of segment S,1 and
Vh C V the set of head words. As our goal is to
compare different feature representations, we de-
fine Φ as a generic feature function mapping pairs
of segments to a d-dimensional real vector:
</bodyText>
<equation confidence="0.9996675">
Φ : Vn x V&apos; * Rd
(S1, S2) H Φ(S1, S2)
</equation>
<bodyText confidence="0.999651666666667">
The goal of learning is to acquire for each relation
a linear classification function f,,,(·), parametrized
by w E Rd, mapping Φ(S1, S2) into {−1, +1}.
Recall that 1w refers to the d-dimensional one-
hot encoding for word w E V. Let us also denote
by ® and ® the vector concatenation operator and
the Kronecker product, respectively. Note that do-
ing a Kronecker product on two vectors u E R&apos;
and v E Rn is equivalent to doing the outer prod-
uct uv E R&apos;×n. Finally, the vec(·) operator
converts a m x n matrix into an mn x 1 column
vector by stacking its columns.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Representations based on head words
</subsectionHeader>
<bodyText confidence="0.9999293">
One of the simplest representation one can con-
struct for a pair of segments (S1, S2) is to con-
sider only their head words: h1 = head(S1) and
h2 = head(S2). In this simple scenario, two
main questions that remain are: (i) which vector
representations do we use for h1 and h2, and (ii)
how do we combine these representations. An im-
portant criterion for word vector combination is
that they retain text ordering information between
text segments which really matters for this task.
</bodyText>
<footnote confidence="0.724303">
1Head extraction will be detailed in section 4.1.
</footnote>
<page confidence="0.9501">
2203
</page>
<bodyText confidence="0.9997963">
Thus, inverting the order between two main verbs
(e.g., push and fall) will often lead to distinct dis-
course relation being inferred, as some relations
are asymmetric (e.g., Result or Explanation).
One-hot representations Starting again with
the simplest case, one can use the one-hot encod-
ings corresponding to the two head words, ✶h1 and
✶h2 respectively, and combine them using either
concatenation or product, leading to our two first
feature mappings:
</bodyText>
<equation confidence="0.9974335">
Φh,✶,⊕(S1, S2) = ✶h1 ® ✶h2
Φh,✶,⊗(S1, S2) = vec(✶h1 ® ✶h2)
</equation>
<bodyText confidence="0.999875173913043">
Note that Φh,✶,⊕(S1, S2) lives in 10,1}2|Vh |and
Φh,✶,⊗ in 10,1}|Vh|2. The latter representation
amounts to assigning one 1 component for each
pair of words in Vh x Vh, and is the sparsest
representation one can construct from head words
alone. In some sense, it is also the most expressive
in that we learn one parameter for each word pair,
hence capturing interaction between words across
segments. By contrast, Φh,✶,⊕(S1, S2) doesn’t ex-
plicitly model word interaction across discourse
segments, treating each word in a given segment
(left or right) as a separate dimension.
Dense representations Alternatively, one can
represent head words through their real low-
dimensional embeddings. Let M denote a n x p
real matrix, wherein the ith row corresponds to
the p-dimensional embedding of the ith word of
Vh, with p « jVhj.2 Using this notation, one
can derive the word embeddings of the head words
h1 and h2 from their one-hot representations us-
ing simple matrix multiplication: M&gt;✶h1 and
M&gt;✶h2, respectively. Concatenation and product
yield two new feature mappings, respectively:
</bodyText>
<equation confidence="0.999824">
Φh,M,⊕(S1,S2) = M&gt;✶h1 ® M&gt;✶h2
Φh,M,⊗(S1, S2) = vec(M&gt;✶h1 ® M&gt;✶h2)
</equation>
<bodyText confidence="0.999090666666667">
These new representations live in a much lower
dimensional real spaces: Φh,M,⊕(S1, S2) lives in
R2p and Φh,M,⊗(S1, S2) in Rp2.
</bodyText>
<subsectionHeader confidence="0.99961">
3.3 Representations based on all words
</subsectionHeader>
<bodyText confidence="0.993707470588235">
The various segment-pair representations that we
derived from pairs of head words can be general-
ized to the case in which we keep all the words in
2For now, we assume that n = Vh which is unrealistic.
See section 4.1 for a discussion of unknown words.
each segment. The additional issue in this context
is in the combination of the different word vec-
tor representations within and across the two seg-
ments, and that of normalizing the segment vec-
tors thus obtained. For simplicity, we assume that
the representation for each segment is computed
by summing over the pairs of words vectors com-
posing the segments.
One-hot representations Following this ap-
proach and recalling that S1 contains n words,
while S2 has m words, one can construct one-hot
encodings for segment pairs as follows:
</bodyText>
<equation confidence="0.989263166666667">
n m
Φall,✶,⊕(S1, S2) = ✶w1i ® ✶w2j
i j
n m
Φall,✶,⊗(S1, S2) = vec(✶w1i ® ✶w2j )
i j
</equation>
<bodyText confidence="0.880581307692308">
If used without any type of frequency thresh-
olding, these mappings result in very high-
dimensional feature representations living in Z2|V|
≥0
and Z|V|2
≥0 , respectively. Interestingly, note that
the feature mapping Φall,✶,⊗(S1, S2) corresponds
to the standard segment-pair representation used
in many previous work, as (Marcu and Echihabi,
2002; Park and Cardie, 2012).
Dense representations We can apply the same
composition operations to denser representations,
yielding two new mappings:
</bodyText>
<equation confidence="0.957962">
Φall,M,⊕(S1, S2) = n,m � M&gt;✶w1i ® M&gt;✶w2j
i,j
Φall,M,⊗(S1, S2)= n,m �
i,j
</equation>
<bodyText confidence="0.999912083333333">
Like their head word versions, these vectors live
in R2p and Rp2, respectively.
Vector Normalization Normalization is impor-
tant as unnormalized composite vectors are sen-
sitive to the number of words present in the seg-
ments. The first type of normalization we consider
is to simply convert our vector representation into
vectors on the unit hypersphere: this is achieved
by dividing each vector by its L2 norm.
Another type of normalization is obtained by in-
verting the order of summation and concatenation
in the construction of composite vectors. Instead
</bodyText>
<equation confidence="0.9322635">
vec(M&gt;✶w1i ®M&gt; ✶w2 j )
�
</equation>
<page confidence="0.939388">
2204
</page>
<bodyText confidence="0.9998216">
of summing over concatenated pairs of word vec-
tors across the two segments, one can first sum in-
dividual word vectors within each segment, then
concatenate the two segment vectors. One can
thus use mapping Vall,✶,® in lieu of Vall,✶,®:
</bodyText>
<equation confidence="0.987694666666667">
n m
V� all,✶,®(S1, S2) =
i 1w1i ® j 1w2j
</equation>
<bodyText confidence="0.993467">
It should be clear that Vall,✶,® provides a nor-
malized version of Vall,✶,® as this latter mapping
amounts to weighted versions of the former:
</bodyText>
<equation confidence="0.970412333333333">
n m
Vall,✶,®(S1, S2) = M 1w1i ® n 1w2j
i j
</equation>
<sectionHeader confidence="0.997836" genericHeader="method">
4 Experiment Settings
</sectionHeader>
<bodyText confidence="0.999990818181818">
Through the comparative framework described in
section 3, our objective is to assess the useful-
ness of different vectorial representations for pairs
of discourse segments. Specifically, we want to
establish whether dense representations are better
than sparse ones, and whether certain word pairs
are more relevant than others, which resource and
which combination schemes are more adapted to
the task, and, finally, whether standard features de-
rived from external databases are still relevant in
the presence of dense representations.
</bodyText>
<subsectionHeader confidence="0.989291">
4.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.998265368421053">
Our main features are primarily lexical in nature
and based on surface word forms. These are de-
fined either on all words used in the relation argu-
ments or only on their heads.
Head Extraction Heads of discourse segments
are first extracted using Collins syntactic head
rules3. In order to retrieve the semantic predi-
cate, we define a heuristic which looks for the past
participle of auxiliaries, the adjectival or nominal
attribute of copula, the infinitive complementing
”have to” forms and the first head of coordination
conjunctions. In case of multiple subtrees, we look
for the head of the first independent clause, or, fail-
ing that, of the first phrase.
Word Representations We use either one-hot
encodings or use word embeddings to build denser
representations as described in section 3. The
Brown clusters (Brown), Collobert-Weston (CnW)
representations, and the hierarchical log-bilinear
</bodyText>
<footnote confidence="0.660069">
3https://github.com/jkkummerfeld/nlp-util
</footnote>
<bodyText confidence="0.998896294117647">
(HLBL) embeddings correspond to the versions
implemented in (Turian et al., 2010)4. They have
been built on Reuters English newswire with case
left intact. We test versions with 100, 320, 1000
and 3,200 clusters for Brown, with 25, 50, 100
and 200 dimensions for CnW and with 50 and
100 dimensions for HLBL. The Hellinger PCA (H-
PCA) embeddings come from (Lebret and Col-
lobert, 2014)5 and have been built over the en-
tire English Wikipedia, the Reuters corpus and the
Wall Street Journal with all words in lower case.
The vocabulary corresponds to the words that ap-
pear at least 100 times and normalized frequency
is computed with the 10, 000 most frequent words
as context words. We test versions with 50, 100
and 200 dimensions for H-PCA. The coverage of
each resource is presented in table 1.
</bodyText>
<table confidence="0.999796666666667">
# words # missing words
All words Head words
HLBL 246,122 5,439 171
CnW 268,810 5,638 171
Brown 247,339 5,413 171
H-PCA 178,080 7,042 190
</table>
<tableCaption confidence="0.944264">
Table 1: Word embeddings and Brown clusters
lexicon coverage.
</tableCaption>
<bodyText confidence="0.999852238095238">
When presenting our results, we distinguish be-
tween systems based on one-hot encoding built
from raw tokens (one-hot) or Brown clusters
(Brown). We group the systems that use embed-
dings under Embed. When relevant, we indicate
the number of dimensions (e.g. Brown 3,200 is the
system using Brown clusters with 3, 200 clusters).
We use the symbols defined in section 3 to repre-
sent the operation linking the arguments represen-
tations (e.g. one-hot ® corresponds to the transfor-
mation defined by Vh,✶,® when using heads and
by Vall,✶,® when using all words).
Vocabulary Sizes For one-hot encoding, the
case is left intact. We ignore the unknown words
when using the Brown clusters following (Ruther-
ford and Xue, 2014). For the word embeddings,
we use the mean of the vectors of all words.
In order to give an idea of the sparsity of
the one-hot encodings, note that we have |V |=
33,649 different tokens considering all implicit
examples without filtering. The Brown clusters
</bodyText>
<footnote confidence="0.9999545">
4http://metaoptimize.com/projects/wordreprs/
5http://lebret.ch/words/
</footnote>
<page confidence="0.989689">
2205
</page>
<bodyText confidence="0.999960842105263">
merge these tokens into 3,190 codes (for 3,200
clusters), 393 (1, 000 clusters), 59 (320 clusters) or
16 (100 clusters). For heads, we count 5,615 dif-
ferent tokens which correspond to 1, 988 codes for
3, 200 clusters and roughly the same number for
the others. For the dense representations, the vo-
cabulary size is twice the number of dimensions of
the embedding, thus from 50 to 400, or the square
of this number, thus from 625 to 40, 000.
Other Features We experiment with additional
features commonly used for this task: produc-
tions rules, average verb phrases length, Levin
verb classes, modality, polarity, General Inquirer
tags, number, first last and first three words. These
feature templates are well described in (Pitler et
al., 2009; Park and Cardie, 2012). They all corre-
spond to a one-hot encoding, except average verb
phrases length which is continuous. We thus con-
catenate these features to the lexical ones.
</bodyText>
<subsectionHeader confidence="0.964906">
4.2 Model
</subsectionHeader>
<bodyText confidence="0.999977037037037">
We use the same classification algorithm for com-
paring all the described feature configurations.
Specifically, we train a Maximum Entropy (ME)
classifier (aka, logistic regression).6 As in previ-
ous studies, we build one binary classifier for each
relation. In order to deal with class imbalance, we
use a sample weighting scheme: each sample re-
ceives a weight inversely proportional to the fre-
quency of its class in the training set. We optimize
the hyper-parameters of the algorithm (i.e., the
regularization norm: L1 or L2, and its strength)
and a filter on the features on the development set,
based on the F1 score. Note that filtering is point-
less for purely dense representations. We test sta-
tistical significancy of the results using t-test and
Wilcoxon test on a split of the test set in 20 folds.
Previous studies have tested several algorithms
generally concluding that Naive Bayes (NB) gives
the best performance (Pitler et al., 2009; Ruther-
ford and Xue, 2014). We found that, when the
hyper-parameters of ME are well tuned, the per-
formance are comparable to NB if not better.
Note that NB cannot be used with word embed-
dings representations as it does not handle neg-
ative value. Concerning the class imbalance is-
sue, the downsampling scheme is the most spread
since (Pitler et al., 2009) but it has been shown
</bodyText>
<footnote confidence="0.7323992">
6We use the implementation provided in Scikit-Learn (Pe-
dregosa et al., 2011), available at: http://scikit-learn.
org/dev/index.html.
that oversampling and instance weighting lead to
better performance (Li and Nenkova, 2014a).
</footnote>
<table confidence="0.999585333333333">
Relation Train Dev Test
Temporal 665 93 68
Contingency 3,281 628 276
Comparison 1,894 401 146
Expansion 6,792 1,253 556
Total 12,632 2,375 1,046
</table>
<tableCaption confidence="0.998823">
Table 2: Number of examples in train, dev, test.
</tableCaption>
<subsectionHeader confidence="0.995697">
4.3 Penn Discourse Treebank
</subsectionHeader>
<bodyText confidence="0.9999408">
We use the Penn Discourse Treebank (Prasad et
al., 2008), a corpus annotated at the discourse
level upon the Penn Treebank, giving access to a
gold syntactic annotation, and composed of arti-
cles from the Wall Street Journal. Five types of
examples are distinguished: implicit, explicit, al-
ternative lexicalizations, entity relations, and no
relation. Each example could carry multiple rela-
tions, up to four for implicit ones, and the relations
are organized into a three-level hierarchy.
We keep only true implicit examples and only
the first annotation. We focus on the top level re-
lations which correspond to general categories in-
cluded in most discursive frameworks. Finally, in
order to make comparison easier, we choose the
most spread split of the data, used in (Pitler et al.,
2009; Park and Cardie, 2012; Rutherford and Xue,
2014) among others. The amount of data for train-
ing (sections 2 − 21), development (00, 01, 23, 24)
and evaluation (21, 22) is summarized in table 2.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999984333333333">
We first discuss the models that use only lexical
features, defined either over all the words that ap-
pear in the arguments or only the head words. We
then compare our best performing lexical configu-
rations with the ones that also integrate additional
standard features, and to state-of-the-art systems.
</bodyText>
<subsectionHeader confidence="0.992179">
5.1 Word Pairs over the Arguments
</subsectionHeader>
<bodyText confidence="0.999960714285714">
Our first finding in this setting is that feature con-
figurations that employ unsupervised word repre-
sentations almost systematically outperform those
that use raw tokens. This is shown in the left
part of table 3. Although the optimal word rep-
resentation differs from one relation to another, it
is always a dense representation that achieves the
</bodyText>
<page confidence="0.97893">
2206
</page>
<table confidence="0.999857444444444">
All words Head words only
Representation Temp. Cont. Compa. Exp. Temp. Cont. Compa. Exp.
One-hot ⊗ 21.14 50.36 34.80 59.43 11.96 43.24 17.30 69.21
One-hot ⊕ 23.04 51.31 34.06 58.96 23.01 49.40 29.23 59.08
Brown 3,200 ⊗ 20.38 50.95 34.85 61.23 11.98 43.77 16.75 68.76
Best Brown ⊗ 15.52 53.85** 30.90 61.87 22.91 45.74 25.83 68.76
Best Brown ⊕ 27.96** 49.48 31.19 67.42** 21.84 47.36 27.52 61.38
Best Embed. ⊗ 22.97 52.76** 34.99 61.87 23.88 51.29 30.59 58.59
Best Embed. ⊕ 25.98* 52.50 33.15 60.17 22.48 47.48 29.82 57.45
</table>
<tableCaption confidence="0.99812">
Table 3: F1 score for systems using all words and only heads for Temporal (Temp.), Contingency (Cont.),
</tableCaption>
<bodyText confidence="0.985391040540541">
Comparison (Compa.) and Expansion (Exp.). * p ≤ 0.1, ** p ≤ 0.05 compared to One-hot ⊗ with t-test
and Wilcoxon ; for head words, all the improvements observed against One-hot ⊗ are significant.
best F1 score. Our baselines correspond to mul-
tiplicative and additive one-hot encodings, noted
One-hot ⊗ and One-hot ⊕, the former being the
most commonly used in previous work. These are
strong baselines in the sense they have been ob-
tained after optimizing a frequency cut-off. Our
best systems based on dense representations corre-
spond to significant improvements in terms of F1
of about 8 points for Expansion, 7 points for Tem-
poral and 3.5 for Contingency. The gains for Com-
parison are not statistically significant. All these
results are obtained using the normalization to unit
vectors possibly combined to the concatenation-
specific normalization described in §3.3.
Comparing Dense Representations The best
results are obtained using the Brown clusters
(Brown) showing that this resource merges words
in a way that is relevant to the task. Strikingly,
the Brown configuration used in (Rutherford and
Xue, 2014) (One-hot Brown 3,200 ⊗) does not
do better than the raw word pair baselines, except
for Expansion. Recall that these authors did not
explicitly provide this comparison. While doing
a little worse, word embeddings (Embed.) also
yield significant improvements for Temporal and
Contingency, and smaller improvements for the
others. This suggests that, even if they were not
built based on discursive criteria, the latent dimen-
sions encode word properties that are relevant to
their rhetorical function. The superiority of Brown
clusters over word embeddings is in line with the
conclusions in (Turian et al., 2010) for two rather
different NLP tasks (i.e., NER and chunking).
Turian et al. (2010) showed that the optimal
word embedding is task dependent. Our exper-
iments suggest that it is relation dependent: the
best scores are obtained with HLBL for Tempo-
ral, CnW for Contingency, H-PCA for Expansion
and CnW (Best Embed. ⊗) and HPCA (Best Em-
bed. ⊕) for Comparison. This again demonstrates
that these four relations have to be considered as
four distinct tasks. Identifying temporal or causal
links is indeed sensitive to very different factors,
the former relying more on temporal expressions
and temporal ordering of events whereas the lat-
ter relies on lexical and encyclopaedic knowledge
on events. We think that this also explains that
the behavior of the F1 against the optimal number
of clusters for Expansion really differs from what
we observed for the other relations: only 100 clus-
ters for the best concatenated system and 320 for
the best multiplicative one. Expansion is the least
semantically marked relation and thus takes less
advantage of fine-grained semantic groupings.
Comparing Word Combinations Comparing
concatenated configurations (⊕ systems) against
multiplicative ones (⊗ systems), we first note that
for raw tokens the concatenated form (one-hot
⊕) yields results that are comparable, and some-
times better, than the standard multiplicative sys-
tem (one-hot ⊗), while failing to explicitly model
word pair interactions. With Brown clusters, the
concatenated form Best Brown ⊕ lead to better F1
scores than Best Brown ⊗ except for Contingency.
When comparing performance on dev set, we
found that the differences between concatenated
and multiplicative forms for Brown (excluding Ex-
pansion for now) depend on the number of clusters
used. Turian et al. (2010) found that the more clus-
ters, the better the performance. This is also the
case here with concatenated forms, but not with
multiplicative forms. In that case, F1 increases un-
</bodyText>
<page confidence="0.97842">
2207
</page>
<bodyText confidence="0.999932833333333">
til 1, 000 clusters and then decreases. There is in-
deed a trade-off between expressivity and sparsity:
having too few clusters means that we loose im-
portant distinctions, but having too many clusters
leads to a loss in generalization. A similar trend is
also found with word embeddings.
</bodyText>
<subsectionHeader confidence="0.996186">
5.2 Head Words Only
</subsectionHeader>
<bodyText confidence="0.999997142857143">
Considering the right part of table 3, the first find-
ing is that performance of systems that use only
head words decrease compared to those using all
words, but much more so with the baseline One-
hot ® than with other representations. One-hot
® has very poor performance for most relations,
losing between 7 and 17 points in F1 score. The
performance loss is much less striking with One-
hot ® and with denser representations, which are
again the best performing. The only exception is
Expansion whose precision however increases. As
said, this relation is the less semantically marked,
making it less likely to take advantage of the use
of word representations. The best performance in
this setting are obtained with word embeddings
(not Brown) with significant gain from 8 to 13
points in F1 for most relations. Moreover, the best
systems are all based on the multiplicative form
confirming that this is a better way of representing
pairs than simple concatenation when the number
of initial dimensions is not too large.
</bodyText>
<subsectionHeader confidence="0.999859">
5.3 Adding Other Features
</subsectionHeader>
<bodyText confidence="0.999898473684211">
Finally, we would like to assess how much im-
provement can still be obtained by adding other
standard features, such as those in §4.1, to word-
based features. Conversely, we want to evaluate
how far we are from state-of-the-art performance
by just using word representations. We compare
our results with those presented in (Rutherford and
Xue, 2014) and in (Ji and Eisenstein, 2014), both
systems deal with sparsity either by using Brown
clusters or by learning task-dependent representa-
tions. To make comparison easier we reproduce
the experiments in (Rutherford and Xue, 2014)
with Naive Bayes (NB)7 and Maximum Entropy
(ME) but without their coreference features and
using gold syntactic parse. These correspond to
the “repr.” lines in table 4. We attribute the small
differences observed with NB by the lack of coref-
erence features and/or the use of different filter-
ing thresholds. Concerning the difference between
</bodyText>
<footnote confidence="0.9010755">
7Implemented in Scikit-Learn, we optimized the hyper-
parameter corresponding to the smoothing.
</footnote>
<bodyText confidence="0.999771825">
NB and ME, the only obvious issue is the low F1
score for Expansion: the system built using NB
predicts all examples as positive thus leading to
a high F1 score whereas the other one produces
more balanced predictions, meaning neither sys-
tems is truly satisfactory. Finally, we give results
using the traditional one-encoding based on word
pairs plus additional features (One-hot ® + addi-
tional features). These results are summarized in
table 4, also including the best results of our ex-
periments without additional features (“only”).
Our first finding is that the addition of extra
features to our previous word-based only config-
uration appears to outperform state-of-the art re-
sults for Temporal and Contingency, thus giving
the best performance to date on these relations.
These improvements are significant compared to
our reproduced systems. Note that we also out-
perform the task-dependent embeddings of Ji and
Eisenstein (2014), except for Expansion. Our ten-
tative explanation for this is that these authors in-
cluded Entity relations and coreference features.
Note that our system corresponding to a reproduc-
tion of (Rutherford and Xue, 2014) gives results
similar to the baseline using raw word pairs (One-
hot ® + additional features) showing that their im-
provements were due to other factors, the opti-
mized filter threshold and the coreference features.
Overall, the addition of these hand-crafted fea-
tures to our best systems do not provide improve-
ments as high as one might have hoped. While
improvements are significant compared to our re-
produced systems, they are not with respect to the
best systems given in table 3. When using all
words, we only have a tendency toward significant
improvement for Contingency8. These very small
differences demonstrate that semantic and syntac-
tic properties encoded in these features are already
taken into account into the unsupervised word rep-
resentations.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.950144222222222">
Automatically identifying implicit relations is
challenging due to the complex nature of the pre-
dictors. Previous studies have thus used many fea-
tures relying on several external resources (Pitler
et al., 2009; Park and Cardie, 2012; Biran and
McKeown, 2013) as the MPQA lexicon (Wilson
et al., 2005) or the General Inquirer lexicon (Stone
and Hunt, 1963), or on constituent or dependency
8p = 0.135 with ttest and p = 0.061 with Wilcoxon.
</bodyText>
<page confidence="0.931426">
2208
</page>
<table confidence="0.976422222222222">
Temporal Contingency Comparison Expansion
System F1 F1 F1 F1
(Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91
(Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23
repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23
repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00
One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57
Best all tokens only 27.96 53.85 34.99 67.42
Best all tokens + add. features 29.30 55.76 36.36 61.76
</table>
<tableCaption confidence="0.727728666666667">
Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported
or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from
previous table (“only”).
</tableCaption>
<bodyText confidence="0.999725395348837">
parsers (Li and Nenkova, 2014b; Lin et al., 2009).
Feature selection methods have been proved nec-
essary to handle all of these features (Park and
Cardie, 2012; Lin et al., 2009). Interestingly, Park
and Cardie (2012) conclude on the worthlessness
of word pair features, given the existence of such
resources. We showed that provided unsupervised
word representations, the opposite was in fact true,
as dense word representations capture a lot of syn-
tactic and semantic information.
The major problem of standard word pair repre-
sentations is their sparsity. A line of work is to deal
with this issue by adding automatically annotated
data from explicit examples (Marcu and Echihabi,
2002), possibly using some kind of filtering or
adaptation methods (Pitler et al., 2009; Biran and
McKeown, 2013; Braud and Denis, 2014). An-
other line of work propose to make use of dense
representations as Brown clusters in (Rutherford
and Xue, 2014). These authors claim that this re-
source provides word representations that are rele-
vant to the task, a conclusion that we considerably
refined. Ji and Eisenstein (2014) propose to learn a
distributed representation from the syntactic trees
representing each argument in way that is more
directly related to the task. Although this is an
attractive idea, the score on top level PDTB rela-
tions are mostly below those reported by (Ruther-
ford and Xue, 2014), possibly because their repre-
sentations are learned on a rather small corpus, the
PDTB itself, whereas building this kind of repre-
sentation requires massive amount of data.
Our work also relates to studies comparing un-
supervised representations for other NLP tasks
such as name entity recognition, chunking (Turian
et al., 2010), sentiment analysis (Lebret and Col-
lobert, 2014) or POS tagging (Stratos and Collins,
2005). In particular, we found some similarities
between our conclusions and those in (Turian et
al., 2010). Our comparison is slightly richer in
that it includes different methods of vector com-
positions and add an extra distributional represen-
tation to our comparison (namely, H-PCA).
</bodyText>
<sectionHeader confidence="0.996846" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999929851851852">
In this paper, we show that one can reach state-of-
the-art results for implicit discourse relation iden-
tification using only shallow lexical features and
existing unsupervised word representations thus
contradicting previous conclusions on the worth-
lessness of these features. We carefully assess
the usefulness of word representations for dis-
course by comparing various formulations and
combination schemes, demonstrating the inade-
quacy of the previously proposed strategy based
on Brown clusters and the distinctive relevance of
head words, and by establishing that the created
dense representations already provide most of the
semantic and syntactic information relevant to the
task thus alleviating the need for traditional exter-
nal resources.
In future work, we first plan to extend our com-
parative framework to a larger set of relations and
to other languages. We also want to explore meth-
ods for learning embeddings that are directly re-
lated to the task of discourse relation classifica-
tion, potentially using existing embeddings as ini-
tialization (Labutov and Lipson, 2013). It is also
clear that seeing discourse segments as bag of
words is too simplistic, we would like to investi-
gate ways of learning adequate segment-wide em-
beddings.
</bodyText>
<page confidence="0.991067">
2209
</page>
<sectionHeader confidence="0.995892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805698113208">
N. Asher and A. Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Chlo´e Braud and Pascal Denis. 2014. Combining
natural and artificial examples to improve implicit
discourse relation identification. In Proceedings of
the 25th International Conference on Computational
Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning.
Yangfeng Ji and Jacob Eisenstein. 2014. One vector
is not enough: Entity-augmented distributional se-
mantics for discourse relations. Transactions of the
Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51th Annual Meeting
of the Association for Computational Linguistics.
R´emi Lebret and Ronan Collobert. 2014. Word
emdeddings through hellinger PCA. In Proceedings
of the 14th Conference of the European Chapter of
the Association for Computational Linguistics.
Junyi Jessy Li and Ani Nenkova. 2014a. Addressing
class imbalance for improved recognition of implicit
discourse relations. In Proceedings of the 15th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue.
Junyi Jessy Li and Ani Nenkova. 2014b. Reducing
sparsity improves the recognition of implicit dis-
course relations. In Proceedings of the 15th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation.
Attapol Rutherford and Nianwen Xue. 2014. Dis-
covering implicit discourse relations through brown
cluster pair representation and coreference patterns.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics.
Philip J. Stone and Earl B. Hunt. 1963. A computer
approach to content analysis: Studies using the gen-
eral inquirer system. In Proceedings of the Spring
Joint Computer Conference.
Karl Stratos and Michael Collins. 2005. Simple semi-
supervised pos tagging. In Proceedings of NAACL
Workshop on Vector Space Modeling for NLP.
</reference>
<page confidence="0.68619">
2210
</page>
<reference confidence="0.999553">
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing.
</reference>
<page confidence="0.991276">
2211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885544">
<title confidence="0.9930345">Comparing Word Representations Implicit Discourse Relation Classification</title>
<author confidence="0.998715">Chlo´e Braud</author>
<affiliation confidence="0.9766765">ALPAGE, Univ Paris &amp; INRIA</affiliation>
<address confidence="0.995941">75013 Paris -</address>
<email confidence="0.988632">chloe.braud@inria.fr</email>
<abstract confidence="0.997596105263158">This paper presents a detailed comparative framework for assessing the usefulness of unsupervised word representations for identifying so-called implicit discourse relations. Specifically, we compare standard one-hot word pair representations against low-dimensional ones based on Brown clusters and word embeddings. We also consider various word vector combination schemes for deriving discourse segment representations from word vectors, and compare representations based either on all words or limited to head words. Our main finding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2088" citStr="Asher and Lascarides, 2003" startWordPosition="305" endWordPosition="308">ons are extremely pervasive in real text corpora: they account for about 50% of all relations in the Penn Discourse Treebank (Prasad et al., 2008). (1) [ Quarterly revenue rose 4.5%, to $2.3 billion from $2.2 billion]arg1 [ For the year, net income tumbled 61% to $86 million, or $1.55 a share]arg2 Pascal Denis MAGNET, INRIA Lille Nord-Europe 59650 Villeneuve d’Ascq - France pascal.denis@inria.fr Automatically classifying implicit relations is difficult in large part because it relies on numerous factors, ranging from syntax, and tense and aspect, to lexical semantics and even world knowledge (Asher and Lascarides, 2003). Consequently, a lot of previous work on this problem have attempted to incorporate some of these information into their systems. These assume the existence of syntactic parsers and lexical databases of various kinds, which are available but for a few languages, and they often involve heavy feature engineering (Pitler et al., 2009; Park and Cardie, 2012). While acknowledging this knowledge bottleneck, this paper focuses on trying to predict implicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8627" citStr="Bengio et al., 2003" startWordPosition="1318" endWordPosition="1321">resentations also yields one-hot encodings but on a much smaller vocabulary size (i.e., the number of clusters). Brown clusters have been used for several NLP tasks, including NER, chunking (Turian et al., 2010), parsing (Koo et al., 2008) and implicit discourse relation classification (Rutherford and Xue, 2014). 2.3 Dense Real-Valued Representations Another approach to induce word representations from raw text is to learn distributed word representations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Collobert and Weston, 2008). The embeddings are learned discriminatively by minimizing a loss between the current n-gram and a corrupted n-gram whose last word comes from the same vocabulary but is different from the last word of the original n-gram. Another example are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007) induced using a probabilistic and linear neural model, with a hierarchical </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34416" citStr="Biran and McKeown, 2013" startWordPosition="5546" endWordPosition="5549">d systems, they are not with respect to the best systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency8. These very small differences demonstrate that semantic and syntactic properties encoded in these features are already taken into account into the unsupervised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. fe</context>
<context position="36070" citStr="Biran and McKeown, 2013" startWordPosition="5812" endWordPosition="5815"> 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples (Marcu and Echihabi, 2002), possibly using some kind of filtering or adaptation methods (Pitler et al., 2009; Biran and McKeown, 2013; Braud and Denis, 2014). Another line of work propose to make use of dense representations as Brown clusters in (Rutherford and Xue, 2014). These authors claim that this resource provides word representations that are relevant to the task, a conclusion that we considerably refined. Ji and Eisenstein (2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possi</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chlo´e Braud</author>
<author>Pascal Denis</author>
</authors>
<title>Combining natural and artificial examples to improve implicit discourse relation identification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="36094" citStr="Braud and Denis, 2014" startWordPosition="5816" endWordPosition="5819">rk and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples (Marcu and Echihabi, 2002), possibly using some kind of filtering or adaptation methods (Pitler et al., 2009; Biran and McKeown, 2013; Braud and Denis, 2014). Another line of work propose to make use of dense representations as Brown clusters in (Rutherford and Xue, 2014). These authors claim that this resource provides word representations that are relevant to the task, a conclusion that we considerably refined. Ji and Eisenstein (2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possibly because their repres</context>
</contexts>
<marker>Braud, Denis, 2014</marker>
<rawString>Chlo´e Braud and Pascal Denis. 2014. Combining natural and artificial examples to improve implicit discourse relation identification. In Proceedings of the 25th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="4073" citStr="Brown et al., 1992" startWordPosition="613" endWordPosition="616">se segments via their main verbs, the corresponding one-hot encoding for the pair (rose,tumbled) would be at equal distance from the synonymic pair (went up,lost) and the antonymic pair (went down,gained), as all three vectors are orthogonal to each others. Various attempts have been made at reducing sparsity of lexical features. Recently, Ruther2201 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2201–2211, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ford and Xue (2014) proposed to use Brown clusters (Brown et al., 1992) for this task, in effect replacing each token by its cluster binary code. These authors conclude that these denser, clusterderived representations significantly improve the identification of implicit discourse relations and report the best performance to date using also additional features. Unfortunately, their claim is somewhat weakened by the fact that they fail to compare the use of their cluster word pairs against other types of word representations, including one-hot encodings of word pairs or other low-dimensional word representations. This work also leaves other important questions ope</context>
<context position="7565" citStr="Brown et al., 1992" startWordPosition="1154" endWordPosition="1157">noted ✶,,,, such that d = |V|: that is, all of this vector’s components are 0’s but for one 1 component corresponding to the word’s index in V. It is easy to see that this representation is extremely sparse, and makes learning difficult as it mechanically blows up the parameter space of the model. 2.2 Clustering-based Word Representations An alternative to these very sparse representations consists in learning word representations in an unsupervised fashion using clustering. An example of this approach are the so-called Brown clusters induced using the Brown hierarchical clustering algorithm (Brown et al., 1992) with the goal of maximizing the mutual information of bigrams. As a result, each word is associated to a binary code corresponding to the cluster it belongs to. Given the hierarchical nature of the algorithm, one can create word classes of different levels of granularity, corresponding to bit codes of different sizes. The less clusters, the less fine-grained the distinctions between words but the less sparsity. Note that this kind of representations also yields one-hot encodings but on a much smaller vocabulary size (i.e., the number of clusters). Brown clusters have been used for several NLP</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5379" citStr="Collobert and Weston, 2008" startWordPosition="808" endWordPosition="811"> discourse segments are truly informative and should be included in the model. Given that word embeddings capture latent syntactic and semantic information, yet another important question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features. This paper fills these gaps and significantly extends the work of (Rutherford and Xue, 2014) by explicitly comparing various types of word representations and vector composition methods. Specifically, we investigate three well-known word embeddings, namely Collobert and Weston (Collobert and Weston, 2008), hierarchical log-bilinear model (Mnih and Hinton, 2007) and Hellinger Principal Component Analysis (Lebret and Collobert, 2014), in addition to Brown clusterbased and standard one-hot representations. All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviating the need for hand-crafted lexical databases. This makes our approach easily extendable to resource-poor languages. In addition, we also investigate the issue of which specific words need to be fed to the model, by comparing using just pairs </context>
<context position="8843" citStr="Collobert and Weston, 2008" startWordPosition="1351" endWordPosition="1354">0), parsing (Koo et al., 2008) and implicit discourse relation classification (Rutherford and Xue, 2014). 2.3 Dense Real-Valued Representations Another approach to induce word representations from raw text is to learn distributed word representations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Collobert and Weston, 2008). The embeddings are learned discriminatively by minimizing a loss between the current n-gram and a corrupted n-gram whose last word comes from the same vocabulary but is different from the last word of the original n-gram. Another example are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007) induced using a probabilistic and linear neural model, with a hierarchical principle used to speed up the model evaluation. The embeddings are obtained by concatenating the embeddings of the n−1 words of a n-gram and learning the embedding of the last word. A final approach is based on the </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>One vector is not enough: Entity-augmented distributional semantics for discourse relations. Transactions of the Association for Computational Linguistics.</title>
<date>2014</date>
<contexts>
<context position="31604" citStr="Ji and Eisenstein, 2014" startWordPosition="5104" endWordPosition="5107">lations. Moreover, the best systems are all based on the multiplicative form confirming that this is a better way of representing pairs than simple concatenation when the number of initial dimensions is not too large. 5.3 Adding Other Features Finally, we would like to assess how much improvement can still be obtained by adding other standard features, such as those in §4.1, to wordbased features. Conversely, we want to evaluate how far we are from state-of-the-art performance by just using word representations. We compare our results with those presented in (Rutherford and Xue, 2014) and in (Ji and Eisenstein, 2014), both systems deal with sparsity either by using Brown clusters or by learning task-dependent representations. To make comparison easier we reproduce the experiments in (Rutherford and Xue, 2014) with Naive Bayes (NB)7 and Maximum Entropy (ME) but without their coreference features and using gold syntactic parse. These correspond to the “repr.” lines in table 4. We attribute the small differences observed with NB by the lack of coreference features and/or the use of different filtering thresholds. Concerning the difference between 7Implemented in Scikit-Learn, we optimized the hyperparameter </context>
<context position="33170" citStr="Ji and Eisenstein (2014)" startWordPosition="5347" endWordPosition="5350">al one-encoding based on word pairs plus additional features (One-hot ® + additional features). These results are summarized in table 4, also including the best results of our experiments without additional features (“only”). Our first finding is that the addition of extra features to our previous word-based only configuration appears to outperform state-of-the art results for Temporal and Contingency, thus giving the best performance to date on these relations. These improvements are significant compared to our reproduced systems. Note that we also outperform the task-dependent embeddings of Ji and Eisenstein (2014), except for Expansion. Our tentative explanation for this is that these authors included Entity relations and coreference features. Note that our system corresponding to a reproduction of (Rutherford and Xue, 2014) gives results similar to the baseline using raw word pairs (Onehot ® + additional features) showing that their improvements were due to other factors, the optimized filter threshold and the coreference features. Overall, the addition of these hand-crafted features to our best systems do not provide improvements as high as one might have hoped. While improvements are significant com</context>
<context position="34689" citStr="Ji and Eisenstein, 2014" startWordPosition="5593" endWordPosition="5596">es are already taken into account into the unsupervised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from previous table (“only”). parsers (Li a</context>
<context position="36378" citStr="Ji and Eisenstein (2014)" startWordPosition="5863" endWordPosition="5866">on. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples (Marcu and Echihabi, 2002), possibly using some kind of filtering or adaptation methods (Pitler et al., 2009; Biran and McKeown, 2013; Braud and Denis, 2014). Another line of work propose to make use of dense representations as Brown clusters in (Rutherford and Xue, 2014). These authors claim that this resource provides word representations that are relevant to the task, a conclusion that we considerably refined. Ji and Eisenstein (2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possibly because their representations are learned on a rather small corpus, the PDTB itself, whereas building this kind of representation requires massive amount of data. Our work also relates to studies comparing unsupervised representations for other NLP tasks such as name entity recognition, chunking (Turian</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2014. One vector is not enough: Entity-augmented distributional semantics for discourse relations. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association</booktitle>
<contexts>
<context position="8246" citStr="Koo et al., 2008" startWordPosition="1266" endWordPosition="1269">As a result, each word is associated to a binary code corresponding to the cluster it belongs to. Given the hierarchical nature of the algorithm, one can create word classes of different levels of granularity, corresponding to bit codes of different sizes. The less clusters, the less fine-grained the distinctions between words but the less sparsity. Note that this kind of representations also yields one-hot encodings but on a much smaller vocabulary size (i.e., the number of clusters). Brown clusters have been used for several NLP tasks, including NER, chunking (Turian et al., 2010), parsing (Koo et al., 2008) and implicit discourse relation classification (Rutherford and Xue, 2014). 2.3 Dense Real-Valued Representations Another approach to induce word representations from raw text is to learn distributed word representations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Collobert and Weston, 2008). T</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Collobert</author>
</authors>
<title>Word emdeddings through hellinger PCA.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5508" citStr="Lebret and Collobert, 2014" startWordPosition="824" endWordPosition="827">c and semantic information, yet another important question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features. This paper fills these gaps and significantly extends the work of (Rutherford and Xue, 2014) by explicitly comparing various types of word representations and vector composition methods. Specifically, we investigate three well-known word embeddings, namely Collobert and Weston (Collobert and Weston, 2008), hierarchical log-bilinear model (Mnih and Hinton, 2007) and Hellinger Principal Component Analysis (Lebret and Collobert, 2014), in addition to Brown clusterbased and standard one-hot representations. All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviating the need for hand-crafted lexical databases. This makes our approach easily extendable to resource-poor languages. In addition, we also investigate the issue of which specific words need to be fed to the model, by comparing using just pairs of verbs against all pairs of words, and how word representations should be combined over discourse segments, comparing component</context>
<context position="10141" citStr="Lebret and Collobert, 2014" startWordPosition="1558" endWordPosition="1561">r meanings. Building word distributional representations is done by computing the raw cooccurrence frequencies between each word and the |D |words that serve as context, with D generally smaller than the overall vocabulary, then applying some transformation (e.g. TF-IDF). As |D |is generally too large to form a tractable representation, a dimensionality reduction algorithm is used to end up with p « |D |dimensions. Like for distributed representations, we end up with a dense low-dimensional real-valued vector for each word. A recent example of such approach is the Hellinger PCA embeddings of (Lebret and Collobert, 2014) which were built using Principal Component Analysis based on Hellinger distance as dimensionality reduction algorithm. An important appeal of these representations is that they are much less time-consuming to train than the ones based on neural language models while allowing similar performance (Lebret and Collobert, 2014). 3 Segment Pair Representations We now turn to the issue of combining the word representations as described in section 2 into composite vectors corresponding to implicit discourse classification instances. Schematically, the representations employed for pairs of discourse s</context>
<context position="19524" citStr="Lebret and Collobert, 2014" startWordPosition="3126" endWordPosition="3130"> one-hot encodings or use word embeddings to build denser representations as described in section 3. The Brown clusters (Brown), Collobert-Weston (CnW) representations, and the hierarchical log-bilinear 3https://github.com/jkkummerfeld/nlp-util (HLBL) embeddings correspond to the versions implemented in (Turian et al., 2010)4. They have been built on Reuters English newswire with case left intact. We test versions with 100, 320, 1000 and 3,200 clusters for Brown, with 25, 50, 100 and 200 dimensions for CnW and with 50 and 100 dimensions for HLBL. The Hellinger PCA (HPCA) embeddings come from (Lebret and Collobert, 2014)5 and have been built over the entire English Wikipedia, the Reuters corpus and the Wall Street Journal with all words in lower case. The vocabulary corresponds to the words that appear at least 100 times and normalized frequency is computed with the 10, 000 most frequent words as context words. We test versions with 50, 100 and 200 dimensions for H-PCA. The coverage of each resource is presented in table 1. # words # missing words All words Head words HLBL 246,122 5,439 171 CnW 268,810 5,638 171 Brown 247,339 5,413 171 H-PCA 178,080 7,042 190 Table 1: Word embeddings and Brown clusters lexico</context>
<context position="37041" citStr="Lebret and Collobert, 2014" startWordPosition="5968" endWordPosition="5972">esentation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possibly because their representations are learned on a rather small corpus, the PDTB itself, whereas building this kind of representation requires massive amount of data. Our work also relates to studies comparing unsupervised representations for other NLP tasks such as name entity recognition, chunking (Turian et al., 2010), sentiment analysis (Lebret and Collobert, 2014) or POS tagging (Stratos and Collins, 2005). In particular, we found some similarities between our conclusions and those in (Turian et al., 2010). Our comparison is slightly richer in that it includes different methods of vector compositions and add an extra distributional representation to our comparison (namely, H-PCA). 7 Conclusions and Future Work In this paper, we show that one can reach state-ofthe-art results for implicit discourse relation identification using only shallow lexical features and existing unsupervised word representations thus contradicting previous conclusions on the wor</context>
</contexts>
<marker>Lebret, Collobert, 2014</marker>
<rawString>R´emi Lebret and Ronan Collobert. 2014. Word emdeddings through hellinger PCA. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyi Jessy Li</author>
<author>Ani Nenkova</author>
</authors>
<title>Addressing class imbalance for improved recognition of implicit discourse relations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="23662" citStr="Li and Nenkova, 2014" startWordPosition="3809" endWordPosition="3812">(Pitler et al., 2009; Rutherford and Xue, 2014). We found that, when the hyper-parameters of ME are well tuned, the performance are comparable to NB if not better. Note that NB cannot be used with word embeddings representations as it does not handle negative value. Concerning the class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test Temporal 665 93 68 Contingency 3,281 628 276 Comparison 1,894 401 146 Expansion 6,792 1,253 556 Total 12,632 2,375 1,046 Table 2: Number of examples in train, dev, test. 4.3 Penn Discourse Treebank We use the Penn Discourse Treebank (Prasad et al., 2008), a corpus annotated at the discourse level upon the Penn Treebank, giving access to a gold syntactic annotation, and composed of articles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carr</context>
<context position="35305" citStr="Li and Nenkova, 2014" startWordPosition="5690" endWordPosition="5693">014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from previous table (“only”). parsers (Li and Nenkova, 2014b; Lin et al., 2009). Feature selection methods have been proved necessary to handle all of these features (Park and Cardie, 2012; Lin et al., 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annota</context>
</contexts>
<marker>Li, Nenkova, 2014</marker>
<rawString>Junyi Jessy Li and Ani Nenkova. 2014a. Addressing class imbalance for improved recognition of implicit discourse relations. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyi Jessy Li</author>
<author>Ani Nenkova</author>
</authors>
<title>Reducing sparsity improves the recognition of implicit discourse relations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="23662" citStr="Li and Nenkova, 2014" startWordPosition="3809" endWordPosition="3812">(Pitler et al., 2009; Rutherford and Xue, 2014). We found that, when the hyper-parameters of ME are well tuned, the performance are comparable to NB if not better. Note that NB cannot be used with word embeddings representations as it does not handle negative value. Concerning the class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test Temporal 665 93 68 Contingency 3,281 628 276 Comparison 1,894 401 146 Expansion 6,792 1,253 556 Total 12,632 2,375 1,046 Table 2: Number of examples in train, dev, test. 4.3 Penn Discourse Treebank We use the Penn Discourse Treebank (Prasad et al., 2008), a corpus annotated at the discourse level upon the Penn Treebank, giving access to a gold syntactic annotation, and composed of articles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carr</context>
<context position="35305" citStr="Li and Nenkova, 2014" startWordPosition="5690" endWordPosition="5693">014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from previous table (“only”). parsers (Li and Nenkova, 2014b; Lin et al., 2009). Feature selection methods have been proved necessary to handle all of these features (Park and Cardie, 2012; Lin et al., 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annota</context>
</contexts>
<marker>Li, Nenkova, 2014</marker>
<rawString>Junyi Jessy Li and Ani Nenkova. 2014b. Reducing sparsity improves the recognition of implicit discourse relations. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="35325" citStr="Lin et al., 2009" startWordPosition="5694" endWordPosition="5697">79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from previous table (“only”). parsers (Li and Nenkova, 2014b; Lin et al., 2009). Feature selection methods have been proved necessary to handle all of these features (Park and Cardie, 2012; Lin et al., 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explic</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2777" citStr="Marcu and Echihabi, 2002" startWordPosition="414" endWordPosition="417">mpted to incorporate some of these information into their systems. These assume the existence of syntactic parsers and lexical databases of various kinds, which are available but for a few languages, and they often involve heavy feature engineering (Pitler et al., 2009; Park and Cardie, 2012). While acknowledging this knowledge bottleneck, this paper focuses on trying to predict implicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (1). Most previous studies on implicit relations, going back to (Marcu and Echihabi, 2002), incorporate word-based information in the form of word pair features defined across the pair of text segments to be related. Such word pairs are often encoded in a one-hot representation, in which each possible word pair corresponds to a single component of a very highdimensional vector. From a machine learning perspective, this type of sparse representation makes parameter estimation extremely difficult and prone to overfitting. It also makes it difficult to achieve any interesting semantic generalization. To see this, consider the distance (e.g., Euclidean or cosine) induced by such repres</context>
<context position="16361" citStr="Marcu and Echihabi, 2002" startWordPosition="2617" endWordPosition="2620">osing the segments. One-hot representations Following this approach and recalling that S1 contains n words, while S2 has m words, one can construct one-hot encodings for segment pairs as follows: n m Φall,✶,⊕(S1, S2) = ✶w1i ® ✶w2j i j n m Φall,✶,⊗(S1, S2) = vec(✶w1i ® ✶w2j ) i j If used without any type of frequency thresholding, these mappings result in very highdimensional feature representations living in Z2|V| ≥0 and Z|V|2 ≥0 , respectively. Interestingly, note that the feature mapping Φall,✶,⊗(S1, S2) corresponds to the standard segment-pair representation used in many previous work, as (Marcu and Echihabi, 2002; Park and Cardie, 2012). Dense representations We can apply the same composition operations to denser representations, yielding two new mappings: Φall,M,⊕(S1, S2) = n,m � M&gt;✶w1i ® M&gt;✶w2j i,j Φall,M,⊗(S1, S2)= n,m � i,j Like their head word versions, these vectors live in R2p and Rp2, respectively. Vector Normalization Normalization is important as unnormalized composite vectors are sensitive to the number of words present in the segments. The first type of normalization we consider is to simply convert our vector representation into vectors on the unit hypersphere: this is achieved by dividin</context>
<context position="35963" citStr="Marcu and Echihabi, 2002" startWordPosition="5795" endWordPosition="5798">ection methods have been proved necessary to handle all of these features (Park and Cardie, 2012; Lin et al., 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples (Marcu and Echihabi, 2002), possibly using some kind of filtering or adaptation methods (Pitler et al., 2009; Biran and McKeown, 2013; Braud and Denis, 2014). Another line of work propose to make use of dense representations as Brown clusters in (Rutherford and Xue, 2014). These authors claim that this resource provides word representations that are relevant to the task, a conclusion that we considerably refined. Ji and Eisenstein (2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea,</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5436" citStr="Mnih and Hinton, 2007" startWordPosition="815" endWordPosition="818">ed in the model. Given that word embeddings capture latent syntactic and semantic information, yet another important question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features. This paper fills these gaps and significantly extends the work of (Rutherford and Xue, 2014) by explicitly comparing various types of word representations and vector composition methods. Specifically, we investigate three well-known word embeddings, namely Collobert and Weston (Collobert and Weston, 2008), hierarchical log-bilinear model (Mnih and Hinton, 2007) and Hellinger Principal Component Analysis (Lebret and Collobert, 2014), in addition to Brown clusterbased and standard one-hot representations. All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviating the need for hand-crafted lexical databases. This makes our approach easily extendable to resource-poor languages. In addition, we also investigate the issue of which specific words need to be fed to the model, by comparing using just pairs of verbs against all pairs of words, and how word represe</context>
<context position="9151" citStr="Mnih and Hinton, 2007" startWordPosition="1398" endWordPosition="1401">eal-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Collobert and Weston, 2008). The embeddings are learned discriminatively by minimizing a loss between the current n-gram and a corrupted n-gram whose last word comes from the same vocabulary but is different from the last word of the original n-gram. Another example are the Hierarchical log-bilinear embeddings (Mnih and Hinton, 2007) induced using a probabilistic and linear neural model, with a hierarchical principle used to speed up the model evaluation. The embeddings are obtained by concatenating the embeddings of the n−1 words of a n-gram and learning the embedding of the last word. A final approach is based on the assumption that words occurring in similar contexts tend to have similar meanings. Building word distributional representations is done by computing the raw cooccurrence frequencies between each word and the |D |words that serve as context, with D generally smaller than the overall vocabulary, then applying</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving implicit discourse relation recognition through feature set optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2445" citStr="Park and Cardie, 2012" startWordPosition="365" endWordPosition="368">’Ascq - France pascal.denis@inria.fr Automatically classifying implicit relations is difficult in large part because it relies on numerous factors, ranging from syntax, and tense and aspect, to lexical semantics and even world knowledge (Asher and Lascarides, 2003). Consequently, a lot of previous work on this problem have attempted to incorporate some of these information into their systems. These assume the existence of syntactic parsers and lexical databases of various kinds, which are available but for a few languages, and they often involve heavy feature engineering (Pitler et al., 2009; Park and Cardie, 2012). While acknowledging this knowledge bottleneck, this paper focuses on trying to predict implicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (1). Most previous studies on implicit relations, going back to (Marcu and Echihabi, 2002), incorporate word-based information in the form of word pair features defined across the pair of text segments to be related. Such word pairs are often encoded in a one-hot representation, in which each possible word pair corresponds to a single component of a very h</context>
<context position="16385" citStr="Park and Cardie, 2012" startWordPosition="2621" endWordPosition="2624">t representations Following this approach and recalling that S1 contains n words, while S2 has m words, one can construct one-hot encodings for segment pairs as follows: n m Φall,✶,⊕(S1, S2) = ✶w1i ® ✶w2j i j n m Φall,✶,⊗(S1, S2) = vec(✶w1i ® ✶w2j ) i j If used without any type of frequency thresholding, these mappings result in very highdimensional feature representations living in Z2|V| ≥0 and Z|V|2 ≥0 , respectively. Interestingly, note that the feature mapping Φall,✶,⊗(S1, S2) corresponds to the standard segment-pair representation used in many previous work, as (Marcu and Echihabi, 2002; Park and Cardie, 2012). Dense representations We can apply the same composition operations to denser representations, yielding two new mappings: Φall,M,⊕(S1, S2) = n,m � M&gt;✶w1i ® M&gt;✶w2j i,j Φall,M,⊗(S1, S2)= n,m � i,j Like their head word versions, these vectors live in R2p and Rp2, respectively. Vector Normalization Normalization is important as unnormalized composite vectors are sensitive to the number of words present in the segments. The first type of normalization we consider is to simply convert our vector representation into vectors on the unit hypersphere: this is achieved by dividing each vector by its L2 </context>
<context position="21951" citStr="Park and Cardie, 2012" startWordPosition="3529" endWordPosition="3532">5 different tokens which correspond to 1, 988 codes for 3, 200 clusters and roughly the same number for the others. For the dense representations, the vocabulary size is twice the number of dimensions of the embedding, thus from 50 to 400, or the square of this number, thus from 625 to 40, 000. Other Features We experiment with additional features commonly used for this task: productions rules, average verb phrases length, Levin verb classes, modality, polarity, General Inquirer tags, number, first last and first three words. These feature templates are well described in (Pitler et al., 2009; Park and Cardie, 2012). They all correspond to a one-hot encoding, except average verb phrases length which is continuous. We thus concatenate these features to the lexical ones. 4.2 Model We use the same classification algorithm for comparing all the described feature configurations. Specifically, we train a Maximum Entropy (ME) classifier (aka, logistic regression).6 As in previous studies, we build one binary classifier for each relation. In order to deal with class imbalance, we use a sample weighting scheme: each sample receives a weight inversely proportional to the frequency of its class in the training set.</context>
<context position="24699" citStr="Park and Cardie, 2012" startWordPosition="3978" endWordPosition="3981">cles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carry multiple relations, up to four for implicit ones, and the relations are organized into a three-level hierarchy. We keep only true implicit examples and only the first annotation. We focus on the top level relations which correspond to general categories included in most discursive frameworks. Finally, in order to make comparison easier, we choose the most spread split of the data, used in (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014) among others. The amount of data for training (sections 2 − 21), development (00, 01, 23, 24) and evaluation (21, 22) is summarized in table 2. 5 Results We first discuss the models that use only lexical features, defined either over all the words that appear in the arguments or only the head words. We then compare our best performing lexical configurations with the ones that also integrate additional standard features, and to state-of-the-art systems. 5.1 Word Pairs over the Arguments Our first finding in this setting is that feature configurations that employ unsu</context>
<context position="34390" citStr="Park and Cardie, 2012" startWordPosition="5542" endWordPosition="5545">mpared to our reproduced systems, they are not with respect to the best systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency8. These very small differences demonstrate that semantic and syntactic properties encoded in these features are already taken into account into the unsupervised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="23516" citStr="Pedregosa et al., 2011" startWordPosition="3791" endWordPosition="3795">t of the test set in 20 folds. Previous studies have tested several algorithms generally concluding that Naive Bayes (NB) gives the best performance (Pitler et al., 2009; Rutherford and Xue, 2014). We found that, when the hyper-parameters of ME are well tuned, the performance are comparable to NB if not better. Note that NB cannot be used with word embeddings representations as it does not handle negative value. Concerning the class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test Temporal 665 93 68 Contingency 3,281 628 276 Comparison 1,894 401 146 Expansion 6,792 1,253 556 Total 12,632 2,375 1,046 Table 2: Number of examples in train, dev, test. 4.3 Penn Discourse Treebank We use the Penn Discourse Treebank (Prasad et al., 2008), a corpus annotated at the discourse level upon the Penn Treebank, giving access to a gold syntactic annotation, and composed of articles from the Wall Street Journal. Fiv</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="1215" citStr="Pitler and Nenkova, 2009" startWordPosition="165" endWordPosition="168">se segment representations from word vectors, and compare representations based either on all words or limited to head words. Our main finding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features. 1 Introduction Identifying discourse relations is an important task, either to build a discourse parser or to help other NLP systems such as text summarization or question-answering. This task is relatively straightforward when a discourse connective, such as but or because, is used (Pitler and Nenkova, 2009). The identification becomes much more challenging when such an overt marker is lacking, and the relation needs to be inferred through other means. In (1), the presence of the pair of verbs (rose,tumbled) triggers a Contrast relation. Such relations are extremely pervasive in real text corpora: they account for about 50% of all relations in the Penn Discourse Treebank (Prasad et al., 2008). (1) [ Quarterly revenue rose 4.5%, to $2.3 billion from $2.2 billion]arg1 [ For the year, net income tumbled 61% to $86 million, or $1.55 a share]arg2 Pascal Denis MAGNET, INRIA Lille Nord-Europe 59650 Vill</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="2421" citStr="Pitler et al., 2009" startWordPosition="361" endWordPosition="364">pe 59650 Villeneuve d’Ascq - France pascal.denis@inria.fr Automatically classifying implicit relations is difficult in large part because it relies on numerous factors, ranging from syntax, and tense and aspect, to lexical semantics and even world knowledge (Asher and Lascarides, 2003). Consequently, a lot of previous work on this problem have attempted to incorporate some of these information into their systems. These assume the existence of syntactic parsers and lexical databases of various kinds, which are available but for a few languages, and they often involve heavy feature engineering (Pitler et al., 2009; Park and Cardie, 2012). While acknowledging this knowledge bottleneck, this paper focuses on trying to predict implicit relations based on easily accessible lexical features, targeting in particular simple word-based features, such as pairs like (rose,tumbled) in (1). Most previous studies on implicit relations, going back to (Marcu and Echihabi, 2002), incorporate word-based information in the form of word pair features defined across the pair of text segments to be related. Such word pairs are often encoded in a one-hot representation, in which each possible word pair corresponds to a sing</context>
<context position="21927" citStr="Pitler et al., 2009" startWordPosition="3525" endWordPosition="3528"> heads, we count 5,615 different tokens which correspond to 1, 988 codes for 3, 200 clusters and roughly the same number for the others. For the dense representations, the vocabulary size is twice the number of dimensions of the embedding, thus from 50 to 400, or the square of this number, thus from 625 to 40, 000. Other Features We experiment with additional features commonly used for this task: productions rules, average verb phrases length, Levin verb classes, modality, polarity, General Inquirer tags, number, first last and first three words. These feature templates are well described in (Pitler et al., 2009; Park and Cardie, 2012). They all correspond to a one-hot encoding, except average verb phrases length which is continuous. We thus concatenate these features to the lexical ones. 4.2 Model We use the same classification algorithm for comparing all the described feature configurations. Specifically, we train a Maximum Entropy (ME) classifier (aka, logistic regression).6 As in previous studies, we build one binary classifier for each relation. In order to deal with class imbalance, we use a sample weighting scheme: each sample receives a weight inversely proportional to the frequency of its cl</context>
<context position="23417" citStr="Pitler et al., 2009" startWordPosition="3775" endWordPosition="3778">ations. We test statistical significancy of the results using t-test and Wilcoxon test on a split of the test set in 20 folds. Previous studies have tested several algorithms generally concluding that Naive Bayes (NB) gives the best performance (Pitler et al., 2009; Rutherford and Xue, 2014). We found that, when the hyper-parameters of ME are well tuned, the performance are comparable to NB if not better. Note that NB cannot be used with word embeddings representations as it does not handle negative value. Concerning the class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test Temporal 665 93 68 Contingency 3,281 628 276 Comparison 1,894 401 146 Expansion 6,792 1,253 556 Total 12,632 2,375 1,046 Table 2: Number of examples in train, dev, test. 4.3 Penn Discourse Treebank We use the Penn Discourse Treebank (Prasad et al., 2008), a corpus annotated at the discourse level upon the Penn Treebank, givin</context>
<context position="24676" citStr="Pitler et al., 2009" startWordPosition="3974" endWordPosition="3977"> and composed of articles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carry multiple relations, up to four for implicit ones, and the relations are organized into a three-level hierarchy. We keep only true implicit examples and only the first annotation. We focus on the top level relations which correspond to general categories included in most discursive frameworks. Finally, in order to make comparison easier, we choose the most spread split of the data, used in (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014) among others. The amount of data for training (sections 2 − 21), development (00, 01, 23, 24) and evaluation (21, 22) is summarized in table 2. 5 Results We first discuss the models that use only lexical features, defined either over all the words that appear in the arguments or only the head words. We then compare our best performing lexical configurations with the ones that also integrate additional standard features, and to state-of-the-art systems. 5.1 Word Pairs over the Arguments Our first finding in this setting is that feature configur</context>
<context position="34367" citStr="Pitler et al., 2009" startWordPosition="5538" endWordPosition="5541">ts are significant compared to our reproduced systems, they are not with respect to the best systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency8. These very small differences demonstrate that semantic and syntactic properties encoded in these features are already taken into account into the unsupervised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only </context>
<context position="36045" citStr="Pitler et al., 2009" startWordPosition="5808" endWordPosition="5811">ie, 2012; Lin et al., 2009). Interestingly, Park and Cardie (2012) conclude on the worthlessness of word pair features, given the existence of such resources. We showed that provided unsupervised word representations, the opposite was in fact true, as dense word representations capture a lot of syntactic and semantic information. The major problem of standard word pair representations is their sparsity. A line of work is to deal with this issue by adding automatically annotated data from explicit examples (Marcu and Echihabi, 2002), possibly using some kind of filtering or adaptation methods (Pitler et al., 2009; Biran and McKeown, 2013; Braud and Denis, 2014). Another line of work propose to make use of dense representations as Brown clusters in (Rutherford and Xue, 2014). These authors claim that this resource provides word representations that are relevant to the task, a conclusion that we considerably refined. Ji and Eisenstein (2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherf</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1607" citStr="Prasad et al., 2008" startWordPosition="229" endWordPosition="232">ld a discourse parser or to help other NLP systems such as text summarization or question-answering. This task is relatively straightforward when a discourse connective, such as but or because, is used (Pitler and Nenkova, 2009). The identification becomes much more challenging when such an overt marker is lacking, and the relation needs to be inferred through other means. In (1), the presence of the pair of verbs (rose,tumbled) triggers a Contrast relation. Such relations are extremely pervasive in real text corpora: they account for about 50% of all relations in the Penn Discourse Treebank (Prasad et al., 2008). (1) [ Quarterly revenue rose 4.5%, to $2.3 billion from $2.2 billion]arg1 [ For the year, net income tumbled 61% to $86 million, or $1.55 a share]arg2 Pascal Denis MAGNET, INRIA Lille Nord-Europe 59650 Villeneuve d’Ascq - France pascal.denis@inria.fr Automatically classifying implicit relations is difficult in large part because it relies on numerous factors, ranging from syntax, and tense and aspect, to lexical semantics and even world knowledge (Asher and Lascarides, 2003). Consequently, a lot of previous work on this problem have attempted to incorporate some of these information into the</context>
<context position="23944" citStr="Prasad et al., 2008" startWordPosition="3856" endWordPosition="3859">class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test Temporal 665 93 68 Contingency 3,281 628 276 Comparison 1,894 401 146 Expansion 6,792 1,253 556 Total 12,632 2,375 1,046 Table 2: Number of examples in train, dev, test. 4.3 Penn Discourse Treebank We use the Penn Discourse Treebank (Prasad et al., 2008), a corpus annotated at the discourse level upon the Penn Treebank, giving access to a gold syntactic annotation, and composed of articles from the Wall Street Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carry multiple relations, up to four for implicit ones, and the relations are organized into a three-level hierarchy. We keep only true implicit examples and only the first annotation. We focus on the top level relations which correspond to general categories included in most discursiv</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Discovering implicit discourse relations through brown cluster pair representation and coreference patterns.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5165" citStr="Rutherford and Xue, 2014" startWordPosition="779" endWordPosition="782">ng one-hot encodings of word pairs or other low-dimensional word representations. This work also leaves other important questions open. In particular, it is unclear whether all word pairs constructed over the two discourse segments are truly informative and should be included in the model. Given that word embeddings capture latent syntactic and semantic information, yet another important question is to which extent the use of these representations dispenses us from using additional hand-crafted syntactic and semantic features. This paper fills these gaps and significantly extends the work of (Rutherford and Xue, 2014) by explicitly comparing various types of word representations and vector composition methods. Specifically, we investigate three well-known word embeddings, namely Collobert and Weston (Collobert and Weston, 2008), hierarchical log-bilinear model (Mnih and Hinton, 2007) and Hellinger Principal Component Analysis (Lebret and Collobert, 2014), in addition to Brown clusterbased and standard one-hot representations. All these word representations are publicly available for English and can be easily acquired for other languages just using raw text data, thus alleviating the need for hand-crafted l</context>
<context position="8320" citStr="Rutherford and Xue, 2014" startWordPosition="1276" endWordPosition="1279">g to the cluster it belongs to. Given the hierarchical nature of the algorithm, one can create word classes of different levels of granularity, corresponding to bit codes of different sizes. The less clusters, the less fine-grained the distinctions between words but the less sparsity. Note that this kind of representations also yields one-hot encodings but on a much smaller vocabulary size (i.e., the number of clusters). Brown clusters have been used for several NLP tasks, including NER, chunking (Turian et al., 2010), parsing (Koo et al., 2008) and implicit discourse relation classification (Rutherford and Xue, 2014). 2.3 Dense Real-Valued Representations Another approach to induce word representations from raw text is to learn distributed word representations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Collobert and Weston, 2008). The embeddings are learned discriminatively by minimizing a loss between th</context>
<context position="11403" citStr="Rutherford and Xue, 2014" startWordPosition="1751" endWordPosition="1754">sions. First, we compare the use of a single word per segment (roughly, the two main verbs) against that of all the words contained in the two segments. Second, we compare the use of sparse (i.e., one-hot) vs. dense representations for words. As discussed, Brown cluster bit representations are a special (i.e., lowdimensional) version of one-hot encoding. Third, we use two different types of combinations of word vectors to yield segment vector representations: concatenation and Kronecker product. The proposed framework is therefore much more general than the one given in previous work such as (Rutherford and Xue, 2014). 3.1 Notation Our classification inputs are pairs of text segments, the two arguments of the relation to be predicted. Let S1 = {w11, ... , w1.} denote the n words that make up the first segment and S2 = {w21, ... , w2„} the m words in the second segment. That is, we regard segments as bags of words. Let V again denote the word vocabulary, that is the set of all words found in the segments. Sometimes, we will find it useful to refer to a particular subset of V. Let head(·) refer to the function that extracts the head word of segment S,1 and Vh C V the set of head words. As our goal is to comp</context>
<context position="20850" citStr="Rutherford and Xue, 2014" startWordPosition="3351" endWordPosition="3355">lt from raw tokens (one-hot) or Brown clusters (Brown). We group the systems that use embeddings under Embed. When relevant, we indicate the number of dimensions (e.g. Brown 3,200 is the system using Brown clusters with 3, 200 clusters). We use the symbols defined in section 3 to represent the operation linking the arguments representations (e.g. one-hot ® corresponds to the transformation defined by Vh,✶,® when using heads and by Vall,✶,® when using all words). Vocabulary Sizes For one-hot encoding, the case is left intact. We ignore the unknown words when using the Brown clusters following (Rutherford and Xue, 2014). For the word embeddings, we use the mean of the vectors of all words. In order to give an idea of the sparsity of the one-hot encodings, note that we have |V |= 33,649 different tokens considering all implicit examples without filtering. The Brown clusters 4http://metaoptimize.com/projects/wordreprs/ 5http://lebret.ch/words/ 2205 merge these tokens into 3,190 codes (for 3,200 clusters), 393 (1, 000 clusters), 59 (320 clusters) or 16 (100 clusters). For heads, we count 5,615 different tokens which correspond to 1, 988 codes for 3, 200 clusters and roughly the same number for the others. For t</context>
<context position="23089" citStr="Rutherford and Xue, 2014" startWordPosition="3716" endWordPosition="3720">ives a weight inversely proportional to the frequency of its class in the training set. We optimize the hyper-parameters of the algorithm (i.e., the regularization norm: L1 or L2, and its strength) and a filter on the features on the development set, based on the F1 score. Note that filtering is pointless for purely dense representations. We test statistical significancy of the results using t-test and Wilcoxon test on a split of the test set in 20 folds. Previous studies have tested several algorithms generally concluding that Naive Bayes (NB) gives the best performance (Pitler et al., 2009; Rutherford and Xue, 2014). We found that, when the hyper-parameters of ME are well tuned, the performance are comparable to NB if not better. Note that NB cannot be used with word embeddings representations as it does not handle negative value. Concerning the class imbalance issue, the downsampling scheme is the most spread since (Pitler et al., 2009) but it has been shown 6We use the implementation provided in Scikit-Learn (Pedregosa et al., 2011), available at: http://scikit-learn. org/dev/index.html. that oversampling and instance weighting lead to better performance (Li and Nenkova, 2014a). Relation Train Dev Test</context>
<context position="24726" citStr="Rutherford and Xue, 2014" startWordPosition="3982" endWordPosition="3985">et Journal. Five types of examples are distinguished: implicit, explicit, alternative lexicalizations, entity relations, and no relation. Each example could carry multiple relations, up to four for implicit ones, and the relations are organized into a three-level hierarchy. We keep only true implicit examples and only the first annotation. We focus on the top level relations which correspond to general categories included in most discursive frameworks. Finally, in order to make comparison easier, we choose the most spread split of the data, used in (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014) among others. The amount of data for training (sections 2 − 21), development (00, 01, 23, 24) and evaluation (21, 22) is summarized in table 2. 5 Results We first discuss the models that use only lexical features, defined either over all the words that appear in the arguments or only the head words. We then compare our best performing lexical configurations with the ones that also integrate additional standard features, and to state-of-the-art systems. 5.1 Word Pairs over the Arguments Our first finding in this setting is that feature configurations that employ unsupervised word representatio</context>
<context position="27309" citStr="Rutherford and Xue, 2014" startWordPosition="4408" endWordPosition="4411">tems based on dense representations correspond to significant improvements in terms of F1 of about 8 points for Expansion, 7 points for Temporal and 3.5 for Contingency. The gains for Comparison are not statistically significant. All these results are obtained using the normalization to unit vectors possibly combined to the concatenationspecific normalization described in §3.3. Comparing Dense Representations The best results are obtained using the Brown clusters (Brown) showing that this resource merges words in a way that is relevant to the task. Strikingly, the Brown configuration used in (Rutherford and Xue, 2014) (One-hot Brown 3,200 ⊗) does not do better than the raw word pair baselines, except for Expansion. Recall that these authors did not explicitly provide this comparison. While doing a little worse, word embeddings (Embed.) also yield significant improvements for Temporal and Contingency, and smaller improvements for the others. This suggests that, even if they were not built based on discursive criteria, the latent dimensions encode word properties that are relevant to their rhetorical function. The superiority of Brown clusters over word embeddings is in line with the conclusions in (Turian e</context>
<context position="31571" citStr="Rutherford and Xue, 2014" startWordPosition="5098" endWordPosition="5101">m 8 to 13 points in F1 for most relations. Moreover, the best systems are all based on the multiplicative form confirming that this is a better way of representing pairs than simple concatenation when the number of initial dimensions is not too large. 5.3 Adding Other Features Finally, we would like to assess how much improvement can still be obtained by adding other standard features, such as those in §4.1, to wordbased features. Conversely, we want to evaluate how far we are from state-of-the-art performance by just using word representations. We compare our results with those presented in (Rutherford and Xue, 2014) and in (Ji and Eisenstein, 2014), both systems deal with sparsity either by using Brown clusters or by learning task-dependent representations. To make comparison easier we reproduce the experiments in (Rutherford and Xue, 2014) with Naive Bayes (NB)7 and Maximum Entropy (ME) but without their coreference features and using gold syntactic parse. These correspond to the “repr.” lines in table 4. We attribute the small differences observed with NB by the lack of coreference features and/or the use of different filtering thresholds. Concerning the difference between 7Implemented in Scikit-Learn,</context>
<context position="33385" citStr="Rutherford and Xue, 2014" startWordPosition="5381" endWordPosition="5384">s (“only”). Our first finding is that the addition of extra features to our previous word-based only configuration appears to outperform state-of-the art results for Temporal and Contingency, thus giving the best performance to date on these relations. These improvements are significant compared to our reproduced systems. Note that we also outperform the task-dependent embeddings of Ji and Eisenstein (2014), except for Expansion. Our tentative explanation for this is that these authors included Entity relations and coreference features. Note that our system corresponding to a reproduction of (Rutherford and Xue, 2014) gives results similar to the baseline using raw word pairs (Onehot ® + additional features) showing that their improvements were due to other factors, the optimized filter threshold and the coreference features. Overall, the addition of these hand-crafted features to our best systems do not provide improvements as high as one might have hoped. While improvements are significant compared to our reproduced systems, they are not with respect to the best systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency8. These very small differ</context>
<context position="34740" citStr="Rutherford and Xue, 2014" startWordPosition="5601" endWordPosition="5604">vised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Systems using additional features (“+ add.features”), state-of-the art results either reported or reproduced (“repr.”) using Naive Bayes (“NB”) or logistic regression (“ME”) and best system from previous table (“only”). parsers (Li and Nenkova, 2014b; Lin et al., 2009). Feature selec</context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>Attapol Rutherford and Nianwen Xue. 2014. Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Earl B Hunt</author>
</authors>
<title>A computer approach to content analysis: Studies using the general inquirer system.</title>
<date>1963</date>
<booktitle>In Proceedings of the Spring Joint Computer Conference.</booktitle>
<marker>Stone, Hunt, 1963</marker>
<rawString>Philip J. Stone and Earl B. Hunt. 1963. A computer approach to content analysis: Studies using the general inquirer system. In Proceedings of the Spring Joint Computer Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Michael Collins</author>
</authors>
<title>Simple semisupervised pos tagging.</title>
<date>2005</date>
<booktitle>In Proceedings of NAACL Workshop on Vector Space Modeling for NLP.</booktitle>
<contexts>
<context position="37084" citStr="Stratos and Collins, 2005" startWordPosition="5976" endWordPosition="5979">ting each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possibly because their representations are learned on a rather small corpus, the PDTB itself, whereas building this kind of representation requires massive amount of data. Our work also relates to studies comparing unsupervised representations for other NLP tasks such as name entity recognition, chunking (Turian et al., 2010), sentiment analysis (Lebret and Collobert, 2014) or POS tagging (Stratos and Collins, 2005). In particular, we found some similarities between our conclusions and those in (Turian et al., 2010). Our comparison is slightly richer in that it includes different methods of vector compositions and add an extra distributional representation to our comparison (namely, H-PCA). 7 Conclusions and Future Work In this paper, we show that one can reach state-ofthe-art results for implicit discourse relation identification using only shallow lexical features and existing unsupervised word representations thus contradicting previous conclusions on the worthlessness of these features. We carefully </context>
</contexts>
<marker>Stratos, Collins, 2005</marker>
<rawString>Karl Stratos and Michael Collins. 2005. Simple semisupervised pos tagging. In Proceedings of NAACL Workshop on Vector Space Modeling for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8218" citStr="Turian et al., 2010" startWordPosition="1260" endWordPosition="1263">mutual information of bigrams. As a result, each word is associated to a binary code corresponding to the cluster it belongs to. Given the hierarchical nature of the algorithm, one can create word classes of different levels of granularity, corresponding to bit codes of different sizes. The less clusters, the less fine-grained the distinctions between words but the less sparsity. Note that this kind of representations also yields one-hot encodings but on a much smaller vocabulary size (i.e., the number of clusters). Brown clusters have been used for several NLP tasks, including NER, chunking (Turian et al., 2010), parsing (Koo et al., 2008) and implicit discourse relation classification (Rutherford and Xue, 2014). 2.3 Dense Real-Valued Representations Another approach to induce word representations from raw text is to learn distributed word representations (aka word embeddings), which are dense, low-dimensional, and real-valued vectors. These are typically learned using neural language models (Bengio et al., 2003). Each dimension corresponding to a latent feature of the word that captures paradigmatic information. An example of such 2202 embeddings are the so-called Collobert and Weston embeddings (Co</context>
<context position="19223" citStr="Turian et al., 2010" startWordPosition="3073" endWordPosition="3076">adjectival or nominal attribute of copula, the infinitive complementing ”have to” forms and the first head of coordination conjunctions. In case of multiple subtrees, we look for the head of the first independent clause, or, failing that, of the first phrase. Word Representations We use either one-hot encodings or use word embeddings to build denser representations as described in section 3. The Brown clusters (Brown), Collobert-Weston (CnW) representations, and the hierarchical log-bilinear 3https://github.com/jkkummerfeld/nlp-util (HLBL) embeddings correspond to the versions implemented in (Turian et al., 2010)4. They have been built on Reuters English newswire with case left intact. We test versions with 100, 320, 1000 and 3,200 clusters for Brown, with 25, 50, 100 and 200 dimensions for CnW and with 50 and 100 dimensions for HLBL. The Hellinger PCA (HPCA) embeddings come from (Lebret and Collobert, 2014)5 and have been built over the entire English Wikipedia, the Reuters corpus and the Wall Street Journal with all words in lower case. The vocabulary corresponds to the words that appear at least 100 times and normalized frequency is computed with the 10, 000 most frequent words as context words. We</context>
<context position="27921" citStr="Turian et al., 2010" startWordPosition="4503" endWordPosition="4506">e, 2014) (One-hot Brown 3,200 ⊗) does not do better than the raw word pair baselines, except for Expansion. Recall that these authors did not explicitly provide this comparison. While doing a little worse, word embeddings (Embed.) also yield significant improvements for Temporal and Contingency, and smaller improvements for the others. This suggests that, even if they were not built based on discursive criteria, the latent dimensions encode word properties that are relevant to their rhetorical function. The superiority of Brown clusters over word embeddings is in line with the conclusions in (Turian et al., 2010) for two rather different NLP tasks (i.e., NER and chunking). Turian et al. (2010) showed that the optimal word embedding is task dependent. Our experiments suggest that it is relation dependent: the best scores are obtained with HLBL for Temporal, CnW for Contingency, H-PCA for Expansion and CnW (Best Embed. ⊗) and HPCA (Best Embed. ⊕) for Comparison. This again demonstrates that these four relations have to be considered as four distinct tasks. Identifying temporal or causal links is indeed sensitive to very different factors, the former relying more on temporal expressions and temporal orde</context>
<context position="29691" citStr="Turian et al. (2010)" startWordPosition="4782" endWordPosition="4785">ems) against multiplicative ones (⊗ systems), we first note that for raw tokens the concatenated form (one-hot ⊕) yields results that are comparable, and sometimes better, than the standard multiplicative system (one-hot ⊗), while failing to explicitly model word pair interactions. With Brown clusters, the concatenated form Best Brown ⊕ lead to better F1 scores than Best Brown ⊗ except for Contingency. When comparing performance on dev set, we found that the differences between concatenated and multiplicative forms for Brown (excluding Expansion for now) depend on the number of clusters used. Turian et al. (2010) found that the more clusters, the better the performance. This is also the case here with concatenated forms, but not with multiplicative forms. In that case, F1 increases un2207 til 1, 000 clusters and then decreases. There is indeed a trade-off between expressivity and sparsity: having too few clusters means that we loose important distinctions, but having too many clusters leads to a loss in generalization. A similar trend is also found with word embeddings. 5.2 Head Words Only Considering the right part of table 3, the first finding is that performance of systems that use only head words </context>
<context position="36992" citStr="Turian et al., 2010" startWordPosition="5962" endWordPosition="5965">(2014) propose to learn a distributed representation from the syntactic trees representing each argument in way that is more directly related to the task. Although this is an attractive idea, the score on top level PDTB relations are mostly below those reported by (Rutherford and Xue, 2014), possibly because their representations are learned on a rather small corpus, the PDTB itself, whereas building this kind of representation requires massive amount of data. Our work also relates to studies comparing unsupervised representations for other NLP tasks such as name entity recognition, chunking (Turian et al., 2010), sentiment analysis (Lebret and Collobert, 2014) or POS tagging (Stratos and Collins, 2005). In particular, we found some similarities between our conclusions and those in (Turian et al., 2010). Our comparison is slightly richer in that it includes different methods of vector compositions and add an extra distributional representation to our comparison (namely, H-PCA). 7 Conclusions and Future Work In this paper, we show that one can reach state-ofthe-art results for implicit discourse relation identification using only shallow lexical features and existing unsupervised word representations t</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="34458" citStr="Wilson et al., 2005" startWordPosition="5554" endWordPosition="5557">st systems given in table 3. When using all words, we only have a tendency toward significant improvement for Contingency8. These very small differences demonstrate that semantic and syntactic properties encoded in these features are already taken into account into the unsupervised word representations. 6 Related Work Automatically identifying implicit relations is challenging due to the complex nature of the predictors. Previous studies have thus used many features relying on several external resources (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013) as the MPQA lexicon (Wilson et al., 2005) or the General Inquirer lexicon (Stone and Hunt, 1963), or on constituent or dependency 8p = 0.135 with ttest and p = 0.061 with Wilcoxon. 2208 Temporal Contingency Comparison Expansion System F1 F1 F1 F1 (Ji and Eisenstein, 2014) 26.91 51.39 35.84 79.91 (Rutherford and Xue, 2014) 28.69 54.42 39.70 70.23 repr. (Rutherford and Xue, 2014) NB 28.05 52.95 37.38 70.23 repr. (Rutherford and Xue, 2014) ME 24.79 53.39 36.46 50.00 One-hot ⊗ all tokens + add. features 23.26 54.41 34.34 62.57 Best all tokens only 27.96 53.85 34.99 67.42 Best all tokens + add. features 29.30 55.76 36.36 61.76 Table 4: Sy</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
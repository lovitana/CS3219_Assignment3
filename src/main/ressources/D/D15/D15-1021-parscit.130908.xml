<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.993191">
A Survey of Current Datasets for Vision and Language Research
</title>
<author confidence="0.783048333333333">
Francis Ferraro1∗, Nasrin Mostafazadeh2∗, Ting-Hao (Kenneth) Huang3,
Lucy Vanderwende4, Jacob Devlin4, Michel Galley4, Margaret Mitchell4
Microsoft Research
</author>
<affiliation confidence="0.799566">
1 Johns Hopkins University, 2 University of Rochester, 3 Carnegie Mellon University,
</affiliation>
<email confidence="0.710891">
4 Corresponding authors: {lucyv,jdevlin,mgalley,memitc}@microsoft.com
</email>
<sectionHeader confidence="0.971982" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965625">
Integrating vision and language has long
been a dream in work on artificial intel-
ligence (AI). In the past two years, we
have witnessed an explosion of work that
brings together vision and language from
images to videos and beyond. The avail-
able corpora have played a crucial role in
advancing this area of research. In this
paper, we propose a set of quality met-
rics for evaluating and analyzing the vi-
sion &amp; language datasets and categorize
them accordingly. Our analyses show that
the most recent datasets have been us-
ing more complex language and more ab-
stract concepts, however, there are differ-
ent strengths and weaknesses in each.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999157114754099">
Bringing together language and vision in one in-
telligent system has long been an ambition in AI
research, beginning with SHRDLU as one of the
first vision-language integration systems (Wino-
grad, 1972) and continuing with more recent at-
tempts on conversational robots grounded in the
visual world (Kollar et al., 2013; Cantrell et al.,
2010; Matuszek et al., 2012; Kruijff et al., 2007;
Roy et al., 2003). In the past few years, an influx
of new, large vision &amp; language corpora, along-
side dramatic advances in vision research, has
sparked renewed interest in connecting vision and
language. Vision &amp; language corpora now provide
alignments between visual content that can be rec-
ognized with Computer Vision (CV) algorithms
and language that can be understood and generated
using Natural Language Processing techniques.
Fueled in part by the newly emerging data, re-
search that blends techniques in vision and in lan-
guage has increased at an incredible rate. In just
∗F.F. and N.M. contributed equally to this work.
the past year, recent work has proposed meth-
ods for image and video captioning (Fang et al.,
2014; Donahue et al., 2014; Venugopalan et al.,
2015), summarization (Kim et al., 2015), refer-
ence (Kazemzadeh et al., 2014), and question an-
swering (Antol et al., 2015; Gao et al., 2015), to
name just a few. The newly crafted large-scale vi-
sion &amp; language datasets have played a crucial role
in defining this research, serving as a foundation
for training/testing and helping to set benchmarks
for measuring system performance.
Crowdsourcing and large image collections
such as those provided by Flickr1 have made it
possible for researchers to propose methods for vi-
sion and language tasks alongside an accompany-
ing dataset. However, as more and more datasets
have emerged in this space, it has become un-
clear how different methods generalize beyond the
datasets they are evaluated on, and what data may
be useful for moving the field beyond a single task,
towards solving larger AI problems.
In this paper, we take a step back to document
this moment in time, making a record of the ma-
jor available corpora that are driving the field. We
provide a quantitative analysis of each of these
corpora in order to understand the characteristics
of each, and how they compare to one another.
The quality of a dataset must be measured and
compared to related datasets, as low quality data
may distort an entire subfield. We propose a set of
criteria for analyzing, evaluating and comparing
the quality of vision &amp; language datasets against
each other. Knowing the details of a dataset com-
pared to similar datasets allows researchers to de-
fine more precisely what task(s) they are trying to
solve, and select the dataset(s) best suited to their
goals, while being aware of the implications and
biases the datasets could impose on a task.
We categorize the available datasets into three
major classes and evaluate them against these cri-
</bodyText>
<footnote confidence="0.96239">
1 http://www.flickr.com
</footnote>
<page confidence="0.919585">
207
</page>
<note confidence="0.9045435">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 207–213,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999875">
teria. The datasets we present here were chosen
because they are all available to the community
and cover the data that has been created to sup-
port the recent focus on image captioning work.
More importantly, we provide an evolving web-
site2 containing pointers and references to many
more vision-to-language datasets, which we be-
lieve will be valuable in unifying the quickly ex-
panding research tasks in language and vision.
</bodyText>
<sectionHeader confidence="0.963177" genericHeader="method">
2 Quality Criteria for Language &amp;
Vision Datasets
</sectionHeader>
<bodyText confidence="0.99995365625">
The quality of a dataset is highly dependent on
the sampling and scraping techniques used early
in the data collection process. However, the con-
tent of datasets can play a major role in narrowing
the focus of the field. Datasets are affected by both
reporting bias (Gordon and Durme, 2013), where
the frequency with which people write about ac-
tions, events, or states does not directly reflect
real-world frequencies of those phenomena; they
are also affected by photographer’s bias (Torralba
and Efros, 2011), where photographs are some-
what predictable within a given domain. This sug-
gests that new datasets may be useful towards the
larger AI goal if provided alongside a set of quanti-
tative metrics that show how they compare against
similar corpora, as well as more general “back-
ground” corpora. Such metrics can be used as in-
dicators of dataset bias and language richness. At
a higher level, we argue that clearly defined met-
rics are necessary to provide quantitative measure-
ments of how a new dataset compares to previous
work. This helps clarify and benchmark how re-
search is progressing towards a broader AI goal as
more and more data comes into play.
In this section, we propose a set of such metrics
that characterize vision &amp; language datasets. We
focus on methods to measure language quality that
can be used across several corpora. We also briefly
examine metrics for vision quality. We evaluate
several recent datasets based on all proposed met-
rics in Section 4, with results reported in Tables 1,
2, and Figure 1.
</bodyText>
<subsectionHeader confidence="0.998072">
2.1 Language Quality
</subsectionHeader>
<bodyText confidence="0.999776">
We define the following criteria for evaluating the
captions or instructions of the datasets:
</bodyText>
<listItem confidence="0.9921745">
• Vocabulary Size (#vocab), the number of
unique vocabulary words.
</listItem>
<footnote confidence="0.587868">
2http://visionandlanguage.net
</footnote>
<listItem confidence="0.9997675">
• Syntactic Complexity (Frazier, Yngve) mea-
sures the amount of embedding/branching in a
sentence’s syntax. We report mean Yngve (Yngve,
1960) and Frazier measurements (Frazier, 1985);
each provides a different counting on the number
of nodes in the phrase markers of syntactic trees.
• Part of Speech Distribution measures the dis-
tribution of nouns, verbs, adjectives, and other
parts of speech.
• Abstract:Concrete Ratio (#Conc, #Abs,
%Abs) indicates the range of visual and non-visual
concepts the dataset covers. Abstract terms are
ideas or concepts, such as ‘love’ or ‘think’ and
concrete terms are all the objects or events that are
mainly available to the senses. For this purpose,
we use a list of most common abstract terms in En-
glish (Vanderwende et al., 2015), and define con-
crete terms as all other words except for a small
set of function words.
• Average Sentence Length (Sent Len.) shows
how rich and descriptive the sentences are.
• Perplexity provides a measure of data skew
</listItem>
<bodyText confidence="0.899982857142857">
by measuring how expected sentences are from
one corpus according to a model trained on an-
other corpus. We analyze perplexity (Ppl) for each
dataset against a 5-gram language model learned
on a generic 30B words English dataset. We
further analyze pair-wise perplexity of datasets
against each other in Section 4.
</bodyText>
<subsectionHeader confidence="0.999472">
2.2 Vision Quality
</subsectionHeader>
<bodyText confidence="0.9999755">
Our focus in this survey is mainly on language,
however, the characteristics of images or videos
and their corresponding annotations is as impor-
tant in vision &amp; language research. The quality of
vision in a dataset can be characterized in part by
the variety of visual subjects and scenes provided,
as well as the richness of the annotations (e.g., seg-
mentation using bounding boxes (BB) or visual de-
pendencies between boxes). Moreover, a vision
corpus can use abstract or real images (Abs/Real).
</bodyText>
<sectionHeader confidence="0.988965" genericHeader="method">
3 The Available Datasets
</sectionHeader>
<bodyText confidence="0.99976225">
We group a representative set of available datasets
based on their content. For a complete list of
datasets and their descriptions, please refer to the
supplementary website.2
</bodyText>
<subsectionHeader confidence="0.992858">
3.1 Captioned Images
</subsectionHeader>
<bodyText confidence="0.9998625">
Several recent vision &amp; language datasets provide
one or multiple captions per image. The captions
</bodyText>
<page confidence="0.990164">
208
</page>
<bodyText confidence="0.999952833333333">
of these datasets are either the original photo ti-
tle and descriptions provided by online users (Or-
donez et al., 2011; Thomee et al., 2015), or the
captions generated by crowd workers for existing
images. The former datasets tend to be larger in
size and contain more contextual descriptions.
</bodyText>
<subsubsectionHeader confidence="0.344925">
3.1.1 User-generated Captions
</subsubsectionHeader>
<listItem confidence="0.970193135135135">
• SBU Captioned Photo Dataset (Ordonez et al.,
2011) contains 1 million images with original user
generated captions, collected in the wild by sys-
tematic querying of Flickr. This dataset is col-
lected by querying Flickr for specific terms such as
objects and actions and then filtered images with
descriptions longer than certain mean length.
• D´ej`a Images Dataset (Chen et al., 2015) con-
sists of 180K unique user-generated captions as-
sociated with 4M Flickr images, where one cap-
tion is aligned with multiple images. This dataset
was collected by querying Flickr for 693 high fre-
quency nouns, then further filtered to have at least
one verb and be judged as “good” captions by
workers on Amazon’s Mechanical Turk (Turkers).
3.1.2 Crowd-sourced Captions
• UIUC Pascal Dataset (Farhadi et al., 2010) is
probably one of the first datasets aligning images
with captions. Pascal dataset contains 1,000 im-
ages with 5 sentences per image.
• Flickr 30K Images (Young et al., 2014) extends
previous Flickr datasets (Rashtchian et al., 2010),
and includes 158,915 crowd-sourced captions that
describe 31,783 images of people involved in ev-
eryday activities and events.
• Microsoft COCO Dataset (MS COCO) (Lin et
al., 2014) includes complex everyday scenes with
common objects in naturally occurring contexts.
Objects in the scene are labeled using per-instance
segmentations. In total, this dataset contains pho-
tos of 91 basic object types with 2.5 million la-
beled instances in 328k images, each paired with 5
captions. This dataset gave rise to the CVPR 2015
image captioning challenge and is continuing to be
a benchmark for comparing various aspects of vi-
sion and language research.
• Abstract Scenes Dataset (Clipart) (Zitnick et
</listItem>
<bodyText confidence="0.84498">
al., 2013) was created with the goal of represent-
ing real-world scenes with clipart to study scene
semantics isolated from object recognition and
segmentation issues in image processing. This re-
moves the burden of low-level vision tasks. This
dataset contains 10,020 images of children playing
outdoors associated with total 60,396 descriptions.
</bodyText>
<subsectionHeader confidence="0.936226">
3.1.3 Captions of Densely Labeled Images
</subsectionHeader>
<bodyText confidence="0.9998494">
Existing caption datasets provide images paired
with captions, but such brief image descriptions
capture only a subset of the content in each image.
Measuring the magnitude of the reporting bias in-
herent in such descriptions helps us to understand
the discrepancy between what we can learn for
the specific task of image captioning versus what
we can learn more generally from the photographs
people take. One dataset useful to this end pro-
vides image annotation for content selection:
</bodyText>
<listItem confidence="0.788449">
• Microsoft Research Dense Visual Annotation
</listItem>
<bodyText confidence="0.916326444444444">
Corpus (Yatskar et al., 2014) provides a set of 500
images from the Flickr 8K dataset (Rashtchian et
al., 2010) that are densely labeled with 100,000
textual labels, with bounding boxes and facets an-
notated for each object. This approximates “gold
standard” visual recognition.
To get a rough estimate of the reporting bias in
image captioning, we determined the percentage
of top-level objects3 that are mentioned in the cap-
tions for this dataset out of all the objects that are
annotated. Of the average 8.04 available top-level
objects in the image, each of the captions only re-
ports an average of 2.7 of these objects.4 A more
detailed analysis of reporting bias is beyond the
scope of this paper, but we found that many of the
biases (e.g., people selection) found with abstract
scenes (Zitnick et al., 2013) are also present with
photos.
</bodyText>
<subsectionHeader confidence="0.999371">
3.2 Video Description and Instruction
</subsectionHeader>
<bodyText confidence="0.998912142857143">
Video datasets aligned with descriptions (Chen et
al., 2010; Rohrbach et al., 2012; Regneri et al.,
2013; Naim et al., 2015; Malmaud et al., 2015)
generally represent limited domains and small lex-
icons, which is due to the fact that video process-
ing and understanding is a very compute-intensive
task. Available datasets include:
</bodyText>
<listItem confidence="0.999268333333333">
• Short Videos Described with Sentences (Yu
and Siskind, 2013) includes 61 video clips (each
35 seconds in length, filmed in three different
</listItem>
<footnote confidence="0.936420777777778">
3This visual annotation consists of a two-level hierarchy,
where multiple Turkers enumerated and located objects and
stuff in each image, and these objects were then further la-
beled with finer-grained object information (Has attributes).
4We did not use an external synonym or paraphrasing re-
source to perform the matching between labels and captions,
as the dataset itself provides paraphrases for each object: each
object is labeled by multiple Turkers, who labeled Isa rela-
tions (e.g., “eagle” is a “bird”).
</footnote>
<page confidence="0.979337">
209
</page>
<table confidence="0.999889666666667">
Size(k) Language Vision
Dataset Img Txt Frazier Yngve Vocab Sent #Conc #Abs %Abs Ppl (A)bs/ BB
Size (k) Len. (R)eal
Balanced Brown - 52 18.5 77.21 47.7 20.82 40411 7264 15.24% 194 - -
User-Gen SBU 1000 1000 9.70 26.03 254.6 13.29 243940 9495 3.74% 346 R -
Deja 4000 180 4.13 4.71 38.3 4.10 34581 3714 9.70% 184 R -
Crowd- Pascal 1 5 8.03 25.78 3.4 10.78 2741 591 17.74% 123 R -
sourced
Flickr30K 32 159 9.50 27.00 20.3 12.98 17214 3033 14.98% 118 R -
COCO 328 2500 9.11 24.92 24.9 11.30 21607 3218 12.96% 121 R Y
Clipart 10 60 6.50 12.24 2.7 7.18 2202 482 17.96% 126 A Y
Video VDC 2 85 6.71 15.18 13.6 7.97 11795 1741 12.86% 148 R -
Beyond VQA 10 330 6.50 14.00 6.2 7.58 5019 1194 19.22% 113 A/R -
CQA 123 118 9.69 11.18 10.2 8.65 8501 1636 16.14% 199 R Y
VML 11 360 6.83 12.72 11.2 7.56 9220 1914 17.19% 110 R Y
</table>
<tableCaption confidence="0.995310333333333">
Table 1: Summary of statistics and quality metrics of a sample set of major datasets. For Brown, we report Frazier and Yngve
scores on automatically acquired parses, but we also compute them for the 24K sentences with gold parses: in this setting, the
mean Frazier score is 15.26 while the mean Yngve score is 58.48.
</tableCaption>
<bodyText confidence="0.999477">
outdoor environments), showing multiple simul-
taneous events between a subset of four objects:
a person, a backpack, a chair, and a trash-can.
Each video was manually annotated (with very re-
stricted grammar and lexicon) with several sen-
tences describing what occurs in the video.
</bodyText>
<listItem confidence="0.851788">
• Microsoft Research Video Description Cor-
</listItem>
<bodyText confidence="0.948134888888889">
pus (MS VDC) (Chen and Dolan, 2011) con-
tains parallel descriptions (85,550 English ones)
of 2,089 short video snippets (10-25 seconds
long). The descriptions are one sentence sum-
maries about the actions or events in the video
as described by Amazon Turkers. In this dataset,
both paraphrase and bilingual alternatives are cap-
tured, hence, the dataset can be useful translation,
paraphrasing, and video description purposes.
</bodyText>
<subsectionHeader confidence="0.997014">
3.3 Beyond Visual Description
</subsectionHeader>
<bodyText confidence="0.9997573">
Recent work has demonstrated that n-gram lan-
guage modeling paired with scene-level under-
standing of an image trained on large enough
datasets can result in reasonable automatically
generated captions (Fang et al., 2014; Donahue
et al., 2014). Some works have proposed to step
beyond description generation, towards deeper AI
tasks such as question answering (Ren et al., 2015;
Malinowski and Fritz, 2014). We present two of
these attempts below:
</bodyText>
<listItem confidence="0.964265384615385">
• Visual Madlibs Dataset (VML) (Yu et al.,
2015) is a subset of 10,783 images from the MS
COCO dataset which aims to go beyond describ-
ing which objects are in the image. For a given
image, three Amazon Turkers were prompted
to complete one of 12 fill-in-the-blank template
questions, such as ‘when I look at this picture,
I feel –’, selected automatically based on the im-
age content. This dataset contains a total of
360,001 MadLib question and answers.
• Visual Question Answering (VQA) Dataset
(Antol et al., 2015) is created for the task of open-
ended VQA, where a system can be presented with
</listItem>
<bodyText confidence="0.909452833333333">
an image and a free-form natural-language ques-
tion (e.g., ‘how many people are in the photo?’),
and should be able to answer the question. This
dataset contains both real images and abstract
scenes, paired with questions and answers. Real
images include 123,285 images from MS COCO
dataset, and 10,000 clip-art abstract scenes, made
up from 20 ‘paperdoll’ human models with ad-
justable limbs and over 100 objects and 31 ani-
mals. Amazon Turkers were prompted to create
‘interesting’ questions, resulting in 215,150 ques-
tions and 430,920 answers.
</bodyText>
<listItem confidence="0.701836">
• Toronto COCO-QA Dataset (CQA) (Ren et
</listItem>
<bodyText confidence="0.982973333333333">
al., 2015) is also a visual question answering
dataset, where the questions are automatically
generated from image captions of MS COCO
dataset. This dataset has a total of 123,287 im-
ages with 117,684 questions with one-word an-
swer about objects, numbers, colors, or locations.
</bodyText>
<sectionHeader confidence="0.993276" genericHeader="evaluation">
4 Analysis
</sectionHeader>
<bodyText confidence="0.99975075">
We analyze the datasets introduced in Section 3
according to the metrics defined in Section 2, us-
ing the Stanford CoreNLP suite to acquire parses
and part-of-speech tags (Manning et al., 2014).
We also include the Brown corpus (Francis and
Kucera, 1979; Marcus et al., 1999) as a reference
point. We find evidence that the VQA dataset cap-
tures more abstract concepts than other datasets,
with almost 20% of the words found in our ab-
stract concept resource. The Deja corpus has the
least number of abstract concepts, followed by
COCO and VDC. This reflects differences in col-
</bodyText>
<page confidence="0.995724">
210
</page>
<table confidence="0.999314818181818">
Brown Clipart Coco Flickr30K CQA VDC VQA Pascal SBU
Brown 237.1 99.6 560.8 405.0 354.039 187.3 126.5 47.8 621.5
Clipart 233.6 11.2 117.4 109.4 210.8 82.5 114.7 28.7 130.6
Coco 274.6 59.2 36.2 75.3 137.0 87.1 236.9 39.3 111.0
Flickr30K 247.8 78.5 54.3 37.8 181.5 72.1 192.2 39.9 125.0
CQA 489.4 186.1 137.0 244.5 33.8 259.0 72.1 74.9 200.1
VDC 200.5 52.4 61.5 51.1 289.9 30.0 180.1 28.7 154.5
VQA 425.9 368.8 366.8 665.8 317.7 455.0 19.6 119.3 281.0
Pascal 265.2 64.5 43.2 63.4 174.2 83.0 228.2 36.0 105.3
SBU 473.9 107.1 346.4 344.0 328.5 230.7 194.3 78.2 119.8
#vocab 14.0k 1.1k 13k 9.4k 5.3k 4.9k 1.4k 1.0k 65.1k
</table>
<tableCaption confidence="0.971471">
Table 2: Perplexities across corpora, where rows represent test sets (20k sentences) and columns training sets (remaining
sentences). To make perplexities comparable, we used the same vocabulary frequency cutoff of 3. All models are 5-grams.
</tableCaption>
<table confidence="0.989435">
Brown SBU Deja Pascal
●
● ● ●
Flickr30K COCO Clipart VDC
● ● ● ●
VQA CQA VML
Simplified Part
● ● ● of Speech
● N J
</table>
<equation confidence="0.420782333333333">
V O
N V J O N V J O N V J O
POS
</equation>
<figureCaption confidence="0.9995908">
Figure 1: Simplified part-of-speech distributions for the eight
datasets. We include the POS tags from the balanced Brown
corpus (Marcus et al., 1999) to contextualize any very shal-
low syntactic biases. We mapped all nouns to “N,” all verbs
to “V,” all adjectives to “J” and all other POS tags to “O.”
</figureCaption>
<bodyText confidence="0.999978404761905">
lecting the various corpora: For example, the Deja
corpus was collected to find specifically visual
phrases that can be used to describe multiple im-
ages. This corpus also has the most syntactically
simple phrases, as measured by both Frazier and
Yngve; this is likely caused by the phrases needing
to be general enough to capture multiple images.
The most syntactically complex sentences are
found in the Flickr30K, COCO and CQA datasets.
However, the CQA dataset suffers from a high per-
plexity against a background corpus relative to the
other datasets, at odds with relatively short sen-
tence lengths. This suggests that the automatic
caption-to-question conversion may be creating
unexpectedly complex sentences that are less re-
flective of general language usage. In contrast,
the COCO and Flickr30K dataset’s relatively high
syntactic complexity is in line with their relatively
high sentence length.
Table 2 illustrates further similarities between
datasets, and a more fine-grained use of perplex-
ity to measure the usefulness of a given train-
ing set for predicting words of a given test set.
Some datasets such as COCO, Flickr30K, and Cli-
part are generally more useful as out-domain data
compared to the QA datasets. Test sets for VQA
and CQA are quite idiosyncratic and yield poor
perplexity unless trained on in-domain data. As
shown in Figure 1, the COCO dataset is balanced
across POS tags most similarly to the balanced
Brown corpus (Marcus et al., 1999). The Clipart
dataset provides the highest proportion of verbs,
which often correspond to actions/poses in vision
research, while the Flickr30K corpus provides the
most nouns, which often correspond to object/stuff
categories in vision research.
We emphasize here that the distinction between
a qualitatively good or bad dataset is task depen-
dent. Therefore, all these metrics and the obtained
results provide the researchers with an objective
set of criteria so that they can make the decision
whether a dataset is suitable to a particular task.
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.994582307692308">
We detail the recent growth of vision &amp; language
corpora and compare and contrast several recently
released large datasets. We argue that newly in-
troduced corpora may measure how they compare
to similar datasets by measuring perplexity, syn-
tactic complexity, abstract:concrete word ratios,
among other metrics. By leveraging such met-
rics and comparing across corpora, research can
be sensitive to how datasets are biased in different
directions, and define new corpora accordingly.
Acknowledgements We thank the three anony-
mous reviewers for their feedback, and Benjamin
Van Durme for discussions on reporting bias.
</bodyText>
<figure confidence="0.998664333333333">
Frequency, by dataset 0.4
0.2
0.0
0.4
0.2
0.0
0.4
0.2
0.0
</figure>
<page confidence="0.996554">
211
</page>
<sectionHeader confidence="0.960003" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998233441441441">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: visual question an-
swering. arXiv preprint arXiv:1505.00468.
Rehj Cantrell, Matthias Scheutz, Paul W. Schermer-
horn, and Xuan Wu. 2010. Robust spoken instruc-
tion understanding for hri. In Pamela J. Hinds, Hi-
roshi Ishiguro, Takayuki Kanda, and Peter H. Kahn
Jr., editors, HRI, pages 275–282. ACM.
David L. Chen and William B. Dolan. 2011. Col-
lecting highly parallel data for paraphrase evalua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 190–200, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. J. Artif.
Int. Res., 37(1):397–436, January.
Jianfu Chen, Polina Kuznetsova, David Warren, and
Yejin Choi. 2015. D´ej`a image-captions: A cor-
pus of expressive descriptions in repetition. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
504–514, Denver, Colorado, May–June. Association
for Computational Linguistics.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2014. Long-term
recurrent convolutional networks for visual recogni-
tion and description. CoRR, abs/1411.4389.
Hao Fang, Saurabh Gupta, Forrest N. Iandola, Ru-
pesh Srivastava, Li Deng, Piotr Doll´ar, Jianfeng
Gao, Xiaodong He, Margaret Mitchell, John C. Platt,
C. Lawrence Zitnick, and Geoffrey Zweig. 2014.
From captions to visual concepts and back. CoRR,
abs/1411.4952.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European Conference on
Computer Vision: Part IV, ECCV’10, pages 15–29,
Berlin, Heidelberg. Springer-Verlag.
W Nelson Francis and Henry Kucera. 1979. Brown
Corpus manual: Manual of information to accom-
pany a standard corpus of present-day edited Amer-
ican English for use with digital computers. Brown
University, Providence, Rhode Island, USA.
L. Frazier. 1985. Syntactic complexity. In D. R.
Dowty, L. Karttunen, and A. M. Zwicky, editors,
Natural Language Parsing: Psychological, Compu-
tational, and Theoretical Perspectives, pages 129–
189. Cambridge University Press, Cambridge.
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang,
Lei Wang, and Wei Xu. 2015. Are you talking to a
machine? dataset and methods for multilingual im-
age question answering. CoRR, abs/1505.05612.
Jonathan Gordon and Benjamin Van Durme. 2013.
Reporting bias and knowledge extraction. In Auto-
mated Knowledge Base Construction (AKBC) 2013:
The 3rd Workshop on Knowledge Extraction, at
CIKM 2013, AKBC’13.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara Berg. 2014. ReferItGame: Refer-
ring to Objects in Photographs of Natural Scenes.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 787–798, Doha, Qatar, October.
Association for Computational Linguistics.
Gunhee Kim, Seungwhan Moon, and Leonid Sigal.
2015. Joint Photo Stream and Blog Post Summa-
rization and Exploration. In 28th IEEE Conference
on Computer Vision and Pattern Recognition (CVPR
2015).
Thomas Kollar, Jayant Krishnamurthy, and Grant
Strimel. 2013. Toward interactive grounded lan-
guage acquisition. In Robotics: Science and Sys-
tems.
Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt,
and Henrik I. Christensen. 2007. Situated dialogue
and spatial organization: What, where... and why?
International Journal ofAdvanced Robotic Systems,
Special Issue on Human and Robot Interactive Com-
munication, 4(2), March.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollar,
and C. Lawrence Zitnick. 2014. Microsoft COCO:
common objects in context. CoRR, abs/1405.0312.
Mateusz Malinowski and Mario Fritz. 2014. A multi-
world approach to question answering about real-
world scenes based on uncertain input. In Advances
in Neural Information Processing Systems 27, pages
1682–1690.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod,
Nicholas Johnston, Andrew Rabinovich, and Kevin
Murphy. 2015. Whats cookin? interpreting cook-
ing videos using text, speech and vision. In North
American Chapter of the Association for Computa-
tional Linguistics Human Language Technologies
(NAACL HLT 2015), May 31 - June 5, 2015, Denver,
Colorado USA.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
</reference>
<page confidence="0.989961">
212
</page>
<reference confidence="0.998951296296296">
Mitchell Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Brown cor-
pus, treebank-3.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded
Attribute Learning. In Proc. of the 2012 Interna-
tional Conference on Machine Learning, Edinburgh,
Scotland, June.
Iftekhar Naim, Young C. Song, Qiguang Liu, Liang
Huang, Henry Kautz, Jiebo Luo, and Daniel Gildea.
2015. Discriminative unsupervised alignment of
natural language instructions with corresponding
video segments. In North American Chapter of the
Association for Computational Linguistics Human
Language Technologies (NAACL HLT 2015), May
31 - June 5, 2015, Denver, Colorado USA.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon’s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, CSLDAMT ’10, pages 139–147, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics (TACL), 1:25–36.
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.
Question answering about images using visual se-
mantic embeddings. In Deep Learning Workshop,
ICML 2015.
Marcus Rohrbach, Sikandar Amin, Mykhaylo An-
driluka, and Bernt Schiele. 2012. A database for
fine grained activity detection of cooking activities.
In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, IEEE, June.
Deb Roy, Kai-Yuh Hsiao, and Nikolaos Mavridis.
2003. Conversational robots: Building blocks for
grounding word meaning. In Proceedings of the
HLT-NAACL 2003 Workshop on Learning Word
Meaning from Non-linguistic Data - Volume 6, HLT-
NAACL-LWM ’04, pages 70–77, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bart Thomee, David A. Shamma, Gerald Fried-
land, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. 2015. The new data
and new challenges in multimedia research. arXiv
preprint arXiv:1503.01817.
A. Torralba and A. A. Efros. 2011. Unbiased look
at dataset bias. In Proceedings of the 2011 IEEE
Conference on Computer Vision and Pattern Recog-
nition, CVPR ’11, pages 1521–1528, Washington,
DC, USA. IEEE Computer Society.
Lucy Vanderwende, Arul Menezes, and Chris Quirk.
2015. An amr parser for english, french, german,
spanish and japanese and a new amr-annotated cor-
pus. Proceedings of NAACL 2015, June.
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2015. Translating videos to natural lan-
guage using deep recurrent neural networks. In
Proceedings the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics – Human Language Technolo-
gies (NAACL HLT 2015), pages 1494–1504, Denver,
Colorado, June.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, New York.
Mark Yatskar, Michel Galley, Lucy Vanderwende, and
Luke Zettlemoyer. 2014. See no evil, say no evil:
Description generation from densely labeled images.
In Proceedings of the Third Joint Conference on
Lexical and Computational Semantics (*SEM 2014),
pages 110–120, Dublin, Ireland, August. Associa-
tion for Computational Linguistics and Dublin City
University.
Victor H. Yngve. 1960. A model and an hypothesis
for language structure. Proceedings of the American
Philosophical Society, 104:444–466.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53–63,
Sofia, Bulgaria. Association for Computational Lin-
guistics. Best Paper Award.
Licheng Yu, Eunbyung Park, Alexander C. Berg, and
Tamara L. Berg. 2015. Visual Madlibs: Fill in the
blank Image Generation and Question Answering.
arXiv preprint arXiv:1506.00278.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE International Conference
on Computer Vision, ICCV 2013, Sydney, Australia,
December 1-8, 2013, pages 1681–1688.
</reference>
<page confidence="0.999377">
213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.298534">
<title confidence="0.9484655">A Survey of Current Datasets for Vision and Language Research Nasrin Ting-Hao (Kenneth)</title>
<author confidence="0.999788">Jacob Michel Margaret</author>
<affiliation confidence="0.794702">Microsoft Research Hopkins University, of Rochester, Mellon</affiliation>
<email confidence="0.579292">authors:</email>
<abstract confidence="0.997291470588235">Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision &amp; language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanislaw Antol</author>
<author>Aishwarya Agrawal</author>
<author>Jiasen Lu</author>
<author>Margaret Mitchell</author>
<author>Dhruv Batra</author>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
</authors>
<title>VQA: visual question answering. arXiv preprint arXiv:1505.00468.</title>
<date>2015</date>
<contexts>
<context position="2311" citStr="Antol et al., 2015" startWordPosition="362" endWordPosition="365">ntent that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset. However, as more and more datasets have emerged in this space, it has become unclear how different methods generalize beyond the datasets the</context>
<context position="16256" citStr="Antol et al., 2015" startWordPosition="2659" endWordPosition="2662">tion answering (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: • Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as ‘when I look at this picture, I feel –’, selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers. • Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (e.g., ‘how many people are in the photo?’), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 ‘paperdoll’ human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create ‘interesting’ questions, resulting in 215,150 questio</context>
</contexts>
<marker>Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, Parikh, 2015</marker>
<rawString>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: visual question answering. arXiv preprint arXiv:1505.00468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rehj Cantrell</author>
<author>Matthias Scheutz</author>
<author>Paul W Schermerhorn</author>
<author>Xuan Wu</author>
</authors>
<title>Robust spoken instruction understanding for hri.</title>
<date>2010</date>
<pages>275--282</pages>
<editor>In Pamela J. Hinds, Hiroshi Ishiguro, Takayuki Kanda, and Peter H. Kahn Jr., editors, HRI,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="1375" citStr="Cantrell et al., 2010" startWordPosition="207" endWordPosition="210"> evaluating and analyzing the vision &amp; language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an</context>
</contexts>
<marker>Cantrell, Scheutz, Schermerhorn, Wu, 2010</marker>
<rawString>Rehj Cantrell, Matthias Scheutz, Paul W. Schermerhorn, and Xuan Wu. 2010. Robust spoken instruction understanding for hri. In Pamela J. Hinds, Hiroshi Ishiguro, Takayuki Kanda, and Peter H. Kahn Jr., editors, HRI, pages 275–282. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>190--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14875" citStr="Chen and Dolan, 2011" startWordPosition="2438" endWordPosition="2441">f a sample set of major datasets. For Brown, we report Frazier and Yngve scores on automatically acquired parses, but we also compute them for the 24K sentences with gold parses: in this setting, the mean Frazier score is 15.26 while the mean Yngve score is 58.48. outdoor environments), showing multiple simultaneous events between a subset of four objects: a person, a backpack, a chair, and a trash-can. Each video was manually annotated (with very restricted grammar and lexicon) with several sentences describing what occurs in the video. • Microsoft Research Video Description Corpus (MS VDC) (Chen and Dolan, 2011) contains parallel descriptions (85,550 English ones) of 2,089 short video snippets (10-25 seconds long). The descriptions are one sentence summaries about the actions or events in the video as described by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically </context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 190–200, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="12462" citStr="Chen et al., 2010" startWordPosition="2010" endWordPosition="2013">in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with f</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. J. Artif. Int. Res., 37(1):397–436, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfu Chen</author>
<author>Polina Kuznetsova</author>
<author>David Warren</author>
<author>Yejin Choi</author>
</authors>
<title>D´ej`a image-captions: A corpus of expressive descriptions in repetition.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>504--514</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="9262" citStr="Chen et al., 2015" startWordPosition="1497" endWordPosition="1500">users (Ordonez et al., 2011; Thomee et al., 2015), or the captions generated by crowd workers for existing images. The former datasets tend to be larger in size and contain more contextual descriptions. 3.1.1 User-generated Captions • SBU Captioned Photo Dataset (Ordonez et al., 2011) contains 1 million images with original user generated captions, collected in the wild by systematic querying of Flickr. This dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends pr</context>
</contexts>
<marker>Chen, Kuznetsova, Warren, Choi, 2015</marker>
<rawString>Jianfu Chen, Polina Kuznetsova, David Warren, and Yejin Choi. 2015. D´ej`a image-captions: A corpus of expressive descriptions in repetition. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 504–514, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Lisa Anne Hendricks</author>
<author>Sergio Guadarrama</author>
<author>Marcus Rohrbach</author>
<author>Subhashini Venugopalan</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Long-term recurrent convolutional networks for visual recognition and description.</title>
<date>2014</date>
<location>CoRR, abs/1411.4389.</location>
<contexts>
<context position="2169" citStr="Donahue et al., 2014" startWordPosition="339" endWordPosition="342">ion research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset.</context>
<context position="15535" citStr="Donahue et al., 2014" startWordPosition="2537" endWordPosition="2540"> English ones) of 2,089 short video snippets (10-25 seconds long). The descriptions are one sentence summaries about the actions or events in the video as described by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014; Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: • Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as ‘when I look at this picture, I feel –’, selected automatically based on the image content. This datase</context>
</contexts>
<marker>Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2014</marker>
<rawString>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2014. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Fang</author>
<author>Saurabh Gupta</author>
<author>Forrest N Iandola</author>
<author>Rupesh Srivastava</author>
<author>Li Deng</author>
<author>Piotr Doll´ar</author>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Margaret Mitchell</author>
<author>John C Platt</author>
<author>C Lawrence Zitnick</author>
<author>Geoffrey Zweig</author>
</authors>
<title>From captions to visual concepts and back.</title>
<date>2014</date>
<location>CoRR, abs/1411.4952.</location>
<marker>Fang, Gupta, Iandola, Srivastava, Deng, Doll´ar, Gao, He, Mitchell, Platt, Zitnick, Zweig, 2014</marker>
<rawString>Hao Fang, Saurabh Gupta, Forrest N. Iandola, Rupesh Srivastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, and Geoffrey Zweig. 2014. From captions to visual concepts and back. CoRR, abs/1411.4952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV’10,</booktitle>
<pages>15--29</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="9676" citStr="Farhadi et al., 2010" startWordPosition="1565" endWordPosition="1568"> dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Brown Corpus manual: Manual of information to accompany a standard corpus of present-day edited American English for use with digital computers.</title>
<date>1979</date>
<institution>Brown University,</institution>
<location>Providence, Rhode Island, USA.</location>
<contexts>
<context position="17460" citStr="Francis and Kucera, 1979" startWordPosition="2857" endWordPosition="2860">g in 215,150 questions and 430,920 answers. • Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. 4 Analysis We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979; Marcus et al., 1999) as a reference point. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with almost 20% of the words found in our abstract concept resource. The Deja corpus has the least number of abstract concepts, followed by COCO and VDC. This reflects differences in col210 Brown Clipart Coco Flickr30K CQA VDC VQA Pascal SBU Brown 237.1 99.6 560.8 405.0 354.039 187.3 126.5 47.8 621.5 Clipart 233.6 11.2 117.4 109.4 210.8 82.5 114.7 28.7 130.6 Coco 274.6 59.2 36.2 75.3 137.0 87.1 236.9 39.3 111.0 Flickr30K 247.8 78.5 54.3 37.8 181.5 72.1 192.2 3</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W Nelson Francis and Henry Kucera. 1979. Brown Corpus manual: Manual of information to accompany a standard corpus of present-day edited American English for use with digital computers. Brown University, Providence, Rhode Island, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>Syntactic complexity. In</title>
<date>1985</date>
<booktitle>Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives,</booktitle>
<pages>129--189</pages>
<editor>D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6590" citStr="Frazier, 1985" startWordPosition="1061" endWordPosition="1062">can be used across several corpora. We also briefly examine metrics for vision quality. We evaluate several recent datasets based on all proposed metrics in Section 4, with results reported in Tables 1, 2, and Figure 1. 2.1 Language Quality We define the following criteria for evaluating the captions or instructions of the datasets: • Vocabulary Size (#vocab), the number of unique vocabulary words. 2http://visionandlanguage.net • Syntactic Complexity (Frazier, Yngve) measures the amount of embedding/branching in a sentence’s syntax. We report mean Yngve (Yngve, 1960) and Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees. • Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech. • Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as ‘love’ or ‘think’ and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define </context>
</contexts>
<marker>Frazier, 1985</marker>
<rawString>L. Frazier. 1985. Syntactic complexity. In D. R. Dowty, L. Karttunen, and A. M. Zwicky, editors, Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, pages 129– 189. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haoyuan Gao</author>
<author>Junhua Mao</author>
<author>Jie Zhou</author>
<author>Zhiheng Huang</author>
<author>Lei Wang</author>
<author>Wei Xu</author>
</authors>
<title>Are you talking to a machine? dataset and methods for multilingual image question answering.</title>
<date>2015</date>
<location>CoRR, abs/1505.05612.</location>
<contexts>
<context position="2330" citStr="Gao et al., 2015" startWordPosition="366" endWordPosition="369">cognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset. However, as more and more datasets have emerged in this space, it has become unclear how different methods generalize beyond the datasets they are evaluated on,</context>
</contexts>
<marker>Gao, Mao, Zhou, Huang, Wang, Xu, 2015</marker>
<rawString>Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a machine? dataset and methods for multilingual image question answering. CoRR, abs/1505.05612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Gordon</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Reporting bias and knowledge extraction.</title>
<date>2013</date>
<booktitle>In Automated Knowledge Base Construction (AKBC) 2013: The 3rd Workshop on Knowledge Extraction, at CIKM 2013,</booktitle>
<pages>13</pages>
<marker>Gordon, Van Durme, 2013</marker>
<rawString>Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge extraction. In Automated Knowledge Base Construction (AKBC) 2013: The 3rd Workshop on Knowledge Extraction, at CIKM 2013, AKBC’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Kazemzadeh</author>
<author>Vicente Ordonez</author>
<author>Mark Matten</author>
<author>Tamara Berg</author>
</authors>
<title>ReferItGame: Referring to Objects in Photographs of Natural Scenes.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>787--798</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="2267" citStr="Kazemzadeh et al., 2014" startWordPosition="354" endWordPosition="357">e corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset. However, as more and more datasets have emerged in this space, it has become unclear how differen</context>
</contexts>
<marker>Kazemzadeh, Ordonez, Matten, Berg, 2014</marker>
<rawString>Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. 2014. ReferItGame: Referring to Objects in Photographs of Natural Scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787–798, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunhee Kim</author>
<author>Seungwhan Moon</author>
<author>Leonid Sigal</author>
</authors>
<title>Joint Photo Stream and Blog Post Summarization and Exploration.</title>
<date>2015</date>
<booktitle>In 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR</booktitle>
<contexts>
<context position="2230" citStr="Kim et al., 2015" startWordPosition="348" endWordPosition="351">and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset. However, as more and more datasets have emerged in this spac</context>
</contexts>
<marker>Kim, Moon, Sigal, 2015</marker>
<rawString>Gunhee Kim, Seungwhan Moon, and Leonid Sigal. 2015. Joint Photo Stream and Blog Post Summarization and Exploration. In 28th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kollar</author>
<author>Jayant Krishnamurthy</author>
<author>Grant Strimel</author>
</authors>
<title>Toward interactive grounded language acquisition.</title>
<date>2013</date>
<booktitle>In Robotics: Science and Systems.</booktitle>
<contexts>
<context position="1352" citStr="Kollar et al., 2013" startWordPosition="203" endWordPosition="206">f quality metrics for evaluating and analyzing the vision &amp; language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in langu</context>
</contexts>
<marker>Kollar, Krishnamurthy, Strimel, 2013</marker>
<rawString>Thomas Kollar, Jayant Krishnamurthy, and Grant Strimel. 2013. Toward interactive grounded language acquisition. In Robotics: Science and Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
<author>Hendrik Zender</author>
<author>Patric Jensfelt</author>
<author>Henrik I Christensen</author>
</authors>
<title>Situated dialogue and spatial organization: What, where... and why?</title>
<date>2007</date>
<journal>International Journal ofAdvanced Robotic Systems, Special Issue on Human and Robot Interactive Communication,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="1420" citStr="Kruijff et al., 2007" startWordPosition="215" endWordPosition="218">e datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. cont</context>
</contexts>
<marker>Kruijff, Zender, Jensfelt, Christensen, 2007</marker>
<rawString>Geert-Jan M. Kruijff, Hendrik Zender, Patric Jensfelt, and Henrik I. Christensen. 2007. Situated dialogue and spatial organization: What, where... and why? International Journal ofAdvanced Robotic Systems, Special Issue on Human and Robot Interactive Communication, 4(2), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Yi Lin</author>
<author>Michael Maire</author>
<author>Serge Belongie</author>
<author>James Hays</author>
<author>Pietro Perona</author>
<author>Deva Ramanan</author>
<author>Piotr Dollar</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Microsoft COCO: common objects in context. CoRR,</title>
<date>2014</date>
<contexts>
<context position="10091" citStr="Lin et al., 2014" startWordPosition="1631" endWordPosition="1634">en further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to s</context>
</contexts>
<marker>Lin, Maire, Belongie, Hays, Perona, Ramanan, Dollar, Zitnick, 2014</marker>
<rawString>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. CoRR, abs/1405.0312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mateusz Malinowski</author>
<author>Mario Fritz</author>
</authors>
<title>A multiworld approach to question answering about realworld scenes based on uncertain input.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>1682--1690</pages>
<contexts>
<context position="15698" citStr="Malinowski and Fritz, 2014" startWordPosition="2562" endWordPosition="2565">scribed by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014; Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: • Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as ‘when I look at this picture, I feel –’, selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers. • Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA,</context>
</contexts>
<marker>Malinowski, Fritz, 2014</marker>
<rawString>Mateusz Malinowski and Mario Fritz. 2014. A multiworld approach to question answering about realworld scenes based on uncertain input. In Advances in Neural Information Processing Systems 27, pages 1682–1690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Malmaud</author>
<author>Jonathan Huang</author>
<author>Vivek Rathod</author>
<author>Nicholas Johnston</author>
<author>Andrew Rabinovich</author>
<author>Kevin Murphy</author>
</authors>
<title>Whats cookin? interpreting cooking videos using text, speech and vision.</title>
<date>2015</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT</booktitle>
<location>Denver, Colorado USA.</location>
<contexts>
<context position="12549" citStr="Malmaud et al., 2015" startWordPosition="2026" endWordPosition="2029">ntioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4We did not use an external synonym o</context>
</contexts>
<marker>Malmaud, Huang, Rathod, Johnston, Rabinovich, Murphy, 2015</marker>
<rawString>Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nicholas Johnston, Andrew Rabinovich, and Kevin Murphy. 2015. Whats cookin? interpreting cooking videos using text, speech and vision. In North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2015), May 31 - June 5, 2015, Denver, Colorado USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="17400" citStr="Manning et al., 2014" startWordPosition="2847" endWordPosition="2850">were prompted to create ‘interesting’ questions, resulting in 215,150 questions and 430,920 answers. • Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. 4 Analysis We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979; Marcus et al., 1999) as a reference point. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with almost 20% of the words found in our abstract concept resource. The Deja corpus has the least number of abstract concepts, followed by COCO and VDC. This reflects differences in col210 Brown Clipart Coco Flickr30K CQA VDC VQA Pascal SBU Brown 237.1 99.6 560.8 405.0 354.039 187.3 126.5 47.8 621.5 Clipart 233.6 11.2 117.4 109.4 210.8 82.5 114.7 28.7 130.6 Coco 274.6 59.2 36.2 75.3 137.0 87.1 236.9 </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<note>Brown corpus, treebank-3.</note>
<contexts>
<context position="17482" citStr="Marcus et al., 1999" startWordPosition="2861" endWordPosition="2864"> 430,920 answers. • Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. 4 Analysis We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979; Marcus et al., 1999) as a reference point. We find evidence that the VQA dataset captures more abstract concepts than other datasets, with almost 20% of the words found in our abstract concept resource. The Deja corpus has the least number of abstract concepts, followed by COCO and VDC. This reflects differences in col210 Brown Clipart Coco Flickr30K CQA VDC VQA Pascal SBU Brown 237.1 99.6 560.8 405.0 354.039 187.3 126.5 47.8 621.5 Clipart 233.6 11.2 117.4 109.4 210.8 82.5 114.7 28.7 130.6 Coco 274.6 59.2 36.2 75.3 137.0 87.1 236.9 39.3 111.0 Flickr30K 247.8 78.5 54.3 37.8 181.5 72.1 192.2 39.9 125.0 CQA 489.4 18</context>
<context position="18940" citStr="Marcus et al., 1999" startWordPosition="3125" endWordPosition="3128">28.5 230.7 194.3 78.2 119.8 #vocab 14.0k 1.1k 13k 9.4k 5.3k 4.9k 1.4k 1.0k 65.1k Table 2: Perplexities across corpora, where rows represent test sets (20k sentences) and columns training sets (remaining sentences). To make perplexities comparable, we used the same vocabulary frequency cutoff of 3. All models are 5-grams. Brown SBU Deja Pascal ● ● ● ● Flickr30K COCO Clipart VDC ● ● ● ● VQA CQA VML Simplified Part ● ● ● of Speech ● N J V O N V J O N V J O N V J O POS Figure 1: Simplified part-of-speech distributions for the eight datasets. We include the POS tags from the balanced Brown corpus (Marcus et al., 1999) to contextualize any very shallow syntactic biases. We mapped all nouns to “N,” all verbs to “V,” all adjectives to “J” and all other POS tags to “O.” lecting the various corpora: For example, the Deja corpus was collected to find specifically visual phrases that can be used to describe multiple images. This corpus also has the most syntactically simple phrases, as measured by both Frazier and Yngve; this is likely caused by the phrases needing to be general enough to capture multiple images. The most syntactically complex sentences are found in the Flickr30K, COCO and CQA datasets. However, </context>
<context position="20556" citStr="Marcus et al., 1999" startWordPosition="3390" endWordPosition="3393">xity is in line with their relatively high sentence length. Table 2 illustrates further similarities between datasets, and a more fine-grained use of perplexity to measure the usefulness of a given training set for predicting words of a given test set. Some datasets such as COCO, Flickr30K, and Clipart are generally more useful as out-domain data compared to the QA datasets. Test sets for VQA and CQA are quite idiosyncratic and yield poor perplexity unless trained on in-domain data. As shown in Figure 1, the COCO dataset is balanced across POS tags most similarly to the balanced Brown corpus (Marcus et al., 1999). The Clipart dataset provides the highest proportion of verbs, which often correspond to actions/poses in vision research, while the Flickr30K corpus provides the most nouns, which often correspond to object/stuff categories in vision research. We emphasize here that the distinction between a qualitatively good or bad dataset is task dependent. Therefore, all these metrics and the obtained results provide the researchers with an objective set of criteria so that they can make the decision whether a dataset is suitable to a particular task. 5 Conclusion We detail the recent growth of vision &amp; </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Brown corpus, treebank-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A Joint Model of Language and Perception for Grounded Attribute Learning.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 International Conference on Machine Learning,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1398" citStr="Matuszek et al., 2012" startWordPosition="211" endWordPosition="214">ng the vision &amp; language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In ju</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A Joint Model of Language and Perception for Grounded Attribute Learning. In Proc. of the 2012 International Conference on Machine Learning, Edinburgh, Scotland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iftekhar Naim</author>
<author>Young C Song</author>
<author>Qiguang Liu</author>
<author>Liang Huang</author>
<author>Henry Kautz</author>
<author>Jiebo Luo</author>
<author>Daniel Gildea</author>
</authors>
<title>Discriminative unsupervised alignment of natural language instructions with corresponding video segments.</title>
<date>2015</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT</booktitle>
<location>Denver, Colorado USA.</location>
<contexts>
<context position="12526" citStr="Naim et al., 2015" startWordPosition="2022" endWordPosition="2025">bjects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4We did not us</context>
</contexts>
<marker>Naim, Song, Liu, Huang, Kautz, Luo, Gildea, 2015</marker>
<rawString>Iftekhar Naim, Young C. Song, Qiguang Liu, Liang Huang, Henry Kautz, Jiebo Luo, and Daniel Gildea. 2015. Discriminative unsupervised alignment of natural language instructions with corresponding video segments. In North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2015), May 31 - June 5, 2015, Denver, Colorado USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="8671" citStr="Ordonez et al., 2011" startWordPosition="1402" endWordPosition="1406">ess of the annotations (e.g., segmentation using bounding boxes (BB) or visual dependencies between boxes). Moreover, a vision corpus can use abstract or real images (Abs/Real). 3 The Available Datasets We group a representative set of available datasets based on their content. For a complete list of datasets and their descriptions, please refer to the supplementary website.2 3.1 Captioned Images Several recent vision &amp; language datasets provide one or multiple captions per image. The captions 208 of these datasets are either the original photo title and descriptions provided by online users (Ordonez et al., 2011; Thomee et al., 2015), or the captions generated by crowd workers for existing images. The former datasets tend to be larger in size and contain more contextual descriptions. 3.1.1 User-generated Captions • SBU Captioned Photo Dataset (Ordonez et al., 2011) contains 1 million images with original user generated captions, collected in the wild by systematic querying of Flickr. This dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10,</booktitle>
<pages>139--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9910" citStr="Rashtchian et al., 2010" startWordPosition="1603" endWordPosition="1606"> user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing vari</context>
<context position="11632" citStr="Rashtchian et al., 2010" startWordPosition="1873" endWordPosition="1876">atasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we fo</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, pages 139–147, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--25</pages>
<contexts>
<context position="12507" citStr="Regneri et al., 2013" startWordPosition="2018" endWordPosition="2021">centage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attribut</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics (TACL), 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengye Ren</author>
<author>Ryan Kiros</author>
<author>Richard Zemel</author>
</authors>
<title>Question answering about images using visual semantic embeddings.</title>
<date>2015</date>
<booktitle>In Deep Learning Workshop, ICML</booktitle>
<contexts>
<context position="15669" citStr="Ren et al., 2015" startWordPosition="2558" endWordPosition="2561">in the video as described by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014; Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: • Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as ‘when I look at this picture, I feel –’, selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers. • Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created f</context>
<context position="16930" citStr="Ren et al., 2015" startWordPosition="2771" endWordPosition="2774"> can be presented with an image and a free-form natural-language question (e.g., ‘how many people are in the photo?’), and should be able to answer the question. This dataset contains both real images and abstract scenes, paired with questions and answers. Real images include 123,285 images from MS COCO dataset, and 10,000 clip-art abstract scenes, made up from 20 ‘paperdoll’ human models with adjustable limbs and over 100 objects and 31 animals. Amazon Turkers were prompted to create ‘interesting’ questions, resulting in 215,150 questions and 430,920 answers. • Toronto COCO-QA Dataset (CQA) (Ren et al., 2015) is also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. 4 Analysis We analyze the datasets introduced in Section 3 according to the metrics defined in Section 2, using the Stanford CoreNLP suite to acquire parses and part-of-speech tags (Manning et al., 2014). We also include the Brown corpus (Francis and Kucera, 1979; Marcus et al., 1999) as a reference point. We find evidence that the</context>
</contexts>
<marker>Ren, Kiros, Zemel, 2015</marker>
<rawString>Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Question answering about images using visual semantic embeddings. In Deep Learning Workshop, ICML 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Rohrbach</author>
<author>Sikandar Amin</author>
<author>Mykhaylo Andriluka</author>
<author>Bernt Schiele</author>
</authors>
<title>A database for fine grained activity detection of cooking activities.</title>
<date>2012</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<publisher>IEEE, IEEE,</publisher>
<contexts>
<context position="12485" citStr="Rohrbach et al., 2012" startWordPosition="2014" endWordPosition="2017">, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object inf</context>
</contexts>
<marker>Rohrbach, Amin, Andriluka, Schiele, 2012</marker>
<rawString>Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele. 2012. A database for fine grained activity detection of cooking activities. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
<author>Kai-Yuh Hsiao</author>
<author>Nikolaos Mavridis</author>
</authors>
<title>Conversational robots: Building blocks for grounding word meaning.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Learning Word Meaning from Non-linguistic Data - Volume 6, HLTNAACL-LWM ’04,</booktitle>
<pages>70--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1439" citStr="Roy et al., 2003" startWordPosition="219" endWordPosition="222">ize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to </context>
</contexts>
<marker>Roy, Hsiao, Mavridis, 2003</marker>
<rawString>Deb Roy, Kai-Yuh Hsiao, and Nikolaos Mavridis. 2003. Conversational robots: Building blocks for grounding word meaning. In Proceedings of the HLT-NAACL 2003 Workshop on Learning Word Meaning from Non-linguistic Data - Volume 6, HLTNAACL-LWM ’04, pages 70–77, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Thomee</author>
<author>David A Shamma</author>
<author>Gerald Friedland</author>
<author>Benjamin Elizalde</author>
<author>Karl Ni</author>
<author>Douglas Poland</author>
<author>Damian Borth</author>
<author>Li-Jia Li</author>
</authors>
<title>The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817.</title>
<date>2015</date>
<contexts>
<context position="8693" citStr="Thomee et al., 2015" startWordPosition="1407" endWordPosition="1410"> (e.g., segmentation using bounding boxes (BB) or visual dependencies between boxes). Moreover, a vision corpus can use abstract or real images (Abs/Real). 3 The Available Datasets We group a representative set of available datasets based on their content. For a complete list of datasets and their descriptions, please refer to the supplementary website.2 3.1 Captioned Images Several recent vision &amp; language datasets provide one or multiple captions per image. The captions 208 of these datasets are either the original photo title and descriptions provided by online users (Ordonez et al., 2011; Thomee et al., 2015), or the captions generated by crowd workers for existing images. The former datasets tend to be larger in size and contain more contextual descriptions. 3.1.1 User-generated Captions • SBU Captioned Photo Dataset (Ordonez et al., 2011) contains 1 million images with original user generated captions, collected in the wild by systematic querying of Flickr. This dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists of 180K unique user-g</context>
</contexts>
<marker>Thomee, Shamma, Friedland, Elizalde, Ni, Poland, Borth, Li, 2015</marker>
<rawString>Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2015. The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>A A Efros</author>
</authors>
<title>Unbiased look at dataset bias.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’11,</booktitle>
<pages>1521--1528</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="5176" citStr="Torralba and Efros, 2011" startWordPosition="829" endWordPosition="832">ying the quickly expanding research tasks in language and vision. 2 Quality Criteria for Language &amp; Vision Datasets The quality of a dataset is highly dependent on the sampling and scraping techniques used early in the data collection process. However, the content of datasets can play a major role in narrowing the focus of the field. Datasets are affected by both reporting bias (Gordon and Durme, 2013), where the frequency with which people write about actions, events, or states does not directly reflect real-world frequencies of those phenomena; they are also affected by photographer’s bias (Torralba and Efros, 2011), where photographs are somewhat predictable within a given domain. This suggests that new datasets may be useful towards the larger AI goal if provided alongside a set of quantitative metrics that show how they compare against similar corpora, as well as more general “background” corpora. Such metrics can be used as indicators of dataset bias and language richness. At a higher level, we argue that clearly defined metrics are necessary to provide quantitative measurements of how a new dataset compares to previous work. This helps clarify and benchmark how research is progressing towards a broa</context>
</contexts>
<marker>Torralba, Efros, 2011</marker>
<rawString>A. Torralba and A. A. Efros. 2011. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’11, pages 1521–1528, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Arul Menezes</author>
<author>Chris Quirk</author>
</authors>
<title>An amr parser for english, french, german, spanish and japanese and a new amr-annotated corpus.</title>
<date>2015</date>
<booktitle>Proceedings of NAACL</booktitle>
<contexts>
<context position="7177" citStr="Vanderwende et al., 2015" startWordPosition="1157" endWordPosition="1160">d Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees. • Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech. • Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as ‘love’ or ‘think’ and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words. • Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are. • Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4. 2.2 Vision Quality Our focus in this survey is mainly on languag</context>
</contexts>
<marker>Vanderwende, Menezes, Quirk, 2015</marker>
<rawString>Lucy Vanderwende, Arul Menezes, and Chris Quirk. 2015. An amr parser for english, french, german, spanish and japanese and a new amr-annotated corpus. Proceedings of NAACL 2015, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhashini Venugopalan</author>
<author>Huijuan Xu</author>
<author>Jeff Donahue</author>
<author>Marcus Rohrbach</author>
<author>Raymond Mooney</author>
<author>Kate Saenko</author>
</authors>
<title>Translating videos to natural language using deep recurrent neural networks.</title>
<date>2015</date>
<booktitle>In Proceedings the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT 2015),</booktitle>
<pages>1494--1504</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="2196" citStr="Venugopalan et al., 2015" startWordPosition="343" endWordPosition="346">ked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗F.F. and N.M. contributed equally to this work. the past year, recent work has proposed methods for image and video captioning (Fang et al., 2014; Donahue et al., 2014; Venugopalan et al., 2015), summarization (Kim et al., 2015), reference (Kazemzadeh et al., 2014), and question answering (Antol et al., 2015; Gao et al., 2015), to name just a few. The newly crafted large-scale vision &amp; language datasets have played a crucial role in defining this research, serving as a foundation for training/testing and helping to set benchmarks for measuring system performance. Crowdsourcing and large image collections such as those provided by Flickr1 have made it possible for researchers to propose methods for vision and language tasks alongside an accompanying dataset. However, as more and more </context>
</contexts>
<marker>Venugopalan, Xu, Donahue, Rohrbach, Mooney, Saenko, 2015</marker>
<rawString>Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In Proceedings the 2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies (NAACL HLT 2015), pages 1494–1504, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1236" citStr="Winograd, 1972" startWordPosition="185" endWordPosition="187">ailable corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision &amp; language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision &amp; language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision &amp; language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Proc</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding Natural Language. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Michel Galley</author>
<author>Lucy Vanderwende</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>See no evil, say no evil: Description generation from densely labeled images.</title>
<date>2014</date>
<booktitle>In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014),</booktitle>
<pages>110--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="11550" citStr="Yatskar et al., 2014" startWordPosition="1858" endWordPosition="1861">0,396 descriptions. 3.1.3 Captions of Densely Labeled Images Existing caption datasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A mor</context>
</contexts>
<marker>Yatskar, Galley, Vanderwende, Zettlemoyer, 2014</marker>
<rawString>Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer. 2014. See no evil, say no evil: Description generation from densely labeled images. In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110–120, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor H Yngve</author>
</authors>
<title>A model and an hypothesis for language structure.</title>
<date>1960</date>
<booktitle>Proceedings of the American Philosophical Society,</booktitle>
<pages>104--444</pages>
<contexts>
<context position="6549" citStr="Yngve, 1960" startWordPosition="1056" endWordPosition="1057">thods to measure language quality that can be used across several corpora. We also briefly examine metrics for vision quality. We evaluate several recent datasets based on all proposed metrics in Section 4, with results reported in Tables 1, 2, and Figure 1. 2.1 Language Quality We define the following criteria for evaluating the captions or instructions of the datasets: • Vocabulary Size (#vocab), the number of unique vocabulary words. 2http://visionandlanguage.net • Syntactic Complexity (Frazier, Yngve) measures the amount of embedding/branching in a sentence’s syntax. We report mean Yngve (Yngve, 1960) and Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees. • Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech. • Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as ‘love’ or ‘think’ and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in Englis</context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Victor H. Yngve. 1960. A model and an hypothesis for language structure. Proceedings of the American Philosophical Society, 104:444–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="9851" citStr="Young et al., 2014" startWordPosition="1595" endWordPosition="1598">es Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning chall</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<volume>1</volume>
<pages>53--63</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics. Best Paper Award.</institution>
<contexts>
<context position="12795" citStr="Yu and Siskind, 2013" startWordPosition="2064" endWordPosition="2067">eporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4We did not use an external synonym or paraphrasing resource to perform the matching between labels and captions, as the dataset itself provides paraphrases for each object: each object is labeled by multiple Turkers, who labeled Isa relations (e.g., “eagle” is a “bird”). 209 Size(k</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 53–63, Sofia, Bulgaria. Association for Computational Linguistics. Best Paper Award.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Licheng Yu</author>
<author>Eunbyung Park</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Visual Madlibs: Fill in the blank Image Generation and Question Answering. arXiv preprint arXiv:1506.00278.</title>
<date>2015</date>
<contexts>
<context position="15788" citStr="Yu et al., 2015" startWordPosition="2578" endWordPosition="2581">hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically generated captions (Fang et al., 2014; Donahue et al., 2014). Some works have proposed to step beyond description generation, towards deeper AI tasks such as question answering (Ren et al., 2015; Malinowski and Fritz, 2014). We present two of these attempts below: • Visual Madlibs Dataset (VML) (Yu et al., 2015) is a subset of 10,783 images from the MS COCO dataset which aims to go beyond describing which objects are in the image. For a given image, three Amazon Turkers were prompted to complete one of 12 fill-in-the-blank template questions, such as ‘when I look at this picture, I feel –’, selected automatically based on the image content. This dataset contains a total of 360,001 MadLib question and answers. • Visual Question Answering (VQA) Dataset (Antol et al., 2015) is created for the task of openended VQA, where a system can be presented with an image and a free-form natural-language question (</context>
</contexts>
<marker>Yu, Park, Berg, Berg, 2015</marker>
<rawString>Licheng Yu, Eunbyung Park, Alexander C. Berg, and Tamara L. Berg. 2015. Visual Madlibs: Fill in the blank Image Generation and Question Answering. arXiv preprint arXiv:1506.00278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Learning the visual interpretation of sentences.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Computer Vision, ICCV 2013,</booktitle>
<pages>1681--1688</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="10613" citStr="Zitnick et al., 2013" startWordPosition="1714" endWordPosition="1717">e involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing outdoors associated with total 60,396 descriptions. 3.1.3 Captions of Densely Labeled Images Existing caption datasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such description</context>
<context position="12334" citStr="Zitnick et al., 2013" startWordPosition="1991" endWordPosition="1994">cets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3This visual annotation consists of a two-level hierarchy, w</context>
</contexts>
<marker>Zitnick, Parikh, Vanderwende, 2013</marker>
<rawString>C. Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 1681–1688.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
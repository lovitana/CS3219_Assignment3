<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.9994165">
Semi-supervised Dependency Parsing
using Bilexical Contextual Features from Auto-Parsed Data
</title>
<author confidence="0.991089">
Eliyahu Kiperwasser
</author>
<affiliation confidence="0.9864935">
Computer Science Department
Bar-Ilan University
</affiliation>
<address confidence="0.616097">
Ramat-Gan, Israel
</address>
<email confidence="0.998058">
elikip@gmail.com
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999628461538462">
We present a semi-supervised approach
to improve dependency parsing accuracy
by using bilexical statistics derived from
auto-parsed data. The method is based
on estimating the attachment potential of
head-modifier words, by taking into ac-
count not only the head and modifier
words themselves, but also the words sur-
rounding the head and the modifier. When
integrating the learned statistics as features
in a graph-based parsing model, we ob-
serve nice improvements in accuracy when
parsing various English datasets.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890166666667">
We are concerned with semi-supervised depen-
dency parsing, namely how to leverage large
amounts of unannotated data, in addition to anno-
tated Treebank data, to improve dependency pars-
ing accuracy. Our method (Section 2) is based
on parsing large amounts of unannotated text us-
ing a baseline parser, extracting word-interaction
statistics from the automatically parsed corpus,
and using these statistics as the basis of additional
parser features. The automatically-parsed data is
used to acquire statistics about lexical interactions,
which are too sparse to estimate well from any
realistically-sized Treebank. Specifically, we at-
tempt to infer a function assoc(head, modifer)
measuring the “goodness” of head-modifier rela-
tions (“how good is an arc in which black is a mod-
ifier of jump”). A similar approach was taken by
Chen et al. (2009) and Van Noord et al. (2007). We
depart from their work by extending the scoring
to include a wider lexical context. That is, given
the sentence fragment in Figure 1, we score the
(incorrect) dependency arc (black, jump) based on
the triplets (the black fox, will jump over). Learn-
ing a function between word triplets raises an
</bodyText>
<note confidence="0.45904">
Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
</note>
<email confidence="0.978247">
yoav.goldberg@gmail.com
</email>
<bodyText confidence="0.99995262962963">
extreme data sparsity issue, which we deal with
by decomposing the interaction between triplets
to a sum of interactions between word pairs.
The decomposition we use is inspired by recent
work in word-embeddings and dense vector rep-
resentations (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013). Indeed, we initially hoped
to leverage the generalization abilities associated
with vector-based representations. However, we
find that in our setup, reverting to direct count-
based statistics achieve roughly the same results
(Section 3).
Our derived features improve the accuracy of a
first-order dependency parser by 0.75 UAS points
(absolute) when evaluated on the in-domain WSJ
test-set, obtaining a final accuracy of 92.32 UAS
for a first-order parser. When comparing to the
strong baseline of using Brown-clusters based fea-
tures (Koo et al., 2008), we find that our triplets-
based method outperform them by over 0.27 UAS
points. This is in contrast to previous works (e.g.
(Bansal et al., 2014)) in which improvements over
using Brown-clusters features were achieved only
by adding to the cluster-based features, not by re-
placing them. As expected, combining both our
features and the brown-cluster features result in
some additional gains.
</bodyText>
<sectionHeader confidence="0.932869" genericHeader="introduction">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.999724">
Our departure point is a graph-based parsing
model (McDonald et al., 2005):
</bodyText>
<equation confidence="0.95389725">
parse(x) = argmax score(x, y)
y∈Y(x)
�score(x, y) = w · Φ(x, y) = w · O(x, part)
part∈y
</equation>
<bodyText confidence="0.9967058">
Given a sentence x we look for the highest-scoring
parse tree y in the space Y(x) of valid dependency
trees over x. The score of a tree is determined by a
linear model parameterized by a weights vector w,
and a feature function Φ(x, y). To make the search
</bodyText>
<page confidence="0.936761">
1348
</page>
<note confidence="0.7064465">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1348–1353,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<equation confidence="0.924395666666667">
Features in 0ij
lex(x, y)
bin(Sij(x, y))
bin(Sij(x, y)) o dist(x,y)
bin(Sij(x, y)) o pos(x) o pos(y)
bin(Sij(x, y)) o pos(x) o pos(y) o dist(x,y)
</equation>
<tableCaption confidence="0.9662">
Table 1: All features are binary indicators. x and y are
</tableCaption>
<figureCaption confidence="0.733460666666667">
token indices. Sij is estimated from auto-parsed corpora as
described in the text. The values of S(·, ·) are in the range
(0, 1), which is split by bin into 10 equally-spaced intervals.
dist is the signed and binned sentence-distance between x
and y. pos(x) is the part of speech of token x. o indicates a
concatenation of features.
</figureCaption>
<bodyText confidence="0.999541111111111">
tractable, the feature function is decomposed into
local feature functions over tree-parts 0(x, part).
The features in 0 are standard graph-based depen-
dency parsing features, capturing mostly structural
information from the parse tree.
We extend the scoring function by adding an ad-
ditional term capturing the strength of lexical as-
sociation between head word h and modifier word
m in each dependency arc:
</bodyText>
<equation confidence="0.962491333333333">
score(x,y) =
� w · 0(x, part) + � assoc(h, m)
part∈y (h,m)∈y
</equation>
<bodyText confidence="0.999736705882353">
The association function assoc is also modeled
as a linear model assoc(h, m) = wlex·0lex(h, m).
While the weights wlex are trained jointly with w
based on supervised training data, the features in
0lex do not look at h and m directly, but instead
rely on a quantity S(h, m) that reflects the “good-
ness” of the arc (h, m). The quantity S(h, m)
ranges between 0 and 1, and is estimated based
on large quantities of auto-parsed data. Given a
value for S(h, m), 0lex is composed of indicator
functions indicating the binned ranges of S(h, m),
possibly conjoined with information such as the
binned surface distance between the tokens h and
m and their parts of speech. The complete spec-
ification of 0lex we use is shown in Table 1 (the
meaning of the ij indices will be discussed in Sec-
tion 2.2).
</bodyText>
<subsectionHeader confidence="0.964893">
2.1 Estimating S(h, m)
</subsectionHeader>
<bodyText confidence="0.983160566666667">
One way of estimating S(h, m), which was also
used in (Chen et al., 2009), is using rank statistics.
Let D be the list of observed (h, m) pairs sorted by
their frequencies, and let rank(h, m) be the index
of the pair (h, m) in D. We now set:
SRANK(h, m) =
While effective, this approach has two related
shortcomings: first, it requires storing counts for
all the pairs (h, m) appearing in the auto-parsed
data, resulting in memory requirement that scales
quadratically with the vocabulary size. Second,
even with very large auto-parsed corpora many
plausible head-modifier pairs are likely to be un-
observed.
An alternative way of estimating S(h, m) that
does not require storing all the observed pairs and
that has a potential of generalizing beyond the
seen examples is using a log-bilinear embedding
model similar to the skip-gram model presented by
Mikolov et al. (2013b) to embed word pairs such
that compatible pairs receive high scores. The
model assigns two disjoint sets of d-dimensional
continuous latent vectors, u and v, where uh E Rd
is an embedding of a head word h, and vm E Rd is
an embedding of a modifier word m. The embed-
ding is done by trying to optimize the following
corpus-wide objective that is maximizing the dot
product of the vectors of observed (h, m) pairs and
minimizing the dot product of vectors of random
h and m pairs. Formally:
</bodyText>
<equation confidence="0.999273666666667">
E� k
ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i))
h,m∈C i=1 mi∼
</equation>
<bodyText confidence="0.994159">
where Q(x) = 1/(1 + e−x), and k is the number
of negative samples, drawn from the corpus-based
Unigram distribution Pm. For further details, see
(Mikolov et al., 2013b; Goldberg and Levy, 2014).
We then take:1
SEMBED(h, m) = Q(uh · vm)
In contrast to the counts based method, this model
is able to estimate the strength of a pair of words
even if the pair did not appear in the corpus due to
sparsity.
Finally, Levy and Goldberg (2014b) show that
the skip-grams with negative-sampling model de-
scribed above achieves its optimal solution when
uh · vm = PMI(h, m) − log k. This gives rise to
another natural way of estimating S:
1The embedding we derive are very similar to the ones de-
scribed in (Levy and Goldberg, 2014a; Bansal et al., 2014),
and which were used by Bansal et al.(2014) for deriving semi-
supervised parsing features. An important difference from
these previous work is that after training, they keep only one
set of vectors (u or v) and ignore the other, basing the fea-
tures on the derived vector representations. In contrast, we
keep both sets of vectors and are interested in the association
measure induced by the dot product uh · vm.
</bodyText>
<equation confidence="0.78343025">
rank(h, m)
|D|
1349
m−1 m0 m+1 ... h−1 h0 h+1
</equation>
<figureCaption confidence="0.8383375">
the black fox ... will jump over
Figure 1: Illustration of the bilexical information including
</figureCaption>
<bodyText confidence="0.993064666666667">
context. When scoring the (incorrect) arc between h0 and
m0, we take into account also the surrounding words h−1,
h+1, m−1 and m+1.
</bodyText>
<equation confidence="0.991984666666667">
p(h,m)
SPMI(h, m) = σ(PMI(h, m)) =
p(h,m)+p(h)p(m)
</equation>
<bodyText confidence="0.999925">
where p(h, m), p(h) and p(m) are unsmoothed
maximum-likelihood estimates based on the auto-
parsed corpus.
Like SRANK and unlike SEMBED, SPMI requires
storing statistics for all observed word pairs, and
is not able of generalizing beyond (h, m) pairs
seen in the auto-parsed data. However, as we
see in Section 3, this method performing similarly
in practice, suggesting that the generalization ca-
pabilities of the embedding-based method do not
benefit the parsing task.
</bodyText>
<subsectionHeader confidence="0.999899">
2.2 Adding additional context
</subsectionHeader>
<bodyText confidence="0.988976947368421">
Estimating the association between a pair of words
is effective. However, we would like to go a step
further and take into account also the context in
which these words occur. Specifically, our com-
plete model attempts to estimate the association
between word trigrams centered around the head
and the modifier words.
A naive solution that defines each trigram as
its own vocabulary item will increase the vocab-
ulary size by two orders of magnitude and re-
sult in severe data sparsity. An alternative so-
lution would be to associate each word in the
triplet (h−1, h0, h+1) with its own unique vocab-
ulary item, and likewise for modifier words. In
the embeddings-based model, this results in 6 vec-
tor sets u−1, u0, u+1, v−1, v0, v+1, where v−1
dog, for
example, represents the word “dog” when it ap-
pears to the left of the modifier word, and u+1
</bodyText>
<subsubsectionHeader confidence="0.48765">
walk
</subsubsectionHeader>
<bodyText confidence="0.8623386">
the word “walk” when it appears to the right
of the head word.2 This amounts to only a 3-
fold increase in the required vocabulary size. We
then model the strength of association between
h−1h0h+1 and m−1m0m+1
</bodyText>
<footnote confidence="0.8029105">
2Sentences are padded by special sentence-boundary
symbols.
</footnote>
<equation confidence="0.961666666666667">
pairwise interactions:3
assoc(h−1h0h+1, m−1m0m+1) =
αij associj(hi, mj)
</equation>
<bodyText confidence="0.9690935">
As before, the pairwise association measure
associj(hi, mj) is modeled as a linear model:
</bodyText>
<equation confidence="0.996068">
associj(hi, mj) = wij
lex · φij
lex(hi, mj)
</equation>
<bodyText confidence="0.962813666666667">
Where φijis again defined in terms of
lex
a goodness function Sij(x, y). For example,
S−1,+1(the, over) corresponds to the goodness of
a head-modifier arc where the word to the left of
the head word is “the” and the word to the right
of the modifier word is “over”. Sij(hi, mj), the
goodness of the arc induced by the pair (hi, mj),
can be estimated by either SRANK, SEMBED or SPMI
as before. For example, in the embeddings model
we set Sij(x, y) = σ(uix · vjy).
We update the bilexical features to include con-
text as explained above. Instead of learning α
and wlex coefficients separately, we absorb the αij
terms into wlex, learning both at the same time:
</bodyText>
<equation confidence="0.999651222222222">
assoc(h−1h0h+1, m−1m0m+1) =
X X αij · associj(hi, mj) =
i∈{−1,0,1} j∈{−1,0,1}
X X αij · wij
i∈{−1,0,1} j∈{−1,0,1} lex · φij
lex(hi, mj) =
X X w0ij
i∈{−1,0,1} j∈{−1,0,1} lex · φij
lex(hi, mj)
</equation>
<bodyText confidence="0.5720085">
Finally, the parser selects a dependency tree which
maximizes the following:
</bodyText>
<equation confidence="0.9967462">
Xw0 · Φ(x, y) = w · φ(x, part)
part∈y
wij
lex · φij
lex(hi, mj)
</equation>
<bodyText confidence="0.544771666666667">
3In the word embeddings literature, it is common to rep-
resent a word triplet as the sum of the individual component
vectors, resulting in ux,y,z · va,b,c = (u−1
</bodyText>
<equation confidence="0.9955155">
x + u0y + u+1
z ) ·
(v−1
a + v0b + v+1
</equation>
<bodyText confidence="0.9804765">
c ). Expanding the terms will result in a very
similar formulation to our proposal, but we allow the extra
flexibility of associating a different strength αij with each
pairwise term.
</bodyText>
<figure confidence="0.74497612">
as a weighted sum of
X1
i=−1
X1
j=−1
X
+
(h,m)∈y
X1
i=−1
X1
j=−1
1350
Dev Test Brown Answers Blogs Email News Reviews
Baseline 91.97 91.57 86.86 81.58 84.88 79.75 82.59 83.22
Base + Brown 92.16 92.05 87.16 81.96 85.46 80.27 83.02 83.56
Base + HM(SRANK) 92.16 91.74 87.01 82.06 85.36 80.29 82.72 83.62
Base + HM(SEMBED) 92.29 91.98 87.04 81.97 85.34 79.93 82.76 83.33
Base + HM(SPMI) 92.35 92.00 87.14 82.20 85.65 80.34 82.83 83.81
Base + TRIP(SRANK) 92.23 91.91 87.02 82.31 85.59 80.50 83.30 83.79
Base + TRIP(SEMBED) 92.38 92.27 87.15 82.34 85.71 80.41 83.21 83.68
Base + TRIP(SPMI) 92.61 92.32 87.29 82.58 85.88 80.43 83.57 84.18
Base + Brown + TRIP(SRANK) 92.43 92.33 87.23 82.60 85.75 80.57 83.48 84.00
Base + Brown + TRIP(SEMBED) 92.51 92.45 87.33 82.42 86.06 80.44 83.40 83.87
Base + Brown + TRIP(SPMI) 92.70 92.40 87.42 82.74 86.08 80.72 83.78 84.26
</figure>
<tableCaption confidence="0.9573785">
Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
on the PTB training set. Dev and Test are sections 22 and 23. Brown is the Brown portion of the PTB. The other columns
correspond to the test portions of the Google Web Treebanks. Automatic POS-tags are assigned in all cases. HM indicates using
assoc(h, m) and TRIP using assoc(h−1h0h1, m−1m0m1). + BROWN indicate using features based on Brown clustering.
</tableCaption>
<sectionHeader confidence="0.995777" genericHeader="background">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999714483870968">
Data Our experiments are based on the Penn
Treebank (PTB) (Marcus et al., 1993) as well as
the Google Web Treebanks (LDC2012T13), cov-
ering both in-domain and out-of-domain scenar-
ios. We use the Stanford-dependencies represen-
tation (de Marneffe and Manning, 2008). All
the constituent-trees are converted to Stanford-
dependencies based on the settings of Version
1.0 of the Universal Treebank (McDonald et al.,
2013).4 These are based on the Stanford Depen-
dencies converter but use some non-default flags,
and change some of the dependency labels. All
of the models are trained on section 2-21 of the
WSJ portion of the PTB. For in-domain data, we
evaluate on sections 22 (Dev) and 23 (Test). All
of the parameter tuning were performed on the
Dev set, and we report test-set numbers only for
the “most interesting” configurations. For out-of-
domain data, we use the Brown portion of the PTB
(Brown), as well as the test-sets of different do-
mains available in the Google Web Treebank: An-
swers, Blogs, Emails, Reviews and Newsgroups.
All trees have automatically assigned part-of-
speech tags, assigned by the TurboTagger POS-
tagger.5 The train-set POS-tags were derived in
a 10-fold jacknifing, and the different test datasets
receive tags from a tagger trained on sections 2-21.
For auto-parsed data, we parse the text of the
BLLIP corpus (Charniak, 2000) using our base-
line parser. This is the same corpus used for deriv-
ing Brown clusters for use as features in (Koo et
</bodyText>
<footnote confidence="0.9906922">
4https://github.com/ryanmcd/
uni-dep-tb/raw/master/universal_
treebanks_v1.0.tar.gz
5http://www.ark.cs.cmu.edu/
TurboParser/
</footnote>
<bodyText confidence="0.994408333333333">
al., 2008). We use the clusters provided by Terry
Koo6. Parsing accuracy is measured by unlabeled
attachment score (UAS) excluding punctuations.
Implementation Details We focus on first-order
parsers, as they are the most practical graph-
based parsers in terms of running time in real-
istic parsing scenarios. Our base model is a re-
implementation of a first-order projective Graph-
based parser (McDonald et al., 2005), which we
extend to support the semi-supervised φlex fea-
tures. The parser is trained for 10 iterations
of online-training with passive-aggressive updates
(Crammer et al., 2006). For the Brown-cluster fea-
tures, we use the feature templates described by
(Koo et al., 2008; Bansal et al., 2014).
The embedding vectors are trained using the
freely available word2vecf software7, by con-
joining each word with its relative position (-1, 0
or 1) and treating the head words as “words” and
the modifier words to be “contexts”. The words
are embedded into 300-dimensional vectors. All
code and vectors will be available at the first au-
thor’s website.
Results The results are shown in Table 2. The
second block (HM) compares the baseline parser
to a parser including the assoc(h, m) lexical com-
ponent, using various ways of computing s(h, m).
We observe a clear improvement above the base-
line from using the lexical component across all
domains. The different estimation methods per-
form very similar to each other.
In the third block (TRIP) we switch to the
triplet-based lexical association. With SRANK,
</bodyText>
<footnote confidence="0.99965025">
6http://people.csail.mit.edu/maestro/
papers/bllip-clusters.gz
7http://www.bitbucket.org/yoavgo/
word2vecf
</footnote>
<page confidence="0.994137">
1351
</page>
<bodyText confidence="0.999772454545455">
there is very little advantage over looking at just
pairs. However, with SEMBED or SPMI the improve-
ment of using the triplet-based method over using
just the head-modifier pairs is clear. The counting-
based PMI method performs on par with the Em-
bedding based approximation of it.
The second line of the first block (Base+Brown)
represents the current state-of-the-art in semi-
supervised training of graph-based parsing: using
Brown-cluster derived features (Koo et al., 2008;
Bansal et al., 2014). The Brown-derived features
provide similar (sometimes larger) gains to using
our HM method, and substantially smaller gains
than our TRIP method. To the best of our knowl-
edge, we are the first to show a semi-supervised
method that significantly outperforms the use of
Brown-clusters without using Brown-clusters as a
component.
As expected, combining our features and the
Brown-based features provide an additional im-
provement, as can be seen in the last block of Table
2 (Base+Brown+TRIP).
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999967577464789">
Semi-supervised approaches to dependency pars-
ing can be roughly categorized into two groups:
those that use unannotated data and those that use
automatically-parsed data. Our proposed method
falls in the second group.
Among the words that use unannotated data, the
dominant approach is to derive either word clus-
ters (Koo et al., 2008) or word vectors (Chen and
Manning, 2014) based on unparsed data, and use
these as additional features for a supervised pars-
ing model. While the word representations used
in such methods are not specifically designed for
the parsing task, they do provide useful features
for parsing, and in particular the method of (Koo
et al., 2008), relying on features derived using the
Brown-clustering algorithm, provides very com-
petitive state-of-the-art results. To the best of our
knowledge, we are the first to show a substantial
improvement over using Brown-clustering derived
features without using Brown-cluster features as a
component.
Among the words that use auto-parsed data, a
dominant approach is self-training (McClosky et
al., 2006), in which a parser A (possibly an en-
semble) is used to parse large amounts of data,
and a parser B is then trained over the union of
the gold data and the auto-parsed data produced
by parser A. In the context of dependency-parsing,
successful uses of self-training require parser A to
be stronger than parser B (Petrov et al., 2010) or
use a selection criteria for training only on high-
quality parses produced by parser A (Sagae and
Tsujii, 2007; Weiss et al., 2015). In contrast, our
work uses the same parser (modulo the feature-set)
for producing the auto-parsed data and for train-
ing the final model, and does not employ a high-
quality parse selection criteria when creating the
auto-parsed corpus. It is possible that high-quality
parse selection can improve our proposed method
even further.
Works that derive features from auto-parsed
data include (Sagae and Gordon, 2009; Bansal et
al., 2014). Such works assign a representation (ei-
ther cluster or vector) for individual word in the
vocabulary based on their syntactic behavior. In
contrast, our learned features are designed to cap-
ture interactions between words. As discussed in
sections (1) and (2), most similar to ours is the
work of (Chen et al., 2009; Van Noord, 2007). We
extend their approach to take into account not only
direct word-word interactions but also the lexical
surroundings in which these interactions occur.
Another recent approach that takes into account
various syntactic interactions was recently intro-
duced by Chen et al. (2014), who propose to learn
to embed complex features that are being used in
a graph-based parser based on other features they
co-occur with in auto-parsed data. Similar to our
approach, the embedded features are then used as
additional features in a conventional graph-based
model. The approaches are to a large extent com-
plementary, and could be combined.
Finally, our work adds additional features to
a graph-based parser which is based on a linear-
model. Recently, progress in dependency parsing
has been made by introducing non-linear, neural-
network based models (Pei et al., 2015; Chen and
Manning, 2014; Weiss et al., 2015; Dyer et al.,
2015; Zhou et al., 2015). Adapting our approach
to work with such models is an interesting research
direction.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999955833333333">
We presented a semi-supervised method for de-
pendency parsing and demonstrated its effective-
ness on a first-order graph-based parser. Taking
into account not only the (head,modifier) word-
pair but also their immediate surrounding words
add a clear benefit to parsing accuracy.
</bodyText>
<page confidence="0.994094">
1352
</page>
<sectionHeader confidence="0.990126" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99949027">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.
Eugene Charniak. 2000. Bllip 1987-89 wsj corpus re-
lease 1. In Linguistic Data Consortium, Philadel-
phia.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP, pages 740–750.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In EMNLP, pages 570–579.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In
Proc. of COLING.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551–585.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ’08, pages 1–8,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proc. of ACL.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving Mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. of ACL, pages 595–603.
Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proc. of ACL, pages
302–308, Baltimore, Maryland.
Omer Levy and Yoav Goldberg. 2014b. Neural word
embeddings as implicit matrix factorization. In
Proc. of NIPS, pages 2177–2185.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proc. of NAACL, pages 152–159.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B Hall, Slav Petrov, Hao Zhang,
Oscar T¨ackstr¨om, et al. 2013. Universal depen-
dency annotation for multilingual parsing. In ACL
(2), pages 92–97.
Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Advances in Neural Information Pro-
cessing Systems, pages 3111–3119.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In NIPS.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2015. An
effective neural network model for graph-based de-
pendency parsing. In Proc. of ACL.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proc. of EMNLP-
2010, October.
Kenji Sagae and Andrew S. Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In IWPT,
pages 192–201.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proc. of CoNLL-2007 Shared
Task.
Gertjan Van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 1–10. Association for
Computational Linguistics.
David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Proc. of ACL.
Hao Zhou, Yue Zhang, and Jiajun Chen. 2015. A
neural probabilistic structured-prediction model for
transition-based dependency parsing. In Proc. of
ACL.
</reference>
<page confidence="0.978934">
1353
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.186121">
<title confidence="0.9833105">Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data</title>
<author confidence="0.491645">Eliyahu</author>
<affiliation confidence="0.734971666666667">Computer Science Bar-Ilan Ramat-Gan,</affiliation>
<email confidence="0.999712">elikip@gmail.com</email>
<abstract confidence="0.998529357142857">We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3003" citStr="Bansal et al., 2014" startWordPosition="449" endWordPosition="452"> with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) �score(x, y) = w · Φ(x, y) = w · O(x, part) part∈y Given a sentence x we look for the highest-scoring parse tree y in the space Y(x) of valid dependency trees over x. The score of a tree is determined by a linear model parame</context>
<context position="7871" citStr="Bansal et al., 2014" startWordPosition="1307" endWordPosition="1310">or further details, see (Mikolov et al., 2013b; Goldberg and Levy, 2014). We then take:1 SEMBED(h, m) = Q(uh · vm) In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, Levy and Goldberg (2014b) show that the skip-grams with negative-sampling model described above achieves its optimal solution when uh · vm = PMI(h, m) − log k. This gives rise to another natural way of estimating S: 1The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for deriving semisupervised parsing features. An important difference from these previous work is that after training, they keep only one set of vectors (u or v) and ignore the other, basing the features on the derived vector representations. In contrast, we keep both sets of vectors and are interested in the association measure induced by the dot product uh · vm. rank(h, m) |D| 1349 m−1 m0 m+1 ... h−1 h0 h+1 the black fox ... will jump over Figure 1: Illustration of the bilexical information including context. When scoring the (incorrect) arc betwe</context>
<context position="15459" citStr="Bansal et al., 2014" startWordPosition="2585" endWordPosition="2588">y unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-order projective Graphbased parser (McDonald et al., 2005), which we extend to support the semi-supervised φlex features. The parser is trained for 10 iterations of online-training with passive-aggressive updates (Crammer et al., 2006). For the Brown-cluster features, we use the feature templates described by (Koo et al., 2008; Bansal et al., 2014). The embedding vectors are trained using the freely available word2vecf software7, by conjoining each word with its relative position (-1, 0 or 1) and treating the head words as “words” and the modifier words to be “contexts”. The words are embedded into 300-dimensional vectors. All code and vectors will be available at the first author’s website. Results The results are shown in Table 2. The second block (HM) compares the baseline parser to a parser including the assoc(h, m) lexical component, using various ways of computing s(h, m). We observe a clear improvement above the baseline from usi</context>
<context position="16869" citStr="Bansal et al., 2014" startWordPosition="2801" endWordPosition="2804">. With SRANK, 6http://people.csail.mit.edu/maestro/ papers/bllip-clusters.gz 7http://www.bitbucket.org/yoavgo/ word2vecf 1351 there is very little advantage over looking at just pairs. However, with SEMBED or SPMI the improvement of using the triplet-based method over using just the head-modifier pairs is clear. The countingbased PMI method performs on par with the Embedding based approximation of it. The second line of the first block (Base+Brown) represents the current state-of-the-art in semisupervised training of graph-based parsing: using Brown-cluster derived features (Koo et al., 2008; Bansal et al., 2014). The Brown-derived features provide similar (sometimes larger) gains to using our HM method, and substantially smaller gains than our TRIP method. To the best of our knowledge, we are the first to show a semi-supervised method that significantly outperforms the use of Brown-clusters without using Brown-clusters as a component. As expected, combining our features and the Brown-based features provide an additional improvement, as can be seen in the last block of Table 2 (Base+Brown+TRIP). 4 Related Work Semi-supervised approaches to dependency parsing can be roughly categorized into two groups:</context>
<context position="19348" citStr="Bansal et al., 2014" startWordPosition="3199" endWordPosition="3202"> A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recently introduced by Chen </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Bllip 1987-89 wsj corpus release 1.</title>
<date>2000</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="14512" citStr="Charniak, 2000" startWordPosition="2448" endWordPosition="2449">on the Dev set, and we report test-set numbers only for the “most interesting” configurations. For out-ofdomain data, we use the Brown portion of the PTB (Brown), as well as the test-sets of different domains available in the Google Web Treebank: Answers, Blogs, Emails, Reviews and Newsgroups. All trees have automatically assigned part-ofspeech tags, assigned by the TurboTagger POStagger.5 The train-set POS-tags were derived in a 10-fold jacknifing, and the different test datasets receive tags from a tagger trained on sections 2-21. For auto-parsed data, we parse the text of the BLLIP corpus (Charniak, 2000) using our baseline parser. This is the same corpus used for deriving Brown clusters for use as features in (Koo et 4https://github.com/ryanmcd/ uni-dep-tb/raw/master/universal_ treebanks_v1.0.tar.gz 5http://www.ark.cs.cmu.edu/ TurboParser/ al., 2008). We use the clusters provided by Terry Koo6. Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-ord</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. Bllip 1987-89 wsj corpus release 1. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>740--750</pages>
<contexts>
<context position="17753" citStr="Chen and Manning, 2014" startWordPosition="2940" endWordPosition="2943">e use of Brown-clusters without using Brown-clusters as a component. As expected, combining our features and the Brown-based features provide an additional improvement, as can be seen in the last block of Table 2 (Base+Brown+TRIP). 4 Related Work Semi-supervised approaches to dependency parsing can be roughly categorized into two groups: those that use unannotated data and those that use automatically-parsed data. Our proposed method falls in the second group. Among the words that use unannotated data, the dominant approach is to derive either word clusters (Koo et al., 2008) or word vectors (Chen and Manning, 2014) based on unparsed data, and use these as additional features for a supervised parsing model. While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of (Koo et al., 2008), relying on features derived using the Brown-clustering algorithm, provides very competitive state-of-the-art results. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among th</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. of EMNLP, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="1575" citStr="Chen et al. (2009)" startWordPosition="227" endWordPosition="230">based on parsing large amounts of unannotated text using a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features. The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank. Specifically, we attempt to infer a function assoc(head, modifer) measuring the “goodness” of head-modifier relations (“how good is an arc in which black is a modifier of jump”). A similar approach was taken by Chen et al. (2009) and Van Noord et al. (2007). We depart from their work by extending the scoring to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel yoav.goldberg@gmail.com extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is</context>
<context position="5777" citStr="Chen et al., 2009" startWordPosition="928" endWordPosition="931">m) that reflects the “goodness” of the arc (h, m). The quantity S(h, m) ranges between 0 and 1, and is estimated based on large quantities of auto-parsed data. Given a value for S(h, m), 0lex is composed of indicator functions indicating the binned ranges of S(h, m), possibly conjoined with information such as the binned surface distance between the tokens h and m and their parts of speech. The complete specification of 0lex we use is shown in Table 1 (the meaning of the ij indices will be discussed in Section 2.2). 2.1 Estimating S(h, m) One way of estimating S(h, m), which was also used in (Chen et al., 2009), is using rank statistics. Let D be the list of observed (h, m) pairs sorted by their frequencies, and let rank(h, m) be the index of the pair (h, m) in D. We now set: SRANK(h, m) = While effective, this approach has two related shortcomings: first, it requires storing counts for all the pairs (h, m) appearing in the auto-parsed data, resulting in memory requirement that scales quadratically with the vocabulary size. Second, even with very large auto-parsed corpora many plausible head-modifier pairs are likely to be unobserved. An alternative way of estimating S(h, m) that does not require st</context>
<context position="19663" citStr="Chen et al., 2009" startWordPosition="3252" endWordPosition="3255"> model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recently introduced by Chen et al. (2014), who propose to learn to embed complex features that are being used in a graph-based parser based on other features they co-occur with in auto-parsed data. Similar to our approach, the embedded features are then used as additional features in a conventional graph-based model. The approaches are to a </context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In EMNLP, pages 570–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature embedding for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="19961" citStr="Chen et al. (2014)" startWordPosition="3297" endWordPosition="3300">2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recently introduced by Chen et al. (2014), who propose to learn to embed complex features that are being used in a graph-based parser based on other features they co-occur with in auto-parsed data. Similar to our approach, the embedded features are then used as additional features in a conventional graph-based model. The approaches are to a large extent complementary, and could be combined. Finally, our work adds additional features to a graph-based parser which is based on a linearmodel. Recently, progress in dependency parsing has been made by introducing non-linear, neuralnetwork based models (Pei et al., 2015; Chen and Manning, 2</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature embedding for dependency parsing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="15344" citStr="Crammer et al., 2006" startWordPosition="2565" endWordPosition="2568">.ark.cs.cmu.edu/ TurboParser/ al., 2008). We use the clusters provided by Terry Koo6. Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-order projective Graphbased parser (McDonald et al., 2005), which we extend to support the semi-supervised φlex features. The parser is trained for 10 iterations of online-training with passive-aggressive updates (Crammer et al., 2006). For the Brown-cluster features, we use the feature templates described by (Koo et al., 2008; Bansal et al., 2014). The embedding vectors are trained using the freely available word2vecf software7, by conjoining each word with its relative position (-1, 0 or 1) and treating the head words as “words” and the modifier words to be “contexts”. The words are embedded into 300-dimensional vectors. All code and vectors will be available at the first author’s website. Results The results are shown in Table 2. The second block (HM) compares the baseline parser to a parser including the assoc(h, m) lex</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory.</title>
<date>2015</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="7323" citStr="Goldberg and Levy, 2014" startWordPosition="1205" endWordPosition="1208"> continuous latent vectors, u and v, where uh E Rd is an embedding of a head word h, and vm E Rd is an embedding of a modifier word m. The embedding is done by trying to optimize the following corpus-wide objective that is maximizing the dot product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally: E� k ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i)) h,m∈C i=1 mi∼ where Q(x) = 1/(1 + e−x), and k is the number of negative samples, drawn from the corpus-based Unigram distribution Pm. For further details, see (Mikolov et al., 2013b; Goldberg and Levy, 2014). We then take:1 SEMBED(h, m) = Q(uh · vm) In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, Levy and Goldberg (2014b) show that the skip-grams with negative-sampling model described above achieves its optimal solution when uh · vm = PMI(h, m) − log k. This gives rise to another natural way of estimating S: 1The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for der</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2857" citStr="Koo et al., 2008" startWordPosition="423" endWordPosition="426">esentations (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) �score(x, y) = w · Φ(x, y) = w · O(x, part) part∈y Given a sentence x we look f</context>
<context position="15437" citStr="Koo et al., 2008" startWordPosition="2581" endWordPosition="2584">racy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-order projective Graphbased parser (McDonald et al., 2005), which we extend to support the semi-supervised φlex features. The parser is trained for 10 iterations of online-training with passive-aggressive updates (Crammer et al., 2006). For the Brown-cluster features, we use the feature templates described by (Koo et al., 2008; Bansal et al., 2014). The embedding vectors are trained using the freely available word2vecf software7, by conjoining each word with its relative position (-1, 0 or 1) and treating the head words as “words” and the modifier words to be “contexts”. The words are embedded into 300-dimensional vectors. All code and vectors will be available at the first author’s website. Results The results are shown in Table 2. The second block (HM) compares the baseline parser to a parser including the assoc(h, m) lexical component, using various ways of computing s(h, m). We observe a clear improvement above</context>
<context position="16847" citStr="Koo et al., 2008" startWordPosition="2797" endWordPosition="2800">exical association. With SRANK, 6http://people.csail.mit.edu/maestro/ papers/bllip-clusters.gz 7http://www.bitbucket.org/yoavgo/ word2vecf 1351 there is very little advantage over looking at just pairs. However, with SEMBED or SPMI the improvement of using the triplet-based method over using just the head-modifier pairs is clear. The countingbased PMI method performs on par with the Embedding based approximation of it. The second line of the first block (Base+Brown) represents the current state-of-the-art in semisupervised training of graph-based parsing: using Brown-cluster derived features (Koo et al., 2008; Bansal et al., 2014). The Brown-derived features provide similar (sometimes larger) gains to using our HM method, and substantially smaller gains than our TRIP method. To the best of our knowledge, we are the first to show a semi-supervised method that significantly outperforms the use of Brown-clusters without using Brown-clusters as a component. As expected, combining our features and the Brown-based features provide an additional improvement, as can be seen in the last block of Table 2 (Base+Brown+TRIP). 4 Related Work Semi-supervised approaches to dependency parsing can be roughly catego</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>302--308</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="7564" citStr="Levy and Goldberg (2014" startWordPosition="1252" endWordPosition="1255">product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally: E� k ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i)) h,m∈C i=1 mi∼ where Q(x) = 1/(1 + e−x), and k is the number of negative samples, drawn from the corpus-based Unigram distribution Pm. For further details, see (Mikolov et al., 2013b; Goldberg and Levy, 2014). We then take:1 SEMBED(h, m) = Q(uh · vm) In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, Levy and Goldberg (2014b) show that the skip-grams with negative-sampling model described above achieves its optimal solution when uh · vm = PMI(h, m) − log k. This gives rise to another natural way of estimating S: 1The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for deriving semisupervised parsing features. An important difference from these previous work is that after training, they keep only one set of vectors (u or v) and ignore the other, basing the features on the derived vector representations. In co</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Dependencybased word embeddings. In Proc. of ACL, pages 302–308, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embeddings as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="7564" citStr="Levy and Goldberg (2014" startWordPosition="1252" endWordPosition="1255">product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally: E� k ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i)) h,m∈C i=1 mi∼ where Q(x) = 1/(1 + e−x), and k is the number of negative samples, drawn from the corpus-based Unigram distribution Pm. For further details, see (Mikolov et al., 2013b; Goldberg and Levy, 2014). We then take:1 SEMBED(h, m) = Q(uh · vm) In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, Levy and Goldberg (2014b) show that the skip-grams with negative-sampling model described above achieves its optimal solution when uh · vm = PMI(h, m) − log k. This gives rise to another natural way of estimating S: 1The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for deriving semisupervised parsing features. An important difference from these previous work is that after training, they keep only one set of vectors (u or v) and ignore the other, basing the features on the derived vector representations. In co</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Neural word embeddings as implicit matrix factorization. In Proc. of NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13242" citStr="Marcus et al., 1993" startWordPosition="2240" endWordPosition="2243">87.42 82.74 86.08 80.72 83.78 84.26 Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained on the PTB training set. Dev and Test are sections 22 and 23. Brown is the Brown portion of the PTB. The other columns correspond to the test portions of the Google Web Treebanks. Automatic POS-tags are assigned in all cases. HM indicates using assoc(h, m) and TRIP using assoc(h−1h0h1, m−1m0m1). + BROWN indicate using features based on Brown clustering. 3 Experiments and Results Data Our experiments are based on the Penn Treebank (PTB) (Marcus et al., 1993) as well as the Google Web Treebanks (LDC2012T13), covering both in-domain and out-of-domain scenarios. We use the Stanford-dependencies representation (de Marneffe and Manning, 2008). All the constituent-trees are converted to Stanforddependencies based on the settings of Version 1.0 of the Universal Treebank (McDonald et al., 2013).4 These are based on the Stanford Dependencies converter but use some non-default flags, and change some of the dependency labels. All of the models are trained on section 2-21 of the WSJ portion of the PTB. For in-domain data, we evaluate on sections 22 (Dev) and</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="18448" citStr="McClosky et al., 2006" startWordPosition="3046" endWordPosition="3049">vised parsing model. While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of (Koo et al., 2008), relying on features derived using the Brown-clustering algorithm, provides very competitive state-of-the-art results. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final m</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proc. of NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3339" citStr="McDonald et al., 2005" startWordPosition="500" endWordPosition="503">uracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) �score(x, y) = w · Φ(x, y) = w · O(x, part) part∈y Given a sentence x we look for the highest-scoring parse tree y in the space Y(x) of valid dependency trees over x. The score of a tree is determined by a linear model parameterized by a weights vector w, and a feature function Φ(x, y). To make the search 1348 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1348–1353, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Features in 0ij lex(x, y) bin(Sij(x, y)) bin(Sij(x, y)</context>
<context position="15167" citStr="McDonald et al., 2005" startWordPosition="2539" endWordPosition="2542">is the same corpus used for deriving Brown clusters for use as features in (Koo et 4https://github.com/ryanmcd/ uni-dep-tb/raw/master/universal_ treebanks_v1.0.tar.gz 5http://www.ark.cs.cmu.edu/ TurboParser/ al., 2008). We use the clusters provided by Terry Koo6. Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-order projective Graphbased parser (McDonald et al., 2005), which we extend to support the semi-supervised φlex features. The parser is trained for 10 iterations of online-training with passive-aggressive updates (Crammer et al., 2006). For the Brown-cluster features, we use the feature templates described by (Koo et al., 2008; Bansal et al., 2014). The embedding vectors are trained using the freely available word2vecf software7, by conjoining each word with its relative position (-1, 0 or 1) and treating the head words as “words” and the modifier words to be “contexts”. The words are embedded into 300-dimensional vectors. All code and vectors will b</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
<author>Yvonne QuirmbachBrundage</author>
<author>Yoav Goldberg</author>
<author>Dipanjan Das</author>
<author>Kuzman Ganchev</author>
<author>Keith B Hall</author>
<author>Slav Petrov</author>
<author>Hao Zhang</author>
<author>Oscar T¨ackstr¨om</author>
</authors>
<title>Universal dependency annotation for multilingual parsing.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>2</volume>
<pages>92--97</pages>
<marker>McDonald, Nivre, QuirmbachBrundage, Goldberg, Das, Ganchev, Hall, Petrov, Zhang, T¨ackstr¨om, 2013</marker>
<rawString>Ryan T McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et al. 2013. Universal dependency annotation for multilingual parsing. In ACL (2), pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Learning Representations (ICLR).</booktitle>
<contexts>
<context position="2273" citStr="Mikolov et al., 2013" startWordPosition="336" endWordPosition="339">ng to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel yoav.goldberg@gmail.com extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that o</context>
<context position="6576" citStr="Mikolov et al. (2013" startWordPosition="1062" endWordPosition="1065">) = While effective, this approach has two related shortcomings: first, it requires storing counts for all the pairs (h, m) appearing in the auto-parsed data, resulting in memory requirement that scales quadratically with the vocabulary size. Second, even with very large auto-parsed corpora many plausible head-modifier pairs are likely to be unobserved. An alternative way of estimating S(h, m) that does not require storing all the observed pairs and that has a potential of generalizing beyond the seen examples is using a log-bilinear embedding model similar to the skip-gram model presented by Mikolov et al. (2013b) to embed word pairs such that compatible pairs receive high scores. The model assigns two disjoint sets of d-dimensional continuous latent vectors, u and v, where uh E Rd is an embedding of a head word h, and vm E Rd is an embedding of a modifier word m. The embedding is done by trying to optimize the following corpus-wide objective that is maximizing the dot product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally: E� k ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i)) h,m∈C i=1 mi∼ where Q(x) = 1/(1 + e−x), and k is the number </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of the International Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2273" citStr="Mikolov et al., 2013" startWordPosition="336" endWordPosition="339">ng to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel yoav.goldberg@gmail.com extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that o</context>
<context position="6576" citStr="Mikolov et al. (2013" startWordPosition="1062" endWordPosition="1065">) = While effective, this approach has two related shortcomings: first, it requires storing counts for all the pairs (h, m) appearing in the auto-parsed data, resulting in memory requirement that scales quadratically with the vocabulary size. Second, even with very large auto-parsed corpora many plausible head-modifier pairs are likely to be unobserved. An alternative way of estimating S(h, m) that does not require storing all the observed pairs and that has a potential of generalizing beyond the seen examples is using a log-bilinear embedding model similar to the skip-gram model presented by Mikolov et al. (2013b) to embed word pairs such that compatible pairs receive high scores. The model assigns two disjoint sets of d-dimensional continuous latent vectors, u and v, where uh E Rd is an embedding of a head word h, and vm E Rd is an embedding of a modifier word m. The embedding is done by trying to optimize the following corpus-wide objective that is maximizing the dot product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally: E� k ln (σ (uh · vm)) − E E Pm ln (σ (uh · vm i)) h,m∈C i=1 mi∼ where Q(x) = 1/(1 + e−x), and k is the number </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2303" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="340" endWordPosition="343">exical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel yoav.goldberg@gmail.com extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outper</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Baobao Chang</author>
</authors>
<title>An effective neural network model for graph-based dependency parsing.</title>
<date>2015</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Pei, Ge, Chang, 2015</marker>
<rawString>Wenzhe Pei, Tao Ge, and Baobao Chang. 2015. An effective neural network model for graph-based dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP2010,</booktitle>
<contexts>
<context position="18781" citStr="Petrov et al., 2010" startWordPosition="3107" endWordPosition="3110">lts. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representat</context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proc. of EMNLP2010, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Andrew S Gordon</author>
</authors>
<title>Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures.</title>
<date>2009</date>
<booktitle>In IWPT,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="19326" citStr="Sagae and Gordon, 2009" startWordPosition="3195" endWordPosition="3198">-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recent</context>
</contexts>
<marker>Sagae, Gordon, 2009</marker>
<rawString>Kenji Sagae and Andrew S. Gordon. 2009. Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures. In IWPT, pages 192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL-2007 Shared Task.</booktitle>
<contexts>
<context position="18894" citStr="Sagae and Tsujii, 2007" startWordPosition="3127" endWordPosition="3130">ng derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contra</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proc. of CoNLL-2007 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan Van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Van Noord, 2007</marker>
<rawString>Gertjan Van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the 10th International Conference on Parsing Technologies, pages 1–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Chris Alberti</author>
<author>Michael Collins</author>
<author>Slav Petrov</author>
</authors>
<title>Structured training for neural network transition-based parsing.</title>
<date>2015</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="18915" citStr="Weiss et al., 2015" startWordPosition="3131" endWordPosition="3134">out using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned featu</context>
</contexts>
<marker>Weiss, Alberti, Collins, Petrov, 2015</marker>
<rawString>David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhou</author>
<author>Yue Zhang</author>
<author>Jiajun Chen</author>
</authors>
<title>A neural probabilistic structured-prediction model for transition-based dependency parsing.</title>
<date>2015</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Zhou, Zhang, Chen, 2015</marker>
<rawString>Hao Zhou, Yue Zhang, and Jiajun Chen. 2015. A neural probabilistic structured-prediction model for transition-based dependency parsing. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
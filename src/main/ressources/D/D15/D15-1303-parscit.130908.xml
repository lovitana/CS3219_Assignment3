<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.9974465">
Deep Convolutional Neural Network Textual Features and Multiple
Kernel Learning for Utterance-Level Multimodal Sentiment Analysis
</title>
<author confidence="0.798202">
Soujanya Poria
</author>
<affiliation confidence="0.6263265">
Temasek Laboratory,
Nanyang Technological
</affiliation>
<address confidence="0.596381">
University, Singapore
</address>
<email confidence="0.997923">
sporia@ntu.edu.sg
</email>
<author confidence="0.992718">
Erik Cambria
</author>
<affiliation confidence="0.821788">
School of Computer Engineering,
Nanyang Technological
</affiliation>
<address confidence="0.671744">
University, Singapore
</address>
<email confidence="0.998233">
cambria@ntu.edu.sg
</email>
<author confidence="0.855607">
Alexander Gelbukh
</author>
<affiliation confidence="0.794404">
Centro de Investigaci6n en
</affiliation>
<address confidence="0.47562">
Computaci6n, Instituto
Politécnico Nacional, Mexico
</address>
<email confidence="0.948768">
www.gelbukh.com
</email>
<sectionHeader confidence="0.992861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999055">
We present a novel way of extracting fea-
tures from short texts, based on the acti-
vation values of an inner layer of a deep
convolutional neural network. We use the
extracted features in multimodal senti-
ment analysis of short video clips repre-
senting one sentence each. We use the
combined feature vectors of textual, vis-
ual, and audio modalities to train a classi-
fier based on multiple kernel learning,
which is known to be good at heteroge-
neous data. We obtain 14% performance
improvement over the state of the art and
present a parallelizable decision-level da-
ta fusion method, which is much faster,
though slightly less accurate.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910931818182">
The advent of the Social Web has enabled any-
one with a smartphone or computer to easily cre-
ate and share their ideas, opinions and content
with millions of other people around the world.
Much of the content being posted and consumed
online is video. With billions of phones, tablets
and PCs shipping today with built-in cameras
and a host of new video-equipped wearables like
Google Glass on the horizon, the amount of vid-
eo on the Internet will only continue to increase.
It has become increasingly difficult for re-
searchers to keep up with this deluge of video
content, let alone organize or make sense of it.
Mining useful knowledge from video is a critical
need that will grow exponentially, in pace with
the global growth of content. This is particularly
important in sentiment analysis (Cambria et al.,
2013a; 2013b; 2014), as both service and product
reviews are gradually shifting from unimodal to
multimodal. We present a method for detecting
sentiment polarity in short video clips of a person
uttering a sentence.
We do it using all three modalities: visual,
such as facial expression, audio, such as pitch,
and textual, the contents of the uttered sentence.
While the visual and the audio modalities pro-
vide additional evidence that improves classifica-
tion accuracy, we found the textual modality to
have the greater impact on the result (Cambria
and Hussain, 2015; Cambria et al., 2013c; Poria
et al., 2015a; 2015b).
In this paper, we propose a novel way for fea-
ture extraction from text. Given a training corpus
with hand-annotated sentiment polarity labels,
following Kim (2014), we train a deep convolu-
tional neural network (CNN) on it. However,
instead of using it as a classifier, as Kim did, we
use the values from its hidden layer as features
for a much more advanced classifier, which gives
superior accuracy. Similar ideas have been sug-
gested in the context of computer vision for deal-
ing with images, but have not been applied in the
context of NLP to textual data, and, specifically,
for sentiment polarity classification.
</bodyText>
<sectionHeader confidence="0.599308" genericHeader="introduction">
2 Overview of the Method
</sectionHeader>
<bodyText confidence="0.999941857142857">
In this paper, we present two different methods
for dealing with multimodal data: feature-level
fusion and decision-level fusion, each one having
its advantages and disadvantages.
We extracted features from the data for each
modality independently. In the case of feature-
level fusion, we then concatenated the obtained
feature vectors and fed the resulting long vector
into a supervised classifier. In the case of deci-
sion-level fusion, we fed the features of each
modality into separate classifiers, and then com-
bined their decisions. Our experimental results
show that both of these methods outperform the
state of the art by a large margin.
</bodyText>
<page confidence="0.928127">
2539
</page>
<note confidence="0.8507435">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2539–2544,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.990677" genericHeader="method">
3 Textual Features
</sectionHeader>
<bodyText confidence="0.936839272727273">
We used a CNN as a trainable feature extractor
to extract features from the textual data. Utter-
ances in the original dataset are in Spanish.
While usually it is better to work directly with
the source language (Wang et al., 2013), in this
work we translated utterances into English using
Google translator. Without the translation into
English, 68.56% accuracy was obtained.
The choice of CNN for feature extraction is
justified by the following considerations:
1. The convolution layers of CNN can be seen
as a feature extractor, whose output is then
fed into a rather simplistic classifier useful
for training the network but not the best at
actual classification. CNN forms local fea-
tures for each word and combine them to
produce a global feature vector for the whole
text. However, the features that CNN builds
internally can be extracted and used as input
for another, more advanced classifier. This
turns CNN, originally a supervised classifier,
into a trainable feature extractor.
</bodyText>
<listItem confidence="0.999077833333333">
2. As a feature extractor, CNN is automatic and
does not rely on handcrafted features. In par-
ticular, it adapts well to the peculiarities of
the specific dataset, in a supervised manner.
3. The features it gives are based on a hierarchy
of local features, reflecting well the context.
</listItem>
<bodyText confidence="0.9998328125">
A drawback of CNN as a classifier is that it
finds only a local optimum, since it uses the
same backpropagation technique as MLP. How-
ever, inspired by ideas introduced in the context
of computer vision (Bluche et al., 2013), we, for
the first time in the context of NLP, extract the
features that CNN builds internally and feed
them into a much more advanced classifier. In
our experiments, this was SVM, or roughly its
multi-kernel version MKL, which is good at
finding the global optimum. Thus, the properties
of CNN and SVM complement each other in
such a way that their advantages are combined.
To form the input for the CNN feature extrac-
tor, for each word in the text we built a 306-
dimensional vector by concatenating two parts:
</bodyText>
<listItem confidence="0.96612">
1. Word embeddings. We used a publicly avail-
</listItem>
<bodyText confidence="0.869493428571429">
able word2vec dictionary (Mikolov et al.,
2013a; 2013b; 2013c), trained on a 100 mil-
lion word corpus from Google News using
the continuous bag of words architecture.
This dictionary provides a 300-dimensional
vector for each word. For words not found in
this dictionary, we used random vectors.
2. Part of speech. We used 6 basic parts of
speech (noun, verb, adjective, adverb, prepo-
sition, conjunction) encoded as a 6-
dimensional binary vector. We used Stanford
Tagger as a part of speech tagger.
For each input text, the input vectors for the
CNN were a concatenation of three parts:
</bodyText>
<listItem confidence="0.997058444444445">
1. Left padding. Two dummy “words” with
zero vectors were added at the beginning of
each text, in order to provide space for con-
volution, since at the convolution layers we
used the kernel size of at most 3.
2. Text. All 306-dimensional vectors corre-
sponding to each word were concatenated,
preserving the word order.
3. Right padding. Again, at least 2 dummy
</listItem>
<bodyText confidence="0.871465272727273">
“words” with zero vectors were added after
each sentence to provide space for convolu-
tion. To form vectors for all texts in the cor-
pus of the same dimensionality, they were al-
so padded at the right with the necessary
amount of additional dummy “words.”
In our experiments, all texts were very short,
consisting of one sentence, the longest one being
of 65 words. Thus all input vectors were of di-
mension 306  (2 + 65 + 2) = 21,114.
The CNN we used consisted of 7 layers:
</bodyText>
<listItem confidence="0.978831388888889">
1. Input layer, of 21,114 neurons.
2. Convolution layer, with a kernel size of 3
and 50 feature maps. The output of this layer
was computed with a non-linear function; we
used the hyperbolic tangent.
3. Max-pool layer with max-pool size of 2.
4. Convolution layer: kernel size of 2, 100 fea-
ture maps, also using the hyperbolic tangent.
5. Max-pool layer with max-pool size of 2.
6. Fully connected layer of 500 neutrons,
whose values were later used as the extracted
features. For regularization, we employed
dropout on the penultimate layer with a con-
straint on L2-norms of the weight vectors.
7. Output softmax layer of 2 neurons, by the
number of training labels—the sentiment po-
larity values: positive or negative. This layer
was used only for training the CNN.
</listItem>
<bodyText confidence="0.999866">
The CNN was trained using a standard back-
propagation procedure. The training data for the
output layer were the known sentiment polarity
labels present in the training corpus for each text.
As features of the given text, we used the val-
ues of the penultimate, fully connected, layer of
the CNN. In this way, we used the last output
</bodyText>
<page confidence="0.941278">
2540
</page>
<table confidence="0.751070222222222">
# features,
with selection
Unimodal
Bimodal
Text Visual Audio Pérez-Rosas Our method
et al. (2013)
without feature with feature
selection selection
500 4568 6373 
437 – – 70.94% 79.14% 79.77%
– 398 – 67.31% 75.22% 76.38%
– – 325 64.85% 74.49% 74.22%
379 109 – 72.39% 84.97% 85.46%
384 – 81 72.88% 83.85% 84.12%
– 242 209 68.86% 82.95% 83.69%
305 74 58 74.09% 87.89% 88.60%
# features, without selection
Multimodal
</table>
<tableCaption confidence="0.996756">
Table 1. Accuracy of state-of-the-art method compared with our method with feature-level fusion.
</tableCaption>
<bodyText confidence="0.976775916666667">
The number of features is for our experiments, not for [16]. Shaded cells are shared with Table 2.
layer of the CNN only for training, but for actual
decision-making, we replaced it with much more
sophisticated classifiers, namely, with SVM or
MKL. Using only CNN as a classifier, 75.50%
was obtained which is in fact lower than the re-
sult (79.77%) obtained when CNN was used to
extract trainable features for the SVM classifier.
We also tried other word vectors having dif-
ferent dimensions, e.g., Glove word vectors and
Collobart’s word vectors. However, the best ac-
curacy was obtained using Google word2vec.
</bodyText>
<sectionHeader confidence="0.997031" genericHeader="method">
4 Visual Features
</sectionHeader>
<bodyText confidence="0.9999710625">
We split each clip into frames (still images).
From each frame, we extracted 68 facial charac-
teristic points (FCPs), such as the position of the
left corner of the left eye, etc., using the facial
recognition library CLM-Z (Baltrušaitis et al.,
2012). For each pair of FCPs, we calculated the
distance. Thus, we characterized each facial ex-
pression by 68  67 / 2 = 2,278 distances. In ad-
dition, for each frame we extracted 6 face posi-
tion coordinates (3D-dimensional displacement
and angular displacement of face and head) using
the GAVAM software. This gave 2,278 + 6 =
2,284 values per frame.
For each of these values, we calculated its
mean value and standard deviation over all
frames of the clip; 4568 features in total.
</bodyText>
<sectionHeader confidence="0.995692" genericHeader="method">
5 Audio Features
</sectionHeader>
<bodyText confidence="0.999992153846154">
We used the openSMILE software (Eyben et al.,
2010) to extract audio features related to the
pitch and voice intensity. This software extracts
the so-called low-level descriptors, such as Mel
frequency cepstral coefficients, spectral centroid,
spectral flux, beat histogram, beat sum, strongest
beat, pause duration, pitch, voice quality, percep-
tual linear predictive coefficients, etc., and their
statistical functions, such as amplitude mean,
arithmetic mean, root quadratic mean, standard
deviation, flatness, skewness, kurtosis, quartiles,
inter-quartile ranges, linear regression slope, etc.
This gave us 6373 audio features in total.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="method">
6 Feature-Level Fusion
</sectionHeader>
<bodyText confidence="0.999839333333334">
Feature-level fusion consisted in concatenation
of the feature vectors obtained for each of the
three modalities. The resulted vectors and along
with the sentiment polarity labels from the train-
ing set, were used to train a classifier with a mul-
tiple kernel learning (MKL) algorithm; we used
the SPF-GMKL implementation (Jain et al.,
2012) designed to deal with heterogeneous data.
Clearly, feature vectors resulted from concatenat-
ing so different data sources are heterogeneous.
The parameters of the classifier were found by
cross validation. We chose a configuration with 8
kernels: 5 RBF with gamma from 0.01 to 0.05
and 3 polynomial with powers 2, 3, 4. We also
tried Simple-MKL; it gave slightly lower results.
</bodyText>
<sectionHeader confidence="0.992927" genericHeader="method">
7 Feature Selection
</sectionHeader>
<bodyText confidence="0.999993384615385">
We significantly reduced the number of features
using feature selection. We used two different
feature selectors: one based on the cyclic correla-
tion-based feature subset selection (CFS) and
another based on principal component analysis
(PCA) with top K features, where K was experi-
mentally selected and varied for different exper-
iment. For example, in case of audio, visual and
textual fusion, K was set to 300.
The union of the features selected by the two
methods was used. For each unimodal, each bi-
modal, and the multimodal experiment, separate
feature extraction was performed. The number of
</bodyText>
<page confidence="0.963791">
2541
</page>
<tableCaption confidence="0.596742">
selected features for each experiment is given in
Table 1. In all cases except for the unimodal ex-
</tableCaption>
<bodyText confidence="0.9899482">
periment with audio modality, feature selection
slightly improved the results, in addition to the
improvement in processing time. In the only case
where feature selection slightly deteriorated the
result, the difference was rather small.
</bodyText>
<sectionHeader confidence="0.973263" genericHeader="method">
8 Unimodal Classification and
Decision-Level Fusion
</sectionHeader>
<bodyText confidence="0.999973063829787">
For unimodal experiments and for decision-level
fusion, we used one classifier per each modality;
specifically, we used SVM. For each modality, in
this way we obtained the probabilities of the la-
bels. In unimodal experiments, we chose the la-
bel with the greater probability.
For decision-level fusion, we added these
probabilities with weights, which were chosen
experimentally, and, again, used the most proba-
ble label. The weights we used for decision-level
fusion were chosen using detailed search with an
intuition that best performing unimodal classifier
has higher importance in the fusion. We do not
claim that these weights are optimal. They are
indeed sub-optimal and hence encourage the
scope of future research.
Knowing a specific decision for the text mo-
dality allowed us to use evidence from a separate
classifier; we used the one based on the Sentic
Patterns (SP) (Poria et al., 2014a). It structures
natural language clauses into a sentiment hierar-
chy used to infer the overall polarity label (posi-
tive vs. negative) for the input sentence. E.g., a
sentence “The car is very old but it is rather not
expensive”, is positive, expressing a favorable
sentiment of the speaker, who recommends pur-
chasing the product. However, “The car is very
old though it is rather not expensive” is negative,
expressing reluctance of the speaker to purchase
the car. Despite the latter contains exactly the
same concepts as the former, the polarity is op-
posite because of the adversative dependency.
On benchmark datasets, SP perform better
than state of the art sentiment classifiers, which
outperforms the textual classifier described in
Section 3. Since SP are a superior classifier, we
used it as a bias to modify the weight of the tex-
tual modality. However, SP do not report a prob-
ability, but only a binary decision, so we only
used them to tweak the weights in the probability
mix: when the text-based unimodal classifier
agreed with SP, we increased the weight of the
text modality. Another benefit of the decision-
level fusion is its speed, since fewer features are
used for each classifier and since SVM, used as a
unimodal classifier, is faster than MKL. In addi-
tion, separate classifiers can be run in parallel.
</bodyText>
<sectionHeader confidence="0.979561" genericHeader="method">
9 Experimental Results
</sectionHeader>
<bodyText confidence="0.991359">
We report results for tenfold cross-validation.
</bodyText>
<subsectionHeader confidence="0.860013">
9.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999990058823529">
We experimented on the dataset described by
Morency et al. (2011). The dataset consists of
498 short video fragments where a person utters
one sentence. The items are manually tagged for
sentiment polarity, which can be positive, nega-
tive, or neutral. We discarded the neutral items
from the dataset, which gave us a dataset of 447
clips tagged as positive or negative.
The video in the dataset is present in MP4
format with the resolution of 360  480, to which
the developers converted all videos originally
collected in different formats with different reso-
lution. The duration of the clips is about 5 sec-
onds on average. About 80% of the clips present
female speakers. The developers provided tran-
scription of the text of the sentences, which we
used in our textual modality processing.
</bodyText>
<sectionHeader confidence="0.80516" genericHeader="method">
9.2 Results for Each Modality Separately
</sectionHeader>
<bodyText confidence="0.999990769230769">
As a baseline, we used classifiers trained on fea-
tures extracted from each modality separately.
The results are shown in Table 1, unimodal sec-
tion. The number of features after feature selec-
tion is indicate for the modality used.
The table shows that the best results were ob-
tained for textual modality; the visual modality
performed worse, and the audio was least useful.
However, even the worst of our results is much
better than the state-of-the-art (Pérez-Rosas et
al., 2013). In each modality separately, our re-
sults outperform the state of the art by about 9%,
which is about 30% reduction in error rate.
</bodyText>
<subsectionHeader confidence="0.985196">
9.3 Results with Feature-level Fusion
</subsectionHeader>
<bodyText confidence="0.99999425">
As a yet another baseline, we tried feature-level
fusion of only two modalities.
The results are shown in Table 1, bimodal sec-
tion. Again, the number of features after feature
selection is indicated for the two modalities used.
As expected, missing the audio features was the
least important, missing the video features was
more significant, and missing the text features
was most painful for the accuracy.
Even the worst result obtained with fusion of
two modalities outperformed our best unimodal
result, as well as the best result of the state of the
</bodyText>
<page confidence="0.969554">
2542
</page>
<table confidence="0.999744727272727">
Sentic Weights Fusion Accuracy
Patterns
Text Visual Audio Feature Decision
Unimodal accuracy 81.73% 79.77% 76.38% 74.22%
Unimodal 3-way majority voting 72.83%
3-way no 0.45 0.3 0.25 81.24%
yes 0.5 / 0.25 0.3 / 0.4 0.2 / 0.35 82.06%
Bimodal + + 0.3 85.46% 85.53%
with unimodal + 0.23 + 84.12% 84.86%
no 0.4 + + 83.69% 84.48%
yes 0.45 / 0.3 + + same 86.27%
</table>
<tableCaption confidence="0.999839">
Table 2. Accuracy of our method with decision-level fusion and feature selection.
</tableCaption>
<bodyText confidence="0.998777">
art. Finally, the best result, shown in the multi-
modal section of Table 1, was obtained when all
three modalities were fused. This result outper-
forms the corresponding result of the state of the
art by 14%, which gives 56% of error reduction.
</bodyText>
<subsectionHeader confidence="0.924505">
9.4 Results with Decision-level Fusion
</subsectionHeader>
<bodyText confidence="0.999678409090909">
The results for decision-level fusion are shown in
Table 2, last column. The shaded cells are shared
with Table 1. In the second section, three classi-
fiers were fused at the decision level. In the third
section, two modalities indicated with the plus
sign were fused at the feature level (giving the
accuracy indicated in the penultimate column)
and then this classifier was fused at the decision
level with the third modality. The weights corre-
spond to the share of each modality. In the last
section, the weight for the unimodal classifier is
shown, and the weight for the bimodal classifier
was its complement to 1.
For the experiments that involved tweaking of
the weights with the SP oracle, pairs of weights
are shown: the weight used when the text mo-
dality results corresponded (left) with the SP
prediction and the weight used when they did not
(right). The accuracy with at least partial feature-
level fusion was better than that for no feature-
level fusion at all (3-way). As in the bimodal sec-
tion of Table 1, excluding audio from feature-
level fusion was least problematic and excluding
text was most problematic.
In all cases, decision-level fusion did not sig-
nificantly improve the accuracy of the best sum-
mand. However, separating text-based classifier
permitted us to use the Sentic Patterns tweak,
which cannot be used if the text-only results are
not known. With this tweak, the best result was
obtained. Even with this improvement, the accu-
racy of decision-level fusion was slightly lower
than that of feature-level fusion; in exchange for
much about twice better processing speed.
A baseline decision level evaluation strategy
was taken which allowed us to take majority vot-
ing among the predicted class labels by unimodal
classifiers. Based on this strategy the final class
label was chosen by the maximum of the three
unimodal models’ votes. For the multimodal fu-
sion using this baseline method only 72.83% ac-
curacy was obtained. As expected the proposed
feature and decision level fusion outperformed
this baseline method by a large margin.
</bodyText>
<sectionHeader confidence="0.993668" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999969827586207">
We have presented a novel method for determin-
ing sentiment polarity in video clips of people
speaking. We combine evidence from the words
they utter, the facial expression, and the speech
sound. The main novelty of this paper consists in
using deep CNN to extract features from text and
in using MKL to classify the multimodal hetero-
geneous fused feature vectors.
We also presented a faster variant of our
method, based on decision-level fusion. In case
of the decision level fusion experiment, the cou-
pling of Sentic Patterns to determine the weight
of textual modality has enriched the performance
of multimodal sentiment analysis framework
considerably. However, the parameter selection
for decision level fusion produced suboptimal
results. A systematic mathematical approach for
decision level fusion is an important future work.
Our future work will focus on extracting more
relevant features from the visual modality. We
will employ deep 3D convolutional neural net-
works on this modality for feature extraction. We
will use a feature selection method to obtain key
features; this will ensure the scalability as well as
stability of the framework. We will continue our
study of reasoning over text (Jimenez et al.,
2015; Pakray et al., 2011; Sidorov et al., 2014;
Sidorov, 2014) and in particular of concept-
based sentiment analysis (Poria et al., 2014b).
</bodyText>
<page confidence="0.967165">
2543
</page>
<sectionHeader confidence="0.990144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99998561682243">
1. Tadas Baltrusaitis, Peter Robinson, and Louis-
Philippe Morency. 2012. 3D constrained local
model for rigid and non-rigid facial tracking. In
IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp. 2610–2617.
2. Théodore Bluche, Hermann Ney, and Christo-
pher Kermorvant. 2013. Feature extraction with
convolutional neural networks for handwritten
word recognition. In 12th International Confer-
ence on Document Analysis and Recognition
(ICDAR), pp. 285–289.
3. Erik Cambria and Amir Hussain. 2015. Sentic
Computing: A Common-Sense-Based Frame-
work for Concept-Level Sentiment Analysis.
Springer, Cham, Switzerland.
4. Erik Cambria, Björn Schuller, Bing Liu, Haixun
Wang, and Catherine Havasi. 2013a. Knowle-
dge-based approaches to concept-level sentiment
analysis. IEEE Intelligent Systems 28(2):12–14.
5. Erik Cambria, Björn Schuller, Bing Liu, Haixun
Wang, and Catherine Havasi. 2013b. Statistical
approaches to concept-level sentiment analysis.
IEEE Intelligent Systems 28(3):6–9.
6. Erik Cambria, Haixun Wang, and Bebo White.
2014. Guest editorial: Big social data analysis.
Knowledge-Based Systems 69:1–2.
7. Erik Cambria, Newton Howard, Jane Hsu, and
Amir Hussain. 2013c. Sentic blending: Scalable
multimodal fusion for the continuous interpreta-
tion of semantics and sentics. In: IEEE SSCI, pp.
108–117, Singapore.
8. Florian Eyben, Martin Wöllmer, and Björn
Schuller. 2010. OpenSMILE: The Munich versa-
tile and fast open-source audio feature extractor.
In Proceedings of the International Conference
on Multimedia, pp. 1459–1462.
9. Ashesh Jain, Swaminathan V. N. Vishwanathan,
and Manik Varma. 2012. SPF-GMKL: General-
ized multiple kernel learning with a million ker-
nels. In Proceedings of the 18th ACM SIGKDD
International Conference on Knowledge Discov-
ery and Data Mining, pp. 750–758.
10. Sergio Jimenez, Fabio A. Gonzalez, and Alexan-
der Gelbukh. 2015. Soft Cardinality in Semantic
Text Processing: Experience of the SemEval In-
ternational Competitions. Polibits 51:63–72.
11. Yoon Kim. 2014. Convolutional Neural Net-
works for Sentence Classification. In EMNLP,
pp. 1746–1751.
12. Tomas Mikolov, Kai Chen, Greg Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of
word representations in vector space. In Intern.
Conference on Learning Representations (ICLR).
13. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In NIPS.
14. Tomas Mikolov, Wen-tau Yih, and Geoffrey
Zweig. 2013c. Linguistic regularities in continu-
ous space word representations. In NAACL-HLT.
15. Louis-Philippe Morency, Rada Mihalcea, and
Payal Doshi. 2011. Towards multimodal senti-
ment analysis: Harvesting opinions from the
web. In 13th International Conference on Multi-
modal Interfaces, pp. 169–176.
16. Verónica Pérez-Rosas, Rada Mihalcea, and Lou-
is-Philippe Morency. 2013. Utterance-Level
Multimodal Sentiment Analysis. In ACL, pp.
973–982.
17. Partha Pakray, Snehasis Neogi, Pinaki Bhaskar,
Soujanya Poria, Sivaji Bandyopadhyay, and Al-
exander Gelbukh. 2011. A textual entailment
system using anaphora resolution. In: System Re-
port. Text Analysis Conference, Recognizing
Textual Entailment Track. Notebook.
18. Soujanya Poria, Erik Cambria, Gregoire Winter-
stein, and Guang-Bin Huang. 2014a. Sentic pat-
terns: Dependency-based rules for concept-level
sentiment analysis. Knowledge-Based Systems
69:45–63.
19. Soujanya Poria, Erik Cambria, Amir Hussain,
and Guang-Bin Huang. 2015a. Towards an intel-
ligent framework for multimodal affective data
analysis. Neural Networks 63:104–116.
20. Soujanya Poria, Erik Cambria, Newton Howard,
Guang-Bin Huang, and Amir Hussain. 2015b.
Fusing audio, visual and textual clues for senti-
ment analysis from multimodal content. Neuro-
computing, DOI: 10.1016/j.neucom.2015.01.095
21. Soujanya Poria, Alexander Gelbukh, Erik Cam-
bria, Amir Hussain, and Guang-Bin Huang.
2014b. EmoSenticSpace: A novel framework for
affective common-sense reasoning. Knowledge-
Based Systems 69:108–123.
22. Grigori Sidorov, Alexander Gelbukh, Helena
Gómez-Adorno, and David Pinto. 2014. Soft
Similarity and Soft Cosine Measure: Similarity
of Features in Vector Space Model. Com-
putación y Sistemas 18(3):491–504.
23. Grigori Sidorov. 2014. Should Syntactic N-
grams Contain Names of Syntactic Relations? In-
ternational Journal of Computational Linguistics
and Applications 5(2):23–46.
24. Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu,
and Amir Hussain. 2013. Common sense
knowledge for handwritten Chinese recognition.
Cognitive Computation 5(2):234–242.
</reference>
<page confidence="0.992148">
2544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.018504">
<title confidence="0.790253333333333">Deep Convolutional Neural Network Textual Features and Kernel Learning for Utterance-Level Multimodal Sentiment Analysis Soujanya Poria</title>
<author confidence="0.503293">Temasek</author>
<affiliation confidence="0.783384">Nanyang</affiliation>
<address confidence="0.568208">University, Singapore</address>
<email confidence="0.9879">sporia@ntu.edu.sg</email>
<author confidence="0.986799">Erik Cambria</author>
<affiliation confidence="0.827417">School of Computer Nanyang</affiliation>
<address confidence="0.65672">University, Singapore</address>
<email confidence="0.984494">cambria@ntu.edu.sg</email>
<author confidence="0.997293">Alexander Gelbukh</author>
<affiliation confidence="0.9120355">Centro de Investigaci6n Computaci6n,</affiliation>
<address confidence="0.557876">Politécnico Nacional, Mexico</address>
<email confidence="0.999756">www.gelbukh.com</email>
<abstract confidence="0.998031117647059">We present a novel way of extracting features from short texts, based on the activation values of an inner layer of a deep convolutional neural network. We use the extracted features in multimodal sentiment analysis of short video clips representing one sentence each. We use the combined feature vectors of textual, visual, and audio modalities to train a classifier based on multiple kernel learning, which is known to be good at heterogeneous data. We obtain 14% performance improvement over the state of the art and present a parallelizable decision-level data fusion method, which is much faster, though slightly less accurate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tadas Baltrusaitis</author>
<author>Peter Robinson</author>
<author>LouisPhilippe Morency</author>
</authors>
<title>3D constrained local model for rigid and non-rigid facial tracking.</title>
<date>2012</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>2610--2617</pages>
<marker>1.</marker>
<rawString>Tadas Baltrusaitis, Peter Robinson, and LouisPhilippe Morency. 2012. 3D constrained local model for rigid and non-rigid facial tracking. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2610–2617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Théodore Bluche</author>
<author>Hermann Ney</author>
<author>Christopher Kermorvant</author>
</authors>
<title>Feature extraction with convolutional neural networks for handwritten word recognition.</title>
<date>2013</date>
<booktitle>In 12th International Conference on Document Analysis and Recognition (ICDAR),</booktitle>
<pages>285--289</pages>
<marker>2.</marker>
<rawString>Théodore Bluche, Hermann Ney, and Christopher Kermorvant. 2013. Feature extraction with convolutional neural networks for handwritten word recognition. In 12th International Conference on Document Analysis and Recognition (ICDAR), pp. 285–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Amir Hussain</author>
</authors>
<title>Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis.</title>
<date>2015</date>
<publisher>Springer,</publisher>
<location>Cham, Switzerland.</location>
<marker>3.</marker>
<rawString>Erik Cambria and Amir Hussain. 2015. Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis. Springer, Cham, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Björn Schuller</author>
<author>Bing Liu</author>
<author>Haixun Wang</author>
<author>Catherine Havasi</author>
</authors>
<title>Knowledge-based approaches to concept-level sentiment analysis.</title>
<date>2013</date>
<journal>IEEE Intelligent Systems</journal>
<volume>28</volume>
<issue>2</issue>
<marker>4.</marker>
<rawString>Erik Cambria, Björn Schuller, Bing Liu, Haixun Wang, and Catherine Havasi. 2013a. Knowledge-based approaches to concept-level sentiment analysis. IEEE Intelligent Systems 28(2):12–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Björn Schuller</author>
<author>Bing Liu</author>
<author>Haixun Wang</author>
<author>Catherine Havasi</author>
</authors>
<title>Statistical approaches to concept-level sentiment analysis.</title>
<date>2013</date>
<journal>IEEE Intelligent Systems</journal>
<volume>28</volume>
<issue>3</issue>
<marker>5.</marker>
<rawString>Erik Cambria, Björn Schuller, Bing Liu, Haixun Wang, and Catherine Havasi. 2013b. Statistical approaches to concept-level sentiment analysis. IEEE Intelligent Systems 28(3):6–9.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Erik Cambria</author>
<author>Haixun Wang</author>
<author>Bebo White</author>
</authors>
<marker>6.</marker>
<rawString>Erik Cambria, Haixun Wang, and Bebo White.</rawString>
</citation>
<citation valid="false">
<title>Guest editorial: Big social data analysis.</title>
<journal>Knowledge-Based Systems</journal>
<pages>69--1</pages>
<contexts>
<context position="2698" citStr="(2014)" startWordPosition="422" endWordPosition="422">lips of a person uttering a sentence. We do it using all three modalities: visual, such as facial expression, audio, such as pitch, and textual, the contents of the uttered sentence. While the visual and the audio modalities provide additional evidence that improves classification accuracy, we found the textual modality to have the greater impact on the result (Cambria and Hussain, 2015; Cambria et al., 2013c; Poria et al., 2015a; 2015b). In this paper, we propose a novel way for feature extraction from text. Given a training corpus with hand-annotated sentiment polarity labels, following Kim (2014), we train a deep convolutional neural network (CNN) on it. However, instead of using it as a classifier, as Kim did, we use the values from its hidden layer as features for a much more advanced classifier, which gives superior accuracy. Similar ideas have been suggested in the context of computer vision for dealing with images, but have not been applied in the context of NLP to textual data, and, specifically, for sentiment polarity classification. 2 Overview of the Method In this paper, we present two different methods for dealing with multimodal data: feature-level fusion and decision-level</context>
</contexts>
<marker>2014.</marker>
<rawString>Guest editorial: Big social data analysis. Knowledge-Based Systems 69:1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Newton Howard</author>
<author>Jane Hsu</author>
<author>Amir Hussain</author>
</authors>
<title>Sentic blending: Scalable multimodal fusion for the continuous interpretation of semantics and sentics.</title>
<date>2013</date>
<booktitle>In: IEEE SSCI,</booktitle>
<pages>108--117</pages>
<marker>7.</marker>
<rawString>Erik Cambria, Newton Howard, Jane Hsu, and Amir Hussain. 2013c. Sentic blending: Scalable multimodal fusion for the continuous interpretation of semantics and sentics. In: IEEE SSCI, pp. 108–117, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Eyben</author>
<author>Martin Wöllmer</author>
<author>Björn Schuller</author>
</authors>
<title>OpenSMILE: The Munich versatile and fast open-source audio feature extractor.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Multimedia,</booktitle>
<pages>1459--1462</pages>
<marker>8.</marker>
<rawString>Florian Eyben, Martin Wöllmer, and Björn Schuller. 2010. OpenSMILE: The Munich versatile and fast open-source audio feature extractor. In Proceedings of the International Conference on Multimedia, pp. 1459–1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashesh Jain</author>
<author>Swaminathan V N Vishwanathan</author>
<author>Manik Varma</author>
</authors>
<title>SPF-GMKL: Generalized multiple kernel learning with a million kernels.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>750--758</pages>
<marker>9.</marker>
<rawString>Ashesh Jain, Swaminathan V. N. Vishwanathan, and Manik Varma. 2012. SPF-GMKL: Generalized multiple kernel learning with a million kernels. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 750–758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Fabio A Gonzalez</author>
<author>Alexander Gelbukh</author>
</authors>
<date>2015</date>
<booktitle>Soft Cardinality in Semantic Text Processing: Experience of the SemEval International Competitions. Polibits</booktitle>
<pages>51--63</pages>
<marker>10.</marker>
<rawString>Sergio Jimenez, Fabio A. Gonzalez, and Alexander Gelbukh. 2015. Soft Cardinality in Semantic Text Processing: Experience of the SemEval International Competitions. Polibits 51:63–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional Neural Networks for Sentence Classification. In</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1746--1751</pages>
<marker>11.</marker>
<rawString>Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In EMNLP, pp. 1746–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Intern. Conference on Learning Representations (ICLR).</booktitle>
<marker>12.</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Intern. Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<marker>13.</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In NAACL-HLT.</booktitle>
<marker>14.</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis-Philippe Morency</author>
<author>Rada Mihalcea</author>
<author>Payal Doshi</author>
</authors>
<title>Towards multimodal sentiment analysis: Harvesting opinions from the web.</title>
<date>2011</date>
<booktitle>In 13th International Conference on Multimodal Interfaces,</booktitle>
<pages>169--176</pages>
<marker>15.</marker>
<rawString>Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi. 2011. Towards multimodal sentiment analysis: Harvesting opinions from the web. In 13th International Conference on Multimodal Interfaces, pp. 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verónica Pérez-Rosas</author>
<author>Rada Mihalcea</author>
<author>Louis-Philippe Morency</author>
</authors>
<title>Utterance-Level Multimodal Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>973--982</pages>
<contexts>
<context position="9188" citStr="[16]" startWordPosition="1520" endWordPosition="1520">ay, we used the last output 2540 # features, with selection Unimodal Bimodal Text Visual Audio Pérez-Rosas Our method et al. (2013) without feature with feature selection selection 500 4568 6373  437 – – 70.94% 79.14% 79.77% – 398 – 67.31% 75.22% 76.38% – – 325 64.85% 74.49% 74.22% 379 109 – 72.39% 84.97% 85.46% 384 – 81 72.88% 83.85% 84.12% – 242 209 68.86% 82.95% 83.69% 305 74 58 74.09% 87.89% 88.60% # features, without selection Multimodal Table 1. Accuracy of state-of-the-art method compared with our method with feature-level fusion. The number of features is for our experiments, not for [16]. Shaded cells are shared with Table 2. layer of the CNN only for training, but for actual decision-making, we replaced it with much more sophisticated classifiers, namely, with SVM or MKL. Using only CNN as a classifier, 75.50% was obtained which is in fact lower than the result (79.77%) obtained when CNN was used to extract trainable features for the SVM classifier. We also tried other word vectors having different dimensions, e.g., Glove word vectors and Collobart’s word vectors. However, the best accuracy was obtained using Google word2vec. 4 Visual Features We split each clip into frames </context>
</contexts>
<marker>16.</marker>
<rawString>Verónica Pérez-Rosas, Rada Mihalcea, and Louis-Philippe Morency. 2013. Utterance-Level Multimodal Sentiment Analysis. In ACL, pp. 973–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pakray</author>
<author>Snehasis Neogi</author>
<author>Pinaki Bhaskar</author>
<author>Soujanya Poria</author>
<author>Sivaji Bandyopadhyay</author>
<author>Alexander Gelbukh</author>
</authors>
<title>A textual entailment system using anaphora resolution. In: System Report. Text Analysis Conference, Recognizing Textual Entailment Track.</title>
<date>2011</date>
<publisher>Notebook.</publisher>
<marker>17.</marker>
<rawString>Partha Pakray, Snehasis Neogi, Pinaki Bhaskar, Soujanya Poria, Sivaji Bandyopadhyay, and Alexander Gelbukh. 2011. A textual entailment system using anaphora resolution. In: System Report. Text Analysis Conference, Recognizing Textual Entailment Track. Notebook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soujanya Poria</author>
<author>Erik Cambria</author>
<author>Gregoire Winterstein</author>
<author>Guang-Bin Huang</author>
</authors>
<title>Sentic patterns: Dependency-based rules for concept-level sentiment analysis. Knowledge-Based Systems 69:45–63.</title>
<date>2014</date>
<marker>18.</marker>
<rawString>Soujanya Poria, Erik Cambria, Gregoire Winterstein, and Guang-Bin Huang. 2014a. Sentic patterns: Dependency-based rules for concept-level sentiment analysis. Knowledge-Based Systems 69:45–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soujanya Poria</author>
<author>Erik Cambria</author>
<author>Amir Hussain</author>
<author>Guang-Bin Huang</author>
</authors>
<title>Towards an intelligent framework for multimodal affective data analysis.</title>
<date>2015</date>
<journal>Neural Networks</journal>
<pages>63--104</pages>
<marker>19.</marker>
<rawString>Soujanya Poria, Erik Cambria, Amir Hussain, and Guang-Bin Huang. 2015a. Towards an intelligent framework for multimodal affective data analysis. Neural Networks 63:104–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soujanya Poria</author>
<author>Erik Cambria</author>
<author>Newton Howard</author>
<author>Guang-Bin Huang</author>
<author>Amir Hussain</author>
</authors>
<title>Fusing audio, visual and textual clues for sentiment analysis from multimodal content.</title>
<date>2015</date>
<pages>10--1016</pages>
<location>Neurocomputing, DOI:</location>
<marker>20.</marker>
<rawString>Soujanya Poria, Erik Cambria, Newton Howard, Guang-Bin Huang, and Amir Hussain. 2015b. Fusing audio, visual and textual clues for sentiment analysis from multimodal content. Neurocomputing, DOI: 10.1016/j.neucom.2015.01.095</rawString>
</citation>
<citation valid="false">
<authors>
<author>Soujanya Poria</author>
</authors>
<title>Alexander Gelbukh, Erik Cambria, Amir Hussain, and Guang-Bin Huang. 2014b. EmoSenticSpace: A novel framework for affective common-sense reasoning. KnowledgeBased Systems 69:108–123.</title>
<marker>21.</marker>
<rawString>Soujanya Poria, Alexander Gelbukh, Erik Cambria, Amir Hussain, and Guang-Bin Huang. 2014b. EmoSenticSpace: A novel framework for affective common-sense reasoning. KnowledgeBased Systems 69:108–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Alexander Gelbukh</author>
<author>Helena Gómez-Adorno</author>
<author>David Pinto</author>
</authors>
<title>Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model. Computación y Sistemas 18(3):491–504.</title>
<date>2014</date>
<marker>22.</marker>
<rawString>Grigori Sidorov, Alexander Gelbukh, Helena Gómez-Adorno, and David Pinto. 2014. Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model. Computación y Sistemas 18(3):491–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
</authors>
<title>Should Syntactic Ngrams Contain Names of Syntactic Relations?</title>
<date>2014</date>
<journal>International Journal of Computational Linguistics and Applications</journal>
<volume>5</volume>
<issue>2</issue>
<marker>23.</marker>
<rawString>Grigori Sidorov. 2014. Should Syntactic Ngrams Contain Names of Syntactic Relations? International Journal of Computational Linguistics and Applications 5(2):23–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiu-Feng Wang</author>
<author>Erik Cambria</author>
<author>Cheng-Lin Liu</author>
<author>Amir Hussain</author>
</authors>
<title>Common sense knowledge for handwritten Chinese recognition.</title>
<date>2013</date>
<journal>Cognitive Computation</journal>
<volume>5</volume>
<issue>2</issue>
<marker>24.</marker>
<rawString>Qiu-Feng Wang, Erik Cambria, Cheng-Lin Liu, and Amir Hussain. 2013. Common sense knowledge for handwritten Chinese recognition. Cognitive Computation 5(2):234–242.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9987785">
Parsing English into Abstract Meaning Representation Using
Syntax-Based Machine Translation
</title>
<author confidence="0.997792">
Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May
</author>
<affiliation confidence="0.998656666666667">
Information Sciences Institute
Computer Science Department
University of Southern California
</affiliation>
<email confidence="0.981953">
{pust, ulf, knight, marcu, jonmay}@isi.edu
</email>
<sectionHeader confidence="0.994486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855307692308">
We present a parser for Abstract Meaning
Representation (AMR). We treat English-
to-AMR conversion within the framework
of string-to-tree, syntax-based machine
translation (SBMT). To make this work,
we transform the AMR structure into a
form suitable for the mechanics of SBMT
and useful for modeling. We introduce
an AMR-specific language model and add
data and features drawn from semantic re-
sources. Our resulting AMR parser signif-
icantly improves upon state-of-the-art re-
sults.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998804304347826">
Abstract Meaning Representation (AMR) is a
compact, readable, whole-sentence semantic an-
notation (Banarescu et al., 2013). It includes en-
tity identification and typing, PropBank semantic
roles (Kingsbury and Palmer, 2002), individual en-
tities playing multiple roles, as well as treatments
of modality, negation, etc. AMR abstracts in nu-
merous ways, e.g., by assigning the same concep-
tual structure to fear (v), fear (n), and afraid (adj).
Figure 1 gives an example.
AMR parsing is a new research problem, with
only a few papers published to date (Flani-
gan et al., 2014; Wang et al., 2015) and a
publicly available corpus of more than 10,000
English/AMR pairs.1 New research problems
can be tackled either by developing new algo-
rithms/techniques (Flanigan et al., 2014; Wang
et al., 2015) or by adapting existing algo-
rithms/techniques to the problem at hand. In this
paper, we investigate the second approach.
The AMR parsing problem bears a strong for-
mal resemblance to syntax-based machine transla-
tion (SBMT) of the string-to-tree variety, in that
</bodyText>
<subsectionHeader confidence="0.74923">
1LDC Catalog number 2014T12
</subsectionHeader>
<bodyText confidence="0.997692">
The soldier was not afraid of dying.
The soldier was not afraid to die.
The soldier did not fear death.
</bodyText>
<figureCaption confidence="0.970603">
Figure 1: An Abstract Meaning Representation
(AMR) with several English renderings.
</figureCaption>
<bodyText confidence="0.963290333333333">
a string is transformed into a nested structure in
both cases. Because of this, it is appealing to ap-
ply the substantial body of techniques already in-
vented for SBMT2 to AMR parsing. By re-using
an SBMT inference engine instead of creating cus-
tom inference procedures, we lose the ability to
embed some task-specific decisions into a custom
transformation process, as is done by Flanigan et
al. (2014) and Wang et al. (2015). However, we
reap the efficiency gains that come from work-
ing within a tested, established framework. Fur-
thermore, since production-level SBMT systems
are widely available, anyone wishing to generate
AMR from text need only follow our recipe and
retrain an existing framework with relevant data to
quickly obtain state-of-the-art results.
Since SBMT and AMR parsing are, in fact,
distinct tasks, as outlined in Figure 2, to adapt
the SBMT parsing framework to AMR parsing,
we develop novel representations and techniques.
Some of our key ideas include:
1. Introducing an AMR-equivalent representa-
tion that is suitable for string-to-tree SBMT
rule extraction and decoding (Section 4.1).
</bodyText>
<footnote confidence="0.676015">
2See e.g. the related work section of Huck et al. (2014).
</footnote>
<figure confidence="0.717288333333333">
soldier
die-01
fear-01
</figure>
<page confidence="0.867876">
1143
</page>
<note confidence="0.997523818181818">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
SBMT AMR parsing
Target tree graph
Nodes labeled unlabeled
Edges unlabeled labeled
Alignments words to leaves words to leaves
+ words to edges
Children ordered unordered
Accuracy BLEU (Papineni Smatch (Cai and
Metric et al., 2002) Knight, 2013)
</note>
<figureCaption confidence="0.99286">
Figure 2: Differences between AMR parsing and
syntax-based machine translation (SBMT).
</figureCaption>
<listItem confidence="0.9777917">
2. Proposing a target-side reordering technique
that leverages the fact that child nodes in
AMR are unordered (Section 4.4).
3. Introducing a hierarchical AMR-specific lan-
guage model to ensure generation of likely
parent-child relationships (Section 5).
4. Integrating several semantic knowledge
sources into the task (Section 6).
5. Developing tuning methods that maximize
Smatch (Cai and Knight, 2013) (Section 7).
</listItem>
<bodyText confidence="0.9188942">
By applying these key ideas, which constitute
lightweight changes on a baseline SBMT system,
we achieve state-of-the-art AMR parsing results.
We next describe our baseline, and then describe
how we adapt it to AMR parsing.
</bodyText>
<sectionHeader confidence="0.967069" genericHeader="introduction">
2 Syntax-Based Machine Translation
</sectionHeader>
<bodyText confidence="0.9894634">
Our baseline SBMT system proceeds as follows.
Given a corpus of (source string, target tree,
source-target word alignment) sentence translation
training tuples and a corpus of (source, target,
score) sentence translation tuning tuples:
</bodyText>
<listItem confidence="0.994191333333333">
1. Rule extraction: A grammar of string-to-
tree rules is induced from training tuples us-
ing the GHKM algorithm (Galley et al., 2004;
Galley et al., 2006).
2. Local feature calculation: Statistical and in-
dicator features, as described by Chiang et al.
(2009), are calculated over the rule grammar.
3. Language model calculation: A Kneser-
Ney-interpolated 5-gram language model
(Chen and Goodman, 1996) is learned from
the yield of the target training trees.
4. Decoding: A beamed bottom-up chart de-
coder (Pust and Knight, 2009; Hopkins
and Langmead, 2010) calculates the optimal
derivations given a source string and feature
parameter set.
5. Tuning: Feature parameters are optimized
using the MIRA learning approach (Chiang
</listItem>
<figure confidence="0.42038025">
Corpus Sentences Tokens
10,313 218,021
1,368 29,484
1,371 30,263
</figure>
<tableCaption confidence="0.958089">
Table 1: Data splits of AMR 1.0, used in this work.
Tokens are English, after tokenization.
</tableCaption>
<bodyText confidence="0.998470333333333">
et al., 2009) to maximize the objective, typi-
cally BLEU (Papineni et al., 2002), associated
with a tuning corpus.
We initially use this system with no modifica-
tions and pretend that English–AMR is a language
pair indistinct from any other.
</bodyText>
<sectionHeader confidence="0.991901" genericHeader="method">
3 Data and Comparisons
</sectionHeader>
<bodyText confidence="0.9999787">
We use English–AMR data from the AMR 1.0 cor-
pus, LDC Catalog number 2014T12. In contrast
to narrow-domain data sources that are often used
in work related to semantic parsing (Price, 1990;
Zelle, 1995; Kuhlmann et al., 2004), the AMR cor-
pus covers a broad range of news and web forum
data. We use the training, development, and test
splits specified in the AMR corpus (Table 1). The
training set is used for rule extraction, language
modeling, and statistical rule feature calculation.
The development set is used both for parameter
optimization and qualitatively for hill-climbing.
The test set is held out blind for evaluation. We
preprocess the English with a simple rule-based
tokenizer and, except where noted, lowercase all
data. We obtain English–AMR alignments by us-
ing the unsupervised alignment approach of Pour-
damghani et al. (2014), which linearizes the AMR
and then applies the models of Brown et al. (1993)
with an additional symmetrization constraint.
All parsing results reported in this work are
obtained with the Smatch 1.0 software (Cai and
Knight, 2013). We compare our results to those of
Flanigan et al. (2014) on the AMR 1.0 data splits;
we run that work’s JAMR software according to
the provided instructions.3 We also compare our
results to published scores in the recent work of
Wang et al. (2015). Their work uses slightly dif-
ferent data than that used here 4 but in practice we
have not seen significant variation in results.
</bodyText>
<footnote confidence="0.998390666666667">
3https://github.com/jflanigan/jamr
4LDC2013E117, a pre-released version of LDC2014T12
that is not generally available.
</footnote>
<figure confidence="0.966358666666667">
Training
Development
Test
1144
polarity
-
fear-01
die-01
soldier
(b) Disconnecting multiple parents of
(a)
X
Pfear-01 PARG0 X PARG1 X
ARG1
X
soldier
(c) Edge labels of (b) pushed to leaves,
preterminals added
fear-01 ARG0
PsoldierPdie-01 PARG1
die-01 ARG1 P**
Ppolarity
X
die-01
soldier
fear-01
(a) The original AMR
X
X
fear-01
ARG1
fear-01
X
ARG1
PARG1
X
Ppolarity
X
polarity
X
-
ARG0
</figure>
<equation confidence="0.897447768292683">
X
PARG1
ARG1
X
Ppolarity
X
X
X
Pfear-01 PARG0 X PARG1 X
polarity -
Spolarity
Ppolarity
-
Pfear-01 PARG0
ARG0 soldier
P*
die-01 ARG1 *
Pfear-01 PARG0
ARG0 soldier
P*
die-01 ARG1 *
fear-01 ARG0
soldier
polarity
P*
die-01 ARG1 *
fear-01
Psoldier Pdie-01 PARG1
fear-01
Psoldier Pdie-01PARG1
ARG1
PsoldierPdie-01 PARG1
X
(d) Restructuring (c) with concept la- (e) Restructuring (c) with role labels
bels as intermediates as intermediates (f) String preterminal relabeling of (c)
X
polarity
die-01 ARG1
fear-01
X
ARG0Spolarity Pdie-01
soldier
PARG0
Psoldier
Ppolarity
X
ARG0
polarity
Pfear-01
PARG1
ARG1
X
PARG1
X
P*
*
X
X
PARG0
X
X
P*
PARG1
ARG0
soldier
Ppolarity
polarity
Psoldier
PARG1
X Pfear-01
Pdie-01
*
ARG1
ARG1
fear-01
die-01
Pfear-01 PARG0 X PARG1 X
ARG1
PsoldierPdie-01 PARG1
P*
die-01 ARG1
*
</equation>
<bodyText confidence="0.256535">
The soldier was not afraid of dying
</bodyText>
<figure confidence="0.903505666666667">
The soldier was not afraid of dying
(g) Original alignment of English to (i) Restructured, relabeled, and re-
(c) (h) After reordering of (g) ordered tree: (e), (f), and (h)
</figure>
<figureCaption confidence="0.845328">
Figure 3: Transformation of AMR into tree structure that is acceptable to GHKM rule extraction (Galley
et al., 2004; Galley et al., 2006) and yields good performance.
</figureCaption>
<figure confidence="0.4906185">
Ppolarity
X
fear-01 ARG0
X
polarity
soldier
</figure>
<sectionHeader confidence="0.943793" genericHeader="method">
4 AMR Transformations
</sectionHeader>
<bodyText confidence="0.999708230769231">
In this section we discuss various transformations
to our AMR data. Initially, we concern ourselves
with converting AMR into a form that is amenable
to GHKM rule extraction and string to tree decod-
ing. We then turn to structural transformations
designed to improve system performance. Fig-
ure 3 progressively shows all the transformations
described in this section; the example we follow is
shown in its original form in Figure 3a. We note
that all transformations are done internally; the in-
put to the final system is a sentence and the output
is an AMR. We further observe that all transfor-
mations are data-driven and language agnostic.
</bodyText>
<subsectionHeader confidence="0.998874">
4.1 Massaging AMRs into Syntax-Style Trees
</subsectionHeader>
<bodyText confidence="0.924865038461538">
The relationships in AMR form a directed acyclic
graph (DAG), but GHKM requires a tree, so
we must begin our transformations by discarding
some information. We arbitrarily disconnect all
but a single parent from each node (see Figure 3b).
This is the only lossy modification we make to
our AMR data. As multi-parent relationships oc-
cur 1.05 times per training sentence and at least
once in 48% of training sentences, this is indeed a
regrettable loss. We nevertheless make this mod-
ification, since it allows us to use the rest of our
string-to-tree tools.
AMR also contains labeled edges, unlike the
constituent parse trees we are used to working with
in SBMT. These labeled edges have informative
content and we would like to use the alignment
procedure of Pourdamghani et al. (2014), which
aligns words to edges as well as to terminal nodes.
So that our AMR trees are compatible with both
our desired alignment approach and our desired
rule extraction approach, we propagate edge labels
to terminals via the following procedure:
1. For each node n in the AMR tree we create
a corresponding node m with the all-purpose
symbol ‘X’ in the SBMT-like tree. Outgoing
edges from n come in two flavors: concept
</bodyText>
<page confidence="0.964583">
1145
</page>
<bodyText confidence="0.999094529411765">
edges, labeled ‘inst,’ which connect n to a
terminal concept such as fear-01, and role
edges, which have a variety of labels such as
ARG0 and name, and connect n to another
instance or to a string.5 A node has one in-
stance edge and zero or more role edges. We
consider each type of edge separately.
2. For each outgoing role edge we insert two un-
labeled edges into the corresponding trans-
formation; the first is an edge from m to a
terminal bearing the original edge’s role label
(a so-called role label edge), and the second
(a role filler edge) connects m to the trans-
formation of the original edge’s target node,
which we process recursively. String targets
of a role receive an ‘X’ preterminal to be con-
sistent with the form of role filler edges.
</bodyText>
<listItem confidence="0.597599">
3. For the outgoing concept edge we insert an
unlabeled edge connecting m and the con-
cept. It is unambiguous to determine which
of m’s edges is the concept edge and which
edges constitute role label edges and their
corresponding role filler edges, as long as
paired label and filler edges are adjacent.
4. Since SBMT expects trees with preterminals,
we simply replicate the label identities of
concepts and role labels, adding a marker (‘P’
in Figure 3) to distinguish preterminals.
</listItem>
<bodyText confidence="0.98868975">
The complete transformation can be seen in Fig-
ure 3c. Apart from multiple parent ancestry, the
original AMR can be reconstructed deterministi-
cally from this SBMT-compliant rewrite.
</bodyText>
<subsectionHeader confidence="0.996542">
4.2 Tree Restructuring
</subsectionHeader>
<bodyText confidence="0.999975">
While the transformation in Figure 3c is accept-
able to GHKM, and hence an entire end-to-end
AMR parser may now be built with SBMT tools,
the resulting parser does not exhibit very good per-
formance (Table 3, first line). The trees we are
learning on are exceedingly flat, and thus yield
rules that do not generalize sufficiently. Rules pro-
duced from the top of the tree in Figure 3c, such
as that in Figure 4a, are only appropriate for cases
where fear-01 has exactly three roles: ARG0
(agent), ARG1 (patient), and polarity.
We follow the lead of Wang et al. (2010), who
in turn were influenced by similar approaches in
monolingual parsing (Collins, 1997; Charniak,
2000), and re-structure trees at nodes with more
</bodyText>
<footnote confidence="0.906835">
5In Figure 3 the negative polarity marker ‘-’ is a string.
Disconnected referents labeled ‘*’ are treated as AMR in-
stances with no roles.
</footnote>
<bodyText confidence="0.999813379310345">
than three children (i.e. instances with more than
one role), to allow generalization of flat structures.
However, our trees are unlike syntactic con-
stituent trees in that they do not have labeled non-
terminal nodes, so we have no natural choice of
an intermediate (“bar”) label. We must choose a
meaningful label to characterize an instance and
its roles. We initially choose the concept label,
resulting in trees like that in Figure 3d, in which a
chain of fear-01 nodes is used to unflatten the root,
which has instance fear-01.
This attempt at re-structuring yields rules like
that in Figure 4b, which are general in form but
are tied to the concept context in which they were
extracted. This leads to many redundant rules and
blows up the nonterminal vocabulary size to ap-
proximately 8,000, the size of the concept vocabu-
lary. Furthermore, the rules elicited by this proce-
dure encourage undesirable behavior such as the
immediate juxtaposition of two rules generating
ARG1.
We next consider restructuring with the imme-
diately dominant role labels, resulting in trees like
that in Figure 3e and rules like that in Figure 4c.
The shape of the structure added is the same as in
Figure 3d but the bar nodes now take their labels
from their second children. This approach leads to
more useful rules with fewer undesirable proper-
ties.
</bodyText>
<subsectionHeader confidence="0.999302">
4.3 Tree Relabeling
</subsectionHeader>
<bodyText confidence="0.999998375">
AMR strings have an effective preterminal label of
‘X,’ which allows them to compete with full AMR
instances at decode time. However, whether or not
a role is filled by a string or an instance is highly
dependent on the kind of role being filled. The
polarity and mode roles, for instance, are nearly
always filled by strings, but ARG0 and ARG1 are
always filled by instances. The quant role, which
is used for representation of numerical quantities,
can be filled by an instance (e.g. for approximate
quantities such as ‘about 3’) or a string. To capture
this behavior we relabel string preterminals of the
tree with labels indicating role identity and string
subsumption. This relabeling, replaces, for exam-
ple, one ‘X’ preterminal in Figure 3c with “Spo-
larity,” as shown in Figure 3f.
</bodyText>
<subsectionHeader confidence="0.995984">
4.4 Tree Reordering
</subsectionHeader>
<bodyText confidence="0.999634666666667">
Finally, let us consider the alignments between
English and AMR. As is known in SBMT, non-
monotone alignments can lead to large, unwieldy
</bodyText>
<page confidence="0.957974">
1146
</page>
<figure confidence="0.985823571428571">
ARG1
x1
x2
x2
x1
ARG1
PARG1
fear-01
PARG1
x1:fear-01
ARG1
x1:ARG0
x2:X
(a) A rule extracted from the AMR tree of
</figure>
<figureCaption confidence="0.9919585">
Figure 3c. All roles seen in training must be
used.
(b) A rule from the AMR tree of
Figure 3d. Many nearly iden-
tical rules of this type are ex-
tracted, and this rule can be
used multiple times in a single
derivation.
</figureCaption>
<figure confidence="0.874383">
x2:X
</figure>
<figureCaption confidence="0.8667444">
(c) A rule from the AMR tree of Fig-
ure 3e. This rule can be used indepen-
dent of the concept context it was ex-
tracted from and multiple reuse is dis-
couraged.
</figureCaption>
<figure confidence="0.998136384615384">
x1
The
was not afraid x2
X
Pfear-01 PARG0 PARG1
fear-01 ARG0
x1:X
ARG1
x2:X
Ppolarity
polarity
X
-
</figure>
<figureCaption confidence="0.999993">
Figure 4: Impact of restructuring on rule extraction.
</figureCaption>
<bodyText confidence="0.999765">
rules and in general make decoding more diffi-
cult (May and Knight, 2007). While this is often
an unavoidable fact of life when trying to trans-
late between two natural languages with different
syntactic behavior, it is an entirely artificial phe-
nomenon in this case. AMR is an unordered repre-
sentation, yet in order to use an SBMT infrastruc-
ture we must declare an order of the AMR tree.
This means we are free to choose whatever order
is most convenient to us, as long as we keep role
label edges immediately adjacent to their corre-
sponding role filler edges to preserve conversion
back to the edge-labeled AMR form. We thus
choose the order that is as close as possible to
that of the source yet still preserves these con-
straints. We use a simple greedy bottom-up ap-
proach that permutes the children of each internal
node of the unrestructured tree so as to minimize
crossings. This leads to a 79% overall reduction in
crossings and is exemplified in Figure 3g (before)
and Figure 3h (after). We may then restructure our
trees, as described above, in an instance-outward
manner. The final restructured, relabeled, and re-
ordered tree is shown in Figure 3i.
</bodyText>
<sectionHeader confidence="0.993554" genericHeader="method">
5 AMR Language Models
</sectionHeader>
<bodyText confidence="0.995810269230769">
We now turn to language models of AMRs, which
help us prefer reasonable target structures over un-
reasonable ones.
Our first language model is unintuitively
simple—we pretend there is a language called
AMRese that consists of yields of our restruc-
tured AMRs. An example AMRese string from
Figure 3i is ‘ARG0 soldier polarity -
fear-01 ARG1 die-01 ARG1 *.’ We then
build a standard n-gram model for AMRese.
It also seems sensible to judge the correctness of
an AMR by calculating the empirical probability
of the concepts and their relations to each other.
This is the motivation behind the following model
of an AMR:6
We define an AMR instance i = (c, R), where
c is a concept and R is a set of roles. We
define an AMR role r = (l, i), where l is a
role label, and i is an AMR instance labeled l.
For an AMR instance i let ˆci be the concept of
i’s parent instance, and ˆli be the label of the
role that i fills with respect to its parent. We
also define the special instance and role labels
ROOT and STOP. Then, we define PAMR(i|ˆli, ˆci),
the conditional probability of AMR instance i
given its ancestry as PAMR(i = (c, R)|ˆli, ˆci) =
</bodyText>
<equation confidence="0.872383666666667">
P(c|ˆli, ˆci) H PRole(r|c) × P(STOP|c), where
r∈R
PRole(r = (l,i)|c) = P(l|c)PAMR(i|l,c).
</equation>
<bodyText confidence="0.999705222222222">
We define P(c|ˆli, ˆci), P(l|c), and P(STOP|c)
as empirical conditional probabilities, Witten-Bell
interpolated (Witten and Bell, 1991) to lower-
order models by progressively discarding context
from the right.7 We model exactly one STOP
event per instance. We define the probability of
a full-sentence AMR i as PAMR(i|ROOT) where
ROOT in this case serves as both parent concept
and role label.
</bodyText>
<footnote confidence="0.993381833333333">
6This model is only defined over AMRs that can be rep-
resented as trees, and not over all AMRs. Since tree AMRs
are a prerequisite of our system we did not yet investigate
whether this model could be sufficiently generalized.
7That is, P(c|ˆli, ˆci) is interpolated with P(c|ˆli) and then
P(c).
</footnote>
<page confidence="0.895431">
1147
</page>
<table confidence="0.99519025">
System Tune Test
AMRese n-gram LM 61.7 59.7
AMR LM 59.1 57.1
both LMs 62.3 60.6
</table>
<tableCaption confidence="0.763192">
Table 2: The effect of AMRese n-gram and AMR
LMs on Smatch quality.
</tableCaption>
<figure confidence="0.990768260869565">
polarity
-
fear-01
die-01 ARG1
*
X
ARG0
PARG0
X
X
X
skilled-worker
ARG1
ARG0
soldier
think
die
Ppolarity
P*
PARG1
polarity
Spolarity
PARG1
</figure>
<bodyText confidence="0.90654225">
As an example, the instance associated
with concept die-01 in Figure 3b has ˆli =
ARG1 and ˆci = fear-01, so we may
score it as P(die-01|ARG1, fear-01) ×
</bodyText>
<equation confidence="0.9970735">
P(ARG1|die-01) × P(STOP|die-01) ×
P(*|ARG1, die-01)
</equation>
<bodyText confidence="0.9999065">
In Table 2 we compare the effect of varying
LMs on Smatch quality. The AMR LM by itself
is inferior to the AMRese n-gram LM, but com-
bining the two yields superior quality.
</bodyText>
<sectionHeader confidence="0.768825" genericHeader="method">
6 Adding External Semantic Resources
</sectionHeader>
<bodyText confidence="0.9997435">
Although we are engaged in the task of semantic
parsing, we have not yet discussed the use of any
semantic resources. In this section we rectify that
omission.
</bodyText>
<subsectionHeader confidence="0.9568385">
6.1 Rules from Numerical Quantities and
Named Entities
</subsectionHeader>
<bodyText confidence="0.999975947368421">
While the majority of string-to-tree rules in SBMT
systems are extracted from aligned parallel data, it
is common practice to dynamically generate addi-
tional rules to handle the translation of dates and
numerical quantities, as these follow common pat-
terns and are easily detected at decode-time. We
follow this practice here, and additionally detect
person names at decode-time using the Stanford
Named Entity Recognizer (Finkel et al., 2005).
We use cased, tokenized source data to build the
decode-time rules. We add indicator features to
these rules so that our tuning methods can decide
how favorable the resources are. We leave as fu-
ture work the incorporation of named-entity rules
for other classes, since most available named-
entity recognition beyond person names is at a
granularity level that is incompatible with AMR
(e.g. we can recognize ‘Location’ but not distin-
guish between ‘City’ and ‘Country’).
</bodyText>
<subsectionHeader confidence="0.999477">
6.2 Hierarchical Semantic Categories
</subsectionHeader>
<bodyText confidence="0.96175658974359">
In order to further generalize our rules, we mod-
ify our training data AMRs once more, this time
replacing the identity preterminals over concepts
Figure 5: Final modification of the AMR data; se-
mantically clustered preterminal labels are added
to concepts.
with preterminals designed to enhance the appli-
cability of our rules in semantically similar con-
texts. For each concept c expressed in AMR, we
consult WordNet (Fellbaum, 1998) and a curated
set of gazetteers and vocabulary lists to identify
a hierarchy of increasingly general semantic cat-
egories that describe the concept. So as not to
be overwhelmed by the many fine-grained distinc-
tions present in WordNet, we pre-select around
100 salient semantic categories from the WordNet
ontology. When traversing the WordNet hierarchy,
we propagate a smoothed count8 of the number
of examples seen per concept sense,9 combining
counts when paths meet. For each selected se-
mantic category s encountered in the traversal, we
calculate a weight by dividing the propagated ex-
ample count for c at s by the frequency s was pro-
posed over all AMR concepts. We then assign c
to the highest scoring semantic category s. An ex-
ample calculation for the concept computer is
shown in Figure 7.
We apply semantic categories to our data
as replacements for identity preterminals of
concepts. This leads to more general, more
widely-applicable rules. For example, with
this transformation, we can parse correctly
not only contexts in which “soldiers die,” but
also contexts in which other kinds of “skilled
workers die.” Figure 5 shows the addition of
semantic preterminals to the tree from Figure
3i. We also incorporate semantic categories
into the AMR LM. For concept c, let sc be
the semantic category of c. Then we reformu-
</bodyText>
<footnote confidence="0.9978464">
8We use very simple smoothing, and add 0.1 to the pro-
vided example counts.
9Since WordNet senses do not correspond directly to
PropBank or AMR senses, we simply use a lexical match and
must consider all observed senses for that match.
</footnote>
<page confidence="0.963632">
1148
</page>
<table confidence="0.9945175">
System Sec. Tune Test
flat trees 4.1 51.6 49.9
concept restructuring 4.2 57.2 55.3
role restructuring (rr) 4.2 60.8 58.6
rr + string preterminal relabeling (rl) 4.3 61.3 59.7
rr + rl + reordering (ro) 4.4 61.7 59.7
rr + rl + ro + AMR LM 5 62.3 60.6
rr + rl + ro + AMR LM + date/number/name rules (dn) 6.1 63.3 61.3
rr + rl + ro + AMR LM + dn + semantic categories (sc) 6.2 66.2 64.3
rr + rl + ro + AMR LM + dn + sc + morphological normalization (mn) 6.3 67.3 65.4
rr + rl + ro + AMR LM + dn + sc + mn, rule-based alignments 6.4 68.3 66.3
rr + rl + ro + AMR LM + dn + sc + mn, rule-based + unsupervised alignments 6.4 69.0 67.1
JAMR (Flanigan et al., 2014) 9 58.8 58.2
dependency parse-based (Wang et al., 2015) 9 N/A 63
</table>
<tableCaption confidence="0.686853">
Table 3: AMR parsing Smatch scores for the experiments in this work. We provide a cross-reference to
the section of this paper that describes each of the evaluated systems. Entries in bold are improvements
over JAMR (Flanigan et al., 2014). Test entries underlined are improvements over the dependency-based
work of Wang et al. (2015). Human inter-annotator Smatch performance is in the 79-83 range (Cai and
Knight, 2013).
</tableCaption>
<figure confidence="0.990408666666667">
English AMRese
tiger
asbestos
quietly quiet
executive polarity ‘-’
break-up-08
</figure>
<figureCaption confidence="0.366543">
Table 4: Lexical conversions to AMRese form
due to the morphological normalization rules de-
scribed in Section 6.3.
</figureCaption>
<bodyText confidence="0.653511">
late PAMR(i|ˆli, ˆci) as PAMR(i = (c, R)|ˆli, ˆci) =
</bodyText>
<equation confidence="0.90208075">
P(sc|ˆli, sˆci, ˆci)P(c|sc, ˆli, sˆci, ˆci) H
r∈R
P(STOP|sc, c), where PRole(r = (l, i)|c) =
P(l|sc, c) x PAMR(i|l, c).
</equation>
<subsectionHeader confidence="0.997986">
6.3 Morphological Normalization
</subsectionHeader>
<bodyText confidence="0.940910615384616">
While we rely heavily on the relationships be-
tween words in-text and concept nodes expressed
in parallel training data, we find this is not suf-
ficient for complete coverage. Thus we also in-
clude a run-time module that generates AMRese
base forms at the lexical level, expressing relation-
ships such as those depicted in Table 4. We build
these dictionary rules using three resources:
1. An inflectional morphological normalizing
table, comprising a lexicon with 84,558 en-
tries, hand-written rules for regular inflec-
tional morphology, and hand-written lists of
irregular verbs, nouns, and adjectives.
</bodyText>
<listItem confidence="0.989085833333333">
2. Lists of derivational mappings (e.g. ‘quietly’
-* ‘quiet’, ‘naval’ -* ‘navy’).
3. PropBank framesets, which we use, e.g., to
map the morphologically normalized ‘break
up’ (from ‘broke up’) into a sense match,
such as break-up-08.
</listItem>
<subsectionHeader confidence="0.884519">
6.4 Semantically informed Rule-based
Alignments
</subsectionHeader>
<bodyText confidence="0.999973714285714">
For our final incorporation of semantic resources
we revisit the English-to-AMR alignments used to
extract rules. As an alternative to the unsupervised
approach of Pourdamghani et al. (2014), we build
alignments by taking a linguistically-aware, super-
vised heuristic approach to alignment:
First, we generate a large number of poten-
tial links between English and AMR. We attempt
to link English and AMR tokens after conver-
sion through resources such as a morphological
analyzer, a list of 3,235 pertainym pairs (e.g.
adj-‘gubernatorial’ -* noun-‘governor’), a list of
2,444 adverb/adjective pairs (e.g. ‘humbly’ -*
‘humble’), a list of 2,076 negative polarity pairs
(e.g. ‘illegal’ -* ‘legal’), and a list of 2,794 known
English-AMR transformational relationships (e.g.
‘asleep’ -* sleep-01, ‘advertiser’ -* person
ARG0-of advertise-01, ‘Greenwich Mean
Time’ -* GMT). These links are then culled based
on context and AMR structure. For example, in
the sentence “The big fish ate the little fish,” ini-
</bodyText>
<figure confidence="0.94674">
tigers
asbestos
nonexecutive
broke up
PRole(r|c) x
1149
6.1
entity = 6.2/4381
</figure>
<figureCaption confidence="0.9907195">
Figure 6: BLEU of AMRese and Smatch correlate
closely when tuning.
</figureCaption>
<figure confidence="0.995571777777778">
machine
6.1
computing device
0.1
computer
6.1
6.2
physical-object = 6.2/2218
artefact = 6.1/1143
6.1
unit
6.1
0.1
causal-agency = 0.1/930
0.1
person = 0.1/814
0.1
estimator
</figure>
<bodyText confidence="0.921716153846154">
tially both English ‘fish’ are aligned to both AMR
fish. However, based on the context of ‘big’ and
‘little’ the spurious links are removed.
In our experiments we explore both replacing
the unsupervised alignments of Pourdamghani et
al. (2014) with these alignments and concatenat-
ing the two alignment sets together, essentially
doubling the size of the training corpus. Because
the different alignments yield different target-side
tree reorderings, it is necessary to build separate
5-gram AMRese language models.10 When us-
ing both alignment sets together, we also use both
AMRese language models simultaneously.
</bodyText>
<sectionHeader confidence="0.961629" genericHeader="method">
7 Tuning
</sectionHeader>
<bodyText confidence="0.999930166666667">
We would like to tune our feature weights to max-
imize Smatch directly. However, a very con-
venient alternative is to compare the AMRese
yields of candidate AMR parses to those of ref-
erence AMRese strings, using a BLEU objective
and forest-based MIRA (Chiang et al., 2009). Fig-
ure 6 shows that MIRA tuning with BLEU over
AMRese tracks closely with Smatch. Note that,
for experiments using reordered AMR trees, this
requires obtaining similarly permuted reference
tuning AMRese and hence requires alignments on
the development corpus. When using unsuper-
vised alignments we may simply run inference
on the trained alignment model to obtain devel-
opment alignments. The rule-based aligner runs
one sentence at a time and can be employed on
the development corpus. When using both sets of
alignments, each approach’s AMRese is used as
</bodyText>
<footnote confidence="0.8862605">
10The AMR LM is insensitive to reordering so we do not
need to vary it when varying alignments.
</footnote>
<figureCaption confidence="0.840225">
Figure 7: WordNet hierarchy for computer.
Pre-selected salient WordNet categories are
</figureCaption>
<bodyText confidence="0.941948888888889">
boxed. Smoothed sense counts are propagated
up the hierarchy and re-combined at join points.
Scores are calculated by dividing propagated
sense count by count of the category’s prevalence
over the set of AMR concepts. The double box
indicates the selection of artefact as the category
label for computer.
a development reference (i.e. each development
sentence has two possible reference translations).
</bodyText>
<sectionHeader confidence="0.996899" genericHeader="evaluation">
8 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99954465">
Our AMR parser’s performance is shown in Ta-
ble 3. We progressively show the incremental im-
provements and compare to the systems of Flani-
gan et al. (2014) and Wang et al. (2015). Purely
transforming AMR data into a form that is com-
patible with the SBMT pipeline yields suboptimal
results, but by adding role-based restructuring, re-
labeling, and reordering, as described in Section
4, we are able to surpass Flanigan et al. (2014).
Adding an AMR LM and semantic resources in-
creases scores further, outperforming Wang et al.
(2015). Rule-based alignments are an improve-
ment upon unsupervised alignments, but concate-
nating the two alignments is even better. We com-
pare rule set sizes of the various systems in Ta-
ble 5; initially we improve the rule set by remov-
ing numerous overly brittle rules but then succes-
sive changes progressively add useful rules. The
parser is available for public download and use at
http://amr.isi.edu.
</bodyText>
<page confidence="0.967448">
1150
</page>
<table confidence="0.999530555555556">
System Rules
flat trees 1,430,124
concept restructuring 678,265
role restructuring (rr) 660,582
rr + preterminal relabeling (rl) 661,127
rr + rl + semantic categories (sc) 765,720
rr + rl + sc + reordering (ro) 790,624
rr + rl + sc + ro + rule-based alignments 908,318
rr + rl + sc + ro + both alignments 1,306,624
</table>
<tableCaption confidence="0.994621">
Table 5: Comparison of extracted rule set size on the systems evaluated in this work. Note that, as
compared to Table 3, only systems that affect the rule size are listed.
</tableCaption>
<sectionHeader confidence="0.99762" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999970480769231">
The first work that addressed AMR parsing was
that of Flanigan et al. (2014). In that work, mul-
tiple discriminatively trained models are used to
identify individual concept instances and then a
minimum spanning tree algorithm connects the
concepts. That work was extended and improved
upon by Werling et al. (2015). Recent work by
Wang et al. (2015) also uses a two-pass approach;
dependency parses are modified by a tree-walking
algorithm that adds edge labels and restructures to
resolve discrepancies between dependency stan-
dards and AMR’s specification. In contrast to
these works, we use a single-pass approach and
re-use existing machine translation architecture,
adapting to the AMR parsing task by modify-
ing training data and adding lightweight AMR-
specific features.
Several other recent works have used a ma-
chine translation approach to semantic parsing,
but all have been applied to domain data that is
much narrower and an order of magnitude smaller
than that of AMR, primarily the Geoquery cor-
pus (Zelle and Mooney, 1996). The WASP sys-
tem of Wong and Mooney (2006) uses hierarchi-
cal SMT techniques and does not apply semantic-
specific improvements; its extension (Wong and
Mooney, 2007) incorporates a target-side reorder-
ing component much like the one presented in
Section 4.4. Jones et al. (2012a) cast semantic
parsing as an instance of hyperedge replacement
grammar transduction; like this work they use an
IBM model-influenced alignment algorithm and a
GHKM-based extraction algorithm. Andreas et
al. (2013) use phrase-based and hierarchical SMT
techniques on Geoquery. Like this work, they per-
form a transformation of the input semantic repre-
sentation so that it is amenable to use in an exist-
ing machine translation system. However, they are
unable to reach the state of the art in performance.
Li et al. (2013) directly address GHKM’s word-to-
terminal alignment requirement by extending that
algorithm to handle word-to-node alignment.
Our SBMT system is grounded in the theory of
tree transducers, which were applied to the task of
semantic parsing by Jones et al. (2011; 2012b).
Semantic parsing in general and AMR parsing
specifically can be considered a subsumption of
many semantic resolution sub-tasks, e.g. named
entity recognition (Nadeau and Sekine, 2007), se-
mantic role labeling (Gildea and Jurafsky, 2002),
word sense disambiguation (Navigli, 2009) and re-
lation finding (Bach and Badaskar, 2007).
</bodyText>
<sectionHeader confidence="0.993185" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999971090909091">
By restructuring our AMRs we are able to convert
a sophisticated SBMT engine into a baseline se-
mantic parser with little additional effort. By fur-
ther restructuring our data to appropriately model
the behavior we want to capture, we are able to
rapidly achieve state-of-the-art results. Finally, by
incorporating novel language models and external
semantic resources, we are able to increase quality
even more. This is not the last word on AMR pars-
ing, as fortunately, machine translation technology
provides more low-hanging fruit to pursue.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998898">
Thanks to Julian Schamper and Allen Schmaltz
for early attempts at this problem. This work
was sponsored by DARPA DEFT (FA8750-13-2-
0045), DARPA BOLT (HR0011-12-C-0014), and
DARPA Big Mechanism (W911NF-14-1-0364).
</bodyText>
<page confidence="0.991247">
1151
</page>
<sectionHeader confidence="0.983583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862936936937">
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 47–52, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Nguyen Bach and Sameer Badaskar. 2007. A Review
of Relation Extraction. Unpublished. http:
//www.cs.cmu.edu/˜nbach/papers/
A-survey-on-Relation-Extraction.
pdf.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.
Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
132–139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ’96, pages 310–318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 218–226, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics, pages 16–23, Madrid, Spain,
July. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 363–370, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract mean-
ing representation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1426–
1436, Baltimore, Maryland, June. Association for
Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273–280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288, September.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646–655, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Matthias Huck, Hieu Hoang, and Philipp Koehn. 2014.
Augmenting string-to-tree and tree-to-string transla-
tion with non-syntactic phrases. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation, pages 486–498, Baltimore, Maryland, USA,
June. Association for Computational Linguistics.
Bevan Jones, Mark Johnson, and Sharon Goldwa-
ter. 2011. Formalizing semantic parsing with tree
transducers. In Proceedings of the Australasian
Language Technology Association Workshop 2011,
pages 19–28, Canberra, Australia, December.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012a.
</reference>
<page confidence="0.895892">
1152
</page>
<reference confidence="0.999489614678899">
Semantics-based machine translation with hy-
peredge replacement grammars. In Proceedings
of COLING 2012, pages 1359–1376, Mumbai,
India, December. The COLING 2012 Organizing
Committee.
Bevan Jones, Mark Johnson, and Sharon Goldwater.
2012b. Semantic parsing with bayesian tree trans-
ducers. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 488–496, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In In Language Resources and
Evaluation.
Gregory Kuhlmann, Peter Stone, Raymond J. Mooney,
and Jude W. Shavlik. 2004. Guiding a reinforce-
ment learner with natural language advice: Initial
results in robocup soccer. In The AAAI-2004 Work-
shop on Supervisory Control of Learning and Adap-
tive Systems, July.
Peng Li, Yang Liu, and Maosong Sun. 2013. An ex-
tended ghkm algorithm for inducing lambda-scfg.
In Marie desJardins and Michael L. Littman, editors,
AAAI. AAAI Press.
Jonathan May and Kevin Knight. 2007. Syntactic
re-alignment models for machine translation. In
Jason Eisner and Taku Kudo, editors, Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 360–368,
Prague, Czech Republic, June 28 – June 30. Associ-
ation for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3–26, January. Pub-
lisher: John Benjamins Publishing Company.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1–10:69,
February.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning English strings with
Abstract Meaning Representation graphs. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 425–429, Doha, Qatar, October. Association
for Computational Linguistics.
P. J. Price. 1990. Evaluation of spoken language sys-
tems: The ATIS domain. In Proceedings of the
Workshop on Speech and Natural Language, HLT
’90, pages 91–95, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141–144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and
re-aligning for syntax-based machine translation.
Computational Linguistics, 36(2):247–277, June.
Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015. A transition-based algorithm for amr parsing.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 366–375, Denver, Colorado, May–June. As-
sociation for Computational Linguistics.
Keenon Werling, Gabor Angeli, and Christopher D.
Manning. 2015. Robust subgraph generation im-
proves abstract meaning representation parsing. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
982–991, Beijing, China, July. Association for Com-
putational Linguistics.
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel
events in adaptive text compression. IEEE Trans-
actions on Information Theory, 37(4).
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 439–446, New York City, USA, June.
Association for Computational Linguistics.
Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 960–967, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI’96, pages 1050–1055. AAAI Press.
</reference>
<page confidence="0.652313">
1153
</page>
<reference confidence="0.9984845">
John Marvin Zelle. 1995. Using Inductive Logic Pro-
gramming to Automate the Construction of Natural
Language Parsers. Ph.D. thesis, University of Texas
at Austin, Austin, TX, USA.
</reference>
<page confidence="0.996064">
1154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829110">
<title confidence="0.9996675">Parsing English into Abstract Meaning Representation Syntax-Based Machine Translation</title>
<author confidence="0.999971">Michael Pust</author>
<author confidence="0.999971">Ulf Hermjakob</author>
<author confidence="0.999971">Kevin Knight</author>
<author confidence="0.999971">Daniel Marcu</author>
<author confidence="0.999971">Jonathan</author>
<affiliation confidence="0.999067333333333">Information Sciences Computer Science University of Southern</affiliation>
<email confidence="0.930938">ulf,knight,marcu,</email>
<abstract confidence="0.992323142857143">We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Andreas Vlachos</author>
<author>Stephen Clark</author>
</authors>
<title>Semantic parsing as machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>47--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="31892" citStr="Andreas et al. (2013)" startWordPosition="5256" endWordPosition="5259">ta that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in gene</context>
</contexts>
<marker>Andreas, Vlachos, Clark, 2013</marker>
<rawString>Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 47–52, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Sameer Badaskar</author>
</authors>
<title>A Review of Relation Extraction.</title>
<date>2007</date>
<note>Unpublished. http: //www.cs.cmu.edu/˜nbach/papers/ A-survey-on-Relation-Extraction. pdf.</note>
<contexts>
<context position="32793" citStr="Bach and Badaskar, 2007" startWordPosition="5397" endWordPosition="5400">performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation technology provides more low-hanging fruit to pursue. Acknowledgments Thanks to Julian Schamper</context>
</contexts>
<marker>Bach, Badaskar, 2007</marker>
<rawString>Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. Unpublished. http: //www.cs.cmu.edu/˜nbach/papers/ A-survey-on-Relation-Extraction. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="923" citStr="Banarescu et al., 2013" startWordPosition="122" endWordPosition="125">stract We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results. 1 Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing ne</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6764" citStr="Brown et al. (1993)" startWordPosition="1038" endWordPosition="1041">training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3https://github.com/jflanigan/jamr 4LDC2013E117, a pre-released ve</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: an evaluation metric for semantic feature structures.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>748--752</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4194" citStr="Cai and Knight, 2013" startWordPosition="631" endWordPosition="634">s to leaves + words to edges Children ordered unordered Accuracy BLEU (Papineni Smatch (Cai and Metric et al., 2002) Knight, 2013) Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section 5). 4. Integrating several semantic knowledge sources into the task (Section 6). 5. Developing tuning methods that maximize Smatch (Cai and Knight, 2013) (Section 7). By applying these key ideas, which constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using t</context>
<context position="6917" citStr="Cai and Knight, 2013" startWordPosition="1061" endWordPosition="1064">istical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3https://github.com/jflanigan/jamr 4LDC2013E117, a pre-released version of LDC2014T12 that is not generally available. Training Development Test 1144 polarity - fear-01 die-01 soldier (b) Disconnecting multiple parents </context>
<context position="24276" citStr="Cai and Knight, 2013" startWordPosition="4050" endWordPosition="4053">nts 6.4 68.3 66.3 rr + rl + ro + AMR LM + dn + sc + mn, rule-based + unsupervised alignments 6.4 69.0 67.1 JAMR (Flanigan et al., 2014) 9 58.8 58.2 dependency parse-based (Wang et al., 2015) 9 N/A 63 Table 3: AMR parsing Smatch scores for the experiments in this work. We provide a cross-reference to the section of this paper that describes each of the evaluated systems. Entries in bold are improvements over JAMR (Flanigan et al., 2014). Test entries underlined are improvements over the dependency-based work of Wang et al. (2015). Human inter-annotator Smatch performance is in the 79-83 range (Cai and Knight, 2013). English AMRese tiger asbestos quietly quiet executive polarity ‘-’ break-up-08 Table 4: Lexical conversions to AMRese form due to the morphological normalization rules described in Section 6.3. late PAMR(i|ˆli, ˆci) as PAMR(i = (c, R)|ˆli, ˆci) = P(sc|ˆli, sˆci, ˆci)P(c|sc, ˆli, sˆci, ˆci) H r∈R P(STOP|sc, c), where PRole(r = (l, i)|c) = P(l|sc, c) x PAMR(i|l, c). 6.3 Morphological Normalization While we rely heavily on the relationships between words in-text and concept nodes expressed in parallel training data, we find this is not sufficient for complete coverage. Thus we also include a ru</context>
</contexts>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748–752, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13041" citStr="Charniak, 2000" startWordPosition="2097" endWordPosition="2098">GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, resulting in trees like</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000, pages 132–139, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5100" citStr="Chen and Goodman, 1996" startWordPosition="768" endWordPosition="771">e SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We in</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96, pages 310–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4958" citStr="Chiang et al. (2009)" startWordPosition="748" endWordPosition="751">ng results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are Englis</context>
<context position="27768" citStr="Chiang et al., 2009" startWordPosition="4588" endWordPosition="4591">catenating the two alignment sets together, essentially doubling the size of the training corpus. Because the different alignments yield different target-side tree reorderings, it is necessary to build separate 5-gram AMRese language models.10 When using both alignment sets together, we also use both AMRese language models simultaneously. 7 Tuning We would like to tune our feature weights to maximize Smatch directly. However, a very convenient alternative is to compare the AMRese yields of candidate AMR parses to those of reference AMRese strings, using a BLEU objective and forest-based MIRA (Chiang et al., 2009). Figure 6 shows that MIRA tuning with BLEU over AMRese tracks closely with Smatch. Note that, for experiments using reordered AMR trees, this requires obtaining similarly permuted reference tuning AMRese and hence requires alignments on the development corpus. When using unsupervised alignments we may simply run inference on the trained alignment model to obtain development alignments. The rule-based aligner runs one sentence at a time and can be employed on the development corpus. When using both sets of alignments, each approach’s AMRese is used as 10The AMR LM is insensitive to reordering </context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 218–226, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="13024" citStr="Collins, 1997" startWordPosition="2095" endWordPosition="2096"> acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, result</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="21567" citStr="Fellbaum, 1998" startWordPosition="3566" endWordPosition="3567">ames is at a granularity level that is incompatible with AMR (e.g. we can recognize ‘Location’ but not distinguish between ‘City’ and ‘Country’). 6.2 Hierarchical Semantic Categories In order to further generalize our rules, we modify our training data AMRs once more, this time replacing the identity preterminals over concepts Figure 5: Final modification of the AMR data; semantically clustered preterminal labels are added to concepts. with preterminals designed to enhance the applicability of our rules in semantically similar contexts. For each concept c expressed in AMR, we consult WordNet (Fellbaum, 1998) and a curated set of gazetteers and vocabulary lists to identify a hierarchy of increasingly general semantic categories that describe the concept. So as not to be overwhelmed by the many fine-grained distinctions present in WordNet, we pre-select around 100 salient semantic categories from the WordNet ontology. When traversing the WordNet hierarchy, we propagate a smoothed count8 of the number of examples seen per concept sense,9 combining counts when paths meet. For each selected semantic category s encountered in the traversal, we calculate a weight by dividing the propagated example count</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="20627" citStr="Finkel et al., 2005" startWordPosition="3415" endWordPosition="3418"> semantic parsing, we have not yet discussed the use of any semantic resources. In this section we rectify that omission. 6.1 Rules from Numerical Quantities and Named Entities While the majority of string-to-tree rules in SBMT systems are extracted from aligned parallel data, it is common practice to dynamically generate additional rules to handle the translation of dates and numerical quantities, as these follow common patterns and are easily detected at decode-time. We follow this practice here, and additionally detect person names at decode-time using the Stanford Named Entity Recognizer (Finkel et al., 2005). We use cased, tokenized source data to build the decode-time rules. We add indicator features to these rules so that our tuning methods can decide how favorable the resources are. We leave as future work the incorporation of named-entity rules for other classes, since most available namedentity recognition beyond person names is at a granularity level that is incompatible with AMR (e.g. we can recognize ‘Location’ but not distinguish between ‘City’ and ‘Country’). 6.2 Hierarchical Semantic Categories In order to further generalize our rules, we modify our training data AMRs once more, this t</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 363–370, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1426--1436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1370" citStr="Flanigan et al., 2014" startWordPosition="196" endWordPosition="200">proves upon state-of-the-art results. 1 Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1LDC Catalog number 2014T12 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not</context>
<context position="6976" citStr="Flanigan et al. (2014)" startWordPosition="1072" endWordPosition="1075">sed both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3https://github.com/jflanigan/jamr 4LDC2013E117, a pre-released version of LDC2014T12 that is not generally available. Training Development Test 1144 polarity - fear-01 die-01 soldier (b) Disconnecting multiple parents of (a) X Pfear-01 PARG0 X PARG1 X ARG1 X soldier (c) Edge l</context>
<context position="23790" citStr="Flanigan et al., 2014" startWordPosition="3971" endWordPosition="3974">4.1 51.6 49.9 concept restructuring 4.2 57.2 55.3 role restructuring (rr) 4.2 60.8 58.6 rr + string preterminal relabeling (rl) 4.3 61.3 59.7 rr + rl + reordering (ro) 4.4 61.7 59.7 rr + rl + ro + AMR LM 5 62.3 60.6 rr + rl + ro + AMR LM + date/number/name rules (dn) 6.1 63.3 61.3 rr + rl + ro + AMR LM + dn + semantic categories (sc) 6.2 66.2 64.3 rr + rl + ro + AMR LM + dn + sc + morphological normalization (mn) 6.3 67.3 65.4 rr + rl + ro + AMR LM + dn + sc + mn, rule-based alignments 6.4 68.3 66.3 rr + rl + ro + AMR LM + dn + sc + mn, rule-based + unsupervised alignments 6.4 69.0 67.1 JAMR (Flanigan et al., 2014) 9 58.8 58.2 dependency parse-based (Wang et al., 2015) 9 N/A 63 Table 3: AMR parsing Smatch scores for the experiments in this work. We provide a cross-reference to the section of this paper that describes each of the evaluated systems. Entries in bold are improvements over JAMR (Flanigan et al., 2014). Test entries underlined are improvements over the dependency-based work of Wang et al. (2015). Human inter-annotator Smatch performance is in the 79-83 range (Cai and Knight, 2013). English AMRese tiger asbestos quietly quiet executive polarity ‘-’ break-up-08 Table 4: Lexical conversions to A</context>
<context position="29089" citStr="Flanigan et al. (2014)" startWordPosition="4796" endWordPosition="4800">-selected salient WordNet categories are boxed. Smoothed sense counts are propagated up the hierarchy and re-combined at join points. Scores are calculated by dividing propagated sense count by count of the category’s prevalence over the set of AMR concepts. The double box indicates the selection of artefact as the category label for computer. a development reference (i.e. each development sentence has two possible reference translations). 8 Results and Discussion Our AMR parser’s performance is shown in Table 3. We progressively show the incremental improvements and compare to the systems of Flanigan et al. (2014) and Wang et al. (2015). Purely transforming AMR data into a form that is compatible with the SBMT pipeline yields suboptimal results, but by adding role-based restructuring, relabeling, and reordering, as described in Section 4, we are able to surpass Flanigan et al. (2014). Adding an AMR LM and semantic resources increases scores further, outperforming Wang et al. (2015). Rule-based alignments are an improvement upon unsupervised alignments, but concatenating the two alignments is even better. We compare rule set sizes of the various systems in Table 5; initially we improve the rule set by r</context>
<context position="30445" citStr="Flanigan et al. (2014)" startWordPosition="5030" endWordPosition="5033">ownload and use at http://amr.isi.edu. 1150 System Rules flat trees 1,430,124 concept restructuring 678,265 role restructuring (rr) 660,582 rr + preterminal relabeling (rl) 661,127 rr + rl + semantic categories (sc) 765,720 rr + rl + sc + reordering (ro) 790,624 rr + rl + sc + ro + rule-based alignments 908,318 rr + rl + sc + ro + both alignments 1,306,624 Table 5: Comparison of extracted rule set size on the systems evaluated in this work. Note that, as compared to Table 3, only systems that affect the rule size are listed. 9 Related Work The first work that addressed AMR parsing was that of Flanigan et al. (2014). In that work, multiple discriminatively trained models are used to identify individual concept instances and then a minimum spanning tree algorithm connects the concepts. That work was extended and improved upon by Werling et al. (2015). Recent work by Wang et al. (2015) also uses a two-pass approach; dependency parses are modified by a tree-walking algorithm that adds edge labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapti</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1426– 1436, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="4832" citStr="Galley et al., 2004" startWordPosition="728" endWordPosition="731">plying these key ideas, which constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus S</context>
<context position="8924" citStr="Galley et al., 2004" startWordPosition="1394" endWordPosition="1397">01 X ARG0Spolarity Pdie-01 soldier PARG0 Psoldier Ppolarity X ARG0 polarity Pfear-01 PARG1 ARG1 X PARG1 X P* * X X PARG0 X X P* PARG1 ARG0 soldier Ppolarity polarity Psoldier PARG1 X Pfear-01 Pdie-01 * ARG1 ARG1 fear-01 die-01 Pfear-01 PARG0 X PARG1 X ARG1 PsoldierPdie-01 PARG1 P* die-01 ARG1 * The soldier was not afraid of dying The soldier was not afraid of dying (g) Original alignment of English to (i) Restructured, relabeled, and re(c) (h) After reordering of (g) ordered tree: (e), (f), and (h) Figure 3: Transformation of AMR into tree structure that is acceptable to GHKM rule extraction (Galley et al., 2004; Galley et al., 2006) and yields good performance. Ppolarity X fear-01 ARG0 X polarity soldier 4 AMR Transformations In this section we discuss various transformations to our AMR data. Initially, we concern ourselves with converting AMR into a form that is amenable to GHKM rule extraction and string to tree decoding. We then turn to structural transformations designed to improve system performance. Figure 3 progressively shows all the transformations described in this section; the example we follow is shown in its original form in Figure 3a. We note that all transformations are done internall</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4854" citStr="Galley et al., 2006" startWordPosition="732" endWordPosition="735">s, which constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313</context>
<context position="8946" citStr="Galley et al., 2006" startWordPosition="1398" endWordPosition="1401">ie-01 soldier PARG0 Psoldier Ppolarity X ARG0 polarity Pfear-01 PARG1 ARG1 X PARG1 X P* * X X PARG0 X X P* PARG1 ARG0 soldier Ppolarity polarity Psoldier PARG1 X Pfear-01 Pdie-01 * ARG1 ARG1 fear-01 die-01 Pfear-01 PARG0 X PARG1 X ARG1 PsoldierPdie-01 PARG1 P* die-01 ARG1 * The soldier was not afraid of dying The soldier was not afraid of dying (g) Original alignment of English to (i) Restructured, relabeled, and re(c) (h) After reordering of (g) ordered tree: (e), (f), and (h) Figure 3: Transformation of AMR into tree structure that is acceptable to GHKM rule extraction (Galley et al., 2004; Galley et al., 2006) and yields good performance. Ppolarity X fear-01 ARG0 X polarity soldier 4 AMR Transformations In this section we discuss various transformations to our AMR data. Initially, we concern ourselves with converting AMR into a form that is amenable to GHKM rule extraction and string to tree decoding. We then turn to structural transformations designed to improve system performance. Figure 3 progressively shows all the transformations described in this section; the example we follow is shown in its original form in Figure 3a. We note that all transformations are done internally; the input to the fi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="32703" citStr="Gildea and Jurafsky, 2002" startWordPosition="5384" endWordPosition="5387">sting machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation techn</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>646--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="5254" citStr="Hopkins and Langmead, 2010" startWordPosition="794" endWordPosition="797">nd a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use Eng</context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Augmenting string-to-tree and tree-to-string translation with non-syntactic phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>486--498</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="3234" citStr="Huck et al. (2014)" startWordPosition="496" endWordPosition="499">duction-level SBMT systems are widely available, anyone wishing to generate AMR from text need only follow our recipe and retrain an existing framework with relevant data to quickly obtain state-of-the-art results. Since SBMT and AMR parsing are, in fact, distinct tasks, as outlined in Figure 2, to adapt the SBMT parsing framework to AMR parsing, we develop novel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2See e.g. the related work section of Huck et al. (2014). soldier die-01 fear-01 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. SBMT AMR parsing Target tree graph Nodes labeled unlabeled Edges unlabeled labeled Alignments words to leaves words to leaves + words to edges Children ordered unordered Accuracy BLEU (Papineni Smatch (Cai and Metric et al., 2002) Knight, 2013) Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering techn</context>
</contexts>
<marker>Huck, Hoang, Koehn, 2014</marker>
<rawString>Matthias Huck, Hieu Hoang, and Philipp Koehn. 2014. Augmenting string-to-tree and tree-to-string translation with non-syntactic phrases. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Formalizing semantic parsing with tree transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>pages</pages>
<location>Canberra, Australia,</location>
<contexts>
<context position="32458" citStr="Jones et al. (2011" startWordPosition="5349" endWordPosition="5352">KM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapi</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2011</marker>
<rawString>Bevan Jones, Mark Johnson, and Sharon Goldwater. 2011. Formalizing semantic parsing with tree transducers. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 19–28, Canberra, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<date>2012</date>
<contexts>
<context position="31677" citStr="Jones et al. (2012" startWordPosition="5226" endWordPosition="5229">parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending tha</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012a.</rawString>
</citation>
<citation valid="true">
<title>Semantics-based machine translation with hyperedge replacement grammars.</title>
<date></date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1359--1376</pages>
<location>Mumbai, India,</location>
<marker></marker>
<rawString>Semantics-based machine translation with hyperedge replacement grammars. In Proceedings of COLING 2012, pages 1359–1376, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>488--496</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="31677" citStr="Jones et al. (2012" startWordPosition="5226" endWordPosition="5229">parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending tha</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan Jones, Mark Johnson, and Sharon Goldwater. 2012b. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 488–496, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From treebank to propbank. In</title>
<date>2002</date>
<booktitle>In Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1023" citStr="Kingsbury and Palmer, 2002" startWordPosition="136" endWordPosition="139">version within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results. 1 Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithm</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From treebank to propbank. In In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Kuhlmann</author>
<author>Peter Stone</author>
<author>Raymond J Mooney</author>
<author>Jude W Shavlik</author>
</authors>
<title>Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer.</title>
<date>2004</date>
<booktitle>In The AAAI-2004 Workshop on Supervisory Control of Learning and Adaptive Systems,</booktitle>
<contexts>
<context position="6068" citStr="Kuhlmann et al., 2004" startWordPosition="925" endWordPosition="928">s 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdam</context>
</contexts>
<marker>Kuhlmann, Stone, Mooney, Shavlik, 2004</marker>
<rawString>Gregory Kuhlmann, Peter Stone, Raymond J. Mooney, and Jude W. Shavlik. 2004. Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer. In The AAAI-2004 Workshop on Supervisory Control of Learning and Adaptive Systems, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>An extended ghkm algorithm for inducing lambda-scfg.</title>
<date>2013</date>
<editor>In Marie desJardins and Michael L. Littman, editors, AAAI.</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="32198" citStr="Li et al. (2013)" startWordPosition="5309" endWordPosition="5312">target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 </context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. An extended ghkm algorithm for inducing lambda-scfg. In Marie desJardins and Michael L. Littman, editors, AAAI. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<volume>28</volume>
<pages>360--368</pages>
<editor>In Jason Eisner and Taku Kudo, editors,</editor>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="16242" citStr="May and Knight, 2007" startWordPosition="2655" endWordPosition="2658">m the AMR tree of Figure 3c. All roles seen in training must be used. (b) A rule from the AMR tree of Figure 3d. Many nearly identical rules of this type are extracted, and this rule can be used multiple times in a single derivation. x2:X (c) A rule from the AMR tree of Figure 3e. This rule can be used independent of the concept context it was extracted from and multiple reuse is discouraged. x1 The was not afraid x2 X Pfear-01 PARG0 PARG1 fear-01 ARG0 x1:X ARG1 x2:X Ppolarity polarity X - Figure 4: Impact of restructuring on rule extraction. rules and in general make decoding more difficult (May and Knight, 2007). While this is often an unavoidable fact of life when trying to translate between two natural languages with different syntactic behavior, it is an entirely artificial phenomenon in this case. AMR is an unordered representation, yet in order to use an SBMT infrastructure we must declare an order of the AMR tree. This means we are free to choose whatever order is most convenient to us, as long as we keep role label edges immediately adjacent to their corresponding role filler edges to preserve conversion back to the edge-labeled AMR form. We thus choose the order that is as close as possible t</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic re-alignment models for machine translation. In Jason Eisner and Taku Kudo, editors, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 360–368, Prague, Czech Republic, June 28 – June 30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Linguisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<publisher>Publisher: John Benjamins Publishing Company.</publisher>
<contexts>
<context position="32651" citStr="Nadeau and Sekine, 2007" startWordPosition="5376" endWordPosition="5379">esentation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AM</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):3–26, January. Publisher: John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Comput. Surv.,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="32746" citStr="Navigli, 2009" startWordPosition="5391" endWordPosition="5392">ble to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation technology provides more low-hanging fruit to pu</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2):10:1–10:69, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5660" citStr="Papineni et al., 2002" startWordPosition="857" endWordPosition="860">ey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule ext</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nima Pourdamghani</author>
<author>Yang Gao</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
</authors>
<title>Aligning English strings with Abstract Meaning Representation graphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>425--429</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="6687" citStr="Pourdamghani et al. (2014)" startWordPosition="1023" endWordPosition="1027">, 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation i</context>
<context position="10507" citStr="Pourdamghani et al. (2014)" startWordPosition="1656" endWordPosition="1659">e arbitrarily disconnect all but a single parent from each node (see Figure 3b). This is the only lossy modification we make to our AMR data. As multi-parent relationships occur 1.05 times per training sentence and at least once in 48% of training sentences, this is indeed a regrettable loss. We nevertheless make this modification, since it allows us to use the rest of our string-to-tree tools. AMR also contains labeled edges, unlike the constituent parse trees we are used to working with in SBMT. These labeled edges have informative content and we would like to use the alignment procedure of Pourdamghani et al. (2014), which aligns words to edges as well as to terminal nodes. So that our AMR trees are compatible with both our desired alignment approach and our desired rule extraction approach, we propagate edge labels to terminals via the following procedure: 1. For each node n in the AMR tree we create a corresponding node m with the all-purpose symbol ‘X’ in the SBMT-like tree. Outgoing edges from n come in two flavors: concept 1145 edges, labeled ‘inst,’ which connect n to a terminal concept such as fear-01, and role edges, which have a variety of labels such as ARG0 and name, and connect n to another i</context>
<context position="25747" citStr="Pourdamghani et al. (2014)" startWordPosition="4274" endWordPosition="4277">g a lexicon with 84,558 entries, hand-written rules for regular inflectional morphology, and hand-written lists of irregular verbs, nouns, and adjectives. 2. Lists of derivational mappings (e.g. ‘quietly’ -* ‘quiet’, ‘naval’ -* ‘navy’). 3. PropBank framesets, which we use, e.g., to map the morphologically normalized ‘break up’ (from ‘broke up’) into a sense match, such as break-up-08. 6.4 Semantically informed Rule-based Alignments For our final incorporation of semantic resources we revisit the English-to-AMR alignments used to extract rules. As an alternative to the unsupervised approach of Pourdamghani et al. (2014), we build alignments by taking a linguistically-aware, supervised heuristic approach to alignment: First, we generate a large number of potential links between English and AMR. We attempt to link English and AMR tokens after conversion through resources such as a morphological analyzer, a list of 3,235 pertainym pairs (e.g. adj-‘gubernatorial’ -* noun-‘governor’), a list of 2,444 adverb/adjective pairs (e.g. ‘humbly’ -* ‘humble’), a list of 2,076 negative polarity pairs (e.g. ‘illegal’ -* ‘legal’), and a list of 2,794 known English-AMR transformational relationships (e.g. ‘asleep’ -* sleep-01</context>
<context position="27118" citStr="Pourdamghani et al. (2014)" startWordPosition="4486" endWordPosition="4489"> example, in the sentence “The big fish ate the little fish,” initigers asbestos nonexecutive broke up PRole(r|c) x 1149 6.1 entity = 6.2/4381 Figure 6: BLEU of AMRese and Smatch correlate closely when tuning. machine 6.1 computing device 0.1 computer 6.1 6.2 physical-object = 6.2/2218 artefact = 6.1/1143 6.1 unit 6.1 0.1 causal-agency = 0.1/930 0.1 person = 0.1/814 0.1 estimator tially both English ‘fish’ are aligned to both AMR fish. However, based on the context of ‘big’ and ‘little’ the spurious links are removed. In our experiments we explore both replacing the unsupervised alignments of Pourdamghani et al. (2014) with these alignments and concatenating the two alignment sets together, essentially doubling the size of the training corpus. Because the different alignments yield different target-side tree reorderings, it is necessary to build separate 5-gram AMRese language models.10 When using both alignment sets together, we also use both AMRese language models simultaneously. 7 Tuning We would like to tune our feature weights to maximize Smatch directly. However, a very convenient alternative is to compare the AMRese yields of candidate AMR parses to those of reference AMRese strings, using a BLEU obj</context>
</contexts>
<marker>Pourdamghani, Gao, Hermjakob, Knight, 2014</marker>
<rawString>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning English strings with Abstract Meaning Representation graphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425–429, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Price</author>
</authors>
<title>Evaluation of spoken language systems: The ATIS domain.</title>
<date>1990</date>
<booktitle>In Proceedings of the Workshop on Speech and Natural Language, HLT ’90,</booktitle>
<pages>91--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6031" citStr="Price, 1990" startWordPosition="921" endWordPosition="922">ang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsup</context>
</contexts>
<marker>Price, 1990</marker>
<rawString>P. J. Price. 1990. Evaluation of spoken language systems: The ATIS domain. In Proceedings of the Workshop on Speech and Natural Language, HLT ’90, pages 91–95, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pust</author>
<author>Kevin Knight</author>
</authors>
<title>Faster mt decoding through pervasive laziness.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>141--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5225" citStr="Pust and Knight, 2009" startWordPosition="790" endWordPosition="793">ation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang Corpus Sentences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Da</context>
</contexts>
<marker>Pust, Knight, 2009</marker>
<rawString>Michael Pust and Kevin Knight. 2009. Faster mt decoding through pervasive laziness. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 141–144, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, re-labeling, and re-aligning for syntax-based machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="12935" citStr="Wang et al. (2010)" startWordPosition="2080" endWordPosition="2083">m this SBMT-compliant rewrite. 4.2 Tree Restructuring While the transformation in Figure 3c is acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and re-aligning for syntax-based machine translation. Computational Linguistics, 36(2):247–277, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan Wang</author>
<author>Nianwen Xue</author>
<author>Sameer Pradhan</author>
</authors>
<title>A transition-based algorithm for amr parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>366--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="1390" citStr="Wang et al., 2015" startWordPosition="201" endWordPosition="204">e-art results. 1 Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1LDC Catalog number 2014T12 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure </context>
<context position="7167" citStr="Wang et al. (2015)" startWordPosition="1106" endWordPosition="1109">ere noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3https://github.com/jflanigan/jamr 4LDC2013E117, a pre-released version of LDC2014T12 that is not generally available. Training Development Test 1144 polarity - fear-01 die-01 soldier (b) Disconnecting multiple parents of (a) X Pfear-01 PARG0 X PARG1 X ARG1 X soldier (c) Edge labels of (b) pushed to leaves, preterminals added fear-01 ARG0 PsoldierPdie-01 PARG1 die-01 ARG1 P** Ppolarity X die-01 soldier fear-01 (a) The original AMR X X fear-01 ARG1 fear-01 X ARG1 PA</context>
<context position="23845" citStr="Wang et al., 2015" startWordPosition="3980" endWordPosition="3983">ructuring (rr) 4.2 60.8 58.6 rr + string preterminal relabeling (rl) 4.3 61.3 59.7 rr + rl + reordering (ro) 4.4 61.7 59.7 rr + rl + ro + AMR LM 5 62.3 60.6 rr + rl + ro + AMR LM + date/number/name rules (dn) 6.1 63.3 61.3 rr + rl + ro + AMR LM + dn + semantic categories (sc) 6.2 66.2 64.3 rr + rl + ro + AMR LM + dn + sc + morphological normalization (mn) 6.3 67.3 65.4 rr + rl + ro + AMR LM + dn + sc + mn, rule-based alignments 6.4 68.3 66.3 rr + rl + ro + AMR LM + dn + sc + mn, rule-based + unsupervised alignments 6.4 69.0 67.1 JAMR (Flanigan et al., 2014) 9 58.8 58.2 dependency parse-based (Wang et al., 2015) 9 N/A 63 Table 3: AMR parsing Smatch scores for the experiments in this work. We provide a cross-reference to the section of this paper that describes each of the evaluated systems. Entries in bold are improvements over JAMR (Flanigan et al., 2014). Test entries underlined are improvements over the dependency-based work of Wang et al. (2015). Human inter-annotator Smatch performance is in the 79-83 range (Cai and Knight, 2013). English AMRese tiger asbestos quietly quiet executive polarity ‘-’ break-up-08 Table 4: Lexical conversions to AMRese form due to the morphological normalization rules</context>
<context position="29112" citStr="Wang et al. (2015)" startWordPosition="4802" endWordPosition="4805">ategories are boxed. Smoothed sense counts are propagated up the hierarchy and re-combined at join points. Scores are calculated by dividing propagated sense count by count of the category’s prevalence over the set of AMR concepts. The double box indicates the selection of artefact as the category label for computer. a development reference (i.e. each development sentence has two possible reference translations). 8 Results and Discussion Our AMR parser’s performance is shown in Table 3. We progressively show the incremental improvements and compare to the systems of Flanigan et al. (2014) and Wang et al. (2015). Purely transforming AMR data into a form that is compatible with the SBMT pipeline yields suboptimal results, but by adding role-based restructuring, relabeling, and reordering, as described in Section 4, we are able to surpass Flanigan et al. (2014). Adding an AMR LM and semantic resources increases scores further, outperforming Wang et al. (2015). Rule-based alignments are an improvement upon unsupervised alignments, but concatenating the two alignments is even better. We compare rule set sizes of the various systems in Table 5; initially we improve the rule set by removing numerous overly</context>
<context position="30718" citStr="Wang et al. (2015)" startWordPosition="5074" endWordPosition="5077"> ro + rule-based alignments 908,318 rr + rl + sc + ro + both alignments 1,306,624 Table 5: Comparison of extracted rule set size on the systems evaluated in this work. Note that, as compared to Table 3, only systems that affect the rule size are listed. 9 Related Work The first work that addressed AMR parsing was that of Flanigan et al. (2014). In that work, multiple discriminatively trained models are used to identify individual concept instances and then a minimum spanning tree algorithm connects the concepts. That work was extended and improved upon by Werling et al. (2015). Recent work by Wang et al. (2015) also uses a two-pass approach; dependency parses are modified by a tree-walking algorithm that adds edge labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnit</context>
</contexts>
<marker>Wang, Xue, Pradhan, 2015</marker>
<rawString>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for amr parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366–375, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Keenon Werling</author>
<author>Gabor Angeli</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust subgraph generation improves abstract meaning representation parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>982--991</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="30683" citStr="Werling et al. (2015)" startWordPosition="5067" endWordPosition="5070">reordering (ro) 790,624 rr + rl + sc + ro + rule-based alignments 908,318 rr + rl + sc + ro + both alignments 1,306,624 Table 5: Comparison of extracted rule set size on the systems evaluated in this work. Note that, as compared to Table 3, only systems that affect the rule size are listed. 9 Related Work The first work that addressed AMR parsing was that of Flanigan et al. (2014). In that work, multiple discriminatively trained models are used to identify individual concept instances and then a minimum spanning tree algorithm connects the concepts. That work was extended and improved upon by Werling et al. (2015). Recent work by Wang et al. (2015) also uses a two-pass approach; dependency parses are modified by a tree-walking algorithm that adds edge labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is m</context>
</contexts>
<marker>Werling, Angeli, Manning, 2015</marker>
<rawString>Keenon Werling, Gabor Angeli, and Christopher D. Manning. 2015. Robust subgraph generation improves abstract meaning representation parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 982–991, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>T C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="18710" citStr="Witten and Bell, 1991" startWordPosition="3090" endWordPosition="3093">e l is a role label, and i is an AMR instance labeled l. For an AMR instance i let ˆci be the concept of i’s parent instance, and ˆli be the label of the role that i fills with respect to its parent. We also define the special instance and role labels ROOT and STOP. Then, we define PAMR(i|ˆli, ˆci), the conditional probability of AMR instance i given its ancestry as PAMR(i = (c, R)|ˆli, ˆci) = P(c|ˆli, ˆci) H PRole(r|c) × P(STOP|c), where r∈R PRole(r = (l,i)|c) = P(l|c)PAMR(i|l,c). We define P(c|ˆli, ˆci), P(l|c), and P(STOP|c) as empirical conditional probabilities, Witten-Bell interpolated (Witten and Bell, 1991) to lowerorder models by progressively discarding context from the right.7 We model exactly one STOP event per instance. We define the probability of a full-sentence AMR i as PAMR(i|ROOT) where ROOT in this case serves as both parent concept and role label. 6This model is only defined over AMRs that can be represented as trees, and not over all AMRs. Since tree AMRs are a prerequisite of our system we did not yet investigate whether this model could be sufficiently generalized. 7That is, P(c|ˆli, ˆci) is interpolated with P(c|ˆli) and then P(c). 1147 System Tune Test AMRese n-gram LM 61.7 59.7</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>I.H. Witten and T.C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>439--446</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="31445" citStr="Wong and Mooney (2006)" startWordPosition="5190" endWordPosition="5193"> labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 439–446, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>960--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="31566" citStr="Wong and Mooney, 2007" startWordPosition="5208" endWordPosition="5211">ese works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art </context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI’96,</booktitle>
<pages>1050--1055</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31402" citStr="Zelle and Mooney, 1996" startWordPosition="5181" endWordPosition="5184">d by a tree-walking algorithm that adds edge labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation o</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI’96, pages 1050–1055. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Marvin Zelle</author>
</authors>
<title>Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas at Austin,</institution>
<location>Austin, TX, USA.</location>
<contexts>
<context position="6044" citStr="Zelle, 1995" startWordPosition="923" endWordPosition="924">ntences Tokens 10,313 218,021 1,368 29,484 1,371 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. et al., 2009) to maximize the objective, typically BLEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised align</context>
</contexts>
<marker>Zelle, 1995</marker>
<rawString>John Marvin Zelle. 1995. Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. Ph.D. thesis, University of Texas at Austin, Austin, TX, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.991877">
Diversity in Spectral Learning for Natural Language Parsing
</title>
<author confidence="0.995747">
Shashi Narayan and Shay B. Cohen
</author>
<affiliation confidence="0.9983655">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.984953">
Edinburgh, EH8 9LE, UK
</address>
<email confidence="0.999058">
{snaraya2,scohen}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986222222222">
We describe an approach to create a di-
verse set of predictions with spectral learn-
ing of latent-variable PCFGs (L-PCFGs).
Our approach works by creating multiple
spectral models where noise is added to
the underlying features in the training set
before the estimation of each model. We
describe three ways to decode with mul-
tiple models. In addition, we describe a
simple variant of the spectral algorithm for
L-PCFGs that is fast and leads to compact
models. Our experiments for natural lan-
guage parsing, for English and German,
show that we get a significant improve-
ment over baselines comparable to state of
the art. For English, we achieve the F1
score of 90.18, and for German we achieve
the F1 score of 83.38.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836">
It has been long identified in NLP that a diverse set
of solutions from a decoder can be reranked or re-
combined in order to improve the accuracy in var-
ious problems (Henderson and Brill, 1999). Such
problems include machine translation (Macherey
and Och, 2007), syntactic parsing (Charniak and
Johnson, 2005; Sagae and Lavie, 2006; Fossum
and Knight, 2009; Zhang et al., 2009; Petrov,
2010; Choe et al., 2015) and others (Van Halteren
et al., 2001).
The main argument behind the use of such a di-
verse set of solutions (such as k-best list of parses
for a natural language sentence) is the hope that
each solution in the set is mostly correct. There-
fore, recombination or reranking of solutions in
that set will further optimize the choice of a solu-
tion, combining together the information from all
solutions.
In this paper, we explore another angle for the
use of a set of parse tree predictions, where all pre-
dictions are made for the same sentence. More
specifically, we describe techniques to exploit di-
versity with spectral learning algorithms for natu-
ral language parsing. Spectral techniques and the
method of moments have been recently used for
various problems in natural language processing,
including parsing, topic modeling and the deriva-
tion of word embeddings (Luque et al., 2012; Co-
hen et al., 2013; Stratos et al., 2014; Dhillon et al.,
2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu
et al., 2015).
Cohen et al. (2013) showed how to estimate an
L-PCFG using spectral techniques, and showed
that such estimation outperforms the expectation-
maximization algorithm (Matsuzaki et al., 2005).
Their result still lags behind state of the art in natu-
ral language parsing, with methods such as coarse-
to-fine (Petrov et al., 2006).
We further advance the accuracy of natural lan-
guage parsing with spectral techniques and L-
PCFGs, yielding a result that outperforms the orig-
inal Berkeley parser from Petrov and Klein (2007).
Instead of exploiting diversity from a k-best list
from a single model, we estimate multiple models,
where the underlying features are perturbed with
several perturbation schemes. Each such model,
during test time, yields a single parse, and all
parses are then used together in several ways to
select a single best parse.
The main contributions of this paper are two-
fold. First, we present an algorithm for estimating
L-PCFGs, akin to the spectral algorithm of Cohen
et al. (2012), but simpler to understand and imple-
ment. This algorithm has value for readers who
are interested in learning more about spectral al-
gorithms – it demonstrates some of the core ideas
in spectral learning in a rather intuitive way. In
addition, this algorithm leads to sparse grammar
estimates and compact models.
Second, we describe how a diverse set of predic-
tors can be used with spectral learning techniques.
</bodyText>
<page confidence="0.945135">
1868
</page>
<note confidence="0.984806">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999921">
Our approach relies on adding noise to the feature
functions that help the spectral algorithm compute
the latent states. Our noise schemes are similar
to those described by Wang et al. (2013). We add
noise to the whole training data, then train a model
using our algorithm (or other spectral algorithms;
Cohen et al., 2013), and repeat this process mul-
tiple times. We then use the set of parses we get
from all models in a recombination step.
The rest of the paper is organized as follows.
In §2 we describe notation and background about
L-PCFG parsing. In §3 we describe our new spec-
tral algorithm for estimating L-PCFGs. It is based
on similar intuitions as older spectral algorithms
for L-PCFGs. In §4 we describe the various noise
schemes we use with our spectral algorithm and
the spectral algorithm of Cohen et al. (2013). In
§5 we describe how to decode with multiple mod-
els, each arising from a different noise setting. In
§6 we describe our experiments with natural lan-
guage parsing for English and German.
</bodyText>
<sectionHeader confidence="0.950225" genericHeader="introduction">
2 Background and Notation
</sectionHeader>
<bodyText confidence="0.888094636363637">
We denote by [n] the set of integers {1, . . . , n}.
For a statement Γ, we denote by [[Γ]] its indicator
function, with values 0 when the assertion is false
and 1 when it is true.
An L-PCFG is a 5-tuple (N, I, P, m, n) where:
• N is the set of nonterminal symbols in the
grammar. I ⊂ N is a finite set of intermi-
nals. P ⊂ N is a finite set of preterminals.
We assume that N = I ∪ P, and I ∩ P = ∅.
Hence we have partitioned the set of nonter-
minals into two subsets.
</bodyText>
<listItem confidence="0.991884571428571">
• [m] is the set of possible hidden states.
• [n] is the set of possible words.
• For all a ∈ I, b ∈ N, c ∈ N, h1, h2,h3 ∈
[m], we have a binary context-free rule
a(h1) → b(h2) c(h3).
• For all a ∈ P, h ∈ [m], x ∈ [n], we have a
lexical context-free rule a(h) → x.
</listItem>
<bodyText confidence="0.588071285714286">
Latent-variable PCFGs are essentially equiv-
alent to probabilistic regular tree grammars
(PRTGs; Knight and Graehl, 2005) where the
righthand side trees are of depth 1. With gen-
eral PRTGs, the righthand side can be of arbitrary
depth, where the leaf nodes of these trees corre-
spond to latent states in the L-PCFG formulation
</bodyText>
<figureCaption confidence="0.978203">
Figure 1: The inside tree (left) and outside
tree (right) for the nonterminal VP in the parse
</figureCaption>
<equation confidence="0.6248965">
tree (S (NP (D the) (N dog)) (VP (V
saw) (NP (D the) (N woman)))).
</equation>
<bodyText confidence="0.999461416666667">
above and the internal nodes of these trees corre-
spond to interminal symbols in the L-PCFG for-
mulation.
Two important concepts that will be used
throughout of the paper are that of an “inside tree”
and an “outside tree.” Given a tree, the inside tree
for a node contains the entire subtree below that
node; the outside tree contains everything in the
tree excluding the inside tree. See Figure 1 for an
example. Given a grammar, we denote the space
of inside trees by T and the space of outside trees
by O.
</bodyText>
<sectionHeader confidence="0.7886675" genericHeader="method">
3 Clustering Algorithm for Estimating
L-PCFGs
</sectionHeader>
<bodyText confidence="0.9998119">
We assume two feature functions, φ: T → Rd
and ψ: O → Rd&apos;, mapping inside and outside
trees, respectively, to a real vector. Our training
data consist of examples (a(Z), t(Z), o(Z), b(Z)) for
i ∈ {1... M}, where a(Z) ∈ N; t(Z) is an inside
tree; o(Z) is an outside tree; and b(Z) = 1 if a(Z) is
the root of tree, 0 otherwise. These are obtained
by splitting all trees in the training set into inside
and outside trees at each node in each tree. We
then define Ωa ∈ Rd×d&apos;:
</bodyText>
<equation confidence="0.999499666666667">
�� Z=1[[a(Z) = a]]φ(t(Z))(ψ(o(Z)))�
Ωa = 1)
EM1 [[a(Z) = a]]
</equation>
<bodyText confidence="0.9991779">
This matrix is an empirical estimate for the
cross-covariance matrix between the inside trees
and the outside trees of a given nonterminal a. An
inside tree and an outside tree are conditionally in-
dependent according to the L-PCFG model, when
the latent state at their connecting point is known.
This means that the latent state can be identified
by finding patterns that co-occur together in in-
side and outside trees – it is the only random vari-
able that can explain such correlations. As such,
</bodyText>
<figure confidence="0.683888192307692">
saw
D
D
N
N
the
the
dog
woman
V
VP
NP
NP
S
VP
1869
Inputs: An input treebank with the following additional in-
formation: training examples (a(i), t(i), o(i), b(i)) for i E
{1 ... M}, where a(i) E N; t(i) is an inside tree; o(i) is
an outside tree; and b(i) = 1 if the rule is at the root of tree,
0 otherwise. A function φ that maps inside trees t to feature-
vectors φ(t) E Rd. A function ψ that maps outside trees o
to feature-vectors ψ(o) E Rd&apos;. An integer k denoting the
thin-SVD rank. An integer m denoting the number of latent
states.
Algorithm:
</figure>
<figureCaption confidence="0.333492">
(Step 1: Singular Value Decompositions)
</figureCaption>
<listItem confidence="0.979400555555556">
• Calculate SVD on Ωa to get ˆUa E R(dxk) and Vˆa E
R(d&apos;xk) for each a E N.
(Step 1: Projection)
• For all i E [M], compute y(i) = (ˆUa&amp;quot;)Tφ(t(i)) and
z(i) = (Vˆa&amp;quot;)Tψ(o(i)).
• For all i E [M], set x(i) to be the concatenation of y(i)
and z(i).
(Step 2: Cluster Projections)
• For all a E N, cluster the set {x(i)  |a(i) = a} to
get a clustering function γ : R2k → [m] that maps a
projected vector x(i) to a cluster in [m].
(Step 3: Compute Final Parameters)
• Annotate each node in the treebank with γ(x(i)).
• Compute the probability of a rule p(a[h1] →
b[h2] c[h3]  |a[h1]) as the relative frequency of its ap-
pearance in the cluster-annotated treebank.
• Similarly, compute the root probabilities π(a[h]) and
preterminal rules p(a[h] → x  |a[h]).
</listItem>
<figureCaption confidence="0.9609615">
Figure 2: The clustering estimation algorithm for
L-PCFGs.
</figureCaption>
<bodyText confidence="0.999783285714286">
if we reduce the dimensions of Qa using singu-
lar value decomposition (SVD), we essentially get
representations for the inside trees and the outside
trees that correspond to the latent states.
This intuition leads to the algorithm that appears
in Figure 2. The algorithm we describe takes as in-
put training data, in the form of a treebank, decom-
posed into inside and outside trees at each node in
each tree in the training set.
The algorithm first performs SVD for each of
the set of inside and outside trees for all nontermi-
nals.1 This step is akin to CCA, which has been
used in various contexts in NLP, mostly to derive
representations for words (Dhillon et al., 2015;
</bodyText>
<footnote confidence="0.845187">
1We normalize features by their variance.
</footnote>
<bodyText confidence="0.996857344827586">
Rastogi et al., 2015). The algorithm then takes
the representations induced by the SVD step, and
clusters them – we use k-means to do the clus-
tering. Finally, it maps each SVD representation
to a cluster, and as a result, gets a cluster identi-
fier for each node in each tree in the training data.
These clusters are now treated as latent states that
are “observed.” We subsequently follow up with
frequency count maximum likelihood estimate to
estimate the probabilities of each parameter in the
L-PCFG.
Consider for example the estimation of rules of
the form a → x. Following the clustering step we
obtain for each nonterminal a and latent state h a
set of rules of the form a[h] → x. Each such in-
stance comes from a single training example of a
lexical rule. Next, we compute the probability of
the rule a[h] → x by counting how many times
that rule appears in the training instances, and nor-
malize by the total count of a[h] in the training
instances. Similarly, we compute probabilities for
binary rules of the form a → b c.
The features that we use for φ and ψ are sim-
ilar to those used in Cohen et al. (2013). These
features look at the local neighborhood surround-
ing a given node. More specifically, we indicate
the following information with the inside features
(throughout these definitions assume that a → b c
is at the root of the inside tree t):
</bodyText>
<listItem confidence="0.996612125">
• The pair of nonterminals (a, b). E.g., for the
inside tree in Figure 1 this would be the pair
(VP, V).
• The pair (a, c). E.g., (VP, NP).
• The rule a → b c. E.g., VP → V NP.
• The rule a → b c paired with the rule at the
node b. E.g., for the inside tree in Figure 1
this would correspond to the tree fragment
(VP (V saw) NP).
• The rule a → b c paired with the rule at the
node c. E.g., the tree fragment (VP V (NP D
N)).
• The head part-of-speech of t paired with a.
E.g., the pair (VP, V).
• The number of words dominated by t paired
with a. E.g., the pair (VP, 3).
</listItem>
<bodyText confidence="0.9995215">
In the case of an inside tree consisting of a sin-
gle rule a → x the feature vector simply indicates
the identity of that rule.
For the outside features, we use:
</bodyText>
<page confidence="0.926881">
1870
</page>
<listItem confidence="0.995508666666666">
• The rule above the foot node. E.g., for the
outside tree in Figure 1 this would be the rule
S → NP VP∗ (the foot nonterminal is marked
with ∗).
• The two-level and three-level rule fragments
above the foot node. These features are ab-
sent in the outside tree in Figure 1.
• The label of the foot node, together with the
label of its parent. E.g., the pair (VP, S).
• The label of the foot node, together with the
label of its parent and grandparent.
• The part-of-speech of the first head word
along the path from the foot of the outside
tree to the root of the tree which is different
from the head node of the foot node.
• The width of the spans to the left and to the
right of the foot node, paired with the label of
the foot node.
</listItem>
<bodyText confidence="0.996680352941177">
Other Spectral Algorithms The SVD step on
the Ω&apos; matrix is pivotal to many algorithms, and
has been used in the past for other L-PCFG esti-
mation algorithms. Cohen et al. (2012) used it for
developing a spectral algorithm that identifies the
parameters of the L-PCFG up to a linear transfor-
mation. Their algorithm generalizes the work of
Hsu et al. (2009) and Bailly et al. (2010).
Cohen and Collins (2014) also developed an al-
gorithm that makes use of an SVD step on the
inside-outside. It relies on the idea of “pivot
features” – features that uniquely identify latent
states.
Louis and Cohen (2015) used a clustering al-
gorithm that resembles ours but does not sepa-
rate inside trees from outside trees or follows up
with a singular value decomposition step. Their
algorithm was applied to both L-PCFGs and lin-
ear context-free rewriting systems. Their applica-
tion was the analysis of hierarchical structure of
conversations in online forums.
In our preliminary experiments, we found out
that the clustering algorithm by itself performs
worse than the spectral algorithm of Cohen et al.
(2013). We believe that the reason is two-fold: (a)
k-means finds a local maximum during clustering;
(b) we do hard clustering instead of soft cluster-
ing. However, we detected that the clustering algo-
rithm gives a more diverse set of solutions, when
the features are perturbed. As such, in the next
sections, we explain how to perturb the models we
get from the clustering algorithm (and the spectral
algorithm) in order to improve the accuracy of the
clustering and spectral algorithms.
</bodyText>
<sectionHeader confidence="0.974813" genericHeader="method">
4 Spectral Estimation with Noise
</sectionHeader>
<bodyText confidence="0.999891733333333">
It has been shown that a diverse set of predictions
can be used to help improve decoder accuracy for
various problems in NLP (Henderson and Brill,
1999). Usually a k-best list from a single model
is used to exploit model diversity. Instead, we es-
timate multiple models, where the underlying fea-
tures are filtered with various noising schemes.
We try three different types of noise schemes for
the algorithm in Figure 2:
Dropout noise: Let Q E [0, 1]. We set each el-
ement in the feature vectors φ(t) and ψ(o) to 0
with probability Q.
Gaussian (additive): Let Q &gt; 0. For each x(z),
we draw a vector E E R2k of Gaussians with
mean 0 and variance Q2, and then set x(z) ←
x(z) + E.
Gaussian (multiplicative): Let Q &gt; 0. For each
x(z), we draw a vector E E R2k of Gaussians with
mean 0 and variance Q2, and then set x(z) ←
x(z) ® (1 + E), where ® is coordinate-wise mul-
tiplication.
Note the distinction between the dropout noise
and the Gaussian noise schemes: the first is per-
formed on the feature vectors before the SVD step,
and the second is performed after the SVD step. It
is not feasible to add Gaussian noise prior to the
SVD step, since the matrix Ω&apos; will no longer be
sparse, and its SVD computation will be computa-
tionally demanding.
Our use of dropout noise here is inspired by
“dropout” as is used in neural network training,
where various connections between units in the
neural network are dropped during training in or-
der to avoid overfitting of these units to the data
(Srivastava et al., 2014).
The three schemes we described were also used
by Wang et al. (2013) to train log-linear models.
Wang et al.’s goal was to prevent overfitting by
introducing this noise schemes as additional reg-
ularizer terms, but without explicitly changing the
training data. We do filter the data through these
noise schemes, and show in §6 that all of these
noise schemes do not improve the performance of
our estimation on their own. However, when mul-
tiple models are created with these noise schemes,
</bodyText>
<page confidence="0.965459">
1871
</page>
<bodyText confidence="0.999188793103448">
and then combined together, we get an improved
performance. As such, our approach is related to
the one of Petrov (2010), who builds a commit-
tee of latent-variable PCFGs in order to improve a
natural language parser.
We also use these perturbation schemes to cre-
ate multiple models for the algorithm of Cohen et
al. (2012). The dropout scheme stays the same,
but for the Gaussian noising schemes, we follow a
slightly different procedure. After noising the pro-
jections of the inside and outside feature functions
we get from the SVD step, we use these projected
noised features as a new set of inside and outside
feature functions, and re-run the spectral algorithm
of Cohen et al. (2012) on them.
We are required to add this extra SVD step be-
cause the spectral algorithm of Cohen et al. as-
sumes the existence of linearly transformed pa-
rameter estimates, where the parameters of each
nonterminal a is linearly transformed by unknown
invertible matrices. These matrices cancel out
when the inside-outside algorithm is run with the
spectral estimate output. In order to ensure that
these matrices still exactly cancel out, we have to
follow with another SVD step as described above.
The latter SVD step is performed on a dense Qa E
Rm�m but this is not an issue considering m (the
number of latent states) is much smaller than d or
d&apos;.
</bodyText>
<sectionHeader confidence="0.943135" genericHeader="method">
5 Decoding with Multiple Models
</sectionHeader>
<bodyText confidence="0.975867933333333">
Let G1, ... , Gp be a set of L-PCFG grammars. In
§6, we create such models using the noising tech-
niques described above. The question that remains
is how to combine these models together to get a
single best output parse tree given an input sen-
tence.
With L-PCFGs, decoding a single sentence re-
quires marginalizing out the latent states to find
the best skeletal tree2 for a given string. Let s be a
sentence. We define t(Gi, s) to be the output tree
according to minimum Bayes risk decoding. This
means we follow Goodman (1996), who uses dy-
namic programming to compute the tree that maxi-
mizes the sum of all marginals of all nonterminals
in the output tree. Each marginal, for each span
(a, i, j) (where a is a nonterminal and i and j are
endpoints in the sentence), is computed by using
the inside-outside algorithm.
2A skeletal tree is a derivation tree without latent states
decorating the nonterminals.
In addition, let µ(a, i, j|Gk, s) be the marginal,
as computed by the inside-outside algorithm, for
the span (a, i, j) with grammar Gk for string s.
We use the notation (a, i, j) E t to denote that a
span (a, i, j) is in a tree t.
We suggest the following three ways for decod-
ing with multiple models G1, ... , Gp:
Maximal tree coverage: Using dynamic pro-
gramming, we return the tree that is the solution
to:
</bodyText>
<equation confidence="0.96033525">
�
t* = arg max
t
(a,i,j)Et
</equation>
<bodyText confidence="0.9863585">
This implies that we find the tree that maximizes
its coverage with respect to all other trees that are
decoded using G1, ... , Gp.
Maximal marginal coverage: Using dynamic
programming, we return the tree that is the
solution to:
</bodyText>
<equation confidence="0.96656625">
�
t* = arg max
t
(a,i,j)Et
</equation>
<bodyText confidence="0.999541052631579">
This is similar to maximal tree coverage, only in-
stead of considering just the single decoded tree
for each model among G1, ... , Gp, we make our
decoding “softer,” and rely on the marginals that
each model gives.
MaxEnt reranking: We train a MaxEnt reranker
on a training set that includes outputs from mul-
tiple models, and then, during testing time, de-
code with each of the models, and use the trained
reranker to select one of the parses. We use the
reranker of Charniak and Johnson (2005).3
As we see later in §6, it is sometimes possible to
extract more information from the training data by
using a network, or a hierarchy of the above tree
combination methods. For example, we get our
best result for parsing by first using MaxEnt with
several subsets of the models, and then combining
the output of these MaxEnt models using maximal
tree coverage.
</bodyText>
<footnote confidence="0.886102">
3Implementation: https://github.com/BLLIP/
</footnote>
<figureCaption confidence="0.59653425">
bllip-parser. More specifically, we used the
programs extract-spfeatures, cvlm-lbfgs and
best-indices. cvlm-lbfgs was used with the default
hyperparameters from the Makefile.
</figureCaption>
<figure confidence="0.6753165">
� p [[(a, i, j) E t(Gk, s)]].
k=1
� p µ(a,i,j|Gk,s).
k=1
</figure>
<page confidence="0.868048">
1872
</page>
<figureCaption confidence="0.938536">
Figure 3: F1 scores of noisy models. Each data
</figureCaption>
<bodyText confidence="0.8731545">
point gives the F1 accuracy of a single model on
the development set, based on the legend. The x-
axis enumerates the models (80 in total for each
noise scheme).
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999775">
In this section, we describe parsing experiments
with two languages: English and German.
</bodyText>
<sectionHeader confidence="0.797409" genericHeader="method">
6.1 Results for English
</sectionHeader>
<bodyText confidence="0.999511106060606">
For our English parsing experiments, we use a
standard setup. More specifically, we use the Penn
WSJ treebank (Marcus et al., 1993) for our experi-
ments, with sections 2–21 as the training data, and
section 22 used as the development data. Section
23 is used as the final test set. We binarize the
trees in training data, but transform them back be-
fore evaluating them.
For efficiency, we use a base PCFG without
latent states to prune marginals which receive
a value less than 0.00005 in the dynamic pro-
gramming chart. The parser takes part-of-speech
tagged sentences as input. We tag all datasets us-
ing Turbo Tagger (Martins et al., 2010), trained on
sections 2–21. We use the F1 measure according
to the PARSEVAL metric (Black et al., 1991) for
the evaluation.
Preliminary experiments We first experiment
with the number of latent states for the clustering
algorithm without perturbations. We use k = 100
for the SVD step. Whenever we need to cluster
a set of points, we run the k-means algorithm 10
times with random restarts and choose the clus-
tering result with the lowest objective value. On
section 22, the clustering algorithm achieves the
following results (F1 measure): m = 8: 84.30%,
m = 16: 85.98%, m = 24: 86.48%, m = 32:
85.84%, m = 36: 86.05%, m = 40: 85.43%.
As we increase the number of states, performance
improves, but plateaus at m = 24. For the rest of
our experiments, both with the spectral algorithm
of Cohen et al. (2012) and the clustering algorithm
presented in this paper, we use m = 24.
Compact models One of the advantage of the
clustering algorithm is that it leads to much more
compact models. The number of nonzero param-
eters with m = 24 for the clustering algorithm is
approximately 97K, while the spectral algorithms
lead to a significantly larger number of nonzero
parameters with the same number of latent states:
approximately 54 million.
Oracle experiments To what extent do we get
a diverse set of solutions from the different mod-
els we estimate? This question can be answered by
testing the oracle accuracy in the different settings.
For each type of noising scheme, we generated 80
models, 20 for each Q E 10.05, 0.1, 0.15, 0.2}.
Each noisy model by itself lags behind the best
model (see Figure 3). However, when choosing
the best tree among these models, the additively-
noised models get an oracle accuracy of 95.91%
on section 22; the multiplicatively-noised models
get an oracle accuracy of 95.81%; and the dropout-
noised models get an oracle accuracy of 96.03%.
Finally all models combined get an oracle accu-
racy of 96.67%. We found out that these oracle
scores are comparable to the one Charniak and
Johnson (2005) report.
We also tested our oracle results, comparing
the spectral algorithm of Cohen et al. (2013) to
the clustering algorithm. We generated 20 mod-
els for each type of noising scheme, 5 for each
Q E 10.05, 0.1, 0.15, 0.2}) for the spectral al-
gorithm.4 Surprisingly, even though the spectral
models were smoothed, their oracle accuracy was
lower than the accuracy of the clustering algo-
</bodyText>
<footnote confidence="0.6028558">
4There are two reasons we use a smaller number of mod-
els with the spectral algorithm: (a) models are not compact
(see text) and (b) as such, parsing takes comparatively longer.
However, in the above comparison, we use 20 models for the
clustering algorithm as well.
</footnote>
<page confidence="0.924472">
1873
</page>
<table confidence="0.999724142857143">
MaxTre Clustering MaxEnt Spectral (smoothing) MaxEnt Spectral (no smoothing) MaxEnt
MaxMrg MaxTre MaxMrg MaxTre MaxMrg
Add 88.68 88.64 89.50 88.20 88.28 88.59 86.72 86.85 87.94
Mul 88.74 88.66 89.89 88.48 88.70 89.46 86.97 86.53 89.04
Dropout 88.68 88.56 89.80 88.64 88.71 89.47 88.37 88.06 89.52
All 88.84 88.75 89.95 88.38 88.75 89.45 87.49 87.00 89.85
No noise 86.48 88.53 (Cohen et al., 2013) 86.47 (Cohen et al., 2013)
</table>
<tableCaption confidence="0.999329">
Table 1: Results on section 22 (WSJ). MaxTre denotes decoding using maximal tree coverage, MaxMrg
</tableCaption>
<bodyText confidence="0.956326230769231">
denotes decoding using maximal marginal coverage, and MaxEnt denotes the use of a discriminative
reranker. Add, Mul and Dropout denote the use of additive Gaussian noise, multiplicative Gaussian
noise and dropout noise, respectively. The number of models used in the first three rows for the clustering
algorithm is 80: 20 for each Q ∈ {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q
(see footnotes). The number of latent states is m = 24. For All, we use all models combined from the
first three rows. The “No noise” baseline for the spectral algorithm is taken from Cohen et al. (2013).
The best figure in each algorithm block is in boldface.
rithm: 92.81% vs. 95.73%.5 This reinforces two
ideas: (i) that noising acts as a regularizer, and has
a similar role to backoff smoothing, as we see be-
low; and (ii) the noisy estimation for the clustering
algorithm produces a more diverse set of parses
than that produced with the spectral algorithm.
</bodyText>
<table confidence="0.999001714285714">
Method Fi
Best Spectral (unsmoothed) 89.21
Spectral (smoothed) 88.87
Clustering 89.25
Hier Spectral (unsmoothed) 89.09
Spectral (smoothed) 89.06
Clustering 90.18
</table>
<tableCaption confidence="0.8585265">
Table 2: Results on section 23 (English). The first
three results (Best) are taken with the best model
</tableCaption>
<bodyText confidence="0.9353236">
in each corresponding block in Table 1. The last
three results (Hier) use a hierarchy of the above
tree combination methods in each block. It com-
bines all MaxEnt results using the maximal tree
coverage (see text).
It is also important to note that the high ora-
cle accuracy is not just the result of k-means not
finding the global maximum for the clustering ob-
jective. If we just run the clustering algorithms
with 80 models as before, without perturbing the
features, the oracle accuracy is 95.82%, which is
lower than the oracle accuracy with the additive
and dropout perturbed models. To add to this, we
see below that perturbing the training set with the
spectral algorithm of Cohen et al. improves the ac-
</bodyText>
<footnote confidence="0.6327">
5Oracle scores for the clustering algorithm: 95.73% (20
models for each noising scheme) and 96.67% (80 models for
each noising scheme).
</footnote>
<bodyText confidence="0.99475254054054">
curacy of the spectral algorithm. Since the spectral
algorithm of Cohen et al. does not maximize any
objective locally, it shows that the role of the per-
turbations we use is important.
Results Results on the development set are
given in Table 1 with our three decoding methods.
We present the results from three algorithms: the
clustering algorithm and the spectral algorithms
(smoothed and unsmoothed).6
It seems that dropout noise for the spectral algo-
rithm acts as a regularizer, similarly to the back-
off smoothing techniques that are used in Cohen
et al. (2013). This is evident from the two spectral
algorithm blocks in Table 1, where dropout noise
does not substantially improve the smoothed spec-
tral model (Cohen et al. report accuracy of 88.53%
with smoothed spectral model for m = 24 without
noise) – the accuracy is 88.64%–88.71%–89.47%,
but the accuracy substantially improves for the un-
smoothed spectral model, where dropout brings an
accuracy of 86.47% up to 89.52%.
All three blocks in Table 1 demonstrate that
decoding with the MaxEnt reranker performs the
best. Also it is interesting to note that our results
continue to improve when combining the output of
previous combination steps further. The best re-
sult on section 22 is achieved when we combine,
using maximal tree coverage, all MaxEnt outputs
of the clustering algorithm (the first block in Ta-
6Cohen et al. (2013) propose two variants of spectral
estimation for L-PCFGs: smoothed and unsmoothed. The
smoothed model uses a simple backedoff smoothing method
which leads to significant improvements over the unsmoothed
one. Here we compare our clustering algorithm against both
of these models. However unless specified otherwise, the
spectral algorithm of Cohen et al. (2013) refers to their best
model, i.e. the smoothed model.
</bodyText>
<page confidence="0.973244">
1874
</page>
<table confidence="0.999744">
MaxTre Clustering MaxEnt Spectral (smoothing) MaxEnt Spectral (no smoothing) MaxEnt
MaxMrg MaxTre MaxMrg MaxTre MaxMrg
Add 77.34 76.87 80.01 77.76 77.85 78.09 77.44 77.56 77.91
Mul 77.80 77.80 80.34 77.80 77.76 78.89 77.62 77.85 78.94
Dropout 77.37 77.17 80.94 77.94 78.06 79.02 77.97 78.17 79.18
All 77.71 77.51 80.86 78.04 77.89 79.46 77.73 77.91 79.66
No noise 75.04 77.71 77.07
</table>
<tableCaption confidence="0.999336">
Table 3: Results on the development set for German. See Table 1 for interpretation of MaxTre, MaxMrg,
</tableCaption>
<bodyText confidence="0.959892238095238">
MaxEnt and Add, Mul and Dropout. The number of models used in the first three rows for the clustering
algorithm is 80: 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q.
The number of latent states is m = 8. For All, we use all models combined from the first three rows. The
best figure in each algorithm block is in boldface.
ble 1). This yields a 90.68% F1 accuracy. This is
also the best result we get on the test set (section
23), 90.18%. See Table 2 for results on section 23.
Our results are comparable to state-of-the-art
results for parsing. For example, Sagae and Lavie
(2006), Fossum and Knight (2009) and Zhang et
al. (2009) report an accuracy of 93.2%-93.3% us-
ing parsing recombination; Shindo et al. (2012)
report an accuracy of 92.4 F1 using a Bayesian
tree substitution grammar; Petrov (2010) reports
an accuracy of 92.0% using product of L-PCFGs;
Charniak and Johnson (2005) report accuracy of
91.4 using a discriminative reranking model; Car-
reras et al. (2008) report 91.1 F1 accuracy for a
discriminative, perceptron-trained model; Petrov
and Klein (2007) report an accuracy of 90.1 F1.
Collins (2003) reports an accuracy of 88.2 F1.
</bodyText>
<subsectionHeader confidence="0.776991">
6.2 Results for German
</subsectionHeader>
<bodyText confidence="0.99998144">
For the German experiments, we used the NEGRA
corpus (Skut et al., 1997). We use the same setup
as in Petrov (2010), and use the first 18,602 sen-
tences as a training set, the next 1,000 sentences as
a development set and the last 1,000 sentences as
a test set. This corresponds to an 80%-10%-10%
split of the treebank.
Our German experiments follow the same set-
ting as in our English experiments. For the clus-
tering algorithm we generated 80 models, 20 for
each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral
algorithm, we generate 20 models, 5 for each Q.
For the reranking experiment, we had to modify
the BLLIP parser (Charniak and Johnson, 2005)
to use the head features from the German tree-
bank. We based our modifications on the docu-
mentation for the NEGRA corpus (our modifica-
tions are based mostly on mapping of nontermi-
nals to coarse syntactic categories).
Preliminary experiments For German, we also
experiment with the number of latent states. On
the development set, we observe that the F1 mea-
sure is: 75.04% for m = 8, 73.44% for m = 16
and 70.84% for m = 24. For the rest of our experi-
ments, we fix the number of latent states at m = 8.
</bodyText>
<table confidence="0.998629428571428">
Method F1
Best Spectral (unsmoothed) 80.88
Spectral (smoothed) 80.31
Clustering 81.94
Hier Spectral (unsmoothed) 80.64
Spectral (smoothed) 79.96
Clustering 83.38
</table>
<tableCaption confidence="0.992447">
Table 4: Results on the test set for the German
</tableCaption>
<bodyText confidence="0.995539904761905">
data. The first three results (Best) are taken with
the best model in each corresponding block in Ta-
ble 3. The last three results (Hier) use a hierarchy
of the above tree combination methods.
Oracle experiments The additively-noised
models get an oracle accuracy of 90.58% on
the development set; the multiplicatively-noised
models get an oracle accuracy of 90.47%; and
the dropout-noised models get an oracle accuracy
of 90.69%. Finally all models combined get an
oracle accuracy of 92.38%.
We compared our oracle results to those given
by the spectral algorithm of Cohen et al. (2013).
With 20 models for each type of noising scheme,
all spectral models combined achieve an oracle ac-
curacy of 83.45%. The clustering algorithm gets
the oracle score of 90.12% when using the same
number of models.
Results Results on the development set and on
the test set are given in Table 3 and Table 4 re-
spectively.
</bodyText>
<page confidence="0.969824">
1875
</page>
<bodyText confidence="0.999687875">
Like English, in all three blocks in Table 3, de-
coding with the MaxEnt reranking performs the
best. Our results continue to improve when fur-
ther combining the output of previous combina-
tion steps. The best result of 82.04% on the devel-
opment set is achieved when we combine, using
maximal tree coverage, all MaxEnt outputs of the
clustering algorithm (the first block in Table 3).
This also leads to the best result of 83.38% on the
test set. See Table 4 for results on the test set.
Our results are comparable to state-of-the-art
results for German parsing. For example, Petrov
(2010) reports an accuracy of 84.5% using prod-
uct of L-PCFGs; Petrov and Klein (2007) report
an accuracy of 80.1 Fl; and Dubey (2005) reports
an accuracy of 76.3 Fl.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999958838709677">
From a theoretical point of view, one of the
great advantages of spectral learning techniques
for latent-variable models is that they yield consis-
tent parameter estimates. Our clustering algorithm
for L-PCFG estimation breaks this, but there is a
work-around to obtain an algorithm which would
be statistically consistent.
The main reason that our algorithm is not a con-
sistent estimator is that it relies on k-means clus-
tering, which maximizes a non-convex objective
using hard clustering steps. The k-means algo-
rithm can be viewed as “hard EM” for a Gaussian
mixture model (GMM), where each latent state is
associated with one of the mixture components in
the GMM. This means that instead of following up
with k-means, we could have identified the param-
eters and the posteriors for a GMM, where the ob-
servations correspond to the vectors that we clus-
ter. There are now algorithms, some of which are
spectral, that aim to solve this estimation problem
with theoretical guarantees (Vempala and Wang,
2004; Kannan et al., 2005; Moitra and Valiant,
2010).
With theoretical guarantees on the correctness
of the posteriors from this step, the subsequent
use of maximum likelihood estimation step could
yield consistent parameter estimates. The con-
sistency guarantees will largely depend on the
amount of information that exists in the base fea-
ture functions about the latent states according to
the L-PCFG model.
</bodyText>
<sectionHeader confidence="0.997805" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999945692307692">
We presented a novel estimation algorithm for
latent-variable PCFGs. This algorithm is based
on clustering of continuous tree representations,
and it also leads to sparse grammar estimates and
compact models. We also showed how to get a
diverse set of parse tree predictions with this algo-
rithm and also older spectral algorithms. Each pre-
diction in the set is made by training an L-PCFG
model after perturbing the underlying features that
estimation algorithm uses from the training data.
We showed that such a diverse set of predictions
can be used to improve the parsing accuracy of En-
glish and German.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999853">
The authors would like to thank David McClosky
for his help with running the BLLIP parser and the
three anonymous reviewers for their helpful com-
ments. This research was supported by an EPSRC
grant (EP/L02411X/1).
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999652642857143">
Rapha¨el Bailly, Amaury Habrard, and Franc¸ois Denis.
2010. A spectral approach for probabilistic gram-
matical inference on trees. In Proceedings of ALT.
Ezra W. Black, Steven Abney, Daniel P. Flickinger,
Claudia Gdaniec, Ralph Grishman, Philip Harri-
son, Donald Hindle, Robert J. P. Ingria, Freder-
ick Jelinek, Judith L. Klavans, Mark Y. Liberman,
Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of DARPA
Workshop on Speech and Natural Language.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-rich Parsing. In Pro-
ceedings of CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Do Kook Choe, David McClosky, and Eugene Char-
niak. 2015. Syntactic parse fusion. In Proceedings
of EMNLP.
Shay B. Cohen and Michael Collins. 2014. A prov-
ably correct learning algorithm for latent-variable
PCFGs. In Proceedings of ACL.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable PCFGs. In Proceedings of ACL.
</reference>
<page confidence="0.836235">
1876
</page>
<reference confidence="0.999919698113207">
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language processing. Computational
Linguistics, 29:589–637.
Paramveer Dhillon, Dean Foster, and Lyle Ungar.
2015. Eigenwords: Spectral word embeddings.
Journal of Machine Learning Research (to appear).
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: Parsing German with suffix analysis and
smoothing. In Proceedings of ACL.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In Proceedings of HLT-NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of EMNLP.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A spectral algorithm for learning hidden Markov
models. In Proceedings of COLT.
Ravindran Kannan, Hadi Salmasian, and Santosh Vem-
pala. 2005. The spectral method for general mix-
ture models. In Learning Theory, volume 3559 of
Lecture Notes in Computer Science, pages 444–457.
Springer.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for nat-
ural language processing. In Computational lin-
guistics and intelligent text processing, volume 3406
of Lecture Notes in Computer Science, pages 1–24.
Springer.
Annie Louis and Shay B. Cohen. 2015. Conversation
trees: A grammar model for topic structure in fo-
rums. In Proceedings of EMNLP.
Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual corre-
lation for improved word embeddings. In Proceed-
ings of NAACL.
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for non-
deterministic dependency parsing. In Proceedings
of EACL.
Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
Proceedings of EMNLP-CoNLL.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19:313–330.
Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing,
M´ario A. T. Figueiredo, and Pedro M. Q. Aguiar.
2010. TurboParsers: Dependency parsing by ap-
proximate variational inference. In Proceedings of
EMNLP.
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings ofACL.
Ankur Moitra and Gregory Valiant. 2010. Settling the
polynomial learnability of mixtures of gaussians. In
Proceedings of IEEE Symposium on Foundations of
Computer Science (FOCS).
Thang Nguyen, Jordan Boyd-Graber, Jeffrey Lund,
Kevin Seppi, and Eric Ringger. 2015. Is your an-
chor going up or down? Fast and accurate super-
vised topic models. In Proceedings of NAACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT-NAACL.
Pushpendre Rastogi, Benjamin Van Durme, and Raman
Arora. 2015. Multiview LSA: Representation learn-
ing via generalized CCA. In Proceedings of NAACL.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT-NAACL.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings ofACL.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings ofANLP.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.
Karl Stratos, Do-kyum Kim, Michael Collins, and
Daniel Hsu. 2014. A spectral algorithm for learn-
ing class-based n-gram models of natural language.
Proceedings of UAI.
Hans Van Halteren, Jakub Zavrel, and Walter Daele-
mans. 2001. Improving accuracy in word class tag-
ging through the combination of machine learning
systems. Computational linguistics, 27(2):199–229.
Santosh Vempala and Grant Wang. 2004. A spectral
algorithm for learning mixture models. Journal of
Computer and System Sciences, 68(4):841–860.
</reference>
<page confidence="0.849124">
1877
</page>
<reference confidence="0.997593">
Sida Wang, Mengqiu Wang, Stefan Wager, Percy
Liang, and Christopher D Manning. 2013. Feature
noising for log-linear structured prediction. In Pro-
ceedings of EMNLP.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP.
</reference>
<page confidence="0.994286">
1878
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451984">
<title confidence="0.998543">Diversity in Spectral Learning for Natural Language Parsing</title>
<author confidence="0.998104">Shashi Narayan</author>
<author confidence="0.998104">B Shay</author>
<affiliation confidence="0.995143">School of University of</affiliation>
<address confidence="0.508473">Edinburgh, EH8 9LE,</address>
<abstract confidence="0.994105842105263">We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of art. For English, we achieve the score of 90.18, and for German we achieve of 83.38.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amaury Habrard Rapha¨el Bailly</author>
<author>Franc¸ois Denis</author>
</authors>
<title>A spectral approach for probabilistic grammatical inference on trees.</title>
<date>2010</date>
<booktitle>In Proceedings of ALT.</booktitle>
<marker>Rapha¨el Bailly, Denis, 2010</marker>
<rawString>Rapha¨el Bailly, Amaury Habrard, and Franc¸ois Denis. 2010. A spectral approach for probabilistic grammatical inference on trees. In Proceedings of ALT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra W Black</author>
<author>Steven Abney</author>
<author>Daniel P Flickinger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>Philip Harrison</author>
<author>Donald Hindle</author>
<author>Robert J P Ingria</author>
<author>Frederick Jelinek</author>
<author>Judith L Klavans</author>
<author>Mark Y Liberman</author>
<author>Mitchell P Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="21525" citStr="Black et al., 1991" startWordPosition="3861" endWordPosition="3864">., 1993) for our experiments, with sections 2–21 as the training data, and section 22 used as the development data. Section 23 is used as the final test set. We binarize the trees in training data, but transform them back before evaluating them. For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. The parser takes part-of-speech tagged sentences as input. We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2–21. We use the F1 measure according to the PARSEVAL metric (Black et al., 1991) for the evaluation. Preliminary experiments We first experiment with the number of latent states for the clustering algorithm without perturbations. We use k = 100 for the SVD step. Whenever we need to cluster a set of points, we run the k-means algorithm 10 times with random restarts and choose the clustering result with the lowest objective value. On section 22, the clustering algorithm achieves the following results (F1 measure): m = 8: 84.30%, m = 16: 85.98%, m = 24: 86.48%, m = 32: 85.84%, m = 36: 86.05%, m = 40: 85.43%. As we increase the number of states, performance improves, but plat</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra W. Black, Steven Abney, Daniel P. Flickinger, Claudia Gdaniec, Ralph Grishman, Philip Harrison, Donald Hindle, Robert J. P. Ingria, Frederick Jelinek, Judith L. Klavans, Mark Y. Liberman, Mitchell P. Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="30013" citStr="Carreras et al. (2008)" startWordPosition="5290" endWordPosition="5294"> F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank. Our German experiments follow the same setting as in our English experiments. For t</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1240" citStr="Charniak and Johnson, 2005" startWordPosition="198" endWordPosition="201">PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all </context>
<context position="19805" citStr="Charniak and Johnson (2005)" startWordPosition="3574" endWordPosition="3577">... , Gp. Maximal marginal coverage: Using dynamic programming, we return the tree that is the solution to: � t* = arg max t (a,i,j)Et This is similar to maximal tree coverage, only instead of considering just the single decoded tree for each model among G1, ... , Gp, we make our decoding “softer,” and rely on the marginals that each model gives. MaxEnt reranking: We train a MaxEnt reranker on a training set that includes outputs from multiple models, and then, during testing time, decode with each of the models, and use the trained reranker to select one of the parses. We use the reranker of Charniak and Johnson (2005).3 As we see later in §6, it is sometimes possible to extract more information from the training data by using a network, or a hierarchy of the above tree combination methods. For example, we get our best result for parsing by first using MaxEnt with several subsets of the models, and then combining the output of these MaxEnt models using maximal tree coverage. 3Implementation: https://github.com/BLLIP/ bllip-parser. More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices. cvlm-lbfgs was used with the default hyperparameters from the Makefile. � p [[(a, i, j) E </context>
<context position="23443" citStr="Charniak and Johnson (2005)" startWordPosition="4194" endWordPosition="4197">racle accuracy in the different settings. For each type of noising scheme, we generated 80 models, 20 for each Q E 10.05, 0.1, 0.15, 0.2}. Each noisy model by itself lags behind the best model (see Figure 3). However, when choosing the best tree among these models, the additivelynoised models get an oracle accuracy of 95.91% on section 22; the multiplicatively-noised models get an oracle accuracy of 95.81%; and the dropoutnoised models get an oracle accuracy of 96.03%. Finally all models combined get an oracle accuracy of 96.67%. We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report. We also tested our oracle results, comparing the spectral algorithm of Cohen et al. (2013) to the clustering algorithm. We generated 20 models for each type of noising scheme, 5 for each Q E 10.05, 0.1, 0.15, 0.2}) for the spectral algorithm.4 Surprisingly, even though the spectral models were smoothed, their oracle accuracy was lower than the accuracy of the clustering algo4There are two reasons we use a smaller number of models with the spectral algorithm: (a) models are not compact (see text) and (b) as such, parsing takes comparatively longer. However, in the above comparison, we </context>
<context position="29926" citStr="Charniak and Johnson (2005)" startWordPosition="5277" endWordPosition="5280">e rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treeba</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Do Kook Choe</author>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Syntactic parse fusion.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1342" citStr="Choe et al., 2015" startWordPosition="216" endWordPosition="219">German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diver</context>
</contexts>
<marker>Choe, McClosky, Charniak, 2015</marker>
<rawString>Do Kook Choe, David McClosky, and Eugene Charniak. 2015. Syntactic parse fusion. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Michael Collins</author>
</authors>
<title>A provably correct learning algorithm for latent-variable PCFGs.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13143" citStr="Cohen and Collins (2014)" startWordPosition="2390" endWordPosition="2393">e foot of the outside tree to the root of the tree which is different from the head node of the foot node. • The width of the spans to the left and to the right of the foot node, paired with the label of the foot node. Other Spectral Algorithms The SVD step on the Ω&apos; matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clusterin</context>
</contexts>
<marker>Cohen, Collins, 2014</marker>
<rawString>Shay B. Cohen and Michael Collins. 2014. A provably correct learning algorithm for latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3347" citStr="Cohen et al. (2012)" startWordPosition="550" endWordPosition="553">arsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more about spectral algorithms – it demonstrates some of the core ideas in spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational L</context>
<context position="12914" citStr="Cohen et al. (2012)" startWordPosition="2351" endWordPosition="2354">oot node, together with the label of its parent. E.g., the pair (VP, S). • The label of the foot node, together with the label of its parent and grandparent. • The part-of-speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of the foot node. • The width of the spans to the left and to the right of the foot node, paired with the label of the foot node. Other Spectral Algorithms The SVD step on the Ω&apos; matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm w</context>
<context position="16668" citStr="Cohen et al. (2012)" startWordPosition="3006" endWordPosition="3009">izer terms, but without explicitly changing the training data. We do filter the data through these noise schemes, and show in §6 that all of these noise schemes do not improve the performance of our estimation on their own. However, when multiple models are created with these noise schemes, 1871 and then combined together, we get an improved performance. As such, our approach is related to the one of Petrov (2010), who builds a committee of latent-variable PCFGs in order to improve a natural language parser. We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the projections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them. We are required to add this extra SVD step because the spectral algorithm of Cohen et al. assumes the existence of linearly transformed parameter estimates, where the parameters of each nonterminal a is linearly transformed </context>
<context position="22229" citStr="Cohen et al. (2012)" startWordPosition="3987" endWordPosition="3990">atent states for the clustering algorithm without perturbations. We use k = 100 for the SVD step. Whenever we need to cluster a set of points, we run the k-means algorithm 10 times with random restarts and choose the clustering result with the lowest objective value. On section 22, the clustering algorithm achieves the following results (F1 measure): m = 8: 84.30%, m = 16: 85.98%, m = 24: 86.48%, m = 32: 85.84%, m = 36: 86.05%, m = 40: 85.43%. As we increase the number of states, performance improves, but plateaus at m = 24. For the rest of our experiments, both with the spectral algorithm of Cohen et al. (2012) and the clustering algorithm presented in this paper, we use m = 24. Compact models One of the advantage of the clustering algorithm is that it leads to much more compact models. The number of nonzero parameters with m = 24 for the clustering algorithm is approximately 97K, while the spectral algorithms lead to a significantly larger number of nonzero parameters with the same number of latent states: approximately 54 million. Oracle experiments To what extent do we get a diverse set of solutions from the different models we estimate? This question can be answered by testing the oracle accurac</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable PCFGs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2245" citStr="Cohen et al., 2013" startWordPosition="368" endWordPosition="372">in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petr</context>
<context position="4282" citStr="Cohen et al., 2013" startWordPosition="698" endWordPosition="701">ond, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In §2 we describe notation and background about L-PCFG parsing. In §3 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In §4 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of Cohen et al. (2013). In §5 we describe how to decode with multiple models, each arising from a different noise sett</context>
<context position="11020" citStr="Cohen et al. (2013)" startWordPosition="1966" endWordPosition="1969">L-PCFG. Consider for example the estimation of rules of the form a → x. Following the clustering step we obtain for each nonterminal a and latent state h a set of rules of the form a[h] → x. Each such instance comes from a single training example of a lexical rule. Next, we compute the probability of the rule a[h] → x by counting how many times that rule appears in the training instances, and normalize by the total count of a[h] in the training instances. Similarly, we compute probabilities for binary rules of the form a → b c. The features that we use for φ and ψ are similar to those used in Cohen et al. (2013). These features look at the local neighborhood surrounding a given node. More specifically, we indicate the following information with the inside features (throughout these definitions assume that a → b c is at the root of the inside tree t): • The pair of nonterminals (a, b). E.g., for the inside tree in Figure 1 this would be the pair (VP, V). • The pair (a, c). E.g., (VP, NP). • The rule a → b c. E.g., VP → V NP. • The rule a → b c paired with the rule at the node b. E.g., for the inside tree in Figure 1 this would correspond to the tree fragment (VP (V saw) NP). • The rule a → b c paired </context>
<context position="13830" citStr="Cohen et al. (2013)" startWordPosition="2502" endWordPosition="2505">side-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft clustering. However, we detected that the clustering algorithm gives a more diverse set of solutions, when the features are perturbed. As such, in the next sections, we explain how to perturb the models we get from the clustering algorithm (and the spectral algorithm) in order to improve the accuracy of the clustering and spectral algorithms. 4 Spectral Estimation with Noise It has been shown that a diverse set of predictions can be used to help improve decod</context>
<context position="23542" citStr="Cohen et al. (2013)" startWordPosition="4210" endWordPosition="4213">ach Q E 10.05, 0.1, 0.15, 0.2}. Each noisy model by itself lags behind the best model (see Figure 3). However, when choosing the best tree among these models, the additivelynoised models get an oracle accuracy of 95.91% on section 22; the multiplicatively-noised models get an oracle accuracy of 95.81%; and the dropoutnoised models get an oracle accuracy of 96.03%. Finally all models combined get an oracle accuracy of 96.67%. We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report. We also tested our oracle results, comparing the spectral algorithm of Cohen et al. (2013) to the clustering algorithm. We generated 20 models for each type of noising scheme, 5 for each Q E 10.05, 0.1, 0.15, 0.2}) for the spectral algorithm.4 Surprisingly, even though the spectral models were smoothed, their oracle accuracy was lower than the accuracy of the clustering algo4There are two reasons we use a smaller number of models with the spectral algorithm: (a) models are not compact (see text) and (b) as such, parsing takes comparatively longer. However, in the above comparison, we use 20 models for the clustering algorithm as well. 1873 MaxTre Clustering MaxEnt Spectral (smoothi</context>
<context position="25236" citStr="Cohen et al. (2013)" startWordPosition="4496" endWordPosition="4499">xMrg denotes decoding using maximal marginal coverage, and MaxEnt denotes the use of a discriminative reranker. Add, Mul and Dropout denote the use of additive Gaussian noise, multiplicative Gaussian noise and dropout noise, respectively. The number of models used in the first three rows for the clustering algorithm is 80: 20 for each Q ∈ {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q (see footnotes). The number of latent states is m = 24. For All, we use all models combined from the first three rows. The “No noise” baseline for the spectral algorithm is taken from Cohen et al. (2013). The best figure in each algorithm block is in boldface. rithm: 92.81% vs. 95.73%.5 This reinforces two ideas: (i) that noising acts as a regularizer, and has a similar role to backoff smoothing, as we see below; and (ii) the noisy estimation for the clustering algorithm produces a more diverse set of parses than that produced with the spectral algorithm. Method Fi Best Spectral (unsmoothed) 89.21 Spectral (smoothed) 88.87 Clustering 89.25 Hier Spectral (unsmoothed) 89.09 Spectral (smoothed) 89.06 Clustering 90.18 Table 2: Results on section 23 (English). The first three results (Best) are ta</context>
<context position="27269" citStr="Cohen et al. (2013)" startWordPosition="4834" endWordPosition="4837">heme) and 96.67% (80 models for each noising scheme). curacy of the spectral algorithm. Since the spectral algorithm of Cohen et al. does not maximize any objective locally, it shows that the role of the perturbations we use is important. Results Results on the development set are given in Table 1 with our three decoding methods. We present the results from three algorithms: the clustering algorithm and the spectral algorithms (smoothed and unsmoothed).6 It seems that dropout noise for the spectral algorithm acts as a regularizer, similarly to the backoff smoothing techniques that are used in Cohen et al. (2013). This is evident from the two spectral algorithm blocks in Table 1, where dropout noise does not substantially improve the smoothed spectral model (Cohen et al. report accuracy of 88.53% with smoothed spectral model for m = 24 without noise) – the accuracy is 88.64%–88.71%–89.47%, but the accuracy substantially improves for the unsmoothed spectral model, where dropout brings an accuracy of 86.47% up to 89.52%. All three blocks in Table 1 demonstrate that decoding with the MaxEnt reranker performs the best. Also it is interesting to note that our results continue to improve when combining the </context>
<context position="32156" citStr="Cohen et al. (2013)" startWordPosition="5660" endWordPosition="5663">s on the test set for the German data. The first three results (Best) are taken with the best model in each corresponding block in Table 3. The last three results (Hier) use a hierarchy of the above tree combination methods. Oracle experiments The additively-noised models get an oracle accuracy of 90.58% on the development set; the multiplicatively-noised models get an oracle accuracy of 90.47%; and the dropout-noised models get an oracle accuracy of 90.69%. Finally all models combined get an oracle accuracy of 92.38%. We compared our oracle results to those given by the spectral algorithm of Cohen et al. (2013). With 20 models for each type of noising scheme, all spectral models combined achieve an oracle accuracy of 83.45%. The clustering algorithm gets the oracle score of 90.12% when using the same number of models. Results Results on the development set and on the test set are given in Table 3 and Table 4 respectively. 1875 Like English, in all three blocks in Table 3, decoding with the MaxEnt reranking performs the best. Our results continue to improve when further combining the output of previous combination steps. The best result of 82.04% on the development set is achieved when we combine, us</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--589</pages>
<contexts>
<context position="30155" citStr="Collins (2003)" startWordPosition="5314" endWordPosition="5315">ble to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank. Our German experiments follow the same setting as in our English experiments. For the clustering algorithm we generated 80 models, 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, we generate 20 models, 5 f</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29:589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Eigenwords: Spectral word embeddings.</title>
<date>2015</date>
<journal>Journal of Machine Learning Research</journal>
<note>(to appear).</note>
<contexts>
<context position="2289" citStr="Dhillon et al., 2015" startWordPosition="377" endWordPosition="380">ce of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting d</context>
<context position="9861" citStr="Dhillon et al., 2015" startWordPosition="1753" endWordPosition="1756">value decomposition (SVD), we essentially get representations for the inside trees and the outside trees that correspond to the latent states. This intuition leads to the algorithm that appears in Figure 2. The algorithm we describe takes as input training data, in the form of a treebank, decomposed into inside and outside trees at each node in each tree in the training set. The algorithm first performs SVD for each of the set of inside and outside trees for all nonterminals.1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; 1We normalize features by their variance. Rastogi et al., 2015). The algorithm then takes the representations induced by the SVD step, and clusters them – we use k-means to do the clustering. Finally, it maps each SVD representation to a cluster, and as a result, gets a cluster identifier for each node in each tree in the training data. These clusters are now treated as latent states that are “observed.” We subsequently follow up with frequency count maximum likelihood estimate to estimate the probabilities of each parameter in the L-PCFG. Consider for example the estimation of rules of the </context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2015</marker>
<rawString>Paramveer Dhillon, Dean Foster, and Lyle Ungar. 2015. Eigenwords: Spectral word embeddings. Journal of Machine Learning Research (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
</authors>
<title>What to do when lexicalization fails: Parsing German with suffix analysis and smoothing.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="33191" citStr="Dubey (2005)" startWordPosition="5845" endWordPosition="5846">Our results continue to improve when further combining the output of previous combination steps. The best result of 82.04% on the development set is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in Table 3). This also leads to the best result of 83.38% on the test set. See Table 4 for results on the test set. Our results are comparable to state-of-the-art results for German parsing. For example, Petrov (2010) reports an accuracy of 84.5% using product of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.1 Fl; and Dubey (2005) reports an accuracy of 76.3 Fl. 7 Discussion From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they yield consistent parameter estimates. Our clustering algorithm for L-PCFG estimation breaks this, but there is a work-around to obtain an algorithm which would be statistically consistent. The main reason that our algorithm is not a consistent estimator is that it relies on k-means clustering, which maximizes a non-convex objective using hard clustering steps. The k-means algorithm can be viewed as “hard EM” for a Ga</context>
</contexts>
<marker>Dubey, 2005</marker>
<rawString>Amit Dubey. 2005. What to do when lexicalization fails: Parsing German with suffix analysis and smoothing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Combining constituent parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1288" citStr="Fossum and Knight, 2009" startWordPosition="206" endWordPosition="209"> experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More</context>
<context position="29647" citStr="Fossum and Knight (2009)" startWordPosition="5232" endWordPosition="5235">out. The number of models used in the first three rows for the clustering algorithm is 80: 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q. The number of latent states is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used </context>
</contexts>
<marker>Fossum, Knight, 2009</marker>
<rawString>Victoria Fossum and Kevin Knight. 2009. Combining constituent parsers. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="18240" citStr="Goodman (1996)" startWordPosition="3285" endWordPosition="3286">umber of latent states) is much smaller than d or d&apos;. 5 Decoding with Multiple Models Let G1, ... , Gp be a set of L-PCFG grammars. In §6, we create such models using the noising techniques described above. The question that remains is how to combine these models together to get a single best output parse tree given an input sentence. With L-PCFGs, decoding a single sentence requires marginalizing out the latent states to find the best skeletal tree2 for a given string. Let s be a sentence. We define t(Gi, s) to be the output tree according to minimum Bayes risk decoding. This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree. Each marginal, for each span (a, i, j) (where a is a nonterminal and i and j are endpoints in the sentence), is computed by using the inside-outside algorithm. 2A skeletal tree is a derivation tree without latent states decorating the nonterminals. In addition, let µ(a, i, j|Gk, s) be the marginal, as computed by the inside-outside algorithm, for the span (a, i, j) with grammar Gk for string s. We use the notation (a, i, j) E t to denote that a span (a, i, j) is in</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1125" citStr="Henderson and Brill, 1999" startWordPosition="183" endWordPosition="186">ree ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information fro</context>
<context position="14497" citStr="Henderson and Brill, 1999" startWordPosition="2614" endWordPosition="2617"> (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft clustering. However, we detected that the clustering algorithm gives a more diverse set of solutions, when the features are perturbed. As such, in the next sections, we explain how to perturb the models we get from the clustering algorithm (and the spectral algorithm) in order to improve the accuracy of the clustering and spectral algorithms. 4 Spectral Estimation with Noise It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP (Henderson and Brill, 1999). Usually a k-best list from a single model is used to exploit model diversity. Instead, we estimate multiple models, where the underlying features are filtered with various noising schemes. We try three different types of noise schemes for the algorithm in Figure 2: Dropout noise: Let Q E [0, 1]. We set each element in the feature vectors φ(t) and ψ(o) to 0 with probability Q. Gaussian (additive): Let Q &gt; 0. For each x(z), we draw a vector E E R2k of Gaussians with mean 0 and variance Q2, and then set x(z) ← x(z) + E. Gaussian (multiplicative): Let Q &gt; 0. For each x(z), we draw a vector E E R</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="13092" citStr="Hsu et al. (2009)" startWordPosition="2381" endWordPosition="2384">f the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of the foot node. • The width of the spans to the left and to the right of the foot node, paired with the label of the foot node. Other Spectral Algorithms The SVD step on the Ω&apos; matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preli</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravindran Kannan</author>
<author>Hadi Salmasian</author>
<author>Santosh Vempala</author>
</authors>
<title>The spectral method for general mixture models.</title>
<date>2005</date>
<booktitle>In Learning Theory,</booktitle>
<volume>3559</volume>
<pages>444--457</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="34262" citStr="Kannan et al., 2005" startWordPosition="6020" endWordPosition="6023">on k-means clustering, which maximizes a non-convex objective using hard clustering steps. The k-means algorithm can be viewed as “hard EM” for a Gaussian mixture model (GMM), where each latent state is associated with one of the mixture components in the GMM. This means that instead of following up with k-means, we could have identified the parameters and the posteriors for a GMM, where the observations correspond to the vectors that we cluster. There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010). With theoretical guarantees on the correctness of the posteriors from this step, the subsequent use of maximum likelihood estimation step could yield consistent parameter estimates. The consistency guarantees will largely depend on the amount of information that exists in the base feature functions about the latent states according to the L-PCFG model. 8 Conclusion We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering of continuous tree representations, and it also leads to sparse grammar estimates and compact mo</context>
</contexts>
<marker>Kannan, Salmasian, Vempala, 2005</marker>
<rawString>Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. 2005. The spectral method for general mixture models. In Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 444–457. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Computational linguistics and intelligent text processing,</booktitle>
<volume>3406</volume>
<pages>1--24</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5852" citStr="Knight and Graehl, 2005" startWordPosition="1010" endWordPosition="1013"> N is the set of nonterminal symbols in the grammar. I ⊂ N is a finite set of interminals. P ⊂ N is a finite set of preterminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of nonterminals into two subsets. • [m] is the set of possible hidden states. • [n] is the set of possible words. • For all a ∈ I, b ∈ N, c ∈ N, h1, h2,h3 ∈ [m], we have a binary context-free rule a(h1) → b(h2) c(h3). • For all a ∈ P, h ∈ [m], x ∈ [n], we have a lexical context-free rule a(h) → x. Latent-variable PCFGs are essentially equivalent to probabilistic regular tree grammars (PRTGs; Knight and Graehl, 2005) where the righthand side trees are of depth 1. With general PRTGs, the righthand side can be of arbitrary depth, where the leaf nodes of these trees correspond to latent states in the L-PCFG formulation Figure 1: The inside tree (left) and outside tree (right) for the nonterminal VP in the parse tree (S (NP (D the) (N dog)) (VP (V saw) (NP (D the) (N woman)))). above and the internal nodes of these trees correspond to interminal symbols in the L-PCFG formulation. Two important concepts that will be used throughout of the paper are that of an “inside tree” and an “outside tree.” Given a tree, </context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Kevin Knight and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Computational linguistics and intelligent text processing, volume 3406 of Lecture Notes in Computer Science, pages 1–24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Shay B Cohen</author>
</authors>
<title>Conversation trees: A grammar model for topic structure in forums.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="13338" citStr="Louis and Cohen (2015)" startWordPosition="2424" endWordPosition="2427">bel of the foot node. Other Spectral Algorithms The SVD step on the Ω&apos; matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do har</context>
</contexts>
<marker>Louis, Cohen, 2015</marker>
<rawString>Annie Louis and Shay B. Cohen. 2015. Conversation trees: A grammar model for topic structure in forums. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Lu</author>
<author>Weiran Wang</author>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Deep multilingual correlation for improved word embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2350" citStr="Lu et al., 2015" startWordPosition="389" endWordPosition="392">utions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate </context>
</contexts>
<marker>Lu, Wang, Bansal, Gimpel, Livescu, 2015</marker>
<rawString>Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Deep multilingual correlation for improved word embeddings. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco M Luque</author>
<author>Ariadna Quattoni</author>
<author>Borja Balle</author>
<author>Xavier Carreras</author>
</authors>
<title>Spectral learning for nondeterministic dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2225" citStr="Luque et al., 2012" startWordPosition="364" endWordPosition="367">anking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berke</context>
</contexts>
<marker>Luque, Quattoni, Balle, Carreras, 2012</marker>
<rawString>Franco M. Luque, Ariadna Quattoni, Borja Balle, and Xavier Carreras. 2012. Spectral learning for nondeterministic dependency parsing. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
</authors>
<title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1193" citStr="Macherey and Och, 2007" startWordPosition="192" endWordPosition="195">ple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz Josef Och. 2007. An empirical study on computing consensus translations from multiple machine translation systems. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="20914" citStr="Marcus et al., 1993" startWordPosition="3754" endWordPosition="3757">gs and best-indices. cvlm-lbfgs was used with the default hyperparameters from the Makefile. � p [[(a, i, j) E t(Gk, s)]]. k=1 � p µ(a,i,j|Gk,s). k=1 1872 Figure 3: F1 scores of noisy models. Each data point gives the F1 accuracy of a single model on the development set, based on the legend. The xaxis enumerates the models (80 in total for each noise scheme). 6 Experiments In this section, we describe parsing experiments with two languages: English and German. 6.1 Results for English For our English parsing experiments, we use a standard setup. More specifically, we use the Penn WSJ treebank (Marcus et al., 1993) for our experiments, with sections 2–21 as the training data, and section 22 used as the development data. Section 23 is used as the final test set. We binarize the trees in training data, but transform them back before evaluating them. For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. The parser takes part-of-speech tagged sentences as input. We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2–21. We use the F1 measure according to the PARSEVAL metric (Black et</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>M´ario A T Figueiredo</author>
<author>Pedro M Q Aguiar</author>
</authors>
<title>TurboParsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21422" citStr="Martins et al., 2010" startWordPosition="3843" endWordPosition="3846">rsing experiments, we use a standard setup. More specifically, we use the Penn WSJ treebank (Marcus et al., 1993) for our experiments, with sections 2–21 as the training data, and section 22 used as the development data. Section 23 is used as the final test set. We binarize the trees in training data, but transform them back before evaluating them. For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. The parser takes part-of-speech tagged sentences as input. We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2–21. We use the F1 measure according to the PARSEVAL metric (Black et al., 1991) for the evaluation. Preliminary experiments We first experiment with the number of latent states for the clustering algorithm without perturbations. We use k = 100 for the SVD step. Whenever we need to cluster a set of points, we run the k-means algorithm 10 times with random restarts and choose the clustering result with the lowest objective value. On section 22, the clustering algorithm achieves the following results (F1 measure): m = 8: 84.30%, m = 16: 85.98%, m = 24: 86.48%, m = 32: 85.8</context>
</contexts>
<marker>Martins, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, M´ario A. T. Figueiredo, and Pedro M. Q. Aguiar. 2010. TurboParsers: Dependency parsing by approximate variational inference. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2538" citStr="Matsuzaki et al., 2005" startWordPosition="416" endWordPosition="419">techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used toget</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankur Moitra</author>
<author>Gregory Valiant</author>
</authors>
<title>Settling the polynomial learnability of mixtures of gaussians.</title>
<date>2010</date>
<booktitle>In Proceedings of IEEE Symposium on Foundations of Computer Science (FOCS).</booktitle>
<contexts>
<context position="34289" citStr="Moitra and Valiant, 2010" startWordPosition="6024" endWordPosition="6027">, which maximizes a non-convex objective using hard clustering steps. The k-means algorithm can be viewed as “hard EM” for a Gaussian mixture model (GMM), where each latent state is associated with one of the mixture components in the GMM. This means that instead of following up with k-means, we could have identified the parameters and the posteriors for a GMM, where the observations correspond to the vectors that we cluster. There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010). With theoretical guarantees on the correctness of the posteriors from this step, the subsequent use of maximum likelihood estimation step could yield consistent parameter estimates. The consistency guarantees will largely depend on the amount of information that exists in the base feature functions about the latent states according to the L-PCFG model. 8 Conclusion We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering of continuous tree representations, and it also leads to sparse grammar estimates and compact models. We also showed how to</context>
</contexts>
<marker>Moitra, Valiant, 2010</marker>
<rawString>Ankur Moitra and Gregory Valiant. 2010. Settling the polynomial learnability of mixtures of gaussians. In Proceedings of IEEE Symposium on Foundations of Computer Science (FOCS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Jeffrey Lund</author>
<author>Kevin Seppi</author>
<author>Eric Ringger</author>
</authors>
<title>Is your anchor going up or down? Fast and accurate supervised topic models.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2332" citStr="Nguyen et al., 2015" startWordPosition="385" endWordPosition="388">ormation from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single m</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Lund, Seppi, Ringger, 2015</marker>
<rawString>Thang Nguyen, Jordan Boyd-Graber, Jeffrey Lund, Kevin Seppi, and Eric Ringger. 2015. Is your anchor going up or down? Fast and accurate supervised topic models. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="2864" citStr="Petrov and Klein (2007)" startWordPosition="471" endWordPosition="474">2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more a</context>
<context position="30109" citStr="Petrov and Klein (2007)" startWordPosition="5304" endWordPosition="5307">le 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank. Our German experiments follow the same setting as in our English experiments. For the clustering algorithm we generated 80 models, 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the </context>
<context position="33143" citStr="Petrov and Klein (2007)" startWordPosition="5834" endWordPosition="5837">e 3, decoding with the MaxEnt reranking performs the best. Our results continue to improve when further combining the output of previous combination steps. The best result of 82.04% on the development set is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in Table 3). This also leads to the best result of 83.38% on the test set. See Table 4 for results on the test set. Our results are comparable to state-of-the-art results for German parsing. For example, Petrov (2010) reports an accuracy of 84.5% using product of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.1 Fl; and Dubey (2005) reports an accuracy of 76.3 Fl. 7 Discussion From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they yield consistent parameter estimates. Our clustering algorithm for L-PCFG estimation breaks this, but there is a work-around to obtain an algorithm which would be statistically consistent. The main reason that our algorithm is not a consistent estimator is that it relies on k-means clustering, which maximizes a non-convex objective using hard clustering steps. The k-mea</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="2673" citStr="Petrov et al., 2006" startWordPosition="440" endWordPosition="443">nts have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1322" citStr="Petrov, 2010" startWordPosition="214" endWordPosition="215">r English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniq</context>
<context position="16466" citStr="Petrov (2010)" startWordPosition="2973" endWordPosition="2974">he three schemes we described were also used by Wang et al. (2013) to train log-linear models. Wang et al.’s goal was to prevent overfitting by introducing this noise schemes as additional regularizer terms, but without explicitly changing the training data. We do filter the data through these noise schemes, and show in §6 that all of these noise schemes do not improve the performance of our estimation on their own. However, when multiple models are created with these noise schemes, 1871 and then combined together, we get an improved performance. As such, our approach is related to the one of Petrov (2010), who builds a committee of latent-variable PCFGs in order to improve a natural language parser. We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the projections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them. We are required to add</context>
<context position="29843" citStr="Petrov (2010)" startWordPosition="5266" endWordPosition="5267">tes is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,</context>
<context position="33064" citStr="Petrov (2010)" startWordPosition="5822" endWordPosition="5823"> Table 4 respectively. 1875 Like English, in all three blocks in Table 3, decoding with the MaxEnt reranking performs the best. Our results continue to improve when further combining the output of previous combination steps. The best result of 82.04% on the development set is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in Table 3). This also leads to the best result of 83.38% on the test set. See Table 4 for results on the test set. Our results are comparable to state-of-the-art results for German parsing. For example, Petrov (2010) reports an accuracy of 84.5% using product of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.1 Fl; and Dubey (2005) reports an accuracy of 76.3 Fl. 7 Discussion From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they yield consistent parameter estimates. Our clustering algorithm for L-PCFG estimation breaks this, but there is a work-around to obtain an algorithm which would be statistically consistent. The main reason that our algorithm is not a consistent estimator is that it relies on k-means clustering</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pushpendre Rastogi</author>
<author>Benjamin Van Durme</author>
<author>Raman Arora</author>
</authors>
<title>Multiview LSA: Representation learning via generalized CCA.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Rastogi, Van Durme, Arora, 2015</marker>
<rawString>Pushpendre Rastogi, Benjamin Van Durme, and Raman Arora. 2015. Multiview LSA: Representation learning via generalized CCA. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1263" citStr="Sagae and Lavie, 2006" startWordPosition="202" endWordPosition="205"> to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made fo</context>
<context position="29621" citStr="Sagae and Lavie (2006)" startWordPosition="5228" endWordPosition="5231">nt and Add, Mul and Dropout. The number of models used in the first three rows for the clustering algorithm is 80: 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q. The number of latent states is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the Ge</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian symbol-refined tree substitution grammars for syntactic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="29755" citStr="Shindo et al. (2012)" startWordPosition="5250" endWordPosition="5253">, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q. The number of latent states is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 se</context>
</contexts>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP.</booktitle>
<contexts>
<context position="30283" citStr="Skut et al., 1997" startWordPosition="5335" endWordPosition="5338">(2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank. Our German experiments follow the same setting as in our English experiments. For the clustering algorithm we generated 80 models, 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, we generate 20 models, 5 for each Q. For the reranking experiment, we had to modify the BLLIP parser (Charniak and Johnson, 2005) to use the head features</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Do-kyum Kim</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>A spectral algorithm for learning class-based n-gram models of natural language.</title>
<date>2014</date>
<booktitle>Proceedings of UAI.</booktitle>
<contexts>
<context position="2267" citStr="Stratos et al., 2014" startWordPosition="373" endWordPosition="376">ther optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). I</context>
</contexts>
<marker>Stratos, Kim, Collins, Hsu, 2014</marker>
<rawString>Karl Stratos, Do-kyum Kim, Michael Collins, and Daniel Hsu. 2014. A spectral algorithm for learning class-based n-gram models of natural language. Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through the combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>Van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>Hans Van Halteren, Jakub Zavrel, and Walter Daelemans. 2001. Improving accuracy in word class tagging through the combination of machine learning systems. Computational linguistics, 27(2):199–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santosh Vempala</author>
<author>Grant Wang</author>
</authors>
<title>A spectral algorithm for learning mixture models.</title>
<date>2004</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>68</volume>
<issue>4</issue>
<contexts>
<context position="34241" citStr="Vempala and Wang, 2004" startWordPosition="6016" endWordPosition="6019">mator is that it relies on k-means clustering, which maximizes a non-convex objective using hard clustering steps. The k-means algorithm can be viewed as “hard EM” for a Gaussian mixture model (GMM), where each latent state is associated with one of the mixture components in the GMM. This means that instead of following up with k-means, we could have identified the parameters and the posteriors for a GMM, where the observations correspond to the vectors that we cluster. There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010). With theoretical guarantees on the correctness of the posteriors from this step, the subsequent use of maximum likelihood estimation step could yield consistent parameter estimates. The consistency guarantees will largely depend on the amount of information that exists in the base feature functions about the latent states according to the L-PCFG model. 8 Conclusion We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering of continuous tree representations, and it also leads to sparse grammar est</context>
</contexts>
<marker>Vempala, Wang, 2004</marker>
<rawString>Santosh Vempala and Grant Wang. 2004. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Mengqiu Wang</author>
<author>Stefan Wager</author>
<author>Percy Liang</author>
<author>Christopher D Manning</author>
</authors>
<title>Feature noising for log-linear structured prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4150" citStr="Wang et al. (2013)" startWordPosition="675" endWordPosition="678"> spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In §2 we describe notation and background about L-PCFG parsing. In §3 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In §4 we describe the various noise schemes we use with our spectral algorithm and the spect</context>
<context position="15919" citStr="Wang et al. (2013)" startWordPosition="2879" endWordPosition="2882">he first is performed on the feature vectors before the SVD step, and the second is performed after the SVD step. It is not feasible to add Gaussian noise prior to the SVD step, since the matrix Ω&apos; will no longer be sparse, and its SVD computation will be computationally demanding. Our use of dropout noise here is inspired by “dropout” as is used in neural network training, where various connections between units in the neural network are dropped during training in order to avoid overfitting of these units to the data (Srivastava et al., 2014). The three schemes we described were also used by Wang et al. (2013) to train log-linear models. Wang et al.’s goal was to prevent overfitting by introducing this noise schemes as additional regularizer terms, but without explicitly changing the training data. We do filter the data through these noise schemes, and show in §6 that all of these noise schemes do not improve the performance of our estimation on their own. However, when multiple models are created with these noise schemes, 1871 and then combined together, we get an improved performance. As such, our approach is related to the one of Petrov (2010), who builds a committee of latent-variable PCFGs in </context>
</contexts>
<marker>Wang, Wang, Wager, Liang, Manning, 2013</marker>
<rawString>Sida Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. 2013. Feature noising for log-linear structured prediction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1308" citStr="Zhang et al., 2009" startWordPosition="210" endWordPosition="213">language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we de</context>
<context position="29671" citStr="Zhang et al. (2009)" startWordPosition="5237" endWordPosition="5240">d in the first three rows for the clustering algorithm is 80: 20 for each Q E 10.05, 0.1, 0.15, 0.2}. For the spectral algorithm, it is 20, 5 for each Q. The number of latent states is m = 8. For All, we use all models combined from the first three rows. The best figure in each algorithm block is in boldface. ble 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23. Our results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% using parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1. 6.2 Results for German For the German experiments, we used the NEGRA corpus (Skut e</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
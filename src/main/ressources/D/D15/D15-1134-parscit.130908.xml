<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001914">
<title confidence="0.991269">
A model of rapid phonotactic generalization
</title>
<author confidence="0.999549">
Tal Linzen Timothy J. O’Donnell
</author>
<affiliation confidence="0.8266225">
Department of Linguistics Brain and Cognitive Sciences
New York University Massachusetts Institute of Technology
</affiliation>
<email confidence="0.999246">
linzen@nyu.edu timod@mit.edu
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932210526316">
The phonotactics of a language describes
the ways in which the sounds of the
language combine to form possible mor-
phemes and words. Humans can learn
phonotactic patterns at the level of abstract
classes, generalizing across sounds (e.g.,
“words can end in a voiced stop”). More-
over, they rapidly acquire these general-
izations, even before they acquire sound-
specific patterns. We present a probabilis-
tic model intended to capture this early-
abstraction phenomenon. The model rep-
resents both abstract and concrete gen-
eralizations in its hypothesis space from
the outset of learning. This—combined
with a parsimony bias in favor of compact
descriptions of the input data—leads the
model to favor rapid abstraction in a way
similar to human learners.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970866666667">
Natural languages place restrictions on the ways
in which sounds can combine to form words (the
phonotactics of the language). The velar nasal [q],
for example, occurs at the end of English sylla-
bles, as in ring [iIq] or finger [fIq9@i], but never at
the beginning of a syllable: English does not have
words like *ngir [qIi]. English speakers are aware
of this constraint, and judge forms that start with a
[q] as impossible English words.
Sounds that share articulatory and/or perceptual
properties often have similar phonotactic distribu-
tions. German, for example, allows voiced obstru-
ents,1 such as [b] and [9], to occur anywhere in
the word except at its end: [bal] is a valid German
word, but [lab] isn’t.
Speakers use such features of sounds to form
phonotactic generalizations, which can then apply
to sounds that do not appear in their language. Al-
though no English words start with either [si] or
[mb], English speakers judge srip to be a better po-
tential word of English than mbip (Scholes, 1966);
this is likely because [si] shares properties with
strident-liquid clusters that do exist in English,
such as [sl] as in slip and [Si] as in shrewd, whereas
[mb] does not benefit from any sonorant-stop on-
set sequences (*[nt])—none exist in English.
Recent studies have investigated how humans
acquire generalizations over phonological classes
in an artificial language paradigm (Linzen and
Gallagher, 2014; Linzen and Gallagher, 2015).
The central finding of these studies was that
participants rapidly learned abstract phonotactic
constraints and exhibited evidence of generaliza-
tions over classes of sounds before evidence of
phoneme-specific knowledge.
This paper presents a probabilistic genera-
tive model, the Phonotactics As Infinite Mixture
(PAIM) model, which exhibits similar behavior.
This behavior arises from the combination of two
factors: the early availability of abstract phono-
logical classes in the learner’s hypothesis space;
and a parsimony bias implemented as a Dirichlet
process mixture, which favors descriptions of the
data using a single pattern over ones that make ref-
erence to multiple specific patterns.
</bodyText>
<sectionHeader confidence="0.950764" genericHeader="method">
2 Summary of behavioral data
</sectionHeader>
<bodyText confidence="0.9987115">
The experiments are described in detail in Linzen
and Gallagher (2014) and Linzen and Gallagher
(2015); we summarize the main details here.
Design: Participants were exposed to varying
numbers of auditorily-presented words in one of
two artificial languages, VOICING and IDENTITY.
</bodyText>
<footnote confidence="0.982661">
1See Hayes (2011) for an introduction to phonological
features.
</footnote>
<page confidence="0.832225">
1126
</page>
<note confidence="0.863352">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1126–1131,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.995073444444444">
Exposure
Test
Exposure
Test
ganu gimi
balu bini
vimu voni
zalu zili
Dano Damu
CONF- CONF- NONCONF-
ATT UNATT UNATT
zonu dila tumu
zini dimu talu
CONF- CONF-
ATT UNATT
papi keku
SeSu sasi
gugi d3id3e
nanu mamu
pipa
SuSe
gapu
nuni
kesa
mud3e
d3uki
semi
</figure>
<tableCaption confidence="0.981728">
Table 1: VOICING: Design for one of the lists
</tableCaption>
<bodyText confidence="0.985890465116279">
(voiced exposure, [d] held out). The table shows
the complete list of exposure words that a partici-
pant in the two exposure sets group might receive.
Following the exposure phase, they were asked
to judge for a set of novel test words whether
those words could be part of the language they had
learned (the possible answers were “yes” or “no”).
In the VOICING experiment, all exposure words
began with consonants that had the same value for
their voicing feature (all voiced or all voiceless,
e.g., A1 = {g, v, z, D, b} or A2 = {k, f, s, T, p}).
Some of the sounds with the relevant voicing were
held out to be used during testing (e.g., [d] for A1
or [t] for A2). All exposure and test words had the
form CVMV (tumi), where C stands for the onset
consonant, V for a randomized vowel, and M for
[m], [n] or [l] (see Table 1).
Participants judged three types of novel test
words: ones with the same onset as one or more
of the words in exposure (CONF-ATT;2 e.g., zonu
for A1); ones whose onset was not encountered
in exposure but had the same voicing as the ex-
posure onsets (CONF-UNATT; e.g., dila); and ones
whose onset had different voicing from the expo-
sure words (NONCONF-UNATT, e.g., tomu). The
vowels and the second consonant were random-
ized across conditions such that only the onsets
reliably discriminated the three conditions.
In the IDENTITY language, words had the form
C1VC2V. The generalization in this language was
C1 = C2 (e.g., pipa). Here it was probabilistic:
only half of the words in the exposure stage con-
formed to the generalization. As such, there was a
fourth test condition, NONCONF-ATT, of exposure
words that did not conform to the generalization.
Participants were recruited on Amazon Me-
chanical Turk (280 participants in the VOICING
2ATT (attested): the consonants in the word (though not
the full word) were encountered in exposure; UNATT (unat-
tested): consonants were not encountered in exposures; CONF
(conforming): the consonants conform to the abstract pat-
tern (voicing or identity); NONCONF (nonconforming): con-
sonants don’t conform to the abstract pattern.
</bodyText>
<figure confidence="0.794326666666667">
NONCONF- NONCONF-
ATT UNATT
kasi pina
med3a nage
d3uke gaSe
sami Sipu
</figure>
<tableCaption confidence="0.434879">
Table 2: IDENTITY: A complete list of exposure
</tableCaption>
<bodyText confidence="0.999510870967742">
and test words that a participant in the one expo-
sure set group might receive.
experiment and 288 in the IDENTITY experiment).
They were divided into four groups, which re-
ceived 1, 2, 4 or 8 sets of words. In the VOICING
experiment, each of the sets contained five words,
one starting with each of the five CONF-ATT on-
sets; in the IDENTITY experiment, each of the sets
contained eight words, one with each of the CONF-
ATT and NONCONF-ATT consonant pairs (Tables 1
and 2).
Results: Human experimental results are plotted
in Figure 1. Endorsement rates represent the pro-
portion of trials in which participants judged the
word to be well-formed. Participants learned gen-
eralizations involving abstract classes of sounds
after a single exposure set: in the VOICING exper-
iment, they judged voiced word onsets to be bet-
ter than voiceless ones, and in the IDENTITY ex-
periment they judged words with identical conso-
nants as better than words with nonidentical ones.3
Participants did not start distinguishing CONF-ATT
from CONF-UNATT patterns until they received
two or more sets of exposure to the language.
Participants continued to generalize to CONF-
UNATT patterns even after significant exposure to
the language. Endorsement rates were higher than
50% across the board, likely because even words
with NONCONF-UNATT consonant patterns were
similar to the exposure words in all other respects
(e.g., length, syllable structure, number of vowels
</bodyText>
<footnote confidence="0.8228425">
3All differences discussed in this section are statistically
significant.
</footnote>
<page confidence="0.99263">
1127
</page>
<figureCaption confidence="0.996462">
Figure 1: Human behavioral results. Error bars
indicate bootstrapped 95% confidence intervals.
</figureCaption>
<bodyText confidence="0.885298">
and consonants).
</bodyText>
<sectionHeader confidence="0.996529" genericHeader="method">
3 The model
</sectionHeader>
<bodyText confidence="0.99988275">
PAIM is a generative model: it describes the prob-
abilistic process by which the phonological forms
of words are generated. Phonotactic knowledge is
expressed as a set of word-form templates, repre-
sented as sequences of phonological classes. For
example, the template ([+voiced], V, C, V) cap-
tures a generalization over words beginning with a
voiced consonant.
Prior over phonological classes: Phonemes are
represented as phonological feature-value matri-
ces.4 We generate a phonological class for each
position in the template using this feature sys-
tem and a parameter p E [0, 1], which con-
trols the model’s willingness to consider more
or less abstract phonological classes: low val-
ues of p encourage underspecified classes, such
as [] or [+voice], whereas high values of p favor
highly specified classes, such as [+voice, labial,
-continuant]. Given a particular value of p, we de-
fine the distribution G(c) over classes as follows:
</bodyText>
<listItem confidence="0.990896571428571">
• For each phonological feature f whose set of
possible values is Vf:
1. Draw P — Bernoulli(p).
2. If P = 1, draw v — Uniform(Vf) and
include it in c.
3. Otherwise, leave the feature unspeci-
fied, allowing the class to be abstract.
</listItem>
<footnote confidence="0.6692478">
4The particular feature system is treated as a parameter of
the model. In the simulation below we used a simplified ver-
sion of the phonological feature inventory described in Hayes
(2011), which only included features that are distinctive in
English consonants.
</footnote>
<bodyText confidence="0.999771571428571">
Generating words from templates: Given a
choice of phonological template t, we assume that
each of the segments that instantiate t has the same
probability of being sampled (cf. the “size prin-
ciple” of Tenenbaum and Griffiths (2001)). Con-
sider again the class c0 = [+continuant, labial].
Under the assumption that the model’s segment in-
ventory is the English one, there are only two seg-
ments that are labial continuants: [v] and [f]. The
probability of each one of them being generated
from c0 will be P(s|c0) = 1/2.
Prior over template sets: The sounds of the
language can be generated from a variety of tem-
plates at varying levels of abstraction. We there-
fore extend the model to be a mixture of template
distributions of the type described above. The
number of templates is inferred from the data us-
ing a Dirichlet process mixture model (Antoniak,
1974).
This prior can be constructed as a process. Sup-
pose that si is an ordering of the input sounds,
and that we know which templates generated the
first n — 1 sounds s1, ... , sn−1. If K is the num-
ber of templates that have been posited so far and
n1, ... , nK indicate the number of sounds that
have been drawn from each template, then the
probability distribution over the template zn that
the sound sn will be drawn from is given by
</bodyText>
<equation confidence="0.869545">
nk ifk&lt;K
(1)
α otherwise
</equation>
<bodyText confidence="0.999892263157895">
Since the probability that an existing template
generated sn is proportional to the number of seg-
ments currently assigned to that template, this
prior encourages partitions in which a few tem-
plates explain most of the sounds (the “rich get
richer” property), which amounts to a parsimony
bias. Higher values of α can make this bias
weaker.
Modeling phoneme spreading: To simulate the
generalization made by participants in the IDEN-
TITY experiment, templates must be able to state
that two phonemes need to be identical. This is
analogous to mechanisms of “spreading” widely
assumed in phonology (Colavin et al., 2010; Gold-
smith, 1976; McCarthy, 1986). For our simula-
tions below, we simplify by only considering iden-
tity constraints between the initial and medial con-
sonants in exposure and test forms. We sample a
template over these positions as follows:
</bodyText>
<figure confidence="0.997240705882353">
Attested
Unattested
Conforming
Nonconforming
VOICING
IDENTITY
100%
80%
60%
40%
Endorsement rate
1 2 4 8 1 2 4 8
Exposure sets
P(zn = k|z1:n−1) OC
⎧
⎨
⎩
</figure>
<page confidence="0.953862">
1128
</page>
<listItem confidence="0.959159428571429">
1. Draw a class c1 — G, where G is the distribu-
tion over phonological classes defined above.
2. Draw Q — Bernoulli(q).
3. If Q = 1, return an identity template, i.e.,
(s, s) such that s E c1.
4. Otherwise, draw c2 — G and return the tem-
plate (s1, s2) such that s1 E c1 and s2 E c2.
</listItem>
<bodyText confidence="0.9978162">
Inference: We perform inference to find the
posterior over template sets given the exposure
datasets used in the human experiments described
above. We also infer the hyperparameters using
the following prior distributions:
</bodyText>
<equation confidence="0.999450666666667">
p — Beta(1,1)
α — Gamma(2, 4) (2)
q — Beta(1,1)
</equation>
<bodyText confidence="0.999982161290323">
Inference for the Dirichlet process mixture was
performed using the Gibbs sampler described in
Neal (2000). After each Gibbs sweep, slice sam-
pling (Neal, 2003) was used to obtain a new value
for p and q. A new value for α was sampled using
the method described by Escobar and West (1995).
We ran the sampler for 3000 iterations, discarded
the first 100 samples and kept every fifth sample of
the remaining samples, for a total of 580 samples
from the posterior distribution.
Predicting human data: Participants in the
behavioral experiments gave binary judgments
(“could the word be part of the language?”) rather
than probability estimates. To link our model’s
predictions to participants’ binary responses, we
sample m template instantiations from the poste-
rior predictive distribution.5 If the relevant part
of the test word appeared in these m samples, the
model responds “yes”; this can be understood to
be related to a sampling-based view of human in-
ference (Vul et al., 2014). In the simulations below
we fix m to be 10.
Human endorsement rates were consistently
above 50%, while the model’s ratings were of-
ten close to 0%. This is likely to be because hu-
man ratings were also informed by the unmodeled
(fixed) parts of the templates, such as word length
or number of vowels. We therefore linearly trans-
form the model’s ratings to the range exhibited by
human participants: if the untransformed rate is r,
the ultimate simulated rate will be (1 + r)/2.
</bodyText>
<footnote confidence="0.972273">
5Template instantiations only include the modeled (speci-
fied) part of the template: an onset consonant in our model of
the VOICING language or a consonant pair for the IDENTITY
language.
</footnote>
<figureCaption confidence="0.994978">
Figure 2: PAIM: Simulated endorsement rates.
</figureCaption>
<sectionHeader confidence="0.997327" genericHeader="method">
4 Simulations
</sectionHeader>
<bodyText confidence="0.999927714285714">
We only modeled those aspects of phonotactic
templates that are relevant to the experimental
results. For the VOICING experiment, we con-
strained the template to be ( , V, C, V) (inference
is only performed on the first consonant); for the
IDENTITY experiment, we constrained it to be ( ,
V, , V).
Figure 2 shows the simulated endorsement
rates. After a single exposure to each pattern (one
exposure set), PAIM behaved in a qualitatively
similar way to participants in both experiments:
it distinguished CONF from NONCONF words, but
did not distinguish ATT from UNATT words.
PAIM was less willing than humans to general-
ize to CONF-UNATT items after multiple exposure
sets: in the IDENTITY experiment the generaliza-
tion had no effect by the eighth exposure set; in the
VOICING experiment its effect was weaker after
eight than after four exposures sets. By contrast,
human generalization in both languages showed
no sign of weakening after multiple exposure sets.
</bodyText>
<sectionHeader confidence="0.923438" genericHeader="method">
5 Comparison to related models
</sectionHeader>
<bodyText confidence="0.999879571428571">
Hayes and Wilson (2008) propose a Maximum
Entropy model of phonotactics (MaxEnt; see also
Goldwater and Johnson (2003)). Like PAIM,
MaxEnt is based on phonological classes defined
as feature matrices. Each class c is assigned a
weight wc. The predicted probability in MaxEnt
of a sound s is
</bodyText>
<equation confidence="0.980046333333333">
�
1 w`I`(s)
ˆp(s) = Z e ` (3)
</equation>
<bodyText confidence="0.861665">
where Z = �s ˆp(s) and Ic(s) = 1 if s E c and
0 otherwise.
</bodyText>
<figure confidence="0.998476307692308">
Attested
Unattested
Conforming
Nonconforming
VOICING
IDENTITY
100%
80%
60%
40%
Simulated rate
1 2 4 8 1 2 4 8
Exposure sets
</figure>
<page confidence="0.995896">
1129
</page>
<bodyText confidence="0.9999624">
We simulated endorsement rates from a Max-
Ent model for the VOICING language. Following
Hayes and Wilson (2008), we used l2 regulariza-
tion; that is, if the exposure words were s1,... , sn,
the objective function was
</bodyText>
<equation confidence="0.868658">
(wc − µ)2 (4)
2σ2
</equation>
<bodyText confidence="0.999343609756098">
Figure 3 shows the simulated endorsement rates
for different values of σ (we set µ = 0 in all
simulations). For σ = 0.05, the model showed
little learning after a single exposure set. When
σ was set to higher values, MaxEnt rapidly pre-
ferred attested to unattested items, failing to re-
produce the human early generalization pattern.
Like PAIM, but unlike humans, generalization to
CONF-UNATT items diminished after multiple ex-
posure sets (in particular for σ = 0.5). A straight-
forward implementation of MaxEnt is therefore
incapable of simulating the human results; better
results could potentially be achieved with a regu-
larization method that encouraged sparsity (Good-
man, 2004; Johnson et al., 2015).
Another proposed model of phonotactics is
the Minimal Generalization Learner, or MGL
(Albright, 2009); Linzen and Gallagher (2014)
showed that MGL can simulate relevant human
behavioral data in some circumstances. In contrast
with PAIM and MaxEnt, which converge to the
empirical distribution given sufficient data, MGL
reserves a fixed amount of probability mass to un-
seen events. It would therefore able to simulate a
sustained generalization pattern.
Our prior over phonological classes bears some
resemblance to the Rational Rules model of vi-
sual categorization (Goodman et al., 2008). In that
model, classes are generated from a probabilis-
tic context free grammar (PCFG); highly specified
rules are therefore implicitly less probable, as in
our model. Relatedly, Hayes and Wilson (2008)
use a greedy feature selection procedure that starts
from simpler phonological classes and gradually
adds more complex ones; this procedure also en-
codes an implicit bias in favor of simple classes.
Finally, our implementation of a parsimony bias
using a Dirichlet process is related to similar bi-
ases incorporated into other models of language
learning (Frank and Tenenbaum, 2011; Johnson et
al., 2007; O’Donnell, 2015).
</bodyText>
<figure confidence="0.9666132">
σ = 0.05 σ = 0.3 σ = 0.5
1 2 4 8 1 2 4 8 1 2 4 8
Exposure sets
Conforming
Nonconforming
</figure>
<figureCaption confidence="0.954065333333333">
Figure 3: Maximum entropy model: Simulated
endorsement rates for the VOICING language, with
different values of the regularization parameter σ.
</figureCaption>
<sectionHeader confidence="0.999706" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999998272727273">
We have presented a probabilistic model of phono-
tactic generalization that captures the pattern of
rapid, abstract generalization characteristic of hu-
man learners. The model’s performance is driven
by two crucial assumptions. First, it allows hy-
potheses that make reference to abstract, broad
classes of phones from the beginning of the learn-
ing process. Second, it prefers to learn compact
or parsimonious explanations of the input corpus,
using a small number of phonotactic templates.
This second property is enforced by our use of the
Dirichlet process as a prior over template sets.
These two properties interact. When the model
has seen only a few data items, the availability
of abstract generalizations allows it to explain all
items using a single template, and the prior bias
towards parsimony drives it to do so. As the num-
ber of data items increases, repeated instances of
specific phonemes no longer seem like acciden-
tal observations from a more general template, but
rather like significant templates in their own right;
the model begins to capture such item-specificity.
The model stopped generalizing earlier than hu-
mans did; we intend to explore ways to explain
this discrepancy. Additional human data would
need to be collected to determine whether humans
keep generalizing indefinitely, or eventually con-
verge on the attested sounds. Finally, to facilitate
inference, we only tested our model on the parts
of the word that were relevant to the human data.
In future work, we intend to extend the model to
learn larger templates that include syllable struc-
ture and phonological tiers (Goldsmith, 1976).
</bodyText>
<figure confidence="0.9383472">
n
log ˆp(sz) − �
z=1 c
Simulated rate 100%
80%
60%
40%
Attested
Unattested
1130
Tal Linzen and Gillian Gallagher. 2015.
Rapid generalization in phonotactic learning.
http://tallinzen.net/media/papers/
linzen_gallagher_2015.pdf.
References
</figure>
<bodyText confidence="0.620156666666667">
Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9–41.
</bodyText>
<reference confidence="0.999510794117647">
Charles E. Antoniak. 1974. Mixtures of Dirichlet pro-
cesses with applications to Bayesian nonparametric
problems. The Annals of Statistics, pages 1152–
1174.
Rebecca S. Colavin, Roger Levy, and Sharon Rose.
2010. Modeling OCP-Place in Amharic with the
Maximum Entropy phonotactic learner. In Proceed-
ings of the 46th meeting of the Chicago Linguistics
Society.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577–588.
Michael C. Frank and Joshua B. Tenenbaum. 2011.
Three ideal observer models for rule learning in sim-
ple languages. Cognition, 120(3):360–371.
John A. Goldsmith. 1976. Autosegmental Phonology.
Ph.D. thesis, MIT.
Sharon Goldwater and Mark Johnson. 2003. Learning
OT constraint rankings using a maximum entropy
model. In Proceedings of the Stockholm workshop
on variation within Optimality Theory, pages 111–
120.
Noah D. Goodman, Joshua B. Tenenbaum, Jacob Feld-
man, and Thomas L. Griffiths. 2008. A rational
analysis of rule-based concept learning. Cognitive
Science, 32(1):108–154.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In HLT-NAACL, pages 305–
312.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39(3):379–440.
Bruce Hayes. 2011. Introductory phonology. Wiley-
Blackwell, Malden, MA and Oxford.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In Advances in Neural Information Pro-
cessing Systems 19, Cambridge, MA. MIT Press.
Mark Johnson, Joe Pater, Robert Staubs, and Em-
manuel Dupoux. 2015. Sign constraints on feature
weights improve a joint model of word segmentation
and phonology. In HLT-NAACL, pages 303–313.
Tal Linzen and Gillian Gallagher. 2014. The time-
course of generalization in phonotactic learning. In
Proceedings of Phonology 2013.
John J. McCarthy. 1986. OCP effects: Gemination and
antigemination. Linguistic Inquiry, 17(2):207–263.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249–
265.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705–767.
Timothy J. O’Donnell. 2015. Productivity and reuse
in language: A theory of linguistic computation
and storage. The MIT Press, Cambridge, Mas-
sachusetts.
Robert J. Scholes. 1966. Phonotactic grammaticality.
Mouton, The Hague.
Joshua B. Tenenbaum and Thomas L. Griffiths. 2001.
Generalization, similarity, and Bayesian inference.
Behavioral and Brain Sciences, 24(4):629–640.
Edward Vul, Noah Goodman, Thomas L. Griffiths, and
Joshua B. Tenenbaum. 2014. One and done? Op-
timal decisions from very few samples. Cognitive
Science, 38(4):599–637.
</reference>
<page confidence="0.994591">
1131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959531">
<title confidence="0.999697">A model of rapid phonotactic generalization</title>
<author confidence="0.999991">Tal Linzen Timothy J O’Donnell</author>
<affiliation confidence="0.990801">Department of Linguistics Brain and Cognitive Sciences New York University Massachusetts Institute of Technology</affiliation>
<email confidence="0.999453">linzen@nyu.edutimod@mit.edu</email>
<abstract confidence="0.9987271">The phonotactics of a language describes the ways in which the sounds of the language combine to form possible morphemes and words. Humans can learn phonotactic patterns at the level of abstract classes, generalizing across sounds (e.g., “words can end in a voiced stop”). Moreover, they rapidly acquire these generalizations, even before they acquire soundspecific patterns. We present a probabilistic model intended to capture this earlyabstraction phenomenon. The model represents both abstract and concrete generalizations in its hypothesis space from the outset of learning. This—combined with a parsimony bias in favor of compact descriptions of the input data—leads the model to favor rapid abstraction in a way similar to human learners.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles E Antoniak</author>
</authors>
<title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics,</title>
<date>1974</date>
<pages>1152--1174</pages>
<contexts>
<context position="10124" citStr="Antoniak, 1974" startWordPosition="1639" endWordPosition="1640">er again the class c0 = [+continuant, labial]. Under the assumption that the model’s segment inventory is the English one, there are only two segments that are labial continuants: [v] and [f]. The probability of each one of them being generated from c0 will be P(s|c0) = 1/2. Prior over template sets: The sounds of the language can be generated from a variety of templates at varying levels of abstraction. We therefore extend the model to be a mixture of template distributions of the type described above. The number of templates is inferred from the data using a Dirichlet process mixture model (Antoniak, 1974). This prior can be constructed as a process. Suppose that si is an ordering of the input sounds, and that we know which templates generated the first n — 1 sounds s1, ... , sn−1. If K is the number of templates that have been posited so far and n1, ... , nK indicate the number of sounds that have been drawn from each template, then the probability distribution over the template zn that the sound sn will be drawn from is given by nk ifk&lt;K (1) α otherwise Since the probability that an existing template generated sn is proportional to the number of segments currently assigned to that template, t</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>Charles E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, pages 1152– 1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca S Colavin</author>
<author>Roger Levy</author>
<author>Sharon Rose</author>
</authors>
<title>Modeling OCP-Place in Amharic with the Maximum Entropy phonotactic learner.</title>
<date>2010</date>
<booktitle>In Proceedings of the 46th meeting of the Chicago Linguistics Society.</booktitle>
<contexts>
<context position="11199" citStr="Colavin et al., 2010" startWordPosition="1828" endWordPosition="1831">ise Since the probability that an existing template generated sn is proportional to the number of segments currently assigned to that template, this prior encourages partitions in which a few templates explain most of the sounds (the “rich get richer” property), which amounts to a parsimony bias. Higher values of α can make this bias weaker. Modeling phoneme spreading: To simulate the generalization made by participants in the IDENTITY experiment, templates must be able to state that two phonemes need to be identical. This is analogous to mechanisms of “spreading” widely assumed in phonology (Colavin et al., 2010; Goldsmith, 1976; McCarthy, 1986). For our simulations below, we simplify by only considering identity constraints between the initial and medial consonants in exposure and test forms. We sample a template over these positions as follows: Attested Unattested Conforming Nonconforming VOICING IDENTITY 100% 80% 60% 40% Endorsement rate 1 2 4 8 1 2 4 8 Exposure sets P(zn = k|z1:n−1) OC ⎧ ⎨ ⎩ 1128 1. Draw a class c1 — G, where G is the distribution over phonological classes defined above. 2. Draw Q — Bernoulli(q). 3. If Q = 1, return an identity template, i.e., (s, s) such that s E c1. 4. Otherwis</context>
</contexts>
<marker>Colavin, Levy, Rose, 2010</marker>
<rawString>Rebecca S. Colavin, Roger Levy, and Sharon Rose. 2010. Modeling OCP-Place in Amharic with the Maximum Entropy phonotactic learner. In Proceedings of the 46th meeting of the Chicago Linguistics Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Escobar</author>
<author>Mike West</author>
</authors>
<title>Bayesian density estimation and inference using mixtures.</title>
<date>1995</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>90</volume>
<issue>430</issue>
<contexts>
<context position="12436" citStr="Escobar and West (1995)" startWordPosition="2054" endWordPosition="2057"> and return the template (s1, s2) such that s1 E c1 and s2 E c2. Inference: We perform inference to find the posterior over template sets given the exposure datasets used in the human experiments described above. We also infer the hyperparameters using the following prior distributions: p — Beta(1,1) α — Gamma(2, 4) (2) q — Beta(1,1) Inference for the Dirichlet process mixture was performed using the Gibbs sampler described in Neal (2000). After each Gibbs sweep, slice sampling (Neal, 2003) was used to obtain a new value for p and q. A new value for α was sampled using the method described by Escobar and West (1995). We ran the sampler for 3000 iterations, discarded the first 100 samples and kept every fifth sample of the remaining samples, for a total of 580 samples from the posterior distribution. Predicting human data: Participants in the behavioral experiments gave binary judgments (“could the word be part of the language?”) rather than probability estimates. To link our model’s predictions to participants’ binary responses, we sample m template instantiations from the posterior predictive distribution.5 If the relevant part of the test word appeared in these m samples, the model responds “yes”; this</context>
</contexts>
<marker>Escobar, West, 1995</marker>
<rawString>Michael D. Escobar and Mike West. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90(430):577–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Three ideal observer models for rule learning in simple languages.</title>
<date>2011</date>
<journal>Cognition,</journal>
<volume>120</volume>
<issue>3</issue>
<contexts>
<context position="17465" citStr="Frank and Tenenbaum, 2011" startWordPosition="2880" endWordPosition="2883">sual categorization (Goodman et al., 2008). In that model, classes are generated from a probabilistic context free grammar (PCFG); highly specified rules are therefore implicitly less probable, as in our model. Relatedly, Hayes and Wilson (2008) use a greedy feature selection procedure that starts from simpler phonological classes and gradually adds more complex ones; this procedure also encodes an implicit bias in favor of simple classes. Finally, our implementation of a parsimony bias using a Dirichlet process is related to similar biases incorporated into other models of language learning (Frank and Tenenbaum, 2011; Johnson et al., 2007; O’Donnell, 2015). σ = 0.05 σ = 0.3 σ = 0.5 1 2 4 8 1 2 4 8 1 2 4 8 Exposure sets Conforming Nonconforming Figure 3: Maximum entropy model: Simulated endorsement rates for the VOICING language, with different values of the regularization parameter σ. 6 Discussion We have presented a probabilistic model of phonotactic generalization that captures the pattern of rapid, abstract generalization characteristic of human learners. The model’s performance is driven by two crucial assumptions. First, it allows hypotheses that make reference to abstract, broad classes of phones fr</context>
</contexts>
<marker>Frank, Tenenbaum, 2011</marker>
<rawString>Michael C. Frank and Joshua B. Tenenbaum. 2011. Three ideal observer models for rule learning in simple languages. Cognition, 120(3):360–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Autosegmental Phonology.</title>
<date>1976</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="11216" citStr="Goldsmith, 1976" startWordPosition="1832" endWordPosition="1834">ity that an existing template generated sn is proportional to the number of segments currently assigned to that template, this prior encourages partitions in which a few templates explain most of the sounds (the “rich get richer” property), which amounts to a parsimony bias. Higher values of α can make this bias weaker. Modeling phoneme spreading: To simulate the generalization made by participants in the IDENTITY experiment, templates must be able to state that two phonemes need to be identical. This is analogous to mechanisms of “spreading” widely assumed in phonology (Colavin et al., 2010; Goldsmith, 1976; McCarthy, 1986). For our simulations below, we simplify by only considering identity constraints between the initial and medial consonants in exposure and test forms. We sample a template over these positions as follows: Attested Unattested Conforming Nonconforming VOICING IDENTITY 100% 80% 60% 40% Endorsement rate 1 2 4 8 1 2 4 8 Exposure sets P(zn = k|z1:n−1) OC ⎧ ⎨ ⎩ 1128 1. Draw a class c1 — G, where G is the distribution over phonological classes defined above. 2. Draw Q — Bernoulli(q). 3. If Q = 1, return an identity template, i.e., (s, s) such that s E c1. 4. Otherwise, draw c2 — G an</context>
</contexts>
<marker>Goldsmith, 1976</marker>
<rawString>John A. Goldsmith. 1976. Autosegmental Phonology. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Learning OT constraint rankings using a maximum entropy model.</title>
<date>2003</date>
<booktitle>In Proceedings of the Stockholm workshop on variation within Optimality Theory,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="14966" citStr="Goldwater and Johnson (2003)" startWordPosition="2468" endWordPosition="2471">s: it distinguished CONF from NONCONF words, but did not distinguish ATT from UNATT words. PAIM was less willing than humans to generalize to CONF-UNATT items after multiple exposure sets: in the IDENTITY experiment the generalization had no effect by the eighth exposure set; in the VOICING experiment its effect was weaker after eight than after four exposures sets. By contrast, human generalization in both languages showed no sign of weakening after multiple exposure sets. 5 Comparison to related models Hayes and Wilson (2008) propose a Maximum Entropy model of phonotactics (MaxEnt; see also Goldwater and Johnson (2003)). Like PAIM, MaxEnt is based on phonological classes defined as feature matrices. Each class c is assigned a weight wc. The predicted probability in MaxEnt of a sound s is � 1 w`I`(s) ˆp(s) = Z e ` (3) where Z = �s ˆp(s) and Ic(s) = 1 if s E c and 0 otherwise. Attested Unattested Conforming Nonconforming VOICING IDENTITY 100% 80% 60% 40% Simulated rate 1 2 4 8 1 2 4 8 Exposure sets 1129 We simulated endorsement rates from a MaxEnt model for the VOICING language. Following Hayes and Wilson (2008), we used l2 regularization; that is, if the exposure words were s1,... , sn, the objective functio</context>
</contexts>
<marker>Goldwater, Johnson, 2003</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2003. Learning OT constraint rankings using a maximum entropy model. In Proceedings of the Stockholm workshop on variation within Optimality Theory, pages 111– 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah D Goodman</author>
<author>Joshua B Tenenbaum</author>
<author>Jacob Feldman</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A rational analysis of rule-based concept learning.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="16882" citStr="Goodman et al., 2008" startWordPosition="2790" endWordPosition="2793">an, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman et al., 2008). In that model, classes are generated from a probabilistic context free grammar (PCFG); highly specified rules are therefore implicitly less probable, as in our model. Relatedly, Hayes and Wilson (2008) use a greedy feature selection procedure that starts from simpler phonological classes and gradually adds more complex ones; this procedure also encodes an implicit bias in favor of simple classes. Finally, our implementation of a parsimony bias using a Dirichlet process is related to similar biases incorporated into other models of language learning (Frank and Tenenbaum, 2011; Johnson et al.,</context>
</contexts>
<marker>Goodman, Tenenbaum, Feldman, Griffiths, 2008</marker>
<rawString>Noah D. Goodman, Joshua B. Tenenbaum, Jacob Feldman, and Thomas L. Griffiths. 2008. A rational analysis of rule-based concept learning. Cognitive Science, 32(1):108–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>305--312</pages>
<contexts>
<context position="16269" citStr="Goodman, 2004" startWordPosition="2699" endWordPosition="2701">s of σ (we set µ = 0 in all simulations). For σ = 0.05, the model showed little learning after a single exposure set. When σ was set to higher values, MaxEnt rapidly preferred attested to unattested items, failing to reproduce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF-UNATT items diminished after multiple exposure sets (in particular for σ = 0.5). A straightforward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regularization method that encouraged sparsity (Goodman, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman </context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In HLT-NAACL, pages 305– 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning.</title>
<date>2008</date>
<journal>Linguistic Inquiry,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="14871" citStr="Hayes and Wilson (2008)" startWordPosition="2454" endWordPosition="2457">osure set), PAIM behaved in a qualitatively similar way to participants in both experiments: it distinguished CONF from NONCONF words, but did not distinguish ATT from UNATT words. PAIM was less willing than humans to generalize to CONF-UNATT items after multiple exposure sets: in the IDENTITY experiment the generalization had no effect by the eighth exposure set; in the VOICING experiment its effect was weaker after eight than after four exposures sets. By contrast, human generalization in both languages showed no sign of weakening after multiple exposure sets. 5 Comparison to related models Hayes and Wilson (2008) propose a Maximum Entropy model of phonotactics (MaxEnt; see also Goldwater and Johnson (2003)). Like PAIM, MaxEnt is based on phonological classes defined as feature matrices. Each class c is assigned a weight wc. The predicted probability in MaxEnt of a sound s is � 1 w`I`(s) ˆp(s) = Z e ` (3) where Z = �s ˆp(s) and Ic(s) = 1 if s E c and 0 otherwise. Attested Unattested Conforming Nonconforming VOICING IDENTITY 100% 80% 60% 40% Simulated rate 1 2 4 8 1 2 4 8 Exposure sets 1129 We simulated endorsement rates from a MaxEnt model for the VOICING language. Following Hayes and Wilson (2008), we</context>
<context position="17085" citStr="Hayes and Wilson (2008)" startWordPosition="2821" endWordPosition="2824"> human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman et al., 2008). In that model, classes are generated from a probabilistic context free grammar (PCFG); highly specified rules are therefore implicitly less probable, as in our model. Relatedly, Hayes and Wilson (2008) use a greedy feature selection procedure that starts from simpler phonological classes and gradually adds more complex ones; this procedure also encodes an implicit bias in favor of simple classes. Finally, our implementation of a parsimony bias using a Dirichlet process is related to similar biases incorporated into other models of language learning (Frank and Tenenbaum, 2011; Johnson et al., 2007; O’Donnell, 2015). σ = 0.05 σ = 0.3 σ = 0.5 1 2 4 8 1 2 4 8 1 2 4 8 Exposure sets Conforming Nonconforming Figure 3: Maximum entropy model: Simulated endorsement rates for the VOICING language, wit</context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39(3):379–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
</authors>
<title>Introductory phonology.</title>
<date>2011</date>
<location>WileyBlackwell, Malden, MA and Oxford.</location>
<contexts>
<context position="3453" citStr="Hayes (2011)" startWordPosition="532" endWordPosition="533">ctors: the early availability of abstract phonological classes in the learner’s hypothesis space; and a parsimony bias implemented as a Dirichlet process mixture, which favors descriptions of the data using a single pattern over ones that make reference to multiple specific patterns. 2 Summary of behavioral data The experiments are described in detail in Linzen and Gallagher (2014) and Linzen and Gallagher (2015); we summarize the main details here. Design: Participants were exposed to varying numbers of auditorily-presented words in one of two artificial languages, VOICING and IDENTITY. 1See Hayes (2011) for an introduction to phonological features. 1126 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1126–1131, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Exposure Test Exposure Test ganu gimi balu bini vimu voni zalu zili Dano Damu CONF- CONF- NONCONFATT UNATT UNATT zonu dila tumu zini dimu talu CONF- CONFATT UNATT papi keku SeSu sasi gugi d3id3e nanu mamu pipa SuSe gapu nuni kesa mud3e d3uki semi Table 1: VOICING: Design for one of the lists (voiced exposure, [d] held out). The table shows the complet</context>
<context position="9193" citStr="Hayes (2011)" startWordPosition="1481" endWordPosition="1482">[+voice], whereas high values of p favor highly specified classes, such as [+voice, labial, -continuant]. Given a particular value of p, we define the distribution G(c) over classes as follows: • For each phonological feature f whose set of possible values is Vf: 1. Draw P — Bernoulli(p). 2. If P = 1, draw v — Uniform(Vf) and include it in c. 3. Otherwise, leave the feature unspecified, allowing the class to be abstract. 4The particular feature system is treated as a parameter of the model. In the simulation below we used a simplified version of the phonological feature inventory described in Hayes (2011), which only included features that are distinctive in English consonants. Generating words from templates: Given a choice of phonological template t, we assume that each of the segments that instantiate t has the same probability of being sampled (cf. the “size principle” of Tenenbaum and Griffiths (2001)). Consider again the class c0 = [+continuant, labial]. Under the assumption that the model’s segment inventory is the English one, there are only two segments that are labial continuants: [v] and [f]. The probability of each one of them being generated from c0 will be P(s|c0) = 1/2. Prior ov</context>
</contexts>
<marker>Hayes, 2011</marker>
<rawString>Bruce Hayes. 2011. Introductory phonology. WileyBlackwell, Malden, MA and Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="17487" citStr="Johnson et al., 2007" startWordPosition="2884" endWordPosition="2887">n et al., 2008). In that model, classes are generated from a probabilistic context free grammar (PCFG); highly specified rules are therefore implicitly less probable, as in our model. Relatedly, Hayes and Wilson (2008) use a greedy feature selection procedure that starts from simpler phonological classes and gradually adds more complex ones; this procedure also encodes an implicit bias in favor of simple classes. Finally, our implementation of a parsimony bias using a Dirichlet process is related to similar biases incorporated into other models of language learning (Frank and Tenenbaum, 2011; Johnson et al., 2007; O’Donnell, 2015). σ = 0.05 σ = 0.3 σ = 0.5 1 2 4 8 1 2 4 8 1 2 4 8 Exposure sets Conforming Nonconforming Figure 3: Maximum entropy model: Simulated endorsement rates for the VOICING language, with different values of the regularization parameter σ. 6 Discussion We have presented a probabilistic model of phonotactic generalization that captures the pattern of rapid, abstract generalization characteristic of human learners. The model’s performance is driven by two crucial assumptions. First, it allows hypotheses that make reference to abstract, broad classes of phones from the beginning of th</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In Advances in Neural Information Processing Systems 19, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Joe Pater</author>
<author>Robert Staubs</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Sign constraints on feature weights improve a joint model of word segmentation and phonology.</title>
<date>2015</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>303--313</pages>
<contexts>
<context position="16292" citStr="Johnson et al., 2015" startWordPosition="2702" endWordPosition="2705">µ = 0 in all simulations). For σ = 0.05, the model showed little learning after a single exposure set. When σ was set to higher values, MaxEnt rapidly preferred attested to unattested items, failing to reproduce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF-UNATT items diminished after multiple exposure sets (in particular for σ = 0.5). A straightforward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regularization method that encouraged sparsity (Goodman, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman et al., 2008). In that </context>
</contexts>
<marker>Johnson, Pater, Staubs, Dupoux, 2015</marker>
<rawString>Mark Johnson, Joe Pater, Robert Staubs, and Emmanuel Dupoux. 2015. Sign constraints on feature weights improve a joint model of word segmentation and phonology. In HLT-NAACL, pages 303–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tal Linzen</author>
<author>Gillian Gallagher</author>
</authors>
<title>The timecourse of generalization in phonotactic learning.</title>
<date>2014</date>
<booktitle>In Proceedings of Phonology</booktitle>
<contexts>
<context position="2401" citStr="Linzen and Gallagher, 2014" startWordPosition="377" endWordPosition="380">ich can then apply to sounds that do not appear in their language. Although no English words start with either [si] or [mb], English speakers judge srip to be a better potential word of English than mbip (Scholes, 1966); this is likely because [si] shares properties with strident-liquid clusters that do exist in English, such as [sl] as in slip and [Si] as in shrewd, whereas [mb] does not benefit from any sonorant-stop onset sequences (*[nt])—none exist in English. Recent studies have investigated how humans acquire generalizations over phonological classes in an artificial language paradigm (Linzen and Gallagher, 2014; Linzen and Gallagher, 2015). The central finding of these studies was that participants rapidly learned abstract phonotactic constraints and exhibited evidence of generalizations over classes of sounds before evidence of phoneme-specific knowledge. This paper presents a probabilistic generative model, the Phonotactics As Infinite Mixture (PAIM) model, which exhibits similar behavior. This behavior arises from the combination of two factors: the early availability of abstract phonological classes in the learner’s hypothesis space; and a parsimony bias implemented as a Dirichlet process mixtur</context>
<context position="16424" citStr="Linzen and Gallagher (2014)" startWordPosition="2720" endWordPosition="2723">r values, MaxEnt rapidly preferred attested to unattested items, failing to reproduce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF-UNATT items diminished after multiple exposure sets (in particular for σ = 0.5). A straightforward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regularization method that encouraged sparsity (Goodman, 2004; Johnson et al., 2015). Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); Linzen and Gallagher (2014) showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to unseen events. It would therefore able to simulate a sustained generalization pattern. Our prior over phonological classes bears some resemblance to the Rational Rules model of visual categorization (Goodman et al., 2008). In that model, classes are generated from a probabilistic context free grammar (PCFG); highly specified rules are therefore implicitly less </context>
</contexts>
<marker>Linzen, Gallagher, 2014</marker>
<rawString>Tal Linzen and Gillian Gallagher. 2014. The timecourse of generalization in phonotactic learning. In Proceedings of Phonology 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J McCarthy</author>
</authors>
<title>OCP effects: Gemination and antigemination.</title>
<date>1986</date>
<journal>Linguistic Inquiry,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="11233" citStr="McCarthy, 1986" startWordPosition="1835" endWordPosition="1836">ing template generated sn is proportional to the number of segments currently assigned to that template, this prior encourages partitions in which a few templates explain most of the sounds (the “rich get richer” property), which amounts to a parsimony bias. Higher values of α can make this bias weaker. Modeling phoneme spreading: To simulate the generalization made by participants in the IDENTITY experiment, templates must be able to state that two phonemes need to be identical. This is analogous to mechanisms of “spreading” widely assumed in phonology (Colavin et al., 2010; Goldsmith, 1976; McCarthy, 1986). For our simulations below, we simplify by only considering identity constraints between the initial and medial consonants in exposure and test forms. We sample a template over these positions as follows: Attested Unattested Conforming Nonconforming VOICING IDENTITY 100% 80% 60% 40% Endorsement rate 1 2 4 8 1 2 4 8 Exposure sets P(zn = k|z1:n−1) OC ⎧ ⎨ ⎩ 1128 1. Draw a class c1 — G, where G is the distribution over phonological classes defined above. 2. Draw Q — Bernoulli(q). 3. If Q = 1, return an identity template, i.e., (s, s) such that s E c1. 4. Otherwise, draw c2 — G and return the temp</context>
</contexts>
<marker>McCarthy, 1986</marker>
<rawString>John J. McCarthy. 1986. OCP effects: Gemination and antigemination. Linguistic Inquiry, 17(2):207–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Markov chain sampling methods for Dirichlet process mixture models.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>265</pages>
<contexts>
<context position="12255" citStr="Neal (2000)" startWordPosition="2020" endWordPosition="2021">ion over phonological classes defined above. 2. Draw Q — Bernoulli(q). 3. If Q = 1, return an identity template, i.e., (s, s) such that s E c1. 4. Otherwise, draw c2 — G and return the template (s1, s2) such that s1 E c1 and s2 E c2. Inference: We perform inference to find the posterior over template sets given the exposure datasets used in the human experiments described above. We also infer the hyperparameters using the following prior distributions: p — Beta(1,1) α — Gamma(2, 4) (2) q — Beta(1,1) Inference for the Dirichlet process mixture was performed using the Gibbs sampler described in Neal (2000). After each Gibbs sweep, slice sampling (Neal, 2003) was used to obtain a new value for p and q. A new value for α was sampled using the method described by Escobar and West (1995). We ran the sampler for 3000 iterations, discarded the first 100 samples and kept every fifth sample of the remaining samples, for a total of 580 samples from the posterior distribution. Predicting human data: Participants in the behavioral experiments gave binary judgments (“could the word be part of the language?”) rather than probability estimates. To link our model’s predictions to participants’ binary response</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford M. Neal. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249– 265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="12308" citStr="Neal, 2003" startWordPosition="2029" endWordPosition="2030">Q — Bernoulli(q). 3. If Q = 1, return an identity template, i.e., (s, s) such that s E c1. 4. Otherwise, draw c2 — G and return the template (s1, s2) such that s1 E c1 and s2 E c2. Inference: We perform inference to find the posterior over template sets given the exposure datasets used in the human experiments described above. We also infer the hyperparameters using the following prior distributions: p — Beta(1,1) α — Gamma(2, 4) (2) q — Beta(1,1) Inference for the Dirichlet process mixture was performed using the Gibbs sampler described in Neal (2000). After each Gibbs sweep, slice sampling (Neal, 2003) was used to obtain a new value for p and q. A new value for α was sampled using the method described by Escobar and West (1995). We ran the sampler for 3000 iterations, discarded the first 100 samples and kept every fifth sample of the remaining samples, for a total of 580 samples from the posterior distribution. Predicting human data: Participants in the behavioral experiments gave binary judgments (“could the word be part of the language?”) rather than probability estimates. To link our model’s predictions to participants’ binary responses, we sample m template instantiations from the poste</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31(3):705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J O’Donnell</author>
</authors>
<title>Productivity and reuse in language: A theory of linguistic computation and storage.</title>
<date>2015</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>O’Donnell, 2015</marker>
<rawString>Timothy J. O’Donnell. 2015. Productivity and reuse in language: A theory of linguistic computation and storage. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert J Scholes</author>
</authors>
<title>Phonotactic grammaticality.</title>
<date>1966</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="1994" citStr="Scholes, 1966" startWordPosition="318" endWordPosition="319">t start with a [q] as impossible English words. Sounds that share articulatory and/or perceptual properties often have similar phonotactic distributions. German, for example, allows voiced obstruents,1 such as [b] and [9], to occur anywhere in the word except at its end: [bal] is a valid German word, but [lab] isn’t. Speakers use such features of sounds to form phonotactic generalizations, which can then apply to sounds that do not appear in their language. Although no English words start with either [si] or [mb], English speakers judge srip to be a better potential word of English than mbip (Scholes, 1966); this is likely because [si] shares properties with strident-liquid clusters that do exist in English, such as [sl] as in slip and [Si] as in shrewd, whereas [mb] does not benefit from any sonorant-stop onset sequences (*[nt])—none exist in English. Recent studies have investigated how humans acquire generalizations over phonological classes in an artificial language paradigm (Linzen and Gallagher, 2014; Linzen and Gallagher, 2015). The central finding of these studies was that participants rapidly learned abstract phonotactic constraints and exhibited evidence of generalizations over classes</context>
</contexts>
<marker>Scholes, 1966</marker>
<rawString>Robert J. Scholes. 1966. Phonotactic grammaticality. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua B Tenenbaum</author>
<author>Thomas L Griffiths</author>
</authors>
<date>2001</date>
<booktitle>Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="9500" citStr="Tenenbaum and Griffiths (2001)" startWordPosition="1527" endWordPosition="1530"> 2. If P = 1, draw v — Uniform(Vf) and include it in c. 3. Otherwise, leave the feature unspecified, allowing the class to be abstract. 4The particular feature system is treated as a parameter of the model. In the simulation below we used a simplified version of the phonological feature inventory described in Hayes (2011), which only included features that are distinctive in English consonants. Generating words from templates: Given a choice of phonological template t, we assume that each of the segments that instantiate t has the same probability of being sampled (cf. the “size principle” of Tenenbaum and Griffiths (2001)). Consider again the class c0 = [+continuant, labial]. Under the assumption that the model’s segment inventory is the English one, there are only two segments that are labial continuants: [v] and [f]. The probability of each one of them being generated from c0 will be P(s|c0) = 1/2. Prior over template sets: The sounds of the language can be generated from a variety of templates at varying levels of abstraction. We therefore extend the model to be a mixture of template distributions of the type described above. The number of templates is inferred from the data using a Dirichlet process mixtur</context>
</contexts>
<marker>Tenenbaum, Griffiths, 2001</marker>
<rawString>Joshua B. Tenenbaum and Thomas L. Griffiths. 2001. Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24(4):629–640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Vul</author>
<author>Noah Goodman</author>
<author>Thomas L Griffiths</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>One and done? Optimal decisions from very few samples.</title>
<date>2014</date>
<journal>Cognitive Science,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="13131" citStr="Vul et al., 2014" startWordPosition="2164" endWordPosition="2167">kept every fifth sample of the remaining samples, for a total of 580 samples from the posterior distribution. Predicting human data: Participants in the behavioral experiments gave binary judgments (“could the word be part of the language?”) rather than probability estimates. To link our model’s predictions to participants’ binary responses, we sample m template instantiations from the posterior predictive distribution.5 If the relevant part of the test word appeared in these m samples, the model responds “yes”; this can be understood to be related to a sampling-based view of human inference (Vul et al., 2014). In the simulations below we fix m to be 10. Human endorsement rates were consistently above 50%, while the model’s ratings were often close to 0%. This is likely to be because human ratings were also informed by the unmodeled (fixed) parts of the templates, such as word length or number of vowels. We therefore linearly transform the model’s ratings to the range exhibited by human participants: if the untransformed rate is r, the ultimate simulated rate will be (1 + r)/2. 5Template instantiations only include the modeled (specified) part of the template: an onset consonant in our model of the</context>
</contexts>
<marker>Vul, Goodman, Griffiths, Tenenbaum, 2014</marker>
<rawString>Edward Vul, Noah Goodman, Thomas L. Griffiths, and Joshua B. Tenenbaum. 2014. One and done? Optimal decisions from very few samples. Cognitive Science, 38(4):599–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
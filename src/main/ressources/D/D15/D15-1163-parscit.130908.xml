<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.947452">
Improving Statistical Machine Translation with a
Multilingual Paraphrase Database
</title>
<author confidence="0.993163">
Ramtin Mehdizadeh Seraj, Maryam Siahbani, Anoop Sarkar
</author>
<affiliation confidence="0.907575333333333">
School of Computing Science
Simon Fraser University
Burnaby BC. Canada
</affiliation>
<email confidence="0.995863">
rmehdiza,msiahban,anoop@cs.sfu.ca
</email>
<sectionHeader confidence="0.994734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821913043478">
The multilingual Paraphrase Database
(PPDB) is a freely available automatically
created resource of paraphrases in mul-
tiple languages. In statistical machine
translation, paraphrases can be used to
provide translation for out-of-vocabulary
(OOV) phrases. In this paper, we show
that a graph propagation approach that
uses PPDB paraphrases can be used to im-
prove overall translation quality. We pro-
vide an extensive comparison with previ-
ous work and show that our PPDB-based
method improves the BLEU score by up
to 1.79 percent points. We show that
our approach improves on the state of the
art in three different settings: when faced
with limited amount of parallel training
data; a domain shift between training
and test data; and handling a morpho-
logically complex source language. Our
PPDB-based method outperforms the use
of distributional profiles from monolin-
gual source data.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901745454545">
Translation coverage is a major concern in statis-
tical machine translation (SMT) which relies on
large amounts of parallel, sentence-aligned text. In
(Callison-Burch et al., 2006), even with a training
data size of 10 million word tokens, source vocab-
ulary coverage in unseen data does not go above
90%. The problem is worse with multi-word OOV
phrases. Copying OOVs to the output is the most
common solution. However, even noisy transla-
tions of OOVs can improve reordering and lan-
guage model scores (Zhang et al., 2012). Translit-
eration is useful but not a panacea for the OOV
problem (Irvine and Callison-Burch, 2014b). We
find and remove the named entities, dates, etc. in
the source and focus on the use of paraphrases to
help translate the remaining OOVs. In Sec. 5.2 we
show that handling such OOVs correctly does im-
prove translation scores.
In this paper, we build on the following re-
search: Bilingual lexicon induction is the task
of learning translations of words from monolin-
gual data in source and target languages (Schafer
and Yarowsky, 2002; Koehn and Knight, 2002;
Haghighi et al., 2008). The distributional pro-
file (DP) approach uses context vectors to link
words as potential paraphrases to translation can-
didates (Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al., 2008; Garera et al., 2009). DPs
have been used in SMT to assign translation can-
didates to OOVs (Marton et al., 2009; Daum´e
and Jagarlamudi, 2011; Irvine et al., 2013; Irvine
and Callison-Burch, 2014a). Graph-based semi-
supervised methods extend this approach and
propagate translation candidates across a graph
with phrasal nodes connected via weighted para-
phrase relationships (Razmara et al., 2013; Saluja
et al., 2014; Zhao et al., 2015). Saluja et al. (2014)
extend paraphrases for SMT from the words to
phrases, which we also do in this work. Bilin-
gual pivoting uses parallel data instead of con-
text vectors for paraphrase extraction (Mann and
Yarowsky, 2001; Schafer and Yarowsky, 2002;
Bannard and Callison-Burch, 2005; Callison-
Burch et al., 2006; Zhao et al., 2008; Callison-
Burch, 2008). Ganitkevitch and Callison-Burch
(2014) published a large-scale multilingual Para-
phrase Database (PPDB) http://paraphrase.
org which includes lexical, phrasal, and syntactic
paraphrases (available for 22 languages with up to
170 million paraphrases each).
To our knowledge, this paper is the first com-
prehensive study of the use of PPDB for statistical
machine translation model training. Our frame-
work has three stages: 1) a novel graph con-
struction approach for PPDB paraphrases linked
</bodyText>
<page confidence="0.966227">
1379
</page>
<note confidence="0.984894">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1379–1390,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999951375">
with phrases from parallel training data. 2) Graph
propagation that uses PPDB paraphrases. 3) An
SMT model that incorporates new translation can-
didates. Sec. 3 explains these three stages in detail.
Using PPDB has several advantages: 1) Re-
sources such as PPDB can be built and used for
many different tasks including but not limited to
SMT. 2) PPDB contains many features that are
useful to rank the strength of a paraphrase con-
nection and with more information than distribu-
tional profiles. 3) Paraphrases in PPDB are often
better than paraphrases extracted from monolin-
gual or comparable corpora because a large-scale
multilingual paraphrase database such as PPDB
can pivot through a large amount of data in many
different languages. It is not limited to using
the source language data for finding paraphrases
which distinguishes it from previous uses of para-
phrases for SMT.
PPDB is a natural resource for paraphrases.
However, PPDB was not built with the specific ap-
plication to SMT in mind. Other applications such
as text-to-text generation have used PPDB (Gan-
itkevitch et al., 2011) but SMT brings along a
specific set of concerns when using paraphrases:
translation candidates should be transferred suit-
ably across paraphrases. There are many cases,
e.g. when faced with different word senses where
transfer of a translation is not appropriate. Our
proposed methods of using PPDB use graph prop-
agation to transfer translation candidates in a way
that is sensitive to SMT concerns.
In our experiments (Sec. 5) we compare our
approach with the state-of-the-art in three differ-
ent settings in SMT: 1) when faced with limited
amount of parallel training data; 2) a domain shift
between training and test data; and 3) handling
a morphologically complex source language. In
each case, we show that our PPDB-based approach
outperforms the distributional profile approach.
</bodyText>
<sectionHeader confidence="0.973028" genericHeader="method">
2 Paraphrase Extraction
</sectionHeader>
<bodyText confidence="0.999872222222222">
Our goal is to produce translations for OOV
phrases by exploiting paraphrases from the mul-
tilingual PPDB (Ganitkevitch and Callison-Burch,
2014) by using graph propagation. Since our ap-
proach relies on phrase-level paraphrases we com-
pare with the current state of the art approaches
that use monolingual data and distributional pro-
files to construct paraphrases and use graph prop-
agation (Razmara et al., 2013; Saluja et al., 2014).
</bodyText>
<subsectionHeader confidence="0.986503">
2.1 Paraphrases from Distributional Profiles
</subsectionHeader>
<bodyText confidence="0.999436333333333">
A distributional profile (DP) of a word or phrase
was first proposed in (Rapp, 1995) for SMT. Given
a word f, its distributional profile is:
</bodyText>
<equation confidence="0.996846">
DP(f) = {(A(f,wi))  |wi E V }
</equation>
<bodyText confidence="0.999495923076923">
V is the vocabulary and the surrounding words
wi are taken from a monolingual corpus using a
fixed window size. We use a window size of 4
words based on the experiments in (Razmara et al.,
2013). DPs need an association measure A(·, ·) to
compute distances between potential paraphrases.
A comparison of different association measures
appears in (Marton et al., 2009; Razmara et al.,
2013; Saluja et al., 2014) and our preliminary ex-
periments validated the choice of the same asso-
ciation measure as in these papers, namely Point-
wise Mutual Information (Lin, 1998) (PMI). For
each potential context word wi:
</bodyText>
<equation confidence="0.999638333333333">
P(f, wi)
A(f, wi) = log2 (1)
P(f)P(wi)
</equation>
<bodyText confidence="0.999105">
To evaluate the similarity between two phrases we
use cosine similarity. The cosine coefficient of two
phrases f1 and f2 is:
</bodyText>
<equation confidence="0.9939235">
S(f1, f2) = cos(DP(f1), DP(f2)) =
E
wi∈V A(f1, wi)A(f2, wi)
&apos; Ewi∈V A(f1, wi)2&apos; Ewi∈V A(f2, wi)2
</equation>
<bodyText confidence="0.999856727272727">
where V is the vocabulary. Note that in Eqn. (2)
wi’s are the words that appear in the context of f1
or f2, otherwise the PMI values would be zero.
Considering all possible candidate paraphrases
is very expensive. Thus, we use the heuristic ap-
plied in previous works (Marton et al., 2009; Raz-
mara et al., 2013; Saluja et al., 2014) to reduce the
search space. For each phrase we keep candidate
paraphrases which appear in one of the surround-
ing context (e.g. Left Right) among all occur-
rences of the phrase.
</bodyText>
<subsectionHeader confidence="0.997982">
2.2 Paraphrases from bilingual pivoting
</subsectionHeader>
<bodyText confidence="0.999369">
Bilingual pivoting uses parallel corpora between
the source language, F, and a pivot language T.
If two phrases, f1 and f2, in a same language are
paraphrases, then they share a translation in other
languages with p(f1|f2) as a paraphrase score:
</bodyText>
<equation confidence="0.971503">
S(f1, f2) = p(f1|f2) = � p(f1|t)p(t|f2) (3)
t
(2)
</equation>
<page confidence="0.936914">
1380
</page>
<figure confidence="0.992921842105263">
...
Source
Target
P (e  |f)
Phrase table (trk) كرﺗ
جرﺧ
بھذ
بھذ
بھذ
بھذ
(*hb&amp;)
0.1
(*hbt)
کرﺗ
کرﺗ
کرﺗ
کرﺗ
(*hb)
(gwld)
دﻟوﻏ
8
(Al*hb)
(jwld)
0.3
دﻟوﺟ
بA!ﻟI
5
7
0.2
بA!
6
0.4
(xrj)
2
0.1
4 تﺑA!
0.5
3
IوﺑA!
1
gold
went
be
left
0.5
0.2
0.1
0.2
leave
left
drop
dropped
0.5
0.2
0.1
0.2
Paraphrase pairs list
</figure>
<table confidence="0.8931101">
Source1 Source 2 Score
بھذ جرﺧ 0.4
بھذ تﺑھذ 0.2
بھذ اوﺑھذ 0.1
بھذ بھذﻟا 0.1
بھذ دﻟوﺟ 0.2
جرﺧ کرﺗ 0.5
جرﺧ بھذ 0.4
دﻟوﺟ بھذ 0.2
دﻟوﺟ دﻟوﻏ 0.3
</table>
<figure confidence="0.76484">
...
</figure>
<figureCaption confidence="0.989359727272727">
Figure 2: A small sample of the real graph constructed from the Arabic PPDB for Arabic to English translation. Filled nodes
(1 and 6) are phrases from the SMT phrase table (unfilled nodes are not). Edge weights are set using a log-linear combination
of scores from PPDB. Phrase #6 has different senses (‘gold’ or ‘left’); and it has a paraphrase in phrase #7 for the ‘gold’ sense
and a paraphrase in phrase #2 for the ‘left’ sense. After propagation, phrase #2 receives translation candidates from phrase #6
and phrase #1 reducing the probability of translation from unrelated senses (like the ‘gold’ sense). Phrase #8 is a misspelling
of phrase #7 and is also captured as a paraphrase. Phrase #6 propagates translation candidates to phrase #8 through phrase
#7. Morphological variants of phrase #6 (shown in bold) also receive translation candidates through graph propagation giving
translation candidates for morphologically rich OOVs.
Figure 1: English paraphrases extracted by pivot-
ing over German shared translation (Bannard and
Callison-Burch, 2005).
</figureCaption>
<bodyText confidence="0.999985227272727">
where t is a phrase in language T. p(f1|t) and
p(t|f2) are taken from the phrase table extracted
from parallel data for languages F and T. In Fig. 1
from (Bannard and Callison-Burch, 2005) we see
that paraphrase pairs like (in check, under con-
trol) can be extracted by pivoting over the German
phrase unter kontrolle.
The multilingual Paraphrase Database
(PPDB) (Ganitkevitch and Callison-Burch,
2014) is a published resource for paraphrases
extracted using bilingual pivoting. It leverages
syntactic information and other resources to filters
and scores each paraphrase pair using a large set
of features. These features can be used by a log
linear model to score paraphrases (Zhao et al.,
2008). We used a linear combination of these fea-
tures using the equation in Sec. 3 of (Ganitkevitch
and Callison-Burch, 2014) to score paraphrase
pairs. PPDB version 1 is broken into different
levels of coverage. The smaller sizes contain only
better-scoring, high-precision paraphrases, while
larger sizes aim for high coverage.
</bodyText>
<equation confidence="0.906855444444445">
Algorithm 1 PPDB Graph Propagation for SMT
PhrTable = PhraseTableGeneration();
ParaDB = ParaphraseExtraction(); (Sec. 2)
InitGraph = GraphConstruct(PhrTable, ParaDB); (Sec. 3.1)
PropGraph = GraphPropagation(InitGraph); (Sec. 3.2)
for phrase ∈ {OOVs} do
newTrans = TranslationFinder(PropGraph, phrase);
Augment(PhrTable, newTrans); (Sec. 3.3)
TuneMT(PhrTable);
</equation>
<sectionHeader confidence="0.994305" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999904666666667">
After paraphrase extraction we have paraphrase
pairs, (f1, f2) and a score 5(f1, f2) we can in-
duce new translation rules for OOV phrases us-
ing the steps in Algo. (1): 1) A graph of source
phrases is constructed as in (Razmara et al., 2013);
2) translations are propagated as labels through the
graph as explained in Fig. 2; and 3) new trans-
lation rules obtained from graph-propagation are
integrated with the original phrase table.
</bodyText>
<subsectionHeader confidence="0.998498">
3.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999984461538462">
We construct a graph G(V, E, W) over all source
phrases in the paraphrase database and the source
language phrases from the SMT phrase table ex-
tracted from the available parallel data. V cor-
responds to the set of vertices (source phrases),
E is the set of edges between phrases and W is
weight of each using the score function 5 defined
in Sec. 2. V has two types of nodes: seed (labeled)
nodes, Vs, from the SMT phrase table, and regu-
lar nodes, Vr. Note that in this step OOVs are part
of these regular nodes, and we try to find transla-
tion in the propagation step for all of these regu-
lar nodes. In graph construction and propagation,
</bodyText>
<page confidence="0.953126">
1381
</page>
<bodyText confidence="0.999922615384615">
we do not know which phrasal nodes correspond
to OOVs in the dev and test set. Fig. 2 shows a
small slice of the actual graph used in one of our
experiments; This graph is constructed using the
paraphrase database on the right side of the figure.
Filled nodes have a distribution over translations
(the possible “labels” for that node). In our setting,
we consider the translation e to be the “label” and
so we propagate the labeling distribution p(e|f)
which is taken from the feature function for the
SMT log-linear model that is taken from the SMT
phrase table and we propagate this distribution to
unlabeled nodes in the graph.
</bodyText>
<subsectionHeader confidence="0.996827">
3.2 Graph Propagation
</subsectionHeader>
<bodyText confidence="0.999980166666667">
Considering the translation candidates of known
phrases in the SMT phrase table as the “labels” we
apply a soft label propagation algorithm in order to
assign translation candidates to “unlabeled” nodes
in the graph, which include our OOV phrases.
As described by the example in Fig. 2 we wish
two outcomes: 1) transfer of translations (or “la-
bels”) to unlabeled nodes (OOV phrases) from la-
beled nodes, and 2) smoothing the label distribu-
tion at each node. We use the Modified Adsorption
(MAD) algorithm (Talukdar and Crammer, 2009)
for graph propagation. Suppose we have m dif-
ferent possible labels plus one dummy label, a soft
label Yˆ E Am+1 is a m + 1 dimension probabil-
ity vector. The dummy label is used when there is
low confidence on correct labels. Based on MAD,
we want to find soft label vectors for each node by
optimizing the objective function below:
</bodyText>
<equation confidence="0.9753165">
�µ3 P3,v||ˆYv − Rv||22
vEV
</equation>
<bodyText confidence="0.99998748">
In this objective function, µi and Pi,v are hyper-
parameters (Vv : EiPi,v = 1). Rv E Am+1 is
our prior belief about labeling. First component
of the function tries to minimize the difference of
new distribution to the original distribution for the
seed nodes. The second component insures that
nearby neighbours have similar distributions, and
the final component is to make sure that the dis-
tribution does not stray from a prior distribution.
At the end of propagation, we wish to find a la-
bel distribution for our OOV phrases. We describe
in Sec. 4.2.2 the reasons for choosing MAD over
other graph propagation algorithms. The MAD
graph propagation generalizes the approach used
in (Razmara et al., 2013). The Structured Label
Propagation algorithm (SLP) was used in (Saluja
et al., 2014; Zhao et al., 2015) which uses a graph
structure on the target side phrases as well. How-
ever, we have found that in our diverse experimen-
tal settings (see Sec. 5) MAD had two properties
we needed compared to SLP: one was the use of
graph random walks which allowed us to control
translation candidates and MAD also has the abil-
ity to penalize nodes with a large number of edges
(also see Sec. 4.2.2).
</bodyText>
<subsectionHeader confidence="0.99932">
3.3 Phrase Table Integration
</subsectionHeader>
<bodyText confidence="0.999991571428571">
After propagation, for each potential OOV phrase
we have a list of possible translations with corre-
sponding probabilities. A potential OOV is any
phrase which does not appear in training, but could
appear in unseen data. We do not look at the dev
or test data to produce the augmented phrase ta-
ble. The original phrase table is now augmented
with new entries providing translation candidates
for potential OOVs; Last column in Table 2 shows
how many entries have been added to the phrase
table for each experimental settings. A new fea-
ture is added to the standard SMT log-linear dis-
criminative model and introduced into the phrase
table. This new feature is set to either 1.0 for
the phrase table entries that already existed; or `i
which is the log probability (from graph propaga-
tion) for the translation candidate i for potential
OOVs. In case the dummy label exists with high
probability or the label distribution is uniform, an
identity rule is added to the phrase table (copy over
source to target).
</bodyText>
<sectionHeader confidence="0.860733" genericHeader="method">
4 Analysis of the Framework
</sectionHeader>
<subsectionHeader confidence="0.999958">
4.1 Propagation of poor translations
</subsectionHeader>
<bodyText confidence="0.999685363636364">
Automatic paraphrase extraction generates many
possible paraphrase candidates and many of them
are likely to be false positives for finding transla-
tion candidates for OOVs. Distributional profiles
rely on context information which is not sufficient
to derive accurate paraphrases for many phrases
and this results in many low quality paraphrase
candidates. Bilingual pivoting uses word align-
ments which can also introduce errors depending
on the size and quality of the bilingual data used.
Alignment errors also introduce poor translations.
</bodyText>
<equation confidence="0.6749415">
min �µ1 P1,v||Yv − Yˆ||22 +
Yˆ vEV3
�µ2 P2,vWv,u||ˆYv − ˆYu||22 + (4)
vEV,uEN(v)
</equation>
<page confidence="0.649765">
1382
</page>
<table confidence="0.9996615">
Size Nodes Edges Max Ave
Neigh. Neigh.
S 23K 31K 32 1.38
M 41K 69K 33 1.69
L 74K 199K 67 2.69
XL 103K 548K 330 5.33
XXL 122K 2073K 1231 16.968
XXXL 125K 7558K 5255 60.27
</table>
<tableCaption confidence="0.998915">
Table 1: Statistics of the graph constructed using
</tableCaption>
<bodyText confidence="0.969959909090909">
the English lexical PPDB. We have built similar
graphs for French and Arabic.
In graph propagation, these errors may be propa-
gated and result in poor translations for OOVs.
We could address this issue by aggressively
pruning the potential paraphrase candidates to im-
prove the precision. However, this results in a dra-
matic drop in coverage and many OOV phrases do
not obtain any translation candidates. We use a
combination of the following three steps to aug-
ment our graph propagation framework.
</bodyText>
<subsectionHeader confidence="0.546278">
4.1.1 Graph pruning and PPDB sizes
</subsectionHeader>
<bodyText confidence="0.999654045454546">
Pruning the graph avoids error propagation by re-
moving unreliable edges. Pruning removes edges
with an edge weight lower than a minimum thresh-
old or by limiting the number of neighbours to the
top-K edges (Talukdar, 2009). PPDB has different
sizes with different levels of accuracy and cover-
age. We can do graph pruning simply by choosing
to use different sizes of PPDB. As we can see in
Fig. 3 results vary from language to language de-
pending on the pruning used. For instance, the L
size results in the best score for French-English.
We choose the best size of PPDB for each lan-
guage based on a separate held-out set and inde-
pendently from each of the SMT-based tasks in our
experimental results. Our conclusion from our ex-
periments with the different sizes of PPDB is that
removing phrases (or nodes in our graph) is not
desirable. However, removing unreliable edges is
useful. As seen in Table 1, increasing the size
of PPDB leads to a rapid increase in nodes fol-
lowed by a larger number of edges in the very large
PPDB sizes.
</bodyText>
<subsectionHeader confidence="0.685773">
4.1.2 Pruning the translation candidates
</subsectionHeader>
<bodyText confidence="0.993699">
Another solution to the error propagation issue is
to propagate all translation candidates but when
providing translations to OOVs in the final phrase
</bodyText>
<figure confidence="0.86286575">
BLEU score 30
29.5
29
Base S M L XL
</figure>
<figureCaption confidence="0.9900485">
Figure 3: Effect of PPDB size on improving
BLEU score for Spanish and French
</figureCaption>
<bodyText confidence="0.9992026">
table to eliminate all but the top L translations
for each phrase (which is the usual ttable limit in
phrase-based SMT (Koehn et al., 2003)). Based
on a development set, separate from the test sets
we used, we found that the best value of L was 10.
</bodyText>
<subsubsectionHeader confidence="0.849722">
4.1.3 External Resources for Filtering
</subsubsectionHeader>
<bodyText confidence="0.99998425">
Applying more informative filters can be also used
to improve paraphrase quality. This can be done
through additional features for paraphrase pairs.
For example, edit distance can be used to capture
misspelled paraphrases. We use a Named Entity
Recognizer to exclude names, numbers and dates
from the paraphrase candidates. Even after remov-
ing these tokens, 3.32% of tokens of test set are
still OOVs . In addition, we use a list of stop words
to remove nodes which have too many connec-
tions. These two filters improve our results (more
in Sec. 5).
</bodyText>
<subsectionHeader confidence="0.996778">
4.2 Path sensitivity
</subsectionHeader>
<bodyText confidence="0.965664117647059">
Graph propagation has been used in many NLP
tasks like POS tagging, parsing, etc. but propa-
gating translations in a graph as labels is much
more challenging. Due to huge number of pos-
sible labels (translations) and many low quality
edges, it is very likely that many wrong transla-
tions are rapidly propagated in few steps. Raz-
mara et al. (2013) show that unlabeled nodes in-
side the graph, called bridge nodes, are useful for
the transfer of translations when there is no other
connection between an OOV phrase and a node
with known translation candidates. However, they
show that using the full graph with long paths of
bridge nodes hurts performance. Thus the propa-
gation has to be constrained using path sensitivity.
Fig. 4 shows this issue in a part of an English para-
Spanish - English French - English
</bodyText>
<page confidence="0.879184">
1383
</page>
<figureCaption confidence="0.813463">
Figure 4: Sensitivity issue in graph propagation
</figureCaption>
<bodyText confidence="0.935308">
for translations. “Lager” is a translation candidate
for “stock”, which is transferred to “majority” af-
ter 3 iterations.
phrase graph. After three iterations, German trans-
lation “Lager” reaches “majority” which is totally
irrelevant as a translation candidate. Transfer of
translation candidates should prefer close neigh-
bours and only with a very low probability to other
nodes in the graph.
</bodyText>
<subsectionHeader confidence="0.500449">
4.2.1 Pre-structuring the graph
</subsectionHeader>
<bodyText confidence="0.999717375">
Razmara et al. (2013) avoid a fully connected
graph structure. They pre-structure the graph
into bipartite graphs (only connections between
phrases with known translation and OOV phrases)
and tripartite graphs (connections can also go from
a known phrasal node to an OOV phrasal node
through one node that is a paraphrase of both
but does not have translations, i.e. it is an unla-
beled node). In these pre-structured graphs there
are no connections between nodes of the same
type (known, OOV or unlabeled). We apply this
method in our low resource setting experiments
(Sec. 5.3) to compare our bipartite and tripartite
results to Razmara et al. (2013). In the rest of the
experiments we use the tripartite approach since it
outperforms the bipartite approach.
</bodyText>
<subsectionHeader confidence="0.491091">
4.2.2 Graph random walks
</subsectionHeader>
<bodyText confidence="0.99997780952381">
Our goal is to limit the number of hops in the prop-
agation of translation candidates preferring closely
connected and highly probable edge weights. Op-
timization for the Modified Adsorption (MAD)
objective function in Sec. 3.2 can be viewed as
a controlled random walk (Talukdar et al., 2008;
Talukdar and Crammer, 2009). This is formal-
ized as three actions: inject, continue and aban-
don with corresponding pre-defined probabilities
Pinj, Pcont and Pabnd respectively as in (Taluk-
dar and Crammer, 2009). A random walk through
the graph will transfer labels from one node to an-
other node, and probabilities Pcont and Pabnd con-
trol exploration of the graph. By reducing the val-
ues of Pcont and increasing Pabnd we can control
the label propagation process to optimize the qual-
ity of translations for OOV phrases. Again, this is
done on a held-out development set and not on the
test data. The optimal values in our experiments
for these probabilities are Pinj = 0.9, Pcont =
0.001, Pabnd = 0.01.
</bodyText>
<subsectionHeader confidence="0.689138">
4.2.3 Early stopping of propagation
</subsectionHeader>
<bodyText confidence="0.999931545454545">
In Modified Adsorption (MAD) (see Sec. 3.2)
nodes in the graph that are closely linked will tend
to similar label distributions as the number of it-
erations increase (even when the path lengths in-
crease). In our setting, smoothing the label distri-
bution helps in the first few iterations, but is harm-
ful as the number of iterations increase due to the
factors shown in Fig. 4. We use early stopping
which limits the number of iterations. We varied
the number of iterations from 1 to 10 on a held-out
dev set and found that 5 iterations was optimal.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9993662">
We first show the effect of OOVs on translation
quality, then evaluate our approach in three dif-
ferent SMT settings: low resource SMT, domain
shift, and morphologically complex languages.
In each case, we compare results of using para-
phrases extracted by Distributional Profile (DP)
and PPDB in an end-to-end SMT system.
Important: no subset of the test data sentences
are used in the bilingual corpora for paraphrase ex-
traction process.
</bodyText>
<subsectionHeader confidence="0.976554">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999993625">
We use CDEC1 (Dyer et al., 2010) as an end-
to-end SMT pipeline with its standard features2.
fast align (Dyer et al., 2013) is used for word
alignment, and weights are tuned by minimizing
BLEU loss on the dev set using MIRA (Cram-
mer and Singer, 2003). This setup is used for
most of our experiments: oracle (Sec. 5.2), do-
main adaptation (Sec. 5.4) and morphologically
complex languages (Sec. 5.5). But as we wish
to fairly compare our approach with Razmara et
al. (2013) on low resource setting, we follow their
setup in Sec. 5.3: Moses (Koehn et al., 2007) as
SMT pipeline, GIZA++ (Och and Ney, 2003) for
word alignment and MERT (Och, 2003) for tun-
ing. We add our own feature to the SMT log-linear
model as described in Sec. 3.3.
</bodyText>
<footnote confidence="0.998109">
1http://www.cdec-decoder.org
</footnote>
<page confidence="0.472487">
2EgivenFCoherent, SampleCountF, CountEF, MaxLexF-
givenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF
</page>
<figure confidence="0.60959325">
Lager
stock
iter1 iter2 iter3
bank margin majority
</figure>
<page confidence="0.641518">
1384
</page>
<table confidence="0.8583818">
Experiments OOV type/token Rules added
Case 1 1830 / 2163 7.0K
Case 2 - Med. 2294 / 4190 7.8K
Case 2 - Sci. 5272 / 14121 10.4K
Case 3 1543 / 1895 8.1K
</table>
<tableCaption confidence="0.977475">
Table 2: Statistics of settings in Sec. 5. Last col-
umn shows how many rules added in the phrase
table integration step.
</tableCaption>
<bodyText confidence="0.997970666666667">
KenLM (Heafield, 2011) is used to train a 5-
gram language model on English Gigaword (V5:
LDC2011T07). For scalable graph propagation
we use the Junto framework3. We use maximum
phrase length 10. For our experiments we use
the Hadoop distributed computing framework ex-
ecuted on a cluster with 12 nodes (each node has
8 cores and 16GB of RAM). Each graph propaga-
tion iteration takes about 3 minutes.
For French, we apply a simple heuristic to de-
tect named entities: words that are capitalized in
the original dev/test set that do not appear at the
beginning of a sentence are named entities. Based
on eyeballing the results, this works very well in
our data. For Arabic, AQMAR is used to exclude
named-entities (Mohit et al., 2012). For each of
the experimental settings below we show the OOV
statistics in Table 2.
</bodyText>
<subsectionHeader confidence="0.998127">
5.2 Impact of OOVs: Oracle experiment
</subsectionHeader>
<bodyText confidence="0.99995875">
This oracle experiment shows that translation of
OOVs beyond named entities, dates, etc. is poten-
tially very useful in improving output translation.
We trained a SMT system on 10K French-English
sentences from the Europarl corpus(v7) (Koehn,
2005). WMT 2011 and WMT 2012 are used as
dev and test data respectively. Table 4 shows the
results in terms of BLEU on dev and test. The
first row is baseline which simply copies OOVs to
output. The second and third rows show the re-
sult of augmenting phrase-table by adding transla-
tions for single-word OOVs and phrases contain-
ing OOVs. The last row shows the oracle result
where dev and test sentences exist inside the train-
ing data and all the OOVs are known (Fully ob-
servers cannot avoid model and search errors).
</bodyText>
<subsectionHeader confidence="0.994907">
5.3 Case 1: Limited Parallel Data
</subsectionHeader>
<bodyText confidence="0.922046">
In this experiment we use a setup similar to (Raz-
mara et al., 2013). To have fair comparison,
</bodyText>
<footnote confidence="0.981889">
3Junto : https://github.com/parthatalukdar/junto
</footnote>
<table confidence="0.9995202">
Fr-En Dev Test
Baseline 27.90 28.08
+ Lexical OOV 28.10 28.31
+ Phrasal OOV 28.50 28.85
Fully observed 46.88 49.21
</table>
<tableCaption confidence="0.999891">
Table 4: The impact of translating OOVs.
</tableCaption>
<bodyText confidence="0.999722789473684">
we use 10K French-English parallel sentences,
randomly chosen from Europarl to train trans-
lation system, as reported in (Razmara et al.,
2013). ACL/WMT 20054 is used for dev and test
data. We re-implement their paraphrase extraction
method (DP) to extract paraphrases from French
side of Europarl (2M sentences). We use unigram
nodes to construct graphs for both DP and PPDB.
In bipartite graphs, each node is connected to at
most 20 nodes. For tripartite graphs, each node is
connected to 15 labeled and 5 unlabeled nodes.
For intrinsic evaluation, we use Mean-
Reciprocal-Rank (MRR) and Recall. MRR is
the mean of reciprocal rank of the candidate list
compared to the gold list (Eqn. 5). Recall shows
percentage of gold list covered by the candidate
list (Eqn. 6). Gold translations for OOVs are
given by concatenating the test data to training
and running a word aligner.
</bodyText>
<equation confidence="0.967234">
1 for O = {OOVs} (5)
Recall = |{gold list} n {candidate list} |(6)
|{gold list}|
</equation>
<bodyText confidence="0.9996894375">
Table 5 compares DP and PPDB in terms of
BLEU, MRR and Recall. It indicates that PPDB
(large size) outperforms DP in both intrinsic and
extrinsic evaluation measures. Although tripartite
graph did not improve the results for DP, it results
in statistically significantly better BLEU score for
PPDB in comparison to DP (evaluated by MultE-
val (Clark et al., 2011)). Thus we use tripartite
graph in the rest of experiments. The last row in
the table shows the result of combining DP and
PPDB by multiplying the normalized scores of
both paraphrase lists.
This setting is included for three reasons: 1)
we exploit the small data size to explore differ-
ent choices in our approach such as, e.g. choos-
ing bipartite versus tripartite graph structures; 2)
</bodyText>
<footnote confidence="0.600567">
4http://www.statmt.org/wpt05/mt-shared-task/
</footnote>
<equation confidence="0.967940833333333">
1
MRR =
|O|
� |O|
i=1
ranki
</equation>
<page confidence="0.961714">
1385
</page>
<table confidence="0.96603025">
OOV PPDB NNs DP NNs Reference sentence PPDB output DP output
proc´ed´es processus m´ethodes ... an agreement on proce- ... an agreement on the ... an agreement on
outils dures in itself is a good procedure is a good ... products is a good ...
mat´eriaux thing ...
quantique quantiques - ... allowed us to achieve ... allowed quantum de- ... quantique allowed
quantum degeneracy ... generacy ... degeneracy ...
mlzm mlzmA ADTr ... voted 97-0 last week for ... voted 97 last week on ... voted 97 last week on
a non-binding resolution ... not binding resolution ... having resolution ...
</table>
<tableCaption confidence="0.87797">
Table 3: Examples comparing DP versus PPDB outputs on the test sets. NNs refer to nearest neighbours
in the graph for OOV phrase. Each row respectively corresponds to experimental settings (cases 1 to 3).
</tableCaption>
<table confidence="0.999913571428571">
System MRR Recall BLEU
baseline - - 28.89
DP-bipartite 5.34 11.90 29.27
DP-tripartite 5.34 11.95 29.27
PPDBfr (L)-bipartite 12.05 22.08 29.46
PPDBfr (L)-tripartite 10.22 22.87 29.52
Combined-tripartite - - 29.28
</table>
<tableCaption confidence="0.999838">
Table 5: Results of PPDB and DP techniques.
</tableCaption>
<bodyText confidence="0.9995675">
to show how well our PPDB approach does com-
pared to the DP approach in terms of MRR and
recall; and 3) to show applicability of our ap-
proach for a low-resource language. However we
used French instead of a language which is truly
resource-poor due to the lack of available para-
phrases for a true resource poor language, e.g.
Malagasy.
</bodyText>
<subsectionHeader confidence="0.997861">
5.4 Case 2: Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.9979416">
Domain adaptation is another case that suffers
from massive number of OOVs. We compare our
approach with Marginal Matching (Irvine et al.,
2013), a state of the art approach in SMT domain
adaptation. We use their setup and data and com-
pare our results to their reported results (Irvine et
al., 2013). 250K lines of Hansard parliamentary
proceeding are used for training MT. Dev and test
sets are available for two different domains: Medi-
cal and Science domains. For medical domain ran-
dom subset of EMEA corpus (Tiedemann, 2009)
and for the science domain a corpus of scientific
articles (Carpuat et al., 2012) has been used. Un-
igram paraphrases using DP are extracted from
French side of Europarl.
Table 6 compares the results in terms of BLEU
score. In both medical and science domains,
graph-propagation approach using PPDB (large)
performs significantly better than DP (p &lt; 0.02),
and has comparable results to Marginal Matching.
</bodyText>
<table confidence="0.9963358">
Systems Science Medical
baseline 22.20 25.32
DP-tripartite 22.76 25.81
PPDBfr (L)-tripartite 22.97 27.11
Marginal Matching 23.62 26.97
</table>
<tableCaption confidence="0.904732">
Table 6: BLEU scores for domain adaptation.
</tableCaption>
<table confidence="0.9980605">
Systems BLEU
baseline 29.59
DP-tripartite 30.08
PPDBarabic (L)-tripartite 31.12
</table>
<tableCaption confidence="0.999584">
Table 7: BLEU score results for Arabic-English.
</tableCaption>
<bodyText confidence="0.97449425">
Marginal Matching performs better in science do-
main but graph-propagation approach with PPDB
outperforms it in medical domain getting a +1.79
BLEU score improvement over the baseline.
</bodyText>
<subsectionHeader confidence="0.996062">
5.5 Case 3: Morphologically Rich Languages
</subsectionHeader>
<bodyText confidence="0.999759230769231">
Both Distribution Profiling and Bilingual Pivot-
ing propose morphological variants of a word as
paraphrase pairs. Even more so in PPDB due to
pivoting over English. We choose Arabic-English
task for this experiment. We train the SMT system
on 685K sentence pairs (randomly selected from
LDC2007T08 and LDC2008T09) and use NIST
OpenMT 2012 for dev and test data. Arabic side of
1M sentences of LDC2007T08 and LDC2008T09
is used to extract unigram paraphrases for DP. Ta-
ble 7 shows that PPDB (large; with phrases) re-
sulted in +1.53 BLEU score improvement over
DP which only slightly improved over baseline.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99980075">
Sentence level paraphrasing has been used for gen-
erating alternative reference translations (Madnani
et al., 2007; Kauchak and Barzilay, 2006), or
augmenting the training data with sentential para-
</bodyText>
<page confidence="0.982852">
1386
</page>
<bodyText confidence="0.99941552112676">
phrases (Bond et al., 2008; Nakov, 2008; Mirkin et
al., 2009). Phrase level paraphrasing was done us-
ing crowdsourcing (Resnik et al., 2010) or by us-
ing paraphrases in lattice decoding (Onishi et al.,
2010; Du et al., 2010).
Daum´e and Jagarlamudi (2011) apply a genera-
tive model to domain adaptation based on canon-
ical correlation analysis Haghighi et al. (2008).
However, they use artificially created monolingual
corpora very related to the same domain as test
data. Irvine and Callison-Burch (2014a) gener-
ate a large, noisy phrase table by composing un-
igram translations which are obtained by a super-
vised method (Irvine and Callison-Burch, 2013).
Comparable monolingual data is used to re-score
and filter the phrase table. Zhang and Zong (2013)
use a large manually generated lexicon for do-
main adaptation. In contrast to these methods, our
method is unsupervised.
Alexandrescu and Kirchhoff (2009) use a
graph-based semi-supervised model determine
similarities between sentences, then use it to re-
rank the n-best translation hypothesis. Liu et al.
(2012) extend this model to derive some features
to be used during decoding. These approaches are
orthogonal to our approach. Saluja et al. (2014)
use Structured Label Propagation (Liu et al., 2012)
in two parallel graphs constructed on source and
target paraphrases. In their case the graph con-
struction is extremely expensive. Leveraging a
morphological analyzer, they reach significant im-
provement on Arabic. We can not directly com-
pare our results to (Saluja et al., 2014) because
they exploit several external resources such as
a morphological analyzer and also had different
sizes of training and test. In experiments (Sec. 5)
we obtained comparable BLEU score improve-
ment on Arabic-English by using bilingual pivot-
ing only on source phrases. (Saluja et al., 2014)
also use methods similar to (Habash, 2008) that
expand the phrase table with spelling and morpho-
logical variants of OOVs in test data. We do not
use the dev/test data to augment the phrase table.
Using comparable corpora to extract parallel
sentences and phrases (Munteanu and Marcu,
2006; Smith et al., 2010; Tamura et al., 2012) are
orthogonal to the approach we discuss here.
Bilingual and multilingual word and phrase rep-
resentation using neural networks have been ap-
plied to machine translation (Zou et al., 2013;
Mikolov et al., 2013a; Zhang et al., 2014). How-
ever, most of these methods focus on frequent
words or an available bilingual phrase table (Zou
et al., 2013; Zhang et al., 2014; Gao et al., 2014).
Mikolov et al. (2013a) learn a global linear projec-
tion from source to target using representation of
frequent words on both sides. This model can be
used to generate translations for new words, but a
large amounts of bilingual data is required to cre-
ate such a model. (Mikolov et al., 2013b) also
uses bilingual data to project new translation rules.
Zhao et al. (2015) extend Mikolov’s model to learn
one local linear projection for each phrase. Their
model reaches comparable results to Saluja et al.
(2014) while works faster. Alkhouli et al. (2014)
use neural network phrase representation for para-
phrasing OOVs and find translation for them using
a phrase-table created from limited parallel data.
Our experimental settings is different from the ap-
proaches in (Alkhouli et al., 2014; Mikolov et al.,
2013a; Mikolov et al., 2013b).
</bodyText>
<sectionHeader confidence="0.989359" genericHeader="conclusions">
7 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999959444444444">
In future work, we would like to include transla-
tions for infrequent phrases which are not OOVs.
We would like to explore new propagation meth-
ods that can directly use confidence estimates and
control propagation based on label sparsity. We
also would like to expand this work for mor-
phologically rich languages by exploiting other
resources like morphological analyzer and cam-
pare our approach to the current state of art ap-
proaches which are using these types of resources.
In conclusion, we have shown significant improve-
ments to the quality of statistical machine transla-
tion in three different cases: low resource SMT,
domain shift, and morphologically complex lan-
guages. Through the use of semi-supervised graph
propagation, a large scale multilingual paraphrase
database can be used to improve the quality of sta-
tistical machine translation.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999831857142857">
The authors would like to thank Chris Callison-
Burch and Juri Ganitkevitch for providing us the
latest version of PPDB, the anonymous reviewers
for their comments. The research was supported
by the Natural Sciences and Engineering Research
Council of Canada (NSERC RGPIN 262313 and
RGPAS 446348) to the last author.
</bodyText>
<page confidence="0.981939">
1387
</page>
<bodyText confidence="0.8543778">
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In NAACL HLT
2011.
</bodyText>
<note confidence="0.80120325">
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In NAACL 2009.
</note>
<reference confidence="0.998786854166667">
Tamer Alkhouli, Andreas Guta, and Hermann Ney.
2014. Vector space models for phrase-based ma-
chine translation. In EMNLP 2014: Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
2005.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine
translation by paraphrasing the training data. In
IWSLT 2008.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In NAACL 2006.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
EMNLP 2008.
Marine Carpuat, H Daum´e III, Alexander Fraser, Chris
Quirk, Fabienne Braune, Ann Clifton, Ann Irvine,
Jagadeesh Jagarlamudi, John Morgan, Majid Raz-
mara, Aleˇs Tamchyna, Katharine Henry, and Rachel
Rudinger. 2012. Domain adaptation in machine
translation: Final report. In 2012 Johns Hopkins
Summer Workshop.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL 2011.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
The Journal of Machine Learning Research.
Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In ACL 2011.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facili-
tating translation using source language paraphrase
lattices. In EMNLP 2010.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL 2010.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A Simple, Fast, and Effective Reparameter-
ization of IBM Model 2. In NAACL HLT 2013.
Juri Ganitkevitch and Chris Callison-Burch. 2014. The
multilingual paraphrase database. In LREC 2014.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In ACL 2014.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In CoNLL
2009.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In ACL
2008.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL 2008.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT 2011.
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In NAACL 2013.
Ann Irvine and Chris Callison-Burch. 2014a. Hal-
lucinating phrase translations for low resource MT.
CoNLL-2014.
Ann Irvine and Chris Callison-Burch. 2014b. Using
comparable corpora to adapt mt models to new do-
mains. ACL 2014.
Ann Irvine, Chris Quirk, and Hal Daum´e III. 2013.
Monolingual marginal matching for translation
model adaptation. In EMNLP 2013.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In NAACL 2008.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL 2002 workshop on unsupervised lexical acqui-
sition.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, and et al. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL 2007.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit 2005,
volume 5.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In ACL 1998.
</reference>
<page confidence="0.980785">
1388
</page>
<note confidence="0.537853">
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured label
propagation. In ACL 2012.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kron-
rod, Alex Quinn, and Benjamin B Bederson. 2010.
Improving translation via targeted paraphrasing. In
EMNLP 2010.
</note>
<reference confidence="0.999814126315789">
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
WMT 2007.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In NAACL 2001.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
EMNLP 2009.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages for
machine translation. CoRR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS 2013.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In ACL-IJCNLP 2009.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A Smith. 2012.
Recall-oriented learning of named entities in arabic
wikipedia. In EACL 2012, pages 162–173.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In ACL 2006.
Preslav Nakov. 2008. Improved statistical machine
translation using monolingual paraphrases. In ECAI
2008: 18th European Conference on Artificial Intel-
ligence. IOS Press.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In ACL 2003.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine
translation. In ACL 2010.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In ACL 1995.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In ACL 2013.
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In ACL 2014.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In CoNLL 2002.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
NAACL 2010.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In European Conference on Machine
Learning.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In EMNLP 2008.
Partha Pratim Talukdar. 2009. Topics in graph con-
struction for semi-supervised learning. Technical
Report MS-CIS-09-13, University of Pennsylvania,
Dept of Computer and Info. Sci.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In EMNLP-CoNLL
2012.
J¨org Tiedemann. 2009. News from OPUS-A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent advances in natural language
processing.
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In ACL
2013.
Jiajun Zhang, Feifei Zhai, and Chengqing Zong.
2012. Handling unknown words in statistical ma-
chine translation from a new perspective. In Natu-
ral Language Processing and Chinese Computing.
Springer.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In ACL
2014.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In ACL 2008.
</reference>
<page confidence="0.884959">
1389
</page>
<reference confidence="0.999197">
Kai Zhao, Hany Hassan, and Michael Auli. 2015.
Learning translation models from monolingual con-
tinuous representations. In NAACL 2015.
Will Zou, Richard Socher, Daniel Cer, and Christopher
Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In EMNLP 2013.
</reference>
<page confidence="0.989787">
1390
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422212">
<title confidence="0.999245">Improving Statistical Machine Translation with Multilingual Paraphrase Database</title>
<author confidence="0.697394">Ramtin Mehdizadeh Seraj</author>
<author confidence="0.697394">Maryam Siahbani</author>
<author confidence="0.697394">Anoop</author>
<affiliation confidence="0.672035">School of Computing</affiliation>
<author confidence="0.854513">Simon Fraser Burnaby BC</author>
<email confidence="0.994245">rmehdiza,msiahban,anoop@cs.sfu.ca</email>
<abstract confidence="0.999025083333333">The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages. In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases. In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality. We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional profiles from monolingual source data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tamer Alkhouli</author>
<author>Andreas Guta</author>
<author>Hermann Ney</author>
</authors>
<title>Vector space models for phrase-based machine translation.</title>
<date>2014</date>
<booktitle>In EMNLP 2014: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="35404" citStr="Alkhouli et al. (2014)" startWordPosition="5870" endWordPosition="5873">rase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b). 7 Conclusion and Future work In future work, we would like to include translations for infrequent phrases which are not OOVs. We would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity. We also would like to expand this work for morpho</context>
</contexts>
<marker>Alkhouli, Guta, Ney, 2014</marker>
<rawString>Tamer Alkhouli, Andreas Guta, and Hermann Ney. 2014. Vector space models for phrase-based machine translation. In EMNLP 2014: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="3158" citStr="Bannard and Callison-Burch, 2005" startWordPosition="490" endWordPosition="493">s (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked 1379 Proceedings of the 2015 Conference on</context>
<context position="9678" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1568" endWordPosition="1571">t’ sense. After propagation, phrase #2 receives translation candidates from phrase #6 and phrase #1 reducing the probability of translation from unrelated senses (like the ‘gold’ sense). Phrase #8 is a misspelling of phrase #7 and is also captured as a paraphrase. Phrase #6 propagates translation candidates to phrase #8 through phrase #7. Morphological variants of phrase #6 (shown in bold) also receive translation candidates through graph propagation giving translation candidates for morphologically rich OOVs. Figure 1: English paraphrases extracted by pivoting over German shared translation (Bannard and Callison-Burch, 2005). where t is a phrase in language T. p(f1|t) and p(t|f2) are taken from the phrase table extracted from parallel data for languages F and T. In Fig. 1 from (Bannard and Callison-Burch, 2005) we see that paraphrase pairs like (in check, under control) can be extracted by pivoting over the German phrase unter kontrolle. The multilingual Paraphrase Database (PPDB) (Ganitkevitch and Callison-Burch, 2014) is a published resource for paraphrases extracted using bilingual pivoting. It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of f</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Eric Nichols</author>
<author>Darren Scott Appling</author>
<author>Michael Paul</author>
</authors>
<title>Improving statistical machine translation by paraphrasing the training data. In</title>
<date>2008</date>
<booktitle>IWSLT</booktitle>
<contexts>
<context position="32337" citStr="Bond et al., 2008" startWordPosition="5371" endWordPosition="5374">MT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine an</context>
</contexts>
<marker>Bond, Nichols, Appling, Paul, 2008</marker>
<rawString>Francis Bond, Eric Nichols, Darren Scott Appling, and Michael Paul. 2008. Improving statistical machine translation by paraphrasing the training data. In IWSLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="1328" citStr="Callison-Burch et al., 2006" startWordPosition="189" endWordPosition="192">show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional profiles from monolingual source data. 1 Introduction Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text. In (Callison-Burch et al., 2006), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%. The problem is worse with multi-word OOV phrases. Copying OOVs to the output is the most common solution. However, even noisy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we sho</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>H Daum´e Alexander Fraser</author>
<author>Chris Quirk</author>
<author>Fabienne Braune</author>
</authors>
<title>Domain adaptation in machine translation: Final report.</title>
<date>2012</date>
<booktitle>In 2012 Johns Hopkins Summer Workshop.</booktitle>
<location>Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Aleˇs Tamchyna, Katharine</location>
<contexts>
<context position="30633" citStr="Carpuat et al., 2012" startWordPosition="5111" endWordPosition="5114"> Adaptation Domain adaptation is another case that suffers from massive number of OOVs. We compare our approach with Marginal Matching (Irvine et al., 2013), a state of the art approach in SMT domain adaptation. We use their setup and data and compare our results to their reported results (Irvine et al., 2013). 250K lines of Hansard parliamentary proceeding are used for training MT. Dev and test sets are available for two different domains: Medical and Science domains. For medical domain random subset of EMEA corpus (Tiedemann, 2009) and for the science domain a corpus of scientific articles (Carpuat et al., 2012) has been used. Unigram paraphrases using DP are extracted from French side of Europarl. Table 6 compares the results in terms of BLEU score. In both medical and science domains, graph-propagation approach using PPDB (large) performs significantly better than DP (p &lt; 0.02), and has comparable results to Marginal Matching. Systems Science Medical baseline 22.20 25.32 DP-tripartite 22.76 25.81 PPDBfr (L)-tripartite 22.97 27.11 Marginal Matching 23.62 26.97 Table 6: BLEU scores for domain adaptation. Systems BLEU baseline 29.59 DP-tripartite 30.08 PPDBarabic (L)-tripartite 31.12 Table 7: BLEU sco</context>
</contexts>
<marker>Carpuat, Fraser, Quirk, Braune, 2012</marker>
<rawString>Marine Carpuat, H Daum´e III, Alexander Fraser, Chris Quirk, Fabienne Braune, Ann Clifton, Ann Irvine, Jagadeesh Jagarlamudi, John Morgan, Majid Razmara, Aleˇs Tamchyna, Katharine Henry, and Rachel Rudinger. 2012. Domain adaptation in machine translation: Final report. In 2012 Johns Hopkins Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="28149" citStr="Clark et al., 2011" startWordPosition="4692" endWordPosition="4695">hows percentage of gold list covered by the candidate list (Eqn. 6). Gold translations for OOVs are given by concatenating the test data to training and running a word aligner. 1 for O = {OOVs} (5) Recall = |{gold list} n {candidate list} |(6) |{gold list}| Table 5 compares DP and PPDB in terms of BLEU, MRR and Recall. It indicates that PPDB (large size) outperforms DP in both intrinsic and extrinsic evaluation measures. Although tripartite graph did not improve the results for DP, it results in statistically significantly better BLEU score for PPDB in comparison to DP (evaluated by MultEval (Clark et al., 2011)). Thus we use tripartite graph in the rest of experiments. The last row in the table shows the result of combining DP and PPDB by multiplying the normalized scores of both paraphrase lists. This setting is included for three reasons: 1) we exploit the small data size to explore different choices in our approach such as, e.g. choosing bipartite versus tripartite graph structures; 2) 4http://www.statmt.org/wpt05/mt-shared-task/ 1 MRR = |O| � |O| i=1 ranki 1385 OOV PPDB NNs DP NNs Reference sentence PPDB output DP output proc´ed´es processus m´ethodes ... an agreement on proce- ... an agreement </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="23953" citStr="Crammer and Singer, 2003" startWordPosition="3979" endWordPosition="3983">in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages. In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSin</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In ACL</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinhua Du</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>Facilitating translation using source language paraphrase lattices.</title>
<date>2010</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="32533" citStr="Du et al., 2010" startWordPosition="5405" endWordPosition="5408">to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contras</context>
</contexts>
<marker>Du, Jiang, Way, 2010</marker>
<rawString>Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating translation using source language paraphrase lattices. In EMNLP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="23737" citStr="Dyer et al., 2010" startWordPosition="3940" endWordPosition="3943"> We varied the number of iterations from 1 to 10 on a held-out dev set and found that 5 iterations was optimal. 5 Evaluation We first show the effect of OOVs on translation quality, then evaluate our approach in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages. In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<date>2013</date>
<journal>A Simple, Fast, and Effective Reparameterization of IBM Model</journal>
<booktitle>In NAACL HLT</booktitle>
<volume>2</volume>
<contexts>
<context position="23826" citStr="Dyer et al., 2013" startWordPosition="3956" endWordPosition="3959">terations was optimal. 5 Evaluation We first show the effect of OOVs on translation quality, then evaluate our approach in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages. In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Se</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A Simple, Fast, and Effective Reparameterization of IBM Model 2. In NAACL HLT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The multilingual paraphrase database.</title>
<date>2014</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="3267" citStr="Ganitkevitch and Callison-Burch (2014)" startWordPosition="506" endWordPosition="509">014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked 1379 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1379–1390, Lisbon, Portugal, 17-21 September 2015. c</context>
<context position="5957" citStr="Ganitkevitch and Callison-Burch, 2014" startWordPosition="923" endWordPosition="926">gation to transfer translation candidates in a way that is sensitive to SMT concerns. In our experiments (Sec. 5) we compare our approach with the state-of-the-art in three different settings in SMT: 1) when faced with limited amount of parallel training data; 2) a domain shift between training and test data; and 3) handling a morphologically complex source language. In each case, we show that our PPDB-based approach outperforms the distributional profile approach. 2 Paraphrase Extraction Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f, its distributional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fix</context>
<context position="10081" citStr="Ganitkevitch and Callison-Burch, 2014" startWordPosition="1633" endWordPosition="1636">receive translation candidates through graph propagation giving translation candidates for morphologically rich OOVs. Figure 1: English paraphrases extracted by pivoting over German shared translation (Bannard and Callison-Burch, 2005). where t is a phrase in language T. p(f1|t) and p(t|f2) are taken from the phrase table extracted from parallel data for languages F and T. In Fig. 1 from (Bannard and Callison-Burch, 2005) we see that paraphrase pairs like (in check, under control) can be extracted by pivoting over the German phrase unter kontrolle. The multilingual Paraphrase Database (PPDB) (Ganitkevitch and Callison-Burch, 2014) is a published resource for paraphrases extracted using bilingual pivoting. It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of features. These features can be used by a log linear model to score paraphrases (Zhao et al., 2008). We used a linear combination of these features using the equation in Sec. 3 of (Ganitkevitch and Callison-Burch, 2014) to score paraphrase pairs. PPDB version 1 is broken into different levels of coverage. The smaller sizes contain only better-scoring, high-precision paraphrases, while larger sizes aim</context>
</contexts>
<marker>Ganitkevitch, Callison-Burch, 2014</marker>
<rawString>Juri Ganitkevitch and Chris Callison-Burch. 2014. The multilingual paraphrase database. In LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="34849" citStr="Gao et al., 2014" startWordPosition="5776" endWordPosition="5779">logical variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. In CoNLL</title>
<date>2009</date>
<contexts>
<context position="2459" citStr="Garera et al., 2009" startWordPosition="380" endWordPosition="383"> focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphras</context>
</contexts>
<marker>Garera, Callison-Burch, Yarowsky, 2009</marker>
<rawString>Nikesh Garera, Chris Callison-Burch, and David Yarowsky. 2009. Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. In CoNLL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four Techniques for Online Handling of Out-of-Vocabulary Words in ArabicEnglish Statistical Machine Translation. In</title>
<date>2008</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="34178" citStr="Habash, 2008" startWordPosition="5663" endWordPosition="5664">n two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingua</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four Techniques for Online Handling of Out-of-Vocabulary Words in ArabicEnglish Statistical Machine Translation. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="2250" citStr="Haghighi et al., 2008" startWordPosition="347" endWordPosition="350">anguage model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2</context>
<context position="32673" citStr="Haghighi et al. (2008)" startWordPosition="5427" endWordPosition="5430">which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similari</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In WMT</booktitle>
<contexts>
<context position="24911" citStr="Heafield, 2011" startWordPosition="4144" endWordPosition="4145">y, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF Lager stock iter1 iter2 iter3 bank margin majority 1384 Experiments OOV type/token Rules added Case 1 1830 / 2163 7.0K Case 2 - Med. 2294 / 4190 7.8K Case 2 - Sci. 5272 / 14121 10.4K Case 3 1543 / 1895 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 2011) is used to train a 5- gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework3. We use maximum phrase length 10. For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM). Each graph propagation iteration takes about 3 minutes. For French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In WMT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Supervised bilingual lexicon induction with multiple monolingual signals.</title>
<date>2013</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="32960" citStr="Irvine and Callison-Burch, 2013" startWordPosition="5471" endWordPosition="5474">al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 20</context>
</contexts>
<marker>Irvine, Callison-Burch, 2013</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2013. Supervised bilingual lexicon induction with multiple monolingual signals. In NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Hallucinating phrase translations for low resource MT.</title>
<date>2014</date>
<publisher>CoNLL-2014.</publisher>
<contexts>
<context position="1767" citStr="Irvine and Callison-Burch, 2014" startWordPosition="265" endWordPosition="268">Introduction Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text. In (Callison-Burch et al., 2006), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%. The problem is worse with multi-word OOV phrases. Copying OOVs to the output is the most common solution. However, even noisy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation</context>
<context position="32811" citStr="Irvine and Callison-Burch (2014" startWordPosition="5447" endWordPosition="5450">eference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some featur</context>
</contexts>
<marker>Irvine, Callison-Burch, 2014</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2014a. Hallucinating phrase translations for low resource MT. CoNLL-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Using comparable corpora to adapt mt models to new domains. ACL</title>
<date>2014</date>
<contexts>
<context position="1767" citStr="Irvine and Callison-Burch, 2014" startWordPosition="265" endWordPosition="268">Introduction Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text. In (Callison-Burch et al., 2006), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%. The problem is worse with multi-word OOV phrases. Copying OOVs to the output is the most common solution. However, even noisy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation</context>
<context position="32811" citStr="Irvine and Callison-Burch (2014" startWordPosition="5447" endWordPosition="5450">eference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some featur</context>
</contexts>
<marker>Irvine, Callison-Burch, 2014</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2014b. Using comparable corpora to adapt mt models to new domains. ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Quirk</author>
<author>Hal Daum´e</author>
</authors>
<title>Monolingual marginal matching for translation model adaptation. In</title>
<date>2013</date>
<booktitle>EMNLP</booktitle>
<marker>Irvine, Quirk, Daum´e, 2013</marker>
<rawString>Ann Irvine, Chris Quirk, and Hal Daum´e III. 2013. Monolingual marginal matching for translation model adaptation. In EMNLP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="32252" citStr="Kauchak and Barzilay, 2006" startWordPosition="5357" endWordPosition="5360">due to pivoting over English. We choose Arabic-English task for this experiment. We train the SMT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table b</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In NAACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="2226" citStr="Koehn and Knight, 2002" startWordPosition="343" endWordPosition="346">improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al.</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL 2002 workshop on unsupervised lexical acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="18865" citStr="Koehn et al., 2003" startWordPosition="3123" endWordPosition="3126">eliable edges is useful. As seen in Table 1, increasing the size of PPDB leads to a rapid increase in nodes followed by a larger number of edges in the very large PPDB sizes. 4.1.2 Pruning the translation candidates Another solution to the error propagation issue is to propagate all translation candidates but when providing translations to OOVs in the final phrase BLEU score 30 29.5 29 Base S M L XL Figure 3: Effect of PPDB size on improving BLEU score for Spanish and French table to eliminate all but the top L translations for each phrase (which is the usual ttable limit in phrase-based SMT (Koehn et al., 2003)). Based on a development set, separate from the test sets we used, we found that the best value of L was 10. 4.1.3 External Resources for Filtering Applying more informative filters can be also used to improve paraphrase quality. This can be done through additional features for paraphrase pairs. For example, edit distance can be used to capture misspelled paraphrases. We use a Named Entity Recognizer to exclude names, numbers and dates from the paraphrase candidates. Even after removing these tokens, 3.32% of tokens of test set are still OOVs . In addition, we use a list of stop words to remo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="24260" citStr="Koehn et al., 2007" startWordPosition="4033" endWordPosition="4036">rpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF Lager stock iter1 iter2 iter3 bank margin majority 1384 Experiments OOV type/token Rules added Case 1 1830 / 2163 7.0K Case 2 - Med. 2294 / 4190 7.8K Case 2 - Sci. 5272 / 14121 10.4K Case 3 1543 / 1895 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the ph</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, and et al. 2007. Moses: open source toolkit for statistical machine translation. In ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit</booktitle>
<volume>5</volume>
<contexts>
<context position="25989" citStr="Koehn, 2005" startWordPosition="4325" endWordPosition="4326">zed in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results, this works very well in our data. For Arabic, AQMAR is used to exclude named-entities (Mohit et al., 2012). For each of the experimental settings below we show the OOV statistics in Table 2. 5.2 Impact of OOVs: Oracle experiment This oracle experiment shows that translation of OOVs beyond named entities, dates, etc. is potentially very useful in improving output translation. We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7) (Koehn, 2005). WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). 5.3 Case 1: Limited Parallel Data In this experiment we use a setup similar to (Razmara </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit 2005, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="7024" citStr="Lin, 1998" startWordPosition="1104" endWordPosition="1105">ional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI). For each potential context word wi: P(f, wi) A(f, wi) = log2 (1) P(f)P(wi) To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is: S(f1, f2) = cos(DP(f1), DP(f2)) = E wi∈V A(f1, wi)A(f2, wi) &apos; Ewi∈V A(f1, wi)2&apos; Ewi∈V A(f2, wi)2 where V is the vocabulary. Note that in Eqn. (2) wi’s are the words that appear in the context of f1 or f2, otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; Razmar</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In WMT</booktitle>
<contexts>
<context position="32223" citStr="Madnani et al., 2007" startWordPosition="5353" endWordPosition="5356"> Even more so in PPDB due to pivoting over English. We choose Arabic-English task for this experiment. We train the SMT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate </context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie J Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In WMT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages. In</title>
<date>2001</date>
<booktitle>NAACL</booktitle>
<contexts>
<context position="3096" citStr="Mann and Yarowsky, 2001" startWordPosition="482" endWordPosition="485">n used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolingually-derived paraphrases.</title>
<date>2009</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="2548" citStr="Marton et al., 2009" startWordPosition="397" endWordPosition="400"> that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-B</context>
<context position="6827" citStr="Marton et al., 2009" startWordPosition="1069" endWordPosition="1072">a et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f, its distributional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI). For each potential context word wi: P(f, wi) A(f, wi) = log2 (1) P(f)P(wi) To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is: S(f1, f2) = cos(DP(f1), DP(f2)) = E wi∈V A(f1, wi)A(f2, wi) &apos; Ewi∈V A(f1, wi)2&apos; Ewi∈V A(f2, wi)2 where V is the vocabulary. Note that in Eqn. (2) wi’s are the words that appear in the context of </context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolingually-derived paraphrases. In EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="34675" citStr="Mikolov et al., 2013" startWordPosition="5744" endWordPosition="5747">nglish by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear proj</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="34675" citStr="Mikolov et al., 2013" startWordPosition="5744" endWordPosition="5747">nglish by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear proj</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Ido Dagan</author>
<author>Marc Dymetman</author>
<author>Idan Szpektor</author>
</authors>
<title>Source-language entailment modeling for translating unknown terms.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP</booktitle>
<contexts>
<context position="32372" citStr="Mirkin et al., 2009" startWordPosition="5377" endWordPosition="5380"> (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable</context>
</contexts>
<marker>Mirkin, Specia, Cancedda, Dagan, Dymetman, Szpektor, 2009</marker>
<rawString>Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, and Idan Szpektor. 2009. Source-language entailment modeling for translating unknown terms. In ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Nathan Schneider</author>
<author>Rishav Bhowmick</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Recall-oriented learning of named entities in arabic wikipedia.</title>
<date>2012</date>
<booktitle>In EACL 2012,</booktitle>
<pages>162--173</pages>
<contexts>
<context position="25619" citStr="Mohit et al., 2012" startWordPosition="4265" endWordPosition="4268">able graph propagation we use the Junto framework3. We use maximum phrase length 10. For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM). Each graph propagation iteration takes about 3 minutes. For French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results, this works very well in our data. For Arabic, AQMAR is used to exclude named-entities (Mohit et al., 2012). For each of the experimental settings below we show the OOV statistics in Table 2. 5.2 Impact of OOVs: Oracle experiment This oracle experiment shows that translation of OOVs beyond named entities, dates, etc. is potentially very useful in improving output translation. We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7) (Koehn, 2005). WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the resul</context>
</contexts>
<marker>Mohit, Schneider, Bhowmick, Oflazer, Smith, 2012</marker>
<rawString>Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, and Noah A Smith. 2012. Recall-oriented learning of named entities in arabic wikipedia. In EACL 2012, pages 162–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from nonparallel corpora.</title>
<date>2006</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="34424" citStr="Munteanu and Marcu, 2006" startWordPosition="5702" endWordPosition="5705">pare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate transl</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from nonparallel corpora. In ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved statistical machine translation using monolingual paraphrases.</title>
<date>2008</date>
<booktitle>In ECAI 2008: 18th European Conference on Artificial Intelligence.</booktitle>
<publisher>IOS Press.</publisher>
<contexts>
<context position="32350" citStr="Nakov, 2008" startWordPosition="5375" endWordPosition="5376">entence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Bu</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improved statistical machine translation using monolingual paraphrases. In ECAI 2008: 18th European Conference on Artificial Intelligence. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="24304" citStr="Och and Ney, 2003" startWordPosition="4041" endWordPosition="4044">Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF Lager stock iter1 iter2 iter3 bank margin majority 1384 Experiments OOV type/token Rules added Case 1 1830 / 2163 7.0K Case 2 - Med. 2294 / 4190 7.8K Case 2 - Sci. 5272 / 14121 10.4K Case 3 1543 / 1895 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="24344" citStr="Och, 2003" startWordPosition="4050" endWordPosition="4051">10) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF Lager stock iter1 iter2 iter3 bank margin majority 1384 Experiments OOV type/token Rules added Case 1 1830 / 2163 7.0K Case 2 - Med. 2294 / 4190 7.8K Case 2 - Sci. 5272 / 14121 10.4K Case 3 1543 / 1895 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 2011) is used to train a 5- gram langu</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Onishi</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Paraphrase lattice for statistical machine translation.</title>
<date>2010</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="32515" citStr="Onishi et al., 2010" startWordPosition="5401" endWordPosition="5404">d LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adap</context>
</contexts>
<marker>Onishi, Utiyama, Sumita, 2010</marker>
<rawString>Takashi Onishi, Masao Utiyama, and Eiichiro Sumita. 2010. Paraphrase lattice for statistical machine translation. In ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="2390" citStr="Rapp, 1995" startWordPosition="370" endWordPosition="371">nd remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual</context>
<context position="6375" citStr="Rapp, 1995" startWordPosition="992" endWordPosition="993">tional profile approach. 2 Paraphrase Extraction Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f, its distributional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In ACL 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Maryam Siahbani</author>
<author>Reza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation.</title>
<date>2013</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="2832" citStr="Razmara et al., 2013" startWordPosition="436" endWordPosition="439">d Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 </context>
<context position="6222" citStr="Razmara et al., 2013" startWordPosition="966" endWordPosition="969">een training and test data; and 3) handling a morphologically complex source language. In each case, we show that our PPDB-based approach outperforms the distributional profile approach. 2 Paraphrase Extraction Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f, its distributional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al.,</context>
<context position="7638" citStr="Razmara et al., 2013" startWordPosition="1212" endWordPosition="1216"> 1998) (PMI). For each potential context word wi: P(f, wi) A(f, wi) = log2 (1) P(f)P(wi) To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is: S(f1, f2) = cos(DP(f1), DP(f2)) = E wi∈V A(f1, wi)A(f2, wi) &apos; Ewi∈V A(f1, wi)2&apos; Ewi∈V A(f2, wi)2 where V is the vocabulary. Note that in Eqn. (2) wi’s are the words that appear in the context of f1 or f2, otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase. 2.2 Paraphrases from bilingual pivoting Bilingual pivoting uses parallel corpora between the source language, F, and a pivot language T. If two phrases, f1 and f2, in a same language are paraphrases, then they share a translation in other languages with p(f1|f2) as a paraphrase score: S(f1, f2) = p(f1|f2) = � p(f1|t)p(t|f2) (3) t (2) 1380 ... Source Target P (e |f) Phrase table (trk) كرﺗ جرﺧ بھذ بھ</context>
<context position="11314" citStr="Razmara et al., 2013" startWordPosition="1816" endWordPosition="1819">verage. Algorithm 1 PPDB Graph Propagation for SMT PhrTable = PhraseTableGeneration(); ParaDB = ParaphraseExtraction(); (Sec. 2) InitGraph = GraphConstruct(PhrTable, ParaDB); (Sec. 3.1) PropGraph = GraphPropagation(InitGraph); (Sec. 3.2) for phrase ∈ {OOVs} do newTrans = TranslationFinder(PropGraph, phrase); Augment(PhrTable, newTrans); (Sec. 3.3) TuneMT(PhrTable); 3 Methodology After paraphrase extraction we have paraphrase pairs, (f1, f2) and a score 5(f1, f2) we can induce new translation rules for OOV phrases using the steps in Algo. (1): 1) A graph of source phrases is constructed as in (Razmara et al., 2013); 2) translations are propagated as labels through the graph as explained in Fig. 2; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table. 3.1 Graph Construction We construct a graph G(V, E, W) over all source phrases in the paraphrase database and the source language phrases from the SMT phrase table extracted from the available parallel data. V corresponds to the set of vertices (source phrases), E is the set of edges between phrases and W is weight of each using the score function 5 defined in Sec. 2. V has two types of nodes: seed (labe</context>
<context position="14423" citStr="Razmara et al., 2013" startWordPosition="2358" endWordPosition="2361">Am+1 is our prior belief about labeling. First component of the function tries to minimize the difference of new distribution to the original distribution for the seed nodes. The second component insures that nearby neighbours have similar distributions, and the final component is to make sure that the distribution does not stray from a prior distribution. At the end of propagation, we wish to find a label distribution for our OOV phrases. We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in (Razmara et al., 2013). The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2). 3.3 Phrase Table Integration After propagation, for each potential OOV phrase we have a list of possible translatio</context>
<context position="19928" citStr="Razmara et al. (2013)" startWordPosition="3306" endWordPosition="3310">rom the paraphrase candidates. Even after removing these tokens, 3.32% of tokens of test set are still OOVs . In addition, we use a list of stop words to remove nodes which have too many connections. These two filters improve our results (more in Sec. 5). 4.2 Path sensitivity Graph propagation has been used in many NLP tasks like POS tagging, parsing, etc. but propagating translations in a graph as labels is much more challenging. Due to huge number of possible labels (translations) and many low quality edges, it is very likely that many wrong translations are rapidly propagated in few steps. Razmara et al. (2013) show that unlabeled nodes inside the graph, called bridge nodes, are useful for the transfer of translations when there is no other connection between an OOV phrase and a node with known translation candidates. However, they show that using the full graph with long paths of bridge nodes hurts performance. Thus the propagation has to be constrained using path sensitivity. Fig. 4 shows this issue in a part of an English paraSpanish - English French - English 1383 Figure 4: Sensitivity issue in graph propagation for translations. “Lager” is a translation candidate for “stock”, which is transferr</context>
<context position="21520" citStr="Razmara et al. (2013)" startWordPosition="3565" endWordPosition="3568">fully connected graph structure. They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to Razmara et al. (2013). In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. 4.2.2 Graph random walks Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights. Optimization for the Modified Adsorption (MAD) objective function in Sec. 3.2 can be viewed as a controlled random walk (Talukdar et al., 2008; Talukdar and Crammer, 2009). This is formalized as three actions: inject, continue and abandon with corresponding pre-defined probabilities Pinj, Pcont and Pabnd respectively</context>
<context position="24173" citStr="Razmara et al. (2013)" startWordPosition="4017" endWordPosition="4020"> SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF Lager stock iter1 iter2 iter3 bank margin majority 1384 Experiments OOV type/token Rules added Case 1 1830 / 2163 7.0K Case 2 - Med. 2294 / 4190 7.8K Case 2 - Sci. 5272 / 14121 10.4K Case 3 1543 / 1895 8.1K Tabl</context>
<context position="26602" citStr="Razmara et al., 2013" startWordPosition="4436" endWordPosition="4440">n, 2005). WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). 5.3 Case 1: Limited Parallel Data In this experiment we use a setup similar to (Razmara et al., 2013). To have fair comparison, 3Junto : https://github.com/parthatalukdar/junto Fr-En Dev Test Baseline 27.90 28.08 + Lexical OOV 28.10 28.31 + Phrasal OOV 28.50 28.85 Fully observed 46.88 49.21 Table 4: The impact of translating OOVs. we use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in (Razmara et al., 2013). ACL/WMT 20054 is used for dev and test data. We re-implement their paraphrase extraction method (DP) to extract paraphrases from French side of Europarl (2M sentences). We use unigram nodes to construct graphs for both DP an</context>
</contexts>
<marker>Razmara, Siahbani, Haffari, Sarkar, 2013</marker>
<rawString>Majid Razmara, Maryam Siahbani, Reza Haffari, and Anoop Sarkar. 2013. Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation. In ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Hany Hassan</author>
<author>Kristina Toutanova</author>
<author>Chris Quirk</author>
</authors>
<title>Graph-based semi-supervised learning of translation models from monolingual data. In</title>
<date>2014</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="2853" citStr="Saluja et al., 2014" startWordPosition="440" endWordPosition="443">hi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to </context>
<context position="6244" citStr="Saluja et al., 2014" startWordPosition="970" endWordPosition="973">data; and 3) handling a morphologically complex source language. In each case, we show that our PPDB-based approach outperforms the distributional profile approach. 2 Paraphrase Extraction Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f, its distributional profile is: DP(f) = {(A(f,wi)) |wi E V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al.,</context>
<context position="7660" citStr="Saluja et al., 2014" startWordPosition="1217" endWordPosition="1220"> potential context word wi: P(f, wi) A(f, wi) = log2 (1) P(f)P(wi) To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is: S(f1, f2) = cos(DP(f1), DP(f2)) = E wi∈V A(f1, wi)A(f2, wi) &apos; Ewi∈V A(f1, wi)2&apos; Ewi∈V A(f2, wi)2 where V is the vocabulary. Note that in Eqn. (2) wi’s are the words that appear in the context of f1 or f2, otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase. 2.2 Paraphrases from bilingual pivoting Bilingual pivoting uses parallel corpora between the source language, F, and a pivot language T. If two phrases, f1 and f2, in a same language are paraphrases, then they share a translation in other languages with p(f1|f2) as a paraphrase score: S(f1, f2) = p(f1|f2) = � p(f1|t)p(t|f2) (3) t (2) 1380 ... Source Target P (e |f) Phrase table (trk) كرﺗ جرﺧ بھذ بھذ بھذ بھذ (*hb&amp;) 0.1 (</context>
<context position="14506" citStr="Saluja et al., 2014" startWordPosition="2371" endWordPosition="2374">imize the difference of new distribution to the original distribution for the seed nodes. The second component insures that nearby neighbours have similar distributions, and the final component is to make sure that the distribution does not stray from a prior distribution. At the end of propagation, we wish to find a label distribution for our OOV phrases. We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in (Razmara et al., 2013). The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2). 3.3 Phrase Table Integration After propagation, for each potential OOV phrase we have a list of possible translations with corresponding probabilities. A potential OOV is any phrase which does not a</context>
<context position="33511" citStr="Saluja et al. (2014)" startWordPosition="5556" endWordPosition="5559">are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrase</context>
<context position="35361" citStr="Saluja et al. (2014)" startWordPosition="5863" endWordPosition="5866">equent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b). 7 Conclusion and Future work In future work, we would like to include translations for infrequent phrases which are not OOVs. We would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity. We als</context>
</contexts>
<marker>Saluja, Hassan, Toutanova, Quirk, 2014</marker>
<rawString>Avneesh Saluja, Hany Hassan, Kristina Toutanova, and Chris Quirk. 2014. Graph-based semi-supervised learning of translation models from monolingual data. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL</title>
<date>2002</date>
<contexts>
<context position="2202" citStr="Schafer and Yarowsky, 2002" startWordPosition="339" endWordPosition="342">sy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relat</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="34444" citStr="Smith et al., 2010" startWordPosition="5706" endWordPosition="5709">a et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Koby Crammer</author>
</authors>
<title>New Regularized Algorithms for Transductive Learning.</title>
<date>2009</date>
<booktitle>In European Conference on Machine Learning.</booktitle>
<contexts>
<context position="13358" citStr="Talukdar and Crammer, 2009" startWordPosition="2172" endWordPosition="2175">able and we propagate this distribution to unlabeled nodes in the graph. 3.2 Graph Propagation Considering the translation candidates of known phrases in the SMT phrase table as the “labels” we apply a soft label propagation algorithm in order to assign translation candidates to “unlabeled” nodes in the graph, which include our OOV phrases. As described by the example in Fig. 2 we wish two outcomes: 1) transfer of translations (or “labels”) to unlabeled nodes (OOV phrases) from labeled nodes, and 2) smoothing the label distribution at each node. We use the Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009) for graph propagation. Suppose we have m different possible labels plus one dummy label, a soft label Yˆ E Am+1 is a m + 1 dimension probability vector. The dummy label is used when there is low confidence on correct labels. Based on MAD, we want to find soft label vectors for each node by optimizing the objective function below: �µ3 P3,v||ˆYv − Rv||22 vEV In this objective function, µi and Pi,v are hyperparameters (Vv : EiPi,v = 1). Rv E Am+1 is our prior belief about labeling. First component of the function tries to minimize the difference of new distribution to the original distribution f</context>
<context position="21973" citStr="Talukdar and Crammer, 2009" startWordPosition="3638" endWordPosition="3641">pe (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to Razmara et al. (2013). In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. 4.2.2 Graph random walks Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights. Optimization for the Modified Adsorption (MAD) objective function in Sec. 3.2 can be viewed as a controlled random walk (Talukdar et al., 2008; Talukdar and Crammer, 2009). This is formalized as three actions: inject, continue and abandon with corresponding pre-defined probabilities Pinj, Pcont and Pabnd respectively as in (Talukdar and Crammer, 2009). A random walk through the graph will transfer labels from one node to another node, and probabilities Pcont and Pabnd control exploration of the graph. By reducing the values of Pcont and increasing Pabnd we can control the label propagation process to optimize the quality of translations for OOV phrases. Again, this is done on a held-out development set and not on the test data. The optimal values in our experim</context>
</contexts>
<marker>Talukdar, Crammer, 2009</marker>
<rawString>Partha Pratim Talukdar and Koby Crammer. 2009. New Regularized Algorithms for Transductive Learning. In European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
</authors>
<title>Topics in graph construction for semi-supervised learning.</title>
<date>2009</date>
<tech>Technical Report MS-CIS-09-13,</tech>
<institution>University of Pennsylvania, Dept of Computer and Info. Sci.</institution>
<contexts>
<context position="17611" citStr="Talukdar, 2009" startWordPosition="2900" endWordPosition="2901">ult in poor translations for OOVs. We could address this issue by aggressively pruning the potential paraphrase candidates to improve the precision. However, this results in a dramatic drop in coverage and many OOV phrases do not obtain any translation candidates. We use a combination of the following three steps to augment our graph propagation framework. 4.1.1 Graph pruning and PPDB sizes Pruning the graph avoids error propagation by removing unreliable edges. Pruning removes edges with an edge weight lower than a minimum threshold or by limiting the number of neighbours to the top-K edges (Talukdar, 2009). PPDB has different sizes with different levels of accuracy and coverage. We can do graph pruning simply by choosing to use different sizes of PPDB. As we can see in Fig. 3 results vary from language to language depending on the pruning used. For instance, the L size results in the best score for French-English. We choose the best size of PPDB for each language based on a separate held-out set and independently from each of the SMT-based tasks in our experimental results. Our conclusion from our experiments with the different sizes of PPDB is that removing phrases (or nodes in our graph) is n</context>
</contexts>
<marker>Talukdar, 2009</marker>
<rawString>Partha Pratim Talukdar. 2009. Topics in graph construction for semi-supervised learning. Technical Report MS-CIS-09-13, University of Pennsylvania, Dept of Computer and Info. Sci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL</booktitle>
<contexts>
<context position="34466" citStr="Tamura et al., 2012" startWordPosition="5710" endWordPosition="5713">use they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts </context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In EMNLP-CoNLL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS-A collection of multilingual parallel corpora with tools and interfaces. In Recent advances in natural language processing.</title>
<date>2009</date>
<contexts>
<context position="30551" citStr="Tiedemann, 2009" startWordPosition="5099" endWordPosition="5100">aphrases for a true resource poor language, e.g. Malagasy. 5.4 Case 2: Domain Adaptation Domain adaptation is another case that suffers from massive number of OOVs. We compare our approach with Marginal Matching (Irvine et al., 2013), a state of the art approach in SMT domain adaptation. We use their setup and data and compare our results to their reported results (Irvine et al., 2013). 250K lines of Hansard parliamentary proceeding are used for training MT. Dev and test sets are available for two different domains: Medical and Science domains. For medical domain random subset of EMEA corpus (Tiedemann, 2009) and for the science domain a corpus of scientific articles (Carpuat et al., 2012) has been used. Unigram paraphrases using DP are extracted from French side of Europarl. Table 6 compares the results in terms of BLEU score. In both medical and science domains, graph-propagation approach using PPDB (large) performs significantly better than DP (p &lt; 0.02), and has comparable results to Marginal Matching. Systems Science Medical baseline 22.20 25.32 DP-tripartite 22.76 25.81 PPDBfr (L)-tripartite 22.97 27.11 Marginal Matching 23.62 26.97 Table 6: BLEU scores for domain adaptation. Systems BLEU ba</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS-A collection of multilingual parallel corpora with tools and interfaces. In Recent advances in natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Chengqing Zong</author>
</authors>
<title>Learning a phrase-based translation model from monolingual data with application to domain adaptation.</title>
<date>2013</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="33060" citStr="Zhang and Zong (2013)" startWordPosition="5487" endWordPosition="5490"> al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph con</context>
</contexts>
<marker>Zhang, Zong, 2013</marker>
<rawString>Jiajun Zhang and Chengqing Zong. 2013. Learning a phrase-based translation model from monolingual data with application to domain adaptation. In ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Feifei Zhai</author>
<author>Chengqing Zong</author>
</authors>
<title>Handling unknown words in statistical machine translation from a new perspective.</title>
<date>2012</date>
<booktitle>In Natural Language Processing and Chinese Computing.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1669" citStr="Zhang et al., 2012" startWordPosition="249" endWordPosition="252">method outperforms the use of distributional profiles from monolingual source data. 1 Introduction Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text. In (Callison-Burch et al., 2006), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%. The problem is worse with multi-word OOV phrases. Copying OOVs to the output is the most common solution. However, even noisy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributiona</context>
</contexts>
<marker>Zhang, Zhai, Zong, 2012</marker>
<rawString>Jiajun Zhang, Feifei Zhai, and Chengqing Zong. 2012. Handling unknown words in statistical machine translation from a new perspective. In Natural Language Processing and Chinese Computing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="34697" citStr="Zhang et al., 2014" startWordPosition="5748" endWordPosition="5751">al pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot approach for extracting paraphrase patterns from bilingual corpora.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="3205" citStr="Zhao et al., 2008" startWordPosition="499" endWordPosition="502">t al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked 1379 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processi</context>
<context position="10376" citStr="Zhao et al., 2008" startWordPosition="1680" endWordPosition="1683">se table extracted from parallel data for languages F and T. In Fig. 1 from (Bannard and Callison-Burch, 2005) we see that paraphrase pairs like (in check, under control) can be extracted by pivoting over the German phrase unter kontrolle. The multilingual Paraphrase Database (PPDB) (Ganitkevitch and Callison-Burch, 2014) is a published resource for paraphrases extracted using bilingual pivoting. It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of features. These features can be used by a log linear model to score paraphrases (Zhao et al., 2008). We used a linear combination of these features using the equation in Sec. 3 of (Ganitkevitch and Callison-Burch, 2014) to score paraphrase pairs. PPDB version 1 is broken into different levels of coverage. The smaller sizes contain only better-scoring, high-precision paraphrases, while larger sizes aim for high coverage. Algorithm 1 PPDB Graph Propagation for SMT PhrTable = PhraseTableGeneration(); ParaDB = ParaphraseExtraction(); (Sec. 2) InitGraph = GraphConstruct(PhrTable, ParaDB); (Sec. 3.1) PropGraph = GraphPropagation(InitGraph); (Sec. 3.2) for phrase ∈ {OOVs} do newTrans = Translation</context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot approach for extracting paraphrase patterns from bilingual corpora. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Hany Hassan</author>
<author>Michael Auli</author>
</authors>
<title>Learning translation models from monolingual continuous representations.</title>
<date>2015</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="2873" citStr="Zhao et al., 2015" startWordPosition="444" endWordPosition="447"> distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphra</context>
<context position="14526" citStr="Zhao et al., 2015" startWordPosition="2375" endWordPosition="2378">of new distribution to the original distribution for the seed nodes. The second component insures that nearby neighbours have similar distributions, and the final component is to make sure that the distribution does not stray from a prior distribution. At the end of propagation, we wish to find a label distribution for our OOV phrases. We describe in Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in (Razmara et al., 2013). The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2). 3.3 Phrase Table Integration After propagation, for each potential OOV phrase we have a list of possible translations with corresponding probabilities. A potential OOV is any phrase which does not appear in training, b</context>
<context position="35221" citStr="Zhao et al. (2015)" startWordPosition="5841" endWordPosition="5844">n applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b). 7 Conclusion and Future work In future work, we would like to include translations for infrequent phrases which are not OOVs. We woul</context>
</contexts>
<marker>Zhao, Hassan, Auli, 2015</marker>
<rawString>Kai Zhao, Hany Hassan, and Michael Auli. 2015. Learning translation models from monolingual continuous representations. In NAACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="34653" citStr="Zou et al., 2013" startWordPosition="5740" endWordPosition="5743">vement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Zou, Richard Socher, Daniel Cer, and Christopher Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In EMNLP 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
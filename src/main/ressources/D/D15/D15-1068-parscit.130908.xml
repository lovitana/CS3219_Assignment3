<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.99898">
Global Thread-Level Inference for Comment Classification
in Community Question Answering
</title>
<author confidence="0.864299">
Shafiq Joty, Alberto Barr´on-Cede˜no, Giovanni Da San Martino, Simone Filice,
</author>
<affiliation confidence="0.623955">
Lluis M`arquez, Alessandro Moschitti, and Preslav Nakov,
Qatar Computing Research Institute, HBKU
</affiliation>
<email confidence="0.765643">
{sjoty,albarron,gmartino,sfilice,
lmarquez,amoschitti,pnakov}@qf.org.qa
</email>
<sectionHeader confidence="0.993698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837523809524">
Community question answering, a recent
evolution of question answering in the
Web context, allows a user to quickly con-
sult the opinion of a number of people on
a particular topic, thus taking advantage
of the wisdom of the crowd. Here we
try to help the user by deciding automat-
ically which answers are good and which
are bad for a given question. In particular,
we focus on exploiting the output struc-
ture at the thread level in order to make
more consistent global decisions. More
specifically, we exploit the relations be-
tween pairs of comments at any distance
in the thread, which we incorporate in a
graph-cut and in an ILP frameworks. We
evaluated our approach on the benchmark
dataset of SemEval-2015 Task 3. Results
improved over the state of the art, confirm-
ing the importance of using thread level in-
formation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998262844827586">
Community question answering (CQA) is a recent
evolution of question answering, in the Web con-
text, where users pose questions and then receive
answers from other users. This setup is very at-
tractive, as the anonymity on the Web allows users
to ask just about anything and then hope to get
some honest answers from a number of people. On
the negative side, there is no guarantee about the
quality of the answers as people of very different
background, knowledge, and with different moti-
vation contribute answers to a given question.
Unlike traditional question answering (QA), in
CQA answering takes the form of commenting in
a forum. Thus, many comments are only loosely
connected to the original question, and some are
not answers at all, but are rather interactions be-
tween users.
As question-comment threads can get quite
long, finding good answers in a thread can be time-
consuming. This has triggered research in trying
to automatically determine which answers might
be good and which ones are likely to be bad or ir-
relevant. One early work going in this direction is
that of Qu and Liu (2011), who tried to determine
whether a question is “solved” or not, given its as-
sociated thread of comments. As a first step in the
process, they performed a comment-level classi-
fication, considering four classes: problem, solu-
tion, good feedback, and bad feedback.
More recently, the shared task at SemEval 2015
on Answer Selection in CQA (Nakov et al., 2015),
whose benchmark datasets we will use below,
tackled the task of identifying good, potentially
useful, and bad comments within a thread. In that
task, the top participating systems used thread-
level features, in addition to the usual local fea-
tures that only look at the question–answer pair.
For example, the second-best team, HITSZ-ICRC,
used as a feature the position of the comment in
the thread (Hou et al., 2015). Similarly, our par-
ticipation, which achieved the third-best postition,
used features that try to describe a comment in the
context of the entire comment thread, focusing on
user interaction (Nicosia et al., 2015). Finally, the
fifth-best team, ICRC-HIT, treated the answer se-
lection task as a sequence labeling problem and
proposed recurrent convolution neural networks to
recognize good comments (Zhou et al., 2015b).
In a follow-up work, Zhou et al. (2015a) in-
cluded a long-short term memory in their convo-
lution neural network to learn the classification se-
quence for the thread. In parallel, in our recent
work (Barr´on-Cede˜no et al., 2015), we tried to ex-
ploit the dependencies between the thread com-
ments to tackle the same task. We did it by design-
ing features that look globally at the thread and
by applying structured prediction models, such as
Conditional Random Fields (Lafferty et al., 2001).
</bodyText>
<page confidence="0.981738">
573
</page>
<note confidence="0.6585895">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 573–578,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99961209375">
Our goal in this paper goes in the same direction:
we are interested in exploiting the output structure
at the thread level to make more consistent global
assignments.
To the best of our knowledge, there is no work
in QA that identifies good answers based on the
selection of the other answers retrieved for a ques-
tion. This is mainly due to the loose dependencies
between the different answer passages in standard
QA. In contrast, we postulate that in a CQA set-
ting, the answers from different users in a com-
mon thread are strongly interconnected and, thus,
a joint answer selection model should be adopted
to achieve higher accuracy. In particular, we focus
on the relations between two comments at any dis-
tance in the thread. This is more general than pre-
vious approaches, which were either limited to se-
quential interactions or considered conversational
interactions only at the level of features.
We propose a model based on the idea that sim-
ilar comments should have similar labels. Below,
we apply graph-cut and we compare it to an inte-
ger linear programming (ILP) formulation for de-
coding under global constraints; we also provide
results with a linear-chain CRF. We show that the
CRF is ineffective due to long-distance relations,
e.g., a conversation in a thread can branch and then
come back later. On the contrary, the global infer-
ence models (either graph-cut or ILP) using the
similarity between pairs of comments manage to
significantly improve a strong baseline performing
local comment-based classifications.
</bodyText>
<sectionHeader confidence="0.954211" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.999778857142857">
We use the CQA-QL corpus from Subtask A of
SemEval-2015 Task 3 on Answer Selection in
CQA. The corpus contains data from the Qatar
Living forum,1 and is publicly available on the
task’s website.2 The dataset consists of ques-
tions and a list of answers for each question, i.e.,
question-answer threads. Each question, and each
answer, consist of a short title and a more de-
tailed description. There is also meta informa-
tion associated with both, e.g., ID of the user ask-
ing/answering the question, timestamp, category.
The task asks participants to determine for each
answer in the thread whether it is Good, Bad, or
Potentially useful for the given question.
</bodyText>
<footnote confidence="0.941419461538462">
1http://www.qatarliving.com/forum
2http://alt.qcri.org/semeval2015/task3/
Q: Can I obtain Driving License my QID is written Em-
ployee
A1 the word employee is a general term that refers to all the
staff in your company either the manager, secretary up
to the lowest position or whatever positions they have.
you are all considered employees of your company.
A2 your qid should specify what is the actual profession you
have. i think for me, your chances to have a drivers
license is low.
A3 dear richard, his asking if he can obtain. means he have
the driver license
</footnote>
<note confidence="0.500853">
A4 Slim chance ...
</note>
<figureCaption confidence="0.999486">
Figure 1: Example from SemEval-2015 Task 3.
</figureCaption>
<bodyText confidence="0.995084666666667">
A simplified example is shown in Figure 1,3
where answers 2 and 4 are good, answer 1 is po-
tentially useful, and answer 3 is bad. In this paper,
we focus on a 2-class variant of the above Sub-
task A, which is closer to a real CQA application.
We merge Potential and Bad labels into Bad and
we focus on the 2-class problem: Good-vs-Bad.
Table 1 shows some statistics about the resulting
dataset used for development, training and testing.
</bodyText>
<table confidence="0.9948814">
Category Train Dev Test
Questions 2,600 300 329
Comments 16,541 1,645 1,976
Good 8,069 875 997
Bad 8,472 770 979
</table>
<tableCaption confidence="0.999778">
Table 1: Statistics about the CQA-QL dataset:
</tableCaption>
<bodyText confidence="0.685215">
after merging Bad and Potential into Bad.
</bodyText>
<sectionHeader confidence="0.992745" genericHeader="method">
3 Our Proposed Solution
</sectionHeader>
<bodyText confidence="0.9999894375">
We model the pairwise relations between the com-
ments in the answer thread ({ci}ni=1) to produce a
better global assignment: we combine the predic-
tions of a Good-vs-Bad classifier at the comment
level with the output of a pairwise classifier, Same-
vs-Different, which takes two comments and pre-
dicts whether they should have the same label.
Each comment ci has an individual score siK,
provided by the Good-vs-Bad classifier, for being
in class K E {G, B} (i.e., G for Good and B
for Bad). Moreover, for each pair of comments
(ci, cj), we have an association score sij, an esti-
mate by the pairwise classifier about how likely it
is that the comments ci and cj will have the same
label. Next, we define two ways of doing global
inference using these two sources of information.
</bodyText>
<footnote confidence="0.7327035">
3http://www.qatarliving.com/moving-qatar/posts/can-i-
obtain-driving-license-my-qid-written-employee
</footnote>
<page confidence="0.991604">
574
</page>
<subsectionHeader confidence="0.994051">
3.1 Graph Partition Approach
</subsectionHeader>
<bodyText confidence="0.999989">
Here our goal is to find a partition P = (G, B)
that minimizes the following cost:
</bodyText>
<equation confidence="0.989948">
� �
siG +(1−λ)
ciEG,cjEB
</equation>
<bodyText confidence="0.999761928571429">
The first part of the cost function discourages mis-
classification of individual comments, while the
second part encourages similar comments to be in
the same class. The mixing parameter λ E [0, 1]
determines the relative strength of the two compo-
nents. Our approach is inspired by Pang and Lee
(2004), where they model the proximity relation
between sentences for finding subjective sentences
in product reviews, whereas we are interested in
global inference based on local classifiers.
The optimization problem can be efficiently
solved by finding a minimum cut of a weighted
undirected graph G = (V, E). The set of nodes
V = {v1, v2, · · · , vn, s, t} represent the n com-
ments in a thread, the source and the sink. We
connect each comment node vi to the source node
s by adding an edge w(vi, s) with capacity siG,
and to the sink node t by adding an edge w(vi, t)
with capacity siB. Finally, we add edges w(vi, vj)
with capacity sij to connect all pairs of comments.
Minimizing C(P) amounts to finding a parti-
tion (S, T), where S = {s}US&apos; and T = {t}UT&apos;
for s E/ S&apos;, t E/ T&apos;, that minimizes the cut capac-
ity, i.e., the net flow crossing from S to T. One
crucial advantage of this approach is that we can
use max-flow algorithms to find the exact solution
in polynomial time — near-linear in practice (Cor-
men et al., 2001; Boykov and Kolmogorov, 2004).
</bodyText>
<subsectionHeader confidence="0.997929">
3.2 Integer Linear Programming Approach
</subsectionHeader>
<bodyText confidence="0.999234814814815">
Here we follow the inference with classifiers ap-
proach by Roth and Yih (2004), solved with Inte-
ger Linear Programming (ILP). We have one ILP
problem per question–answer thread. We define a
set of binary variables, whose assignment will uni-
vocally define the classification of all comments in
the thread. In particular, we define a pair of vari-
ables for each answer: xiG and xiB, 1 &lt; i &lt; n.
Assigning 1 to xiG means that comment ci in the
thread is classified as Good; assigning it 0 means
that ci is not classified as Good. The same applies
to the other classes (here, only Bad). Also, we
have a pair of variables for each pair of comments
(to capture the pairwise relations): xijS and xijD,
1 &lt; i &lt; j &lt; n. Assigning 1 to xijS means that
ci and cj have the same label; assigning 0 to xijS
means that ci and cj do not have the same label.
The same interpretation holds for the other possi-
ble classes (in this case only Different).4
Let ciG be the cost of classifying ci as Good,
cijS be the cost of assigning the same labels to
ci and cj, etc. Following (Roth and Yih, 2004),
these costs are obtained from local classifiers by
taking log probabilities, i.e., ciG = − log siG,
cijS = − log sij, etc. The goal of the ILP prob-
lem is to find an assignment A to all variables xiG,
xiB, xijS, xijD that minimizes the cost function:
</bodyText>
<equation confidence="0.9980744">
N
C(A) = λ · (ciG · xiG + ciB · xiB) +
i=1
(1 − λ) · N−1� N (cijS · xijS + cijD · xijD)
i=1 j=i+1
</equation>
<bodyText confidence="0.99899425">
subject to the following constraints: (i) All vari-
ables are binary; (ii) One and only one label is
assigned to each comment or pair of comments;
(iii) The assignments to the comment variables
and to the comment-pair variables are consistent:
xijD = xiG ® xjG, bi, j 1 &lt; i &lt; j &lt; n.
λ E [0, 1] is a parameter used to balance the con-
tribution of the two sources of information.
</bodyText>
<sectionHeader confidence="0.996919" genericHeader="method">
4 Local Classifiers
</sectionHeader>
<bodyText confidence="0.99996325">
For classification, we use Maximum Entropy, or
MaxEnt, (Murphy, 2012), as it yields a probability
distribution over the class labels, which we then
use directly for the graph arcs and the ILP costs.
</bodyText>
<subsectionHeader confidence="0.967544">
4.1 Good-vs-Bad Classifier
</subsectionHeader>
<bodyText confidence="0.999665">
Our most important features measure the similar-
ity between the question (q) and the comment (c).
We compare lemmata and POS [1-4]-grams using
Jaccard (1901), containment (Lyon et al., 2001),
and cosine, as well as using some similarities from
DKPro (B¨ar et al., 2012) such as longest com-
mon substring (Allison and Dix, 1986) and greedy
string tiling (Wise, 1996). We also compute sim-
ilarity using partial tree kernels (Moschitti, 2006)
on shallow syntactic trees.
Forty-three Boolean features express whether
(i) c includes URLs or emails, the words “yes”,
“sure”, “no”, “neither”, “okay”, etc., as well as ‘?’
and ‘@’ or starts with “yes” (12 features); (ii) c
includes a word longer than fifteen characters (1);
</bodyText>
<footnote confidence="0.7398845">
4Setting a binary variable for each class label is necessary
to have an objective function that is linear on the labels.
</footnote>
<figure confidence="0.633304333333333">
C(P ) = λ � � �siB +
ciEG ciEB
sij
</figure>
<page confidence="0.989836">
575
</page>
<bodyText confidence="0.9999197">
(iii) q belongs to each of the forum categories (26);
and (iv) c and q were posted by the same user (4).
An extra feature captures the length of c.
Four features explore whether c is close to a
comment by the user who asked the question, uQ:
(i-ii) there is a comment by uQ following c and
(not) containing an acknowledgment or (iii) con-
taining a question, or (iv) among the comments
preceding c there is one by uQ asking a ques-
tion. We model dialogues by identifying conver-
sation chains between two users with three fea-
tures: whether c is at the beginning/middle/end
of a chain. There are copies of these features for
chains in which uQ participates. Another feature
for cu, checks whether the user ui wrote more than
one comment in the current thread. Three more
features fire for the first/middle/last comment by
ui. One extra feature counts the total number of
comments written by ui. Finally, there is a feature
modeling the position of c in the thread.
</bodyText>
<subsectionHeader confidence="0.982977">
4.2 Same-vs-Different Classifier
</subsectionHeader>
<bodyText confidence="0.999987363636364">
We use the following types of features for a pair
of comments (ci, cj): (i) all the features from
the Good-vs-Bad classifier (i.e., we subtracted the
feature vectors representing the two comments,
|vi − vj|)5; (ii) the similarity features between the
two comments, sim(ci, cj); and (iii) the predic-
tion from the Good-vs-Bad classifiers on ci and cj
(i.e., the scores for ci and cj, the product of the
two scores, and five boolean features specifying
whether any of ci and cj are predicted as Good,
Bad, and whether their predictions are identical).
</bodyText>
<sectionHeader confidence="0.994756" genericHeader="method">
5 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999956142857143">
We performed standard pre-processing, and we
further filtered user’s signatures. All parameters
(e.g., Gaussian prior for MaxEnt and the mixing
A for the graph-cut and ILP) were tuned on the
development set. We also trained a second-order
linear-chain CRF to check the contribution of the
sequential relations between comments. We re-
port results on the official SemEval test set for all
methods. For the Same-vs-Different problem, we
explored a variant of training with three classes,
by splitting the Same class into Same-Good and
Same-Bad. At test time, the probabilities of these
two subclasses are added to get the probability of
Same and all the algorithms are run unchanged.
</bodyText>
<footnote confidence="0.981137333333333">
5Subtracting vectors is standard in preference learn-
ing (Shen and Joshi, 2003). The absolute value is necessary
to emphasize comment differences instead of preferences.
</footnote>
<table confidence="0.99970825">
Classifier P R F1 Acc
baseline: Same 69.26
MaxEnt-2C 73.95 90.99 81.59 71.56
MaxEnt-3C 77.15 80.42 78.75 69.94
</table>
<tableCaption confidence="0.88192">
Table 2: Same-vs-Different classification. P, R,
and F1 are calculated with respect to Same.
</tableCaption>
<bodyText confidence="0.9363368">
Table 2 shows the results for the Same-vs-
Different classification. We can see that the two-
class MaxEnt-2C classifier works better than the
three-class MaxEnt-3C. MaxEnt-3C has more bal-
anced P and R, but loses in both F1 and accu-
racy. MaxEnt-2C is very skewed towards the ma-
jority class, but performs better due to the class
imbalance. Overall, it seems very difficult to learn
with the current features, and both methods only
outperform the majority-class baseline by a small
margin. Yet, while the overall accuracy is low,
note that the graph-cut/ILP inference allows us to
recover from some errors, because if nearby utter-
ances are clustered correctly, the wrong decisions
should be outvoted by correct ones.
The results for Good-vs-Bad are shown in Ta-
ble 3. On the top are the best systems at SemEval-
2015 Task 3. We can see that our MaxEnt classifier
is competitive: it shows higher accuracy than two
of them, and the highest F1 overall.6
</bodyText>
<table confidence="0.999916428571429">
System P R F1 Acc
Top-3 at SemEval-2015 Task 3
JAIST 80.23 77.73 78.96 79.10
HITSZ-ICRC 75.91 77.13 76.52 76.11
QCRI 74.33 83.05 78.45 76.97
Instance Classifiers
MaxEnt 75.67 84.33 79.77 78.43
Linear Chain Classifiers 83.45 78.94 77.53
CRF 74.89
Global Inference Classifiers
ILP 77.04 83.53 80.15 79.14‡
Graph-cut 78.30 82.93 80.55 79.80‡
ILP-3C 78.07 80.42 79.23 78.73
Graph-cut-3C 78.26 81.32 79.76 79.19†
</table>
<tableCaption confidence="0.998749">
Table 3: Good-vs-Bad classification. ‡ and †
</tableCaption>
<bodyText confidence="0.991378">
mark statistically significant differences in accu-
racy compared to the baseline MaxEnt classifier
with confidence levels of 99% and 95%, respec-
tively (randomized test).
</bodyText>
<footnote confidence="0.999827">
6This comparison is not strictly fair as the SemEval sys-
tems were trained to predict three classes, and here we
remapped them to two. We just want to show that our base-
line system is very strong.
</footnote>
<page confidence="0.997075">
576
</page>
<bodyText confidence="0.882333666666667">
The CRF model is worse than MaxEnt on all
measures, which suggests that the sequential infor-
mation does not help. This can be because many
interactions between comments are long-distance
and there are gaps in the threads due to the anno-
tation procedure at SemEval (Nakov et al., 2015).
However, global inference with graph-cut and
ILP improves both F1 and accuracy, mostly due to
better recall. Graph-cut works better than ILP as it
has higher precision, which helps F1 and accuracy.
Both yield statistically significant improve-
ments over the MaxEnt classifier; they also im-
prove over the state-of-the-art JAIST system. Note
that the devtest-tuned values of λ for graph-cut
and ILP put much lower weight to the Same-vs-
Different component (values are 0.95 and 0.91, re-
spectively). Finally, as expected, using the predic-
tions of MaxEnt-2C in the global classifiers is bet-
ter than using those from MaxEnt-3C.
Q: I have a female friend who is leaving for a teaching job
in Qatar in January. What would be a useful portable
gift to give her to take with her?
A1 A couple of good best-selling novels. It’s hard to find
much here in Doha in the way of books.
Local: Good, Global: Good, Human: Good
A2 ipod to entertain herself in case of boredom... a lot of
patience for her students...
</bodyText>
<subsectionHeader confidence="0.175157">
Local: Good, Global: Good, Human: Good
</subsectionHeader>
<bodyText confidence="0.691717">
A3 Thanks, please keep suggestions coming, would like to
send her off with a useful gift.
</bodyText>
<subsectionHeader confidence="0.601334">
Local: Bad, Global: Bad, Human: Bad
</subsectionHeader>
<bodyText confidence="0.883005">
As Bacon. Nice bread, bacon, bacon, errmmm bacon and a
pork joint..
</bodyText>
<note confidence="0.6511178">
Local: Bad, Global: Good, Human: Good
Ag Couple of good novels, All time favorite movies,..
Local: Bad, Global: Good, Human: Good
A11 Ditto on the books and dvd’s. Excedrin.
Local: Bad, Global: Bad, Human: Good
</note>
<bodyText confidence="0.8382166">
A12 Ditto on the bacon, pork sausage, pork chops, ham,..can
you tell we miss pork! I think getting a care package
together: her favorite perfume; shampoo; conditioner;
glycerin soaps; set of DVDs of her favorite TV series..
Oh, and did I mention she should pack PATIENCE?
</bodyText>
<note confidence="0.552483">
Local: Bad, Global: Good, Human: Good
</note>
<figureCaption confidence="0.8997985">
Figure 2: An excerpt of a thread with decisions
by local and global classifiers, and humans.
</figureCaption>
<sectionHeader confidence="0.998177" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999988461538462">
We manually examined a number of examples
where our global classfier could successfully re-
cover from the errors made by the local classifier,
and where it failed to do so. In Figure 2, we show
the classification decisions of our local and global
(graph-cut) classifiers along with the human anno-
tations for an excerpt of a thread.
For example, consider answers A6, A9, and
A12, which were initially misclassified as Bad
by the local classifier, but later recovered by the
global classifier exploiting the pairwise informa-
tion. In this case, the votes received by these an-
swers from other Good answers in the thread for
being in the same class won against the votes re-
ceived from other Bad answers.
Now consider A11, which our method failed to
classify correctly as Good. Our investigation re-
vealed that in this case the votes from the Bad an-
swers won against the votes from the Good ones.
The accuracy of the pairwise classifier has proven
to be crucial for the performance of our over-
all framework. We probably need more informa-
tive features (e.g., textual entailment and semantic
similarity to capture the relation between books
and novels, movies and DVDs, etc.) in order to
improve the pairwise classification performance.
</bodyText>
<sectionHeader confidence="0.992083" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999995052631579">
We have investigated the use of thread-level in-
formation for answer selection in CQA. We have
shown that using a pairwise classifier that predicts
whether two comments should get the same label,
followed by a graph-cut (or ILP) global inference
improves significantly over a very strong baseline
as well as over the state of the art. We have fur-
ther shown that using a linear-chain CRF model
does not help, probably because many interactions
between comments are long distance.
In future work, we would like to improve the
pairwise classifiers with richer features, as this is
currently the bottleneck for improving the perfor-
mance in the global model. We further plan to test
our framework on other CQA datasets, including
on other languages.7 Last but not least, we are in-
terested in extending this research with even more
global information, e.g., by modeling global deci-
sion consistency across multiple threads.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.919695">
This research was performed by the Arabic Lan-
guage Technologies (ALT) group at the Qatar
Computing Research Institute (QCRI), HBKU,
part of Qatar Foundation. It is part of the Inter-
active sYstems for Answer Search (Iyas) project,
which is developed in collaboration with MIT-
CSAIL.
7SemEval-2015 Task 3 had an Arabic subtask, but there
the answers were not coming from the same thread.
</bodyText>
<page confidence="0.994579">
577
</page>
<sectionHeader confidence="0.988909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999660681818182">
Lloyd Allison and Trevor Dix. 1986. A bit-string
longest-common-subsequence algorithm. Inf. Pro-
cess. Lett., 23(6):305–310, December.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’12, pages 435–440, Montr´eal, Canada.
Alberto Barr´on-Cede˜no, Simone Filice, Giovanni
Da San Martino, Shafiq Joty, Llu´ıs M`arquez, Preslav
Nakov, and Alessandro Moschitti. 2015. Thread-
level information for comment classification in com-
munity question answering. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, ACL-
IJCNLP ’15, pages 687–693, Beijing, China.
Yuri Boykov and Vladimir Kolmogorov. 2004. An
experimental comparison of min-cut/max-flow al-
gorithms for energy minimization in vision. IEEE
Trans. Pattern Anal. Mach. Intell., 26(9):1124–
1137, September.
Thomas H. Cormen, Clifford Stein, Ronald L. Rivest,
and Charles E. Leiserson. 2001. Introduction to Al-
gorithms. McGraw-Hill Higher Education.
Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: Exploiting classification approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval ’15, pages 196–202,
Denver, CO.
Paul Jaccard. 1901. ´Etude comparative de la distribu-
tion florale dans une portion des Alpes et des Jura.
Bulletin del la Soci´et´e Vaudoise des Sciences Na-
turelles, 37:547–579.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth
International Conference on Machine Learning,
ICML ’01, pages 282–289, San Francisco, CA.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’01, pages 118–125,
Pittsburgh, PA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference
on Machine Learning, ECML ’06, pages 318–329,
Berlin, Germany.
Kevin Murphy. 2012. Machine Learning A Probabilis-
tic Perspective. The MIT Press.
Preslav Nakov, Llu´ıs M`arquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
SemEval-2015 task 3: Answer selection in com-
munity question answering. In Proceedings of the
9th International Workshop on Semantic Evaluation,
SemEval ’15, pages 269–281, Denver, CO.
Massimo Nicosia, Simone Filice, Alberto Barr´on-
Cede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao,
Preslav Nakov, Giovanni Da San Martino, Alessan-
dro Moschitti, Kareem Darwish, Llu´ıs M`arquez,
Shafiq Joty, and Walid Magdy. 2015. QCRI: An-
swer selection for community question answering -
experiments for Arabic and English. In Proceedings
of the 9th International Workshop on Semantic Eval-
uation, SemEval ’15, pages 203–209, Denver, CO.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, pages 271–278,
Barcelona, Spain.
Zhonghua Qu and Yang Liu. 2011. Finding problem
solving threads in online forum. In Proceedings of
5th International Joint Conference on Natural Lan-
guage Processing, IJCNLP ’11, pages 1413–1417,
Chiang Mai, Thailand.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
CoNLL ’04, pages 1–8, Boston, MA.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proceedings of the Seventh Confer-
ence on Natural Language Learning, CONLL ’03,
pages 9–16, Edmonton, Canada.
Michael Wise. 1996. Yap3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the Twenty-seventh SIGCSE Tech-
nical Symposium on Computer Science Education,
SIGCSE ’96, pages 130–134, New York, NY.
Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou
Tang, and Xiaolong Wang. 2015a. Answer se-
quence learning with neural networks for answer se-
lection in community question answering. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 713–718,
Beijing, China.
Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang xiang,
and Xiaolong Wang. 2015b. ICRC-HIT: A deep
learning based comment sequence labeling system
for answer selection challenge. In Proceedings of
the 9th International Workshop on Semantic Evalu-
ation, SemEval ’15, pages 210–214, Denver, CO.
</reference>
<page confidence="0.996663">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441817">
<title confidence="0.998914">Global Thread-Level Inference for Comment in Community Question Answering</title>
<author confidence="0.998564">Alberto Giovanni Da San Martino Joty</author>
<author confidence="0.998564">Simone</author>
<affiliation confidence="0.837144">and Qatar Computing Research Institute,</affiliation>
<abstract confidence="0.9822535">Community question answering, a recent evolution of question answering in the Web context, allows a user to quickly consult the opinion of a number of people on a particular topic, thus taking advantage of the wisdom of the crowd. Here we try to help the user by deciding automatically which answers are good and which are bad for a given question. In particular, we focus on exploiting the output structure at the thread level in order to make more consistent global decisions. More specifically, we exploit the relations between pairs of comments at any distance in the thread, which we incorporate in a graph-cut and in an ILP frameworks. We evaluated our approach on the benchmark dataset of SemEval-2015 Task 3. Results improved over the state of the art, confirming the importance of using thread level information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Inf. Process. Lett.,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="12431" citStr="Allison and Dix, 1986" startWordPosition="2122" endWordPosition="2125">ribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); 4Setting a binary variable for each class label is necessary to have an objective function that is linear on the labels. C(P ) = λ � � �siB + ciEG ciEB sij 575 (iii) q belongs to each of the forum categories (2</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor Dix. 1986. A bit-string longest-common-subsequence algorithm. Inf. Process. Lett., 23(6):305–310, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>435--440</pages>
<location>Montr´eal, Canada.</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 435–440, Montr´eal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alberto Barr´on-Cede˜no</author>
<author>Simone Filice</author>
<author>Giovanni Da San Martino</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Threadlevel information for comment classification in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP ’15,</booktitle>
<pages>687--693</pages>
<location>Beijing, China.</location>
<marker>Barr´on-Cede˜no, Filice, Martino, Joty, M`arquez, Nakov, Moschitti, 2015</marker>
<rawString>Alberto Barr´on-Cede˜no, Simone Filice, Giovanni Da San Martino, Shafiq Joty, Llu´ıs M`arquez, Preslav Nakov, and Alessandro Moschitti. 2015. Threadlevel information for comment classification in community question answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACLIJCNLP ’15, pages 687–693, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Boykov</author>
<author>Vladimir Kolmogorov</author>
</authors>
<title>An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision.</title>
<date>2004</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>26</volume>
<issue>9</issue>
<pages>1137</pages>
<contexts>
<context position="10013" citStr="Boykov and Kolmogorov, 2004" startWordPosition="1669" endWordPosition="1672">ect each comment node vi to the source node s by adding an edge w(vi, s) with capacity siG, and to the sink node t by adding an edge w(vi, t) with capacity siB. Finally, we add edges w(vi, vj) with capacity sij to connect all pairs of comments. Minimizing C(P) amounts to finding a partition (S, T), where S = {s}US&apos; and T = {t}UT&apos; for s E/ S&apos;, t E/ T&apos;, that minimizes the cut capacity, i.e., the net flow crossing from S to T. One crucial advantage of this approach is that we can use max-flow algorithms to find the exact solution in polynomial time — near-linear in practice (Cormen et al., 2001; Boykov and Kolmogorov, 2004). 3.2 Integer Linear Programming Approach Here we follow the inference with classifiers approach by Roth and Yih (2004), solved with Integer Linear Programming (ILP). We have one ILP problem per question–answer thread. We define a set of binary variables, whose assignment will univocally define the classification of all comments in the thread. In particular, we define a pair of variables for each answer: xiG and xiB, 1 &lt; i &lt; n. Assigning 1 to xiG means that comment ci in the thread is classified as Good; assigning it 0 means that ci is not classified as Good. The same applies to the other clas</context>
</contexts>
<marker>Boykov, Kolmogorov, 2004</marker>
<rawString>Yuri Boykov and Vladimir Kolmogorov. 2004. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. IEEE Trans. Pattern Anal. Mach. Intell., 26(9):1124– 1137, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Clifford Stein</author>
<author>Ronald L Rivest</author>
<author>Charles E Leiserson</author>
</authors>
<title>Introduction to Algorithms. McGraw-Hill Higher Education.</title>
<date>2001</date>
<contexts>
<context position="9983" citStr="Cormen et al., 2001" startWordPosition="1664" endWordPosition="1668">and the sink. We connect each comment node vi to the source node s by adding an edge w(vi, s) with capacity siG, and to the sink node t by adding an edge w(vi, t) with capacity siB. Finally, we add edges w(vi, vj) with capacity sij to connect all pairs of comments. Minimizing C(P) amounts to finding a partition (S, T), where S = {s}US&apos; and T = {t}UT&apos; for s E/ S&apos;, t E/ T&apos;, that minimizes the cut capacity, i.e., the net flow crossing from S to T. One crucial advantage of this approach is that we can use max-flow algorithms to find the exact solution in polynomial time — near-linear in practice (Cormen et al., 2001; Boykov and Kolmogorov, 2004). 3.2 Integer Linear Programming Approach Here we follow the inference with classifiers approach by Roth and Yih (2004), solved with Integer Linear Programming (ILP). We have one ILP problem per question–answer thread. We define a set of binary variables, whose assignment will univocally define the classification of all comments in the thread. In particular, we define a pair of variables for each answer: xiG and xiB, 1 &lt; i &lt; n. Assigning 1 to xiG means that comment ci in the thread is classified as Good; assigning it 0 means that ci is not classified as Good. The </context>
</contexts>
<marker>Cormen, Stein, Rivest, Leiserson, 2001</marker>
<rawString>Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and Charles E. Leiserson. 2001. Introduction to Algorithms. McGraw-Hill Higher Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongshuai Hou</author>
<author>Cong Tan</author>
<author>Xiaolong Wang</author>
<author>Yaoyun Zhang</author>
<author>Jun Xu</author>
<author>Qingcai Chen</author>
</authors>
<title>HITSZICRC: Exploiting classification approach for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>196--202</pages>
<location>Denver, CO.</location>
<contexts>
<context position="3055" citStr="Hou et al., 2015" startWordPosition="492" endWordPosition="495">ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in</context>
</contexts>
<marker>Hou, Tan, Wang, Zhang, Xu, Chen, 2015</marker>
<rawString>Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZICRC: Exploiting classification approach for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 196–202, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florale dans une portion des Alpes et des Jura.</title>
<date>1901</date>
<booktitle>Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>37--547</pages>
<contexts>
<context position="12262" citStr="Jaccard (1901)" startWordPosition="2095" endWordPosition="2096">the comment variables and to the comment-pair variables are consistent: xijD = xiG ® xjG, bi, j 1 &lt; i &lt; j &lt; n. λ E [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); 4Setting a binary variable for each class</context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. ´Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles, 37:547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="3962" citStr="Lafferty et al., 2001" startWordPosition="639" endWordPosition="642">task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). 573 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 573–578, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Our goal in this paper goes in the same direction: we are interested in exploiting the output structure at the thread level to make more consistent global assignments. To the best of our knowledge, there is no work in QA that identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passage</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’01,</booktitle>
<pages>118--125</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="12295" citStr="Lyon et al., 2001" startWordPosition="2098" endWordPosition="2101">the comment-pair variables are consistent: xijD = xiG ® xjG, bi, j 1 &lt; i &lt; j &lt; n. λ E [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); 4Setting a binary variable for each class label is necessary to have an ob</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’01, pages 118–125, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning, ECML ’06,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="12542" citStr="Moschitti, 2006" startWordPosition="2141" endWordPosition="2142">, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); 4Setting a binary variable for each class label is necessary to have an objective function that is linear on the labels. C(P ) = λ � � �siB + ciEG ciEB sij 575 (iii) q belongs to each of the forum categories (26); and (iv) c and q were posted by the same user (4). An extra feature captures the length of c. Four features</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of the 17th European Conference on Machine Learning, ECML ’06, pages 318–329, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Machine Learning A Probabilistic Perspective.</title>
<date>2012</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="11942" citStr="Murphy, 2012" startWordPosition="2044" endWordPosition="2045"> xiB, xijS, xijD that minimizes the cost function: N C(A) = λ · (ciG · xiG + ciB · xiB) + i=1 (1 − λ) · N−1� N (cijS · xijS + cijD · xijD) i=1 j=i+1 subject to the following constraints: (i) All variables are binary; (ii) One and only one label is assigned to each comment or pair of comments; (iii) The assignments to the comment variables and to the comment-pair variables are consistent: xijD = xiG ® xjG, bi, j 1 &lt; i &lt; j &lt; n. λ E [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006)</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin Murphy. 2012. Machine Learning A Probabilistic Perspective. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Llu´ıs M`arquez</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Jim Glass</author>
<author>Bilal Randeree</author>
</authors>
<title>SemEval-2015 task 3: Answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>269--281</pages>
<location>Denver, CO.</location>
<marker>Nakov, M`arquez, Magdy, Moschitti, Glass, Randeree, 2015</marker>
<rawString>Preslav Nakov, Llu´ıs M`arquez, Walid Magdy, Alessandro Moschitti, Jim Glass, and Bilal Randeree. 2015. SemEval-2015 task 3: Answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 269–281, Denver, CO.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Massimo Nicosia</author>
<author>Simone Filice</author>
<author>Alberto Barr´onCede˜no</author>
<author>Iman Saleh</author>
<author>Hamdy Mubarak</author>
<author>Wei Gao</author>
<author>Preslav Nakov</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Moschitti</author>
<author>Kareem Darwish</author>
<author>Llu´ıs M`arquez</author>
<author>Shafiq Joty</author>
<author>Walid Magdy</author>
</authors>
<title>QCRI: Answer selection for community question answering -experiments for Arabic and English.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>203--209</pages>
<location>Denver, CO.</location>
<marker>Nicosia, Filice, Barr´onCede˜no, Saleh, Mubarak, Gao, Nakov, Martino, Moschitti, Darwish, M`arquez, Joty, Magdy, 2015</marker>
<rawString>Massimo Nicosia, Simone Filice, Alberto Barr´onCede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao, Preslav Nakov, Giovanni Da San Martino, Alessandro Moschitti, Kareem Darwish, Llu´ıs M`arquez, Shafiq Joty, and Walid Magdy. 2015. QCRI: Answer selection for community question answering -experiments for Arabic and English. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 203–209, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<pages>271--278</pages>
<location>Barcelona,</location>
<contexts>
<context position="8961" citStr="Pang and Lee (2004)" startWordPosition="1470" endWordPosition="1473">ine two ways of doing global inference using these two sources of information. 3http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee 574 3.1 Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: � � siG +(1−λ) ciEG,cjEB The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ E [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1, v2, · · · , vn, s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi, s) with capacity siG, and to the sink node t by adding an edge w(vi, t) with capacity siB. Finally, we add</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, pages 271–278, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhonghua Qu</author>
<author>Yang Liu</author>
</authors>
<title>Finding problem solving threads in online forum.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing, IJCNLP ’11,</booktitle>
<pages>1413--1417</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="2281" citStr="Qu and Liu (2011)" startWordPosition="364" endWordPosition="367"> motivation contribute answers to a given question. Unlike traditional question answering (QA), in CQA answering takes the form of commenting in a forum. Thus, many comments are only loosely connected to the original question, and some are not answers at all, but are rather interactions between users. As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat</context>
</contexts>
<marker>Qu, Liu, 2011</marker>
<rawString>Zhonghua Qu and Yang Liu. 2011. Finding problem solving threads in online forum. In Proceedings of 5th International Joint Conference on Natural Language Processing, IJCNLP ’11, pages 1413–1417, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning, CoNLL ’04,</booktitle>
<pages>1--8</pages>
<location>Boston, MA.</location>
<contexts>
<context position="10132" citStr="Roth and Yih (2004)" startWordPosition="1688" endWordPosition="1691">edge w(vi, t) with capacity siB. Finally, we add edges w(vi, vj) with capacity sij to connect all pairs of comments. Minimizing C(P) amounts to finding a partition (S, T), where S = {s}US&apos; and T = {t}UT&apos; for s E/ S&apos;, t E/ T&apos;, that minimizes the cut capacity, i.e., the net flow crossing from S to T. One crucial advantage of this approach is that we can use max-flow algorithms to find the exact solution in polynomial time — near-linear in practice (Cormen et al., 2001; Boykov and Kolmogorov, 2004). 3.2 Integer Linear Programming Approach Here we follow the inference with classifiers approach by Roth and Yih (2004), solved with Integer Linear Programming (ILP). We have one ILP problem per question–answer thread. We define a set of binary variables, whose assignment will univocally define the classification of all comments in the thread. In particular, we define a pair of variables for each answer: xiG and xiB, 1 &lt; i &lt; n. Assigning 1 to xiG means that comment ci in the thread is classified as Good; assigning it 0 means that ci is not classified as Good. The same applies to the other classes (here, only Bad). Also, we have a pair of variables for each pair of comments (to capture the pairwise relations): </context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning, CoNLL ’04, pages 1–8, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVM based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning, CONLL ’03,</booktitle>
<pages>9--16</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="15310" citStr="Shen and Joshi, 2003" startWordPosition="2610" endWordPosition="2613">ixing A for the graph-cut and ILP) were tuned on the development set. We also trained a second-order linear-chain CRF to check the contribution of the sequential relations between comments. We report results on the official SemEval test set for all methods. For the Same-vs-Different problem, we explored a variant of training with three classes, by splitting the Same class into Same-Good and Same-Bad. At test time, the probabilities of these two subclasses are added to get the probability of Same and all the algorithms are run unchanged. 5Subtracting vectors is standard in preference learning (Shen and Joshi, 2003). The absolute value is necessary to emphasize comment differences instead of preferences. Classifier P R F1 Acc baseline: Same 69.26 MaxEnt-2C 73.95 90.99 81.59 71.56 MaxEnt-3C 77.15 80.42 78.75 69.94 Table 2: Same-vs-Different classification. P, R, and F1 are calculated with respect to Same. Table 2 shows the results for the Same-vsDifferent classification. We can see that the twoclass MaxEnt-2C classifier works better than the three-class MaxEnt-3C. MaxEnt-3C has more balanced P and R, but loses in both F1 and accuracy. MaxEnt-2C is very skewed towards the majority class, but performs bette</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVM based voting algorithm with application to parse reranking. In Proceedings of the Seventh Conference on Natural Language Learning, CONLL ’03, pages 9–16, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96,</booktitle>
<pages>130--134</pages>
<location>New York, NY.</location>
<contexts>
<context position="12469" citStr="Wise, 1996" startWordPosition="2130" endWordPosition="2131">al Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); 4Setting a binary variable for each class label is necessary to have an objective function that is linear on the labels. C(P ) = λ � � �siB + ciEG ciEB sij 575 (iii) q belongs to each of the forum categories (26); and (iv) c and q were posted by th</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96, pages 130–134, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Zhou</author>
<author>Baotian Hu</author>
<author>Qingcai Chen</author>
<author>Buzhou Tang</author>
<author>Xiaolong Wang</author>
</authors>
<title>Answer sequence learning with neural networks for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>713--718</pages>
<location>Beijing, China.</location>
<contexts>
<context position="3472" citStr="Zhou et al., 2015" startWordPosition="555" endWordPosition="558">on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). 573 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 573–578, L</context>
</contexts>
<marker>Zhou, Hu, Chen, Tang, Wang, 2015</marker>
<rawString>Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, and Xiaolong Wang. 2015a. Answer sequence learning with neural networks for answer selection in community question answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 713–718, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Zhou</author>
<author>Baotian Hu</author>
<author>Jiaxin Lin</author>
<author>Yang xiang</author>
<author>Xiaolong Wang</author>
</authors>
<title>ICRC-HIT: A deep learning based comment sequence labeling system for answer selection challenge.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>210--214</pages>
<location>Denver, CO.</location>
<contexts>
<context position="3472" citStr="Zhou et al., 2015" startWordPosition="555" endWordPosition="558">on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). 573 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 573–578, L</context>
</contexts>
<marker>Zhou, Hu, Lin, xiang, Wang, 2015</marker>
<rawString>Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang xiang, and Xiaolong Wang. 2015b. ICRC-HIT: A deep learning based comment sequence labeling system for answer selection challenge. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 210–214, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
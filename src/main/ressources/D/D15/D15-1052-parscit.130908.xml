<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.928544">
Human Evaluation of Grammatical Error Correction Systems
</title>
<author confidence="0.828295">
Roman Grundkiewiczi and Marcin Junczys-Dowmunt&apos; and Edward Gillian&apos;
</author>
<affiliation confidence="0.7133585">
&apos;Information Systems Laboratory
Faculty of Mathematics and Computer Science, Adam Mickiewicz University
{romang,junczys}@amu.edu.pl
&apos;Faculty of English, Adam Mickiewicz University
</affiliation>
<email confidence="0.995317">
egillian@pwsz.pl
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959857142857">
The paper presents the results of the first
large-scale human evaluation of automatic
grammatical error correction (GEC) sys-
tems. Twelve participating systems and
the unchanged input of the CoNLL-2014
shared task have been reassessed in a
WMT-inspired human evaluation proce-
dure. Methods introduced for the Work-
shop of Machine Translation evaluation
campaigns have been adapted to GEC and
extended where necessary. The produced
rankings are used to evaluate standard
metrics for grammatical error correction in
terms of correlation with human judgment.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975653061225">
The field of automatic grammatical error correc-
tion (GEC) has seen a number of shared tasks
of different scope and for different languages.
The most impactful were the CoNLL-2013 and
CoNLL-2014 (Ng et al., 2013; Ng et al., 2014)
shared tasks on Grammatical Error Correction
for ESL (English as a second language) learn-
ers. They were preceded by the HOO shared tasks
(Dale and Kilgarriff, 2011; Dale et al., 2012).
Shared tasks for other languages took place as
well, including the QALB workshops for Arabic
(Mohit et al., 2014) and NLP-TEA competitions
for Chinese. These tasks use automatic metrics to
determine the quality of the participating systems.
However, these efforts pale in comparison to
competitions organized in other fields, e.g. dur-
ing the annual Workshops for Machine Transla-
tion (WMT). It is a central idea of the WMTs that
automatic measures of machine translation quality
are an imperfect substitute for human assessments.
Therefore, manual evaluation of the system out-
puts are conducted and their results are reported as
the final rankings of the workshops. These human
evaluation campaigns are an important driving fac-
tor for the advancement of MT and produce in-
sightful “by-products”, such as a huge number of
human assessments of machine translation outputs
that have been used to evaluate automatic metrics.
We believe that the unavailability of this kind
of quality assessment may stall the development
of GEC, as all the shared tasks and the entire field
have to cope with an inherent uncertainty of their
methods and metrics. We hope to make a step to-
wards alleviating this lack of confidence by pre-
senting the results of the first1 large-scale human
evaluation of automatic grammatical error correc-
tion systems submitted to the CoNLL-2014 shared
task. Most of our inspiration is drawn from the re-
cent WMT edition (Bojar et al., 2014) and its met-
rics task (Macháˇcek and Bojar, 2014).
We also provide an analysis of correlation be-
tween the standard metrics in GEC and human
judgment and show that the commonly used pa-
rameters for standard metrics in the shared task
may not be optimal. The uncertainty about met-
rics quality leads to proposals of new metrics, with
Felice and Briscoe (2015) being a recent example.
Based on human judgments we can show that this
proposed metric maybe less useful than hoped.
</bodyText>
<sectionHeader confidence="0.877257" genericHeader="introduction">
2 Evaluation of GEC systems
</sectionHeader>
<bodyText confidence="0.999771777777778">
Madnani et al. (2011) addresses two problems of
GEC evaluation: 1) a lack of informative metrics
and 2) an inability to directly compare the per-
formance of systems developed by different re-
searchers. Two evaluation methodologies are pre-
sented, both based on crowdsourcing which are
used to grade types of errors rather than system
performance as presented in this work. Chodorow
et al. (2012) draw attention to the many evalua-
</bodyText>
<footnote confidence="0.99658375">
1During the camera-ready preparation phase, we learned
about similar research by Napoles et al. (2015). After con-
tacting the authors, it was agreed to treat both works as fully
concurrent. Future work will compare the results.
</footnote>
<page confidence="0.970151">
461
</page>
<note confidence="0.9852">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999811">
tion issues in error detection which make it hard
to compare different approaches. The lack of con-
sensus is due to the nature of the error detection
task. The authors argue that the choice of the met-
ric should take into account factors such as the
skew of the data and the application that the sys-
tem is used for.
The most recent addition is Felice and Briscoe
(2015) who present a novel evaluation method for
grammatical error correction that scores systems
in terms of improvement on the original text.
</bodyText>
<sectionHeader confidence="0.996392" genericHeader="method">
3 The CoNLL-2014 shared task
</sectionHeader>
<bodyText confidence="0.999959411764706">
The goal of the CoNLL-2014 shared task (Ng et
al., 2014) was to evaluate algorithms and systems
for automatically correcting grammatical errors in
English essays written by second language learn-
ers of English. Training and test data was anno-
tated with 28 error types. Participating teams were
given training data with manually annotated cor-
rections of grammatical errors and were allowed
to use publicly available resources for training.
Twenty-five student non-native speakers of En-
glish were recruited to write essays to be used as
test data. Each student wrote two essays. The 50
test essays were error-annotated by two English
native speakers. The essays and error annotations
were made available after the task. The MaxMatch
(M2) scorer (Dahlmeier and Ng, 2012) has been
used as the official shared task evaluation metric.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="method">
4 Data collection
</sectionHeader>
<subsectionHeader confidence="0.999945">
4.1 Sampling sentences for evaluation
</subsectionHeader>
<bodyText confidence="0.990957473684211">
The system outputs of the CoNLL-2014 shared
task serve as evaluation data. The test set consists
of 1312 sentences, there are twelve system out-
puts available. The thirteenth participant NARA
is missing from this set. However, in GEC eval-
uation there is also the input to consider. Often
system outputs are equal to the unmodified input,
as it is most desirable if there are in fact no errors.
We include INPUT as the thirteenth system.
Due to the small number of modifications that
GEC systems apply to the input, there is not only
a large overlap with the input, but also among all
systems (Figure 1). If we sample systems uni-
formly, we lose easily obtainable pairwise judg-
ments for systems with the same output, and if
we collapse before sampling we introduce a strong
bias towards ties. To counter that bias, we aban-
don uniform sampling of test set sentences and use
N distinct outputs
</bodyText>
<figureCaption confidence="0.82931">
Figure 1: Frequencies of distinct corrected sen-
</figureCaption>
<bodyText confidence="0.986429176470588">
tences produced by 13 systems per input sentence.
instead a parametrized distribution that favors di-
verse sets of outputs.
The probability pi for a set of outputs Oi is cal-
culated as follows: N is the number of systems to
be evaluated, M is the maximum number of sen-
tences presented to the evaluator in a single rank-
ing (we use M = 5). The set of system outputs to
be evaluated E = {O1, ... , On} b1≤i≤n|Oi |=
N, consists of n (= 1312) sets Oi of N output
sentences each. Every sentence in Oi can overlap
with other sentences multiple times, so for each set
Oi we define the corresponding multiset of multi-
plicities Ui, such that Pu∈U, u = N.
We define ci(j) as the number of possible ways
to choose at most M different sentences that cover
j systems for the i-th set of outputs:
</bodyText>
<equation confidence="0.590622666666667">
��( ci (j) S C Ui :|S |≤ M ∧Xu = j
�
u∈S
</equation>
<bodyText confidence="0.78013">
Then the expected number Ci of systems cov-
ered by choosing at most M sentences is
</bodyText>
<equation confidence="0.997628">
Ci = PN M ci(j) · j
PNj=M ci(j) .
j=
</equation>
<bodyText confidence="0.5615885">
The pseudo-probability p0i of sampling the i-th
sentence is defined as
~Ci � = Ci(Ci − 1)
2 2
which is the ratio of pairwise comparisons of M
versus Ci different systems. By normalizing over
</bodyText>
<figure confidence="0.986463428571429">
1 2 3 4 5 6 7 8 9 10111213
No. of sentences 200
100
0
~M � where
2
p0 i = ~Ci~
</figure>
<page confidence="0.691617">
2
462
</page>
<figure confidence="0.997246388888889">
5
4
3
2
1
A B C D E
Output
A B C D E
Output
(b) Overlapping rankings.
Rank
Rank
4
3
5
2
1
(a) Screenshot of Appraise modified for GEC judgment.
</figure>
<figureCaption confidence="0.999925">
Figure 2: Displayed ranking and corresponding overlapping rankings.
</figureCaption>
<bodyText confidence="0.9470875">
the entire set of output sets we obtain the probabil-
ity pi of sampling the i-th set of outputs as
</bodyText>
<subsectionHeader confidence="0.999351">
4.2 Collecting system rankings
</subsectionHeader>
<bodyText confidence="0.997552388888889">
The sets of outputs sampled with the described
method have been prepared for Appraise (Feder-
mann, 2010) and presented to the judges. Judges
were asked to rank sentences from best to worse.
Ties are allowed. Judges were aware that the ab-
solute ranks bear no relevance as ranks are later
turned into relative pairwise judgments. No notion
of “better” or “worse” was imposed by the authors,
we relied on the judges to develop their own intu-
ition. All eight judges are English native speakers
and have extensive backgrounds in linguistics.
Figure 2a displays a screen shot of Appraise
with a judged sentence. Several modifications to
the Appraise framework2 were implemented to ac-
count for the specific nature of GEC:
Only the input sentence is displayed (top, bold),
no reference correction is given. The input sen-
tence is surrounded by one preceding and one fol-
</bodyText>
<footnote confidence="0.9729455">
2A fork of the original source code with can be found at
https://github.com/snukky/Appraise
</footnote>
<bodyText confidence="0.999844">
lowing sentence. Identical corrections are col-
lapsed into one output, system names with the
same output are recorded internally. Edited frag-
ments are highlighted, blue for insertions and sub-
stitutions, pale blue and crossed-out for deletions.
</bodyText>
<subsectionHeader confidence="0.99936">
4.3 Pairwise judgments
</subsectionHeader>
<bodyText confidence="0.92916">
As conducted during the WMT campaigns, we
turn rankings into sets of relative judgments of
the form A&gt;B, A=B, A&lt;B where the lower ranked
system scores a win. Absolute ranks and differ-
ences are lost. As mentioned above, due to the col-
lapsing of identical outputs we obtain significantly
more data than the usual 10 pairs from one ranking
with five sentences. Figure 2a contains a ranking
with overlapping outputs as displayed in the top
graph of Figure 2b. Pairs from within overlaps re-
sult in ties, pairs between overlaps are expanded
as products, (6 ) = 15 pairwise judgments can be
2
extracted. Greater overlap leads to more pairwise
judgments (bottom, (13 ) = 78).
2
Table 1 lists the full statistics for collected rank-
ings by individual annotators. Unexpanded pairs
are WMT-style pairwise judgments before an out-
put A gets split into overlapping systems A1, A2,
A3, etc. The large number of ties for expanded
pairs is to be expected due to the high overlap
</bodyText>
<equation confidence="0.8828535">
pIi
EEI.
j=1 pj
pi =
</equation>
<page confidence="0.99823">
463
</page>
<table confidence="0.9996074">
Judge Ranks Unexpanded Expanded
1 400 3525 (1022) 18400 (10166)
2 299 2684 (1099) 13657 (8429)
3 400 3523 (914) 18912 (9684)
4 201 1750 (550) 9478 (5539)
5 349 3099 (766) 17107 (8972)
6 400 3474 (517) 19313 (9209)
7 70 646 (145) 3383 (1593)
8 200 1815 (681) 8848 (5525)
Total 2319 20516 (5694) 109098 (59117)
</table>
<tableCaption confidence="0.826128333333333">
Table 1: Statistics for collected rankings (Ranks),
unexpanded and expanded pairwise judgments,
numbers for ties are given in parentheses.
</tableCaption>
<bodyText confidence="0.914588">
between systems (on average there are only 5.7
unique outputs among 13 systems).
</bodyText>
<subsectionHeader confidence="0.983396">
4.4 Inter- and intra-annotator agreement
</subsectionHeader>
<bodyText confidence="0.99991475">
Again inspired by the WMT evaluation cam-
paigns, we compute annotator agreement as a
measure of reliability of the pairwise judgments
with Cohen’s kappa coefficient (Cohen, 1960):
</bodyText>
<equation confidence="0.994921">
P(A) − P(E)
1 − P(E) .
</equation>
<bodyText confidence="0.99968696">
where P(A) is the proportion of times that anno-
tators agree, and P(E) is the proportion of times
that they would agree by chance. K assumes values
from 0 (no agreement) to 1 (perfect agreement).
All probabilities are computed as ratios of em-
pirically counted pairwise judgments. As the
judges worked on collapsed outputs, we calculate
agreement scores for unexpanded pairs; otherwise,
the high overlap would unfairly increase agree-
ment.
P(A) is calculated by examining all pairs of
outputs which have been judged by two or more
judges, and counting the proportion of times that
they agreed that A&lt;B, A=B, or A&gt;B.
P(E) = P(A&lt;B)2+P(A=B)2+P(A&gt;B)2 is the
probability that two judges agree randomly. Intra-
annotator agreement as a measure of consistency
is calculated for output sets that have been judged
more than one time by the same annotator.
The agreement numbers in Table 2 are in the
lower range of values reported during WMT. How-
ever, it should be noted that judges never saw the
repeated outputs within one ranking which prob-
ably decreases agreement compared to the MT-
specific task.
</bodyText>
<figure confidence="0.915603944444444">
Agreement Value Degree
Inter-annotator 0.29 Weak
Intra-annotator 0.46 Moderate
(a) Inter-annotator and intra-annotator agree-
ment for all judges
1 2 3 4 5 6 7 8
1 .42 .26 .30 .37 .34 .26 .31 .24
2 – .30 .25 .28 .23 .20 .10 .20
3 – – .50 .35 .44 .34 .46 .26
4 – – – .34 .34 .30 .20 .26
5 – – – – .60 .36 .34 .32
6 – – – – – .44 .35 .25
7 – – – – – * *
–
8 – – – – – – – .48
(b) Pairwise inter-annotator and intra-annotator agree-
ment per judge. Stars indicate too few overlapping
judgements.
</figure>
<tableCaption confidence="0.375150666666667">
Table 2: Inter-annotator and intra-annotator agree-
ment (Cohen’s K) on unexpanded pairwise judg-
ments.
</tableCaption>
<sectionHeader confidence="0.951127" genericHeader="method">
5 Computing ranks
</sectionHeader>
<bodyText confidence="0.999994818181818">
In this section, it is our aim to produce a system
ranking from best to worse by computing the av-
erage number of times each system was judged
better than other systems based on the collected
pairwise rankings. While previously introduced
methods for producing rankings, total orderings,
as well as partial orderings at chosen confidence-
levels, can be directly applied to our data, deter-
mining which ranking is more accurate turns out
to be methodologically and computationally more
involved due to the specific nature of GEC outputs.
</bodyText>
<subsectionHeader confidence="0.920592">
5.1 Ranking methods
</subsectionHeader>
<bodyText confidence="0.999691384615385">
We adapt two ranking methods applied during
WMT13 and WMT14 to GEC evaluation: the Ex-
pected Wins method and a version of TrueSkill.
Expected Wins. Expected Wins (EW) has been
introduced for WMT13 (Bojar et al., 2013) and is
based on an underlying model of “relative ability”
proposed in Koehn (2012). One advantage of this
method is its intuitiveness; the scores reflect the
probability that a system Si will be ranked better
than another system that has been randomly cho-
sen from a pool of opponents {Sj : j =� i}. Defin-
ing the function win(A, B) as the number of times
system A is ranked better than system B, Bojar et
</bodyText>
<equation confidence="0.995523">
K =
</equation>
<page confidence="0.994706">
464
</page>
<table confidence="0.99920365">
# System P R M20.5 # Score Range System # Score Range System
1 CAMB 0.397 0.301 0.373 1 0.628 1 AMU 1 0.273 1 AMU
2 CUUI 0.417 0.248 0.367 2 0.566 2-3 RAC 2 0.182 2 CAMB
3 AMU 0.416 0.214 0.350 0.561 2-4 CAMB 3 0.114 3-4 RAC
4 POST 0.345 0.217 0.308 0.550 3-5 CUUI 0.105 3-5 CUUI
5 NTHU 0.350 0.188 0.299 0.539 4-5 POST 0.080 4-5 POST
6 RAC 0.331 0.149 0.266 3 0.513 6-8 UFC 4 -0.001 6-7 PKU
7 UMC 0.312 0.144 0.253 0.506 6-8 PKU -0.022 6-8 UMC
8 PKU 0.322 0.136 0.253 0.495 7-9 UMC -0.041 7-10 UFC
9 SJTU 0.301 0.051 0.151 0.485 7-10 IITB -0.055 8-11 IITB
10 UFC 0.700 0.017 0.078 0.463 10-11 SJTU -0.062 8-11 INPUT
11 IPN 0.112 0.028 0.071 0.456 9-12 INPUT -0.074 9-11 SJTU
12 IITB 0.307 0.013 0.059 0.437 11-12 NTHU 5 -0.142 12 NTHU
13 INPUT 0.000 0.000 0.000 4 0.300 13 IPN 6 -0.358 13 IPN
(a) Official CoNLL-2014 ranking without un- (b) Human ExpectedWins ranking (c) Human TrueSkill ranking.
published NARA system. (final manual ranking).
AMU INPUT NTHU
RAC IPN
CAMB
CUUI
POST
UFC
PKU
UMC
IITB
SJTU
AMU – .44 ‡ .47 ? .46 † .44 ‡ .34 ‡ .40 ‡ .37 ‡ .32 ‡ .34 ‡ .32 ‡ .31 ‡ .24 ‡
RAC .56 ‡ – .53 .48 .48 .40 ‡ .45 † .44 ‡ .39 ‡ .38 ‡ .38 ‡ .43 ‡ .28 ‡
CAMB .53 ? .47 – .49 .45 ‡ .43 ‡ .43 ‡ .42 ‡ .42 ‡ .43 ‡ .42 ‡ .43 ‡ .34 ‡
CUUI .54 † .52 .51 – .49 .42 ‡ .47 .46 † .42 ‡ .41 ‡ .41 ‡ .42 ‡ .32 ‡
POST .56 ‡ .52 .55 ‡ .51 – .45 ‡ .47 .46 ? .44 ‡ .44 ‡ .43 ‡ .42 ‡ .29 ‡
UFC .66 ‡ .60 ‡ .57 ‡ .58 ‡ .55 ‡ – .54 ? .50 .49 .44 ? .27 † .42 ‡ .21 ‡
PKU .60 ‡ .55 † .57 ‡ .53 .53 .46 ? – .50 .47 .46 ? .46 ? .46 † .35 ‡
UMC .63 ‡ .56 ‡ .58 ‡ .54 † .54 ? .50 .50 – .48 .47 .48 .45 ‡ .35 ‡
IITB .68 ‡ .61 ‡ .58 ‡ .58 ‡ .56 ‡ .51 .53 .52 – .48 .43 .43 ‡ .27 ‡
SJTU .66 ‡ .62 ‡ .57 ‡ .59 ‡ .56 ‡ .56 ? .54 ? .53 .52 – .53 .46 ? .30 ‡
INPUT .68 ‡ .62 ‡ .58 ‡ .59 ‡ .57 ‡ .73 † .54 ? .52 .57 .47 – .43 ‡ .22 ‡
NTHU .69 ‡ .57 ‡ .57 ‡ .58 ‡ .58 ‡ .58 ‡ .54 † .55 ‡ .57 ‡ .54 ? .57 ‡ – .41 ‡
IPN .76 ‡ .72 ‡ .66 ‡ .68 ‡ .71 ‡ .79 ‡ .65 ‡ .65 ‡ .73 ‡ .70 ‡ .78 ‡ .59 ‡ –
(d) Head-to-head comparison for ExpectedWins at P &lt; 0.10 (*), P &lt; 0.05 (†), and P &lt; 0.01 (‡).
</table>
<tableCaption confidence="0.943488">
Table 3: Comparison of official CoNLL-2014 ranking and human rankings. Ranges and clusters have
been calculated with bootstrap resampling at p ≤ 0.05.
</tableCaption>
<equation confidence="0.8108966">
al. (2013) calculate EW scores as follows:
scoreEW(Si) =
1win(Si, Sj)
{Sj} |j _ win(Si, Sj) + win(Sj, Si).
,j
</equation>
<bodyText confidence="0.9999723">
TrueSkill. The TrueSkill ranking system (Her-
brich et al., 2007) is a skill based ranking system
for Xbox Live developed at Microsoft Research. It
is used to identify and model player (GEC systems
in our case) ability in a game to assign players to
competitive matches. The TrueSkill ranking sys-
tem models each player Si by two parameters: the
average relative ability µSi and the degree of un-
certainty in the player’s ability σ2Si. Maintaining
uncertainty allows TS to make greater changes to
the ability estimates at the beginning and smaller
changes after a number of consistent matches has
been played. Due to that TS can identify the abil-
ity of individual players from a smaller number of
pairwise comparisons.
A modification of this approach to the WMT
manual evaluation procedure by Sakaguchi et al.
(2014) has been adopted as the official rank-
ing method during WMT14 replacing EW. The
TrueSkill scores are calculated as inferred means:
</bodyText>
<equation confidence="0.885711">
scoreTS(Si) = µSi.
</equation>
<page confidence="0.995989">
465
</page>
<subsectionHeader confidence="0.992342">
5.2 Rank clusters
</subsectionHeader>
<bodyText confidence="0.999987714285714">
Both ranking methods produce total orderings
without information on the statistical significance
of the obtained ranks. Bojar et al. (2014) notice
that the similarity of the participants in terms of
methods and training data causes some of them
to be very similar and group systems into equiv-
alence classes as proposed by Koehn (2012).
Although the methods and training data among
the systems examined in this paper are quite di-
verse, a great similarity of produced outputs is an
inherent property of GEC. Therefore, in this sec-
tion, for each system Sj placed on rank rj we
also try to determine the true systems rank ranges
[r� j, . . . ,r00j ] at a confidence-level of 95% and clus-
ters of equivalent systems by following the proce-
dure outlined by Koehn (2012).
This is accomplished by applying bootstrap re-
sampling. Pairwise rankings are drawn from the
set of judgments with multiple drawings. Based
on this sample a new ranking is produced. After
repeating this process a 1000 times the obtained
1000 ranks for Sj are sorted, with the top 25 and
bottom 25 ranks being discarded. The interval of
the remaining ranks serves as the final rank range.
Next, these rank ranges are used to produce clus-
ters of overlapping rank ranges. This is the last
step required to produce the rankings in Tables 3b
and 3c for both methods, EW and TS, respectively.
</bodyText>
<subsectionHeader confidence="0.997588">
5.3 Choosing the final ranking
</subsectionHeader>
<bodyText confidence="0.9998181">
Now, we face the question which ranking should
be presented as the final result of the human eval-
uation task. Again, we turn to Bojar et al. (2014)
who choose their rankings based on the ranking
model’s ability to predict pairwise rankings. Ac-
curacy is computed by 100-fold cross-validation.
For each fold a new ranking is trained from 99
parts with the left-over part serving as test data.
In a first step, we calculate the accuracy of
the unclustered total orderings discarding ties. A
ranking based on model scores alone cannot pre-
dict ties, this requires equivalence classes. Bojar et
al. (2014) define a draw radius r such that systems
whose scores differ by less than r are assigned to
one cluster, r is tuned to maximize accuracy.
In our case, due to the large number of ties, their
method of tuning r is trapped in local maxima and
assigns all systems to a single cluster. Alterna-
tively, we propose to calculate clusters according
to the method described in the previous section.
</bodyText>
<table confidence="0.991367666666667">
Method EW TS
Total ordering (non-ties) 58.18 58.15
Bootstrapped clusters 40.12 39.48
</table>
<tableCaption confidence="0.881806">
Table 4: Accuracy for ranking-based prediction of
pairwise judgments.
</tableCaption>
<bodyText confidence="0.9979985">
By fixing p &lt; 0.05 we directly evaluate rank-
ings of the form given in Table 3. The absolute
values of scores and their different interpretations
between methods become irrelevant which makes
it unnecessary to tune a parameter like r. The
main drawback of this approach is its computa-
tional cost. For each of the 100 folds we boot-
strap another 100 rankings with EW and TS, fix
p &lt; 0.05 and calculate rank clusters. The single
clustered ranking for each fold is then used to cal-
culate accuracy for the held-out test data.
For our data, contrary to the MT-specific re-
sults from Bojar et al. (2014), EW beats TS in
both cases (Table 4). We therefore present the
ExpectedWins-based ranking (Table 3b) as the fi-
nal result of the human evaluation effort described
in this work and refer to it in the remainder of the
paper when the human ranking is mentioned.
</bodyText>
<subsectionHeader confidence="0.989735">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999985708333333">
The final human-created ranking (Table 3b) con-
sists of four non-overlapping rank clusters. Rank
ranges have been calculated at a confidence level
of 95%. Comparing the official CoNLL-2014
ranking (Table 3a) with the manually created Ex-
pectedWins ranking shows interesting differences.
The AMU system is judged to be a clear leader
by human judges in its own rank cluster. For six
out of eight judges, AMU has the highest score
(Table 7). The officially winning system CAMB
occupies third place in terms of EW scores and
is placed in the second cluster with four systems.
Only one judge put CAMB in first place. RAC, a
middling system, is elevated to second place oc-
cupying a rank cluster with three other systems.
NTHU, another middling system that based on M2
should be similar to RAC, is put in the second to
last position. Two systems are judged to be worse
than INPUT. The rank cluster that includes INPUT
is the largest among the four clusters.
We also include pairwise comparisons between
all systems according to EW in Table 3d. Each cell
contains the percentage of times the system in that
column was judged to be better than the system in
</bodyText>
<page confidence="0.999224">
466
</page>
<bodyText confidence="0.99819475">
that row. Bold values mark the winner. We ap-
plied the Sign Test to measure statistically signifi-
cant differences, ? indicates statistical significance
at p &lt; 0.10, t at p &lt; 0.05, and tatp&lt;0.01.
</bodyText>
<sectionHeader confidence="0.804526" genericHeader="method">
6 Correlation with GEC metrics
</sectionHeader>
<bodyText confidence="0.9976005">
Since WMT08 (Callison-Burch et al., 2008) the
“metrics task” has been part of the WMT. The aim
of the metrics task is to assess the quality of auto-
matic evaluation metrics for MT in terms of cor-
relation with the collected human judgments. We
attempt the same in the context of GEC.
</bodyText>
<subsectionHeader confidence="0.999904">
6.1 Measures of correlation
</subsectionHeader>
<bodyText confidence="0.979541142857143">
Based on Macháˇcek and Bojar (2013), we use
Spearman’s rank correlation ρ and Pearson’s r to
compare the similarity of rankings produced by
various metrics to the manual ranking from the
previous section.
Spearman’s rank correlation ρ. Spearman’s ρ
for rankings with no ties is defined as
</bodyText>
<equation confidence="0.99418075">
6 P d2
i
ρ = 1 −
n(n2 − 1)
</equation>
<bodyText confidence="0.99942">
where di is the distance between human and met-
ric rank for system i, n is the number of systems.
Pearson’s r. Macháˇcek and Bojar (2013) find
that Spearman’s ρ is too harsh and propose to also
use Pearson’s r, calculated as
where H and M are the vectors of human and met-
ric scores, H and M are corresponding means.
</bodyText>
<subsectionHeader confidence="0.997891">
6.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999913642857143">
The inventory of evaluation metrics for GEC is
significantly smaller than for MT. We hope that
making our data available will fuel the interest in
this area. The following metrics are assessed:
MaxMatch (M2). Due to its adoption as the
main evaluation metric of the CoNLL shared tasks
and the QALB shared tasks (Mohit et al., 2014),
the M2 metric (Dahlmeier and Ng, 2012) can be
seen as a de facto standard. Being an Fβ-score, M2
results are most influenced by the choice of β. Be-
tween the CoNLL-2013 and CoNLL-2014 shared
tasks, the organizers changed β from 1.0 to 0.5,
and motivate this with intuition alone. The QALB
shared tasks for Arabic continue to use β = 1.0.
</bodyText>
<table confidence="0.918696666666667">
Metric Spearman’s ρ Pearson’s r
M2 F1.0 0.648 0.610
M2 F0.5∗ 0.692 0.627
M2 F0.25 0.720 0.680
M2 F0.18 0.758 0.701
M2 F0.1 0.670 0.652
I-WAcc -0.154 -0.098
BLEU -0.346 -0.240
METEOR -0.374 -0.241
</table>
<tableCaption confidence="0.913313">
Table 5: Correlation results for various metrics
and human ranking.
</tableCaption>
<equation confidence="0.837672181818182">
1 2 3 4 5 6 7 8 ρ ρ
1 – .70 .31 .76 .74 .19 .62 .48 .70
2 .72 – .77 .84 .90 .57 .59 .64 .93
3 .53 .89 – .66 .70 .58 .42 .64 .63
4 .82 .79 .69 – .91 .42 .67 .54 .91
5 .65 .85 .82 .87 – .63 .63 .51 .93 .72
6 .32 .71 .67 .56 .86 – .63 .39 .42
7 .72 .74 .57 .76 .72 .63 – .63 .76
8 .64 .85 .86 .69 .72 .57 .75 – .60
r .67 .93 .82 .87 .92 .66 .80 .82 –
r¯ .80
</equation>
<bodyText confidence="0.918293619047619">
Table 6: Inter-annotator correlation (Spearman’s ρ
above the diagonal, Pearson’s r below).
I-measure/Weighted Accuracy (I-WAcc). The
recently proposed I-WAcc metric (Felice and
Briscoe, 2015) tries to address the shortcomings
of M2. The inclusion of true negatives into the for-
mula makes this a very conservative metric; quite
similar to the MT metrics described below. The
metric assigns negative weights to systems that are
harmful with regard to the input text, values from
the range [1, −1] are possible. The reported corre-
lation values have been calculated for the ranking
presented in Felice and Briscoe (2015).
Machine translation evaluation metrics. Bas-
ing most of our results on findings from MT, we
also take a look at two machine translation eval-
uation metrics, BLEU (Papineni et al., 2002) and
METEOR (Denkowski and Lavie, 2011). In order
to use the CoNLL-2014 gold standard with these
metrics, the edit-based annotation has been con-
verted into two plain text files, one per annotator.
</bodyText>
<subsectionHeader confidence="0.998646">
6.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9069945">
The correlation results are collected in table 5. The
M2 metric is generally moderately correlated with
</bodyText>
<equation confidence="0.992660833333333">
Pn Hi −
i=1
r =
qPn qPn
i=1(Hi − ¯H)2 i=1(Mi −¯M)2
H)(Mi − M)
</equation>
<page confidence="0.99584">
467
</page>
<figure confidence="0.615279">
Values of Q
</figure>
<figureCaption confidence="0.743933">
Figure 3: Spearman’s p and Pearson’s r correla-
tion of M2 with human judgment w.r.t. Q. Dashed
line marks official CoNLL-2014 choice Q = 0.5.
</figureCaption>
<bodyText confidence="0.999785945945946">
human judgment and is on the brink of high cor-
relation for values of Q closer to 0.2. Compared
to M2, the other metrics are weakly or moderately
inversely correlated to human judgment. Inverse
correlation with human judgments for metrics that
all assign higher scores to better systems seems
problematic. In the case of I-WAcc, we would
go as far as to state an absence of correlation. It
seems the conservative approach adopted for I-
WAcc does not correspond to the notion of quality
that our judges worked out for themselves. The
switch to Q = 0.5 from Q = 1.0 for the CoNLL-
2014 shared task was a good choice, but a higher
correlation can be achieved for Q = 0.25, the max-
imum is reached for Q = 0.18. Correlation drops
sharply for Q = 0.1. The lack of positive corre-
lation for the MT-metrics is interesting in the light
of improvement that results from a shift towards
precision for M2 as BLEU is based on precision.
Figure 3 contains detailed plots of p and r with
regard to Q within the [0, 1] range. As the CoNLL-
2014 test data included edits from two annotators,
we plot curves for both annotators separately and
for the combined gold standard. In the case of
Spearman’s p having alternative error annotations,
this leads to higher correlation values. Based on
the plots we would recommend setting 0.2 ≤ Q ≤
0.3 instead of 0.5 or even 1.0.
Inter-annotator correlations of rankings com-
puted for individual judges (Table 6) can be treated
as human-level upper bounds for metric correla-
tion. The penultimate column and row contain
correlations of rankings for individual judges with
rankings computed from all judges minus the re-
spective judge. The last column and row con-
tain the respective weighted (w.r.t. judgments per
judge) average of these correlations.
</bodyText>
<sectionHeader confidence="0.972795" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999989904761905">
We have successfully adapted methods from the
WMT human evaluation campaigns to automatic
grammatical error correction. The collected and
produced data has been made available and should
be useful for other researchers. Although we set
out to provide answers, we probably ended up with
more questions. The following (and more) might
be investigated in the future: What makes the win-
ning system special and why do the standard met-
rics fail at identifying this system? Can we come
up with better system-level metrics? Can mean-
ingful sentence-level metrics be developed?
Outside the scope of the particular data, we need
to wonder if our results generalize to other shared
tasks and other languages. The CoNLL-2014 data
concerns ESL learners only and may not be trans-
ferable to systems for native speakers. This would
be in line with the ideas developed by Chodorow et
al. (2012). We would hope to see similar endeav-
ors for the other shared tasks as this would enable
the field to draw more general conclusions.
</bodyText>
<subsectionHeader confidence="0.903827">
Obtaining the data
</subsectionHeader>
<bodyText confidence="0.999441666666667">
The presented data and tools are available from:
https://github.com/grammatical/
evaluation
</bodyText>
<sectionHeader confidence="0.980101" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997958666666667">
Partially funded by the Polish National Science
Centre (Grant No. 2014/15/N/ST6/02330).
The authors would like to thank the following
judges for their hard work on the ranking task:
Sam Bennett, Peter Dunne, Stacia Levy, Kenneth
Turner, and John Winward.
</bodyText>
<figure confidence="0.9992684">
0.00 0.25 0.50 0.75 1.00
0.70
0.65
0.60
0.55
0.75
0.70
0.65
0.60
0.55
Annotator1
Annotator2
Combined
Spearman’s ρ
Pearson’s r
</figure>
<page confidence="0.996665">
468
</page>
<table confidence="0.63306503125">
# Score Range System
1 0.674 1-2 CAMB
0.658 1-2 AMU
2 0.573 3-6 CUUI
0.573 3-6 PKU
0.566 3-7 POST
0.553 3-8 RAC
0.544 4-8 NTHU
0.537 5-8 UMC
3 0.436 9-10 SJTU
0.419 9-11 UFC
0.387 10-12 IITB
0.332 11-12 INPUT
4 0.276 13 IPN
(a) Judge 1
# Score Range System
1 0.641 1-2 AMU
0.631 1-2 CAMB
2 0.581 3-4 RAC
0.578 3-4 CUUI
3 0.507 5-10 UFC
0.494 5-10 POST
0.490 5-10 UMC
0.488 5-11 SJTU
0.486 5-10 PKU
0.473 5-11 INPUT
0.441 9-12 NTHU
0.378 11-12 IITB
4 0.313 13 IPN
(d) Judge 4
# Score Range System
1 0.640 1 AMU
</table>
<footnote confidence="0.7881408">
2 0.559 2-6 CUUI
0.558 2-6 RAC
0.557 2-7 UFC
0.543 2-7 CAMB
0.526 3-9 PKU
0.511 5-10 POST
0.511 4-11 IITB
0.508 5-10 UMC
0.473 7-11 INPUT
0.472 8-11 NTHU
</footnote>
<figure confidence="0.551283833333333">
3 0.398 12 SJTU
4 0.286 13 IPN
(b) Judge 2
# Score Range System
1 0.613 1-2 AMU
0.608 1-2 RAC
</figure>
<footnote confidence="0.969254222222222">
2 0.568 3-5 CUUI
0.554 3-6 CAMB
0.535 4-7 POST
0.526 4-9 UFC
0.515 5-9 PKU
0.496 6-10 SJTU
0.487 6-11 INPUT
0.472 8-11 IITB
0.461 9-11 UMC
</footnote>
<table confidence="0.461533944444445">
3 0.375 12 NTHU
4 0.290 13 IPN
(e) Judge 5
# Score Range System
1 0.612 1-2 AMU
0.578 2-3 CUUI
0.564 2-5 UFC
0.534 3-7 RAC
0.526 4-7 POST
0.517 4-8 UMC
0.508 4-9 IITB
0.474 5-11 INPUT
0.467 8-12 SJTU
0.464 8-12 PKU
0.463 8-12 CAMB
0.458 9-12 NTHU
2 0.333 13 IPN
(c) Judge 3
# Score Range System
1 0.601 1-2 RAC
0.579 1-4 AMU
0.565 1-6 IITB
0.562 2-6 POST
0.548 2-8 INPUT
0.535 3-8 UFC
0.519 6-9 PKU
0.516 6-9 CAMB
0.491 7-10 SJTU
0.474 9-10 CUUI
2 0.428 11 UMC
3 0.368 12 NTHU
4 0.313 13 IPN
(f) Judge 6
# Score Range System
1 0.788 1 AMU
2 0.697 2 CAMB
</table>
<footnote confidence="0.944620272727273">
3 0.553 3-7 RAC
0.544 3-7 UMC
0.537 3-7 POST
0.533 3-10 IITB
0.487 5-10 PKU
0.474 5-12 INPUT
0.462 6-11 CUUI
0.436 6-12 SJTU
0.408 8-12 UFC
0.386 9-12 NTHU
4 0.303 13 IPN
</footnote>
<table confidence="0.760453">
(g) Judge 7
# Score Range System
</table>
<footnote confidence="0.918708615384615">
1 0.660 1-2 AMU
0.625 1-3 CUUI
0.568 2-7 IITB
0.556 3-6 CAMB
0.543 3-7 UMC
0.537 3-7 POST
0.483 5-10 INPUT
0.481 5-10 UFC
0.478 7-11 RAC
0.466 7-11 NTHU
0.420 10-12 PKU
0.419 10-12 SJTU
2 0.260 13 IPN
</footnote>
<table confidence="0.407572">
(h) Judge 8
</table>
<tableCaption confidence="0.8746555">
Table 7: Rankings by individual annotators. Cluster ranks and rank ranges have been computed with
bootstrap resampling at p ≤ 0.1 to accomodate for the reduced number of judgments per judge.
</tableCaption>
<page confidence="0.999646">
469
</page>
<sectionHeader confidence="0.995867" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898559633028">
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc. of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1–44. ACL.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, et al. 2014. Findings of the 2014
Workshop on Statistical Machine Translation. In
Proc. of the Ninth Workshop on Statistical Machine
Translation, pages 12–58. ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proc. of the Third Workshop on Statistical Machine
Translation, pages 70–106. ACL.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel R Tetreault. 2012. Problems in evaluating
grammatical error detection systems. In Proc. of
COLING 2012, pages 611–628. ACL.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proc. of the 2012 Conference of the North Amer-
ican Chapter of the ACL: Human Language Tech-
nologies, pages 568–572. ACL.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proc. of
the 13th European Workshop on Natural Language
Generation, pages 242–249. ACL.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proc. of
the Seventh Workshop on Building Educational Ap-
plications Using NLP, pages 54–62. ACL.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proc. of the EMNLP 2011 Workshop on Statistical
Machine Translation.
Christian Federmann. 2010. Appraise: An open-
source toolkit for manual phrase-based evaluation of
translations. In Proc. of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10). ELRA.
Mariano Felice and Ted Briscoe. 2015. Towards
a standard evaluation method for grammatical er-
ror detection and correction. In Proc. of the 2015
Conference of the North American Chapter of the
ACL: Human Language Technologies (NAACL-HLT
2015). ACL.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2007.
Trueskill(tm): A bayesian skill rating system. In
Advances in Neural Information Processing Systems
20, pages 569–576. MIT Press.
Philipp Koehn. 2012. Simulating human judgment
in machine translation evaluation campaigns. In In-
ternational Workshop on Spoken Language Transla-
tion, pages 179–184.
Matou&amp;quot;s Macháˇcek and Ondˇrej Bojar. 2013. Results
of the WMT13 metrics shared task. In Proc. of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51. ACL.
Matou&amp;quot;s Macháˇcek and Ondˇrej Bojar. 2014. Results
of the WMT14 metrics shared task. In Proc. of the
Ninth Workshop on Statistical Machine Translation,
pages 293–301. ACL.
Nitin Madnani, Joel Tetreault, Martin Chodorow, and
Alla Rozovskaya. 2011. They can help: using
crowdsourcing to improve the evaluation of gram-
matical error detection systems. In Proc. of the 49th
Annual Meeting of the ACL: Human Language Tech-
nologies, pages 508–513. ACL.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The first
QALB shared task on automatic text correction for
arabic. Proc. of the EMNLP 2014 Workshop on Ara-
bic Natural Language Processing, pages 39–47.
Courtney Napoles, Keisuke Sakaguchi, Matt Post, and
Joel Tetreault. 2015. Ground truth for grammatical-
ity correction metrics. In Proc. of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 588–593. ACL.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proc. of the 17th Conference on Computational
Natural Language Learning, pages 1–12. ACL.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris-
tian Hadiwinoto, Raymond Hendy Susanto, , and
Christopher Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In
Proc. of the Eighteenth Conference on Computa-
tional Natural Language Learning, pages 1–14.
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. of the
40th Annual Meeting on ACL, pages 311–318. ACL.
Keisuke Sakaguchi, Matt Post, and Benjamin
Van Durme. 2014. Efficient elicitation of annota-
tions for human evaluation of machine translation.
In Proc. of the Ninth Workshop on Statistical
Machine Translation, pages 1–11. ACL.
</reference>
<page confidence="0.998192">
470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.359218">
<title confidence="0.997318">Human Evaluation of Grammatical Error Correction Systems</title>
<affiliation confidence="0.9226785">apos;Information Systems Laboratory Faculty of Mathematics and Computer Science, Adam Mickiewicz</affiliation>
<email confidence="0.87304">romang@amu.edu.pl</email>
<email confidence="0.87304">junczys@amu.edu.pl</email>
<author confidence="0.583656">&apos;Faculty of English</author>
<author confidence="0.583656">Adam Mickiewicz</author>
<email confidence="0.941624">egillian@pwsz.pl</email>
<abstract confidence="0.998621133333333">The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="13463" citStr="Bojar et al., 2013" startWordPosition="2291" endWordPosition="2294">an other systems based on the collected pairwise rankings. While previously introduced methods for producing rankings, total orderings, as well as partial orderings at chosen confidencelevels, can be directly applied to our data, determining which ranking is more accurate turns out to be methodologically and computationally more involved due to the specific nature of GEC outputs. 5.1 Ranking methods We adapt two ranking methods applied during WMT13 and WMT14 to GEC evaluation: the Expected Wins method and a version of TrueSkill. Expected Wins. Expected Wins (EW) has been introduced for WMT13 (Bojar et al., 2013) and is based on an underlying model of “relative ability” proposed in Koehn (2012). One advantage of this method is its intuitiveness; the scores reflect the probability that a system Si will be ranked better than another system that has been randomly chosen from a pool of opponents {Sj : j =� i}. Defining the function win(A, B) as the number of times system A is ranked better than system B, Bojar et K = 464 # System P R M20.5 # Score Range System # Score Range System 1 CAMB 0.397 0.301 0.373 1 0.628 1 AMU 1 0.273 1 AMU 2 CUUI 0.417 0.248 0.367 2 0.566 2-3 RAC 2 0.182 2 CAMB 3 AMU 0.416 0.214</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Eighth Workshop on Statistical Machine Translation, pages 1–44. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
</authors>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2761" citStr="Bojar et al., 2014" startWordPosition="420" endWordPosition="423">ssments of machine translation outputs that have been used to evaluate automatic metrics. We believe that the unavailability of this kind of quality assessment may stall the development of GEC, as all the shared tasks and the entire field have to cope with an inherent uncertainty of their methods and metrics. We hope to make a step towards alleviating this lack of confidence by presenting the results of the first1 large-scale human evaluation of automatic grammatical error correction systems submitted to the CoNLL-2014 shared task. Most of our inspiration is drawn from the recent WMT edition (Bojar et al., 2014) and its metrics task (Macháˇcek and Bojar, 2014). We also provide an analysis of correlation between the standard metrics in GEC and human judgment and show that the commonly used parameters for standard metrics in the shared task may not be optimal. The uncertainty about metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example. Based on human judgments we can show that this proposed metric maybe less useful than hoped. 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics an</context>
<context position="17313" citStr="Bojar et al. (2014)" startWordPosition="3118" endWordPosition="3121"> estimates at the beginning and smaller changes after a number of consistent matches has been played. Due to that TS can identify the ability of individual players from a smaller number of pairwise comparisons. A modification of this approach to the WMT manual evaluation procedure by Sakaguchi et al. (2014) has been adopted as the official ranking method during WMT14 replacing EW. The TrueSkill scores are calculated as inferred means: scoreTS(Si) = µSi. 465 5.2 Rank clusters Both ranking methods produce total orderings without information on the statistical significance of the obtained ranks. Bojar et al. (2014) notice that the similarity of the participants in terms of methods and training data causes some of them to be very similar and group systems into equivalence classes as proposed by Koehn (2012). Although the methods and training data among the systems examined in this paper are quite diverse, a great similarity of produced outputs is an inherent property of GEC. Therefore, in this section, for each system Sj placed on rank rj we also try to determine the true systems rank ranges [r� j, . . . ,r00j ] at a confidence-level of 95% and clusters of equivalent systems by following the procedure ou</context>
<context position="18701" citStr="Bojar et al. (2014)" startWordPosition="3366" endWordPosition="3369">s sample a new ranking is produced. After repeating this process a 1000 times the obtained 1000 ranks for Sj are sorted, with the top 25 and bottom 25 ranks being discarded. The interval of the remaining ranks serves as the final rank range. Next, these rank ranges are used to produce clusters of overlapping rank ranges. This is the last step required to produce the rankings in Tables 3b and 3c for both methods, EW and TS, respectively. 5.3 Choosing the final ranking Now, we face the question which ranking should be presented as the final result of the human evaluation task. Again, we turn to Bojar et al. (2014) who choose their rankings based on the ranking model’s ability to predict pairwise rankings. Accuracy is computed by 100-fold cross-validation. For each fold a new ranking is trained from 99 parts with the left-over part serving as test data. In a first step, we calculate the accuracy of the unclustered total orderings discarding ties. A ranking based on model scores alone cannot predict ties, this requires equivalence classes. Bojar et al. (2014) define a draw radius r such that systems whose scores differ by less than r are assigned to one cluster, r is tuned to maximize accuracy. In our ca</context>
<context position="20291" citStr="Bojar et al. (2014)" startWordPosition="3639" endWordPosition="3642">n of pairwise judgments. By fixing p &lt; 0.05 we directly evaluate rankings of the form given in Table 3. The absolute values of scores and their different interpretations between methods become irrelevant which makes it unnecessary to tune a parameter like r. The main drawback of this approach is its computational cost. For each of the 100 folds we bootstrap another 100 rankings with EW and TS, fix p &lt; 0.05 and calculate rank clusters. The single clustered ranking for each fold is then used to calculate accuracy for the held-out test data. For our data, contrary to the MT-specific results from Bojar et al. (2014), EW beats TS in both cases (Table 4). We therefore present the ExpectedWins-based ranking (Table 3b) as the final result of the human evaluation effort described in this work and refer to it in the remainder of the paper when the human ranking is mentioned. 5.4 Analysis The final human-created ranking (Table 3b) consists of four non-overlapping rank clusters. Rank ranges have been calculated at a confidence level of 95%. Comparing the official CoNLL-2014 ranking (Table 3a) with the manually created ExpectedWins ranking shows interesting differences. The AMU system is judged to be a clear lead</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. of the Ninth Workshop on Statistical Machine Translation, pages 12–58. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="21981" citStr="Callison-Burch et al., 2008" startWordPosition="3932" endWordPosition="3935">put in the second to last position. Two systems are judged to be worse than INPUT. The rank cluster that includes INPUT is the largest among the four clusters. We also include pairwise comparisons between all systems according to EW in Table 3d. Each cell contains the percentage of times the system in that column was judged to be better than the system in 466 that row. Bold values mark the winner. We applied the Sign Test to measure statistically significant differences, ? indicates statistical significance at p &lt; 0.10, t at p &lt; 0.05, and tatp&lt;0.01. 6 Correlation with GEC metrics Since WMT08 (Callison-Burch et al., 2008) the “metrics task” has been part of the WMT. The aim of the metrics task is to assess the quality of automatic evaluation metrics for MT in terms of correlation with the collected human judgments. We attempt the same in the context of GEC. 6.1 Measures of correlation Based on Macháˇcek and Bojar (2013), we use Spearman’s rank correlation ρ and Pearson’s r to compare the similarity of rankings produced by various metrics to the manual ranking from the previous section. Spearman’s rank correlation ρ. Spearman’s ρ for rankings with no ties is defined as 6 P d2 i ρ = 1 − n(n2 − 1) where di is the</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proc. of the Third Workshop on Statistical Machine Translation, pages 70–106. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Markus Dickinson</author>
<author>Ross Israel</author>
<author>Joel R Tetreault</author>
</authors>
<title>Problems in evaluating grammatical error detection systems.</title>
<date>2012</date>
<booktitle>In Proc. of COLING 2012,</booktitle>
<pages>611--628</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3654" citStr="Chodorow et al. (2012)" startWordPosition="571" endWordPosition="574">ut metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example. Based on human judgments we can show that this proposed metric maybe less useful than hoped. 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1During the camera-ready preparation phase, we learned about similar research by Napoles et al. (2015). After contacting the authors, it was agreed to treat both works as fully concurrent. Future work will compare the results. 461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion issues in error detection which make it hard to compare different approaches. The lack of consensus is due to the nature of the error det</context>
<context position="28185" citStr="Chodorow et al. (2012)" startWordPosition="5048" endWordPosition="5051">wers, we probably ended up with more questions. The following (and more) might be investigated in the future: What makes the winning system special and why do the standard metrics fail at identifying this system? Can we come up with better system-level metrics? Can meaningful sentence-level metrics be developed? Outside the scope of the particular data, we need to wonder if our results generalize to other shared tasks and other languages. The CoNLL-2014 data concerns ESL learners only and may not be transferable to systems for native speakers. This would be in line with the ideas developed by Chodorow et al. (2012). We would hope to see similar endeavors for the other shared tasks as this would enable the field to draw more general conclusions. Obtaining the data The presented data and tools are available from: https://github.com/grammatical/ evaluation Acknowledgements Partially funded by the Polish National Science Centre (Grant No. 2014/15/N/ST6/02330). The authors would like to thank the following judges for their hard work on the ranking task: Sam Bennett, Peter Dunne, Stacia Levy, Kenneth Turner, and John Winward. 0.00 0.25 0.50 0.75 1.00 0.70 0.65 0.60 0.55 0.75 0.70 0.65 0.60 0.55 Annotator1 Ann</context>
</contexts>
<marker>Chodorow, Dickinson, Israel, Tetreault, 2012</marker>
<rawString>Martin Chodorow, Markus Dickinson, Ross Israel, and Joel R Tetreault. 2012. Problems in evaluating grammatical error detection systems. In Proc. of COLING 2012, pages 611–628. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="10973" citStr="Cohen, 1960" startWordPosition="1848" endWordPosition="1849"> (550) 9478 (5539) 5 349 3099 (766) 17107 (8972) 6 400 3474 (517) 19313 (9209) 7 70 646 (145) 3383 (1593) 8 200 1815 (681) 8848 (5525) Total 2319 20516 (5694) 109098 (59117) Table 1: Statistics for collected rankings (Ranks), unexpanded and expanded pairwise judgments, numbers for ties are given in parentheses. between systems (on average there are only 5.7 unique outputs among 13 systems). 4.4 Inter- and intra-annotator agreement Again inspired by the WMT evaluation campaigns, we compute annotator agreement as a measure of reliability of the pairwise judgments with Cohen’s kappa coefficient (Cohen, 1960): P(A) − P(E) 1 − P(E) . where P(A) is the proportion of times that annotators agree, and P(E) is the proportion of times that they would agree by chance. K assumes values from 0 (no agreement) to 1 (perfect agreement). All probabilities are computed as ratios of empirically counted pairwise judgments. As the judges worked on collapsed outputs, we calculate agreement scores for unexpanded pairs; otherwise, the high overlap would unfairly increase agreement. P(A) is calculated by examining all pairs of outputs which have been judged by two or more judges, and counting the proportion of times th</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>568--572</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="5412" citStr="Dahlmeier and Ng, 2012" startWordPosition="854" endWordPosition="857"> in English essays written by second language learners of English. Training and test data was annotated with 28 error types. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use publicly available resources for training. Twenty-five student non-native speakers of English were recruited to write essays to be used as test data. Each student wrote two essays. The 50 test essays were error-annotated by two English native speakers. The essays and error annotations were made available after the task. The MaxMatch (M2) scorer (Dahlmeier and Ng, 2012) has been used as the official shared task evaluation metric. 4 Data collection 4.1 Sampling sentences for evaluation The system outputs of the CoNLL-2014 shared task serve as evaluation data. The test set consists of 1312 sentences, there are twelve system outputs available. The thirteenth participant NARA is missing from this set. However, in GEC evaluation there is also the input to consider. Often system outputs are equal to the unmodified input, as it is most desirable if there are in fact no errors. We include INPUT as the thirteenth system. Due to the small number of modifications that </context>
<context position="23264" citStr="Dahlmeier and Ng, 2012" startWordPosition="4167" endWordPosition="4170">he number of systems. Pearson’s r. Macháˇcek and Bojar (2013) find that Spearman’s ρ is too harsh and propose to also use Pearson’s r, calculated as where H and M are the vectors of human and metric scores, H and M are corresponding means. 6.2 Metrics The inventory of evaluation metrics for GEC is significantly smaller than for MT. We hope that making our data available will fuel the interest in this area. The following metrics are assessed: MaxMatch (M2). Due to its adoption as the main evaluation metric of the CoNLL shared tasks and the QALB shared tasks (Mohit et al., 2014), the M2 metric (Dahlmeier and Ng, 2012) can be seen as a de facto standard. Being an Fβ-score, M2 results are most influenced by the choice of β. Between the CoNLL-2013 and CoNLL-2014 shared tasks, the organizers changed β from 1.0 to 0.5, and motivate this with intuition alone. The QALB shared tasks for Arabic continue to use β = 1.0. Metric Spearman’s ρ Pearson’s r M2 F1.0 0.648 0.610 M2 F0.5∗ 0.692 0.627 M2 F0.25 0.720 0.680 M2 F0.18 0.758 0.701 M2 F0.1 0.670 0.652 I-WAcc -0.154 -0.098 BLEU -0.346 -0.240 METEOR -0.374 -0.241 Table 5: Correlation results for various metrics and human ranking. 1 2 3 4 5 6 7 8 ρ ρ 1 – .70 .31 .76 .</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proc. of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 568–572. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping our own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proc. of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1291" citStr="Dale and Kilgarriff, 2011" startWordPosition="181" endWordPosition="184">ion evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system output</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proc. of the 13th European Workshop on Natural Language Generation, pages 242–249. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>Hoo 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proc. of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1311" citStr="Dale et al., 2012" startWordPosition="185" endWordPosition="188">ve been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and </context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. Hoo 2012: A report on the preposition and determiner error correction shared task. In Proc. of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proc. of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="25022" citStr="Denkowski and Lavie, 2011" startWordPosition="4501" endWordPosition="4504">hortcomings of M2. The inclusion of true negatives into the formula makes this a very conservative metric; quite similar to the MT metrics described below. The metric assigns negative weights to systems that are harmful with regard to the input text, values from the range [1, −1] are possible. The reported correlation values have been calculated for the ranking presented in Felice and Briscoe (2015). Machine translation evaluation metrics. Basing most of our results on findings from MT, we also take a look at two machine translation evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011). In order to use the CoNLL-2014 gold standard with these metrics, the edit-based annotation has been converted into two plain text files, one per annotator. 6.3 Analysis The correlation results are collected in table 5. The M2 metric is generally moderately correlated with Pn Hi − i=1 r = qPn qPn i=1(Hi − ¯H)2 i=1(Mi −¯M)2 H)(Mi − M) 467 Values of Q Figure 3: Spearman’s p and Pearson’s r correlation of M2 with human judgment w.r.t. Q. Dashed line marks official CoNLL-2014 choice Q = 0.5. human judgment and is on the brink of high correlation for values of Q closer to 0.2. Compared to M2, the </context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proc. of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Federmann</author>
</authors>
<title>Appraise: An opensource toolkit for manual phrase-based evaluation of translations.</title>
<date>2010</date>
<booktitle>In Proc. of the Seventh International Conference on Language Resources and Evaluation (LREC’10). ELRA.</booktitle>
<contexts>
<context position="8126" citStr="Federmann, 2010" startWordPosition="1372" endWordPosition="1374">he ratio of pairwise comparisons of M versus Ci different systems. By normalizing over 1 2 3 4 5 6 7 8 9 10111213 No. of sentences 200 100 0 ~M � where 2 p0 i = ~Ci~ 2 462 5 4 3 2 1 A B C D E Output A B C D E Output (b) Overlapping rankings. Rank Rank 4 3 5 2 1 (a) Screenshot of Appraise modified for GEC judgment. Figure 2: Displayed ranking and corresponding overlapping rankings. the entire set of output sets we obtain the probability pi of sampling the i-th set of outputs as 4.2 Collecting system rankings The sets of outputs sampled with the described method have been prepared for Appraise (Federmann, 2010) and presented to the judges. Judges were asked to rank sentences from best to worse. Ties are allowed. Judges were aware that the absolute ranks bear no relevance as ranks are later turned into relative pairwise judgments. No notion of “better” or “worse” was imposed by the authors, we relied on the judges to develop their own intuition. All eight judges are English native speakers and have extensive backgrounds in linguistics. Figure 2a displays a screen shot of Appraise with a judged sentence. Several modifications to the Appraise framework2 were implemented to account for the specific natu</context>
</contexts>
<marker>Federmann, 2010</marker>
<rawString>Christian Federmann. 2010. Appraise: An opensource toolkit for manual phrase-based evaluation of translations. In Proc. of the Seventh International Conference on Language Resources and Evaluation (LREC’10). ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Felice</author>
<author>Ted Briscoe</author>
</authors>
<title>Towards a standard evaluation method for grammatical error detection and correction.</title>
<date>2015</date>
<booktitle>In Proc. of the 2015 Conference of the North American Chapter of the ACL: Human Language Technologies (NAACL-HLT</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3116" citStr="Felice and Briscoe (2015)" startWordPosition="482" endWordPosition="485">ting this lack of confidence by presenting the results of the first1 large-scale human evaluation of automatic grammatical error correction systems submitted to the CoNLL-2014 shared task. Most of our inspiration is drawn from the recent WMT edition (Bojar et al., 2014) and its metrics task (Macháˇcek and Bojar, 2014). We also provide an analysis of correlation between the standard metrics in GEC and human judgment and show that the commonly used parameters for standard metrics in the shared task may not be optimal. The uncertainty about metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example. Based on human judgments we can show that this proposed metric maybe less useful than hoped. 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1During the camera-ready pre</context>
<context position="4479" citStr="Felice and Briscoe (2015)" startWordPosition="707" endWordPosition="710">s as fully concurrent. Future work will compare the results. 461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion issues in error detection which make it hard to compare different approaches. The lack of consensus is due to the nature of the error detection task. The authors argue that the choice of the metric should take into account factors such as the skew of the data and the application that the system is used for. The most recent addition is Felice and Briscoe (2015) who present a novel evaluation method for grammatical error correction that scores systems in terms of improvement on the original text. 3 The CoNLL-2014 shared task The goal of the CoNLL-2014 shared task (Ng et al., 2014) was to evaluate algorithms and systems for automatically correcting grammatical errors in English essays written by second language learners of English. Training and test data was annotated with 28 error types. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use publicly available resources for train</context>
<context position="24373" citStr="Felice and Briscoe, 2015" startWordPosition="4393" endWordPosition="4396">374 -0.241 Table 5: Correlation results for various metrics and human ranking. 1 2 3 4 5 6 7 8 ρ ρ 1 – .70 .31 .76 .74 .19 .62 .48 .70 2 .72 – .77 .84 .90 .57 .59 .64 .93 3 .53 .89 – .66 .70 .58 .42 .64 .63 4 .82 .79 .69 – .91 .42 .67 .54 .91 5 .65 .85 .82 .87 – .63 .63 .51 .93 .72 6 .32 .71 .67 .56 .86 – .63 .39 .42 7 .72 .74 .57 .76 .72 .63 – .63 .76 8 .64 .85 .86 .69 .72 .57 .75 – .60 r .67 .93 .82 .87 .92 .66 .80 .82 – r¯ .80 Table 6: Inter-annotator correlation (Spearman’s ρ above the diagonal, Pearson’s r below). I-measure/Weighted Accuracy (I-WAcc). The recently proposed I-WAcc metric (Felice and Briscoe, 2015) tries to address the shortcomings of M2. The inclusion of true negatives into the formula makes this a very conservative metric; quite similar to the MT metrics described below. The metric assigns negative weights to systems that are harmful with regard to the input text, values from the range [1, −1] are possible. The reported correlation values have been calculated for the ranking presented in Felice and Briscoe (2015). Machine translation evaluation metrics. Basing most of our results on findings from MT, we also take a look at two machine translation evaluation metrics, BLEU (Papineni et </context>
</contexts>
<marker>Felice, Briscoe, 2015</marker>
<rawString>Mariano Felice and Ted Briscoe. 2015. Towards a standard evaluation method for grammatical error detection and correction. In Proc. of the 2015 Conference of the North American Chapter of the ACL: Human Language Technologies (NAACL-HLT 2015). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Tom Minka</author>
<author>Thore Graepel</author>
</authors>
<title>Trueskill(tm): A bayesian skill rating system.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<pages>569--576</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16253" citStr="Herbrich et al., 2007" startWordPosition="2943" endWordPosition="2947"> ‡ .57 ‡ .73 † .54 ? .52 .57 .47 – .43 ‡ .22 ‡ NTHU .69 ‡ .57 ‡ .57 ‡ .58 ‡ .58 ‡ .58 ‡ .54 † .55 ‡ .57 ‡ .54 ? .57 ‡ – .41 ‡ IPN .76 ‡ .72 ‡ .66 ‡ .68 ‡ .71 ‡ .79 ‡ .65 ‡ .65 ‡ .73 ‡ .70 ‡ .78 ‡ .59 ‡ – (d) Head-to-head comparison for ExpectedWins at P &lt; 0.10 (*), P &lt; 0.05 (†), and P &lt; 0.01 (‡). Table 3: Comparison of official CoNLL-2014 ranking and human rankings. Ranges and clusters have been calculated with bootstrap resampling at p ≤ 0.05. al. (2013) calculate EW scores as follows: scoreEW(Si) = 1win(Si, Sj) {Sj} |j _ win(Si, Sj) + win(Sj, Si). ,j TrueSkill. The TrueSkill ranking system (Herbrich et al., 2007) is a skill based ranking system for Xbox Live developed at Microsoft Research. It is used to identify and model player (GEC systems in our case) ability in a game to assign players to competitive matches. The TrueSkill ranking system models each player Si by two parameters: the average relative ability µSi and the degree of uncertainty in the player’s ability σ2Si. Maintaining uncertainty allows TS to make greater changes to the ability estimates at the beginning and smaller changes after a number of consistent matches has been played. Due to that TS can identify the ability of individual pla</context>
</contexts>
<marker>Herbrich, Minka, Graepel, 2007</marker>
<rawString>Ralf Herbrich, Tom Minka, and Thore Graepel. 2007. Trueskill(tm): A bayesian skill rating system. In Advances in Neural Information Processing Systems 20, pages 569–576. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>179--184</pages>
<contexts>
<context position="13546" citStr="Koehn (2012)" startWordPosition="2307" endWordPosition="2308">hods for producing rankings, total orderings, as well as partial orderings at chosen confidencelevels, can be directly applied to our data, determining which ranking is more accurate turns out to be methodologically and computationally more involved due to the specific nature of GEC outputs. 5.1 Ranking methods We adapt two ranking methods applied during WMT13 and WMT14 to GEC evaluation: the Expected Wins method and a version of TrueSkill. Expected Wins. Expected Wins (EW) has been introduced for WMT13 (Bojar et al., 2013) and is based on an underlying model of “relative ability” proposed in Koehn (2012). One advantage of this method is its intuitiveness; the scores reflect the probability that a system Si will be ranked better than another system that has been randomly chosen from a pool of opponents {Sj : j =� i}. Defining the function win(A, B) as the number of times system A is ranked better than system B, Bojar et K = 464 # System P R M20.5 # Score Range System # Score Range System 1 CAMB 0.397 0.301 0.373 1 0.628 1 AMU 1 0.273 1 AMU 2 CUUI 0.417 0.248 0.367 2 0.566 2-3 RAC 2 0.182 2 CAMB 3 AMU 0.416 0.214 0.350 0.561 2-4 CAMB 3 0.114 3-4 RAC 4 POST 0.345 0.217 0.308 0.550 3-5 CUUI 0.105</context>
<context position="17508" citStr="Koehn (2012)" startWordPosition="3154" endWordPosition="3155">arisons. A modification of this approach to the WMT manual evaluation procedure by Sakaguchi et al. (2014) has been adopted as the official ranking method during WMT14 replacing EW. The TrueSkill scores are calculated as inferred means: scoreTS(Si) = µSi. 465 5.2 Rank clusters Both ranking methods produce total orderings without information on the statistical significance of the obtained ranks. Bojar et al. (2014) notice that the similarity of the participants in terms of methods and training data causes some of them to be very similar and group systems into equivalence classes as proposed by Koehn (2012). Although the methods and training data among the systems examined in this paper are quite diverse, a great similarity of produced outputs is an inherent property of GEC. Therefore, in this section, for each system Sj placed on rank rj we also try to determine the true systems rank ranges [r� j, . . . ,r00j ] at a confidence-level of 95% and clusters of equivalent systems by following the procedure outlined by Koehn (2012). This is accomplished by applying bootstrap resampling. Pairwise rankings are drawn from the set of judgments with multiple drawings. Based on this sample a new ranking is </context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In International Workshop on Spoken Language Translation, pages 179–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Macháˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>ACL.</publisher>
<marker>Macháˇcek, Bojar, 2013</marker>
<rawString>Matou&amp;quot;s Macháˇcek and Ondˇrej Bojar. 2013. Results of the WMT13 metrics shared task. In Proc. of the Eighth Workshop on Statistical Machine Translation, pages 45–51. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Macháˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>293--301</pages>
<publisher>ACL.</publisher>
<marker>Macháˇcek, Bojar, 2014</marker>
<rawString>Matou&amp;quot;s Macháˇcek and Ondˇrej Bojar. 2014. Results of the WMT14 metrics shared task. In Proc. of the Ninth Workshop on Statistical Machine Translation, pages 293–301. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
<author>Alla Rozovskaya</author>
</authors>
<title>They can help: using crowdsourcing to improve the evaluation of grammatical error detection systems.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies,</booktitle>
<pages>508--513</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3283" citStr="Madnani et al. (2011)" startWordPosition="511" endWordPosition="514">2014 shared task. Most of our inspiration is drawn from the recent WMT edition (Bojar et al., 2014) and its metrics task (Macháˇcek and Bojar, 2014). We also provide an analysis of correlation between the standard metrics in GEC and human judgment and show that the commonly used parameters for standard metrics in the shared task may not be optimal. The uncertainty about metrics quality leads to proposals of new metrics, with Felice and Briscoe (2015) being a recent example. Based on human judgments we can show that this proposed metric maybe less useful than hoped. 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1During the camera-ready preparation phase, we learned about similar research by Napoles et al. (2015). After contacting the authors, it was agreed to treat both works as fully concurrent. Future</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, Rozovskaya, 2011</marker>
<rawString>Nitin Madnani, Joel Tetreault, Martin Chodorow, and Alla Rozovskaya. 2011. They can help: using crowdsourcing to improve the evaluation of grammatical error detection systems. In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies, pages 508–513. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Alla Rozovskaya</author>
<author>Nizar Habash</author>
<author>Wajdi Zaghouani</author>
<author>Ossama Obeid</author>
</authors>
<title>The first QALB shared task on automatic text correction for arabic.</title>
<date>2014</date>
<booktitle>Proc. of the EMNLP 2014 Workshop on Arabic Natural Language Processing,</booktitle>
<pages>39--47</pages>
<contexts>
<context position="1426" citStr="Mohit et al., 2014" startWordPosition="204" endWordPosition="207">or grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of the WMTs that automatic measures of machine translation quality are an imperfect substitute for human assessments. Therefore, manual evaluation of the system outputs are conducted and their results are reported as the final rankings of the workshops. These human evaluation campaigns are an importan</context>
<context position="23224" citStr="Mohit et al., 2014" startWordPosition="4160" endWordPosition="4163">and metric rank for system i, n is the number of systems. Pearson’s r. Macháˇcek and Bojar (2013) find that Spearman’s ρ is too harsh and propose to also use Pearson’s r, calculated as where H and M are the vectors of human and metric scores, H and M are corresponding means. 6.2 Metrics The inventory of evaluation metrics for GEC is significantly smaller than for MT. We hope that making our data available will fuel the interest in this area. The following metrics are assessed: MaxMatch (M2). Due to its adoption as the main evaluation metric of the CoNLL shared tasks and the QALB shared tasks (Mohit et al., 2014), the M2 metric (Dahlmeier and Ng, 2012) can be seen as a de facto standard. Being an Fβ-score, M2 results are most influenced by the choice of β. Between the CoNLL-2013 and CoNLL-2014 shared tasks, the organizers changed β from 1.0 to 0.5, and motivate this with intuition alone. The QALB shared tasks for Arabic continue to use β = 1.0. Metric Spearman’s ρ Pearson’s r M2 F1.0 0.648 0.610 M2 F0.5∗ 0.692 0.627 M2 F0.25 0.720 0.680 M2 F0.18 0.758 0.701 M2 F0.1 0.670 0.652 I-WAcc -0.154 -0.098 BLEU -0.346 -0.240 METEOR -0.374 -0.241 Table 5: Correlation results for various metrics and human rankin</context>
</contexts>
<marker>Mohit, Rozovskaya, Habash, Zaghouani, Obeid, 2014</marker>
<rawString>Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014. The first QALB shared task on automatic text correction for arabic. Proc. of the EMNLP 2014 Workshop on Arabic Natural Language Processing, pages 39–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Keisuke Sakaguchi</author>
<author>Matt Post</author>
<author>Joel Tetreault</author>
</authors>
<title>Ground truth for grammaticality correction metrics.</title>
<date>2015</date>
<booktitle>In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>588--593</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3790" citStr="Napoles et al. (2015)" startWordPosition="592" endWordPosition="595">can show that this proposed metric maybe less useful than hoped. 2 Evaluation of GEC systems Madnani et al. (2011) addresses two problems of GEC evaluation: 1) a lack of informative metrics and 2) an inability to directly compare the performance of systems developed by different researchers. Two evaluation methodologies are presented, both based on crowdsourcing which are used to grade types of errors rather than system performance as presented in this work. Chodorow et al. (2012) draw attention to the many evalua1During the camera-ready preparation phase, we learned about similar research by Napoles et al. (2015). After contacting the authors, it was agreed to treat both works as fully concurrent. Future work will compare the results. 461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion issues in error detection which make it hard to compare different approaches. The lack of consensus is due to the nature of the error detection task. The authors argue that the choice of the metric should take into account factors such as the skew of the data and the appli</context>
</contexts>
<marker>Napoles, Sakaguchi, Post, Tetreault, 2015</marker>
<rawString>Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground truth for grammaticality correction metrics. In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 588–593. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The CoNLL2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proc. of the 17th Conference on Computational Natural Language Learning,</booktitle>
<pages>1--12</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1109" citStr="Ng et al., 2013" startWordPosition="150" endWordPosition="153">the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is </context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL2013 shared task on grammatical error correction. In Proc. of the 17th Conference on Computational Natural Language Learning, pages 1–12. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
</authors>
<title>The CoNLL-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proc. of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--14</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1127" citStr="Ng et al., 2014" startWordPosition="154" endWordPosition="157">ut of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment. 1 Introduction The field of automatic grammatical error correction (GEC) has seen a number of shared tasks of different scope and for different languages. The most impactful were the CoNLL-2013 and CoNLL-2014 (Ng et al., 2013; Ng et al., 2014) shared tasks on Grammatical Error Correction for ESL (English as a second language) learners. They were preceded by the HOO shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012). Shared tasks for other languages took place as well, including the QALB workshops for Arabic (Mohit et al., 2014) and NLP-TEA competitions for Chinese. These tasks use automatic metrics to determine the quality of the participating systems. However, these efforts pale in comparison to competitions organized in other fields, e.g. during the annual Workshops for Machine Translation (WMT). It is a central idea of </context>
<context position="4702" citStr="Ng et al., 2014" startWordPosition="744" endWordPosition="747">omputational Linguistics. tion issues in error detection which make it hard to compare different approaches. The lack of consensus is due to the nature of the error detection task. The authors argue that the choice of the metric should take into account factors such as the skew of the data and the application that the system is used for. The most recent addition is Felice and Briscoe (2015) who present a novel evaluation method for grammatical error correction that scores systems in terms of improvement on the original text. 3 The CoNLL-2014 shared task The goal of the CoNLL-2014 shared task (Ng et al., 2014) was to evaluate algorithms and systems for automatically correcting grammatical errors in English essays written by second language learners of English. Training and test data was annotated with 28 error types. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use publicly available resources for training. Twenty-five student non-native speakers of English were recruited to write essays to be used as test data. Each student wrote two essays. The 50 test essays were error-annotated by two English native speakers. The essa</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, , and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proc. of the Eighteenth Conference on Computational Natural Language Learning, pages 1–14. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting on ACL,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="24983" citStr="Papineni et al., 2002" startWordPosition="4495" endWordPosition="4498">iscoe, 2015) tries to address the shortcomings of M2. The inclusion of true negatives into the formula makes this a very conservative metric; quite similar to the MT metrics described below. The metric assigns negative weights to systems that are harmful with regard to the input text, values from the range [1, −1] are possible. The reported correlation values have been calculated for the ranking presented in Felice and Briscoe (2015). Machine translation evaluation metrics. Basing most of our results on findings from MT, we also take a look at two machine translation evaluation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011). In order to use the CoNLL-2014 gold standard with these metrics, the edit-based annotation has been converted into two plain text files, one per annotator. 6.3 Analysis The correlation results are collected in table 5. The M2 metric is generally moderately correlated with Pn Hi − i=1 r = qPn qPn i=1(Hi − ¯H)2 i=1(Mi −¯M)2 H)(Mi − M) 467 Values of Q Figure 3: Spearman’s p and Pearson’s r correlation of M2 with human judgment w.r.t. Q. Dashed line marks official CoNLL-2014 choice Q = 0.5. human judgment and is on the brink of high correlation for values o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting on ACL, pages 311–318. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keisuke Sakaguchi</author>
<author>Matt Post</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Efficient elicitation of annotations for human evaluation of machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--11</pages>
<publisher>ACL.</publisher>
<marker>Sakaguchi, Post, Van Durme, 2014</marker>
<rawString>Keisuke Sakaguchi, Matt Post, and Benjamin Van Durme. 2014. Efficient elicitation of annotations for human evaluation of machine translation. In Proc. of the Ninth Workshop on Statistical Machine Translation, pages 1–11. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
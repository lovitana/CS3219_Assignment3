<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002943">
<title confidence="0.94474">
Motivating Personality-aware Machine Translation
</title>
<author confidence="0.884233">
Shachar Mirkin∗
</author>
<affiliation confidence="0.678582">
IBM Research - Haifa
</affiliation>
<address confidence="0.956334">
Mount Carmel, Haifa
31905, Israel
</address>
<email confidence="0.998625">
shacharm@il.ibm.com
</email>
<author confidence="0.945474">
Scott Nowson, Caroline Brun, Julien Perez
</author>
<affiliation confidence="0.888463">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.8615105">
6 chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.997998">
firstname.surname@xrce.xerox.com
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998691">
Language use is known to be influenced
by personality traits as well as by socio-
demographic characteristics such as age or
mother tongue. As a result, it is possible to au-
tomatically identify these traits of the author
from her texts. It has recently been shown that
knowledge of such dimensions can improve
performance in NLP tasks such as topic and
sentiment modeling. We posit that machine
translation is another application that should
be personalized. In order to motivate this, we
explore whether translation preserves demo-
graphic and psychometric traits. We show that,
largely, both translation of the source training
data into the target language, and the target test
data into the source language has a detrimen-
tal effect on the accuracy of predicting author
traits. We argue that this supports the need for
personal and personality-aware machine trans-
lation models.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984563338983051">
Computational personality recognition is garnering in-
creasing interest with a number of recent workshops
exploring the topic (Celli et al., 2014; Tkalˇciˇc et al.,
2014). The addition of personality as target traits in the
PAN Author Profiling challenge in 2015 (Rangel et al.,
2015) is further evidence. Such user modeling – when
performed on text – is built on a long-standing un-
derstanding that language use is influenced by socio-
demographic characteristics such as age, gender, edu-
cation level or mother tongue and personality traits like
agreeableness or openness (Tannen, 1990; Pennebaker
et al., 2003).
In this work we explore multilingual user modelling.
The motivation is not only to enable modeling in multi-
ple languages, but also to enable modeling multilingual
users who may express different sides of their personal-
ity in each language. One way to address multilingual-
ity in this context is to create models separately in each
language, and then fuse the resulting models. How-
ever, labelled data of this nature, particularly in non-
English languages, is often not available. Personality
∗ This work was mostly done while the first author was
at Xerox Research Centre Europe.
labelling is time consuming, requiring the completion
of psychometric questionnaires which may be consid-
ered invasive by many. An alternative is the use of ma-
chine translation (MT) to bootstrap corpora in resource
poor languages, and to translate the user’s content into
a single language before modeling. Translated text, ei-
ther manually or automatically generated, is known to
have different characteristics than native text. Yet, MT
was shown to be of use within traditional NLP tasks
such as sentiment analysis (Balahur and Turchi, 2012).
We explore the utility of MT for classification of demo-
graphic and personality traits.
MT models, even domain-specific, are user-generic.
Thus, the linguistic signals of user traits which are con-
veyed in the original language may not be preserved
over translation. In other words, the attributes on which
we wish to rely for modelling may be lost. This con-
cern is perhaps most observable with gender, a trait of
the speaker that is encoded in the morphology of many
languages, though not in English. Gendered translation
was the topic of research for many years. However, the
gender of the author is largely ignored by MT systems,
and specifically statistical ones, that would often arbi-
trarily (or rather statistically-based) translate into one
gender form or another. Other demographic and per-
sonality traits have not yet been investigated.
One way to address this concern is personalized
translation, or author-aware translation.1 The first step
toward this goal would be to consider the author traits
in the model. Such an approach has already shown to
be useful for several NLP tasks (Volkova et al., 2013;
Hovy, 2015). However, before embarking on this chal-
lenging task, we explore if the above concerns are
founded by addressing the research question: does MT
has an impact on the classification of demographic and
personality traits?
</bodyText>
<sectionHeader confidence="0.982148" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999194">
Oberlander and Nowson (2006) motivated their study
of computational personality recognition by arguing
that automatically understanding an author’s person-
ality would permit the personalization of sentiment
analysis. Such personalized NLP has recently been
</bodyText>
<footnote confidence="0.99921">
1In this work we investigate MT awareness of the author;
in (Mirkin and Meunier, 2015) we address the task of reader-
aware MT.
</footnote>
<page confidence="0.953265">
1102
</page>
<note confidence="0.668232">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99979365">
explored by Volkova et al. (2013). They incorpo-
rated age and gender features for sentiment analysis,
and show improvements in three different languages.
Hovy (2015) extends this work to other languages and
NLP tasks. Using demographically-informed word em-
beddings, they show improvements in sentiment anal-
ysis, topic classification and trait detection. None of
these works, however, addressed cross-lingual issues.
Yet, personality projection goes beyond automatic
detection of traits – there is also human perception to be
considered. The casual reader may not be aware of per-
sonality related linguistic cues. Yet, studies have shown
that traits can be reliably detected following cold read-
ings of texts from unknown authors (Mehl et al., 2006;
Gill et al., 2012) without such explicit knowledge. Al-
though personality projection in different languages is
under-explored, it has been shown that the relationship
between language use and personality traits varies be-
tween domains (Nowson and Gill, 2014). Thus, while
it would seem that there are cues which translate di-
rectly between languages, this may not always be the
case. In English, for example, women tend to use first-
person pronouns such as “I” more than men (Newman
et al., 2008); but this does not guarantee a gender-based
usage difference for, say, “je” in French. Furthermore,
what happens with more subtle, language-specific in-
dicators of personality? For instance, Nowson (2006)
showed that use of contractions (e.g. don’t vs. do not)
is a marker for the Agreeableness trait. These different
forms do not naturally translate into other languages.
It is doubtful that even a human translator would al-
ways pay attention to such subtleties. In investigating
whether these cues are preserved when a text is trans-
lated, we are also beginning to address the question of
consistency in cues between languages.
MT systems do not explicitly consider demographic
or personality traits. Instead, they often exploit “in-
domain” data to create translation models that are
adapted for the domain of interest (Lu et al., 2007; Fos-
ter et al., 2010; Axelrod et al., 2011; Gong et al., 2012;
Mirkin and Besacier, 2014). The term “domain” has
a wide interpretation in the MT literature and may re-
fer to topic, dialect, genre or style (Chen et al., 2013).
However, to the best of our knowledge, MT domain
adaptation does not extend to consider demographic or
personality traits of the author. Gender in translation
has been researched extensively; in human translation
studies, it has been shown that the gender of transla-
tors impact the translation. In SMT, phrase-based mod-
els (Koehn et al., 2003) can correctly pick-up trans-
lations of gender-inflected words, and rule-based MT
systems and factored models (Koehn and Hoang, 2007)
provide more explicit ways for gender translation. Yet,
most SMT systems are unaware of the gender of the
author, neither in the training nor in the test data, and
are therefore unable to adapt their translation beyond
the local inflectional level; in particular when no such
evidence exists, as in English. To a much greater ex-
tent, this is the case with other demographics, such as
age, and with personality traits.
</bodyText>
<sectionHeader confidence="0.995872" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.99633">
3.1 Hypothesis
</subsectionHeader>
<bodyText confidence="0.999996181818182">
The hypothesis of our broader vision is that personal-
ized MT or author-aware translation is an important ne-
cessity. We believe the human understanding of trans-
lated text (of its explicit and implicit meanings, of its
author and of the full context) would be improved if
author traits are better conveyed.
In order to motivate this future work, this paper ex-
plores a supporting hypothesis: that author traits are
not conveyed accurately under machine translation. We
assess this by investigating whether trait detection per-
forms as well on translated data as on native text.
</bodyText>
<subsectionHeader confidence="0.988911">
3.2 Experimental Framework
</subsectionHeader>
<bodyText confidence="0.918100894736842">
To explore our hypothesis, we require data in multiple
languages which is labelled with socio-demographic or
personality traits. Using English as the base language
(as typically the most resource-rich language in NLP
studies), we perform three comparative experiments on
several non-English (“foreign”) corpora. In these ex-
periments we train a classification model:
1. Using only foreign language data. This provides a
baseline, as no translated data is used.
2. Augmenting the foreign training data with English
data translated into the foreign language. Here,
the goal is to assess a scenario where translations
from a resource-rich language supplement scarce
training data in the foreign language, under the as-
sumption that more training data can be beneficial.
3. Translating the foreign test data into English and
classifying it using a model trained on the English
data. This allows us to explore another practical
scenario, where an English model already exists
and we wish to use it to classify data from another
language for which we do not have a robust model.
For this task we use the data from the 2015 PAN
workshop (Rangel et al., 2015) which is labelled for
author gender and personality traits. For more details
see Section 4.1. We also wish to explore if any af-
fect was due strictly to the use of MT or to translation
(or language change) in general. The PAN corpus is
multi-lingual but does not contain parallel data. Such
parallel corpora, however, are not typically labelled
with the type of author information we wish to inves-
tigate. Therefore we required such a corpus to which
we could easily add labels. For these we used a selec-
tion of TED talks which we labelled for gender (see
Section 4.2). The full details of our approach to text
processing, translation and classification can be found
in our technical paper at the PAN workshop (Nowson
et al., 2015); in the interests of space a compressed ver-
sion is presented here.
</bodyText>
<page confidence="0.936809">
1103
</page>
<subsectionHeader confidence="0.997524">
3.3 Preprocessing and feature extraction
</subsectionHeader>
<bodyText confidence="0.999985928571429">
We use the multilingual parser described by Ait-
Mokhtar et al. (2001) to preprocess the texts and ex-
tract a wide range of features. The parser has been cus-
tomized to handle social media data, e.g. by detecting
hashtags, mentions, and emoticons. For English, we
have integrated a normalization dictionary by Han et al.
(2012) in the preprocessing. The English and French
grammars also include a polarity lexicon to recognize
sentiment bearing words or expressions. The features
we extract include: 1-, 2-, 3-grams of surface, nor-
malized and lemmatized forms; part-of-speech tagged
forms, and n-grams of POS; named entities (places,
persons, organization, dates, time expressions), emoti-
cons, hashtags, mentions and URLs.
</bodyText>
<subsectionHeader confidence="0.98264">
3.4 Learning framework
</subsectionHeader>
<bodyText confidence="0.9999938">
To train classification models we first prune features
with a frequency threshold. Next, the remaining set
of features is compressed using truncated singular
value decomposition (SVD). SVD (Golub and Reinsch,
1970) is a widely used technique in sparse dataset sit-
uations. This method copes with noise present in the
data by extracting the principal dimensions describing
the data and projecting the data to a latent space. In the
truncated version, a low-rank approximation, all but the
top-k dimensions are removed. The result is a dense,
low-dimension representation of the data. Finally, en-
semble models (Schapire, 1990; Dietterich, 2000) are
used to predict trait values: for gender, we use the ma-
jority vote of 10 classifiers; for each personality trait,
we use the mean of 10 regression estimators.
</bodyText>
<subsectionHeader confidence="0.913049">
3.5 Machine translation models
</subsectionHeader>
<bodyText confidence="0.999626947368421">
We created standard machine translation models be-
tween English and each one of Spanish, Italian, French
and Dutch. The details are described below.
Parallel corpora We wished to use the same setting
for all language pairs. To that end, we chose parallel
corpora that are available for all of them, namely Eu-
roparl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012),
from the IWSLT 2014 evaluation campaign (Cettolo et
al., 2014). WIT3, consisting of spoken-language tran-
scripts, represents an in-domain corpus for the TED
dataset and a “near-domain” for PAN. The data con-
sisted of approximately 2 million parallel sentences for
each language pair, with 50 million tokens for each lan-
guage. The Europarl corpus comprised more than 90%
of that data. The two corpora were concatenated to cre-
ate the training data for the MT models.
Translation System Moses (Koehn et al., 2007), an
open-source phrase-based MT system,3 was used to
train translation models and translate the data.
</bodyText>
<footnote confidence="0.994807333333333">
2Version 7, www.statmt.org/europarl
3We used version 3.0, downloaded on 16 Feb 2015 from
www.statmt.org/moses.
</footnote>
<bodyText confidence="0.999859666666667">
Preprocessing We used the standard Moses tools to
preprocess the data, including tokenization, lowercas-
ing and removal of sentence pairs where at least one of
the sentences is empty or longer than 80 tokens.
Recasing and Language models We used
SRILM (Stolcke, 2002) version 1.7.1 to train 5-
gram language models on the target side of the parallel
corpus, with modified Kneser-Ney discounting (Chen
and Goodman, 1996). A recasing model was trained
from the same corpus, with a 3-gram KenLM language
model (Heafield, 2011).
Tuning We tuned the translation models using
MERT (Och, 2003), using the development set of the
above mentioned campaign (dev2010), consisting of
887 sentence pairs for each language pair.
Translation and post-processing Each of the tweets
of the PAN training set was preprocessed in the same
fashion as the training data. It was then translated with
the trained model of the corresponding language pair,
and finally underwent quick post-processing, namely
recasing and detokenization.
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999239">
Personality-tagged datasets in multiple languages are
scarce. We used two datasets, with content from twitter
and TED talks, as described in this section.
</bodyText>
<subsectionHeader confidence="0.913459">
4.1 PAN
</subsectionHeader>
<bodyText confidence="0.9995161">
The first corpus we used was the data of the PAN 2015
Author Profiling task (Rangel et al., 2015), drawn from
Twitter (PAN15). For each user, the data consists of
tweets (average n = 100) and gold standard labels:
gender (Male or Female), and personality. The labels
are provided by the author, with scores on five traits
being calculated via self-assessment responses to the
short Big 5 test, BFI-10 (Rammstedt and John, 2007)),
then normalized between -0.5 and +0.5. Table 1 shows
the volume of data per language for the training set.
</bodyText>
<table confidence="0.9857222">
Language Authors Tweets
English (en) 152 14166
Spanish (es) 100 9879
Italian (it) 38 3687
Dutch (nl) 34 3350
</table>
<tableCaption confidence="0.965765">
Table 1: Number of authors and tweets across the four
languages of the PAN dataset.
</tableCaption>
<subsectionHeader confidence="0.848455">
4.2 TED
</subsectionHeader>
<bodyText confidence="0.9997196">
The PAN15 data allows us to assess personality projec-
tion in multilingual data. In addition to exploring au-
tomatic translation, we wish to compare with manual
translation. We turned to TED talks4 for such com-
parative evaluation. We chose the English-French lan-
</bodyText>
<footnote confidence="0.994608">
4www.ted.com
</footnote>
<page confidence="0.996264">
1104
</page>
<bodyText confidence="0.996846666666667">
guage pair, because French is not a language in PAN15,
but also due to the difficulties in obtaining such data, as
described below.
</bodyText>
<subsubsectionHeader confidence="0.672712">
4.2.1 TED English-French
</subsubsectionHeader>
<bodyText confidence="0.999991727272727">
We use data of the MT track of the IWSLT 2014 Evalu-
ation Campaign, which includes parallel corpora from
transcripts of TED talks. The English-French (en-fr)
corpus consists of 1415 talks, with approximately 190k
sentence pairs and 3 million tokens for each of the
source and target sides (before preprocessing). We an-
notated the gender of each speaker with a simple web
interface. Any talk with multiple speakers or where the
majority is not a speech (e.g. a performance) is dis-
carded. After discarding 59 talks, 1012 (75%) were
annotated as male and 344 (25%) as female.5
</bodyText>
<subsubsectionHeader confidence="0.872128">
4.2.2 TED French-English
</subsubsectionHeader>
<bodyText confidence="0.999985551724138">
The WIT3 data seems to also include data in the fr-en
direction. However, in practice, TED hosts only talks
in English and all foreign to English corpora were col-
lected from the translated versions of the site. We there-
fore turned to TEDx6 for fr-en data. TEDx are in-
dependent TED-style events, often including talks in
languages other than English. Unlike en-fr, there is
no easily accessible parallel data available for fr-en,
where the source is native French. We applied the fol-
lowing procedure to collect the necessary data. We
used the Google YouTube Analytics API7 to search
for videos of talks in French. We have extracted the
list of TEDx events in France and their dates via
www.tedxenfrance.fr.8 Each event-name and
year is used as a query in YouTube, e.g. “TEDx Paris
2011”. For each talk, we download the manual French
and English subtitles, i.e. the transcript and the trans-
lation, respectively. These files were annotated using
the same process and criteria described above. This re-
sulted with a small corpus (TED61fr−en) of 61 talks
of which 32 are annotated as male and 29 as female.
TED61en In order to account for any potential effect
of length, we created a subset of the en-fr corpus, that
is of the same size of the fr-en dataset. We matched
files from the French side of the en-fr corpus to each
of those in the fr-en for gender and length (in tokens).
The French en-fr files were truncated after the nearest
line break to the desired size; the corresponding En-
glish en-fr files were truncated at the same point.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999988">
It has been shown that standard approaches to gender
classification on English texts can be sub-optimal for
non-English language data (Ciot et al., 2013). How-
ever, state-of-the-art classification results are not our
focus; rather, our intention is to understand the impact
</bodyText>
<footnote confidence="0.99985375">
5Annotation data is available at cm.xrce.xerox.com
6www.ted.com/watch/tedx-talks
7developers.google.com/youtube
8Accessed on 23/2/15.
</footnote>
<bodyText confidence="0.9997974">
of translation on classification of socio-demographic
and personality traits. Therefore, we fix our models
with a set of parameters selected via cross-validation
(CV) on the native language: the occurrence threshold
is set to 5 and the SVD dimensionality to 500.
</bodyText>
<subsectionHeader confidence="0.959592">
5.1 PAN
</subsectionHeader>
<bodyText confidence="0.98877528">
For each of the three non-English languages of PAN15
we train classification models as explained in Section 3:
using the original training data, adding training data
translated from English, and translating the test data
into English to use the English-trained model.
Results can be seen in Table 2. For the majority of
the traits, the native results outperform both translation
settings, in some cases by considerable margin. The
assumption posited earlier that more training data is be
beneficial appears not to have held up in this context.
The alternative scenario seems to be doing even worse.
The most distinct results are perhaps the accuracy
of gender prediction (for which each corpora is bal-
anced, thus a baseline of 50%). One explanation may
be that the translation is done from and into English,
which does not express gender via morphology, in con-
trast to Italian and Spanish. An interesting exception is
that adding translated English texts into Dutch consid-
erably improves performance. This may be explained
by the lesser expression of gender in Dutch morphol-
ogy, much like English. In this instance it appears
that adding more data – when translation is between
two gender-agnostic languages – does indeed help. For
both Italian and Dutch, English adds a very substantial
amount of data; the outcomes, however, are opposite.
</bodyText>
<subsectionHeader confidence="0.99067">
5.2 TED
</subsectionHeader>
<bodyText confidence="0.999883541666667">
Though the TED data is currently only labelled for
gender, it allows us to make comparisons between man-
ual and machine translation. First we explored if there
were gender signals in the English corpus which a clas-
sifier could uncover. For this, we performed leave-one-
out CV on each of the following three versions: native
English, manually and machine translated into French.
The results, presented in Table 3, show that some gen-
der signal is lost between manual and machine trans-
lation. In the manual translation, the translator, who
is aware of the speaker’s gender is able to reflect that
through morphological and lexical cues, that exist in
French much more than in English. The MT’s abil-
ity to project these features properly was more limited.
Note that the results between English and French are
not directly comparable, since any text classification on
different languages may yield different results.
One interesting observation is the low performance
relative to the baseline and that of PAN15. Though we
do not discuss this in detail here, we suspect this may
be an effect of genre muting (Argamon et al., 2003;
Herring and Paolillo, 2006).
Next, we explore another setting: The English
corpus is modified to exclude the speakers of the
</bodyText>
<page confidence="0.982638">
1105
</page>
<table confidence="0.850338272727273">
Test Gender (%) Extraverted Stable Agreeable Conscientious Open
en 80.5 0.029 0.050 0.030 0.021 0.021
es 82.8 0.023 0.035 0.024 0.024 0.025
es 75.1 0.031 0.042 0.024 0.021 0.020
es--+en 62.6 0.032 0.048 0.021 0.027 0.030
it 80.0 0.009 0.028 0.020 0.010 0.019
it 59.1 0.013 0.028 0.016 0.014 0.016
it--+en 61.7 0.031 0.063 0.020 0.024 0.025
nl 67.6 0.008 0.014 0.014 0.007 0.010
nl 74.0 0.011 0.032 0.020 0.015 0.012
nl--+en 53.2 0.028 0.076 0.018 0.017 0.023
Training
en
es
es + en--+es
en
it
it + en--+it
en
nl
nl + en--+nl
en
</table>
<tableCaption confidence="0.9941085">
Table 2: Cross-validation results on PAN15 for the settings as per Section 5.1. Gender is measured in accuracy; the
remaining traits as mean squared error. Bold highlights the best result. English results are included for comparison.
</tableCaption>
<table confidence="0.98170525">
Corpus English →French
Native 63.1
Manual 66.6
MT 62.7
</table>
<tableCaption confidence="0.9883045">
Table 3: Gender CV accuracy (%) on the English TED
dataset, when translated manually and automatically.
</tableCaption>
<table confidence="0.904632">
Corpus Accuracy (%)
TED61en 58.3
TED61fr−en (Manual) 60.0
TED61fr−en (MT) 52.1
</table>
<tableCaption confidence="0.994488">
Table 4: Results when classifying gender on native,
</tableCaption>
<bodyText confidence="0.993373923076923">
manually translated and machine translated English
texts, from 61 TEDx and TED talks.
TED61en dataset (leaving n = 1295 speakers), and
this data is used to train a classification model. We
then test our three smaller English datasets on this
model: TED61en, TED61fr−en manual translated
and TED61fr−en machine translated. The results in
this case are more comparable since we use the same
model for all datasets and since their sizes are simi-
lar. The classification results are presented in Table 4.
Again, signal is lost in automatic translation in com-
parison to manual translation. Interestingly, the man-
ual translation scores higher than the native English, as
if the translators are adding more gender indications to
the text. Further analysis is required to clarify whether
this is indeed a consequence of the manual translation
or an artifact of the setting.
Author-aware translation may be viewed as a
human-centric domain adaptation task: we can con-
sider the two genders as two different domains, and
apply domain adaptation techniques to train a better-
suited model for each one. To assess this approach, we
conducted a set of experiments with standard domain
adaptation techniques for en-fr, including: separating
the translation models and the language models by gen-
der in various configurations, using only the target gen-
der’s training data from WIT3 (on top of the Europarl
data), and separating tuning sets by gender. We split
the IWSLT test sets by gender, and applied on each part
the respective gender’s model before concatenating the
translations to compute a BLEU (Papineni et al., 2002)
score. Unfortunately, none of these models showed a
significant improvement, if at all, in comparison to our
baseline that used both genders together. This suggests
that alternative methods should be used for our task.
We cannot say, however, that these results are conclu-
sive; specifically, one difficulty in our experiments was
obtaining enough female data, due to the relative small
number of female speakers in WIT3.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99998436">
We are interested in understanding the impact which
the consideration of author traits might have on au-
tomatic translation, in order to preserve projection of
those traits in a target language. However, it is first nec-
essary to understand the inverse: the effect of current
translation approaches on the computational recogni-
tion of these traits. In the initial studies reported here
we have explored two corpora: one of social media
data; one of scripted speeches. Although linguistic sig-
nals of traits are weaker in the latter case, so far it ap-
pears that machine translation is detrimental to the au-
tomatic recognition of these traits. Though we have
tried to account for as many confounding factors in
this work as we could – particularly the availability
of data – naturally there are still some open questions,
and some obvious next steps. We fixed the learning
parameters across languages and traits for comparative
reasons, but would independent optimization provide
better results? What is the impact of the translation
quality on the subsequent classification performance?
We would also like to understand the true relationship
between linguistic features and traits across languages,
along with how native speakers naturally observe these
traits. Overall, however, we are encouraged to pursue
our goal of personalized machine translation.
</bodyText>
<sectionHeader confidence="0.991711" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995595">
We would like to gratefully acknowledge the comments
and feedback we received from the EMNLP reviewers.
</bodyText>
<page confidence="0.994751">
1106
</page>
<sectionHeader confidence="0.948956" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99716547706422">
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of IWPT.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre,
and writing style in formal written texts. Text,
23(3):321–346.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP.
Alexandra Balahur and Marco Turchi. 2012. Mul-
tilingual sentiment analysis using machine transla-
tion? In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, WASSA ’12, pages 52–60, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Fabio Celli, Bruno Lepri, Joan-Isaac Biel, Daniel
Gatica-Perez, Giuseppe Riccardi, and Fabio Pianesi.
2014. The workshop on computational personality
recognition 2014. In Proceedings of the ACM In-
ternational Conference on Multimedia, pages 1245–
1246. ACM.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT.
Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
Proceedings of the eleventh International Workshop
on Spoken Language Translation (IWSLT), Lake
Tahoe, CA, pages 2–17.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics
(ACL 1996), pages 310–318.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of Twitter users in non-
English contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1136–1145, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Thomas G. Dietterich. 2000. Ensemble methods in
machine learning. In Proceedings of the First Inter-
national Workshop on Multiple Classifier Systems,
pages 1–15. Springer-Verlag.
George F. Foster, Cyril Goutte, and Roland Kuhn.
2010. Discriminative instance weighting for domain
adaptation in statistical machine translation. In Pro-
ceedings of EMNLP.
Alastair J. Gill, Carsten Brockmann, and Jon Oberlan-
der. 2012. Perceptions of alignment and personality
in generated dialogue. In Proceedings of the Seventh
International Natural Language Generation Confer-
ence, INLG ’12, pages 40–48. Association for Com-
putational Linguistics.
G. H. Golub and C. Reinsch. 1970. Singular value
decomposition and least squares solutions. Journal
of Numerical Mathematics, 14:403–420.
Li Gong, Aur´elien Max, and Franc¸ois Yvon. 2012. To-
wards contextual adaptation for any-text translation.
In Proceedings of IWSLT.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 421–
432, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Machine
Translation, pages 187–197, Edinburgh, Scotland,
United Kingdom, July.
Susan C. Herring and John C. Paolillo. 2006. Gender
and genre variation in weblogs. Journal of Sociolin-
guistics, 10(4):439–459, September.
Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In 53rd Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In EMNLP-CoNLL, pages 868–876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL Demo and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of EMNLP-CoNLL.
</reference>
<page confidence="0.898992">
1107
</page>
<reference confidence="0.989437428571429">
Matthias R. Mehl, Samuel D. Gosling, and James W.
Pennebaker. 2006. Personality in its natural habitat:
manifestations and implicit folk theories of person-
ality in daily life. Journal of personality and social
psychology, 90(5):862–877, May.
Shachar Mirkin and Laurant Besacier. 2014. Data se-
lection for compact adapted SMT models. In Pro-
ceedings of the eleventh biennial conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2014), Vancouver, Canada, Oct.
Shachar Mirkin and Jean-Luc Meunier. 2015. Person-
alized machine translation: Predicting translational
preferences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Lisbon, Portugal. Association for Com-
putational Linguistics.
Matthew L Newman, Carla J Groom, Lori D Handel-
man, and James W Pennebaker. 2008. Gender dif-
ferences in language use: An analysis of 14,000 text
samples. Discourse Processes, 45(3):211–236.
Scott Nowson and Alastair J. Gill. 2014. Look! Who’s
Talking? Projection of Extraversion Across Differ-
ent Social Contexts. In Proceedings of WCPR14,
Workshop on Computational Personality Recogni-
tion at ACMM (22nd ACM International Conference
on Multimedia).
Scott Nowson, Julien Perez, Caroline Brun, Shachar
Mirkin, and Claude Roux. 2015. XRCE Personal
Language Analytics Engine for Multilingual Author
Profiling. In Working Notes Papers of the CLEF
2015 Evaluation Labs, CEUR Workshop Proceed-
ings. CLEF and CEUR-WS.org, September.
Scott Nowson. 2006. The Language of Weblogs: A
study of genre and individual differences. Ph.D. the-
sis, University of Edinburgh.
Jon Oberlander and Scott Nowson. 2006. Whose
thumb is it anyway? Classifying author personality
from weblog text. In Proceedings of COLING/ACL-
06: 44th Annual Meeting of the Association for
Computational Linguistics and 21st International
Conference on Computational Linguistics, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1 (ACL 2003), ACL ’03,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, Philadelphia, Pennsylvania, USA.
James W Pennebaker, Kate G Niederhoffer, and
Matthias R Mehl. 2003. Psychological aspects of
natural language use: Our words, our selves. Annual
Review of Psychology, 54:547–577, January.
Beatrice Rammstedt and Oliver P. John. 2007. Mea-
suring personality in one minute or less: A 10-item
short version of the big five inventory in english
and german. Journal of Research in Personality,
41(1):203–212, February.
Francisco Rangel, Fabio Celli, Paolo Rosso, Mar-
tin Potthast, Benno Stein, and Walter Daelemans.
2015. Overview of the 3rd Author Profiling Task
at PAN 2015. In Working Notes Papers of the CLEF
2015 Evaluation Labs, CEUR Workshop Proceed-
ings. CLEF and CEUR-WS.org, September.
Robert Schapire. 1990. The strength of weak learn-
ability. Journal of Machine Learning Research, 5.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings Int. Conf.
on Spoken Language Processing (INTERSPEECH
2002), pages 257–286.
Deborah Tannen. 1990. You Just Don’t Understand:
Women and Men in Conversation. Harper Collins,
New York.
Marko Tkalˇciˇc, Berardina De Carolis, Marco de Gem-
mis, Ante Odi´c, and Andrej Koˇsir. 2014. Preface:
Empire 2014. In Proceedings of the 2nd Workshop
Emotions and Personality in Personalized Services
(EMPIRE 2014). CEUR-WS.org, July.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment analy-
sis in social media. In EMNLP, pages 1815–1827.
ACL.
</reference>
<page confidence="0.99591">
1108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595753">
<title confidence="0.998013">Motivating Personality-aware Machine Translation</title>
<affiliation confidence="0.814853">IBM Research - Mount Carmel,</affiliation>
<address confidence="0.864625">31905,</address>
<email confidence="0.999722">shacharm@il.ibm.com</email>
<author confidence="0.999194">Scott Nowson</author>
<author confidence="0.999194">Caroline Brun</author>
<author confidence="0.999194">Julien</author>
<affiliation confidence="0.997751">Xerox Research Centre</affiliation>
<address confidence="0.9626645">6 chemin de 38240 Meylan,</address>
<email confidence="0.99997">firstname.surname@xrce.xerox.com</email>
<abstract confidence="0.998400476190476">Language use is known to be influenced by personality traits as well as by sociodemographic characteristics such as age or mother tongue. As a result, it is possible to automatically identify these traits of the author from her texts. It has recently been shown that knowledge of such dimensions can improve performance in NLP tasks such as topic and sentiment modeling. We posit that machine translation is another application that should be personalized. In order to motivate this, we explore whether translation preserves demographic and psychometric traits. We show that, largely, both translation of the source training data into the target language, and the target test data into the source language has a detrimental effect on the accuracy of predicting author traits. We argue that this supports the need for personal and personality-aware machine translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salah Ait-Mokhtar</author>
<author>Jean-Pierre Chanod</author>
<author>Claude Roux</author>
</authors>
<title>A multi-input dependency parser.</title>
<date>2001</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<marker>Ait-Mokhtar, Chanod, Roux, 2001</marker>
<rawString>Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude Roux. 2001. A multi-input dependency parser. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Moshe Koppel</author>
<author>Jonathan Fine</author>
<author>Anat Rachel Shimoni</author>
</authors>
<title>Gender, genre, and writing style in formal written texts.</title>
<date>2003</date>
<journal>Text,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="20982" citStr="Argamon et al., 2003" startWordPosition="3362" endWordPosition="3365"> translation, the translator, who is aware of the speaker’s gender is able to reflect that through morphological and lexical cues, that exist in French much more than in English. The MT’s ability to project these features properly was more limited. Note that the results between English and French are not directly comparable, since any text classification on different languages may yield different results. One interesting observation is the low performance relative to the baseline and that of PAN15. Though we do not discuss this in detail here, we suspect this may be an effect of genre muting (Argamon et al., 2003; Herring and Paolillo, 2006). Next, we explore another setting: The English corpus is modified to exclude the speakers of the 1105 Test Gender (%) Extraverted Stable Agreeable Conscientious Open en 80.5 0.029 0.050 0.030 0.021 0.021 es 82.8 0.023 0.035 0.024 0.024 0.025 es 75.1 0.031 0.042 0.024 0.021 0.020 es--+en 62.6 0.032 0.048 0.021 0.027 0.030 it 80.0 0.009 0.028 0.020 0.010 0.019 it 59.1 0.013 0.028 0.016 0.014 0.016 it--+en 61.7 0.031 0.063 0.020 0.024 0.025 nl 67.6 0.008 0.014 0.014 0.007 0.010 nl 74.0 0.011 0.032 0.020 0.015 0.012 nl--+en 53.2 0.028 0.076 0.018 0.017 0.023 Training </context>
</contexts>
<marker>Argamon, Koppel, Fine, Shimoni, 2003</marker>
<rawString>Shlomo Argamon, Moshe Koppel, Jonathan Fine, and Anat Rachel Shimoni. 2003. Gender, genre, and writing style in formal written texts. Text, 23(3):321–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6971" citStr="Axelrod et al., 2011" startWordPosition="1083" endWordPosition="1086">t) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-ba</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Marco Turchi</author>
</authors>
<title>Multilingual sentiment analysis using machine translation?</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>52--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2916" citStr="Balahur and Turchi, 2012" startWordPosition="448" endWordPosition="451">is work was mostly done while the first author was at Xerox Research Centre Europe. labelling is time consuming, requiring the completion of psychometric questionnaires which may be considered invasive by many. An alternative is the use of machine translation (MT) to bootstrap corpora in resource poor languages, and to translate the user’s content into a single language before modeling. Translated text, either manually or automatically generated, is known to have different characteristics than native text. Yet, MT was shown to be of use within traditional NLP tasks such as sentiment analysis (Balahur and Turchi, 2012). We explore the utility of MT for classification of demographic and personality traits. MT models, even domain-specific, are user-generic. Thus, the linguistic signals of user traits which are conveyed in the original language may not be preserved over translation. In other words, the attributes on which we wish to rely for modelling may be lost. This concern is perhaps most observable with gender, a trait of the speaker that is encoded in the morphology of many languages, though not in English. Gendered translation was the topic of research for many years. However, the gender of the author i</context>
</contexts>
<marker>Balahur, Turchi, 2012</marker>
<rawString>Alexandra Balahur and Marco Turchi. 2012. Multilingual sentiment analysis using machine translation? In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 52–60, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Celli</author>
<author>Bruno Lepri</author>
<author>Joan-Isaac Biel</author>
<author>Daniel Gatica-Perez</author>
<author>Giuseppe Riccardi</author>
<author>Fabio Pianesi</author>
</authors>
<title>The workshop on computational personality recognition</title>
<date>2014</date>
<booktitle>In Proceedings of the ACM International Conference on Multimedia,</booktitle>
<pages>1245--1246</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1332" citStr="Celli et al., 2014" startWordPosition="195" endWordPosition="198">pplication that should be personalized. In order to motivate this, we explore whether translation preserves demographic and psychometric traits. We show that, largely, both translation of the source training data into the target language, and the target test data into the source language has a detrimental effect on the accuracy of predicting author traits. We argue that this supports the need for personal and personality-aware machine translation models. 1 Introduction Computational personality recognition is garnering increasing interest with a number of recent workshops exploring the topic (Celli et al., 2014; Tkalˇciˇc et al., 2014). The addition of personality as target traits in the PAN Author Profiling challenge in 2015 (Rangel et al., 2015) is further evidence. Such user modeling – when performed on text – is built on a long-standing understanding that language use is influenced by sociodemographic characteristics such as age, gender, education level or mother tongue and personality traits like agreeableness or openness (Tannen, 1990; Pennebaker et al., 2003). In this work we explore multilingual user modelling. The motivation is not only to enable modeling in multiple languages, but also to </context>
</contexts>
<marker>Celli, Lepri, Biel, Gatica-Perez, Riccardi, Pianesi, 2014</marker>
<rawString>Fabio Celli, Bruno Lepri, Joan-Isaac Biel, Daniel Gatica-Perez, Giuseppe Riccardi, and Fabio Pianesi. 2014. The workshop on computational personality recognition 2014. In Proceedings of the ACM International Conference on Multimedia, pages 1245– 1246. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="12609" citStr="Cettolo et al., 2012" startWordPosition="1999" endWordPosition="2002"> data. Finally, ensemble models (Schapire, 1990; Dietterich, 2000) are used to predict trait values: for gender, we use the majority vote of 10 classifiers; for each personality trait, we use the mean of 10 regression estimators. 3.5 Machine translation models We created standard machine translation models between English and each one of Spanish, Italian, French and Dutch. The details are described below. Parallel corpora We wished to use the same setting for all language pairs. To that end, we chose parallel corpora that are available for all of them, namely Europarl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012), from the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). WIT3, consisting of spoken-language transcripts, represents an in-domain corpus for the TED dataset and a “near-domain” for PAN. The data consisted of approximately 2 million parallel sentences for each language pair, with 50 million tokens for each language. The Europarl corpus comprised more than 90% of that data. The two corpora were concatenated to create the training data for the MT models. Translation System Moses (Koehn et al., 2007), an open-source phrase-based MT system,3 was used to train translation models and transla</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Niehues</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<title>Report on the 11th iwslt evaluation campaign, iwslt</title>
<date>2014</date>
<booktitle>In Proceedings of the eleventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>2--17</pages>
<location>Lake Tahoe, CA,</location>
<marker>Cettolo, Niehues, St¨uker, Bentivogli, Federico, 2014</marker>
<rawString>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the eleventh International Workshop on Spoken Language Translation (IWSLT), Lake Tahoe, CA, pages 2–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>310--318</pages>
<contexts>
<context position="13751" citStr="Chen and Goodman, 1996" startWordPosition="2178" endWordPosition="2181">-source phrase-based MT system,3 was used to train translation models and translate the data. 2Version 7, www.statmt.org/europarl 3We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5- gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data P</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL 1996), pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector space model for adaptation in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7152" citStr="Chen et al., 2013" startWordPosition="1116" endWordPosition="1119">on to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither </context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013. Vector space model for adaptation in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Ciot</author>
<author>Morgan Sonderegger</author>
<author>Derek Ruths</author>
</authors>
<title>Gender inference of Twitter users in nonEnglish contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1136--1145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="17987" citStr="Ciot et al., 2013" startWordPosition="2886" endWordPosition="2889">d 29 as female. TED61en In order to account for any potential effect of length, we created a subset of the en-fr corpus, that is of the same size of the fr-en dataset. We matched files from the French side of the en-fr corpus to each of those in the fr-en for gender and length (in tokens). The French en-fr files were truncated after the nearest line break to the desired size; the corresponding English en-fr files were truncated at the same point. 5 Experiments It has been shown that standard approaches to gender classification on English texts can be sub-optimal for non-English language data (Ciot et al., 2013). However, state-of-the-art classification results are not our focus; rather, our intention is to understand the impact 5Annotation data is available at cm.xrce.xerox.com 6www.ted.com/watch/tedx-talks 7developers.google.com/youtube 8Accessed on 23/2/15. of translation on classification of socio-demographic and personality traits. Therefore, we fix our models with a set of parameters selected via cross-validation (CV) on the native language: the occurrence threshold is set to 5 and the SVD dimensionality to 500. 5.1 PAN For each of the three non-English languages of PAN15 we train classificatio</context>
</contexts>
<marker>Ciot, Sonderegger, Ruths, 2013</marker>
<rawString>Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of Twitter users in nonEnglish contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Workshop on Multiple Classifier Systems,</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="12054" citStr="Dietterich, 2000" startWordPosition="1907" endWordPosition="1908">models we first prune features with a frequency threshold. Next, the remaining set of features is compressed using truncated singular value decomposition (SVD). SVD (Golub and Reinsch, 1970) is a widely used technique in sparse dataset situations. This method copes with noise present in the data by extracting the principal dimensions describing the data and projecting the data to a latent space. In the truncated version, a low-rank approximation, all but the top-k dimensions are removed. The result is a dense, low-dimension representation of the data. Finally, ensemble models (Schapire, 1990; Dietterich, 2000) are used to predict trait values: for gender, we use the majority vote of 10 classifiers; for each personality trait, we use the mean of 10 regression estimators. 3.5 Machine translation models We created standard machine translation models between English and each one of Spanish, Italian, French and Dutch. The details are described below. Parallel corpora We wished to use the same setting for all language pairs. To that end, we chose parallel corpora that are available for all of them, namely Europarl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012), from the IWSLT 2014 evaluation campaign (Ce</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble methods in machine learning. In Proceedings of the First International Workshop on Multiple Classifier Systems, pages 1–15. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George F Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6949" citStr="Foster et al., 2010" startWordPosition="1078" endWordPosition="1082">(e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflec</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George F. Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alastair J Gill</author>
<author>Carsten Brockmann</author>
<author>Jon Oberlander</author>
</authors>
<title>Perceptions of alignment and personality in generated dialogue.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh International Natural Language Generation Conference, INLG ’12,</booktitle>
<pages>40--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5619" citStr="Gill et al., 2012" startWordPosition="865" endWordPosition="868">uages. Hovy (2015) extends this work to other languages and NLP tasks. Using demographically-informed word embeddings, they show improvements in sentiment analysis, topic classification and trait detection. None of these works, however, addressed cross-lingual issues. Yet, personality projection goes beyond automatic detection of traits – there is also human perception to be considered. The casual reader may not be aware of personality related linguistic cues. Yet, studies have shown that traits can be reliably detected following cold readings of texts from unknown authors (Mehl et al., 2006; Gill et al., 2012) without such explicit knowledge. Although personality projection in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what happens with more su</context>
</contexts>
<marker>Gill, Brockmann, Oberlander, 2012</marker>
<rawString>Alastair J. Gill, Carsten Brockmann, and Jon Oberlander. 2012. Perceptions of alignment and personality in generated dialogue. In Proceedings of the Seventh International Natural Language Generation Conference, INLG ’12, pages 40–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Golub</author>
<author>C Reinsch</author>
</authors>
<title>Singular value decomposition and least squares solutions.</title>
<date>1970</date>
<journal>Journal of Numerical Mathematics,</journal>
<pages>14--403</pages>
<contexts>
<context position="11627" citStr="Golub and Reinsch, 1970" startWordPosition="1838" endWordPosition="1841">reprocessing. The English and French grammars also include a polarity lexicon to recognize sentiment bearing words or expressions. The features we extract include: 1-, 2-, 3-grams of surface, normalized and lemmatized forms; part-of-speech tagged forms, and n-grams of POS; named entities (places, persons, organization, dates, time expressions), emoticons, hashtags, mentions and URLs. 3.4 Learning framework To train classification models we first prune features with a frequency threshold. Next, the remaining set of features is compressed using truncated singular value decomposition (SVD). SVD (Golub and Reinsch, 1970) is a widely used technique in sparse dataset situations. This method copes with noise present in the data by extracting the principal dimensions describing the data and projecting the data to a latent space. In the truncated version, a low-rank approximation, all but the top-k dimensions are removed. The result is a dense, low-dimension representation of the data. Finally, ensemble models (Schapire, 1990; Dietterich, 2000) are used to predict trait values: for gender, we use the majority vote of 10 classifiers; for each personality trait, we use the mean of 10 regression estimators. 3.5 Machi</context>
</contexts>
<marker>Golub, Reinsch, 1970</marker>
<rawString>G. H. Golub and C. Reinsch. 1970. Singular value decomposition and least squares solutions. Journal of Numerical Mathematics, 14:403–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Gong</author>
<author>Aur´elien Max</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Towards contextual adaptation for any-text translation.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="6990" citStr="Gong et al., 2012" startWordPosition="1087" endWordPosition="1090"> Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and </context>
</contexts>
<marker>Gong, Max, Yvon, 2012</marker>
<rawString>Li Gong, Aur´elien Max, and Franc¸ois Yvon. 2012. Towards contextual adaptation for any-text translation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>421--432</pages>
<location>Jeju Island,</location>
<contexts>
<context position="10994" citStr="Han et al. (2012)" startWordPosition="1748" endWordPosition="1751">(see Section 4.2). The full details of our approach to text processing, translation and classification can be found in our technical paper at the PAN workshop (Nowson et al., 2015); in the interests of space a compressed version is presented here. 1103 3.3 Preprocessing and feature extraction We use the multilingual parser described by AitMokhtar et al. (2001) to preprocess the texts and extract a wide range of features. The parser has been customized to handle social media data, e.g. by detecting hashtags, mentions, and emoticons. For English, we have integrated a normalization dictionary by Han et al. (2012) in the preprocessing. The English and French grammars also include a polarity lexicon to recognize sentiment bearing words or expressions. The features we extract include: 1-, 2-, 3-grams of surface, normalized and lemmatized forms; part-of-speech tagged forms, and n-grams of POS; named entities (places, persons, organization, dates, time expressions), emoticons, hashtags, mentions and URLs. 3.4 Learning framework To train classification models we first prune features with a frequency threshold. Next, the remaining set of features is compressed using truncated singular value decomposition (SV</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 421– 432, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="13855" citStr="Heafield, 2011" startWordPosition="2197" endWordPosition="2198">atmt.org/europarl 3We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5- gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twi</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan C Herring</author>
<author>John C Paolillo</author>
</authors>
<title>Gender and genre variation in weblogs.</title>
<date>2006</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="21011" citStr="Herring and Paolillo, 2006" startWordPosition="3366" endWordPosition="3369">slator, who is aware of the speaker’s gender is able to reflect that through morphological and lexical cues, that exist in French much more than in English. The MT’s ability to project these features properly was more limited. Note that the results between English and French are not directly comparable, since any text classification on different languages may yield different results. One interesting observation is the low performance relative to the baseline and that of PAN15. Though we do not discuss this in detail here, we suspect this may be an effect of genre muting (Argamon et al., 2003; Herring and Paolillo, 2006). Next, we explore another setting: The English corpus is modified to exclude the speakers of the 1105 Test Gender (%) Extraverted Stable Agreeable Conscientious Open en 80.5 0.029 0.050 0.030 0.021 0.021 es 82.8 0.023 0.035 0.024 0.024 0.025 es 75.1 0.031 0.042 0.024 0.021 0.020 es--+en 62.6 0.032 0.048 0.021 0.027 0.030 it 80.0 0.009 0.028 0.020 0.010 0.019 it 59.1 0.013 0.028 0.016 0.014 0.016 it--+en 61.7 0.031 0.063 0.020 0.024 0.025 nl 67.6 0.008 0.014 0.014 0.007 0.010 nl 74.0 0.011 0.032 0.020 0.015 0.012 nl--+en 53.2 0.028 0.076 0.018 0.017 0.023 Training en es es + en--+es en it it +</context>
</contexts>
<marker>Herring, Paolillo, 2006</marker>
<rawString>Susan C. Herring and John C. Paolillo. 2006. Gender and genre variation in weblogs. Journal of Sociolinguistics, 10(4):439–459, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
</authors>
<title>Demographic factors improve classification performance.</title>
<date>2015</date>
<booktitle>In 53rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4041" citStr="Hovy, 2015" startWordPosition="633" endWordPosition="634">nslation was the topic of research for many years. However, the gender of the author is largely ignored by MT systems, and specifically statistical ones, that would often arbitrarily (or rather statistically-based) translate into one gender form or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of re</context>
</contexts>
<marker>Hovy, 2015</marker>
<rawString>Dirk Hovy. 2015. Demographic factors improve classification performance. In 53rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>868--876</pages>
<contexts>
<context position="7629" citStr="Koehn and Hoang, 2007" startWordPosition="1190" endWordPosition="1193">cier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither in the training nor in the test data, and are therefore unable to adapt their translation beyond the local inflectional level; in particular when no such evidence exists, as in English. To a much greater extent, this is the case with other demographics, such as age, and with personality traits. 3 Methodology 3.1 Hypothesis The hypothesis of our broader vision is that personalized MT or author-aware translation is an important necessity. We believe the human understanding o</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In EMNLP-CoNLL, pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7497" citStr="Koehn et al., 2003" startWordPosition="1171" endWordPosition="1174">dapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither in the training nor in the test data, and are therefore unable to adapt their translation beyond the local inflectional level; in particular when no such evidence exists, as in English. To a much greater extent, this is the case with other demographics, such as age, and with personality traits. 3 Methodology 3.1 Hypothesis The hypothesis of ou</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="13119" citStr="Koehn et al., 2007" startWordPosition="2082" endWordPosition="2085">el corpora that are available for all of them, namely Europarl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012), from the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). WIT3, consisting of spoken-language transcripts, represents an in-domain corpus for the TED dataset and a “near-domain” for PAN. The data consisted of approximately 2 million parallel sentences for each language pair, with 50 million tokens for each language. The Europarl corpus comprised more than 90% of that data. The two corpora were concatenated to create the training data for the MT models. Translation System Moses (Koehn et al., 2007), an open-source phrase-based MT system,3 was used to train translation models and translate the data. 2Version 7, www.statmt.org/europarl 3We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5- gram language models on the target side of the parallel corpus, with modified Kneser-Ney disc</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="12576" citStr="Koehn, 2005" startWordPosition="1995" endWordPosition="1996">on representation of the data. Finally, ensemble models (Schapire, 1990; Dietterich, 2000) are used to predict trait values: for gender, we use the majority vote of 10 classifiers; for each personality trait, we use the mean of 10 regression estimators. 3.5 Machine translation models We created standard machine translation models between English and each one of Spanish, Italian, French and Dutch. The details are described below. Parallel corpora We wished to use the same setting for all language pairs. To that end, we chose parallel corpora that are available for all of them, namely Europarl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012), from the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). WIT3, consisting of spoken-language transcripts, represents an in-domain corpus for the TED dataset and a “near-domain” for PAN. The data consisted of approximately 2 million parallel sentences for each language pair, with 50 million tokens for each language. The Europarl corpus comprised more than 90% of that data. The two corpora were concatenated to create the training data for the MT models. Translation System Moses (Koehn et al., 2007), an open-source phrase-based MT system,3 was used to tra</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6928" citStr="Lu et al., 2007" startWordPosition="1074" endWordPosition="1077"> of contractions (e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translat</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias R Mehl</author>
<author>Samuel D Gosling</author>
<author>James W Pennebaker</author>
</authors>
<title>Personality in its natural habitat: manifestations and implicit folk theories of personality in daily life.</title>
<date>2006</date>
<journal>Journal of personality and social psychology,</journal>
<volume>90</volume>
<issue>5</issue>
<contexts>
<context position="5599" citStr="Mehl et al., 2006" startWordPosition="861" endWordPosition="864">hree different languages. Hovy (2015) extends this work to other languages and NLP tasks. Using demographically-informed word embeddings, they show improvements in sentiment analysis, topic classification and trait detection. None of these works, however, addressed cross-lingual issues. Yet, personality projection goes beyond automatic detection of traits – there is also human perception to be considered. The casual reader may not be aware of personality related linguistic cues. Yet, studies have shown that traits can be reliably detected following cold readings of texts from unknown authors (Mehl et al., 2006; Gill et al., 2012) without such explicit knowledge. Although personality projection in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what </context>
</contexts>
<marker>Mehl, Gosling, Pennebaker, 2006</marker>
<rawString>Matthias R. Mehl, Samuel D. Gosling, and James W. Pennebaker. 2006. Personality in its natural habitat: manifestations and implicit folk theories of personality in daily life. Journal of personality and social psychology, 90(5):862–877, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Laurant Besacier</author>
</authors>
<title>Data selection for compact adapted SMT models.</title>
<date>2014</date>
<booktitle>In Proceedings of the eleventh biennial conference of the Association for Machine Translation in the Americas (AMTA-2014),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="7018" citStr="Mirkin and Besacier, 2014" startWordPosition="1091" endWordPosition="1094">t. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and H</context>
</contexts>
<marker>Mirkin, Besacier, 2014</marker>
<rawString>Shachar Mirkin and Laurant Besacier. 2014. Data selection for compact adapted SMT models. In Proceedings of the eleventh biennial conference of the Association for Machine Translation in the Americas (AMTA-2014), Vancouver, Canada, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Jean-Luc Meunier</author>
</authors>
<title>Personalized machine translation: Predicting translational preferences.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4615" citStr="Mirkin and Meunier, 2015" startWordPosition="715" endWordPosition="718">everal NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of readeraware MT. 1102 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explored by Volkova et al. (2013). They incorporated age and gender features for sentiment analysis, and show improvements in three different languages. Hovy (2015) extends this work to other languages and NLP tasks. Using demographically-informed word embeddings, they show improvements in sentiment analysis, topic classification and trait detection. None of</context>
</contexts>
<marker>Mirkin, Meunier, 2015</marker>
<rawString>Shachar Mirkin and Jean-Luc Meunier. 2015. Personalized machine translation: Predicting translational preferences. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew L Newman</author>
<author>Carla J Groom</author>
<author>Lori D Handelman</author>
<author>James W Pennebaker</author>
</authors>
<title>Gender differences in language use: An analysis of 14,000 text samples.</title>
<date>2008</date>
<booktitle>Discourse Processes,</booktitle>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="6093" citStr="Newman et al., 2008" startWordPosition="942" endWordPosition="945">udies have shown that traits can be reliably detected following cold readings of texts from unknown authors (Mehl et al., 2006; Gill et al., 2012) without such explicit knowledge. Although personality projection in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what happens with more subtle, language-specific indicators of personality? For instance, Nowson (2006) showed that use of contractions (e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistenc</context>
</contexts>
<marker>Newman, Groom, Handelman, Pennebaker, 2008</marker>
<rawString>Matthew L Newman, Carla J Groom, Lori D Handelman, and James W Pennebaker. 2008. Gender differences in language use: An analysis of 14,000 text samples. Discourse Processes, 45(3):211–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Nowson</author>
<author>Alastair J Gill</author>
</authors>
<title>Look! Who’s Talking? Projection of Extraversion Across Different Social Contexts.</title>
<date>2014</date>
<booktitle>In Proceedings of WCPR14, Workshop on Computational Personality Recognition at ACMM (22nd ACM International Conference on Multimedia).</booktitle>
<contexts>
<context position="5857" citStr="Nowson and Gill, 2014" startWordPosition="899" endWordPosition="902">r, addressed cross-lingual issues. Yet, personality projection goes beyond automatic detection of traits – there is also human perception to be considered. The casual reader may not be aware of personality related linguistic cues. Yet, studies have shown that traits can be reliably detected following cold readings of texts from unknown authors (Mehl et al., 2006; Gill et al., 2012) without such explicit knowledge. Although personality projection in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what happens with more subtle, language-specific indicators of personality? For instance, Nowson (2006) showed that use of contractions (e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other la</context>
</contexts>
<marker>Nowson, Gill, 2014</marker>
<rawString>Scott Nowson and Alastair J. Gill. 2014. Look! Who’s Talking? Projection of Extraversion Across Different Social Contexts. In Proceedings of WCPR14, Workshop on Computational Personality Recognition at ACMM (22nd ACM International Conference on Multimedia).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Nowson</author>
<author>Julien Perez</author>
<author>Caroline Brun</author>
<author>Shachar Mirkin</author>
<author>Claude Roux</author>
</authors>
<title>XRCE Personal Language Analytics Engine for Multilingual Author Profiling.</title>
<date>2015</date>
<booktitle>In Working Notes Papers of the CLEF 2015 Evaluation Labs, CEUR Workshop Proceedings. CLEF and CEUR-WS.org,</booktitle>
<contexts>
<context position="10557" citStr="Nowson et al., 2015" startWordPosition="1675" endWordPosition="1678"> explore if any affect was due strictly to the use of MT or to translation (or language change) in general. The PAN corpus is multi-lingual but does not contain parallel data. Such parallel corpora, however, are not typically labelled with the type of author information we wish to investigate. Therefore we required such a corpus to which we could easily add labels. For these we used a selection of TED talks which we labelled for gender (see Section 4.2). The full details of our approach to text processing, translation and classification can be found in our technical paper at the PAN workshop (Nowson et al., 2015); in the interests of space a compressed version is presented here. 1103 3.3 Preprocessing and feature extraction We use the multilingual parser described by AitMokhtar et al. (2001) to preprocess the texts and extract a wide range of features. The parser has been customized to handle social media data, e.g. by detecting hashtags, mentions, and emoticons. For English, we have integrated a normalization dictionary by Han et al. (2012) in the preprocessing. The English and French grammars also include a polarity lexicon to recognize sentiment bearing words or expressions. The features we extract</context>
</contexts>
<marker>Nowson, Perez, Brun, Mirkin, Roux, 2015</marker>
<rawString>Scott Nowson, Julien Perez, Caroline Brun, Shachar Mirkin, and Claude Roux. 2015. XRCE Personal Language Analytics Engine for Multilingual Author Profiling. In Working Notes Papers of the CLEF 2015 Evaluation Labs, CEUR Workshop Proceedings. CLEF and CEUR-WS.org, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Nowson</author>
</authors>
<title>The Language of Weblogs: A study of genre and individual differences.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4301" citStr="Nowson (2006)" startWordPosition="673" endWordPosition="674">Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of readeraware MT. 1102 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explored by Volkova et al. (2013). They incorp</context>
<context position="6297" citStr="Nowson (2006)" startWordPosition="973" endWordPosition="974"> in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what happens with more subtle, language-specific indicators of personality? For instance, Nowson (2006) showed that use of contractions (e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the doma</context>
</contexts>
<marker>Nowson, 2006</marker>
<rawString>Scott Nowson. 2006. The Language of Weblogs: A study of genre and individual differences. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
<author>Scott Nowson</author>
</authors>
<title>Whose thumb is it anyway? Classifying author personality from weblog text.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL06: 44th Annual Meeting of the Association for Computational Linguistics and 21st International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="4301" citStr="Oberlander and Nowson (2006)" startWordPosition="671" endWordPosition="674">rm or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of readeraware MT. 1102 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explored by Volkova et al. (2013). They incorp</context>
</contexts>
<marker>Oberlander, Nowson, 2006</marker>
<rawString>Jon Oberlander and Scott Nowson. 2006. Whose thumb is it anyway? Classifying author personality from weblog text. In Proceedings of COLING/ACL06: 44th Annual Meeting of the Association for Computational Linguistics and 21st International Conference on Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<journal>ACL</journal>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume</booktitle>
<volume>1</volume>
<pages>160--167</pages>
<contexts>
<context position="13918" citStr="Och, 2003" startWordPosition="2207" endWordPosition="2208">om www.statmt.org/moses. Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5- gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twitter and TED talks, as described in this section. 4.1 PAN The f</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1 (ACL 2003), ACL ’03, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="23771" citStr="Papineni et al., 2002" startWordPosition="3816" endWordPosition="3819">wo different domains, and apply domain adaptation techniques to train a bettersuited model for each one. To assess this approach, we conducted a set of experiments with standard domain adaptation techniques for en-fr, including: separating the translation models and the language models by gender in various configurations, using only the target gender’s training data from WIT3 (on top of the Europarl data), and separating tuning sets by gender. We split the IWSLT test sets by gender, and applied on each part the respective gender’s model before concatenating the translations to compute a BLEU (Papineni et al., 2002) score. Unfortunately, none of these models showed a significant improvement, if at all, in comparison to our baseline that used both genders together. This suggests that alternative methods should be used for our task. We cannot say, however, that these results are conclusive; specifically, one difficulty in our experiments was obtaining enough female data, due to the relative small number of female speakers in WIT3. 6 Discussion We are interested in understanding the impact which the consideration of author traits might have on automatic translation, in order to preserve projection of those </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Kate G Niederhoffer</author>
<author>Matthias R Mehl</author>
</authors>
<title>Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology,</title>
<date>2003</date>
<pages>54--547</pages>
<contexts>
<context position="1796" citStr="Pennebaker et al., 2003" startWordPosition="269" endWordPosition="272"> 1 Introduction Computational personality recognition is garnering increasing interest with a number of recent workshops exploring the topic (Celli et al., 2014; Tkalˇciˇc et al., 2014). The addition of personality as target traits in the PAN Author Profiling challenge in 2015 (Rangel et al., 2015) is further evidence. Such user modeling – when performed on text – is built on a long-standing understanding that language use is influenced by sociodemographic characteristics such as age, gender, education level or mother tongue and personality traits like agreeableness or openness (Tannen, 1990; Pennebaker et al., 2003). In this work we explore multilingual user modelling. The motivation is not only to enable modeling in multiple languages, but also to enable modeling multilingual users who may express different sides of their personality in each language. One way to address multilinguality in this context is to create models separately in each language, and then fuse the resulting models. However, labelled data of this nature, particularly in nonEnglish languages, is often not available. Personality ∗ This work was mostly done while the first author was at Xerox Research Centre Europe. labelling is time con</context>
</contexts>
<marker>Pennebaker, Niederhoffer, Mehl, 2003</marker>
<rawString>James W Pennebaker, Kate G Niederhoffer, and Matthias R Mehl. 2003. Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54:547–577, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Rammstedt</author>
<author>Oliver P John</author>
</authors>
<title>Measuring personality in one minute or less: A 10-item short version of the big five inventory in english and german.</title>
<date>2007</date>
<journal>Journal of Research in Personality,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="14940" citStr="Rammstedt and John, 2007" startWordPosition="2368" endWordPosition="2371">ecasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twitter and TED talks, as described in this section. 4.1 PAN The first corpus we used was the data of the PAN 2015 Author Profiling task (Rangel et al., 2015), drawn from Twitter (PAN15). For each user, the data consists of tweets (average n = 100) and gold standard labels: gender (Male or Female), and personality. The labels are provided by the author, with scores on five traits being calculated via self-assessment responses to the short Big 5 test, BFI-10 (Rammstedt and John, 2007)), then normalized between -0.5 and +0.5. Table 1 shows the volume of data per language for the training set. Language Authors Tweets English (en) 152 14166 Spanish (es) 100 9879 Italian (it) 38 3687 Dutch (nl) 34 3350 Table 1: Number of authors and tweets across the four languages of the PAN dataset. 4.2 TED The PAN15 data allows us to assess personality projection in multilingual data. In addition to exploring automatic translation, we wish to compare with manual translation. We turned to TED talks4 for such comparative evaluation. We chose the English-French lan4www.ted.com 1104 guage pair,</context>
</contexts>
<marker>Rammstedt, John, 2007</marker>
<rawString>Beatrice Rammstedt and Oliver P. John. 2007. Measuring personality in one minute or less: A 10-item short version of the big five inventory in english and german. Journal of Research in Personality, 41(1):203–212, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Rangel</author>
<author>Fabio Celli</author>
<author>Paolo Rosso</author>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Walter Daelemans</author>
</authors>
<date>2015</date>
<booktitle>Overview of the 3rd Author Profiling Task at PAN 2015. In Working Notes Papers of the CLEF 2015 Evaluation Labs, CEUR Workshop Proceedings. CLEF and CEUR-WS.org,</booktitle>
<contexts>
<context position="1471" citStr="Rangel et al., 2015" startWordPosition="218" endWordPosition="221">traits. We show that, largely, both translation of the source training data into the target language, and the target test data into the source language has a detrimental effect on the accuracy of predicting author traits. We argue that this supports the need for personal and personality-aware machine translation models. 1 Introduction Computational personality recognition is garnering increasing interest with a number of recent workshops exploring the topic (Celli et al., 2014; Tkalˇciˇc et al., 2014). The addition of personality as target traits in the PAN Author Profiling challenge in 2015 (Rangel et al., 2015) is further evidence. Such user modeling – when performed on text – is built on a long-standing understanding that language use is influenced by sociodemographic characteristics such as age, gender, education level or mother tongue and personality traits like agreeableness or openness (Tannen, 1990; Pennebaker et al., 2003). In this work we explore multilingual user modelling. The motivation is not only to enable modeling in multiple languages, but also to enable modeling multilingual users who may express different sides of their personality in each language. One way to address multilingualit</context>
<context position="9827" citStr="Rangel et al., 2015" startWordPosition="1547" endWordPosition="1550">into the foreign language. Here, the goal is to assess a scenario where translations from a resource-rich language supplement scarce training data in the foreign language, under the assumption that more training data can be beneficial. 3. Translating the foreign test data into English and classifying it using a model trained on the English data. This allows us to explore another practical scenario, where an English model already exists and we wish to use it to classify data from another language for which we do not have a robust model. For this task we use the data from the 2015 PAN workshop (Rangel et al., 2015) which is labelled for author gender and personality traits. For more details see Section 4.1. We also wish to explore if any affect was due strictly to the use of MT or to translation (or language change) in general. The PAN corpus is multi-lingual but does not contain parallel data. Such parallel corpora, however, are not typically labelled with the type of author information we wish to investigate. Therefore we required such a corpus to which we could easily add labels. For these we used a selection of TED talks which we labelled for gender (see Section 4.2). The full details of our approac</context>
<context position="14610" citStr="Rangel et al., 2015" startWordPosition="2314" endWordPosition="2317">onsisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twitter and TED talks, as described in this section. 4.1 PAN The first corpus we used was the data of the PAN 2015 Author Profiling task (Rangel et al., 2015), drawn from Twitter (PAN15). For each user, the data consists of tweets (average n = 100) and gold standard labels: gender (Male or Female), and personality. The labels are provided by the author, with scores on five traits being calculated via self-assessment responses to the short Big 5 test, BFI-10 (Rammstedt and John, 2007)), then normalized between -0.5 and +0.5. Table 1 shows the volume of data per language for the training set. Language Authors Tweets English (en) 152 14166 Spanish (es) 100 9879 Italian (it) 38 3687 Dutch (nl) 34 3350 Table 1: Number of authors and tweets across the fo</context>
</contexts>
<marker>Rangel, Celli, Rosso, Potthast, Stein, Daelemans, 2015</marker>
<rawString>Francisco Rangel, Fabio Celli, Paolo Rosso, Martin Potthast, Benno Stein, and Walter Daelemans. 2015. Overview of the 3rd Author Profiling Task at PAN 2015. In Working Notes Papers of the CLEF 2015 Evaluation Labs, CEUR Workshop Proceedings. CLEF and CEUR-WS.org, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Schapire</author>
</authors>
<title>The strength of weak learnability.</title>
<date>1990</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<contexts>
<context position="12035" citStr="Schapire, 1990" startWordPosition="1905" endWordPosition="1906"> classification models we first prune features with a frequency threshold. Next, the remaining set of features is compressed using truncated singular value decomposition (SVD). SVD (Golub and Reinsch, 1970) is a widely used technique in sparse dataset situations. This method copes with noise present in the data by extracting the principal dimensions describing the data and projecting the data to a latent space. In the truncated version, a low-rank approximation, all but the top-k dimensions are removed. The result is a dense, low-dimension representation of the data. Finally, ensemble models (Schapire, 1990; Dietterich, 2000) are used to predict trait values: for gender, we use the majority vote of 10 classifiers; for each personality trait, we use the mean of 10 regression estimators. 3.5 Machine translation models We created standard machine translation models between English and each one of Spanish, Italian, French and Dutch. The details are described below. Parallel corpora We wished to use the same setting for all language pairs. To that end, we chose parallel corpora that are available for all of them, namely Europarl (Koehn, 2005)2 and WIT3 (Cettolo et al., 2012), from the IWSLT 2014 eval</context>
</contexts>
<marker>Schapire, 1990</marker>
<rawString>Robert Schapire. 1990. The strength of weak learnability. Journal of Machine Learning Research, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings Int. Conf. on Spoken Language Processing (INTERSPEECH</booktitle>
<pages>257--286</pages>
<contexts>
<context position="13599" citStr="Stolcke, 2002" startWordPosition="2156" endWordPosition="2157">t data. The two corpora were concatenated to create the training data for the MT models. Translation System Moses (Koehn et al., 2007), an open-source phrase-based MT system,3 was used to train translation models and translate the data. 2Version 7, www.statmt.org/europarl 3We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5- gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then transla</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings Int. Conf. on Spoken Language Processing (INTERSPEECH 2002), pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Tannen</author>
</authors>
<title>You Just Don’t Understand: Women and Men in Conversation. Harper Collins,</title>
<date>1990</date>
<location>New York.</location>
<contexts>
<context position="1770" citStr="Tannen, 1990" startWordPosition="267" endWordPosition="268">lation models. 1 Introduction Computational personality recognition is garnering increasing interest with a number of recent workshops exploring the topic (Celli et al., 2014; Tkalˇciˇc et al., 2014). The addition of personality as target traits in the PAN Author Profiling challenge in 2015 (Rangel et al., 2015) is further evidence. Such user modeling – when performed on text – is built on a long-standing understanding that language use is influenced by sociodemographic characteristics such as age, gender, education level or mother tongue and personality traits like agreeableness or openness (Tannen, 1990; Pennebaker et al., 2003). In this work we explore multilingual user modelling. The motivation is not only to enable modeling in multiple languages, but also to enable modeling multilingual users who may express different sides of their personality in each language. One way to address multilinguality in this context is to create models separately in each language, and then fuse the resulting models. However, labelled data of this nature, particularly in nonEnglish languages, is often not available. Personality ∗ This work was mostly done while the first author was at Xerox Research Centre Eur</context>
</contexts>
<marker>Tannen, 1990</marker>
<rawString>Deborah Tannen. 1990. You Just Don’t Understand: Women and Men in Conversation. Harper Collins, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marko Tkalˇciˇc</author>
</authors>
<title>Berardina De Carolis, Marco de Gemmis, Ante Odi´c, and Andrej Koˇsir.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop Emotions and Personality in Personalized Services (EMPIRE 2014). CEUR-WS.org,</booktitle>
<location>Preface: Empire</location>
<marker>Tkalˇciˇc, 2014</marker>
<rawString>Marko Tkalˇciˇc, Berardina De Carolis, Marco de Gemmis, Ante Odi´c, and Andrej Koˇsir. 2014. Preface: Empire 2014. In Proceedings of the 2nd Workshop Emotions and Personality in Personalized Services (EMPIRE 2014). CEUR-WS.org, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring demographic language variations to improve multilingual sentiment analysis in social media.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1815--1827</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4028" citStr="Volkova et al., 2013" startWordPosition="629" endWordPosition="632"> English. Gendered translation was the topic of research for many years. However, the gender of the author is largely ignored by MT systems, and specifically statistical ones, that would often arbitrarily (or rather statistically-based) translate into one gender form or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address t</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to improve multilingual sentiment analysis in social media. In EMNLP, pages 1815–1827. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001247">
<title confidence="0.989961">
Summarizing Student Responses to Reflection Prompts
</title>
<author confidence="0.997652">
Wencan Luo
</author>
<affiliation confidence="0.909332">
Computer Science Department
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.998228">
wencan@cs.pitt.edu
</email>
<author confidence="0.992801">
Diane Litman
</author>
<affiliation confidence="0.909322666666667">
Computer Science Department and LRDC
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.998875">
litman@cs.pitt.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99952555">
We propose to automatically summarize
student responses to reflection prompts
and introduce a novel summarization algo-
rithm that differs from traditional methods
in several ways. First, since the linguis-
tic units of student inputs range from sin-
gle words to multiple sentences, our sum-
maries are created from extracted phrases
rather than from sentences. Second, the
phrase summarization algorithm ranks the
phrases by the number of students who
semantically mention a phrase in a sum-
mary. Experimental results show that
the proposed phrase summarization ap-
proach achieves significantly better sum-
marization performance on an engineering
course corpus in terms of ROUGE scores
when compared to other summarization
methods, including MEAD, LexRank and
MMR.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889117647059">
Educational research has demonstrated the effec-
tiveness of reflection prompts (Boud et al., 2013)
to enhance interaction between instructors and stu-
dents (Van den Boom et al., 2004; Menekse et al.,
2011). However, summarizing student responses
to these prompts for large courses (e.g., introduc-
tory STEM, MOOCs) is an onerous task for hu-
mans and poses challenges for existing summa-
rization methods. First, the linguistic units of stu-
dent inputs range from single words to multiple
sentences. Second, we assume that the concepts
(represented as phrases) mentioned by more stu-
dents should get more attention from the instruc-
tor. Based on this assumption, we introduce the
notion of student coverage, defined as the number
of students who semantically mention a particular
phrase. The more student coverage a phrase has,
</bodyText>
<figure confidence="0.456523">
Reflection prompt:
Describe what was confusing or needed more detail.
Student Responses:
S1: Graphs of attraction/repulsive &amp; interatomic separation
</figure>
<footnote confidence="0.271791">
S2: Property related to bond strength
</footnote>
<bodyText confidence="0.8929359">
S3: The activity was difficult to comprehend as the text
fuzzing and difficult to read.
S4: Equations with bond strength and Hooke’s law
S5: I didn’t fully understand the concept of thermal
expansion
S6: The activity ( Part III)
S7: Energy vs. distance between atoms graph and what
it tells us
S8: The graphs of attraction and repulsion were confusing
to me...(rest omitted, 53 student responses in total.)
</bodyText>
<subsectionHeader confidence="0.381724">
Human Reference Summary:
</subsectionHeader>
<listItem confidence="0.9766325">
1) Graphs of attraction/ repulsive &amp; atomic separation [10]
2) Properties and equations with bond strength [7]
3) Coefficient of thermal expansion [6]
4) Activity part III [4]
</listItem>
<tableCaption confidence="0.962313">
Table 1: An example reflection prompt, 53 stu-
</tableCaption>
<bodyText confidence="0.99923925">
dent responses and a gold-standard summary. The
numbers in the square brackets indicate the num-
ber of students who semantically mention each
phrase (i.e., student coverage).
the more important it is. To illustrate the new task,
an example is shown in Table. 1.
In this work, we propose a phrase summariza-
tion method that addresses the above challenges.
First, our summaries are created from extracted
phrases rather than from sentences. Phrases are
easy to read and browse like keywords, and fit bet-
ter on small devices when compared to sentences.
For example, including phrases such as “I didn’t
fully understand” (S5) and “were confusing to me”
(S8) in the summary is a waste of space. Second,
we adopt a metric clustering paradigm with a se-
mantic distance to estimate the student coverage
of each phrase in the summary; a semantic metric
allows similar phrases to be grouped together even
if they are in different textual forms. Experimental
</bodyText>
<page confidence="0.928739">
1955
</page>
<note confidence="0.649522">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999501571428571">
results demonstrate the utility of our approach.
Although not the focus of this paper, we have
also built a mobile application called CourseMIR-
ROR1 that utilizes the proposed summarization al-
gorithm (Luo et al., 2015). Fan et al. (2015) report
a preliminary study about the usage of the applica-
tion.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999963733333333">
While summarization systems that extract sen-
tences are dominant, others have published in
“summarization” at other levels besides the sen-
tence. For example, Ueda et al. (2000) de-
veloped an “at-a-glance” summarization method
with handcrafted rules. Recently, keyphrase ex-
traction (Hasan and Ng, 2014; Liu et al., 2009;
Medelyan et al., 2009; Wu et al., 2005) has re-
ceived considerable attention, aiming to select im-
portant phrases from input documents, which is
similar to phrase summarization. In this paper,
we propose a general framework to adapt sentence
summarization to phrase summarization.
Clustering has been used to score sentences and
has shown good improvement in text summariza-
tion (Yang et al., 2012; Li and Li, 2014; Gung
and Kalita, 2012). In this work, we are using a
metric clustering with semantic similarity to esti-
mate the student coverage at a phrase level. Sim-
ilarly, both diversity-based summarization (Car-
bonell and Goldstein, 1998; Zhang et al., 2005;
Zhu et al., 2007) and our proposed method aim to
estimate and maximize student coverage by mini-
mizing redundancy in the output phrases. Differ-
ently, our method performs the redundancy reduc-
tion at a cluster level (a group of phrases) rather
than penalize redundancy with a greedy iterative
procedure sentence by sentence, and not only the
information content is considered, but also the in-
formation source.
</bodyText>
<sectionHeader confidence="0.996779" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9998305">
Our data consists of student responses collected
from 53 undergraduates enrolled in an introduc-
tion to materials science and engineering class.
The students were asked to complete a survey at
the end of each of 25 lectures during a semester,
consisting of three carefully designed reflection
</bodyText>
<footnote confidence="0.98353875">
1Homepage: http://www.coursemirror.com;
free download link in Google Play Store: https:
//play.google.com/store/apps/details?id=
edu.pitt.cs.mips.coursemirror
</footnote>
<table confidence="0.9943516">
min max mean std
Student-WC 1 91 9.2 7.3
TA-PWC 1 26 7.1 4.9
TA-WC 6 103 29.4 23.2
TA-PC 2 12 4.2 2.2
</table>
<tableCaption confidence="0.971786">
Table 2: Word Count (WC) in student responses
</tableCaption>
<bodyText confidence="0.985973086956522">
(Student-WC), WC per phrase in TA’s summary
(TA-PWC), WC in TA’s summary (TA-WC) and
phrase count in TA’s summary (TA-PC)
prompts: 1) “Describe what you found most in-
teresting in today’s class.” 2) “Describe what was
confusing or needed more detail.” 3) “Describe
what you learned about how you learn.”
In total, more than 900 responses were collected
for each prompt. Currently, gold-standard sum-
maries of 12 out of 25 lectures are created by the
teaching assistant for that course for each reflec-
tion prompt. The summaries include not only the
important phrases, but also the number of students
who mentioned them (i.e., student coverage). 4
lectures are randomly selected as a development
set and the remaining data used as a test set, yield-
ing 12 sets of development data and 24 sets of
testing data, each with a prompt, the students’ re-
sponses and the gold-standard summary. 2
The statistics of the student responses and the
TA’s summary are shown in Table 2. The phrases
summarized by the TA are significantly shorter
than the student responses (7.1 vs. 9.2, p&lt;0.001).
</bodyText>
<sectionHeader confidence="0.993784" genericHeader="method">
4 Proposed Method
</sectionHeader>
<bodyText confidence="0.9999462">
We formulate our task as a standard extrac-
tive summarization problem. Unlike standard
sentence-level extraction where the input and out-
put are sentences, the input of our task ranges from
words or phrases to full sentences. The output is a
list of important phrases and the summary length
(either # of phrases or words) is no more than L.
The proposed algorithm involves three stages:
candidate phrase extraction, phrase clustering,
and phrase ranking.
</bodyText>
<subsectionHeader confidence="0.998002">
4.1 Candidate phrase extraction
</subsectionHeader>
<bodyText confidence="0.999861666666667">
We extract noun phrases (NPs) from the input us-
ing a syntax parser from the Senna toolkit (Col-
lobert, 2011), preserving the most important con-
</bodyText>
<footnote confidence="0.994976666666667">
2This data is publicly available at the CourseMIR-
ROR website: http://www.coursemirror.com/
download/dataset.
</footnote>
<page confidence="0.993411">
1956
</page>
<bodyText confidence="0.999898272727273">
tent from the original responses without losing too
much context information compared to keywords.
For example, “the concept of thermal expansion”
(S5) is extracted as a candidate phrase. Only NPs
are considered because all reflection prompts used
in the task are asking about “what”, and knowl-
edge concepts are usually represented as NPs.3
Due to the noisy data, malformed phrases are
excluded, including single stop words (e.g. “it”,
“I”, “there”, “nothing”) and phrases starting with
a punctuation mark (e.g. “’t”, “+ indexing”).
</bodyText>
<subsectionHeader confidence="0.998556">
4.2 Phrase clustering
</subsectionHeader>
<bodyText confidence="0.999896171428571">
Although phrases are more meaningful and less
ambiguous compared to keywords, they suffer
from the sparsity problem, especially in our data
set when 89.9% of the phrases appeared only once.
The challenge is the fact that students use different
words for the same meaning (e.g., “bicycle parts”
and “bike elements”).
We use a clustering paradigm with a seman-
tic distance metric to address this issue. Among
different clustering algorithms, K-Medoids (Kauf-
man and Rousseeuw, 1987) fits well for our prob-
lem. First, it works with an arbitrary distance
matrix between datapoints. It allows to use pair-
wise semantic similarity-based distance between
phrases, yielding metric clustering. Second, it is
robust to noise and outliers because it minimizes a
sum of pairwise dissimilarities instead of squared
Euclidean distances. It shows better performance
than an LDA-based approach to group students’
short answers for the purpose of semi-automated
grading (Basu et al., 2013). Since K-Medoids
picks a random set of seeds to initialize as the clus-
ter centers (called medoids), the clustering algo-
rithm runs 100 times and the cluster with the min-
imal within-cluster sum of the distances is retained
to reduce random effects.
Distance metric. The semantic similarity is
implemented using SEMILAR (Rus et al., 2013),
using the latent semantic analysis trained on the
Touchstone Applied Science Associates corpus
(S¸tef˘anescu et al., 2014). The distance matrix D
is constructed from the similarity matrix S by ap-
plying the following transformation: D = e−S,
which is similar to the common heat kernel but
without normalization4.
</bodyText>
<footnote confidence="0.99116475">
3In our data, no advantage is observed by including other
constituents like verb and prepositional phrases.
4This is not normalized to the range between 0 and 1 since
we only care about the relative distance.
</footnote>
<bodyText confidence="0.934252142857143">
Number of clusters. For setting the number of
clusters without tuning, we adopted a method from
√
Wan and Yang (2008), by letting K = V . where
K is the number of clusters and V is the number
of candidate phrases instead of the number of sen-
tences.
</bodyText>
<subsectionHeader confidence="0.998733">
4.3 Phrase ranking
</subsectionHeader>
<bodyText confidence="0.999990909090909">
In order to estimate the student coverage, phrases
are clustered with the algorithm introduced above.
We assume the phrases in a cluster are semanti-
cally similar to each other and any phrase in a clus-
ter can represent it as a whole. Therefore the cov-
erage of a phrase is assumed to be the same as the
coverage of a cluster, which is a union of the stu-
dents covered by each phrase in the cluster.
To select the most representative phrase in a
cluster, LexRank (Erkan and Radev, 2004), a
graph-based algorithm for computing relative im-
portance of textual units (working for both sen-
tences and phrases), is used to score the extracted
candidate phrases. The top ranked phrase in the
cluster is added to the output summary. This pro-
cess starts from the cluster that has the most es-
timated student coverage and repeats for the next
cluster until the length limit is reached.
Note that when the student coverage is the same
between two clusters, the score of the top-ranked
phrases in the clusters according to LexRank is
used to break the tie: the higher, the better.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999628894736842">
We use the ROUGE evaluation metric (Lin, 2004)
and report R-1 (unigrams), R-2 (bigrams), and R-
SU4 (bigrams with skip distance up to 4 words),
including the recall (R), precision (P) and F-
Measure (F). These scores measure the over-
lap between human-generated summaries and a
machine-generated summary.
We design and compare a number of other
summarization methods to evaluate the proposed
phrase summarization approach.
Keyphrase extraction. Maui (Medelyan et al.,
2009) is selected as the baseline, which is one of
the state-of-the-art keyphrase extraction methods.
Sentence to phrase summarization. Existing
sentence summarization techniques can be used
for phrase summarization by extracting candidate
phrases and treating them as sentences. Within
this framework, we adapt MEAD (Radev et al.,
2004) and LexRank (Erkan and Radev, 2004) to
</bodyText>
<page confidence="0.988568">
1957
</page>
<table confidence="0.9998759">
R R-1 F R R-2 F R R-SU4 F
P P P
Keyphrase .171 .364 .211 .057 .134 .071 .039 .168 .049
OriMEAD .397 .185 .219 .117 .069 .073 .157 .051 .045
MEAD .341 .269 .265 .122 .102 .099 .126 .094 .072
MEAD+MMR .360 .279 .277 .130 .106 .104 .142 .099 .078
LexRank .325 .355 .307 .107 .110 .102 .120 .145 .098
LexRank+MMR .328 .367 .312 .111 .126 .110 .117 .154 .098
Clustering+Medoid .279 .473 .327 .078 .129 .091 .068 .216 .087
Proposed .319 .448†∗ .340† .122 .176†∗ .134 .112 .205†∗ .109†
</table>
<tableCaption confidence="0.757097">
Table 3: Summarization performance. The last row is our proposed approach. The highest score for each
column is shown in bold. † indicates that the improvement over the MEAD+MMR baseline is statistically
significant. ∗ indicates that the improvement over LexRank+MMR is statistically significant.
</tableCaption>
<bodyText confidence="0.996123322580645">
our task. We also include the original MEAD5 for
comparison (named as OriMEAD).
Diversity-based summarization. We ap-
plied the MMR (Carbonell and Goldstein,
1998), a popular diversity-based summa-
rization method as a post-processing step to
the MEAD (MEAD+MMR) and LexRank
(LexRank+MMR) baselines.6
Clustering+Medoid. To show the performance
using the clustering alone, this baseline selects the
medoid phrase instead of using LexRank to rank
the phrases in a cluster to form the summary.
Results. The performance on the test set is
shown in Table 3 with the length limit L as 4
phrases (the average phrase number in the TA’s
summary). Similar results can be observed when
the length limit is based on the number of words,
but cannot be reported here due to page limit.
First, our proposed method (last row), which
clusters the extracted phrases and uses LexRank to
score them, can outperform all the baselines over
all three ROUGE scores in terms of F-measure.
In addition, the proposed model performs better
than the clustering and LexRank alone. Through
a paired t-test, our model outperforms LexRank
statistically in terms of precision for all three
ROUGE scores and significantly improves Clus-
tering+Medoid on all R-2 scores (except the pre-
cision with 0.06 p-value). We believe that the
semantic similarity based clustering complements
LexRank in two ways: 1) LexRank depends on
</bodyText>
<footnote confidence="0.9994992">
5The default Length parameter in MEAD is changed to 1
from its default value 9 and the position feature is removed,
yielding better performance.
6For each MMR based baseline, the parameter is opti-
mized with a grid search on the development data set.
</footnote>
<bodyText confidence="0.999920409090909">
the cosine similarity of TF-IDF vectors to build
the graph while the clustering takes semantic sim-
ilarity into account. 2) The clustering performed
a global selection to form a summary by group-
ing similar phrases and ranking them by the num-
ber of covered students (similar to what the hu-
man did). Compared to LexRank, our approach
captures the student coverage explicitly. While
modifying LexRank by using semantic similarity
is possible, estimating the student coverage is not
straightforward.
Second, OriMEAD tends to select long sen-
tences, resulting in a high recall but a low preci-
sion. The phrase version (MEAD) improves both
the P and F scores by removing unnecessary parts
in the original sentences.
Lastly, the proposed method outperforms the
MMR based baselines on the precision and F-
measure of all three ROUGE scores. We observed
that the MMR baselines suffer from the issue of di-
verse expressions used the students (e.g., “graphs”
and “charts”).
</bodyText>
<sectionHeader confidence="0.995664" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999749454545455">
In this paper, we presented a novel application to
summarize student feedback to reflection prompts
by a combination of phrase extraction, phrase
clustering and phrase ranking. It makes use of
metric clustering to rank the phrases by their
student coverage, taking the information source
into account. Experimental results demonstrate
the good effectiveness of the model. While
the proposed method improved the performance
against MMR, other summarization methods with-
out an additional MMR component do exist, in-
</bodyText>
<page confidence="0.98917">
1958
</page>
<bodyText confidence="0.997759157894737">
cluding SumBasic (Vanderwende et al., 2007),
KLSUM and TopicSUM (Haghighi and Vander-
wende, 2009). An initial experiment shows they
do not yield better performance with default pa-
rameters. However, we will revisit it since these
methods are meant for full sentences and are not
optimized within the phrase framework.
In the future, we plan to have additional an-
notation to evaluate the relative importance us-
ing the student coverage numbers. We also de-
ployed CourseMIRROR in a statistics class in
Spring 2015 and have created gold-standard sum-
maries, which will allow us to both replicate the
intrinsic evaluation of this paper with a new and
larger dataset as well conduct an extrinsic evalua-
tion beyond ROUGE scores. Finally, we are inter-
ested in applying our summarization approach to
other types of user-generated content from mobile-
applications (e.g., review comments).
</bodyText>
<sectionHeader confidence="0.994349" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999952">
This research is partially supported by an internal
grant from the Learning Research and Develop-
ment Center at the University of Pittsburgh. We
thank Muhsin Menekse for providing the data set.
We thank Jingtao Wang and Xiangmin Fan for de-
veloping the CourseMIRROR application and for
valuable suggestions about the proposed summa-
rization algorithm. We also thank anonymous re-
viewers for insightful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.992989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997291311688312">
Sumit Basu, Chuck Jacobs, and Lucy Vanderwende.
2013. Powergrading: a clustering approach to am-
plify human effort for short answer grading. Trans-
actions of the Association for Computational Lin-
guistics, 1:391–402.
David Boud, Rosemary Keogh, David Walker, et al.
2013. Reflection: Turning experience into learning.
Routledge.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’98, pages 335–336. ACM.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
Dan S¸tef˘anescu, Rajendra Banjade, and Vasile Rus.
2014. Latent semantic analysis models on wikipedia
and tasa. In The 9th Language Resources and Eval-
uation Conference (LREC 2014), pages 26–31.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457–479,
December.
Xiangmin Fan, Wencan Luo, Muhsin Menekse, Diane
Litman, and Jingtao Wang. 2015. CourseMIR-
ROR: Enhancing large classroom instructor-student
interactions via mobile interfaces and natural lan-
guage processing. In Works-In-Progress of ACM
Conference on Human Factors in Computing Sys-
tems. ACM.
James Gung and Jugal Kalita. 2012. Summarization
of historical articles using temporal event clustering.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 631–635.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 362–370, Boulder, Col-
orado, June.
Kazi Saidul Hasan and Vincent Ng. 2014. Auto-
matic keyphrase extraction: A survey of the state of
the art. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1262–1273, June.
Leonard Kaufman and Peter Rousseeuw. 1987. Clus-
tering by means of medoids. Statistical Data Anal-
ysis Based on the Ll-Norm and Related Method,
pages 405–416.
Yanran Li and Sujian Li. 2014. Query-focused multi-
document summarization: Combining a topic model
with graph-based semi-supervised learning. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 1197–1207, August.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’09, pages 257–266.
Wencan Luo, Xiangmin Fan, Muhsin Menekse, Jing-
tao Wang, and Diane Litman. 2015. Enhancing
instructor-student and student-student interactions
with mobile interfaces and summarization. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Demonstrations, pages 16–20, June.
</reference>
<page confidence="0.922773">
1959
</page>
<reference confidence="0.999481590909091">
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3, EMNLP ’09, pages
1318–1327.
Muhsin Menekse, Glenda Stump, Stephen J. Krause,
and Michelene T.H. Chi. 2011. The effectiveness of
students daily reflections on learning in engineering
context. In Proceedings of the American Society for
Engineering Education (ASEE) Annual Conference,
Vancouver, Canada.
Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Inf. Process. Manage.,
40(6):919–938, November.
Vasile Rus, Mihai C Lintean, Rajendra Banjade,
Nobal B Niraula, and Dan Stefanescu. 2013. Semi-
lar: The semantic similarity toolkit. In ACL (Con-
ference System Demonstrations), pages 163–168.
Yoshihiro Ueda, Mamiko Oka, Takahiro Koyama, and
Tadanobu Miyauchi. 2000. Toward the ”at-a-
glance” summary: Phrase-representation summa-
rization method. In Proceedings of the 18th Con-
ference on Computational Linguistics - Volume 2,
COLING ’00, pages 878–884.
Gerard Van den Boom, Fred Paas, Jeroen JG Van Mer-
rienboer, and Tamara Van Gog. 2004. Reflection
prompts and tutor feedback in a web-based learn-
ing environment: effects on students’ self-regulated
learning competence. Computers in Human Behav-
ior, 20(4):551 – 567.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606–1618, November.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’08, pages 299–306.
Yi-fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,
and Xin Chen. 2005. Domain-specific keyphrase
extraction. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ’05, pages 283–284.
Rui Yang, Zhan Bu, and Zhengyou Xia. 2012. Auto-
matic summarization for chinese text using affinity
propagation clustering and latent semantic analysis.
In Proceedings of the 2012 International Conference
on Web Information Systems and Mining, WISM’12,
pages 543–550.
Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo
Fan, Zheng Chen, and Wei-Ying Ma. 2005. Im-
proving web search results using affinity graph. In
Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’05, pages 504–511.
ACM.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity
in ranking using absorbing random walks. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 97–104, April.
</reference>
<page confidence="0.991887">
1960
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342208">
<title confidence="0.999698">Summarizing Student Responses to Reflection Prompts</title>
<author confidence="0.981241">Wencan</author>
<affiliation confidence="0.999782">Computer Science University of</affiliation>
<address confidence="0.948489">Pittsburgh, PA 15260,</address>
<email confidence="0.996574">wencan@cs.pitt.edu</email>
<author confidence="0.64982">Diane</author>
<affiliation confidence="0.999361">Computer Science Department and University of</affiliation>
<address confidence="0.945802">Pittsburgh, PA 15260,</address>
<email confidence="0.998156">litman@cs.pitt.edu</email>
<abstract confidence="0.980799857142857">We propose to automatically summarize student responses to reflection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways. First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sumit Basu</author>
<author>Chuck Jacobs</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Powergrading: a clustering approach to amplify human effort for short answer grading.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--391</pages>
<contexts>
<context position="9527" citStr="Basu et al., 2013" startWordPosition="1488" endWordPosition="1491"> semantic distance metric to address this issue. Among different clustering algorithms, K-Medoids (Kaufman and Rousseeuw, 1987) fits well for our problem. First, it works with an arbitrary distance matrix between datapoints. It allows to use pairwise semantic similarity-based distance between phrases, yielding metric clustering. Second, it is robust to noise and outliers because it minimizes a sum of pairwise dissimilarities instead of squared Euclidean distances. It shows better performance than an LDA-based approach to group students’ short answers for the purpose of semi-automated grading (Basu et al., 2013). Since K-Medoids picks a random set of seeds to initialize as the cluster centers (called medoids), the clustering algorithm runs 100 times and the cluster with the minimal within-cluster sum of the distances is retained to reduce random effects. Distance metric. The semantic similarity is implemented using SEMILAR (Rus et al., 2013), using the latent semantic analysis trained on the Touchstone Applied Science Associates corpus (S¸tef˘anescu et al., 2014). The distance matrix D is constructed from the similarity matrix S by applying the following transformation: D = e−S, which is similar to t</context>
</contexts>
<marker>Basu, Jacobs, Vanderwende, 2013</marker>
<rawString>Sumit Basu, Chuck Jacobs, and Lucy Vanderwende. 2013. Powergrading: a clustering approach to amplify human effort for short answer grading. Transactions of the Association for Computational Linguistics, 1:391–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Boud</author>
<author>Rosemary Keogh</author>
<author>David Walker</author>
</authors>
<title>Reflection: Turning experience into learning.</title>
<date>2013</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="1158" citStr="Boud et al., 2013" startWordPosition="160" endWordPosition="163">ords to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR. 1 Introduction Educational research has demonstrated the effectiveness of reflection prompts (Boud et al., 2013) to enhance interaction between instructors and students (Van den Boom et al., 2004; Menekse et al., 2011). However, summarizing student responses to these prompts for large courses (e.g., introductory STEM, MOOCs) is an onerous task for humans and poses challenges for existing summarization methods. First, the linguistic units of student inputs range from single words to multiple sentences. Second, we assume that the concepts (represented as phrases) mentioned by more students should get more attention from the instructor. Based on this assumption, we introduce the notion of student coverage,</context>
</contexts>
<marker>Boud, Keogh, Walker, 2013</marker>
<rawString>David Boud, Rosemary Keogh, David Walker, et al. 2013. Reflection: Turning experience into learning. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5134" citStr="Carbonell and Goldstein, 1998" startWordPosition="792" endWordPosition="796"> et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information content is considered, but also the information source. 3 Data Our data consists of student responses collected from 53 undergraduates enrolled in an introduction to materials science and engineering class. The students were as</context>
<context position="13513" citStr="Carbonell and Goldstein, 1998" startWordPosition="2156" endWordPosition="2159">R .328 .367 .312 .111 .126 .110 .117 .154 .098 Clustering+Medoid .279 .473 .327 .078 .129 .091 .068 .216 .087 Proposed .319 .448†∗ .340† .122 .176†∗ .134 .112 .205†∗ .109† Table 3: Summarization performance. The last row is our proposed approach. The highest score for each column is shown in bold. † indicates that the improvement over the MEAD+MMR baseline is statistically significant. ∗ indicates that the improvement over LexRank+MMR is statistically significant. our task. We also include the original MEAD5 for comparison (named as OriMEAD). Diversity-based summarization. We applied the MMR (Carbonell and Goldstein, 1998), a popular diversity-based summarization method as a post-processing step to the MEAD (MEAD+MMR) and LexRank (LexRank+MMR) baselines.6 Clustering+Medoid. To show the performance using the clustering alone, this baseline selects the medoid phrase instead of using LexRank to rank the phrases in a cluster to form the summary. Results. The performance on the test set is shown in Table 3 with the length limit L as 4 phrases (the average phrase number in the TA’s summary). Similar results can be observed when the length limit is based on the number of words, but cannot be reported here due to page </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98, pages 335–336. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="7854" citStr="Collobert, 2011" startWordPosition="1237" endWordPosition="1239">001). 4 Proposed Method We formulate our task as a standard extractive summarization problem. Unlike standard sentence-level extraction where the input and output are sentences, the input of our task ranges from words or phrases to full sentences. The output is a list of important phrases and the summary length (either # of phrases or words) is no more than L. The proposed algorithm involves three stages: candidate phrase extraction, phrase clustering, and phrase ranking. 4.1 Candidate phrase extraction We extract noun phrases (NPs) from the input using a syntax parser from the Senna toolkit (Collobert, 2011), preserving the most important con2This data is publicly available at the CourseMIRROR website: http://www.coursemirror.com/ download/dataset. 1956 tent from the original responses without losing too much context information compared to keywords. For example, “the concept of thermal expansion” (S5) is extracted as a candidate phrase. Only NPs are considered because all reflection prompts used in the task are asking about “what”, and knowledge concepts are usually represented as NPs.3 Due to the noisy data, malformed phrases are excluded, including single stop words (e.g. “it”, “I”, “there”, “</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan S¸tef˘anescu</author>
<author>Rajendra Banjade</author>
<author>Vasile Rus</author>
</authors>
<title>Latent semantic analysis models on wikipedia and tasa.</title>
<date>2014</date>
<booktitle>In The 9th Language Resources and Evaluation Conference (LREC 2014),</booktitle>
<pages>26--31</pages>
<marker>S¸tef˘anescu, Banjade, Rus, 2014</marker>
<rawString>Dan S¸tef˘anescu, Rajendra Banjade, and Vasile Rus. 2014. Latent semantic analysis models on wikipedia and tasa. In The 9th Language Resources and Evaluation Conference (LREC 2014), pages 26–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11136" citStr="Erkan and Radev, 2004" startWordPosition="1768" endWordPosition="1771">g K = V . where K is the number of clusters and V is the number of candidate phrases instead of the number of sentences. 4.3 Phrase ranking In order to estimate the student coverage, phrases are clustered with the algorithm introduced above. We assume the phrases in a cluster are semantically similar to each other and any phrase in a cluster can represent it as a whole. Therefore the coverage of a phrase is assumed to be the same as the coverage of a cluster, which is a union of the students covered by each phrase in the cluster. To select the most representative phrase in a cluster, LexRank (Erkan and Radev, 2004), a graph-based algorithm for computing relative importance of textual units (working for both sentences and phrases), is used to score the extracted candidate phrases. The top ranked phrase in the cluster is added to the output summary. This process starts from the cluster that has the most estimated student coverage and repeats for the next cluster until the length limit is reached. Note that when the student coverage is the same between two clusters, the score of the top-ranked phrases in the clusters according to LexRank is used to break the tie: the higher, the better. 5 Experiments We us</context>
<context position="12567" citStr="Erkan and Radev, 2004" startWordPosition="1995" endWordPosition="1998">easure the overlap between human-generated summaries and a machine-generated summary. We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach. Keyphrase extraction. Maui (Medelyan et al., 2009) is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods. Sentence to phrase summarization. Existing sentence summarization techniques can be used for phrase summarization by extracting candidate phrases and treating them as sentences. Within this framework, we adapt MEAD (Radev et al., 2004) and LexRank (Erkan and Radev, 2004) to 1957 R R-1 F R R-2 F R R-SU4 F P P P Keyphrase .171 .364 .211 .057 .134 .071 .039 .168 .049 OriMEAD .397 .185 .219 .117 .069 .073 .157 .051 .045 MEAD .341 .269 .265 .122 .102 .099 .126 .094 .072 MEAD+MMR .360 .279 .277 .130 .106 .104 .142 .099 .078 LexRank .325 .355 .307 .107 .110 .102 .120 .145 .098 LexRank+MMR .328 .367 .312 .111 .126 .110 .117 .154 .098 Clustering+Medoid .279 .473 .327 .078 .129 .091 .068 .216 .087 Proposed .319 .448†∗ .340† .122 .176†∗ .134 .112 .205†∗ .109† Table 3: Summarization performance. The last row is our proposed approach. The highest score for each column is </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangmin Fan</author>
<author>Wencan Luo</author>
<author>Muhsin Menekse</author>
<author>Diane Litman</author>
<author>Jingtao Wang</author>
</authors>
<title>CourseMIRROR: Enhancing large classroom instructor-student interactions via mobile interfaces and natural language processing.</title>
<date>2015</date>
<booktitle>In Works-In-Progress of ACM Conference on Human Factors in Computing Systems.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="4100" citStr="Fan et al. (2015)" startWordPosition="628" endWordPosition="631">timate the student coverage of each phrase in the summary; a semantic metric allows similar phrases to be grouped together even if they are in different textual forms. Experimental 1955 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a</context>
</contexts>
<marker>Fan, Luo, Menekse, Litman, Wang, 2015</marker>
<rawString>Xiangmin Fan, Wencan Luo, Muhsin Menekse, Diane Litman, and Jingtao Wang. 2015. CourseMIRROR: Enhancing large classroom instructor-student interactions via mobile interfaces and natural language processing. In Works-In-Progress of ACM Conference on Human Factors in Computing Systems. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gung</author>
<author>Jugal Kalita</author>
</authors>
<title>Summarization of historical articles using temporal event clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>631--635</pages>
<contexts>
<context position="4932" citStr="Gung and Kalita, 2012" startWordPosition="761" endWordPosition="764">he sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information content is considered, bu</context>
</contexts>
<marker>Gung, Kalita, 2012</marker>
<rawString>James Gung and Jugal Kalita. 2012. Summarization of historical articles using temporal event clustering. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 631–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="16567" citStr="Haghighi and Vanderwende, 2009" startWordPosition="2643" endWordPosition="2647">s”). 6 Conclusion In this paper, we presented a novel application to summarize student feedback to reflection prompts by a combination of phrase extraction, phrase clustering and phrase ranking. It makes use of metric clustering to rank the phrases by their student coverage, taking the information source into account. Experimental results demonstrate the good effectiveness of the model. While the proposed method improved the performance against MMR, other summarization methods without an additional MMR component do exist, in1958 cluding SumBasic (Vanderwende et al., 2007), KLSUM and TopicSUM (Haghighi and Vanderwende, 2009). An initial experiment shows they do not yield better performance with default parameters. However, we will revisit it since these methods are meant for full sentences and are not optimized within the phrase framework. In the future, we plan to have additional annotation to evaluate the relative importance using the student coverage numbers. We also deployed CourseMIRROR in a statistics class in Spring 2015 and have created gold-standard summaries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and larger dataset as well conduct an extrinsic evaluation</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Automatic keyphrase extraction: A survey of the state of the art.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1262--1273</pages>
<contexts>
<context position="4477" citStr="Hasan and Ng, 2014" startWordPosition="686" endWordPosition="689">ics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both div</context>
</contexts>
<marker>Hasan, Ng, 2014</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2014. Automatic keyphrase extraction: A survey of the state of the art. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262–1273, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Kaufman</author>
<author>Peter Rousseeuw</author>
</authors>
<title>Clustering by means of medoids.</title>
<date>1987</date>
<booktitle>Statistical Data Analysis Based on the Ll-Norm and Related Method,</booktitle>
<pages>405--416</pages>
<contexts>
<context position="9036" citStr="Kaufman and Rousseeuw, 1987" startWordPosition="1413" endWordPosition="1417"> single stop words (e.g. “it”, “I”, “there”, “nothing”) and phrases starting with a punctuation mark (e.g. “’t”, “+ indexing”). 4.2 Phrase clustering Although phrases are more meaningful and less ambiguous compared to keywords, they suffer from the sparsity problem, especially in our data set when 89.9% of the phrases appeared only once. The challenge is the fact that students use different words for the same meaning (e.g., “bicycle parts” and “bike elements”). We use a clustering paradigm with a semantic distance metric to address this issue. Among different clustering algorithms, K-Medoids (Kaufman and Rousseeuw, 1987) fits well for our problem. First, it works with an arbitrary distance matrix between datapoints. It allows to use pairwise semantic similarity-based distance between phrases, yielding metric clustering. Second, it is robust to noise and outliers because it minimizes a sum of pairwise dissimilarities instead of squared Euclidean distances. It shows better performance than an LDA-based approach to group students’ short answers for the purpose of semi-automated grading (Basu et al., 2013). Since K-Medoids picks a random set of seeds to initialize as the cluster centers (called medoids), the clus</context>
</contexts>
<marker>Kaufman, Rousseeuw, 1987</marker>
<rawString>Leonard Kaufman and Peter Rousseeuw. 1987. Clustering by means of medoids. Statistical Data Analysis Based on the Ll-Norm and Related Method, pages 405–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanran Li</author>
<author>Sujian Li</author>
</authors>
<title>Query-focused multidocument summarization: Combining a topic model with graph-based semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1197--1207</pages>
<contexts>
<context position="4908" citStr="Li and Li, 2014" startWordPosition="757" endWordPosition="760"> levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information c</context>
</contexts>
<marker>Li, Li, 2014</marker>
<rawString>Yanran Li and Sujian Li. 2014. Query-focused multidocument summarization: Combining a topic model with graph-based semi-supervised learning. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1197–1207, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="11777" citStr="Lin, 2004" startWordPosition="1880" endWordPosition="1881">omputing relative importance of textual units (working for both sentences and phrases), is used to score the extracted candidate phrases. The top ranked phrase in the cluster is added to the output summary. This process starts from the cluster that has the most estimated student coverage and repeats for the next cluster until the length limit is reached. Note that when the student coverage is the same between two clusters, the score of the top-ranked phrases in the clusters according to LexRank is used to break the tie: the higher, the better. 5 Experiments We use the ROUGE evaluation metric (Lin, 2004) and report R-1 (unigrams), R-2 (bigrams), and RSU4 (bigrams with skip distance up to 4 words), including the recall (R), precision (P) and FMeasure (F). These scores measure the overlap between human-generated summaries and a machine-generated summary. We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach. Keyphrase extraction. Maui (Medelyan et al., 2009) is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods. Sentence to phrase summarization. Existing sentence summarization techniques can </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>257--266</pages>
<contexts>
<context position="4495" citStr="Liu et al., 2009" startWordPosition="690" endWordPosition="693">rate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summa</context>
</contexts>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 257–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wencan Luo</author>
<author>Xiangmin Fan</author>
<author>Muhsin Menekse</author>
<author>Jingtao Wang</author>
<author>Diane Litman</author>
</authors>
<title>Enhancing instructor-student and student-student interactions with mobile interfaces and summarization.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,</booktitle>
<pages>16--20</pages>
<contexts>
<context position="4081" citStr="Luo et al., 2015" startWordPosition="624" endWordPosition="627">ntic distance to estimate the student coverage of each phrase in the summary; a semantic metric allows similar phrases to be grouped together even if they are in different textual forms. Experimental 1955 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1955–1960, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this </context>
</contexts>
<marker>Luo, Fan, Menekse, Wang, Litman, 2015</marker>
<rawString>Wencan Luo, Xiangmin Fan, Muhsin Menekse, Jingtao Wang, and Diane Litman. 2015. Enhancing instructor-student and student-student interactions with mobile interfaces and summarization. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 16–20, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, EMNLP ’09,</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="4518" citStr="Medelyan et al., 2009" startWordPosition="694" endWordPosition="697">f our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and</context>
<context position="12199" citStr="Medelyan et al., 2009" startWordPosition="1942" endWordPosition="1945">en two clusters, the score of the top-ranked phrases in the clusters according to LexRank is used to break the tie: the higher, the better. 5 Experiments We use the ROUGE evaluation metric (Lin, 2004) and report R-1 (unigrams), R-2 (bigrams), and RSU4 (bigrams with skip distance up to 4 words), including the recall (R), precision (P) and FMeasure (F). These scores measure the overlap between human-generated summaries and a machine-generated summary. We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach. Keyphrase extraction. Maui (Medelyan et al., 2009) is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods. Sentence to phrase summarization. Existing sentence summarization techniques can be used for phrase summarization by extracting candidate phrases and treating them as sentences. Within this framework, we adapt MEAD (Radev et al., 2004) and LexRank (Erkan and Radev, 2004) to 1957 R R-1 F R R-2 F R R-SU4 F P P P Keyphrase .171 .364 .211 .057 .134 .071 .039 .168 .049 OriMEAD .397 .185 .219 .117 .069 .073 .157 .051 .045 MEAD .341 .269 .265 .122 .102 .099 .126 .094 .072 MEAD+MMR .360 .279 .277 .130 .106</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Olena Medelyan, Eibe Frank, and Ian H. Witten. 2009. Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3, EMNLP ’09, pages 1318–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhsin Menekse</author>
<author>Glenda Stump</author>
<author>Stephen J Krause</author>
<author>Michelene T H Chi</author>
</authors>
<title>The effectiveness of students daily reflections on learning in engineering context.</title>
<date>2011</date>
<booktitle>In Proceedings of the American Society for Engineering Education (ASEE) Annual Conference,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1264" citStr="Menekse et al., 2011" startWordPosition="178" endWordPosition="181">. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR. 1 Introduction Educational research has demonstrated the effectiveness of reflection prompts (Boud et al., 2013) to enhance interaction between instructors and students (Van den Boom et al., 2004; Menekse et al., 2011). However, summarizing student responses to these prompts for large courses (e.g., introductory STEM, MOOCs) is an onerous task for humans and poses challenges for existing summarization methods. First, the linguistic units of student inputs range from single words to multiple sentences. Second, we assume that the concepts (represented as phrases) mentioned by more students should get more attention from the instructor. Based on this assumption, we introduce the notion of student coverage, defined as the number of students who semantically mention a particular phrase. The more student coverage</context>
</contexts>
<marker>Menekse, Stump, Krause, Chi, 2011</marker>
<rawString>Muhsin Menekse, Glenda Stump, Stephen J. Krause, and Michelene T.H. Chi. 2011. The effectiveness of students daily reflections on learning in engineering context. In Proceedings of the American Society for Engineering Education (ASEE) Annual Conference, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Małgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Inf. Process. Manage.,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Inf. Process. Manage., 40(6):919–938, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai C Lintean</author>
<author>Rajendra Banjade</author>
<author>Nobal B Niraula</author>
<author>Dan Stefanescu</author>
</authors>
<title>Semilar: The semantic similarity toolkit.</title>
<date>2013</date>
<booktitle>In ACL (Conference System Demonstrations),</booktitle>
<pages>163--168</pages>
<contexts>
<context position="9863" citStr="Rus et al., 2013" startWordPosition="1543" endWordPosition="1546">, it is robust to noise and outliers because it minimizes a sum of pairwise dissimilarities instead of squared Euclidean distances. It shows better performance than an LDA-based approach to group students’ short answers for the purpose of semi-automated grading (Basu et al., 2013). Since K-Medoids picks a random set of seeds to initialize as the cluster centers (called medoids), the clustering algorithm runs 100 times and the cluster with the minimal within-cluster sum of the distances is retained to reduce random effects. Distance metric. The semantic similarity is implemented using SEMILAR (Rus et al., 2013), using the latent semantic analysis trained on the Touchstone Applied Science Associates corpus (S¸tef˘anescu et al., 2014). The distance matrix D is constructed from the similarity matrix S by applying the following transformation: D = e−S, which is similar to the common heat kernel but without normalization4. 3In our data, no advantage is observed by including other constituents like verb and prepositional phrases. 4This is not normalized to the range between 0 and 1 since we only care about the relative distance. Number of clusters. For setting the number of clusters without tuning, we ado</context>
</contexts>
<marker>Rus, Lintean, Banjade, Niraula, Stefanescu, 2013</marker>
<rawString>Vasile Rus, Mihai C Lintean, Rajendra Banjade, Nobal B Niraula, and Dan Stefanescu. 2013. Semilar: The semantic similarity toolkit. In ACL (Conference System Demonstrations), pages 163–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshihiro Ueda</author>
<author>Mamiko Oka</author>
<author>Takahiro Koyama</author>
<author>Tadanobu Miyauchi</author>
</authors>
<title>Toward the ”at-aglance” summary: Phrase-representation summarization method.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics - Volume 2, COLING ’00,</booktitle>
<pages>878--884</pages>
<contexts>
<context position="4354" citStr="Ueda et al. (2000)" startWordPosition="669" endWordPosition="672">Language Processing, pages 1955–1960, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. results demonstrate the utility of our approach. Although not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are</context>
</contexts>
<marker>Ueda, Oka, Koyama, Miyauchi, 2000</marker>
<rawString>Yoshihiro Ueda, Mamiko Oka, Takahiro Koyama, and Tadanobu Miyauchi. 2000. Toward the ”at-aglance” summary: Phrase-representation summarization method. In Proceedings of the 18th Conference on Computational Linguistics - Volume 2, COLING ’00, pages 878–884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Van den Boom</author>
<author>Fred Paas</author>
<author>Jeroen JG Van Merrienboer</author>
<author>Tamara Van Gog</author>
</authors>
<title>Reflection prompts and tutor feedback in a web-based learning environment: effects on students’ self-regulated learning competence.</title>
<date>2004</date>
<journal>Computers in Human Behavior,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>567</pages>
<marker>Van den Boom, Paas, Van Merrienboer, Van Gog, 2004</marker>
<rawString>Gerard Van den Boom, Fred Paas, Jeroen JG Van Merrienboer, and Tamara Van Gog. 2004. Reflection prompts and tutor feedback in a web-based learning environment: effects on students’ self-regulated learning competence. Computers in Human Behavior, 20(4):551 – 567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
<author>Chris Brockett</author>
<author>Ani Nenkova</author>
</authors>
<title>Beyond sumbasic: Taskfocused summarization with sentence simplification and lexical expansion.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="16514" citStr="Vanderwende et al., 2007" startWordPosition="2636" endWordPosition="2639">ns used the students (e.g., “graphs” and “charts”). 6 Conclusion In this paper, we presented a novel application to summarize student feedback to reflection prompts by a combination of phrase extraction, phrase clustering and phrase ranking. It makes use of metric clustering to rank the phrases by their student coverage, taking the information source into account. Experimental results demonstrate the good effectiveness of the model. While the proposed method improved the performance against MMR, other summarization methods without an additional MMR component do exist, in1958 cluding SumBasic (Vanderwende et al., 2007), KLSUM and TopicSUM (Haghighi and Vanderwende, 2009). An initial experiment shows they do not yield better performance with default parameters. However, we will revisit it since these methods are meant for full sentences and are not optimized within the phrase framework. In the future, we plan to have additional annotation to evaluate the relative importance using the student coverage numbers. We also deployed CourseMIRROR in a statistics class in Spring 2015 and have created gold-standard summaries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and l</context>
</contexts>
<marker>Vanderwende, Suzuki, Brockett, Nenkova, 2007</marker>
<rawString>Lucy Vanderwende, Hisami Suzuki, Chris Brockett, and Ani Nenkova. 2007. Beyond sumbasic: Taskfocused summarization with sentence simplification and lexical expansion. Inf. Process. Manage., 43(6):1606–1618, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="10503" citStr="Wan and Yang (2008)" startWordPosition="1647" endWordPosition="1650">mantic analysis trained on the Touchstone Applied Science Associates corpus (S¸tef˘anescu et al., 2014). The distance matrix D is constructed from the similarity matrix S by applying the following transformation: D = e−S, which is similar to the common heat kernel but without normalization4. 3In our data, no advantage is observed by including other constituents like verb and prepositional phrases. 4This is not normalized to the range between 0 and 1 since we only care about the relative distance. Number of clusters. For setting the number of clusters without tuning, we adopted a method from √ Wan and Yang (2008), by letting K = V . where K is the number of clusters and V is the number of candidate phrases instead of the number of sentences. 4.3 Phrase ranking In order to estimate the student coverage, phrases are clustered with the algorithm introduced above. We assume the phrases in a cluster are semantically similar to each other and any phrase in a cluster can represent it as a whole. Therefore the coverage of a phrase is assumed to be the same as the coverage of a cluster, which is a union of the students covered by each phrase in the cluster. To select the most representative phrase in a cluster</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multi-document summarization using cluster-based link analysis. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-fang Brook Wu</author>
<author>Quanzhi Li</author>
<author>Razvan Stefan Bot</author>
<author>Xin Chen</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05,</booktitle>
<pages>283--284</pages>
<contexts>
<context position="4536" citStr="Wu et al., 2005" startWordPosition="698" endWordPosition="701">h not the focus of this paper, we have also built a mobile application called CourseMIRROR1 that utilizes the proposed summarization algorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the application. 2 Related Work While summarization systems that extract sentences are dominant, others have published in “summarization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; </context>
</contexts>
<marker>Wu, Li, Bot, Chen, 2005</marker>
<rawString>Yi-fang Brook Wu, Quanzhi Li, Razvan Stefan Bot, and Xin Chen. 2005. Domain-specific keyphrase extraction. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, pages 283–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yang</author>
<author>Zhan Bu</author>
<author>Zhengyou Xia</author>
</authors>
<title>Automatic summarization for chinese text using affinity propagation clustering and latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on Web Information Systems and Mining, WISM’12,</booktitle>
<pages>543--550</pages>
<contexts>
<context position="4891" citStr="Yang et al., 2012" startWordPosition="753" endWordPosition="756">arization” at other levels besides the sentence. For example, Ueda et al. (2000) developed an “at-a-glance” summarization method with handcrafted rules. Recently, keyphrase extraction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only </context>
</contexts>
<marker>Yang, Bu, Xia, 2012</marker>
<rawString>Rui Yang, Zhan Bu, and Zhengyou Xia. 2012. Automatic summarization for chinese text using affinity propagation clustering and latent semantic analysis. In Proceedings of the 2012 International Conference on Web Information Systems and Mining, WISM’12, pages 543–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benyu Zhang</author>
<author>Hua Li</author>
<author>Yi Liu</author>
<author>Lei Ji</author>
<author>Wensi Xi</author>
<author>Weiguo Fan</author>
<author>Zheng Chen</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Improving web search results using affinity graph.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05,</booktitle>
<pages>504--511</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5154" citStr="Zhang et al., 2005" startWordPosition="797" endWordPosition="800"> has received considerable attention, aiming to select important phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization. Clustering has been used to score sentences and has shown good improvement in text summarization (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to estimate the student coverage at a phrase level. Similarly, both diversity-based summarization (Carbonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by minimizing redundancy in the output phrases. Differently, our method performs the redundancy reduction at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information content is considered, but also the information source. 3 Data Our data consists of student responses collected from 53 undergraduates enrolled in an introduction to materials science and engineering class. The students were asked to complete a su</context>
</contexts>
<marker>Zhang, Li, Liu, Ji, Xi, Fan, Chen, Ma, 2005</marker>
<rawString>Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo Fan, Zheng Chen, and Wei-Ying Ma. 2005. Improving web search results using affinity graph. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05, pages 504–511. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Andrew Goldberg</author>
<author>Jurgen Van Gael</author>
<author>David Andrzejewski</author>
</authors>
<title>Improving diversity in ranking using absorbing random walks.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>97--104</pages>
<marker>Zhu, Goldberg, Van Gael, Andrzejewski, 2007</marker>
<rawString>Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski. 2007. Improving diversity in ranking using absorbing random walks. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 97–104, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
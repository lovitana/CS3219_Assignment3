<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000533">
<title confidence="0.956312">
Personalized Machine Translation: Predicting Translational Preferences
</title>
<author confidence="0.885312">
Shachar Mirkin∗
</author>
<affiliation confidence="0.651245">
IBM Research - Haifa
</affiliation>
<address confidence="0.9036535">
Mount Carmel, Haifa
31905, Israel
</address>
<email confidence="0.99804">
shacharm@il.ibm.com
</email>
<author confidence="0.563647">
Jean-Luc Meunier
</author>
<affiliation confidence="0.510198">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.457824">
6 chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.998197">
jean-luc.meunier@xrce.xerox.com
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888647058824">
Machine Translation (MT) has advanced in re-
cent years to produce better translations for
clients’ specific domains, and sophisticated
tools allow professional translators to obtain
translations according to their prior edits. We
suggest that MT should be further personal-
ized to the end-user level – the receiver or the
author of the text – as done in other applica-
tions. As a step in that direction, we propose a
method based on a recommender systems ap-
proach where the user’s preferred translation
is predicted based on preferences of similar
users. In our experiments, this method outper-
forms a set of non-personalized methods, sug-
gesting that user preference information can be
employed to provide better-suited translations
for each user.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987669550724638">
Technologies are increasingly personalized, accommo-
dating their behavior for each user. Such personaliza-
tion is done through user modeling where the goal is to
“get to know” the user. To that end, personalization is
based on users’ attributes, such as demographics (gen-
der, age etc.), personalities, and preferences. For ex-
ample, in Information Retrieval, results are customized
according to the user’s information and search his-
tory (Speretta and Gauch, 2005), performance of Auto-
matic Speech Recognition substantially improves when
adapted to a specific speaker (Neumeyer et al., 1995),
and Targeted Advertising makes use of the user’s loca-
tion and prior purchases (K¨olmel and Alexakis, 2002).
Personalization in machine translation has a some-
what different nature. Providers of MT tools and ser-
vices offer means to “customize” or “personalize” the
translation engine for each client, mostly through do-
main adaptation techniques, and a great deal of effort
is made to make the human-involved translation pro-
cess more efficient (see Section 2.2). Most of the focus,
though, goes to customization for companies or profes-
sional translators. We argue that Personalized Machine
Translation (PMT below) should and can take the next
step and directly address individual end-users.
∗This work was done while the first author was at Xerox
Research Centre Europe.
The difficulty to objectively determine whether one
(automatic) translation is better than another has been
repeatedly revealed in the MT literature. Our con-
jecture is that one reason is individual preferences, to
which we refer as Translational Preferences (TP). TP
come into play both when the alternative translations
are all correct, and when each of them is wrong in a
different way. In the former case, a preference may be
a stylistic choice, and in the latter, a matter of com-
prehension or a selection of the least intolerable error
in one’s opinion. For instance, one user may prefer
shorter sentences than others; she may favor a more
formal style, while another would rather have it casual.
A user could be fine with some reordering errors but be
more picky concerning punctuations. One user will not
be bothered if some words are left untranslated (per-
haps because the source language belongs to the same
language family as the target language that he speaks),
while another will find it utterly displeasing. Such dif-
ferences may be the result of the type of translation sys-
tem being employed (e.g. syntax- vs. phrased-based),
the specific training data or many other factors. On the
user’s side, a preference may be attributed, for exam-
ple, to her mother tongue, her age or her personality.
Two aspects of end-user PMT may be considered:
(i) Personalized translation of texts written by a spe-
cific user, and (ii) PMT to provide better translations
for a specific reader. In this work we address the sec-
ond task, aiming to identify translations each user is
more likely to prefer.1 Specifically, we consider a set-
ting where at least two MT systems are available, and
the goal is to predict which of the translation systems
the user would choose, assuming we have no knowl-
edge about her preference between them. Benchmark-
ing the systems in advance with respect to a reference
set, or estimating the quality of the translations (Specia
et al., 2009) are viable alternatives for translation selec-
tion; these, however, are not personalized to the target
user. Instead, we employ a user-user Collaborative Fil-
tering approach, common in Recommender Systems,
which we map to the TP prediction task.
We assess this approach using a collection of user
rankings of MT systems from a shared translation task
</bodyText>
<footnote confidence="0.996541666666667">
1In (Mirkin et al., 2015) we investigate the first task, as-
sessing whether the author’s demographic and personality
traits are preserved over machine translation.
</footnote>
<page confidence="0.97771">
2019
</page>
<note confidence="0.6628985">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999951333333333">
(see Section 3). Our results show that the personalized
method modestly, but consistently, outperforms several
other approaches that rank the systems in general, dis-
regarding the specific user. We consider this as an in-
dication that user feedback can be employed towards a
more personalized approach to machine translation.
</bodyText>
<sectionHeader confidence="0.986678" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999069">
2.1 Collaborative filtering
</subsectionHeader>
<bodyText confidence="0.999871392857143">
Collaborative filtering (CF) is a common approach
employed by recommender systems for suggesting to
users items, such as books or movies. A recommender
system may simply suggest to all users the most pop-
ular items; often, however, the recommendations are
personalized for each individual user to fit her taste or
preferences. User-user CF relies on community pref-
erences. The idea is to recommend to the user items
that are liked by users similar to her, as manifested, for
example, by high rating. Similar users are those that
agree with the current user on previously-rated items.
In k-nearest-neighbors CF, a user is typically repre-
sented by a vector of her preferences, where each entry
of the vector is, e.g., a rating of a movie. k similar users
are then identified by measuring the similarity between
the users’ vectors. Cosine similarity is a popular func-
tion for that purpose, and we also use it in our work
(Resnick et al., 1994; Sarwar et al., 2001; Ricci et al.,
2011). An alternative to cosine, Pearson’s correlation
coefficient (Pearson, 1895), allows addressing different
rating patterns across users. In comparison to cosine,
here vector entries are normalized with respect to the
user’s average rating. In our case, such normalization
is not very meaningful since the entries of the users
vectors represent comparisons rather than absolute rat-
ings, as will be made clear in Section 4. Nevertheless,
we have experimented with Pearson correlation as well,
and found no advantage in using it instead of cosine.
</bodyText>
<subsectionHeader confidence="0.9914965">
2.2 Customization, personalization and
adaptation in MT
</subsectionHeader>
<bodyText confidence="0.999575807692308">
Various means of customization and personalization
are available, in both academic and commercial MT.
Many of them target the company, rather than the in-
dividual user, and much of the effort is invested in de-
signing tools for professional translators, aiming to im-
prove their productivity, through intelligent Computer
Aided Translation (CAT).
Domain adaptation methods are commonly used to
adapt to the topic, the genre and even the style of the
translated material. Using the company’s own corpora
is one of the simplest techniques to do so, but many
more approaches have been proposed, including data-
selection (Axelrod et al., 2011; Gasc´o et al., 2012;
Mirkin and Besacier, 2014), mixture models (Foster
and Kuhn, 2007) and table fill-up (Bisazza et al., 2011).
Clients can utilize their own glossaries (Federico et
al., 2014), corpora (parallel or monolingual) and trans-
lation memories (TM), either shared or private ones
(Caskey and Maskey, 2014; Federico et al., 2014).
Through Adaptive and Interactive MT (Nepveu et al.,
2004), the system learns from the translator’s edits, in
order to avoid repeating errors that have already been
corrected. Post-editions can continuously be added to
the translator’s TM or be used as additional training
material, for tighter adaptation to the domain of inter-
est, through batch or incremental training.
</bodyText>
<subsectionHeader confidence="0.999573">
2.3 User preferences in MT
</subsectionHeader>
<bodyText confidence="0.999982735294118">
Many tasks that require annotation by humans are af-
fected by the annotator and not only by the item be-
ing judged. Metrics for inter-rater reliability or inter-
annotator agreement, such as Cohen’s Kappa (Cohen,
1960), help measuring the extent to which annotators
disagree. Disagreement may be due to untrained or
inattentive annotators, a result of a task that is not well
defined, or when there is no obvious “truth”. Such is
the case with the evaluation of translation quality – it
is not always straightforward to tell whether one trans-
lation is better than another. A single sentence can be
translated in multiple correct ways. The decision be-
comes even harder when the translations are automat-
ically produced and are imperfect: Is one error worse
than another? The answer is in the eye of the beholder.
MT papers regularly report rather low Kappa levels,
even when measured on simpler tasks, such as short
segments (Mach´aˇcek and Bojar, 2015).
Turchi et al. (2013) refer to the issue of “subjectiv-
ity” of human annotators. They address the task of
binary classification of “good” vs. “bad” translations,
and show that relying on human annotation for training
a binary quality estimator is less effective than using
automatically-generated labels. This subjectivity is ex-
actly what we are after. We treat it as a preference,
trying to identify the systems or specific translations
which the user subjectively prefers.
Kichhoff et al. (2012) analyze user preferences with
respect to MT errors. They show that some types, e.g.
word order errors, are the most dis-preferred by users,
and that this is a more important factor than the number
of errors. While very relevant for our research, their
analysis is aggregated over all users participating in the
study, and is not focusing on individuals’ preferences.
</bodyText>
<sectionHeader confidence="0.996059" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9999367">
In this work we used the data provided for the MT
Shared Task in the 2013 Workshop on Statistical Ma-
chine Translation (WMT) (Bojar et al., 2013).2 This
data was of a particularly large scale, with crowd-
sourced human judges, either volunteer researchers or
paid Amazon Turkers. For each source sentence, a
judge was presented with the source sentence itself,
a reference translation, and the outputs of five ma-
chine translation systems. The five systems were ran-
domly selected from the pool of participating systems,
</bodyText>
<footnote confidence="0.9554355">
2http://www.statmt.org/wmt13/
translation-task.html
</footnote>
<page confidence="0.98896">
2020
</page>
<bodyText confidence="0.969157310344828">
and were anonymized and randomly-ordered when pre-
sented to the judge. The judge had to rank the transla-
tions, with ties allowed (i.e. two system can receive the
same ranking). Hence, each annotation point provided
with 10 pairwise rankings between systems. Transla-
tions of 10 language pairs were assessed, with 11 to 19
systems for each pair. In total, over 900K non-tied pair-
wise rankings were collected. The Turkers’ annotation
included a control task for quality assurance, rejecting
Turkers failing more than 50% of the control points.
The inter-annotator score showed on average a fair to
moderate level of agreement.
4 Translational preferences with
collaborative filtering
Our method, denoted CTP (Collaborative Translational
Preferences), is based on a k-nearest-neighbors ap-
proach for user-user CF. That is, we predict the trans-
lational preferences of a user based on those of similar
users. In our setting, a user preference is the choice be-
tween two translation systems – which system’s trans-
lations does the user prefer. Given two systems (or
models of the same system) we wish to predict which
one the user would prefer, without assuming the user
has ever expressed her preference between these two
specific systems. It is important to emphasize that
the method presented here considers the users’ overall
preferences of systems, and does not regard the spe-
cific sentence that is being translated. In future work
we intend to make use of this information as well.
</bodyText>
<subsectionHeader confidence="0.987099">
4.1 Representation
</subsectionHeader>
<bodyText confidence="0.962717166666667">
As mentioned in Section 3, each annotation consists
of a ranking of five systems. From that, we extract
pairwise rankings for every pair of systems that were
ranked for a given language pair. For each user u E U
(where U are all users who annotated the language
pair), we create a user-preference vector, pu, that con-
tains an entry for each pair of translation systems. De-
noting the set of systems with S, we have |S|·(|S|−1)
2
system pairs. E.g., for Czech-English, with 11 partic-
ipating systems, the user vector size is 55. Each entry
(i, j) of the vector is assigned the following value:
</bodyText>
<equation confidence="0.9828434">
(i,j)
u
pu(i,j) = wu − l(i,j) (1)
w(i,j) u+ l(i,j)
u
</equation>
<bodyText confidence="0.966528090909091">
where w(i,j)
u and l(i,j)
u are the number of wins and loses
of system si vs. system sj as judged by user u.3
With this representation, a user vector contains val-
ues between −1 (if si always lost to sj) and 1 (if si al-
ways won). If the user always ranked the two systems
identically, the value is 0, and if she has never evalu-
ated the pair, the entry is regarded as a missing value
(NA). Altogether, we have a matrix of users by system
pairs, as depicted in Figure 1.
</bodyText>
<footnote confidence="0.7952525">
3We have also considered including ties in the denomina-
tor of the equation; discarding them was found superior.
</footnote>
<figure confidence="0.978554">
(W)
`il
U
SxS
</figure>
<figureCaption confidence="0.999977">
Figure 1: The user-preferences matrix.
</figureCaption>
<subsectionHeader confidence="0.994024">
4.2 Finding similar users
</subsectionHeader>
<bodyText confidence="0.999979">
Given a user preference to predict for a pair of sys-
tems (si, sj), we compute the similarity between pu
and each one of pug for all other u0 E U. In our exper-
iments we used cosine as the similarity measure. The
k most-similar-users (MSU) are then selected. To be
included in MSU(u), we require that u and u0 have
judged at least 2 common system pairs.
</bodyText>
<subsectionHeader confidence="0.99627">
4.3 Preference prediction
</subsectionHeader>
<bodyText confidence="0.998555666666667">
Given the similarity scores, to predict the user’s prefer-
ence for the target system pair, we compute a weighted
average of the predictions of the users in MSU(u).
We include in the average only users with similar-
ity scores above a certain positive threshold (0.05). We
then require that a minimum number of users meet the
above criteria of common annotations and minimum
similarity (we used 5). If not enough such similar
users are found, we turn to a fallback, where we use
the non-weighted average preference across all users
(AVPF presented in Section 5).4 The prediction is then
the sign of the weighted average. A positive value
means si is the preferred system; a negative one means
it is sj, and a zero is a draw. In our evaluation we com-
pare this prediction to the sign of the actual preference
of the user, pu(i,j). Formally, CTP computes the fol-
lowing prediction function f for a given user u and a
system pair (si, sj):
</bodyText>
<equation confidence="0.995724">
�u, pu, (i,j) · sim(u, u0)
fCTP(u)(i,j) = sign( ) (2)
Eu, sim(u, u0)
</equation>
<bodyText confidence="0.99978575">
where u0 E MSU(u) are the most similar users (the
nearest neighbors) of u; pu (i,j) are the preferences
of user u0 for (si, sj) and sim(u, u0) is the similarity
score between the two users.5
</bodyText>
<sectionHeader confidence="0.996258" genericHeader="method">
5 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.887884">
5.1 Evaluation methodology
</subsectionHeader>
<bodyText confidence="0.9991725">
In our experiments we try to predict which one of two
translation systems would be preferred by a given user.
</bodyText>
<footnote confidence="0.9758942">
4The fallback was used 0.1% of the times.
5The denominator is not required as long as we predict
only the sign since all used similarity scores are positive. We
keep it in order to obtain a normalized score that can be used
for other decisions, e.g. ranking multiple systems.
</footnote>
<equation confidence="0.397948">
Pu
Pu (U)
</equation>
<page confidence="0.953648">
2021
</page>
<bodyText confidence="0.999993470588236">
We evaluate our method, as well as several other pre-
diction functions, when compared with the user’s pair-
wise system preference according to the annotation –
pu(i,j), shown in Equation 1. For each user this is an
aggregated figure over all her pairwise rankings for the
pair, determining the preferred system as the one cho-
sen by the user (i.e. ranked higher) more times.
We conduct a leave-one-out experiment. For each
language pair, we iterate over all non-NA entries in the
user-preferences matrix, remove the entry and try to
predict it. User similarity scores are re-computed for
each evaluation point, to ensure they do not consider
the target pair. The “gold” preference is positive when
the user prefers si, negative when she prefers sj and
0 when she has no preference between them. Hence,
each of the assessed methods is measured by the accu-
racy of predicting the sign of the preference.
</bodyText>
<sectionHeader confidence="0.909506" genericHeader="method">
5.2 Non-personalized methods
</sectionHeader>
<bodyText confidence="0.994661142857143">
We compare CTP to the following prediction methods:
Always i (ALI) This is a naive baseline showing the
score when always predicting that system i wins. Note
that the baseline is not simply 50% due to ties.
Average rank (RANK) Here, two systems are com-
pared by the average of their rankings across all anno-
tations (r ∈ {1, 2,3,4,5}):
</bodyText>
<equation confidence="0.993504">
fRANK(u)(i,j) = sign(rj − ri) (3)
</equation>
<bodyText confidence="0.99964455">
rj and ri are the average ranks of sj and si respec-
tively. Since a smaller value of r corresponds to a
higher rank, we subtract the rank of si from sj and
not the other way around. This way, if for instance,
si is ranked on averaged higher than sj, the prediction
would be positive, as desired.
Expected (EXPT) This metric, proposed by
Koehn (2012) and used by Bojar et al. (2013) in
order to rank the participating systems in the WMT
benchmark, compares the expected wins of the two
systems. Its intuition is explained as follows: “If
the system is compared against a randomly picked
opposing system, on a randomly picked sentence, by a
randomly picked judge, what is the probability that its
translation is ranked higher?” The expected wins of si,
e(si), is the probability of si to win when compared
to another system, estimated as the total number of
wins of si relative to the total number of comparisons
involving it, excluding ties, and normalized by the total
number of systems excluding si, |{sk}|:
</bodyText>
<equation confidence="0.997028">
1 w(i,k)
e(i) = s 4
k}I k#Z w(i&apos;k) + L(i,k) ( )
</equation>
<bodyText confidence="0.999976">
where w(i,k) and l(i,k) are summed over all users.
The preference prediction is therefore:
</bodyText>
<equation confidence="0.990799">
fEXPT(u)(i,j) = sign(e(i) − e(j)) (5)
</equation>
<bodyText confidence="0.998445375">
RANK and EXPT predict preferences based on a sys-
tem’s performance in general, when compared to all
other systems. We propose an additional prediction
function for comparison which uses only the informa-
tion concerning the system pair under consideration.
Average user preference (AVPF) This method takes
into account only the specific system pair and averages
the user preferences for the pair. Formally:
</bodyText>
<equation confidence="0.934765">
fAVPF(u)(i,j) = sign(iuu (,,j)) (6)
</equation>
<bodyText confidence="0.990142">
where u&apos; =6 u, and {u&apos;} are all users except u.
This method can be viewed as a non-personalized
version of CTP, with two differences:
</bodyText>
<listItem confidence="0.999704">
(1) It considers all users, and not only similar ones.
(2) It does not weight the preferences of the other
users by their similarity to the target user.
</listItem>
<sectionHeader confidence="0.497543" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.999987083333333">
Table 1 shows the results of an experiment comparing
the performance of the various methods in terms of pre-
diction accuracy. Figure 2 shows the micro-average
scores, when giving each of the 97,412 test points an
equal weight in the average. CTP outperforms all others
for 9 out of 10 language pairs, and in the overall micro-
averaged results. The difference between CTP and each
of the other metrics was found statistically significance
with p &lt; 5 · 10−6 at worse, as measured with a paired
Wilcoxon signed rank test (Wilcoxon, 1945) on the pre-
dictions of the two methods. The significance test cap-
tures in this case the fact that the methods disagreed in
many more cases than is visible by the score difference.
Our method was found superior to all others also
when computing macro-average, taking the average of
the scores of each language pair, as well as when the
ties are included in the computation of pu.
The parameters with which the above results were
obtained are found within the method’s description in
Section 4. Yet, in our experiments, CTP turned out to
be rather insensitive to their values. In this experiment
we used a global set of parameters and did not tune
them for each language pair separately. It is reasonable
to assume that such tuning would improve results. For
instance, choosing k, the number of users to include in
the average, depends on the total number of users. E.g.,
for en-es, where there are only 57 users in total, reduc-
ing k’s value from 50 to 25, improves results of CTP
from 62.6% to 63.2%, higher than all other methods
(whose scores are not affected).
Specifically in comparison to AVPF, weighting by
the similarity scores was found to be a more significant
factor than selecting a small subset of the users. This
may not come as a surprise, since less similar users that
are added to MSU(u) have a smaller impact on the fi-
nal decision since their weight in the average is smaller.
</bodyText>
<page confidence="0.985314">
2022
</page>
<table confidence="0.9996786">
Lang. f Acc.
ALI 31.6
RANK 62.9
cs-en EXPT 63.5
AVPF 63.5
CTP 64.4
ALI 36.2
RANK 67.8
en-cs EXPT 67.9
AVPF 67.4
CTP 68.2
Lang. f Acc.
ALI 41.7
RANK 62.6
de-en EXPT 62.6
AVPF 62.6
CTP 63.5
ALI 42.0
RANK 67.2
en-de EXPT 66.9
AVPF 66.5
CTP 67.6
Lang. f Acc.
ALI 35.5
RANK 61.0
es-en EXPT 61.2
AVPF 61.4
CTP 63.0
ALI 35.9
RANK 62.3
en-es EXPT 63.0
AVPF 61.5
CTP 62.6
Lang f Acc.
ALI 35.0
RANK 61.3
fr-en EXPT 61.2
AVPF 61.2
CTP 61.8
ALI 35.0
RANK 65.0
en-fr EXPT 65.1
AVPF 64.4
CTP 65.3
Lang. f Acc.
ALI 43.5
RANK 57.6
ru-en EXPT 57.8
AVPF 56.6
CTP 58.2
ALI 44.6
RANK 70.2
en-ru EXPT 72.1
AVPF 71.4
CTP 72.4
</table>
<tableCaption confidence="0.999896">
Table 1: Results in accuracy percentage for the 10 language pairs, including the languages: Czech (cs), English
</tableCaption>
<bodyText confidence="0.9786258">
(en), German (de), Spanish (es), French (fr) and Russian (ru). The best results is in bold. The difference between
CTP and each of the other methods is highly statistically significant. Figure 2 shows a micro-average of these
results.
One weakness of CTP, as well as of other methods,
is that it poorly predict ties. In the above experiment,
approximately 13.5% of the preferences were 0, none
of them was correctly identified. Our analysis showed
that numerical accuracy is not the main cause; setting
any prediction that is smaller than some values of |ε|
to 0 was not found helpful. Arguably, ties need not be
predicted, since if the user has no preference between
two systems, any choice is just as good. Still, we be-
lieve that better ties prediction could lead to general
improvement of our method and we wish to address it
in future work.
</bodyText>
<figureCaption confidence="0.994549">
Figure 2: Micro-average over all 97,412 test points.
</figureCaption>
<sectionHeader confidence="0.997453" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999975309090909">
We addressed the task of predicting user preference
with respect to MT output via a collaborative filtering
approach whose prediction is based on preferences of
similar users. This method predicts TP better than a
set of non-personalized methods. The gain is modest
in absolute numbers, but the results are highly statisti-
cally significant and stable over parameter values.
We consider this work as a step towards more per-
sonalized MT. This line of research can be extended
in multiple ways. First and foremost, as mentioned,
we did not consider the actual content of the sentences,
but rather identified a general preference for one system
over another. It is plausible, however, that one system is
better – from the user’s perspective – at translating one
type of text, while another is preferred for other texts.
Taking the actual texts into account seems therefore es-
sential. Content-based methods for recommender sys-
tems may be useful for this purpose. Another factor
that may be affecting preferences is translation quality:
when compared translations are all poor, preferences
play a less significant role. Hence, it may be informa-
tive to assess TP prediction separately across different
levels of translation quality.
Large parallel corpora are typically required for
training reasonable statistical translation models. Yet,
parallel corpora, and even more so in-domain ones,
are hard to gather. It is virtually impossible to find a
user-specific parallel corpus, and methods for mono-
lingual domain adaptation are easier to envisage if one
wishes to address author-aware PMT (the first PMT
task mentioned in Section 1). Collecting user feed-
back is another challenge, especially since most end-
users do not speak the source language. For that and
other reasons, it currently seems more feasible to col-
lect preference information from professional transla-
tors, explicitly or implicitly.Yet, in this research we
aim at end-users rather than translators whose prefer-
ences are often driven by the ease of correction more
than anything else. We believe that one way to tackle
this issue is to exploit other kinds of feedback, from
which we can infer user preferences and similarity.
Online MT providers are recently collecting end-user
feedback for their proposed translations which may be
useful for TP prediction. For instance, in early 2015
Facebook introduced a feature letting users rate (Bing)
translations, and Google Translate asks for suggested
improvements. We are hopeful that such data becomes
publicly available. Nevertheless, it remains unlikely
to obtain feedback from each and every user. A po-
tential direction for both corpora and feedback col-
lection is personalizing models and identifying prefer-
ences for groups of users based on socio-demographic
traits, such as gender, age or mother tongue, or based
on (e.g. Big 5) personality traits. These can even be
inferred by automatically analyzing user texts.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998262">
We wish to thank Herv´e D´ejean and the EMNLP re-
viewers for their valuable feedback on this work.
</bodyText>
<figure confidence="0.994310058823529">
ALI RANK EXPT AVPF CTP
Prediction accuracy (%)
60
40
30
50
39.3
61.7
61.8
61.4
62.6
Methods
ALI
RANK
EXPT
AVPF
CTP
</figure>
<page confidence="0.928397">
2023
</page>
<sectionHeader confidence="0.983236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982475">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 355–362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT), San Francisco, California, USA.
Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation (WMT), pages 1–44, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Sasha P. Caskey and Sameer Maskey. 2014. Trans-
lation cache prediction, August 12. US Patent
8,805,672.
Jacob Cohen. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement, 20(1):37.
Marcello Federico, Nicola Bertoldi, Mauro Cettolo,
Matteo Negri, Marco Turchi, Marco Trombetti,
Alessandro Cattelan, Antonio Farina, Domenico
Lupinetti, Andrea Martines, Alberto Massidda, Hol-
ger Schwenk, Loic Barrault, Frederic Blain, Philipp
Koehn, Christian Buck, and Ulrich Germann. 2014.
The matecat tool. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 129–
132, Dublin, Ireland, August. Dublin City University
and Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Translation
(WMT), Prague, Czech Republic, June.
Guillem Gasc´o, Martha-Alicia Rocha, Germ´an
Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Fran-
cisco Casacuberta. 2012. Does more data always
yield better translations? In Proceedings of the
13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
152–161, Avignon, France, April. Association for
Computational Linguistics.
Katrin Kirchhoff, Daniel Capurro, and Anne Turner.
2012. Evaluating user preferences in machine
translation using conjoint analysis. Proceedings of
the European Association of Machine Translation,
16:119–126.
Philipp Koehn. 2012. Simulating human judgment
in machine translation evaluation campaigns. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 179–184,
Hong Kong.
Bernhard K¨olmel and Spiros Alexakis. 2002. Location
based advertising. In Proceedings of the First In-
ternational Conference on Mobile Business, Athens,
Greece.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2015. Evaluat-
ing machine translation quality using short segments
annotations. The Prague Bulletin of Mathematical
Linguistics, No. 103:85–110, April.
Shachar Mirkin and Laurant Besacier. 2014. Data se-
lection for compact adapted SMT models. In Pro-
ceedings of the eleventh biennial conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2014), Vancouver, Canada, Oct.
Shachar Mirkin, Scott Nowson, Caroline Brun, and
Julien Perez. 2015. Motivating personality-aware
machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Lisbon, Portugal. Association
for Computational Linguistics.
Laurent Nepveu, P Langlais, G Lapalme, and George
Foster. 2004. Adaptive language and translation
models for interactive machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 190–
197, Barcelona, Spain.
Leonardo Neumeyer, Ananth Sankar, and Vassilios Di-
galakis. 1995. A comparative study of speaker
adaptation techniques. In Fourth European Con-
ference on Speech Communication and Technology,
EUROSPEECH 1995, Madrid, Spain, September
18-21, 1995.
Karl Pearson. 1895. Note on regression and inheri-
tance in the case of two parents. Proceedings of the
Royal Society of London, 58(347-352):240–242.
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-
ter Bergstrom, and John Riedl. 1994. Grouplens:
an open architecture for collaborative filtering of
netnews. In Proceedings of the 1994 ACM con-
ference on Computer supported cooperative work,
pages 175–186. ACM.
Francesco Ricci, Lior Rokach, and Bracha Shapira.
2011. Introduction to recommender systems hand-
book.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the
10th international conference on World Wide Web,
pages 285–295. ACM.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28–37.
</reference>
<page confidence="0.879645">
2024
</page>
<reference confidence="0.999207285714286">
Mirco Speretta and Susan Gauch. 2005. Per-
sonalized search based on user search histories.
In Web Intelligence, 2005. Proceedings. The 2005
IEEE/WIC/ACM International Conference on, pages
622–628. IEEE.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the subjectivity of human judge-
ments in MT quality estimation. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation (WMT), pages 240–251, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80–83,
December.
</reference>
<page confidence="0.993338">
2025
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.385240">
<title confidence="0.96943">Personalized Machine Translation: Predicting Translational Preferences</title>
<affiliation confidence="0.8098295">IBM Research - Mount Carmel,</affiliation>
<address confidence="0.863313">31905,</address>
<email confidence="0.999741">shacharm@il.ibm.com</email>
<author confidence="0.653496">Jean-Luc</author>
<affiliation confidence="0.973742">Xerox Research Centre</affiliation>
<address confidence="0.9603555">6 chemin de 38240 Meylan,</address>
<email confidence="0.99995">jean-luc.meunier@xrce.xerox.com</email>
<abstract confidence="0.998213277777778">Machine Translation (MT) has advanced in recent years to produce better translations for clients’ specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits. We suggest that MT should be further personalized to the end-user level – the receiver or the author of the text – as done in other applications. As a step in that direction, we propose a method based on a recommender systems approach where the user’s preferred translation is predicted based on preferences of similar users. In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7660" citStr="Axelrod et al., 2011" startWordPosition="1204" endWordPosition="1207">of customization and personalization are available, in both academic and commercial MT. Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as addit</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 355–362, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus interpolation methods for phrase-based SMT adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>San Francisco, California, USA.</location>
<contexts>
<context position="7790" citStr="Bisazza et al., 2011" startWordPosition="1225" endWordPosition="1228">han the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training. 2.3 User prefere</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus interpolation methods for phrase-based SMT adaptation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), San Francisco, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="10364" citStr="Bojar et al., 2013" startWordPosition="1645" endWordPosition="1648">ify the systems or specific translations which the user subjectively prefers. Kichhoff et al. (2012) analyze user preferences with respect to MT errors. They show that some types, e.g. word order errors, are the most dis-preferred by users, and that this is a more important factor than the number of errors. While very relevant for our research, their analysis is aggregated over all users participating in the study, and is not focusing on individuals’ preferences. 3 Data In this work we used the data provided for the MT Shared Task in the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013).2 This data was of a particularly large scale, with crowdsourced human judges, either volunteer researchers or paid Amazon Turkers. For each source sentence, a judge was presented with the source sentence itself, a reference translation, and the outputs of five machine translation systems. The five systems were randomly selected from the pool of participating systems, 2http://www.statmt.org/wmt13/ translation-task.html 2020 and were anonymized and randomly-ordered when presented to the judge. The judge had to rank the translations, with ties allowed (i.e. two system can receive the same ranki</context>
<context position="17290" citStr="Bojar et al. (2013)" startWordPosition="2850" endWordPosition="2853">ing that system i wins. Note that the baseline is not simply 50% due to ties. Average rank (RANK) Here, two systems are compared by the average of their rankings across all annotations (r ∈ {1, 2,3,4,5}): fRANK(u)(i,j) = sign(rj − ri) (3) rj and ri are the average ranks of sj and si respectively. Since a smaller value of r corresponds to a higher rank, we subtract the rank of si from sj and not the other way around. This way, if for instance, si is ranked on averaged higher than sj, the prediction would be positive, as desired. Expected (EXPT) This metric, proposed by Koehn (2012) and used by Bojar et al. (2013) in order to rank the participating systems in the WMT benchmark, compares the expected wins of the two systems. Its intuition is explained as follows: “If the system is compared against a randomly picked opposing system, on a randomly picked sentence, by a randomly picked judge, what is the probability that its translation is ranked higher?” The expected wins of si, e(si), is the probability of si to win when compared to another system, estimated as the total number of wins of si relative to the total number of comparisons involving it, excluding ties, and normalized by the total number of sy</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT), pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha P Caskey</author>
<author>Sameer Maskey</author>
</authors>
<title>Translation cache prediction,</title>
<date>2014</date>
<tech>US Patent 8,805,672.</tech>
<contexts>
<context position="7977" citStr="Caskey and Maskey, 2014" startWordPosition="1253" endWordPosition="1256">nslation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training. 2.3 User preferences in MT Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged. Metrics for inter-rater reliability or interannotator agreemen</context>
</contexts>
<marker>Caskey, Maskey, 2014</marker>
<rawString>Sasha P. Caskey and Sameer Maskey. 2014. Translation cache prediction, August 12. US Patent 8,805,672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<contexts>
<context position="8615" citStr="Cohen, 1960" startWordPosition="1357" endWordPosition="1358">Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training. 2.3 User preferences in MT Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged. Metrics for inter-rater reliability or interannotator agreement, such as Cohen’s Kappa (Cohen, 1960), help measuring the extent to which annotators disagree. Disagreement may be due to untrained or inattentive annotators, a result of a task that is not well defined, or when there is no obvious “truth”. Such is the case with the evaluation of translation quality – it is not always straightforward to tell whether one translation is better than another. A single sentence can be translated in multiple correct ways. The decision becomes even harder when the translations are automatically produced and are imperfect: Is one error worse than another? The answer is in the eye of the beholder. MT pape</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
<author>Matteo Negri</author>
<author>Marco Turchi</author>
<author>Marco Trombetti</author>
<author>Alessandro Cattelan</author>
<author>Antonio Farina</author>
<author>Domenico Lupinetti</author>
<author>Andrea Martines</author>
<author>Alberto Massidda</author>
<author>Holger Schwenk</author>
<author>Loic Barrault</author>
<author>Frederic Blain</author>
<author>Philipp Koehn</author>
<author>Christian Buck</author>
<author>Ulrich Germann</author>
</authors>
<title>The matecat tool.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,</booktitle>
<pages>129--132</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="7856" citStr="Federico et al., 2014" startWordPosition="1235" endWordPosition="1238">signing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training. 2.3 User preferences in MT Many tasks that require annotation by humans are affect</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, Negri, Turchi, Trombetti, Cattelan, Farina, Lupinetti, Martines, Massidda, Schwenk, Barrault, Blain, Koehn, Buck, Germann, 2014</marker>
<rawString>Marcello Federico, Nicola Bertoldi, Mauro Cettolo, Matteo Negri, Marco Turchi, Marco Trombetti, Alessandro Cattelan, Antonio Farina, Domenico Lupinetti, Andrea Martines, Alberto Massidda, Holger Schwenk, Loic Barrault, Frederic Blain, Philipp Koehn, Christian Buck, and Ulrich Germann. 2014. The matecat tool. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 129– 132, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7749" citStr="Foster and Kuhn, 2007" startWordPosition="1218" endWordPosition="1221"> Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation (WMT), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillem Gasc´o</author>
<author>Martha-Alicia Rocha</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Jes´us Andr´es-Ferrer</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Does more data always yield better translations?</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>152--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<marker>Gasc´o, Rocha, Sanchis-Trilles, Andr´es-Ferrer, Casacuberta, 2012</marker>
<rawString>Guillem Gasc´o, Martha-Alicia Rocha, Germ´an Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco Casacuberta. 2012. Does more data always yield better translations? In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 152–161, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Daniel Capurro</author>
<author>Anne Turner</author>
</authors>
<title>Evaluating user preferences in machine translation using conjoint analysis.</title>
<date>2012</date>
<booktitle>Proceedings of the European Association of Machine Translation,</booktitle>
<pages>16--119</pages>
<marker>Kirchhoff, Capurro, Turner, 2012</marker>
<rawString>Katrin Kirchhoff, Daniel Capurro, and Anne Turner. 2012. Evaluating user preferences in machine translation using conjoint analysis. Proceedings of the European Association of Machine Translation, 16:119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>179--184</pages>
<location>Hong Kong.</location>
<contexts>
<context position="17258" citStr="Koehn (2012)" startWordPosition="2845" endWordPosition="2846">score when always predicting that system i wins. Note that the baseline is not simply 50% due to ties. Average rank (RANK) Here, two systems are compared by the average of their rankings across all annotations (r ∈ {1, 2,3,4,5}): fRANK(u)(i,j) = sign(rj − ri) (3) rj and ri are the average ranks of sj and si respectively. Since a smaller value of r corresponds to a higher rank, we subtract the rank of si from sj and not the other way around. This way, if for instance, si is ranked on averaged higher than sj, the prediction would be positive, as desired. Expected (EXPT) This metric, proposed by Koehn (2012) and used by Bojar et al. (2013) in order to rank the participating systems in the WMT benchmark, compares the expected wins of the two systems. Its intuition is explained as follows: “If the system is compared against a randomly picked opposing system, on a randomly picked sentence, by a randomly picked judge, what is the probability that its translation is ranked higher?” The expected wins of si, e(si), is the probability of si to win when compared to another system, estimated as the total number of wins of si relative to the total number of comparisons involving it, excluding ties, and norm</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 179–184, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard K¨olmel</author>
<author>Spiros Alexakis</author>
</authors>
<title>Location based advertising.</title>
<date>2002</date>
<booktitle>In Proceedings of the First International Conference on Mobile Business,</booktitle>
<location>Athens, Greece.</location>
<marker>K¨olmel, Alexakis, 2002</marker>
<rawString>Bernhard K¨olmel and Spiros Alexakis. 2002. Location based advertising. In Proceedings of the First International Conference on Mobile Business, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Evaluating machine translation quality using short segments annotations. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2015</date>
<tech>No. 103:85–110,</tech>
<marker>Mach´acek, Bojar, 2015</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2015. Evaluating machine translation quality using short segments annotations. The Prague Bulletin of Mathematical Linguistics, No. 103:85–110, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Laurant Besacier</author>
</authors>
<title>Data selection for compact adapted SMT models.</title>
<date>2014</date>
<booktitle>In Proceedings of the eleventh biennial conference of the Association for Machine Translation in the Americas (AMTA-2014),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="7709" citStr="Mirkin and Besacier, 2014" startWordPosition="1212" endWordPosition="1215">ailable, in both academic and commercial MT. Many of them target the company, rather than the individual user, and much of the effort is invested in designing tools for professional translators, aiming to improve their productivity, through intelligent Computer Aided Translation (CAT). Domain adaptation methods are commonly used to adapt to the topic, the genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation t</context>
</contexts>
<marker>Mirkin, Besacier, 2014</marker>
<rawString>Shachar Mirkin and Laurant Besacier. 2014. Data selection for compact adapted SMT models. In Proceedings of the eleventh biennial conference of the Association for Machine Translation in the Americas (AMTA-2014), Vancouver, Canada, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Scott Nowson</author>
<author>Caroline Brun</author>
<author>Julien Perez</author>
</authors>
<title>Motivating personality-aware machine translation.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4742" citStr="Mirkin et al., 2015" startWordPosition="749" endWordPosition="752"> translation systems the user would choose, assuming we have no knowledge about her preference between them. Benchmarking the systems in advance with respect to a reference set, or estimating the quality of the translations (Specia et al., 2009) are viable alternatives for translation selection; these, however, are not personalized to the target user. Instead, we employ a user-user Collaborative Filtering approach, common in Recommender Systems, which we map to the TP prediction task. We assess this approach using a collection of user rankings of MT systems from a shared translation task 1In (Mirkin et al., 2015) we investigate the first task, assessing whether the author’s demographic and personality traits are preserved over machine translation. 2019 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2019–2025, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (see Section 3). Our results show that the personalized method modestly, but consistently, outperforms several other approaches that rank the systems in general, disregarding the specific user. We consider this as an indication that user feedback can be employed</context>
</contexts>
<marker>Mirkin, Nowson, Brun, Perez, 2015</marker>
<rawString>Shachar Mirkin, Scott Nowson, Caroline Brun, and Julien Perez. 2015. Motivating personality-aware machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>P Langlais</author>
<author>G Lapalme</author>
<author>George Foster</author>
</authors>
<title>Adaptive language and translation models for interactive machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>190--197</pages>
<location>Barcelona,</location>
<contexts>
<context position="8060" citStr="Nepveu et al., 2004" startWordPosition="1266" endWordPosition="1269"> genre and even the style of the translated material. Using the company’s own corpora is one of the simplest techniques to do so, but many more approaches have been proposed, including dataselection (Axelrod et al., 2011; Gasc´o et al., 2012; Mirkin and Besacier, 2014), mixture models (Foster and Kuhn, 2007) and table fill-up (Bisazza et al., 2011). Clients can utilize their own glossaries (Federico et al., 2014), corpora (parallel or monolingual) and translation memories (TM), either shared or private ones (Caskey and Maskey, 2014; Federico et al., 2014). Through Adaptive and Interactive MT (Nepveu et al., 2004), the system learns from the translator’s edits, in order to avoid repeating errors that have already been corrected. Post-editions can continuously be added to the translator’s TM or be used as additional training material, for tighter adaptation to the domain of interest, through batch or incremental training. 2.3 User preferences in MT Many tasks that require annotation by humans are affected by the annotator and not only by the item being judged. Metrics for inter-rater reliability or interannotator agreement, such as Cohen’s Kappa (Cohen, 1960), help measuring the extent to which annotato</context>
</contexts>
<marker>Nepveu, Langlais, Lapalme, Foster, 2004</marker>
<rawString>Laurent Nepveu, P Langlais, G Lapalme, and George Foster. 2004. Adaptive language and translation models for interactive machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 190– 197, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Neumeyer</author>
<author>Ananth Sankar</author>
<author>Vassilios Digalakis</author>
</authors>
<title>A comparative study of speaker adaptation techniques. In</title>
<date>1995</date>
<booktitle>Fourth European Conference on Speech Communication and Technology, EUROSPEECH 1995,</booktitle>
<location>Madrid, Spain,</location>
<contexts>
<context position="1639" citStr="Neumeyer et al., 1995" startWordPosition="237" endWordPosition="240">ions for each user. 1 Introduction Technologies are increasingly personalized, accommodating their behavior for each user. Such personalization is done through user modeling where the goal is to “get to know” the user. To that end, personalization is based on users’ attributes, such as demographics (gender, age etc.), personalities, and preferences. For example, in Information Retrieval, results are customized according to the user’s information and search history (Speretta and Gauch, 2005), performance of Automatic Speech Recognition substantially improves when adapted to a specific speaker (Neumeyer et al., 1995), and Targeted Advertising makes use of the user’s location and prior purchases (K¨olmel and Alexakis, 2002). Personalization in machine translation has a somewhat different nature. Providers of MT tools and services offer means to “customize” or “personalize” the translation engine for each client, mostly through domain adaptation techniques, and a great deal of effort is made to make the human-involved translation process more efficient (see Section 2.2). Most of the focus, though, goes to customization for companies or professional translators. We argue that Personalized Machine Translation</context>
</contexts>
<marker>Neumeyer, Sankar, Digalakis, 1995</marker>
<rawString>Leonardo Neumeyer, Ananth Sankar, and Vassilios Digalakis. 1995. A comparative study of speaker adaptation techniques. In Fourth European Conference on Speech Communication and Technology, EUROSPEECH 1995, Madrid, Spain, September 18-21, 1995.</rawString>
</citation>
<citation valid="false">
<title>Note on regression and inheritance in the case of two parents.</title>
<journal>Proceedings of the Royal Society of London,</journal>
<pages>58--347</pages>
<marker></marker>
<rawString>Karl Pearson. 1895. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Resnick</author>
<author>Neophytos Iacovou</author>
<author>Mitesh Suchak</author>
<author>Peter Bergstrom</author>
<author>John Riedl</author>
</authors>
<title>Grouplens: an open architecture for collaborative filtering of netnews.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 ACM conference on Computer supported cooperative work,</booktitle>
<pages>175--186</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6383" citStr="Resnick et al., 1994" startWordPosition="1006" endWordPosition="1009">rences. User-user CF relies on community preferences. The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating. Similar users are those that agree with the current user on previously-rated items. In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie. k similar users are then identified by measuring the similarity between the users’ vectors. Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011). An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users. In comparison to cosine, here vector entries are normalized with respect to the user’s average rating. In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4. Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine. 2.2 Customiza</context>
</contexts>
<marker>Resnick, Iacovou, Suchak, Bergstrom, Riedl, 1994</marker>
<rawString>Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. 1994. Grouplens: an open architecture for collaborative filtering of netnews. In Proceedings of the 1994 ACM conference on Computer supported cooperative work, pages 175–186. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Ricci</author>
<author>Lior Rokach</author>
<author>Bracha Shapira</author>
</authors>
<title>Introduction to recommender systems handbook.</title>
<date>2011</date>
<contexts>
<context position="6425" citStr="Ricci et al., 2011" startWordPosition="1014" endWordPosition="1017">eferences. The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating. Similar users are those that agree with the current user on previously-rated items. In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie. k similar users are then identified by measuring the similarity between the users’ vectors. Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011). An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users. In comparison to cosine, here vector entries are normalized with respect to the user’s average rating. In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4. Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine. 2.2 Customization, personalization and adaptation in MT</context>
</contexts>
<marker>Ricci, Rokach, Shapira, 2011</marker>
<rawString>Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Introduction to recommender systems handbook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badrul Sarwar</author>
<author>George Karypis</author>
<author>Joseph Konstan</author>
<author>John Riedl</author>
</authors>
<title>Item-based collaborative filtering recommendation algorithms.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>285--295</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6404" citStr="Sarwar et al., 2001" startWordPosition="1010" endWordPosition="1013">elies on community preferences. The idea is to recommend to the user items that are liked by users similar to her, as manifested, for example, by high rating. Similar users are those that agree with the current user on previously-rated items. In k-nearest-neighbors CF, a user is typically represented by a vector of her preferences, where each entry of the vector is, e.g., a rating of a movie. k similar users are then identified by measuring the similarity between the users’ vectors. Cosine similarity is a popular function for that purpose, and we also use it in our work (Resnick et al., 1994; Sarwar et al., 2001; Ricci et al., 2011). An alternative to cosine, Pearson’s correlation coefficient (Pearson, 1895), allows addressing different rating patterns across users. In comparison to cosine, here vector entries are normalized with respect to the user’s average rating. In our case, such normalization is not very meaningful since the entries of the users vectors represent comparisons rather than absolute ratings, as will be made clear in Section 4. Nevertheless, we have experimented with Pearson correlation as well, and found no advantage in using it instead of cosine. 2.2 Customization, personalization</context>
</contexts>
<marker>Sarwar, Karypis, Konstan, Riedl, 2001</marker>
<rawString>Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pages 285–295. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Marco Turchi</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In 13th Conference of the European Association for Machine Translation,</booktitle>
<pages>28--37</pages>
<contexts>
<context position="4367" citStr="Specia et al., 2009" startWordPosition="689" endWordPosition="692">considered: (i) Personalized translation of texts written by a specific user, and (ii) PMT to provide better translations for a specific reader. In this work we address the second task, aiming to identify translations each user is more likely to prefer.1 Specifically, we consider a setting where at least two MT systems are available, and the goal is to predict which of the translation systems the user would choose, assuming we have no knowledge about her preference between them. Benchmarking the systems in advance with respect to a reference set, or estimating the quality of the translations (Specia et al., 2009) are viable alternatives for translation selection; these, however, are not personalized to the target user. Instead, we employ a user-user Collaborative Filtering approach, common in Recommender Systems, which we map to the TP prediction task. We assess this approach using a collection of user rankings of MT systems from a shared translation task 1In (Mirkin et al., 2015) we investigate the first task, assessing whether the author’s demographic and personality traits are preserved over machine translation. 2019 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Proces</context>
</contexts>
<marker>Specia, Turchi, Cancedda, Dymetman, Cristianini, 2009</marker>
<rawString>Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In 13th Conference of the European Association for Machine Translation, pages 28–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirco Speretta</author>
<author>Susan Gauch</author>
</authors>
<title>Personalized search based on user search histories. In Web Intelligence,</title>
<date>2005</date>
<booktitle>Proceedings. The 2005 IEEE/WIC/ACM International Conference on,</booktitle>
<pages>622--628</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1512" citStr="Speretta and Gauch, 2005" startWordPosition="219" endWordPosition="222">s a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user. 1 Introduction Technologies are increasingly personalized, accommodating their behavior for each user. Such personalization is done through user modeling where the goal is to “get to know” the user. To that end, personalization is based on users’ attributes, such as demographics (gender, age etc.), personalities, and preferences. For example, in Information Retrieval, results are customized according to the user’s information and search history (Speretta and Gauch, 2005), performance of Automatic Speech Recognition substantially improves when adapted to a specific speaker (Neumeyer et al., 1995), and Targeted Advertising makes use of the user’s location and prior purchases (K¨olmel and Alexakis, 2002). Personalization in machine translation has a somewhat different nature. Providers of MT tools and services offer means to “customize” or “personalize” the translation engine for each client, mostly through domain adaptation techniques, and a great deal of effort is made to make the human-involved translation process more efficient (see Section 2.2). Most of the</context>
</contexts>
<marker>Speretta, Gauch, 2005</marker>
<rawString>Mirco Speretta and Susan Gauch. 2005. Personalized search based on user search histories. In Web Intelligence, 2005. Proceedings. The 2005 IEEE/WIC/ACM International Conference on, pages 622–628. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Coping with the subjectivity of human judgements in MT quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>240--251</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9370" citStr="Turchi et al. (2013)" startWordPosition="1482" endWordPosition="1485"> task that is not well defined, or when there is no obvious “truth”. Such is the case with the evaluation of translation quality – it is not always straightforward to tell whether one translation is better than another. A single sentence can be translated in multiple correct ways. The decision becomes even harder when the translations are automatically produced and are imperfect: Is one error worse than another? The answer is in the eye of the beholder. MT papers regularly report rather low Kappa levels, even when measured on simpler tasks, such as short segments (Mach´aˇcek and Bojar, 2015). Turchi et al. (2013) refer to the issue of “subjectivity” of human annotators. They address the task of binary classification of “good” vs. “bad” translations, and show that relying on human annotation for training a binary quality estimator is less effective than using automatically-generated labels. This subjectivity is exactly what we are after. We treat it as a preference, trying to identify the systems or specific translations which the user subjectively prefers. Kichhoff et al. (2012) analyze user preferences with respect to MT errors. They show that some types, e.g. word order errors, are the most dis-pref</context>
</contexts>
<marker>Turchi, Negri, Federico, 2013</marker>
<rawString>Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT), pages 240–251, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wilcoxon</author>
</authors>
<title>Individual Comparisons by Ranking Methods.</title>
<date>1945</date>
<journal>Biometrics Bulletin,</journal>
<volume>1</volume>
<issue>6</issue>
<contexts>
<context position="19366" citStr="Wilcoxon, 1945" startWordPosition="3204" endWordPosition="3205">preferences of the other users by their similarity to the target user. 5.3 Results Table 1 shows the results of an experiment comparing the performance of the various methods in terms of prediction accuracy. Figure 2 shows the micro-average scores, when giving each of the 97,412 test points an equal weight in the average. CTP outperforms all others for 9 out of 10 language pairs, and in the overall microaveraged results. The difference between CTP and each of the other metrics was found statistically significance with p &lt; 5 · 10−6 at worse, as measured with a paired Wilcoxon signed rank test (Wilcoxon, 1945) on the predictions of the two methods. The significance test captures in this case the fact that the methods disagreed in many more cases than is visible by the score difference. Our method was found superior to all others also when computing macro-average, taking the average of the scores of each language pair, as well as when the ties are included in the computation of pu. The parameters with which the above results were obtained are found within the method’s description in Section 4. Yet, in our experiments, CTP turned out to be rather insensitive to their values. In this experiment we use</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics Bulletin, 1(6):80–83, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
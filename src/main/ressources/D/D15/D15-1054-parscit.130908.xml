<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001298">
<note confidence="0.8854002">
Joint Embedding of Query and Ad by Leveraging Implicit Feedback
Sungjin Lee and Yifan Hu
Yahoo Labs
229 West 43rd Street, New York, NY 10036, USA
{junion, yifanhu}@yahoo-inc.com
</note>
<sectionHeader confidence="0.979149" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977620689655">
Sponsored search is at the center of a multibil-
lion dollar market established by search tech-
nology. Accurate ad click prediction is a key
component for this market to function since
the pricing mechanism heavily relies on the
estimation of click probabilities. Lexical fea-
tures derived from the text of both the query
and ads play a significant role, complementing
features based on historical click information.
The purpose of this paper is to explore the use
of word embedding techniques to generate ef-
fective text features that can capture not only
lexical similarity between query and ads but
also the latent user intents. We identify several
potential weaknesses of the plain application
of conventional word embedding methodolo-
gies for ad click prediction. These observa-
tions motivated us to propose a set of novel
joint word embedding methods by leveraging
implicit click feedback. We verify the effec-
tiveness of these new word embedding models
by adding features derived from the new mod-
els to the click prediction system of a com-
mercial search engine. Our evaluation results
clearly demonstrate the effectiveness of the
proposed methods. To the best of our knowl-
edge this work is the first successful applica-
tion of word embedding techniques for the task
of click prediction in sponsored search.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843428571429">
Sponsored search is a multibillion dollar market
(Easley and Kleinberg, 2010) that makes most search
engine revenue and is one of the most successful ways
for advertisers to reach their intended audiences. When
search engines deliver results to a user, sponsored ad-
vertisement impressions (ads) are shown alongside the
organic search results (Figure 1). Typically the adver-
tiser pays the search engine based on the pay-per-click
model. In this model the advertiser pays only if the im-
pression that accompanies the search results is clicked.
The price is usually set by a generalized second-price
(GSP) auction (Edelman et al., 2005) that encourages
advertisers to bid truthfully. An advertiser wins if the
expected revenue for this advertiser, which is the bid
</bodyText>
<figureCaption confidence="0.9920655">
Figure 1: Sponsored ads when “pizza” was searched at
Yahoo! (www.yahoo.com).
</figureCaption>
<bodyText confidence="0.999966866666667">
price times the expected click probability (also know
as click through rate, or CTR), is ranked the highest.
The price the advertiser pays, known as cost-per-click
(CPC), is the bid price for the second ranked advertiser
times the ratio of the expected CTR between the sec-
ond and first ranked advertisers. From this discussion
it should be clear that CTR plays a key role in deciding
both the ranking and the pricing of the ads. Therefore
it is very important to predict CTR accurately.
The state of the art search engine typically uses a
machine learning model to predict CTR by exploiting
various features that have been found useful in prac-
tice. These include historical click performance fea-
tures such as historical click probability for the query,
the ad, the user, and a combination of these; contextual
features such as temporal and geographical informa-
tion; and text-based features such as query keywords or
ad title and description. Among these, historical click
performance features often have the most predictive
power for queries, ads and users that have registered
many impressions. For queries, ads and users that have
not registered many impressions, however, historical
CTR may have too high a variance to be useful. Hillard
et al. (2011) observed that the number of impressions
and clicks recorded on query-ad pairs have a very long
tail: only 61% of queries has greater than three clicks.
They also reported a drastic drop in the accuracy of the
click prediction model when fewer historical observa-
tions are available. Furthermore, fine-grained historical
CTR information takes a huge amount of space, which
</bodyText>
<page confidence="0.953141">
482
</page>
<note confidence="0.9852435">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 482–491,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999940911392406">
makes it costly to maintain. On the other hand, text
features are always readily available, and thus are par-
ticularly useful for those cases for which there is insuf-
ficient historical information.
Multiple researchers, for example (Richardson,
2007; Cheng and Cant´u-Paz, 2010), reported the us-
age of text features including simple lexical similarity
scores between the query and ads, word or phrase over-
laps and the number of overlapping words and charac-
ters. Such features rely on the assumption that query-ad
overlap is correlated with perceived relevance. While
this is true to a certain extent, the use of simple lexical
similarity cannot capture semantic information such as
synonyms, entities of the same type and strong rela-
tionships between entities (e.g. CEO-company, brand-
model, part-of). Recently a host of studies on word
embedding have been conducted; all map words into
a vector space such that semantically relevant words
are placed near each other in the space (Mikolov et al.,
2013a; Pennington et al., 2014; Baroni et al., 2014).
The use of continuous word vectors has been shown
to be helpful for a wide range of NLP tasks by better
capturing both syntactic and semantic information than
simple lexical features (Socher et al., 2012a).
No previous research on sponsored search has suc-
cessfully used word embeddings to generate text fea-
tures. In this paper, we explore the use of word em-
beddings for click prediction. However, it is clear that
conventional word embeddings (which solely rely on
word co-occurrence in a context window) can only of-
fer limited discriminative power because queries and
ad text are typically very short. In addition, conven-
tional word embeddings cannot capture user intents,
preferences and desires. Wang et al. (2013) showed
that specific frequently occurring lexical patterns, e.g.,
x% off, guaranteed return in x days and official site, are
effective in triggering users desires, and thus lead to
significant differences in CTR. Conventional word em-
beddings cannot capture these phenomena since they
do not incorporate the implicit feedback users provide
through clicks and non-clicks. These observations nat-
urally lead us to leverage click feedback to infuse users’
intentions and desires into the vector space.
The simplest way to harness click feedback is to
train conventional word embedding models on a cor-
pus that only includes clicked impressions, where each
“sentence” is constructed by mixing the query and ad
text. Having trained a word embedding model, we sim-
ply take the average of word vectors of the query and
ads respectively to obtain sentence (or paragraph) vec-
tors, which in turn are used to compute the similarity
scores between the query and ads. Our experiments
show that this method does improve click prediction
performance. However, this method has several po-
tential weaknesses. First, the use of only clicked im-
pressions ignores the large amount of negative signals
contained in the non-clicked ad impressions. Second,
the use of indirect signals (word co-occurrences) can
be noisy or even harmful to our ultimate goal (accurate
click prediction) when it is combined with direct sig-
nals (impressions with click feedback). Third, without
explicit consideration about the averaging step in the
training process of word embedding models, a simple
averaging scheme across word vectors may be a subop-
timal. We therefore propose several joint word embed-
ding models; all of these aim to put query vectors close
to relevant ad vectors by explicitly utilizing both posi-
tive and negative click feedback. We evaluate all these
models against a large sponsored search data set from
a commercial search engine, and demonstrate that our
proposed models significantly improve click prediction
performance.
The rest of this paper is organized as follows. In Sec-
tion 2 we present a brief summary of related work. In
Section 3 we give some background information on ad
click prediction in sponsored search. In Section 4 we
describe our methods. In Section 5 we discuss our ex-
periments. We finish with some conclusions and future
directions in Section 6.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.985376171428571">
Text features for predicting click probability There
have been many studies on the use of text features
for click prediction. For example, Dembczynski et
al. (2008) used a decision rule-based approach involv-
ing such lexical features as the number of words in
ad title and description, the number of segments and
length of the ad URL, and individual words and terms
in ad title and description. Cheng et al. (2010) used
a logistic regression model that used both historical
click performance features and simple lexical features
such as word or phrase overlap between query and ad
title and description. Trofimov et al. (2012) used a
variant of boosted decision trees with similar features.
Richardson et al. (2007) specifically considered new
ads (which lack historical click prediction data) and
proposed to use the CTR for ad terms, the frequency
of certain unigrams (e.g., dollar signs) and general En-
glish usage patterns, and simple lexical distance be-
tween the query and ads. In all this previous work,
text features consisted only of surface-level text fea-
tures. To the best of our knowledge, there is no previ-
ous work adopting semantic-level text features for the
purpose of click prediction, in particular word embed-
dings to measure query-ad relevance. In a similar vein
of research, Grbovic et al. (2015) adopted word embed-
dings to the task of query rewriting for a better match
between queries and keywords that advertisers entered
into an auction. Using the embeddings, semantically
similar queries are mapped into vectors close in the em-
bedding space, which allows expansion of a query via
K-nearest neighbor search.
Word embeddings for language processing Recently
many NLP systems have obtained improved perfor-
mance with less human engineering by adopting dis-
tributed word representations (Socher et al., 2012a).
</bodyText>
<page confidence="0.998901">
483
</page>
<bodyText confidence="0.999693888888889">
In particular, neural word embedding techniques are
now known to be effective in capturing syntactic and
semantic relationships, and more computationally ef-
ficient than many other competitors (Mikolov et al.,
2013a; Pennington et al., 2014; Baroni et al., 2014). On
top of word embeddings, a new stream of research on
sentence and paragraph embeddings has also emerged
to tackle higher level tasks such as sentiment analy-
sis, machine translation and semantic relatedness tasks.
Recursive Neural Networks (RNNs) have been used for
syntactic parsing (Socher et al., 2012b; Socher et al.,
2013; Socher et al., 2014; Irsoy and Cardie, 2014).
Long Short-Term Memory (LSTM) networks have been
applied to machine translation (Bahdanau et al., 2014;
Sutskever et al., 2014) and semantic processing (Tai
et al., 2015). Interestingly Convolutional Neural Net-
works (CNNs), widely used for image processing, have
recently emerged as a strong class of models for NLP
tasks (Kim, 2014; Blunsom et al., 2014). As apposed
to the models above, PhraseVector (Le and Mikolov,
2014) takes a less structured but unsupervised approach
by treating a piece of text as a token and performing
word embedding-like training with an unlimited con-
text window. None of this previous work exactly fits
the click prediction task. Since queries and ads are
much less structured than usual text, it is not attractive
to use models with complex structures, such as RNNs,
at the cost of speed and scalability. PhraseVector is less
structured but it does not support compositionality, suf-
fering from sparseness or requiring to train new vectors
for each unseen query and ad. Interestingly, as reported
in (Tai et al., 2015), a simple averaging scheme (mean
vector) was found to be very competitive to more com-
plex models for high level semantic tasks despite its
simplicity. These observations lead us to one of our
models that aims to improve the mean vector method
by directly optimizing mean vectors instead of word
vectors.
Joint embedding to bridge multiple views. Multi-
ple studies have explored the task of bringing multiple
views into the same vector space. For example, there
is now a large body of research on joint modeling of
text and image information (Frome et al., 2013; Karpa-
thy and Fei-Fei, 2014; Socher et al., 2014). The mul-
timodal embedding space helps find appropriate align-
ments between image regions and corresponding pieces
of text description. Joint embedding has also been ap-
plied to question answering (Wang et al., 2014) and se-
mantic understanding (Yang et al., 2014). In contrast
to the tasks above, there is no natural component-wise
correspondence between queries and ads; instead the
relationship is more implicit and pragmatic. Because of
this, our methods rely on global rather than component-
level signals for model training.
</bodyText>
<sectionHeader confidence="0.994757" genericHeader="method">
3 Baseline Click Prediction Model
</sectionHeader>
<bodyText confidence="0.99995875">
We first present a high-level description of sponsored
search. The process consists of several stages. First,
given a user query, a list of candidate ads are retrieved,
either by exactly matching query terms to the bid terms
of the advertiser, or by first using query term expansion
to obtain a longer list of matched ads. Some candi-
date ads may be filtered out based on metrics such as
ad quality. Then, a click prediction model scores the
candidate ads to estimate how likely it is that each will
be clicked. This click probability serves a crucial role
both in the user experience and in the revenue for the
search engine. The ads with the highest click probabil-
ities are placed in the search results page. The price-
per-click for each ad shown is determined based on the
click probabilities and the GSP auction.
Our baseline click prediction model is formulated as
a supervised learning problem. Specifically we use Lo-
gistic Regression (LR) since LR is well suited for prob-
ability estimation. Given a variety of features, the prob-
ability of a click is expressed as:
</bodyText>
<equation confidence="0.998447">
1
p(c|q, a, u) = 1 + exp{ Ei wifi(q, a, u)}, (1)
</equation>
<bodyText confidence="0.999807166666667">
where c E {1, 0} is the label (1: click or 0: non-click),
fi(q, a, u) is the ith feature derived for query-ad-user
triple (q, a, u) and wi is the associated weight. The
model is trained using a stochastic gradient descent al-
gorithm on a per impression basis with l1 regularization
to avoid overfitting.
An accurate LR model relies greatly on the effective-
ness of its features. Our baseline model is furnished
with a rich set of features that are typically used in
commercial search engines. The first feature type is
based on the historical CTR of user, query, ad triples
(if there is enough historical information on this). We
use two groups of features of this type: COEC based
features and user factor features. The second feature
type is based on query and ad text.
Due to the significant decrease of CTR depending on
the ad position, it has become common practice to use
position-normalized CTR (a.k.a. Clicks Over Expected
</bodyText>
<equation confidence="0.9669342">
Clicks):
E
pcp
COEC = E , (2)
p ip ∗ CTRp
</equation>
<bodyText confidence="0.999992933333333">
where the numerator is the total number of clicks re-
ceived by the configuration of interest; the denomi-
nator is the expected clicks (ECs) that an average ad
would receive after ip times impressions at position
p, and CTRp is the average CTR at position p, cal-
culated over all queries, ads and users. We use user-
independent features derived from COEC statistics for
specific query-ad pairs. However, many impressions
are needed for these statistics to be reliable and there-
fore data for specific query-ad pairs can be sparse and
noisy (only around 70% of queries and about 50% of
query-URL, query-bid term pairs have historical CTR).
To alleviate this problem, additional COEC statistics
over aggregations of queries or ads are also used. The
exact description of these aggregations is beyond the
</bodyText>
<page confidence="0.995007">
484
</page>
<bodyText confidence="0.999952931818182">
scope of this paper, but briefly we exploit the catego-
rization of the ads in ad groups, campaigns, and ac-
counts defined by advertisers.
Since it is well known that personalization features
are crucial to obtain accurate click prediction mod-
els (Cheng and Cant´u-Paz, 2010), we also use features
that measure user factors relating to CTR. The user
click feedback features capture the inclination of indi-
vidual users to click on ads in general. The user-query
click feedback features indicate the propensity of users
to click for certain queries or groups of queries. Fi-
nally user-ad features dictate the user preferences on
certain ads or advertisers. However the data sparseness
problem becomes even more serious when it comes to
user-specific features (we use a threshold of 100 for
statistical confidence). For example, only about 5%
of user-URL pairs and 1% of user, query, URL triples
have historical CTR information. Therefore a set of
segment-level features can be extracted as back-off fea-
tures where users, queries and ads are clustered into
groups, and group level historical CTRs are collected.
Our second major feature type involves the lexical
similarity between query and ad text. These text fea-
tures assume that users are more likely to click on ads
that seem to be relevant to the query, and that per-
ceived relevance is correlated with the degree of query-
ad overlap. These features include the number of over-
lapping words and characters in query-ad URL, query-
ad title, and query-ad description, and the number of
words and characters in the query. The discrimination
power of simple lexical features is relatively limited be-
cause query and ad text are typically very short.
Finally, we use other contextual features that are
helpful in predicting the click probabilities.; for exam-
ple, time of day, day of week, and geographic infor-
mation. To model interactions among features, some
features are selected by domain knowledge to be con-
joined. All together, our baseline model utilize a com-
prehensive set of historical CTR, lexical, and contex-
tual features (over a hundred features in total). The fact
that this baseline model is highly optimized makes the
subsequent performance improvement from our pro-
posed algorithm meaningful. This baseline model is
used in production in part of a major search platform.
</bodyText>
<sectionHeader confidence="0.881965" genericHeader="method">
4 Joint Embedding for Click Prediction
</sectionHeader>
<bodyText confidence="0.9994382">
We now describe several methods that jointly embed
words in both the query and the ads into the same vec-
tor space. In our experiments, we incorporate these
methods as features in our click prediction model. We
start by defining the notation used in this section. A
sponsored search dataset D is a set of tuples for each
ad impression (q, t, d, y) where q - {qj} is a multi-
word query string, t - {tk}, d - {dl} are multiword
ad title and description strings, and y is a binary in-
dicator for whether the ad is clicked. We have two
choices in defining the vocabulary V from which words
are drawn: we can use a unified vocabulary for both
query and ads or define a separate vocabulary for each
– V - Vq ⊕ Va. In our initial experiments the uni-
fied vocabulary constantly yielded better performances,
thus we always use the unified vocabulary here. We use
bold letters qj, tk, dl to denote the corresponding em-
bedding representations of {qj, tk, dl}. Finally we use
W to represent the vocabulary matrix; in W each col-
umn is a word vector.
</bodyText>
<subsectionHeader confidence="0.997945">
4.1 Exploiting word2vec embedding
</subsectionHeader>
<bodyText confidence="0.999966">
Typically word embeddings are learned from a given
text corpus through implicit supervision of predicting
the current word given a window of its surrounding text
or predicting each word in the window of the current
word. The former approach is known as continuous
bag-of-words (CBOW) and the latter Skip-gram. For
simplicity’s sake we use negative-sampling for training
word embedding models (Mikolov et al., 2013b). More
formally we define the binary conditional probability
for a pair of words (v, w): 1
</bodyText>
<equation confidence="0.9973405">
1
p(v, w) = 1 + exp(−vTw), (3)
</equation>
<bodyText confidence="0.999254">
The CBOW algorithm learns word embeddings by
minimizing the following logloss of each impression i
with regard to W:
</bodyText>
<equation confidence="0.996822333333333">
CBi(W) = − logp(wi, µC)
11 − log (1 − p(v, µC)) , (4)
vEN(w;)
</equation>
<bodyText confidence="0.9995475">
where the context C of a word wi comes from a win-
dow of size k around the word in a sentence of n words
w1,...,wn: C = wi−k,...,wi−1,wi+1,...,wi+k. µC is
the averaged context vector of wi; µ 1e = IC |EvEC v.
N(wi) is the set of negative examples which is drawn
according to the unigram distribution of the corpus
raised to the 3/4th power. Similarly to (Mikolov et al.,
2013b), we adopt a dynamic window size – for each
word the actual window size is sampled uniformly from
1,...,k.
Similarly the Skip-gram algorithm minimizes the
logloss of each impression i with regard to W:
</bodyText>
<equation confidence="0.99831925">
SKi(W) = − 11 log p(wi, v)
11 −
vEC log (1 − p(wi, v)) . (5)
vEN(w;)
</equation>
<bodyText confidence="0.9996926">
In our first word embedding model that incorporates
click feedback, we construct a corpus by taking only
clicked impressions from D and then mixing (q, t, d)
of each impression into a sentence. Then we simply
train CBOW and Skip-gram models on the corpus.
</bodyText>
<footnote confidence="0.985516">
1We use only a single vector for a word unlike (Mikolov
et al., 2013b) where two vectors (“input” and “output”) for a
word are used. This halves the required space to store vectors
without performance loss.
</footnote>
<page confidence="0.993981">
485
</page>
<subsectionHeader confidence="0.872007">
4.2 Joint word embedding using click feedback
</subsectionHeader>
<bodyText confidence="0.999929461538462">
Although the CBOW and Skip-gram models trained on
a specially constructed corpus can capture signals from
both click feedback and word co-occurrence, they have
a couple of drawbacks. First, by ingesting only clicked
impressions we “waste” the large amount of negative
signals contained in the non-clicked impressions. Sec-
ond, the incorporation of indirect signals such as word
co-occurrences can be rather harmful for achieving ac-
curate click prediction as these are very noisy com-
pared to direct signals such as click feedback.
For our second word embedding model that incorpo-
rates click feedback, we define a joint word embedding
model that minimizes the following weighted logloss:
</bodyText>
<equation confidence="0.9859625">
n o
JWi(W) = η(yi) l(yi,qi, ti) + l(yi,qi, di) , (6)
</equation>
<bodyText confidence="0.998734">
where the component loss function l(·, ·,·) is defined
as follows:
</bodyText>
<equation confidence="0.910644111111111">
− y log p(ak, bl)
− (1 − y) log (1 − p(ak, bl)) .
(7)
Here η(yi) is a function that returns a small weight η
only to negative examples:
(
η, if yi = 0,
η(yi) = (8)
1, otherwise.
</equation>
<bodyText confidence="0.99952375">
The fact that the user did not click on an ad does
not necessarily mean that the ad is not what the user
wanted; it often means that the ad is just less favored
than the clicked ads (Rendle et al., 2009). Since the
scope of this work is restricted to estimating click prob-
ability on a per-impression basis, we adopt a weighting
scheme rather than optimizing rank-based loss over a
set of related impressions.
</bodyText>
<subsectionHeader confidence="0.993942">
4.3 Joint mean vector optimization
</subsectionHeader>
<bodyText confidence="0.999865857142857">
The joint word embedding models defined in the pre-
vious sections do not define how to aggregate a vari-
able length sequence of word vectors into a sentence
(or paragraph) vector to facilitate the computation of
sentence-level similarity scores. One approach to this
aggregation task is mean vector: simply average the
word-level embeddings across the sentence or para-
graph. As noted by (Tai et al., 2015), this approach
is a strong competitor to more complex models such
as RNNs or LSTMs despite its simple composition
method. However, this method may generate subop-
timal sentence vectors. With weight logloss, we aim
to optimize sentence vectors instead of individual word
vectors:
</bodyText>
<equation confidence="0.95848">
n o
JMi(W) = η(yi) ¯l(yi,qi,ti) + ¯l(yi,qi,di) , (9)
</equation>
<bodyText confidence="0.999178">
where the component loss function l(·, ·,·) is defined
as follows:
</bodyText>
<equation confidence="0.999269">
(10)
− (1 − y) log (1 − p(µa, µb)) ,
</equation>
<bodyText confidence="0.996918">
where µs returns the average vector for the multiword
string s, i.e. µs = |3 |Pks |sk.
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999250523809524">
Data The data used in our experiments were collected
from a random bucket of the Yahoo! sponsored search
traffic logs for a period of 4 weeks in October 2014.
In the data there are approximately 65 million unique
users, 150 million unique queries and 12 million unique
ads. There are approximately 985 million ad impres-
sion events in total. We split the data into 3 partitions
with respect to time: the first 14 days’ data are used
for training word embedding models, the next 7 days’
data for training click prediction models, and the last 7
days’ data for testing. Table 1 presents more detailed
statistics for the data.
Models We are interested in evaluating the usefulness
of different word embedding models as features for
click prediction, and already have a very good baseline
system for this task. Consequently, in all experiments
below, we used the same personalized historical CTRs,
contextual and lexical features as the baseline system
described in Section 3. We tested models with the fol-
lowing additional features derived from word embed-
dings:
</bodyText>
<listItem confidence="0.9997638">
1. cosine similarity between the mean vectors of the
query and ad title
2. cosine similarity between the mean vectors of the
query and ad description
3. sum of 1 and 2
4. sigmoid function value for the dot product of the
mean vectors of the query and ad title
5. sigmoid function value for the dot product of the
query and ad description
6. sum of 4 and 5
</listItem>
<bodyText confidence="0.887898">
All these continuous features are quantized into 50
bins. We compared five different word embedding al-
gorithms:
</bodyText>
<listItem confidence="0.999073142857143">
1. Skip-gram trained on Wikipedia (SK-WIKI)
2. Skip-gram trained on clicked impressions (SK-
CI), see Section 4.1
3. CBOW trained on clicked impressions (CB-CI),
see Section 4.1
4. Joint individual word vector embedding (JIWV),
see Section 4.2
</listItem>
<equation confidence="0.998656857142857">
|a|
X
k
l(y, a, b) =
X |b|
l
l(y, a, b) = − y log p(µa, µb)
</equation>
<page confidence="0.998172">
486
</page>
<table confidence="0.999691071428571">
Embedding Train Test Total
Number of impressions 489M 252M 244M 985M
Number of unique queries 82M 46M 45M 150M
Number of unique ads 8M 6M 6M 12M
Number of unique users 40M 25M 25M 65M
Number of unique words in query 6757K 6039K 5774K 16028K
Number of unique words in ad title 3371K 2586K 2557K 4598K
Number of unique words in ad description 1993K 1583K 1589K 2628K
Number of words in query 1600M 830M 800M 3230M
Number of words in ad title 3260M 1690M 1635M 6585M
Number of words in ad description 4311M 2206M 2131M 8650M
Average number of words in query per impression 3.275 3.284 3.289 3.281
Average number of words in ad title per impression 6.667 6.708 6.706 6.687
Average number of words in ad description per impression 8.818 8.759 8.741 8.784
</table>
<tableCaption confidence="0.999374">
Table 1: Data description
</tableCaption>
<bodyText confidence="0.988628108108108">
5. Joint mean vector embedding (JMV), see Sec-
tion 4.3
We used the skip-gram mode of word2vec 2 with a win-
dow size of 5 and negative sampling to train the SK-
WIKI model. For all other models we used in-house
implementation which employs AdagradRDA (Duchi
et al., 2011) to minimize the loss functions introduced
in Section 4. The dimension of a word vector is set
to 100 for all algorithms3. We removed a set of pre-
fixed stop-words and all words occurring fewer than
100 times; the resulting vocabulary comprised 126K
unique words. To process our web-scale data, we im-
plemented a multithread program where each thread
randomly traverses over a partition of the data D to
compute gradients and update the matrix W stored in a
shared memory. For computational efficiency, the hog-
wild lock-free approach (Recht et al., 2011) is used.
We set η in Eq. 8 and Eq. 9 to 0.2 through grid search
based on two-fold cross-validation. This small value
indeed verifies the idea of weak negative feedback for
unclicked impressions.
In order to circumvent the severe influence of ad po-
sition on the click prediction model, only the impres-
sions that are placed at the top position were used for
training the click prediction model. Note that CTR
at other positions can be derived from that of the ad
at the top position through scaling. Dembczynski et
al. (2008) showed that CTR can be decomposed as a
product of the probability of an ad getting clicked given
its being seen and the probability of an ad being seen at
a particular position.
Results We ordered query-ad pairs by the predicted
score to compute AucLoss (i.e. 1 - AUC where AUC
is the area under the Receiver Operating Characteris-
tic (ROC) curve). The ROC AUC is known to have a
correlation with the quality of ranking by the predicted
score (Fawcett, 2006); thus is one of the most important
</bodyText>
<footnote confidence="0.995337">
2Available at https://code.google.com/p/word2vec/
3Higher vector dimensions such as 200 were also tried but
did not give a significant improvement.
</footnote>
<table confidence="0.968700428571428">
metrics for click prediction (McMahan et al., 2013).
AucLoss Reduction (%)
Baseline + SK-WIKI 0.530
Baseline + SK-CI 0.595
Baseline + CB-CI 0.656
Baseline + JIWV 2.276
Baseline + JMV 4.114
</table>
<tableCaption confidence="0.7843775">
Table 2: Comparative evaluation results in AucLoss re-
duction from the baseline system
</tableCaption>
<bodyText confidence="0.999965620689655">
Our experimental results (Table 2) show that the use
of features derived from our proposed word embed-
ding models significantly reduce AucLoss by up to
4.1%. For commercial search engines which have a
very strong baseline AucLoss, a reduction of 1% can be
considered large (McMahan et al., 2013). Moreover, as
expected, the more issues (as identified in Section 4) an
algorithm addresses, the better performance it achieves.
From the comparison between the SK-WIKI model and
the rest, we can recognize the importance of word em-
bedding models specialized to domain text and super-
vision signals. Also the difference between the CB-CI
(SK-CI) model and the JIWV model indicates the nois-
iness of indirect signals such as word co-occurrence
compared to direct signals like click feedback. Finally
the gap between the JIWV and JWV models highlights
the significance of considering compositionality in the
word embedding training process.
Eyeballing the most similar words to several queries
in the vector space is often helpful for getting some
sense about how different methods influence the result-
ing vector spaces. Table 3 lists the top 20 most similar
words to the query “metal watch.” The top words for
the SK-WIKI model are not semantically interesting in
terms of capturing the intent or desires. This clearly
shows the limitation of word embedding methods that
only rely on indirect signals (i.e. word co-occurrences)
from a generic text corpus (e.g. Wikipedia) for spon-
sored search click prediction. Noticeably the methods
</bodyText>
<page confidence="0.99657">
487
</page>
<table confidence="0.999901666666667">
SK-WIKI CB-CI JIWV JMV
watch (0.733) metal (0.718) wwhl (0.711) tacticalwatch.com (0.701)
grind (0.687) previews (0.652) cbs (0.704) watchrepairsusa.com (0.695)
grease (0.682) yidio.com (0.648) station (0.668) omegas (0.689)
kites (0.676) episodes (0.633) putlocker.com (0.662) watchco.com (0.682)
hammer (0.675) steel (0.626) iwatch (0.659) wach (0.670)
spinning (0.672) whatch (0.615) kidizoom (0.658) station (0.656)
flashing (0.671) bobcometal.com (0.586) freesports360.com (0.658) shockwarehouse.com (0.628)
trash (0.670) ridiculousness (0.582) tedtalks (0.655) freesports360.com (0.618)
flame (0.669) www.abc.com (0.574) 11/10c (0.650) akribos (0.616)
flaming (0.665) a&amp;e (0.573) movie2k (0.640) 18mm (0.616)
cigar (0.664) utube (0.570) nfl.com/now (0.637) narutoget.com (0.614)
home-made (0.663) instantly (0.565) criminalsgonewilddvd (0.631) watchstation.com (0.611)
glow (0.662) khoobsurat (0.562) espnnfllive.com (0.631) authenticwatches.com (0.610)
bouncing (0.662) outnumbered (0.561) foxsports1 (0.623) interrupted (0.603)
filler (0.662) itv (0.559) viooz (0.623) criminalsgonewilddvd (0.601)
smoke (0.660) premiere (0.556) bubble (0.623) whatch (0.600)
shoot (0.658) films (0.555) wewood (0.621) $109.99 (0.597)
scoop (0.653) stainless (0.554) westclox (0.617) tirebuyer.com (0.596)
noises (0.652) fabricators (0.550) potlocker (0.613) skagen.com (0.588)
rocking (0.651) fabrication (0.550) nickelodeon (0.613) wewood (0.584)
</table>
<tableCaption confidence="0.998924">
Table 3: Top 20 most similar words to “metal watch”
</tableCaption>
<table confidence="0.32439">
SK-WIKI CB-CI JIWV JMV
</table>
<equation confidence="0.907191544303797">
costumes (0.762)
bride (0.723)
princesses (0.722)
serene (0.708)
highness (0.676)
bess (0.674)
princess]] (0.671)
attire (0.670)
princess (0.662)
dresses (0.662)
highness (0.658)
jewels (0.652)
wedding (0.651)
robes (0.648)
prince (0.644)
clothes (0.644)
dancing (0.644)
sophie (0.640)
consorts (0.639)
glamorous (0.639)
princess (0.833)
costumed (0.814)
customes (0.808)
costume (0.806)
costomes (0.806)
costimes (0.803)
m.buycostumes.com (0.800)
custumes (0.796)
officialprincesscostumes.com (0.792)
custume (0.789)
coustome (0.789)
cosyumes (0.787)
coustume (0.786)
coustums (0.786)
buycostumes.com (0.785)
codtume (0.784)
leia (0.784)
coustumes (0.784)
costums (0.783)
coatumes (0.782)
costumes (0.831)
wonder (0.784)
cleopatra (0.753)
new-costumes.com (0.752)
girls.simple (0.749)
werewolf (0.749)
yoshi(0.747)
leia (0.747)
merida (0.742)
$49.90 (0.735)
low-budget (0.727)
babies (0.727)
fembot (0.726)
costums (0.722)
$3.90 (0.721)
hermione (0.718)
supergirl (0.715)
toothless (0.713)
starlord (0.709)
new-costumes.com (0.815)
coustume (0.808)
$35.90 (0.789)
2-days (0.781)
$36.90 (0.776)
costume (0.766)
$28.90 (0.764)
princess (0.758)
cistumes (0.756)
spider-woman (0.756)
cotumes (0.748)
coneheads (0.747)
$23.90 (0.739)
$7.90 (0.739)
costomes (0.738)
$19.90 (0.737)
sweetiegames.com (0.782) namefully.com (0.800)
the-wristband-factory.com (0.750)
$17.90 (0.750)
sugarsmascotcostumes˙com (0.748)
</equation>
<tableCaption confidence="0.961336">
Table 4: Top 20 most similar words to “princess costumes”
</tableCaption>
<bodyText confidence="0.99968828125">
that incorporate click feedback find more words related
to products, services or websites instead of just con-
ceptuatlly related words. Given that real products or
services can be regarded as the best possible surrogates
to user intents and desires, this demonstrates the effec-
tiveness of our methods. This tendency gets stronger as
a method takes into account both positive and negative
click feedback.
Another very interesting observation comes from the
fact that none of the methods except JMV successfully
captures the composite meaning of “metal watch”; they
tend to either find related words separately for each
query word (e.g. “watch” is strongly associated to the
sense of watching something like movie or other types
of video) or find totally unrelated words (particularly
SK-WIKI). This demonstrates that it is crucial to ad-
dress compositionality in the very process of learning
word vectors.
Table 4 shows the top 20 most similar words to the
query “princess costumes.” In this example we can spot
another surprising result. The JMV model pushes a lot
of price related expressions to the top4. This may im-
ply that many parents search for lower cost costumes,
clearly showing a clear psychological desire in the fi-
nancial dimension. This observation confirms the find-
ings in (Wang et al., 2013) about the significant role of
certain ad expressions in triggering users’ psychologi-
cal desires. We also note that the CB-CI model returns
a lot of misspells for “costume(s)”, which would not
be possible with simple lexical features of the baseline
system. A close look at this example generally con-
firms the observations we made for the previous ex-
</bodyText>
<footnote confidence="0.9971955">
4We have not tried any normalization for numbers but it
might be worth doing given the important role they play.
</footnote>
<page confidence="0.987917">
488
</page>
<table confidence="0.999855619047619">
SK-WIKI CB-CI JIWV JMV
kids (0.724) game (0.629) kids (0.795) program (0.764)
pac-man (0.708) gams (0.615) kidsfootlocker.com (0.788) scorekeepers (0.757)
games (0.703) games (0.613) flight: (0.754) gamingchairs.hayneedle.com (0.754)
pinball (0.687) petrasplanet.com (0.601) edit: (0.744) teepees (0.752)
boy (0.680) for (0.590) raz (0.737) nn2 (0.750)
adventure (0.664) gamse (0.584) program (0.733) game (0.749)
roleplaying (0.664) ganes (0.574) abercrombiekids.com (0.729) girls (0.746)
quests (0.658) collectors (0.574) sumdog (0.721) cranium (0.746)
d&amp;d (0.658) awsome (0.551) take20 (0.702) ggg (0.738)
genie (0.655) kid (0.550) freegamesdownload.com (0.699) pac-man (0.732)
solitaire (0.654) game.com (0.537) program; (0.695) kidsfootlocker.com (0.730)
gamers (0.653) g2u.com (0.536) gromsocial.com (0.694) equestriagirls.hasbro.com (0.721)
games=== (0.651) bouncing (0.533) softschools (0.693) kids (0.718)
game; (0.649) for.kids (0.532) gapkids (0.691) mahjong (0.718)
consoles]] (0.645) catching (0.528) downnload (0.682) tvo (0.717)
in-game (0.645) gamesfreak (0.519) edheads(0.681) sumdog (0.701)
rpg (0.644) minecraft (0.516) ruum.com (0.674) cpm.wargaming.net (0.699)
wargames (0.644) firetruck (0.514) gamestop (0.672) osgood-schlatter (0.698)
game]] (0.643) games: (0.513) a&amp;f (0.668) gamestop (0.697)
fast-paced (0.641) todlers (0.512) pac-man (0.663) $4.01 (0.695)
</table>
<tableCaption confidence="0.999812">
Table 5: Top 20 most similar words to “game for kids”
</tableCaption>
<bodyText confidence="0.999325666666667">
ample. Finally Table 5 shows top the 20 most similar
words for the query “game for kids.” Once again we
found the same analysis holds for this case.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999852888888889">
In this paper we explored the use of word embedding
techniques to overcome the shortcomings of traditional
lexical features for ad click prediction in sponsored
search. We identified several potential weaknesses of
the plain application of conventional word embedding
methodologies: the lack of the right machinery to har-
ness both positive and negative click feedback, the lim-
ited utility of pure word co-occurrence signals, and no
consideration of vector composition in the word em-
bedding training process. We proposed a set of new
implicit feedback-based joint word embedding meth-
ods to address those issues. We evaluated the new word
embedding methods in the context of a very good base-
line click prediction system, on a large scale data set
collected from Yahoo! search engine logs. Our exper-
imental results clearly demonstrate the effectiveness of
the proposed methods. We also presented several ex-
amples for qualitative analysis to advance our under-
standing on how each algorithm really contributes to
the improved performance. To the best of our knowl-
edge this work is the first successful application of
word embedding techniques for the sponsored search
task.
There are multiple interesting research directions for
future work. One of these directions is to extend the
vocabulary by identifying significant phrases (as well
as words) before training word vectors. Hillard et
al. (2011) employed Conditional Random Fields to di-
vide queries with multiple words into segments and
collected historical CTR on the segment level. We also
like to investigate more structured embedding methods
such as RNNs (probably for ad descriptions). In case
the computational cost of such methods are too high to
be practical for sponsored search, we can employ them
only for a small fraction of ads filtered by faster meth-
ods.
It may be possible to deal with the implicit nega-
tive feedback of unclicked ad impressions in a more
principled way by adopting ranking-based loss func-
tions. However, this is only possible with the extra cost
of identifying and aggregating related ads into a single
transaction.
Though not directly related to NLP, yet another
promising direction is to jointly embed not only text
data but also a variety of user activities (e.g., organic
search results, mobile app usages, other daily activities)
all together in the same vector space. Since many of the
different sources contain their own unique information,
we might be able to obtain a much better understand-
ing about the user state and intent through this rich joint
embedding space. Joint embedding with rich informa-
tion can also help us to perform automatic clustering of
users, eventually leading to powerful smoothing meth-
ods for personalized historical CTR statistics.
</bodyText>
<sectionHeader confidence="0.985663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.774957916666667">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics, volume 1, pages
238–247.
Phil Blunsom, Edward Grefenstette, Nal Kalchbrenner,
</reference>
<page confidence="0.99926">
489
</page>
<bodyText confidence="0.861839444444444">
et al. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52ndAn-
nual Meeting of the Association for Computational
Linguistics. Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics.
Haibin Cheng and Erick Cant´u-Paz. 2010. Person-
alized click prediction in sponsored search. In Pro-
ceedings of the ThirdACMInternational Conference
</bodyText>
<reference confidence="0.99479081372549">
on Web Search and Data Mining, WSDM ’10, pages
351–360, New York, NY, USA. ACM.
K. Dembczynski, W.Kotlowski, and D.Weiss. 2008.
Predicting ads click-through rate with decision rules.
In Proceedings of WWW 08.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
David Easley and Jon Kleinberg. 2010. Networks,
Crowds, and Markets: Reasoning about a Highly
Connected World. Cambridge University Press.
Benjamin Edelman, Michael Ostrovsky, Michael
Schwarz, Thank Drew Fudenberg, Louis Kaplow,
Robin Lee, Paul Milgrom, Muriel Niederle, and
Ariel Pakes. 2005. Internet advertising and the
generalized second price auction: Selling billions of
dollars worth of keywords. American Economic Re-
view, 97.
Tom Fawcett. 2006. An introduction to ROC analysis.
Pattern Recognition Letters, 27(8):861–874, June.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Tomas Mikolov, et al. 2013.
Devise: A deep visual-semantic embedding model.
In Advances in Neural Information Processing Sys-
tems, pages 2121–2129.
Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavl-
jevic, Fabrizio Silvestri, and Narayan Bhamidipati.
2015. Context- and content-aware embeddings for
query rewriting in sponsored search. In Proceedings
of the 38th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR ’15, pages 383–392, New York, NY, USA.
ACM.
Dustin Hillard, Eren Manavoglu, Hema Raghavan,
Chris Leggetter, Erick Cant´u-Paz, and Rukmini Iyer.
2011. The sum of its parts: reducing sparsity in
click estimation with query segments. Inf. Retr.,
14(3):315–336.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Andrej Karpathy and Li Fei-Fei. 2014. Deep visual-
semantic alignments for generating image descrip-
tions. arXiv preprint arXiv:1412.2306.
Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.
H. Brendan McMahan, Daniel Golovin, Sharat
Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar
Hrafnkelsson, Tom Boulos, Jeremy Kubica, Gary
Holt, D. Sculley, Michael Young, Dietmar Ebner,
Julian Grady, Lan Nie, Todd Phillips, and Eugene
Davydov. 2013. Ad Click Prediction: a View
from the Trenches. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining - KDD ’13, page
1222, New York, New York, USA, August. ACM
Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A Lock-Free Approach
to Parallelizing Stochastic Gradient Descent. In Ad-
vances in Neural Information Processing Systems,
pages 693–701.
Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. Bpr:
Bayesian personalized ranking from implicit feed-
back. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artificial Intelligence, UAI ’09,
pages 452–461, Arlington, Virginia, United States.
AUAI Press.
Matthew Richardson. 2007. Predicting clicks: Esti-
mating the click-through rate for new ads. In In Pro-
ceedings of the 16th International World Wide Web
Conference (WWW-07, pages 521–530. ACM Press.
Richard Socher, Yoshua Bengio, and Christopher D.
Manning. 2012a. Deep learning for nlp (without
magic). In Tutorial Abstracts of ACL 2012, ACL
’12, pages 5–5, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012b. Semantic composition-
ality through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
</reference>
<page confidence="0.977891">
490
</page>
<reference confidence="0.998286808510638">
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201–
1211. Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP),
volume 1631, page 1642.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.
Ilya Trofimov, Anna Kornetova, and Valery Topinskiy.
2012. Using boosted trees for click-through rate pre-
diction for sponsored search. In Proceedings of the
Sixth International Workshop on Data Mining for
Online Advertising and Internet Economy, ADKDD
’12, pages 2:1–2:6, New York, NY, USA. ACM.
Taifeng Wang, Jiang Bian, Shusen Liu, Yuyu Zhang,
and Tie-Yan Liu. 2013. Psychological advertis-
ing: exploring user psychology for click prediction
in sponsored search. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 563–571.
ACM.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Proceedings of the Empiricial Methods
in Natural Language Processing (EMNLP 2014).
Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-
Chang Rim. 2014. Joint relational embeddings for
knowledge-based question answering. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
645–650.
</reference>
<page confidence="0.998748">
491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.392083">
<title confidence="0.825923">Joint Embedding of Query and Ad by Leveraging Implicit Feedback Sungjin Lee and Yifan</title>
<author confidence="0.482569">Yahoo</author>
<address confidence="0.971423">229 West 43rd Street, New York, NY 10036,</address>
<abstract confidence="0.999227533333333">Sponsored search is at the center of a multibillion dollar market established by search technology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical features derived from the text of both the query and ads play a significant role, complementing features based on historical click information. The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional word embedding methodologies for ad click prediction. These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<contexts>
<context position="10866" citStr="Bahdanau et al., 2014" startWordPosition="1730" endWordPosition="1733">ionships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structu</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>238--247</pages>
<contexts>
<context position="5215" citStr="Baroni et al., 2014" startWordPosition="827" endWordPosition="830">overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very sh</context>
<context position="10387" citStr="Baroni et al., 2014" startWordPosition="1655" endWordPosition="1658">eddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs),</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 238–247.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Phil Blunsom</author>
</authors>
<title>Edward Grefenstette, Nal Kalchbrenner, on Web Search and Data Mining,</title>
<journal>WSDM</journal>
<volume>10</volume>
<pages>351--360</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Blunsom, </marker>
<rawString>Phil Blunsom, Edward Grefenstette, Nal Kalchbrenner, on Web Search and Data Mining, WSDM ’10, pages 351–360, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dembczynski</author>
<author>W Kotlowski</author>
<author>D Weiss</author>
</authors>
<title>Predicting ads click-through rate with decision rules.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW 08.</booktitle>
<contexts>
<context position="8459" citStr="Dembczynski et al. (2008)" startWordPosition="1348" endWordPosition="1351">engine, and demonstrate that our proposed models significantly improve click prediction performance. The rest of this paper is organized as follows. In Section 2 we present a brief summary of related work. In Section 3 we give some background information on ad click prediction in sponsored search. In Section 4 we describe our methods. In Section 5 we discuss our experiments. We finish with some conclusions and future directions in Section 6. 2 Related Work Text features for predicting click probability There have been many studies on the use of text features for click prediction. For example, Dembczynski et al. (2008) used a decision rule-based approach involving such lexical features as the number of words in ad title and description, the number of segments and length of the ad URL, and individual words and terms in ad title and description. Cheng et al. (2010) used a logistic regression model that used both historical click performance features and simple lexical features such as word or phrase overlap between query and ad title and description. Trofimov et al. (2012) used a variant of boosted decision trees with similar features. Richardson et al. (2007) specifically considered new ads (which lack histo</context>
<context position="27492" citStr="Dembczynski et al. (2008)" startWordPosition="4600" endWordPosition="4603">in a shared memory. For computational efficiency, the hogwild lock-free approach (Recht et al., 2011) is used. We set η in Eq. 8 and Eq. 9 to 0.2 through grid search based on two-fold cross-validation. This small value indeed verifies the idea of weak negative feedback for unclicked impressions. In order to circumvent the severe influence of ad position on the click prediction model, only the impressions that are placed at the top position were used for training the click prediction model. Note that CTR at other positions can be derived from that of the ad at the top position through scaling. Dembczynski et al. (2008) showed that CTR can be decomposed as a product of the probability of an ad getting clicked given its being seen and the probability of an ad being seen at a particular position. Results We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 - AUC where AUC is the area under the Receiver Operating Characteristic (ROC) curve). The ROC AUC is known to have a correlation with the quality of ranking by the predicted score (Fawcett, 2006); thus is one of the most important 2Available at https://code.google.com/p/word2vec/ 3Higher vector dimensions such as 200 were also tried bu</context>
</contexts>
<marker>Dembczynski, Kotlowski, Weiss, 2008</marker>
<rawString>K. Dembczynski, W.Kotlowski, and D.Weiss. 2008. Predicting ads click-through rate with decision rules. In Proceedings of WWW 08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="26416" citStr="Duchi et al., 2011" startWordPosition="4414" endWordPosition="4417">ords in ad title 3260M 1690M 1635M 6585M Number of words in ad description 4311M 2206M 2131M 8650M Average number of words in query per impression 3.275 3.284 3.289 3.281 Average number of words in ad title per impression 6.667 6.708 6.706 6.687 Average number of words in ad description per impression 8.818 8.759 8.741 8.784 Table 1: Data description 5. Joint mean vector embedding (JMV), see Section 4.3 We used the skip-gram mode of word2vec 2 with a window size of 5 and negative sampling to train the SKWIKI model. For all other models we used in-house implementation which employs AdagradRDA (Duchi et al., 2011) to minimize the loss functions introduced in Section 4. The dimension of a word vector is set to 100 for all algorithms3. We removed a set of prefixed stop-words and all words occurring fewer than 100 times; the resulting vocabulary comprised 126K unique words. To process our web-scale data, we implemented a multithread program where each thread randomly traverses over a partition of the data D to compute gradients and update the matrix W stored in a shared memory. For computational efficiency, the hogwild lock-free approach (Recht et al., 2011) is used. We set η in Eq. 8 and Eq. 9 to 0.2 thr</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Easley</author>
<author>Jon Kleinberg</author>
</authors>
<title>Networks, Crowds, and Markets: Reasoning about a Highly Connected World.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1584" citStr="Easley and Kleinberg, 2010" startWordPosition="250" endWordPosition="253">bservations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search. 1 Introduction Sponsored search is a multibillion dollar market (Easley and Kleinberg, 2010) that makes most search engine revenue and is one of the most successful ways for advertisers to reach their intended audiences. When search engines deliver results to a user, sponsored advertisement impressions (ads) are shown alongside the organic search results (Figure 1). Typically the advertiser pays the search engine based on the pay-per-click model. In this model the advertiser pays only if the impression that accompanies the search results is clicked. The price is usually set by a generalized second-price (GSP) auction (Edelman et al., 2005) that encourages advertisers to bid truthfull</context>
</contexts>
<marker>Easley, Kleinberg, 2010</marker>
<rawString>David Easley and Jon Kleinberg. 2010. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Edelman</author>
<author>Michael Ostrovsky</author>
<author>Michael Schwarz</author>
<author>Thank Drew Fudenberg</author>
<author>Louis Kaplow</author>
<author>Robin Lee</author>
<author>Paul Milgrom</author>
<author>Muriel Niederle</author>
<author>Ariel Pakes</author>
</authors>
<title>Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords.</title>
<date>2005</date>
<journal>American Economic Review,</journal>
<volume>97</volume>
<contexts>
<context position="2139" citStr="Edelman et al., 2005" startWordPosition="339" endWordPosition="342">earch is a multibillion dollar market (Easley and Kleinberg, 2010) that makes most search engine revenue and is one of the most successful ways for advertisers to reach their intended audiences. When search engines deliver results to a user, sponsored advertisement impressions (ads) are shown alongside the organic search results (Figure 1). Typically the advertiser pays the search engine based on the pay-per-click model. In this model the advertiser pays only if the impression that accompanies the search results is clicked. The price is usually set by a generalized second-price (GSP) auction (Edelman et al., 2005) that encourages advertisers to bid truthfully. An advertiser wins if the expected revenue for this advertiser, which is the bid Figure 1: Sponsored ads when “pizza” was searched at Yahoo! (www.yahoo.com). price times the expected click probability (also know as click through rate, or CTR), is ranked the highest. The price the advertiser pays, known as cost-per-click (CPC), is the bid price for the second ranked advertiser times the ratio of the expected CTR between the second and first ranked advertisers. From this discussion it should be clear that CTR plays a key role in deciding both the r</context>
</contexts>
<marker>Edelman, Ostrovsky, Schwarz, Fudenberg, Kaplow, Lee, Milgrom, Niederle, Pakes, 2005</marker>
<rawString>Benjamin Edelman, Michael Ostrovsky, Michael Schwarz, Thank Drew Fudenberg, Louis Kaplow, Robin Lee, Paul Milgrom, Muriel Niederle, and Ariel Pakes. 2005. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. American Economic Review, 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Fawcett</author>
</authors>
<title>An introduction to ROC analysis.</title>
<date>2006</date>
<journal>Pattern Recognition Letters,</journal>
<volume>27</volume>
<issue>8</issue>
<contexts>
<context position="27950" citStr="Fawcett, 2006" startWordPosition="4684" endWordPosition="4685">e click prediction model. Note that CTR at other positions can be derived from that of the ad at the top position through scaling. Dembczynski et al. (2008) showed that CTR can be decomposed as a product of the probability of an ad getting clicked given its being seen and the probability of an ad being seen at a particular position. Results We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 - AUC where AUC is the area under the Receiver Operating Characteristic (ROC) curve). The ROC AUC is known to have a correlation with the quality of ranking by the predicted score (Fawcett, 2006); thus is one of the most important 2Available at https://code.google.com/p/word2vec/ 3Higher vector dimensions such as 200 were also tried but did not give a significant improvement. metrics for click prediction (McMahan et al., 2013). AucLoss Reduction (%) Baseline + SK-WIKI 0.530 Baseline + SK-CI 0.595 Baseline + CB-CI 0.656 Baseline + JIWV 2.276 Baseline + JMV 4.114 Table 2: Comparative evaluation results in AucLoss reduction from the baseline system Our experimental results (Table 2) show that the use of features derived from our proposed word embedding models significantly reduce AucLoss</context>
</contexts>
<marker>Fawcett, 2006</marker>
<rawString>Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861–874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg S Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeff Dean</author>
<author>Tomas Mikolov</author>
</authors>
<title>Devise: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2121--2129</pages>
<contexts>
<context position="12371" citStr="Frome et al., 2013" startWordPosition="1979" endWordPosition="1982">y and ad. Interestingly, as reported in (Tai et al., 2015), a simple averaging scheme (mean vector) was found to be very competitive to more complex models for high level semantic tasks despite its simplicity. These observations lead us to one of our models that aims to improve the mean vector method by directly optimizing mean vectors instead of word vectors. Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014). The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training. 3 Baseline Click Predict</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihajlo Grbovic</author>
<author>Nemanja Djuric</author>
<author>Vladan Radosavljevic</author>
<author>Fabrizio Silvestri</author>
<author>Narayan Bhamidipati</author>
</authors>
<title>Context- and content-aware embeddings for query rewriting in sponsored search.</title>
<date>2015</date>
<booktitle>In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’15,</booktitle>
<pages>383--392</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9608" citStr="Grbovic et al. (2015)" startWordPosition="1537" endWordPosition="1540">chardson et al. (2007) specifically considered new ads (which lack historical click prediction data) and proposed to use the CTR for ad terms, the frequency of certain unigrams (e.g., dollar signs) and general English usage patterns, and simple lexical distance between the query and ads. In all this previous work, text features consisted only of surface-level text features. To the best of our knowledge, there is no previous work adopting semantic-level text features for the purpose of click prediction, in particular word embeddings to measure query-ad relevance. In a similar vein of research, Grbovic et al. (2015) adopted word embeddings to the task of query rewriting for a better match between queries and keywords that advertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in c</context>
</contexts>
<marker>Grbovic, Djuric, Radosavljevic, Silvestri, Bhamidipati, 2015</marker>
<rawString>Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, and Narayan Bhamidipati. 2015. Context- and content-aware embeddings for query rewriting in sponsored search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’15, pages 383–392, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Eren Manavoglu</author>
<author>Hema Raghavan</author>
<author>Chris Leggetter</author>
<author>Erick Cant´u-Paz</author>
<author>Rukmini Iyer</author>
</authors>
<title>The sum of its parts: reducing sparsity in click estimation with query segments.</title>
<date>2011</date>
<journal>Inf. Retr.,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Hillard, Manavoglu, Raghavan, Leggetter, Cant´u-Paz, Iyer, 2011</marker>
<rawString>Dustin Hillard, Eren Manavoglu, Hema Raghavan, Chris Leggetter, Erick Cant´u-Paz, and Rukmini Iyer. 2011. The sum of its parts: reducing sparsity in click estimation with query segments. Inf. Retr., 14(3):315–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="10762" citStr="Irsoy and Cardie, 2014" startWordPosition="1715" endWordPosition="1718">, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None o</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</title>
<date>2014</date>
<contexts>
<context position="12399" citStr="Karpathy and Fei-Fei, 2014" startWordPosition="1983" endWordPosition="1987">gly, as reported in (Tai et al., 2015), a simple averaging scheme (mean vector) was found to be very competitive to more complex models for high level semantic tasks despite its simplicity. These observations lead us to one of our models that aims to improve the mean vector method by directly optimizing mean vectors instead of word vectors. Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014). The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training. 3 Baseline Click Prediction Model We first present a</context>
</contexts>
<marker>Karpathy, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy and Li Fei-Fei. 2014. Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</title>
<date>2014</date>
<contexts>
<context position="11096" citStr="Kim, 2014" startWordPosition="1768" endWordPosition="1769">emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it is not attractive to use models with complex structures, such as RNNs, at the cost of speed and scalability. PhraseVector is less structured but it does not support compositionality, suffering from sparsene</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</title>
<date>2014</date>
<contexts>
<context position="11188" citStr="Mikolov, 2014" startWordPosition="1783" endWordPosition="1784">d semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it is not attractive to use models with complex structures, such as RNNs, at the cost of speed and scalability. PhraseVector is less structured but it does not support compositionality, suffering from sparseness or requiring to train new vectors for each unseen query and ad. Interestingly, as reporte</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Brendan McMahan</author>
<author>Daniel Golovin</author>
<author>Sharat Chikkerur</author>
<author>Dan Liu</author>
<author>Martin Wattenberg</author>
<author>Arnar Mar Hrafnkelsson</author>
<author>Tom Boulos</author>
<author>Jeremy Kubica</author>
<author>Gary Holt</author>
<author>D Sculley</author>
<author>Michael Young</author>
<author>Dietmar Ebner</author>
<author>Julian Grady</author>
<author>Lan Nie</author>
<author>Todd Phillips</author>
<author>Eugene Davydov</author>
</authors>
<title>Ad Click Prediction: a View from the Trenches.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’13,</booktitle>
<pages>1222</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA,</location>
<contexts>
<context position="28185" citStr="McMahan et al., 2013" startWordPosition="4716" endWordPosition="4719"> ad getting clicked given its being seen and the probability of an ad being seen at a particular position. Results We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 - AUC where AUC is the area under the Receiver Operating Characteristic (ROC) curve). The ROC AUC is known to have a correlation with the quality of ranking by the predicted score (Fawcett, 2006); thus is one of the most important 2Available at https://code.google.com/p/word2vec/ 3Higher vector dimensions such as 200 were also tried but did not give a significant improvement. metrics for click prediction (McMahan et al., 2013). AucLoss Reduction (%) Baseline + SK-WIKI 0.530 Baseline + SK-CI 0.595 Baseline + CB-CI 0.656 Baseline + JIWV 2.276 Baseline + JMV 4.114 Table 2: Comparative evaluation results in AucLoss reduction from the baseline system Our experimental results (Table 2) show that the use of features derived from our proposed word embedding models significantly reduce AucLoss by up to 4.1%. For commercial search engines which have a very strong baseline AucLoss, a reduction of 1% can be considered large (McMahan et al., 2013). Moreover, as expected, the more issues (as identified in Section 4) an algorithm</context>
</contexts>
<marker>McMahan, Golovin, Chikkerur, Liu, Wattenberg, Hrafnkelsson, Boulos, Kubica, Holt, Sculley, Young, Ebner, Grady, Nie, Phillips, Davydov, 2013</marker>
<rawString>H. Brendan McMahan, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, and Eugene Davydov. 2013. Ad Click Prediction: a View from the Trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’13, page 1222, New York, New York, USA, August. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="5167" citStr="Mikolov et al., 2013" startWordPosition="819" endWordPosition="822"> ads, word or phrase overlaps and the number of overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power b</context>
<context position="10339" citStr="Mikolov et al., 2013" startWordPosition="1647" endWordPosition="1650">vertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Int</context>
<context position="19712" citStr="Mikolov et al., 2013" startWordPosition="3226" endWordPosition="3229">rs qj, tk, dl to denote the corresponding embedding representations of {qj, tk, dl}. Finally we use W to represent the vocabulary matrix; in W each column is a word vector. 4.1 Exploiting word2vec embedding Typically word embeddings are learned from a given text corpus through implicit supervision of predicting the current word given a window of its surrounding text or predicting each word in the window of the current word. The former approach is known as continuous bag-of-words (CBOW) and the latter Skip-gram. For simplicity’s sake we use negative-sampling for training word embedding models (Mikolov et al., 2013b). More formally we define the binary conditional probability for a pair of words (v, w): 1 1 p(v, w) = 1 + exp(−vTw), (3) The CBOW algorithm learns word embeddings by minimizing the following logloss of each impression i with regard to W: CBi(W) = − logp(wi, µC) 11 − log (1 − p(v, µC)) , (4) vEN(w;) where the context C of a word wi comes from a window of size k around the word in a sentence of n words w1,...,wn: C = wi−k,...,wi−1,wi+1,...,wi+k. µC is the averaged context vector of wi; µ 1e = IC |EvEC v. N(wi) is the set of negative examples which is drawn according to the unigram distributio</context>
<context position="20986" citStr="Mikolov et al., 2013" startWordPosition="3466" endWordPosition="3469">o (Mikolov et al., 2013b), we adopt a dynamic window size – for each word the actual window size is sampled uniformly from 1,...,k. Similarly the Skip-gram algorithm minimizes the logloss of each impression i with regard to W: SKi(W) = − 11 log p(wi, v) 11 − vEC log (1 − p(wi, v)) . (5) vEN(w;) In our first word embedding model that incorporates click feedback, we construct a corpus by taking only clicked impressions from D and then mixing (q, t, d) of each impression into a sentence. Then we simply train CBOW and Skip-gram models on the corpus. 1We use only a single vector for a word unlike (Mikolov et al., 2013b) where two vectors (“input” and “output”) for a word are used. This halves the required space to store vectors without performance loss. 485 4.2 Joint word embedding using click feedback Although the CBOW and Skip-gram models trained on a specially constructed corpus can capture signals from both click feedback and word co-occurrence, they have a couple of drawbacks. First, by ingesting only clicked impressions we “waste” the large amount of negative signals contained in the non-clicked impressions. Second, the incorporation of indirect signals such as word co-occurrences can be rather harmf</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="5167" citStr="Mikolov et al., 2013" startWordPosition="819" endWordPosition="822"> ads, word or phrase overlaps and the number of overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power b</context>
<context position="10339" citStr="Mikolov et al., 2013" startWordPosition="1647" endWordPosition="1650">vertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Int</context>
<context position="19712" citStr="Mikolov et al., 2013" startWordPosition="3226" endWordPosition="3229">rs qj, tk, dl to denote the corresponding embedding representations of {qj, tk, dl}. Finally we use W to represent the vocabulary matrix; in W each column is a word vector. 4.1 Exploiting word2vec embedding Typically word embeddings are learned from a given text corpus through implicit supervision of predicting the current word given a window of its surrounding text or predicting each word in the window of the current word. The former approach is known as continuous bag-of-words (CBOW) and the latter Skip-gram. For simplicity’s sake we use negative-sampling for training word embedding models (Mikolov et al., 2013b). More formally we define the binary conditional probability for a pair of words (v, w): 1 1 p(v, w) = 1 + exp(−vTw), (3) The CBOW algorithm learns word embeddings by minimizing the following logloss of each impression i with regard to W: CBi(W) = − logp(wi, µC) 11 − log (1 − p(v, µC)) , (4) vEN(w;) where the context C of a word wi comes from a window of size k around the word in a sentence of n words w1,...,wn: C = wi−k,...,wi−1,wi+1,...,wi+k. µC is the averaged context vector of wi; µ 1e = IC |EvEC v. N(wi) is the set of negative examples which is drawn according to the unigram distributio</context>
<context position="20986" citStr="Mikolov et al., 2013" startWordPosition="3466" endWordPosition="3469">o (Mikolov et al., 2013b), we adopt a dynamic window size – for each word the actual window size is sampled uniformly from 1,...,k. Similarly the Skip-gram algorithm minimizes the logloss of each impression i with regard to W: SKi(W) = − 11 log p(wi, v) 11 − vEC log (1 − p(wi, v)) . (5) vEN(w;) In our first word embedding model that incorporates click feedback, we construct a corpus by taking only clicked impressions from D and then mixing (q, t, d) of each impression into a sentence. Then we simply train CBOW and Skip-gram models on the corpus. 1We use only a single vector for a word unlike (Mikolov et al., 2013b) where two vectors (“input” and “output”) for a word are used. This halves the required space to store vectors without performance loss. 485 4.2 Joint word embedding using click feedback Although the CBOW and Skip-gram models trained on a specially constructed corpus can capture signals from both click feedback and word co-occurrence, they have a couple of drawbacks. First, by ingesting only clicked impressions we “waste” the large amount of negative signals contained in the non-clicked impressions. Second, the incorporation of indirect signals such as word co-occurrences can be rather harmf</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="5193" citStr="Pennington et al., 2014" startWordPosition="823" endWordPosition="826">erlaps and the number of overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text</context>
<context position="10365" citStr="Pennington et al., 2014" startWordPosition="1651" endWordPosition="1654">an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional N</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Recht</author>
<author>Christopher Re</author>
<author>Stephen Wright</author>
<author>Feng Niu</author>
</authors>
<title>Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>693--701</pages>
<contexts>
<context position="26968" citStr="Recht et al., 2011" startWordPosition="4507" endWordPosition="4510">-house implementation which employs AdagradRDA (Duchi et al., 2011) to minimize the loss functions introduced in Section 4. The dimension of a word vector is set to 100 for all algorithms3. We removed a set of prefixed stop-words and all words occurring fewer than 100 times; the resulting vocabulary comprised 126K unique words. To process our web-scale data, we implemented a multithread program where each thread randomly traverses over a partition of the data D to compute gradients and update the matrix W stored in a shared memory. For computational efficiency, the hogwild lock-free approach (Recht et al., 2011) is used. We set η in Eq. 8 and Eq. 9 to 0.2 through grid search based on two-fold cross-validation. This small value indeed verifies the idea of weak negative feedback for unclicked impressions. In order to circumvent the severe influence of ad position on the click prediction model, only the impressions that are placed at the top position were used for training the click prediction model. Note that CTR at other positions can be derived from that of the ad at the top position through scaling. Dembczynski et al. (2008) showed that CTR can be decomposed as a product of the probability of an ad </context>
</contexts>
<marker>Recht, Re, Wright, Niu, 2011</marker>
<rawString>Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In Advances in Neural Information Processing Systems, pages 693–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Zeno Gantner</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Bpr: Bayesian personalized ranking from implicit feedback.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09,</booktitle>
<pages>452--461</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia, United States.</location>
<contexts>
<context position="22361" citStr="Rendle et al., 2009" startWordPosition="3709" endWordPosition="3712">hat incorporates click feedback, we define a joint word embedding model that minimizes the following weighted logloss: n o JWi(W) = η(yi) l(yi,qi, ti) + l(yi,qi, di) , (6) where the component loss function l(·, ·,·) is defined as follows: − y log p(ak, bl) − (1 − y) log (1 − p(ak, bl)) . (7) Here η(yi) is a function that returns a small weight η only to negative examples: ( η, if yi = 0, η(yi) = (8) 1, otherwise. The fact that the user did not click on an ad does not necessarily mean that the ad is not what the user wanted; it often means that the ad is just less favored than the clicked ads (Rendle et al., 2009). Since the scope of this work is restricted to estimating click probability on a per-impression basis, we adopt a weighting scheme rather than optimizing rank-based loss over a set of related impressions. 4.3 Joint mean vector optimization The joint word embedding models defined in the previous sections do not define how to aggregate a variable length sequence of word vectors into a sentence (or paragraph) vector to facilitate the computation of sentence-level similarity scores. One approach to this aggregation task is mean vector: simply average the word-level embeddings across the sentence </context>
</contexts>
<marker>Rendle, Freudenthaler, Gantner, Schmidt-Thieme, 2009</marker>
<rawString>Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 452–461, Arlington, Virginia, United States. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
</authors>
<title>Predicting clicks: Estimating the click-through rate for new ads. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International World Wide Web Conference (WWW-07,</booktitle>
<pages>521--530</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4416" citStr="Richardson, 2007" startWordPosition="700" endWordPosition="701">y of the click prediction model when fewer historical observations are available. Furthermore, fine-grained historical CTR information takes a huge amount of space, which 482 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 482–491, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. makes it costly to maintain. On the other hand, text features are always readily available, and thus are particularly useful for those cases for which there is insufficient historical information. Multiple researchers, for example (Richardson, 2007; Cheng and Cant´u-Paz, 2010), reported the usage of text features including simple lexical similarity scores between the query and ads, word or phrase overlaps and the number of overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding hav</context>
</contexts>
<marker>Richardson, 2007</marker>
<rawString>Matthew Richardson. 2007. Predicting clicks: Estimating the click-through rate for new ads. In In Proceedings of the 16th International World Wide Web Conference (WWW-07, pages 521–530. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Yoshua Bengio</author>
<author>Christopher D Manning</author>
</authors>
<title>Deep learning for nlp (without magic).</title>
<date>2012</date>
<booktitle>In Tutorial Abstracts of ACL 2012, ACL ’12,</booktitle>
<pages>5--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5420" citStr="Socher et al., 2012" startWordPosition="862" endWordPosition="865">ity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very short. In addition, conventional word embeddings cannot capture user intents, preferences and desires. Wang et al. (2013) showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed </context>
<context position="10118" citStr="Socher et al., 2012" startWordPosition="1615" endWordPosition="1618">articular word embeddings to measure query-ad relevance. In a similar vein of research, Grbovic et al. (2015) adopted word embeddings to the task of query rewriting for a better match between queries and keywords that advertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; </context>
</contexts>
<marker>Socher, Bengio, Manning, 2012</marker>
<rawString>Richard Socher, Yoshua Bengio, and Christopher D. Manning. 2012a. Deep learning for nlp (without magic). In Tutorial Abstracts of ACL 2012, ACL ’12, pages 5–5, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="5420" citStr="Socher et al., 2012" startWordPosition="862" endWordPosition="865">ity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very short. In addition, conventional word embeddings cannot capture user intents, preferences and desires. Wang et al. (2013) showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed </context>
<context position="10118" citStr="Socher et al., 2012" startWordPosition="1615" endWordPosition="1618">articular word embeddings to measure query-ad relevance. In a similar vein of research, Grbovic et al. (2015) adopted word embeddings to the task of query rewriting for a better match between queries and keywords that advertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012b. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201– 1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP),</booktitle>
<volume>volume</volume>
<pages>1631--1642</pages>
<contexts>
<context position="10716" citStr="Socher et al., 2013" startWordPosition="1707" endWordPosition="1710"> (Socher et al., 2012a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like tra</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="10737" citStr="Socher et al., 2014" startWordPosition="1711" endWordPosition="1714">a). 483 In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimit</context>
<context position="12421" citStr="Socher et al., 2014" startWordPosition="1988" endWordPosition="1991">al., 2015), a simple averaging scheme (mean vector) was found to be very competitive to more complex models for high level semantic tasks despite its simplicity. These observations lead us to one of our models that aims to improve the mean vector method by directly optimizing mean vectors instead of word vectors. Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014). The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training. 3 Baseline Click Prediction Model We first present a high-level descriptio</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="10891" citStr="Sutskever et al., 2014" startWordPosition="1734" endWordPosition="1737">utationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it i</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</title>
<date>2015</date>
<contexts>
<context position="10934" citStr="Tai et al., 2015" startWordPosition="1741" endWordPosition="1744"> (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014). Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015). Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014). As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it is not attractive to use models with complex</context>
<context position="23005" citStr="Tai et al., 2015" startWordPosition="3813" endWordPosition="3816">ork is restricted to estimating click probability on a per-impression basis, we adopt a weighting scheme rather than optimizing rank-based loss over a set of related impressions. 4.3 Joint mean vector optimization The joint word embedding models defined in the previous sections do not define how to aggregate a variable length sequence of word vectors into a sentence (or paragraph) vector to facilitate the computation of sentence-level similarity scores. One approach to this aggregation task is mean vector: simply average the word-level embeddings across the sentence or paragraph. As noted by (Tai et al., 2015), this approach is a strong competitor to more complex models such as RNNs or LSTMs despite its simple composition method. However, this method may generate suboptimal sentence vectors. With weight logloss, we aim to optimize sentence vectors instead of individual word vectors: n o JMi(W) = η(yi) ¯l(yi,qi,ti) + ¯l(yi,qi,di) , (9) where the component loss function l(·, ·,·) is defined as follows: (10) − (1 − y) log (1 − p(µa, µb)) , where µs returns the average vector for the multiword string s, i.e. µs = |3 |Pks |sk. 5 Experiments Data The data used in our experiments were collected from a ran</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Trofimov</author>
<author>Anna Kornetova</author>
<author>Valery Topinskiy</author>
</authors>
<title>Using boosted trees for click-through rate prediction for sponsored search.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD ’12,</booktitle>
<pages>2--1</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8920" citStr="Trofimov et al. (2012)" startWordPosition="1425" endWordPosition="1428">t features for predicting click probability There have been many studies on the use of text features for click prediction. For example, Dembczynski et al. (2008) used a decision rule-based approach involving such lexical features as the number of words in ad title and description, the number of segments and length of the ad URL, and individual words and terms in ad title and description. Cheng et al. (2010) used a logistic regression model that used both historical click performance features and simple lexical features such as word or phrase overlap between query and ad title and description. Trofimov et al. (2012) used a variant of boosted decision trees with similar features. Richardson et al. (2007) specifically considered new ads (which lack historical click prediction data) and proposed to use the CTR for ad terms, the frequency of certain unigrams (e.g., dollar signs) and general English usage patterns, and simple lexical distance between the query and ads. In all this previous work, text features consisted only of surface-level text features. To the best of our knowledge, there is no previous work adopting semantic-level text features for the purpose of click prediction, in particular word embedd</context>
</contexts>
<marker>Trofimov, Kornetova, Topinskiy, 2012</marker>
<rawString>Ilya Trofimov, Anna Kornetova, and Valery Topinskiy. 2012. Using boosted trees for click-through rate prediction for sponsored search. In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD ’12, pages 2:1–2:6, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taifeng Wang</author>
<author>Jiang Bian</author>
<author>Shusen Liu</author>
<author>Yuyu Zhang</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Psychological advertising: exploring user psychology for click prediction in sponsored search.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>563--571</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5934" citStr="Wang et al. (2013)" startWordPosition="944" endWordPosition="947">tter capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a). No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very short. In addition, conventional word embeddings cannot capture user intents, preferences and desires. Wang et al. (2013) showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed return in x days and official site, are effective in triggering users desires, and thus lead to significant differences in CTR. Conventional word embeddings cannot capture these phenomena since they do not incorporate the implicit feedback users provide through clicks and non-clicks. These observations naturally lead us to leverage click feedback to infuse users’ intentions and desires into the vector space. The simplest way to harness click feedback is to train conventional word embedding models on a corpus </context>
<context position="34220" citStr="Wang et al., 2013" startWordPosition="5554" endWordPosition="5557">ing something like movie or other types of video) or find totally unrelated words (particularly SK-WIKI). This demonstrates that it is crucial to address compositionality in the very process of learning word vectors. Table 4 shows the top 20 most similar words to the query “princess costumes.” In this example we can spot another surprising result. The JMV model pushes a lot of price related expressions to the top4. This may imply that many parents search for lower cost costumes, clearly showing a clear psychological desire in the financial dimension. This observation confirms the findings in (Wang et al., 2013) about the significant role of certain ad expressions in triggering users’ psychological desires. We also note that the CB-CI model returns a lot of misspells for “costume(s)”, which would not be possible with simple lexical features of the baseline system. A close look at this example generally confirms the observations we made for the previous ex4We have not tried any normalization for numbers but it might be worth doing given the important role they play. 488 SK-WIKI CB-CI JIWV JMV kids (0.724) game (0.629) kids (0.795) program (0.764) pac-man (0.708) gams (0.615) kidsfootlocker.com (0.788)</context>
</contexts>
<marker>Wang, Bian, Liu, Zhang, Liu, 2013</marker>
<rawString>Taifeng Wang, Jiang Bian, Shusen Liu, Yuyu Zhang, and Tie-Yan Liu. 2013. Psychological advertising: exploring user psychology for click prediction in sponsored search. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 563–571. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph and text jointly embedding.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="12635" citStr="Wang et al., 2014" startWordPosition="2021" endWordPosition="2024">ims to improve the mean vector method by directly optimizing mean vectors instead of word vectors. Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014). The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training. 3 Baseline Click Prediction Model We first present a high-level description of sponsored search. The process consists of several stages. First, given a user query, a list of candidate ads are retrieved, either by exactly matching query terms to the bid terms of the advertiser, or by firs</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Chul Yang</author>
<author>Nan Duan</author>
<author>Ming Zhou</author>
<author>HaeChang Rim</author>
</authors>
<title>Joint relational embeddings for knowledge-based question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>645--650</pages>
<contexts>
<context position="12682" citStr="Yang et al., 2014" startWordPosition="2029" endWordPosition="2032">ly optimizing mean vectors instead of word vectors. Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014). The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training. 3 Baseline Click Prediction Model We first present a high-level description of sponsored search. The process consists of several stages. First, given a user query, a list of candidate ads are retrieved, either by exactly matching query terms to the bid terms of the advertiser, or by first using query term expansion to obtain a longer</context>
</contexts>
<marker>Yang, Duan, Zhou, Rim, 2014</marker>
<rawString>Min-Chul Yang, Nan Duan, Ming Zhou, and HaeChang Rim. 2014. Joint relational embeddings for knowledge-based question answering. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
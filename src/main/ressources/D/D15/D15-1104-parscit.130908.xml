<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987325">
Joint Named Entity Recognition and Disambiguation
</title>
<author confidence="0.99058">
Gang Luo1, Xiaojiang Huang2, Chin-Yew Lin2, Zaiqing Nie2
</author>
<affiliation confidence="0.834404">
1Microsoft, California, USA
</affiliation>
<address confidence="0.664509">
2Microsoft Research, Beijing, China
</address>
<email confidence="0.949686">
{gluo, xiaojih, cyl, znie}@microsoft.com
</email>
<sectionHeader confidence="0.993289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961653846154">
Extracting named entities in text and link-
ing extracted names to a given knowledge
base are fundamental tasks in applications
for text understanding. Existing systems
typically run a named entity recognition
(NER) model to extract entity names first,
then run an entity linking model to link ex-
tracted names to a knowledge base. NER
and linking models are usually trained sep-
arately, and the mutual dependency be-
tween the two tasks is ignored. We pro-
pose JERL, Joint Entity Recognition and
Linking, to jointly model NER and link-
ing tasks and capture the mutual depen-
dency between them. It allows the in-
formation from each task to improve the
performance of the other. To the best of
our knowledge, JERL is the first model to
jointly optimize NER and linking tasks to-
gether completely. In experiments on the
CoNLL’03/AIDA data set, JERL outper-
forms state-of-art NER and linking sys-
tems, and we find improvements of 0.4%
absolute F1 for NER on CoNLL’03, and
0.36% absolute precision@1 for linking
on AIDA.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995556097561">
In applications of complex Natural Language Pro-
cessing tasks, such as automatic knowledge base
construction, entity summarization, and question
answering systems, it is essential to first have high
quality systems for lower level tasks, such as part-
of-speech (POS) tagging, chunking, named en-
tity recognition (NER), entity linking, and parsing
among others. These lower level tasks are usually
decoupled and optimized separately to keep the
system tractable. The disadvantage of the decou-
pled approach is that each lower level task is not
aware of other tasks and thus not able to leverage
information provided by others to improve perfor-
mance. What is more, there is no guarantee that
their outputs will be consistent.
This paper addresses the problem by building
a joint model for Entity Recognition and Disam-
biguation (ERD). The goal of ERD is to extract
named entities in text and link extracted names to
a knowledge base, usually Wikipedia or Freebase.
ERD is closely related to NER and linking tasks.
NER aims to identify named entities in text and
classify mentions into predefined categories such
as persons, organizations, locations, etc. Given a
mention and context as input, entity linking con-
nects the mention to a referent entity in a knowl-
edge base.
Existing ERD systems typically run a NER to
extract entity mentions first, then run an entity
linking model to link mentions to a knowledge
base. Such a decoupled approach makes the sys-
tem tractable, and both NER and linking models
can be optimized separately. The disadvantages
are also obvious: 1) errors caused by NER will
be propagated to linking and are not recoverable
2) NER can not benefit from information available
used in entity linking; 3) NER and linking may
create inconsistent outputs.
We argue that there is strong mutual depen-
dency between NER and linking tasks. Consider
the following two examples:
</bodyText>
<listItem confidence="0.99766475">
1. The New York Times (NYT) is an American
daily newspaper.
2. Clinton plans to have more news conferences
in 2nd term. WASHINGTON 1996-12-06
</listItem>
<bodyText confidence="0.998538">
Example 1 is the first sentence from the
Wikipedia article about “The New York Times”.
It is reasonable but incorrect for NER to identify
“New York Times” without “The” as a named en-
tity, while entity linking has no trouble connect-
ing “The New York Times” to the correct entity.
</bodyText>
<page confidence="0.98262">
879
</page>
<note confidence="0.985064">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 879–888,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999627911764706">
Example 2 is a news title where our NER classi-
fies “WASHINGTON” as a location, since a lo-
cation followed by a date is a frequent pattern
in news articles it learned, while the entity link-
ing prefers linking this mention to the U.S. pres-
ident “George Washington” since another presi-
dent’s name “Clinton” is mentioned in the con-
text. Both the entity boundaries and entity types
predicted by NER are correlated to the knowledge
of entities linked by entity linking. Modeling such
mutual dependency is helpful in resolving incon-
sistency and improving performance for both NER
and linking.
We propose JERL, Joint Entity Recognition and
Linking, to jointly model NER and linking tasks
and capture the mutual dependency between them.
It allows the information from each task to im-
prove the performance of the other. If NER is
highly confident on its outputs of entity boundaries
and types, it will encourage entity linking to link
an entity which is consistent with NER’s outputs,
and vice versa. In other words, JERL is able to
model how consistent NER and linking’s outputs
are, and predict coherent outputs. According to
our experiments, this approach does improve the
end to end performance. To the best of our knowl-
edge, JERL is the first model to jointly optimize
NER and linking tasks together completely .
Sil (2013) also proposes jointly conducting
NER and linking tasks. They leverage existing
NER/chunking systems and Freebase to over gen-
erate mention candidates and leave the linking al-
gorithm to make final decisions, which is a re-
ranking model. Their model captures the depen-
dency between entity linking decisions and men-
tion boundary decisions with impressive results.
The difference between our model and theirs is
that our model jointly models NER and linking
tasks from the training phrase, while their model
is a combined one which depends on an existing
state-of-art NER system. Our model is more pow-
erful in capturing mutual dependency by consider-
ing entity type and confidences information, while
in their model the confidence of outputs is lost
in the linking phrase. Furthermore, in our model
NER can naturally benefit from entity linking’s
decision since both decisions are made together,
while in their model, it is not clear how the linking
decision can help the NER decision in return.
Joint optimization is costly. It increases the
problem complexity, is usually inefficient, and
requires the careful consideration of features of
multiple tasks and mutual dependency, making
proper assumptions and approximations to enable
tractable training and inference. However, we
believe that joint optimization is a promising di-
rection for improving performance for NLP tasks
since it is closer to how human beings process text
information. Experiment result indicates that our
joint model does a better job at both NER and
linking tasks than separate models with the same
features, and outperforms state-of-art systems on
a widely used data set. We found improvements
of 0.4% absolute F1 for NER on CoNLL’03 and
0.36% absolute precision@1 for linking on AIDA.
NER is a widely studied problem, and we believe
our improvement is significant.
The contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.946542666666667">
1. We identify the mutual dependency between
NER and linking tasks, and argue that NER and
linking should be conducted together to improve
the end to end performance.
2. We propose the first completely joint NER and
linking model, JERL, to train and inference the
two tasks together. Efficient training and inference
algorithms are also presented.
3. The JERL outperforms the best NER record
on the CoNLL’03 data set, which demonstrates
how NER could be improved further by leverag-
ing knowledge base and linking techniques.
</listItem>
<bodyText confidence="0.999849571428571">
The remainder of this paper is organized as fol-
lows: the next section discusses related works on
NER, entity linking, and joint optimization; sec-
tion 3 presents our Joint Entity Recognition and
Linking model in detail; section 4 describes ex-
periments, results, and analysis; and section 5 con-
cludes.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999686461538462">
The NER problem has been widely addressed by
symbolic, statistical, as well as hybrid approaches.
It has been encouraged by several editions of eval-
uation campaigns such as MUC (Chinchor and
Marsh, 1998), the CoNLL 2003 NER shared task
(Tjong Kim Sang and De Meulder, 2003) and ACE
(Doddington et al., 2004). Along with the im-
provement of Machine Learning techniques, sta-
tistical approaches have become a major direc-
tion for research on NER, especially after Condi-
tional Random Field is proposed by Lafferty et al.
(2001). The well known state-of-art NER systems
are Stanford NER (Finkel et al., 2005) and UIUC
</bodyText>
<page confidence="0.992253">
880
</page>
<bodyText confidence="0.999817511627907">
NER (Ratinov and Roth, 2009). Liang (2005)
compares the performance of the 2nd order lin-
ear chain CRF and Semi-CRF (Sarawagi and Co-
hen, 2004) in his thesis. Lin and Wu (2009) clus-
ter tens of millions of phrases and use the result-
ing clusters as features in NER reporting the best
performance on the CoNLL’03 English NER data
set. Recent works on NER have started to focus
on multi-lingual named entity recognition or NER
on short text, e.g. Twitter.
Entity linking was initiated with Wikipedia-
based works on entity disambiguation (Bunescu
and Pasca, 2006; Cucerzan, 2007). This task is
encouraged by the TAC 2009 KB population task1
first and receives more and more attention from the
research community (Hoffart et al., 2011; Ratinov
et al., 2011; Han and Sun, 2011). Linking usu-
ally takes mentions detected by NER as its input.
Stern et al. (2012) and Wang et al. (2012) present
joint NER and linking systems and evaluate their
systems on French and Chinese data sets. Sil and
Yates (2013) take a re-ranking based approach and
achieve the best result on the AIDA data set. In
2014, Microsoft and Google jointly hosted “En-
tity Recognition and Disambiguation Challenge”
which focused on the end to end performance of
linking system 2.
Joint optimization models have been studied at
great length. E.g. Dynamic CRF (McCallum
et al., 2003) has been proposed to conduct Part-
of-Speech Tagging and Chunking tasks together.
Finkel and Manning (2009) show how to model
parsing and named entity recognition together. Yu
et al. (2011) work on jointly entity identifica-
tion and relation extraction from Wikipedia. Sil’s
(2013) work on jointly NER and linking is de-
scribed in the introduction section of this paper.
It is worth noting that joint optimization does not
always work. The CoNLL 2008 shared task (Sur-
deanu et al., 2008) was intended to encourage
jointly optimize parsing and semantic role label-
ing, but the top performing systems decoupled the
two tasks.
</bodyText>
<sectionHeader confidence="0.954096" genericHeader="method">
3 Joint Entity Recognition and Linking
</sectionHeader>
<bodyText confidence="0.90362">
Named entity recognition is usually formalized as
a sequence labeling task, in which each word is
classified to not-an-entity or entity labels. Condi-
tional Random Fields (CRFs) is one of the popu-
</bodyText>
<footnote confidence="0.999538">
1http://www.nist.gov/tac/2014/KBP/
2http://web-ngram.research.microsoft.com/ERD2014/
</footnote>
<figureCaption confidence="0.999954">
Figure 1: The factor graph of JERL model
</figureCaption>
<bodyText confidence="0.99999472">
lar models used. Most features used in NER are
word-level (e.g. a word sequence appears at po-
sition i or whether a word contains exactly four
digits). It is hard, if not impossible, to encode
entity-level features (such as ”entity length” and
”correlation to known entities”) in traditional CRF.
Entity linking is typically formalized as a ranking
task. Features used for entity linking are at entity-
level inherently (such as entity prior probability;
whether there are any related entity names or dis-
criminative keywords occurring in the context).
The main challenges of joint optimization be-
tween NER and linking are: how to combine a se-
quence labeling model and a ranking model; and
how to incorporate word-level and entity-level fea-
tures. In a linear chain CRF model, each word’s
label is assumed to depend on the observations
and the label of its previous word. Semi-CRF
carefully relaxes the Markov assumption between
words in CRF, and models the distribution of seg-
mentation boundaries directly. We further extend
Semi-CRF to model entity distribution and mu-
tual dependency over segmentations, and name it
Joint Entity Recognition and Linking (JERL). The
model is described below.
</bodyText>
<subsectionHeader confidence="0.99095">
3.1 JERL
</subsectionHeader>
<bodyText confidence="0.9989208">
Let x = {xi} be a word sequence containing |x|
words. Let s = {sj} be a segmentation assign-
ment over x, where segment sj = (uj, vj) con-
sist of a start position uj and an end position vj.
All segments have a positive length and are adja-
cent to each other, so every (uj, vj) always satis-
fies 1 G uj G vj G |x |and uj+1 = vj + 1. Let
y = {yj} be labels in a fixed label alphabet Y
over a segmentation assignment s. Here Y is the
set of types NER to predict. xsj = (xuj ... xvj)
</bodyText>
<page confidence="0.988639">
881
</page>
<bodyText confidence="0.999532130434783">
is the corresponding word sequence to sj, and
£sj = {ej,k} is a set of entities in the knowl-
edge base (KB), which may be referred by word
sequence xsj in the entity linking task. Each en-
tity ej,k is associated with a label yej,k E {0, 1}.
Label yej,k takes 1 iff xsj referring to entity ej,k,
and 0 otherwise. If xsj does not refer to any entity
in the KB, yej,0 takes 1, which is analogous to the
NIL3 identifier in entity linking.
Based on the preliminaries and notations, Fig-
ure 1 shows the factor graph (Kschischang et al.,
2001) of JERL. There are similar factor nodes for
every (uj, vj, yej,k), we only show the first one
(uj, vj, yej,0) for clarity.
Given x, let a = (s, y, ye) be a joint assign-
ment, and g(x, j, a) be local functions for xsj,
namely features, each of which maps an assign-
ment a to a measurement gk(x, j, a) E R. Then
G(x, a) = P|s |j=1 g(x, j, a) is the factor graph
defining a probability distribution of assignment
a conditioned on word sequence x.
Then JERL, conditional probability of a over x,
is defined as:
</bodyText>
<equation confidence="0.813874">
Z(x)
</equation>
<bodyText confidence="0.9607198">
where w is the weight vector corresponding to G
will be learned later, and Z(x) is the normaliza-
tion factor Z(x) = Pa∈A ew·G(x,a), in which A
is the union of all possible assignments over x.
JERL is a probabilistic graphical model. More
specificly, as shown in Figure 1, there are three
groups of local functions and one constrain intro-
duced. Each of them take a different role in JERL,
as described below:
Features defined on x, sj, yj, yj−1 are written
as gner(x, sj, yj, yj−1). These functions model
segmentation and entity types’ distribution over x.
Actually, every local features used in NER can be
formulated in this way, and thus can be included in
JERL. We thus refer to them as “NER features”.
Features defined on x, sj, ye j,k are written as
gel(x, sj, yej,k) and are called “linking features”.
These features model joint probabilities of word
sequence xsj and linking decisions ykj,k = 1(0 &lt;
k &lt; |£sj|) given context x. JERL incorporates all
linking features in this way.
Features defined on yj, ye j,k are written as
gcr(yj, yej,k). These features model “mutual de-
3In the entity linking task, if a given mention refers to
an entity which is not in the knowledge base, linking system
should return a special identifier “NIL”.
pendency” between NER and linking’s outputs.
For each entity ej,k, there is additional informa-
tion available in the knowledge base, e.g. cate-
gories information, popularity and relationship to
other entities. These features encourage predicting
coherent outputs for NER and linking.
There is one constrain for each yej that the cor-
responding xsj can refer to only one entity ej,k E
£sj or NIL. This is equivalent toP|Esj |
</bodyText>
<equation confidence="0.9875275">
k=0
yej,k = 1.
</equation>
<bodyText confidence="0.970275333333333">
Based on the above description, G(x, a)
in equation 1 is the sum of conjunction
(gner, gel, gcr)
</bodyText>
<equation confidence="0.997777666666667">
G(x, a) = P|s|
j=1( gner(x, sj, yj, yj−1)
, P|Ej|
k=0 gel(x, sj, yej,k)
, P|Ej|
k=0 gcr(x, yj, ye j,k) )
</equation>
<bodyText confidence="0.999891">
In summary, JERL jointly models the NER
and linking, and leverages mutual dependency be-
tween them to predict coherent outputs. Previ-
ous works (Cucerzan, 2007; Ratinov et al., 2011;
Sil and Yates, 2013) on linking argued that en-
tity linking systems often suffer because of errors
involved in mention detection phrase, especially
false negative errors, and try to mitigate it via over-
generating mention candidates. From the mention
generation perspective, JERL actually considers
every possible assignment and is able to find the
optimal a.
</bodyText>
<subsectionHeader confidence="0.99928">
3.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9994265">
We describe how to conduct parameter estima-
tion for JERL in this section. Given independent
and identically distributed (i.i.d.) training data
T = {(xt, at)}Nt=1, the goal of parameter estima-
tion is to find optimal w∗ to maximize the joint
probability of the assignments {at} over {xt}.
</bodyText>
<equation confidence="0.988839666666667">
N
w∗ = argmaxw∈&lt;|G |Y P(at|xt, w)
t=1
</equation>
<bodyText confidence="0.904215333333333">
We use conditional log likelihood with `2 norm as
the objective function in training,
L(T, w) = Pt logP(at|xt, w) − 1
2σ2 ||w||22
The above function is concave, adding regulariza-
tion to ensure that it has exactly one global opti-
mum. We adopt a limited-memory quasi-Newton
method (Liu and Nocedal, 1989) to solve the opti-
mization problem.
</bodyText>
<equation confidence="0.93481725">
1
P(a|x, w) =
ew·G(x,a) (1)
over s, and can be rewritten as,
</equation>
<page confidence="0.972975">
882
</page>
<bodyText confidence="0.9944725">
The gradient of L(T, w) is derived as,
As shown in Figure 1, our model’s factor graph is
a tree, which means the calculation of the gradient
is tractable.
Inspired by the forward backward algorithm
(Sha and Pereira, 2003) and Semi-CRF (Sarawagi
and Cohen, 2004), we leverage dynamic program-
ming techniques to compute the normalization
factor Zw and marginal probability P(a0j|xt, w)
when w is given.(Sutton and McCallum, 2006)
The parameter estimation algorithm is abstracted
in Algorithm 1.
</bodyText>
<equation confidence="0.689509875">
Algorithm 1: JERL parameter estimation
input : training data T = {(xt, at)}Nt=1
output: the optimal w
w ← 0;
while weight w is not converged do
Z ← 0;
w0 ← 0;
for t ← 1 to N do
</equation>
<bodyText confidence="0.967445333333333">
calculate αt, βt according to eq.3;
calculate Zt according to eq.4
calculate w0t according to eq.2, 5;
</bodyText>
<equation confidence="0.903327333333333">
Z ← Z + Zt;
w0 ← w0 + w0t;
end
</equation>
<bodyText confidence="0.8935193">
update w to maximize log likelihood
L(T, w) under (Z, w0) via L-BFGS;
end
Let αi,y (i ∈ [0, |x|], y ∈ Y) be the sum of po-
tential functions of all possible assignments over
(x1 ... xi) whose last segmentation’s labels are y.
Then αi,y can be calculated recursively from i = 0
to i = |x |as below.
We first define base cases as α0,y = 1|{yEY}.
When i ∈ (0, |x|]:
</bodyText>
<equation confidence="0.8526434">
O!er
αi−d,y, i−d+1,i,y,y,
(3)
ψel.cr
i−d+1,i,y,yej )
</equation>
<bodyText confidence="0.871292666666667">
where L is the max segmentation length in Semi-
CRF, and Y e∗ jis all valid assignments for yej
which satisfies|£sj |q�e 1. The ner
</bodyText>
<equation confidence="0.660645166666667">
Pk=0 .Yj,k uj,vj,yj,yj−1
ψandψel.cr
uj,vj,yj,ye j are precomputed ahead as below,
ner = ewner· gner(x,sj,yj,yj−1
uj ,vj ,yj ,yj−1
ewelgel(x,sj,yej,k)+wcrgcr(yj,yej,k)
</equation>
<bodyText confidence="0.986628333333333">
where wner, wel and wcr are weights for gner, gel
and gcr in w accordingly.
The value of Zw can then be written as
</bodyText>
<equation confidence="0.994852">
XZw(x) = α|x|,y (4)
y
</equation>
<bodyText confidence="0.943417">
Define βi,y (i ∈ [0, |x|], y ∈ Y) as the sum
of potential functions of all possible assignments
over (xi+1 ... x|x|) whose first segmentation’s la-
bels are y. βi,y is calculated in a similar way, ex-
cept they are calculated from i = |x |to left i = 0.
Once we get {αi,j} and {βi,j}, the
marginal probability of arbitrary assignment
aj = (sj, yj, yej), where sj = (uj, vj), can be
calculated as below:
</bodyText>
<equation confidence="0.9988566">
P(sj,yj|x,w) =
,,
ner /�
Py,EY αuj−1,y,0Uj,vj,yj,y, Ovj,yj
Zw(x)
</equation>
<subsectionHeader confidence="0.77738">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.9970125">
Given a new word sequence x and model weights
w trained on a training set, the goal of in-
ference is to find the best assignment, a∗ =
argmaxaP(a|x, w) for x. We extend the Viterbi
algorithm to exactly infer the best assignment. The
inference algorithm is shown in Algorithm 2.
Let φ(uj, vj, yj, yj−1) be the product of poten-
tials depending on (sj, yj, yj−1) as,
</bodyText>
<equation confidence="0.715539717948718">
φ(uj,vj,yj,yj−1) =
ψner( X0el.cr ) (6)
4 e
uj ,vj ,yj ,yj−1 �j rvj ,yj ryj
yej EY e*
j
∂L
(G(xt, at)
∂w
X=
t
G(xt,a0)P(a0|xt,w)) − w2
σ
X−
a,
(2)
and
P(aj|xt, w) =
P(sj, yj|x, w)
el.cr (5)
Pye0 EYe* uj,vj,yj,ye0
ψel.cr
j
uj,vj,yj ,ye
XL
d=1
αi,y =
X
y,EY
X
(
yej EY e*
j
ψel.cr =
uj,vj ,yj,ye
j
|£j |
Y
k=0
</equation>
<page confidence="0.528613">
883
</page>
<figure confidence="0.901821588235294">
Algorithm 2: JERL inference
input : one word sequence x and weights w
output: the best assignment a over x
// shrink JERL graph to a Semi-CRF graph;
foru+— 1 to jxj do
for v +— u + 1 to jxj do
for (y, y&apos;) E Y x Y do
calculate Ou,v,y,y0 // see eq.6;
end
end
end
// infer the best assignment of (s*, y*);
for i +— 1 to jxj do
for y E Y do
calculate Vi,y // see eq.7;
end
end
</figure>
<equation confidence="0.899409333333333">
(s*, y*) +— argmax(Vi,y);
//infer the best assignment of {yej };
forj+— 1 to js*j do
yej +— argmax(P(jx, w, s*j, yj* ))
end
a* +— (s*, y*, ye*);
</equation>
<bodyText confidence="0.9961794">
and let V (i, y) denotes the largest value of (w �
G(x, a&apos;)) where a&apos; could be any possible par-
tial assignment starting from x1 to xi. The best
(s*, y*) are derived during the following recur-
sive calculation,
</bodyText>
<equation confidence="0.9994564">
Vi,y =
{ maxy0EY,dE [(1,L]
(Vi −d,y0 + OG—d+1, i, y, yT i &gt; 0
0 i = 0
−oo i &lt; 0
</equation>
<bodyText confidence="0.5842835">
where L is the maximum segmentation length for
Semi-CRF.
</bodyText>
<equation confidence="0.99117675">
Once (s*, y*) are found, the corresponding
ye*
j = argmax{ye EY e∗}(ψUj v∗j y∗j ,y�) is also the
j
</equation>
<bodyText confidence="0.962626">
optimal one. Then a* = (s*, y*, ye*) is the best
assignment for the given x and w.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999960333333333">
In our experiments, we first construct two base-
line models JERLner and JERLel, which use exact
NER and EL feature sets used in JERL. Then eval-
uate JERL and the two baseline models against
several state-of-art NER and linking systems. Af-
ter that, we evaluate JERL under different feature
</bodyText>
<table confidence="0.994668">
CoNLL’03 Training Dev set Test
Articles 946 216 231
Sentences 14,987 3,466 3,684
Tokens 203,621 51,362 46,435
Entities 23,499 5,942 5,648
NIL Entities 4,857 1,129 1,133
</table>
<tableCaption confidence="0.999891">
Table 1: Overview of CoNLL’03/AIDA data set
</tableCaption>
<bodyText confidence="0.97219675">
settings to analysis the contributions of each fea-
tures set, and show some examples we find. We
also compare the training speed under different
settings.
</bodyText>
<subsectionHeader confidence="0.997238">
4.1 Data set
</subsectionHeader>
<bodyText confidence="0.99768356">
We take the CoNLL’03/AIDA English data set
to evaluate the performance of NER and linking
systems. CoNLL’03 is extensively used in prior
work on NER evaluation (Tjong Kim Sang and
De Meulder, 2003). The English data is taken
from Reuters news articles published between Au-
gust 1966 and August 1997. Four types of en-
tities persons (PER), organizations (ORG), loca-
tions (LOC), and miscellaneous names (MISC) are
annotated. Hoffart et al. (2011) hand-annotated
all proper nouns with corresponding entities wiht
YAGO2, Freebase and Wikipedia IDs. This data
is referenced as AIDA here. To the best of our
knowledge, this data set is the biggest data set
which has been labeled for both NER and linking
tasks. It becomes a really good starting point for
our work. Table 1 contains of an overview of the
CoNLL’03/AIDA data set.
For entity linking, we take Wikipedia as the ref-
erent knowledge base. We use a Wikipedia snap-
shot dumped in May 2013, which contains around
4.8 million articles. We also align our Wikipedia
dump with additional knowledge bases, Freebase
and Satori (a Microsoft internal knowledge base),
to enrich the information of these entities.
</bodyText>
<subsectionHeader confidence="0.974596">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9997111">
We follow the CoNLL’03 metrics to evaluate NER
performance by precision, recall, and F1 scores,
and follow Hoffart’s (2011) experiment setting
to evaluate linking performance by micro preci-
sion@1. Since the linking labels of CONLL’03
were annotated in 2011, it is not completely con-
sistent with the Wikipedia dump we used in the
case. We only consider mention entity pairs where
the ground truth are known, and ignore around
20% of NIL mentions in the ground truth.
</bodyText>
<equation confidence="0.728153">
(7)
</equation>
<page confidence="0.994687">
884
</page>
<table confidence="0.99940685">
Category Features
NER Word unigram / bigram
Lower cased unigram / bigram
Word shape unigram / bigram
Stemmed unigram / bigram
POS unigram / bigram
Chunk unigram / bigram
Words in the 4 left/right window
Character n-grams, n ≤ 4
Brown clusters
WordNet clusters
Dictionaries
Linking Alternative names
Entity priors
Entity name priors
Entity priors over names
Context scores
Geo distance
Related entities
Mutual Type-category correlation
</table>
<tableCaption confidence="0.995635">
Table 2: JERL feature list
</tableCaption>
<subsectionHeader confidence="0.98075">
4.3 JERL Implementation
</subsectionHeader>
<bodyText confidence="0.999462333333333">
Table 2 shows features used in our models. JERL
uses all features in the three categories, while
JERLner and JERLej use only one corresponding
category. All three models are trained on the train
and development set, and evaluated on the test set
of CoNLL’03/AIDA.
</bodyText>
<sectionHeader confidence="0.413538" genericHeader="evaluation">
4.3.1 NER
</sectionHeader>
<bodyText confidence="0.999864">
Features in the NER category are relevant to NER.
We considered the most commonly used features
in literatures (Finkel et al., 2005; Liang, 2005;
Ratinov and Roth, 2009). We collect several
known name lists, like popular English first/last
names for people, organization lists and so on
from Wikipedia and Freebase. UIUC NER’s lists
are also included. In addition, we extract entity
name lists from the knowledge base we used for
entity linking, and construct 655 more lists. Al-
though those lists are noisy, we find that statisti-
cally they do improve the performance of our NER
baseline by a significant amount.
</bodyText>
<subsectionHeader confidence="0.984074">
4.3.2 Linking
</subsectionHeader>
<bodyText confidence="0.999961">
Features in linking category are relevant to entity
linking. An entity can be referred by its canoni-
cal name, nick names, alias, and first/last names.
Those names are defined as alternative names for
this entity. We collect all alternative names for all
known entities and build a name to entity index.
This index is used to select entity candidates for
any word sequence, also known as surface form.
Following previous work by Han and Sun (2011),
we calculate entity priors and entity name priors
from Wikipedia. Context scores are calculated
based on discriminative keywords. Geo distance
and related entities capture the relatedness among
entities in the given context.
</bodyText>
<subsectionHeader confidence="0.92698">
4.3.3 Mutual
</subsectionHeader>
<bodyText confidence="0.999939416666667">
Features in this category capture the mutual de-
pendency between NER and linking’s outputs. For
each entity in a knowledge base, there is category
information available. We aggregate around 1000
distinct categories from multiple sources. One en-
tity can have multiple categories. For example,
London is connected to 29 categories. We use all
combinations between NER types and categories
as features in JERL, and let the model learn the
correlation of each combination. This encourages
coherent NER and EL decisions, which is one of
the key contributions of our work.
</bodyText>
<subsubsectionHeader confidence="0.533517">
4.3.4 Non-local features
</subsubsectionHeader>
<bodyText confidence="0.9993731">
Features capturing long distance dependency be-
tween hidden labels are classified as non-local
features. Those features are very helpful in im-
proving NER system performance but are costly.
Since this is not the focus of this paper, we take
a simple approach to incorporate non-local fea-
tures. We cache history results of previous sen-
tences in a 1000 words window, and adopt sev-
eral heuristic rules for personal names. This ap-
proach contributes 0.2 points to the final NER F1
score. Non-local features are also considered in
linking (Ratinov et al., 2011; Han et al., 2011).
We try several features, which has been proved to
be helpful in TAC data set. However, the gain on
CoNLL’03/AIDA data set is not obvious, we do
not optimize linking globally.
Lastly, based on preliminary studies and exper-
iments, we set the maximum segmentation length
to 6 and max candidate count per segmentation to
5 for efficient training and inference.
</bodyText>
<subsectionHeader confidence="0.99898">
4.4 State-of-Art systems
</subsectionHeader>
<bodyText confidence="0.9991128">
We take three state-of-art NER systems: NereL
(Sil and Yates, 2013), UIUC NER (Ratinov and
Roth, 2009) and Stanford NER (Finkel et al.,
2005). NereL firstly over generates mentions and
decomposes them to sets of connected compo-
</bodyText>
<page confidence="0.996145">
885
</page>
<table confidence="0.999924">
Dataset System Prec. Recall F1
CoNLL’03 Stanford 95.1 78.3 85.9
UIUC 91.2 90.5 90.8
NereL 86.8 89.5 88.2
JERL,,,, 90.0 89.9 89.9
JERL 91.5 91.4 91.2
</table>
<tableCaption confidence="0.999345">
Table 3: NER evaluation results
</tableCaption>
<bodyText confidence="0.9997782">
nents, then trains a maximum-entropy model to
re-rank different assignments. UIUC NER uses
a regularized averaged perceptron model and ex-
ternal gazetteers to achieve strong performance.
In Addition, NereL also uses UIUC NER to gen-
erate mentions. Stanford NER uses Conditional
Random Fields and Gibbs sampling to incorporate
non-local features into its model.
For entity linking systems, NereL, Kul09
(Kulkarni et al., 2009) and Hof11 (Hoffart et al.,
2011) are compared with our models. NereL
achieves the best precision@1. Kul09 formulates
the local compatibility and global coherence in en-
tity linking, and optimizes the overall entity as-
signment for all entities in a document via a lo-
cal hill-climbing approach. Hof11 unifies the prior
probability of an entity being mentioned, the simi-
larity between context and entity, and the coher-
ence between entity candidates among all men-
tions in a dense graph.
</bodyText>
<subsectionHeader confidence="0.591273">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.999648190476191">
Table 3 shows the performance of different NER
systems on the CoNLL’03 testb data set. We re-
fer the numbers of state-of-art systems reported by
Sil and Yates (2013). Stanford NER achieves the
best precision, but its recall is low. UIUC reports
the (almost) best recorded F1. JERLner considers
features only in the NER category, which could
be treated as a pure NER system implemented in
Semi-CRF. Actually CRF-based implementation
with a similar feature set has comparable perfor-
mance. Our baseline JERLner is strong enough.
We argue that that it is mainly because of the ad-
ditional dictionaries derived from the knowledge
base. JERL further pushes the F1 to 91.2, which
outperforms UIUC by 0.4 points in F1 score. To
the best of our knowledge, it is the best F1 on
CoNLL’03 since 2009. The reason our model
can outperform state-of-art systems is that, it has
more knowledge about entities via incorporate en-
tity linking techniques. If an entity can be linked
to a well known entity via entity linking in high
</bodyText>
<table confidence="0.997285666666667">
Dataset System Precision@1
Kul09 76.74
Hof11 81.91
CoNLL’03 Nerel 84.22
JERLel 81.49
JERL 84.58
</table>
<tableCaption confidence="0.971924">
Table 4: Linking evaluation results
</tableCaption>
<table confidence="0.928176333333333">
# Feature set description NER F1
0 JERLner (baseline) 89.9
1 + candidate 88.7
2 + candidate + linking 89.9
3 + candidate + mutual 90.6
4 + candidate + mutual + linking 91.2
</table>
<tableCaption confidence="0.996014">
Table 5: JERL features analysis
</tableCaption>
<bodyText confidence="0.997802794117647">
confidence, its mention boundary and entity type
are confirmed implicitly.
Table 4 shows the performance of different en-
tity linking systems on the AIDA test set. Kul09
and Hof11 use only the correct mentions detected
by the Stanford NER as input, and thus their re-
call is bound by the recall of NER. NereL uses
its overgeneration techniques to generate mention
candidates, and outperforms Hoff11 in both preci-
sion and recall. Our baseline model JERL,j is also
evaluated on Stanford NER generated mentions,
which has comparable performance with Kul09
and Hof11. JERL achieves precision@1 84.58
which is better than NereL.
We run 15 trials for both NER and linking’s ex-
periments and report the average numbers above.
The standard deviations are 0.11% and 0.08% for
NER and linking separately, which pass the stan-
dard t-test with confidence level 5%, demonstrat-
ing the significance of our results.
In order to investigate how different features
contribute to the overall gain. We compare
JERLner with four different feature sets. Table
5 summaries the results. In the trial “+candidate”,
JERL expands every possible segmentation with
corresponding entity list and builds its factor graph
without any linking and mutual features. This ver-
sion’s F1 drops to 88.7 which indicates the created
structure is quite noisy. In the “+candidate +link-
ing” trial, only linking features are enabled and
the F1 is comparable to the baseline. On the other
side, in the “+candidate +mutual” trial when mu-
tual features are enabled the F1 increases to 90.6.
If we combine both linking and mutual features,
</bodyText>
<page confidence="0.99489">
886
</page>
<table confidence="0.9947135">
Category PER LOC ORG Other
people.person 3.65 0.817 1.260 -1.782
location.city -0.187 0.712 0.491 -0.188
sports.team -0.180 2.382 3.595 -2.019
</table>
<tableCaption confidence="0.793382">
Table 6: Learned mutual dependency
</tableCaption>
<table confidence="0.887684666666667">
Setting NER Linking Training
Fi prec@1 time (min)
MSL MRC
</table>
<figure confidence="0.8507392">
4 5 87.9 76.74 195
5 5 90.8 84.01 234
6 1 90.8 80.13 37
6 3 91.0 83.21 109
6 5 91.2 84.58 280
</figure>
<tableCaption confidence="0.975007">
Table 7: Training time under different settings
</tableCaption>
<bodyText confidence="0.999905620689655">
JERL achieves the reported performance. The re-
sult indicates that mutual features are the deter-
mining factor to the performance gain.
Table 6 shows weights of learned mutual depen-
dency of three categories ”people.person”, ”loca-
tion.city”, and ”sports.team”. The bigger a weight
is, the more consistent this combination would
be. From the weights, we find several interest-
ing things. If an entity belongs to any of the
three categories, it is less likely to be predicted
as non-an-entity by NER. If an entity belongs to
the category of ”people.person”, it more likely
to be predicted as PER. When an entity belongs
to the category ”location.city” or ”sports.team”,
NER may predict it as ORG or LOC. This is be-
cause in the CoNLL’03/AIDA data set, there are
many sports teams mentioned by their city/country
names. JERL successfully models such unex-
pected mutual dependency.
Table 7 compares the performance and training
time under different settings of max segmentation
length (MSL) and max referent count (MRC). We
use machines with Intel Xeon E5620 @ 2.4GHz
CPU (8 cores / 16 logical processors) and 48GB
memory. We run every setting 10 times and report
the averages. As MSL and MRC increasing, the
performance is slightly better, but the training time
increased a lot. MSL has linear impact on training
time, while MRC affects training time more.
</bodyText>
<sectionHeader confidence="0.995645" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999954266666667">
In this paper, we address the problem of joint opti-
mization of named entity recognition and linking.
We propose a novel model, JERL, to jointly train
and infer for NER and linking tasks. To the best of
our knowledge, this is the first model which trains
two tasks at the same time. The joint model is able
to leverage mutual dependency of the two tasks,
and predict coherent outputs. JERL outperforms
the state-of-art systems on both NER and linking
tasks on the CoNLL’03/AIDA data set.
For future works, we would like to study how
to leverage existing partial labeled data, either for
NER or for linking only, in joint optimization, and
incorporate more NLP tasks together for multi-
tasks joint optimization.
</bodyText>
<sectionHeader confidence="0.996881" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999929125">
This work was performed when the first author
was working at Microsoft Research. The first au-
thor is sponsored by Microsoft Bing Core Rele-
vance team. Thanks Shuming Shi, Bin Gao, and
Yohn Cao for their helpful guidance and valuable
discussions. Additionally, we would like to thank
the three anonymous reviewers for their insightful
suggestions and detailed comments.
</bodyText>
<sectionHeader confidence="0.979728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.845504">
Razvan C Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In EACL, volume 6, pages 9–16.
Nancy Chinchor and Elaine Marsh. 1998. Muc-7 in-
formation extraction task definition. In Proceeding
of the seventh message understanding conference
(MUC-7), Appendices, pages 359–367.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, volume 7, pages 708–716.
George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC.
Jenny Rose Finkel and Christopher D Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 326–334. Association for Computa-
tional Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.978565">
887
</page>
<reference confidence="0.998873135135135">
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945–
954. Association for Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765–774. ACM.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782–792. Association for Computational
Linguistics.
Frank R Kschischang, Brendan J Frey, and H-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. Information Theory, IEEE Transactions
on, 47(2):498–519.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 457–466. ACM.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1030–1038. Association for
Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503–528.
Andrew McCallum, Khashayar Rohanimanesh, and
Charles Sutton. 2003. Dynamic conditional ran-
dom fields for jointly labeling multiple sequences.
In nips workshop on syntax, semantics and statistics.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155. Association for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375–1384. Associ-
ation for Computational Linguistics.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, pages 1185–1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 134–
141. Association for Computational Linguistics.
Avirup Sil and Alexander Yates. 2013. Re-ranking for
joint named-entity recognition and linking. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information &amp; knowledge man-
agement, pages 2369–2374. ACM.
Rosa Stern, Benoit Sagot, and Fr´ed´eric B´echet. 2012.
A joint named entity recognition and entity linking
system. In Proceedings of the Workshop on Innova-
tive Hybrid Approaches to the Processing of Textual
Data, pages 52–60. Association for Computational
Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 159–177. Association for
Computational Linguistics.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. Introduction to statistical relational learn-
ing, pages 93–128.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.
Longyue Wang, Shuo Li, Derek F Wong, and Lidia S
Chao. 2012. A joint chinese named entity recog-
nition and disambiguation system. CLP 2012, page
146.
Xiaofeng Yu, Irwin King, and Michael R Lyu. 2011.
Towards a top-down and bottom-up bidirectional ap-
proach to joint information extraction. In Proceed-
ings of the 20th ACM international conference on In-
formation and knowledge management, pages 847–
856. ACM.
</reference>
<page confidence="0.997536">
888
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425168">
<title confidence="0.7772625">Joint Named Entity Recognition and Disambiguation Xiaojiang Chin-Yew Zaiqing</title>
<address confidence="0.7781275">California, Research, Beijing,</address>
<email confidence="0.973181">xiaojih,cyl,</email>
<abstract confidence="0.998867703703704">Extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding. Existing systems typically run a named entity recognition (NER) model to extract entity names first, then run an entity linking model to link extracted names to a knowledge base. NER and linking models are usually trained separately, and the mutual dependency between the two tasks is ignored. We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. To the best of our knowledge, JERL is the first model to jointly optimize NER and linking tasks together completely. In experiments on the CoNLL’03/AIDA data set, JERL outperforms state-of-art NER and linking systems, and we find improvements of 0.4% NER on CoNLL’03, and 0.36% absolute precision@1 for linking on AIDA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<volume>6</volume>
<pages>9--16</pages>
<contexts>
<context position="8948" citStr="Bunescu and Pasca, 2006" startWordPosition="1455" endWordPosition="1458">-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambigua</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In EACL, volume 6, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Elaine Marsh</author>
</authors>
<title>Muc-7 information extraction task definition.</title>
<date>1998</date>
<booktitle>In Proceeding of the seventh message understanding conference (MUC-7), Appendices,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="7982" citStr="Chinchor and Marsh, 1998" startWordPosition="1290" endWordPosition="1293"> which demonstrates how NER could be improved further by leveraging knowledge base and linking techniques. The remainder of this paper is organized as follows: the next section discusses related works on NER, entity linking, and joint optimization; section 3 presents our Joint Entity Recognition and Linking model in detail; section 4 describes experiments, results, and analysis; and section 5 concludes. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens </context>
</contexts>
<marker>Chinchor, Marsh, 1998</marker>
<rawString>Nancy Chinchor and Elaine Marsh. 1998. Muc-7 information extraction task definition. In Proceeding of the seventh message understanding conference (MUC-7), Appendices, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL,</booktitle>
<volume>7</volume>
<pages>708--716</pages>
<contexts>
<context position="8965" citStr="Cucerzan, 2007" startWordPosition="1459" endWordPosition="1460">ford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” w</context>
<context position="15463" citStr="Cucerzan, 2007" startWordPosition="2594" endWordPosition="2595">onship to other entities. These features encourage predicting coherent outputs for NER and linking. There is one constrain for each yej that the corresponding xsj can refer to only one entity ej,k E £sj or NIL. This is equivalent toP|Esj | k=0 yej,k = 1. Based on the above description, G(x, a) in equation 1 is the sum of conjunction (gner, gel, gcr) G(x, a) = P|s| j=1( gner(x, sj, yj, yj−1) , P|Ej| k=0 gel(x, sj, yej,k) , P|Ej| k=0 gcr(x, yj, ye j,k) ) In summary, JERL jointly models the NER and linking, and leverages mutual dependency between them to predict coherent outputs. Previous works (Cucerzan, 2007; Ratinov et al., 2011; Sil and Yates, 2013) on linking argued that entity linking systems often suffer because of errors involved in mention detection phrase, especially false negative errors, and try to mitigate it via overgenerating mention candidates. From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a. 3.2 Parameter Estimation We describe how to conduct parameter estimation for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(xt, at)}Nt=1, the goal of parameter estimati</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLPCoNLL, volume 7, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark A Przybocki</author>
<author>Lance A Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph M Weischedel</author>
</authors>
<title>The automatic content extraction (ace) program-tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="8086" citStr="Doddington et al., 2004" startWordPosition="1309" endWordPosition="1312">. The remainder of this paper is organized as follows: the next section discusses related works on NER, entity linking, and joint optimization; section 3 presents our Joint Entity Recognition and Linking model in detail; section 4 describes experiments, results, and analysis; and section 5 concludes. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance </context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph M Weischedel. 2004. The automatic content extraction (ace) program-tasks, data, and evaluation. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>326--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9836" citStr="Finkel and Manning (2009)" startWordPosition="1603" endWordPosition="1606">s its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and named entity recognition together. Yu et al. (2011) work on jointly entity identification and relation extraction from Wikipedia. Sil’s (2013) work on jointly NER and linking is described in the introduction section of this paper. It is worth noting that joint optimization does not always work. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to encourage jointly optimize parsing and semantic role labeling, but the top performing systems decoupled the two tasks. 3 Joint Entity Recognition and Linking Named entity recognition is usually formalized a</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D Manning. 2009. Joint parsing and named entity recognition. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 326–334. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8380" citStr="Finkel et al., 2005" startWordPosition="1357" endWordPosition="1360">es. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is </context>
<context position="23668" citStr="Finkel et al., 2005" startWordPosition="4060" endWordPosition="4063"> Linking Alternative names Entity priors Entity name priors Entity priors over names Context scores Geo distance Related entities Mutual Type-category correlation Table 2: JERL feature list 4.3 JERL Implementation Table 2 shows features used in our models. JERL uses all features in the three categories, while JERLner and JERLej use only one corresponding category. All three models are trained on the train and development set, and evaluated on the test set of CoNLL’03/AIDA. 4.3.1 NER Features in the NER category are relevant to NER. We considered the most commonly used features in literatures (Finkel et al., 2005; Liang, 2005; Ratinov and Roth, 2009). We collect several known name lists, like popular English first/last names for people, organization lists and so on from Wikipedia and Freebase. UIUC NER’s lists are also included. In addition, we extract entity name lists from the knowledge base we used for entity linking, and construct 655 more lists. Although those lists are noisy, we find that statistically they do improve the performance of our NER baseline by a significant amount. 4.3.2 Linking Features in linking category are relevant to entity linking. An entity can be referred by its canonical n</context>
<context position="26536" citStr="Finkel et al., 2005" startWordPosition="4527" endWordPosition="4530">ore. Non-local features are also considered in linking (Ratinov et al., 2011; Han et al., 2011). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL’03/AIDA data set is not obvious, we do not optimize linking globally. Lastly, based on preliminary studies and experiments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference. 4.4 State-of-Art systems We take three state-of-art NER systems: NereL (Sil and Yates, 2013), UIUC NER (Ratinov and Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes them to sets of connected compo885 Dataset System Prec. Recall F1 CoNLL’03 Stanford 95.1 78.3 85.9 UIUC 91.2 90.5 90.8 NereL 86.8 89.5 88.2 JERL,,,, 90.0 89.9 89.9 JERL 91.5 91.4 91.2 Table 3: NER evaluation results nents, then trains a maximum-entropy model to re-rank different assignments. UIUC NER uses a regularized averaged perceptron model and external gazetteers to achieve strong performance. In Addition, NereL also uses UIUC NER to generate mentions. Stanford NER uses Conditional Random Fields and Gibbs sampling to incorporate non-l</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
</authors>
<title>A generative entitymention model for linking entities with knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>945--954</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Han, Le Sun, 2011</marker>
<rawString>Xianpei Han and Le Sun. 2011. A generative entitymention model for linking entities with knowledge base. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 945– 954. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
<author>Jun Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>765--774</pages>
<publisher>ACM.</publisher>
<marker>Han, Le Sun, Zhao, 2011</marker>
<rawString>Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 765–774. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782–792. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank R Kschischang</author>
<author>Brendan J Frey</author>
<author>H-A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm. Information Theory,</title>
<date>2001</date>
<journal>IEEE Transactions on,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="12918" citStr="Kschischang et al., 2001" startWordPosition="2137" endWordPosition="2140">gmentation assignment s. Here Y is the set of types NER to predict. xsj = (xuj ... xvj) 881 is the corresponding word sequence to sj, and £sj = {ej,k} is a set of entities in the knowledge base (KB), which may be referred by word sequence xsj in the entity linking task. Each entity ej,k is associated with a label yej,k E {0, 1}. Label yej,k takes 1 iff xsj referring to entity ej,k, and 0 otherwise. If xsj does not refer to any entity in the KB, yej,0 takes 1, which is analogous to the NIL3 identifier in entity linking. Based on the preliminaries and notations, Figure 1 shows the factor graph (Kschischang et al., 2001) of JERL. There are similar factor nodes for every (uj, vj, yej,k), we only show the first one (uj, vj, yej,0) for clarity. Given x, let a = (s, y, ye) be a joint assignment, and g(x, j, a) be local functions for xsj, namely features, each of which maps an assignment a to a measurement gk(x, j, a) E R. Then G(x, a) = P|s |j=1 g(x, j, a) is the factor graph defining a probability distribution of assignment a conditioned on word sequence x. Then JERL, conditional probability of a over x, is defined as: Z(x) where w is the weight vector corresponding to G will be learned later, and Z(x) is the no</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>Frank R Kschischang, Brendan J Frey, and H-A Loeliger. 2001. Factor graphs and the sum-product algorithm. Information Theory, IEEE Transactions on, 47(2):498–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="27230" citStr="Kulkarni et al., 2009" startWordPosition="4635" endWordPosition="4638">onnected compo885 Dataset System Prec. Recall F1 CoNLL’03 Stanford 95.1 78.3 85.9 UIUC 91.2 90.5 90.8 NereL 86.8 89.5 88.2 JERL,,,, 90.0 89.9 89.9 JERL 91.5 91.4 91.2 Table 3: NER evaluation results nents, then trains a maximum-entropy model to re-rank different assignments. UIUC NER uses a regularized averaged perceptron model and external gazetteers to achieve strong performance. In Addition, NereL also uses UIUC NER to generate mentions. Stanford NER uses Conditional Random Fields and Gibbs sampling to incorporate non-local features into its model. For entity linking systems, NereL, Kul09 (Kulkarni et al., 2009) and Hof11 (Hoffart et al., 2011) are compared with our models. NereL achieves the best precision@1. Kul09 formulates the local compatibility and global coherence in entity linking, and optimizes the overall entity assignment for all entities in a document via a local hill-climbing approach. Hof11 unifies the prior probability of an entity being mentioned, the similarity between context and entity, and the coherence between entity candidates among all mentions in a dense graph. 4.5 Results Table 3 shows the performance of different NER systems on the CoNLL’03 testb data set. We refer the numbe</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457–466. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="8300" citStr="Lafferty et al. (2001)" startWordPosition="1344" endWordPosition="1347">ail; section 4 describes experiments, results, and analysis; and section 5 concludes. 2 Related Work The NER problem has been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works o</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="8436" citStr="Liang (2005)" startWordPosition="1369" endWordPosition="1370">symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and</context>
<context position="23681" citStr="Liang, 2005" startWordPosition="4064" endWordPosition="4065">names Entity priors Entity name priors Entity priors over names Context scores Geo distance Related entities Mutual Type-category correlation Table 2: JERL feature list 4.3 JERL Implementation Table 2 shows features used in our models. JERL uses all features in the three categories, while JERLner and JERLej use only one corresponding category. All three models are trained on the train and development set, and evaluated on the test set of CoNLL’03/AIDA. 4.3.1 NER Features in the NER category are relevant to NER. We considered the most commonly used features in literatures (Finkel et al., 2005; Liang, 2005; Ratinov and Roth, 2009). We collect several known name lists, like popular English first/last names for people, organization lists and so on from Wikipedia and Freebase. UIUC NER’s lists are also included. In addition, we extract entity name lists from the knowledge base we used for entity linking, and construct 655 more lists. Although those lists are noisy, we find that statistically they do improve the performance of our NER baseline by a significant amount. 4.3.2 Linking Features in linking category are relevant to entity linking. An entity can be referred by its canonical name, nick nam</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1030--1038</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8568" citStr="Lin and Wu (2009)" startWordPosition="1392" endWordPosition="1395"> MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linki</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1030–1038. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="16497" citStr="Liu and Nocedal, 1989" startWordPosition="2761" endWordPosition="2764">scribe how to conduct parameter estimation for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(xt, at)}Nt=1, the goal of parameter estimation is to find optimal w∗ to maximize the joint probability of the assignments {at} over {xt}. N w∗ = argmaxw∈&lt;|G |Y P(at|xt, w) t=1 We use conditional log likelihood with `2 norm as the objective function in training, L(T, w) = Pt logP(at|xt, w) − 1 2σ2 ||w||22 The above function is concave, adding regularization to ensure that it has exactly one global optimum. We adopt a limited-memory quasi-Newton method (Liu and Nocedal, 1989) to solve the optimization problem. 1 P(a|x, w) = ew·G(x,a) (1) over s, and can be rewritten as, 882 The gradient of L(T, w) is derived as, As shown in Figure 1, our model’s factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P(a0j|xt, w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1. Algorit</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
<author>Charles Sutton</author>
</authors>
<title>Dynamic conditional random fields for jointly labeling multiple sequences. In nips workshop on syntax, semantics and statistics.</title>
<date>2003</date>
<contexts>
<context position="9730" citStr="McCallum et al., 2003" startWordPosition="1587" endWordPosition="1590">t al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and named entity recognition together. Yu et al. (2011) work on jointly entity identification and relation extraction from Wikipedia. Sil’s (2013) work on jointly NER and linking is described in the introduction section of this paper. It is worth noting that joint optimization does not always work. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to encourage jointly optimize parsing and semantic role labeling, but the top performing systems decoup</context>
</contexts>
<marker>McCallum, Rohanimanesh, Sutton, 2003</marker>
<rawString>Andrew McCallum, Khashayar Rohanimanesh, and Charles Sutton. 2003. Dynamic conditional random fields for jointly labeling multiple sequences. In nips workshop on syntax, semantics and statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8422" citStr="Ratinov and Roth, 2009" startWordPosition="1365" endWordPosition="1368">been widely addressed by symbolic, statistical, as well as hybrid approaches. It has been encouraged by several editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population t</context>
<context position="23706" citStr="Ratinov and Roth, 2009" startWordPosition="4066" endWordPosition="4069">priors Entity name priors Entity priors over names Context scores Geo distance Related entities Mutual Type-category correlation Table 2: JERL feature list 4.3 JERL Implementation Table 2 shows features used in our models. JERL uses all features in the three categories, while JERLner and JERLej use only one corresponding category. All three models are trained on the train and development set, and evaluated on the test set of CoNLL’03/AIDA. 4.3.1 NER Features in the NER category are relevant to NER. We considered the most commonly used features in literatures (Finkel et al., 2005; Liang, 2005; Ratinov and Roth, 2009). We collect several known name lists, like popular English first/last names for people, organization lists and so on from Wikipedia and Freebase. UIUC NER’s lists are also included. In addition, we extract entity name lists from the knowledge base we used for entity linking, and construct 655 more lists. Although those lists are noisy, we find that statistically they do improve the performance of our NER baseline by a significant amount. 4.3.2 Linking Features in linking category are relevant to entity linking. An entity can be referred by its canonical name, nick names, alias, and first/last</context>
<context position="26497" citStr="Ratinov and Roth, 2009" startWordPosition="4520" endWordPosition="4523">tributes 0.2 points to the final NER F1 score. Non-local features are also considered in linking (Ratinov et al., 2011; Han et al., 2011). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL’03/AIDA data set is not obvious, we do not optimize linking globally. Lastly, based on preliminary studies and experiments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference. 4.4 State-of-Art systems We take three state-of-art NER systems: NereL (Sil and Yates, 2013), UIUC NER (Ratinov and Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes them to sets of connected compo885 Dataset System Prec. Recall F1 CoNLL’03 Stanford 95.1 78.3 85.9 UIUC 91.2 90.5 90.8 NereL 86.8 89.5 88.2 JERL,,,, 90.0 89.9 89.9 JERL 91.5 91.4 91.2 Table 3: NER evaluation results nents, then trains a maximum-entropy model to re-rank different assignments. UIUC NER uses a regularized averaged perceptron model and external gazetteers to achieve strong performance. In Addition, NereL also uses UIUC NER to generate mentions. Stanford NER uses Conditional Random Fields </context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147– 155. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1375--1384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9141" citStr="Ratinov et al., 2011" startWordPosition="1487" endWordPosition="1490">d Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been p</context>
<context position="15485" citStr="Ratinov et al., 2011" startWordPosition="2596" endWordPosition="2599">entities. These features encourage predicting coherent outputs for NER and linking. There is one constrain for each yej that the corresponding xsj can refer to only one entity ej,k E £sj or NIL. This is equivalent toP|Esj | k=0 yej,k = 1. Based on the above description, G(x, a) in equation 1 is the sum of conjunction (gner, gel, gcr) G(x, a) = P|s| j=1( gner(x, sj, yj, yj−1) , P|Ej| k=0 gel(x, sj, yej,k) , P|Ej| k=0 gcr(x, yj, ye j,k) ) In summary, JERL jointly models the NER and linking, and leverages mutual dependency between them to predict coherent outputs. Previous works (Cucerzan, 2007; Ratinov et al., 2011; Sil and Yates, 2013) on linking argued that entity linking systems often suffer because of errors involved in mention detection phrase, especially false negative errors, and try to mitigate it via overgenerating mention candidates. From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a. 3.2 Parameter Estimation We describe how to conduct parameter estimation for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(xt, at)}Nt=1, the goal of parameter estimation is to find optimal </context>
<context position="25992" citStr="Ratinov et al., 2011" startWordPosition="4436" endWordPosition="4439"> one of the key contributions of our work. 4.3.4 Non-local features Features capturing long distance dependency between hidden labels are classified as non-local features. Those features are very helpful in improving NER system performance but are costly. Since this is not the focus of this paper, we take a simple approach to incorporate non-local features. We cache history results of previous sentences in a 1000 words window, and adopt several heuristic rules for personal names. This approach contributes 0.2 points to the final NER F1 score. Non-local features are also considered in linking (Ratinov et al., 2011; Han et al., 2011). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL’03/AIDA data set is not obvious, we do not optimize linking globally. Lastly, based on preliminary studies and experiments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference. 4.4 State-of-Art systems We take three state-of-art NER systems: NereL (Sil and Yates, 2013), UIUC NER (Ratinov and Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes t</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1375–1384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1185--1192</pages>
<contexts>
<context position="8535" citStr="Sarawagi and Cohen, 2004" startWordPosition="1384" endWordPosition="1388"> editions of evaluation campaigns such as MUC (Chinchor and Marsh, 1998), the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and ACE (Doddington et al., 2004). Along with the improvement of Machine Learning techniques, statistical approaches have become a major direction for research on NER, especially after Conditional Random Field is proposed by Lafferty et al. (2001). The well known state-of-art NER systems are Stanford NER (Finkel et al., 2005) and UIUC 880 NER (Ratinov and Roth, 2009). Liang (2005) compares the performance of the 2nd order linear chain CRF and Semi-CRF (Sarawagi and Cohen, 2004) in his thesis. Lin and Wu (2009) cluster tens of millions of phrases and use the resulting clusters as features in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al.</context>
<context position="16859" citStr="Sarawagi and Cohen, 2004" startWordPosition="2826" endWordPosition="2829">`2 norm as the objective function in training, L(T, w) = Pt logP(at|xt, w) − 1 2σ2 ||w||22 The above function is concave, adding regularization to ensure that it has exactly one global optimum. We adopt a limited-memory quasi-Newton method (Liu and Nocedal, 1989) to solve the optimization problem. 1 P(a|x, w) = ew·G(x,a) (1) over s, and can be rewritten as, 882 The gradient of L(T, w) is derived as, As shown in Figure 1, our model’s factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P(a0j|xt, w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1. Algorithm 1: JERL parameter estimation input : training data T = {(xt, at)}Nt=1 output: the optimal w w ← 0; while weight w is not converged do Z ← 0; w0 ← 0; for t ← 1 to N do calculate αt, βt according to eq.3; calculate Zt according to eq.4 calculate w0t according to eq.2, 5; Z ← Z + Zt; w0 ← w0 + w0t; end update w to maximize log likelihood L(T, w) under (Z, w0) </context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W Cohen. 2004. Semimarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, pages 1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>134--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16819" citStr="Sha and Pereira, 2003" startWordPosition="2820" endWordPosition="2823"> use conditional log likelihood with `2 norm as the objective function in training, L(T, w) = Pt logP(at|xt, w) − 1 2σ2 ||w||22 The above function is concave, adding regularization to ensure that it has exactly one global optimum. We adopt a limited-memory quasi-Newton method (Liu and Nocedal, 1989) to solve the optimization problem. 1 P(a|x, w) = ew·G(x,a) (1) over s, and can be rewritten as, 882 The gradient of L(T, w) is derived as, As shown in Figure 1, our model’s factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P(a0j|xt, w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1. Algorithm 1: JERL parameter estimation input : training data T = {(xt, at)}Nt=1 output: the optimal w w ← 0; while weight w is not converged do Z ← 0; w0 ← 0; for t ← 1 to N do calculate αt, βt according to eq.3; calculate Zt according to eq.4 calculate w0t according to eq.2, 5; Z ← Z + Zt; w0 ← w0 + w0t; end update w to maximi</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 134– 141. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avirup Sil</author>
<author>Alexander Yates</author>
</authors>
<title>Re-ranking for joint named-entity recognition and linking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>2369--2374</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9385" citStr="Sil and Yates (2013)" startWordPosition="1531" endWordPosition="1534"> on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and named entity recognition together. Yu et al. (2011) work on jointly entity identification and relation extraction from</context>
<context position="15507" citStr="Sil and Yates, 2013" startWordPosition="2600" endWordPosition="2603">es encourage predicting coherent outputs for NER and linking. There is one constrain for each yej that the corresponding xsj can refer to only one entity ej,k E £sj or NIL. This is equivalent toP|Esj | k=0 yej,k = 1. Based on the above description, G(x, a) in equation 1 is the sum of conjunction (gner, gel, gcr) G(x, a) = P|s| j=1( gner(x, sj, yj, yj−1) , P|Ej| k=0 gel(x, sj, yej,k) , P|Ej| k=0 gcr(x, yj, ye j,k) ) In summary, JERL jointly models the NER and linking, and leverages mutual dependency between them to predict coherent outputs. Previous works (Cucerzan, 2007; Ratinov et al., 2011; Sil and Yates, 2013) on linking argued that entity linking systems often suffer because of errors involved in mention detection phrase, especially false negative errors, and try to mitigate it via overgenerating mention candidates. From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a. 3.2 Parameter Estimation We describe how to conduct parameter estimation for JERL in this section. Given independent and identically distributed (i.i.d.) training data T = {(xt, at)}Nt=1, the goal of parameter estimation is to find optimal w∗ to maximize the joi</context>
<context position="26462" citStr="Sil and Yates, 2013" startWordPosition="4514" endWordPosition="4517">ersonal names. This approach contributes 0.2 points to the final NER F1 score. Non-local features are also considered in linking (Ratinov et al., 2011; Han et al., 2011). We try several features, which has been proved to be helpful in TAC data set. However, the gain on CoNLL’03/AIDA data set is not obvious, we do not optimize linking globally. Lastly, based on preliminary studies and experiments, we set the maximum segmentation length to 6 and max candidate count per segmentation to 5 for efficient training and inference. 4.4 State-of-Art systems We take three state-of-art NER systems: NereL (Sil and Yates, 2013), UIUC NER (Ratinov and Roth, 2009) and Stanford NER (Finkel et al., 2005). NereL firstly over generates mentions and decomposes them to sets of connected compo885 Dataset System Prec. Recall F1 CoNLL’03 Stanford 95.1 78.3 85.9 UIUC 91.2 90.5 90.8 NereL 86.8 89.5 88.2 JERL,,,, 90.0 89.9 89.9 JERL 91.5 91.4 91.2 Table 3: NER evaluation results nents, then trains a maximum-entropy model to re-rank different assignments. UIUC NER uses a regularized averaged perceptron model and external gazetteers to achieve strong performance. In Addition, NereL also uses UIUC NER to generate mentions. Stanford </context>
<context position="27889" citStr="Sil and Yates (2013)" startWordPosition="4745" endWordPosition="4748">ompared with our models. NereL achieves the best precision@1. Kul09 formulates the local compatibility and global coherence in entity linking, and optimizes the overall entity assignment for all entities in a document via a local hill-climbing approach. Hof11 unifies the prior probability of an entity being mentioned, the similarity between context and entity, and the coherence between entity candidates among all mentions in a dense graph. 4.5 Results Table 3 shows the performance of different NER systems on the CoNLL’03 testb data set. We refer the numbers of state-of-art systems reported by Sil and Yates (2013). Stanford NER achieves the best precision, but its recall is low. UIUC reports the (almost) best recorded F1. JERLner considers features only in the NER category, which could be treated as a pure NER system implemented in Semi-CRF. Actually CRF-based implementation with a similar feature set has comparable performance. Our baseline JERLner is strong enough. We argue that that it is mainly because of the additional dictionaries derived from the knowledge base. JERL further pushes the F1 to 91.2, which outperforms UIUC by 0.4 points in F1 score. To the best of our knowledge, it is the best F1 o</context>
</contexts>
<marker>Sil, Yates, 2013</marker>
<rawString>Avirup Sil and Alexander Yates. 2013. Re-ranking for joint named-entity recognition and linking. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 2369–2374. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosa Stern</author>
<author>Benoit Sagot</author>
<author>Fr´ed´eric B´echet</author>
</authors>
<title>A joint named entity recognition and entity linking system.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,</booktitle>
<pages>52--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Stern, Sagot, B´echet, 2012</marker>
<rawString>Rosa Stern, Benoit Sagot, and Fr´ed´eric B´echet. 2012. A joint named entity recognition and entity linking system. In Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data, pages 52–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The conll-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning. Introduction to statistical relational learning,</title>
<date>2006</date>
<pages>93--128</pages>
<contexts>
<context position="17024" citStr="Sutton and McCallum, 2006" startWordPosition="2850" endWordPosition="2853">s exactly one global optimum. We adopt a limited-memory quasi-Newton method (Liu and Nocedal, 1989) to solve the optimization problem. 1 P(a|x, w) = ew·G(x,a) (1) over s, and can be rewritten as, 882 The gradient of L(T, w) is derived as, As shown in Figure 1, our model’s factor graph is a tree, which means the calculation of the gradient is tractable. Inspired by the forward backward algorithm (Sha and Pereira, 2003) and Semi-CRF (Sarawagi and Cohen, 2004), we leverage dynamic programming techniques to compute the normalization factor Zw and marginal probability P(a0j|xt, w) when w is given.(Sutton and McCallum, 2006) The parameter estimation algorithm is abstracted in Algorithm 1. Algorithm 1: JERL parameter estimation input : training data T = {(xt, at)}Nt=1 output: the optimal w w ← 0; while weight w is not converged do Z ← 0; w0 ← 0; for t ← 1 to N do calculate αt, βt according to eq.3; calculate Zt according to eq.4 calculate w0t according to eq.2, 5; Z ← Z + Zt; w0 ← w0 + w0t; end update w to maximize log likelihood L(T, w) under (Z, w0) via L-BFGS; end Let αi,y (i ∈ [0, |x|], y ∈ Y) be the sum of potential functions of all possible assignments over (x1 ... xi) whose last segmentation’s labels are y.</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. Introduction to statistical relational learning, pages 93–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,</booktitle>
<pages>142--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142–147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longyue Wang</author>
<author>Shuo Li</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
</authors>
<title>A joint chinese named entity recognition and disambiguation system.</title>
<date>2012</date>
<booktitle>CLP 2012,</booktitle>
<pages>146</pages>
<contexts>
<context position="9266" citStr="Wang et al. (2012)" startWordPosition="1511" endWordPosition="1514">in NER reporting the best performance on the CoNLL’03 English NER data set. Recent works on NER have started to focus on multi-lingual named entity recognition or NER on short text, e.g. Twitter. Entity linking was initiated with Wikipediabased works on entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). This task is encouraged by the TAC 2009 KB population task1 first and receives more and more attention from the research community (Hoffart et al., 2011; Ratinov et al., 2011; Han and Sun, 2011). Linking usually takes mentions detected by NER as its input. Stern et al. (2012) and Wang et al. (2012) present joint NER and linking systems and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and</context>
</contexts>
<marker>Wang, Li, Wong, Chao, 2012</marker>
<rawString>Longyue Wang, Shuo Li, Derek F Wong, and Lidia S Chao. 2012. A joint chinese named entity recognition and disambiguation system. CLP 2012, page 146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yu</author>
<author>Irwin King</author>
<author>Michael R Lyu</author>
</authors>
<title>Towards a top-down and bottom-up bidirectional approach to joint information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management,</booktitle>
<pages>847--856</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9918" citStr="Yu et al. (2011)" startWordPosition="1617" endWordPosition="1620">ms and evaluate their systems on French and Chinese data sets. Sil and Yates (2013) take a re-ranking based approach and achieve the best result on the AIDA data set. In 2014, Microsoft and Google jointly hosted “Entity Recognition and Disambiguation Challenge” which focused on the end to end performance of linking system 2. Joint optimization models have been studied at great length. E.g. Dynamic CRF (McCallum et al., 2003) has been proposed to conduct Partof-Speech Tagging and Chunking tasks together. Finkel and Manning (2009) show how to model parsing and named entity recognition together. Yu et al. (2011) work on jointly entity identification and relation extraction from Wikipedia. Sil’s (2013) work on jointly NER and linking is described in the introduction section of this paper. It is worth noting that joint optimization does not always work. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to encourage jointly optimize parsing and semantic role labeling, but the top performing systems decoupled the two tasks. 3 Joint Entity Recognition and Linking Named entity recognition is usually formalized as a sequence labeling task, in which each word is classified to not-an-entity or e</context>
</contexts>
<marker>Yu, King, Lyu, 2011</marker>
<rawString>Xiaofeng Yu, Irwin King, and Michael R Lyu. 2011. Towards a top-down and bottom-up bidirectional approach to joint information extraction. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 847– 856. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
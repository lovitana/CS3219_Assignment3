<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023487">
<title confidence="0.995039">
Cross-document Event Coreference Resolution
based on Cross-media Features
</title>
<author confidence="0.999637">
Tongtao Zhang&apos;, Hongzhi Lie, Heng Ji&apos;, Shih-Fu Chang&apos;
</author>
<affiliation confidence="0.998869">
1Computer Science Department, Rensselaer Polytechnic Institute
</affiliation>
<email confidence="0.840291">
{zhangt13, jih}@rpi.edu
</email>
<affiliation confidence="0.994686">
2Department of Computer Science, Columbia University
</affiliation>
<email confidence="0.999145">
{hongzhi.li, shih.fu.chang}@columbia.edu
</email>
<sectionHeader confidence="0.993961" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99890025">
In this paper we focus on a new problem of
event coreference resolution across televi-
sion news videos. Based on the observa-
tion that the contents from multiple data
modalities are complementary, we develop
a novel approach to jointly encode effec-
tive features from both closed captions
and video key frames. Experiment re-
sults demonstrate that visual features pro-
vided 7.2% absolute F-score gain on state-
of-the-art text based event extraction and
coreference resolution.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999102">
TV news is the medium that broadcasts events,
stories and other information via television. The
broadcast is conducted in programs with the name
of “Newscast”. Typically, newscasts require one
or several anchors who are introducing stories and
coordinating transition among topics, reporters or
journalists who are presenting events in the fields
and scenes that are captured by cameramen. Sim-
ilar to newspapers, the same stories are often re-
ported by multiple newscast agents. Moreover, in
order to increase the impact on audience, the same
stories and events are reported for mutliple times.
TV audience passively receives redundant infor-
mation, and often has difficulty in obtaining clear
and useful digest of ongoing events. These proper-
ties lead to needs for automatic methods to cluster
information and remove redundancy. We propose
a new research problem of event coreference reso-
lution across multiple news videos.
To tackle this problem, a good starting point
is processing the Closed Captions (CC) which
is accompanying videos in newcasts. The CC
is either generated by automatic speech recogni-
tion (ASR) systems or transcribed by a human
stenotype operator who inputs phonetics which are
</bodyText>
<figureCaption confidence="0.906094">
Figure 1: Similar visual contents improve detec-
tion of a coreferential event pair which has a low
text-based confidence score.
</figureCaption>
<bodyText confidence="0.995328736842105">
Closed Captions: “It ’s not clear when it was
killed.”; “Jordan just executed two ISIS prisoners,
direct retaliation for the capture of the killing
Jordanian pilot.”
instantly and automatically translated into texts,
where events can be extracted. There exist some
previous event coreference resolution work such
as (Chen and Ji, 2009b; Chen et al., 2009; Lee et
al., 2012; Bejan and Harabagiu, 2010). However,
they only focused on formally written newswire
articles and utilized textual features. Such ap-
proaches do not perform well on CC due to (1).
the propagated errors from upper stream compo-
nents (e.g., automatic speech/stenotype recogni-
tion and event extraction); (2). the incomplete-
ness of information. Different from written news,
newscasts are often limited in time due to fixed
TV program schedules, thus, anchors and journal-
ists are trained and expected to organize reports
</bodyText>
<page confidence="0.976016">
201
</page>
<note confidence="0.658176">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 201–206,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99997324489796">
which are comprehensively informative with com-
plementary visual and CC descriptions within a
short time. These two sides have minimal over-
lapped information while they are inter-dependent.
For example, anchors and reporters introduce the
background story which are not presented in the
videos, and thus the events extracted from CC of-
ten lack information about participants.
For example, as shown in Figure 1, these two
Conflict.Attack event mentions are coreferential.
However, in the first event mention, a mistake
in Closed Caption (“he was killed” → “it was
killed”) makes event extraction and text based
coreference systems unable to detect and link “it”
to the entity of “Jordanian pilot”. Fortunately,
videos often illustrate brief descriptions by vivid
visual contents. Moreover, diverse anchors, re-
porters and TV channels tend to use similar or
identical video contents to describe the same story,
even though they usually use different words and
phrases. Therefore, the challenges in coreference
resolution methods based on text information can
be addressed by incorporating visual similarity. In
this example, the visual similarity between the cor-
responding video frames is high because both of
them show the scene of the Jordanian pilot.
Similar work such as (Kong et al., 2014), (Ra-
manathan et al., 2014), (Motwani and Mooney,
2012) and (Ramanathan et al., 2013) have ex-
plored methods of linking visual materials with
texts. However, these methods mainly focus on
connecting image concepts with entities in text
mentions; and some of them do not clearly distin-
guish entity and event in the documents since the
definition of visual concepts often require both of
them. Moreover, the aforementioned work mainly
focuses on improving visual contents recognition
by introducing text features while our work will
take the opposite route, which takes advantage of
visual information to improve event coreference
resolution.
In this paper, we propose to jointly incorporate
features from both speech (textual) and video (vi-
sual) channels for the first time. We also build a
newscast crawling system that can automatically
accumulate video records and transcribe closed
captions. With the crawler, we created a bench-
mark dataset which is fully annotated with cross-
document coreferential events 1.
</bodyText>
<footnote confidence="0.923494">
1Dataset can be found at
http://www.ee.columbia.edu/dvmm/newDownloads.htm
</footnote>
<sectionHeader confidence="0.969513" genericHeader="introduction">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.999086">
2.1 Event Extraction
</subsectionHeader>
<bodyText confidence="0.99603475">
Given unstructured transcribed CC, we extract en-
tities and events and present them in structured
forms. We follow the terminologies used in ACE
(Automatic Content Extraction) (NIST, 2005):
</bodyText>
<listItem confidence="0.999434857142857">
• Entity: an object or set of objects in the world,
such as person, organization and facility.
• Entity mention: words or phrases in the texts
that mention an entity.
• Event: a specific occurrence involving partici-
pants.
• Event trigger: the word that most clearly ex-
presses an event’s occurrence.
• Event argument: an entity, or a temporal expres-
sion or a value that has a certain role (e.g., Time-
Within, Place) in an event.
• Event mention: a sentence (or a text span ex-
tent) that mentions an event, including a distinct
trigger and arguments involved.
</listItem>
<subsectionHeader confidence="0.99519">
2.2 Text based Event Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999987758620689">
Coreferential events are defined as the same spe-
cific occurrence mentioned in different sentences,
documents and transcript texts. Coreferential
events should happen in the same place and within
the same time period, and the entities involved and
their roles should be identical. From the perspec-
tive of extracted events, each specific attribute and
argument from those events should match. How-
ever, mentions for the same event may appear in
forms of diverse words and phrases; and they do
not always cover all arguments or attributes.
To tackle these challenges, we adopt a Maxi-
mum Entropy (MaxEnt) model as in (Chen and
Ji, 2009b). We consider every pair of event men-
tions which share the same event type as a can-
didate and exploit features proposed in (Chen and
Ji, 2009b; Chen et al., 2009). Note that the goal
in (Chen and Ji, 2009b; Chen et al., 2009) was
to resolve event coreference within the same doc-
ument, whereas our scenario yields to a cross-
document/video transcript setting, so we remove
some improper and invalid features. We also in-
vestigated the approaches by (Lee et al., 2012)
and (Bejan and Harabagiu, 2010), but the con-
fidence estimation results from these alternative
methods are not reliable. Moreover, the input
of event coreference are automatic results from
event extraction instead of gold standard, so the
noise and errors significantly impact the corefer-
</bodyText>
<page confidence="0.989027">
202
</page>
<bodyText confidence="0.999869">
ence performance, especially for unsupervised ap-
proaches (Bejan and Harabagiu, 2010). Neverthe-
less, we still incorporate features from the afore-
mentioned methods. Table 1 shows the features
that constitute the input of the MaxEnt model.
</bodyText>
<subsectionHeader confidence="0.999586">
2.3 Visual Similarity
</subsectionHeader>
<bodyText confidence="0.999975525">
Visual content provides useful cues complemen-
tary with those used in text-based approach in
event coreference resolution. For example, two
coreferential events typically show similar or even
duplicate scenes, objects, and activities in the vi-
sual channel. Coherence of such visual content
has been used in grouping multiple video shots
into the same video story (Hsu et al., 2003), but
it has not been used for event coreference res-
olution. Recent work in computer vision has
demonstrated tremendous progress in large-scale
visual content recognition. In this work, we adopt
the state-of-the-art techniques (Krizhevsky et al.,
2012) and (Simonyan and Zisserman, 2014) that
train robust convolutional neural networks (CNN)
over millions of web images to detect 20,000 se-
mantic categories defined in ImageNet (Deng et
al., 2009) from each image. The 2nd to the
last layer features from such deep network can
be considered as high-level visual representation
that can be used to discriminate various seman-
tic classes (scenes, objects, activity). It has been
found effective in computing visual similarity be-
tween images, by directly computing the L2 dis-
tance of such features or through further met-
ric learning. To compute the similarity between
videos associated with two candidate event men-
tions, we sample multiple frames from each video
and aggregate the similarity scores of the few
most similar image pairs between the videos. Let
{fZ1, fZ2, ..., , fZl } be the key frames sampled from
video VZ and {fj1, fj2, ..., , fjl } be key frames sam-
pled from video Vj. All the frames are resized to a
fixed resolution of 256 x 256 and fed into our pre-
trained CNN model. We get the high-level visual
representation Fm = FC7(fm) for each frame fm
from the output of the 2nd to the last fully con-
nected layer (FC7) of CNN model. Fm is a 4096
dimension vector. The visual distance of frames
fm and fn is defined by L2 distance, which is
</bodyText>
<equation confidence="0.947423">
Dmn = ||FC7(fZm) − FC7(fjn)||2. (1)
</equation>
<bodyText confidence="0.908045">
The distance of video pair (VZ, Vj) is computed as
</bodyText>
<equation confidence="0.9803765">
1 E ¯DZj = k ∗ Dmn (2)
(f.,fn)
</equation>
<bodyText confidence="0.999862857142857">
, where (fm, fn) is the top k of most similar frame
pairs. In our experiment, we use k = 3. Such
aggregation method among the top matches is in-
tended to capture similarity between videos that
share only partially overlapped content.
Each news video story typically starts with an
introduction by an anchor person followed by
news footages showing the visual scenes or activ-
ities of the event. Therefore, when computing vi-
sual similarity, it’s important to exclude the anchor
shot and focus on the story-related clips. Anchor
frame detection (Hsu et al., 2003) is a well studied
problem. In order to detect anchor frames auto-
matically, a face detector is applied to all I-frames
of a video. We can obtain the location and size
of each detected face. After checking the tempo-
ral consistency of the detected faces within each
shot, we get a set of candidate anchor faces. The
detected face regions are further extended to re-
gions of interest that may include hair and upper
body. All the candidate faces detected from the
same video are clustered based on their HSV color
histogram. It is reasonable to assume that the most
frequent face cluster is the one corresponding to
the anchor faces. Once the anchor frames are de-
tected, they are excluded and only the non-anchor
frames are used to compute the visual similarity
between videos associated with event mentions.
</bodyText>
<subsectionHeader confidence="0.986072">
2.4 Joint Re-ranking
</subsectionHeader>
<bodyText confidence="0.9938582">
Using the visual distance calculated from Sec-
tion 2.3, we can rerank the confidence values from
Section 2.2 using the text-based MaxEnt model.
We use the following empirical equation to adjust
the confidence:
W 0 Zj =WZj∗e−
where WZj denotes the original coreference con-
fidence between event mentions i and j, DZj de-
notes the visual distance between the correspond-
ing video frames where the event mentions were
spoken and α is a parameter which is used to ad-
just the impact of visual distance. In the current
implementation, we empirically set it as the aver-
age of pair-wised visual distances between videos
of all event coreference candidates. With this α
</bodyText>
<equation confidence="0.964646333333333">
DS3 +1
α
, (3)
</equation>
<page confidence="0.984856">
203
</page>
<table confidence="0.459503894736842">
Category Features Remarks (EMi: the first event mention, EMS: the sec-
ond event mention)
Baseline type subtype pair of event type and subtype in EMi
trigger pair trigger pair of EMi and EMS
pos pair part-of-speech pair of triggers of EMi and EMS
nominal 1 if the trigger of EMi is nominal
nom number “plural” or “singular” if the trigger of EMi is nominal
pronominal 1 if the trigger of EMi is pronominal
exact match 1 if the trigger spelling in EMi matches that in EMS
stem match 1 if the trigger stem in EMi matches that in EMS
trigger sim the semantic similarity scores between triggers of EMi
and EMS using WordNet(Miller, 1995)
Arguments argument match 1 if arguments holding the same roles in both EMi and
EMS matches
Attributes mod,pol,gen,ten four event attributes in EMi: modality, polarity, gener-
icity and tense
mod conflict, 1 if the attributes of EMi and EMS conflict
pol conflict, gen conflict,
ten conflict
</table>
<tableCaption confidence="0.963011">
Table 1: Features for Event Coreference Resolution
</tableCaption>
<bodyText confidence="0.9996458">
we generally enhance the confidence of event pairs
with small visual distances and penalize those with
large ones. An alternative way for setting the alpha
parameter is through cross validation over separate
data partitions.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999821">
3.1 Data and Setting
</subsectionHeader>
<bodyText confidence="0.9999689">
We establish a system that actively monitors over
100 U.S. major broadcast TV channels such as
ABC, CNN and FOX, and crawls newscasts from
these channels for more than two years (Li et
al., 2013a). With this crawler, we retrieve 100
videos and their correspondent transcribed CC
with the topic of “ISIS”2. This system also tem-
porally aligns the CC text with the transcribed text
from automatic speech recognition following the
methods in (Huang et al., 2003). This provides ac-
curate time alignment between the CC text and the
video frames. As CC consists of capitalized let-
ters, we apply the true-casing tool from Standford
CoreNLP (Manning et al., 2014) on CC. Then we
apply a state-of-the-art event extraction system (Li
et al., 2013b; Li et al., 2014) to extract event men-
tions from CC. We asked two human annotators
to investigate all event pairs and annotate coref-
erential pairs as the ground truth. Kappa coeffi-
cient for measuring inter-annotator agreement is
</bodyText>
<subsectionHeader confidence="0.47838">
2abbreviation for Islamic State of Iraq and Syria
</subsectionHeader>
<bodyText confidence="0.999625">
74.11%. In order to evaluate our system perfor-
mance, we rank the confidence scores of all event
mention pairs and present the results in Precision
vs. Detection Depth curve. Finally we find the
video frames corresponding to the event mentions,
remove the anchor frames and calculate the visual
similarity between the videos. Our final dataset
consists of 85 videos, 207 events and 848 event
pairs, where 47 pairs are considered coreferential.
We adopt the MaxEnt-based coreference reso-
lution system from (Chen and Ji, 2009b; Chen et
al., 2009) as our baseline, and use ACE 2005 En-
glish Corpus as the training set for the model. A
5-fold cross-validation is conducted on the train-
ing set and the average f-score is 56%. It is lower
than results from (Chen and Ji, 2009a) since we
remove some features which are not available for
the cross-document scenario.
</bodyText>
<subsectionHeader confidence="0.903952">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.998485">
The peak F-score for the baseline system is
44.23% while our cross-media method boosts it
to 51.43%. Figure 2 shows the improvement af-
ter incorporating the visual information. We adopt
Wilcoxon signed-rank test to determine the signif-
icance between the pairs of precision scores at the
same depth. The z-ratio is 3.22, which shows the
improvement is significant.
For example, the event pair “So why hasn’t
U.S. air strikes targeted Kobani within the city
</bodyText>
<page confidence="0.998286">
204
</page>
<figureCaption confidence="0.965455666666667">
Figure 2: Performance comparison between base-
line and our cross-media method on top 150 pairs.
Circles indicate the peak F-scores.
</figureCaption>
<bodyText confidence="0.9994157">
limits” and “Our strikes continue alongside our
partners.” was mistakenly considered coreferen-
tial by text features. In fact, the former “strikes”
mentions the airstrike and the latter refers to the
war or battle, therefore, they are not coreferential.
The corresponding video shots demonstrate two
different scenes: the former one shows bombing
while the latter shows that the president is giving
a speech about the strike. Thus the visual distance
successfully corrected this error.
</bodyText>
<subsectionHeader confidence="0.993939">
3.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99978148275862">
However, from Figure 2 we can also notice that
there are still some errors caused by the vi-
sual features. One major error type resides in
the negative pairs with both “relatively” high
textual coreference confidence scores and “rela-
tively” high visual similarity. From the text side,
the event pair contains similar events, for exam-
ple: “The Penn(tagon) says coalition air strikes
in and around the Syrian city of Kobani have kill
hundreds of ISIS fighters but more are stream-
ing in even as the air campaign intensifies.” and
“Throughout the day, explosions from coalition
air strikes sent plums of smoke towering into the
sky.”. They talk about two airstrikes during differ-
ent time periods and are not coreferential, but the
baseline system produces a high rank. Our current
approach limits the image frames to those over-
lapped with the speech of an event mention, and in
this error, both videos show “battle” scene, yield-
ing a small visual distance. The aforementioned
assumption that anchors and journalists tend to use
similar videos when describing the same events ,
which may introduce risk of error caused by sim-
ilar text event mentions with similar video shots.
For such errors, one potential solution is to expand
the video frame windows to capture more events
and concepts from videos. Expanding the detec-
tion range to include visual events in the temporal
neighborhood can also differentiate the events.
</bodyText>
<subsectionHeader confidence="0.919104">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999988">
A systematic way of choosing α in Equation 3 will
be useful. One idea is to adapt the α value for dif-
ferent types of events, e.g., we expect some event
types are more visually oriented than others and
thus use a smaller α value.
We also notice the impact of the errors from the
upstream event extraction system. According to
(Li et al., 2014) the F-score of event trigger label-
ing is 65.3%, and event argument labeling is 45%.
Missing arguments in events is a main problem,
thus the performance on automatically extracted
event mentions is significantly worse. About 20
more coreferential pairs could be detected if events
and arguments are perfectly extracted.
</bodyText>
<sectionHeader confidence="0.995562" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999484375">
In this paper, we improved event coreference res-
olution on newscast speech by incorporating vi-
sual similarity. We also build a crawler that pro-
vides a benchmark dataset of videos with aligned
closed captions. This system can also help cre-
ate more datasets to conduct research on video de-
scription generation. In the future, we will focus
on improving event extraction from texts by intro-
ducing more fine-grained cross-media information
such as object, concept and event detection results
from videos. Moreover, joint detection of events
from both sides is our ultimate goal, however, we
need to explore the mapping among events from
both text and visual sides, and automatic detection
of a wide range of objects and events from news
video itself is still challenging.
</bodyText>
<sectionHeader confidence="0.953074" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99945">
This work was supported by the U.S. DARPA
DEFT Program No. FA8750-13-2-0041, ARL
NS-CTA No. W911NF-09-2-0053, NSF CA-
REER Award IIS-1523198, AFRL DREAM
project, gift awards from IBM, Google, Disney
and Bosch. The views and conclusions contained
in this document are those of the authors and
should not be interpreted as representing the of-
ficial policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.
</bodyText>
<figure confidence="0.998060875">
1
0.8
0.6
0.4
0.2
0 50 100 150
Baseline
Our Method
</figure>
<page confidence="0.995331">
205
</page>
<sectionHeader confidence="0.989098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99662946875">
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412–1422.
Zheng Chen and Heng Ji. 2009a. Event coref-
erence resolution: Algorithm, feature impact and
evaluation. In Proceedings of Events in Emerging
Text Types (eETTs) Workshop, in conjunction with
RANLP, Bulgaria.
Zheng Chen and Heng Ji. 2009b. Graph-based
event coreference resolution. In Proceedings of the
2009 Workshop on Graph-based Methods for Natu-
ral Language Processing, TextGraphs-4, pages 54–
57.
Zheng Chen, Heng Ji, and Robert Haralick. 2009.
A pairwise event coreference model, feature impact
and evaluation for event coreference resolution. In
Proceedings of the Workshop on Events in Emerging
Text Types, eETTs ’09, pages 17–22.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recog-
nition, 2009., pages 248–255.
Winston Hsu, Shih-Fu Chang, Chih-Wei Huang, Lyn-
don Kennedy, Ching-Yung Lin, and Giridharan
Iyengar. 2003. Discovery and fusion of salient mul-
timodal features toward news story segmentation. In
Electronic Imaging 2004, pages 244–258.
Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.
2003. Automatic closed caption alignment based on
speech recognition transcripts. Rapport technique,
Columbia.
Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun,
and Sanja Fidler. 2014. What are you talking about?
text-to-image coreference. In Proceedings of 2014
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3558–3565.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500.
Hongzhi Li, Brendan Jou, Jospeh G Ellis, Daniel Mo-
rozoff, and Shih-Fu Chang. 2013a. News rover:
Exploring topical structures and serendipity in het-
erogeneous multimedia news. In Proceedings of the
21st ACM international conference on Multimedia,
pages 449–450.
Qi Li, Heng Ji, and Liang Huang. 2013b. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73–82.
Qi Li, Heng Ji, Yu HONG, and Sujian Li. 2014.
Constructing information networks using one single
model. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1846–1851.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41, Novem-
ber.
Tanvi S Motwani and Raymond J Mooney. 2012.
Improving video activity recognition using object
recognition and text mining. In Proceedings of the
20th European Conference on Artificial Intelligence,
pages 600–605.
NIST. 2005. The ace 2005 evaluation plan. http:
//www.itl.nist.gov/iad/mig/tests/
ace/ace05/doc/ace05-evaplan.v3.pdf.
Vignesh Ramanathan, Percy Liang, and Li Fei-Fei.
2013. Video event understanding using natural
language descriptions. In Proceedings of 2013
IEEE International Conference on Computer Vision,
pages 905–912.
Vignesh Ramanathan, Armand Joulin, Percy Liang,
and Li Fei-Fei. 2014. Linking people in videos with
their names using coreference resolution. In Com-
puter Vision–ECCV 2014, pages 95–110.
Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.
</reference>
<page confidence="0.998836">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540726">
<title confidence="0.9976745">Cross-document Event Coreference based on Cross-media Features</title>
<author confidence="0.995214">Hongzhi Heng Shih-Fu</author>
<affiliation confidence="0.7549065">Science Department, Rensselaer Polytechnic of Computer Science, Columbia</affiliation>
<abstract confidence="0.997957076923077">In this paper we focus on a new problem of event coreference resolution across television news videos. Based on the observation that the contents from multiple data modalities are complementary, we develop a novel approach to jointly encode effective features from both closed captions and video key frames. Experiment results demonstrate that visual features provided 7.2% absolute F-score gain on stateof-the-art text based event extraction and coreference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1412--1422</pages>
<contexts>
<context position="2530" citStr="Bejan and Harabagiu, 2010" startWordPosition="374" endWordPosition="377">ognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 201–206, Lisbon</context>
<context position="7538" citStr="Bejan and Harabagiu, 2010" startWordPosition="1163" endWordPosition="1166">over all arguments or attributes. To tackle these challenges, we adopt a Maximum Entropy (MaxEnt) model as in (Chen and Ji, 2009b). We consider every pair of event mentions which share the same event type as a candidate and exploit features proposed in (Chen and Ji, 2009b; Chen et al., 2009). Note that the goal in (Chen and Ji, 2009b; Chen et al., 2009) was to resolve event coreference within the same document, whereas our scenario yields to a crossdocument/video transcript setting, so we remove some improper and invalid features. We also investigated the approaches by (Lee et al., 2012) and (Bejan and Harabagiu, 2010), but the confidence estimation results from these alternative methods are not reliable. Moreover, the input of event coreference are automatic results from event extraction instead of gold standard, so the noise and errors significantly impact the corefer202 ence performance, especially for unsupervised approaches (Bejan and Harabagiu, 2010). Nevertheless, we still incorporate features from the aforementioned methods. Table 1 shows the features that constitute the input of the MaxEnt model. 2.3 Visual Similarity Visual content provides useful cues complementary with those used in text-based a</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1412–1422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Event coreference resolution: Algorithm, feature impact and evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of Events in Emerging Text Types (eETTs) Workshop, in conjunction with RANLP,</booktitle>
<contexts>
<context position="2464" citStr="Chen and Ji, 2009" startWordPosition="362" endWordPosition="365">casts. The CC is either generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empir</context>
<context position="7040" citStr="Chen and Ji, 2009" startWordPosition="1075" endWordPosition="1078"> events are defined as the same specific occurrence mentioned in different sentences, documents and transcript texts. Coreferential events should happen in the same place and within the same time period, and the entities involved and their roles should be identical. From the perspective of extracted events, each specific attribute and argument from those events should match. However, mentions for the same event may appear in forms of diverse words and phrases; and they do not always cover all arguments or attributes. To tackle these challenges, we adopt a Maximum Entropy (MaxEnt) model as in (Chen and Ji, 2009b). We consider every pair of event mentions which share the same event type as a candidate and exploit features proposed in (Chen and Ji, 2009b; Chen et al., 2009). Note that the goal in (Chen and Ji, 2009b; Chen et al., 2009) was to resolve event coreference within the same document, whereas our scenario yields to a crossdocument/video transcript setting, so we remove some improper and invalid features. We also investigated the approaches by (Lee et al., 2012) and (Bejan and Harabagiu, 2010), but the confidence estimation results from these alternative methods are not reliable. Moreover, the</context>
<context position="14932" citStr="Chen and Ji, 2009" startWordPosition="2405" endWordPosition="2408">t for measuring inter-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system performance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event mentions, remove the anchor frames and calculate the visual similarity between the videos. Our final dataset consists of 85 videos, 207 events and 848 event pairs, where 47 pairs are considered coreferential. We adopt the MaxEnt-based coreference resolution system from (Chen and Ji, 2009b; Chen et al., 2009) as our baseline, and use ACE 2005 English Corpus as the training set for the model. A 5-fold cross-validation is conducted on the training set and the average f-score is 56%. It is lower than results from (Chen and Ji, 2009a) since we remove some features which are not available for the cross-document scenario. 3.2 Results The peak F-score for the baseline system is 44.23% while our cross-media method boosts it to 51.43%. Figure 2 shows the improvement after incorporating the visual information. We adopt Wilcoxon signed-rank test to determine the significance between the </context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Zheng Chen and Heng Ji. 2009a. Event coreference resolution: Algorithm, feature impact and evaluation. In Proceedings of Events in Emerging Text Types (eETTs) Workshop, in conjunction with RANLP, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Graph-based event coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, TextGraphs-4,</booktitle>
<pages>54--57</pages>
<contexts>
<context position="2464" citStr="Chen and Ji, 2009" startWordPosition="362" endWordPosition="365">casts. The CC is either generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empir</context>
<context position="7040" citStr="Chen and Ji, 2009" startWordPosition="1075" endWordPosition="1078"> events are defined as the same specific occurrence mentioned in different sentences, documents and transcript texts. Coreferential events should happen in the same place and within the same time period, and the entities involved and their roles should be identical. From the perspective of extracted events, each specific attribute and argument from those events should match. However, mentions for the same event may appear in forms of diverse words and phrases; and they do not always cover all arguments or attributes. To tackle these challenges, we adopt a Maximum Entropy (MaxEnt) model as in (Chen and Ji, 2009b). We consider every pair of event mentions which share the same event type as a candidate and exploit features proposed in (Chen and Ji, 2009b; Chen et al., 2009). Note that the goal in (Chen and Ji, 2009b; Chen et al., 2009) was to resolve event coreference within the same document, whereas our scenario yields to a crossdocument/video transcript setting, so we remove some improper and invalid features. We also investigated the approaches by (Lee et al., 2012) and (Bejan and Harabagiu, 2010), but the confidence estimation results from these alternative methods are not reliable. Moreover, the</context>
<context position="14932" citStr="Chen and Ji, 2009" startWordPosition="2405" endWordPosition="2408">t for measuring inter-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system performance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event mentions, remove the anchor frames and calculate the visual similarity between the videos. Our final dataset consists of 85 videos, 207 events and 848 event pairs, where 47 pairs are considered coreferential. We adopt the MaxEnt-based coreference resolution system from (Chen and Ji, 2009b; Chen et al., 2009) as our baseline, and use ACE 2005 English Corpus as the training set for the model. A 5-fold cross-validation is conducted on the training set and the average f-score is 56%. It is lower than results from (Chen and Ji, 2009a) since we remove some features which are not available for the cross-document scenario. 3.2 Results The peak F-score for the baseline system is 44.23% while our cross-media method boosts it to 51.43%. Figure 2 shows the improvement after incorporating the visual information. We adopt Wilcoxon signed-rank test to determine the significance between the </context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Zheng Chen and Heng Ji. 2009b. Graph-based event coreference resolution. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, TextGraphs-4, pages 54– 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
<author>Robert Haralick</author>
</authors>
<title>A pairwise event coreference model, feature impact and evaluation for event coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Events in Emerging Text Types, eETTs ’09,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="2484" citStr="Chen et al., 2009" startWordPosition="366" endWordPosition="369">her generated by automatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empirical Methods in Natu</context>
<context position="7204" citStr="Chen et al., 2009" startWordPosition="1106" endWordPosition="1109">e place and within the same time period, and the entities involved and their roles should be identical. From the perspective of extracted events, each specific attribute and argument from those events should match. However, mentions for the same event may appear in forms of diverse words and phrases; and they do not always cover all arguments or attributes. To tackle these challenges, we adopt a Maximum Entropy (MaxEnt) model as in (Chen and Ji, 2009b). We consider every pair of event mentions which share the same event type as a candidate and exploit features proposed in (Chen and Ji, 2009b; Chen et al., 2009). Note that the goal in (Chen and Ji, 2009b; Chen et al., 2009) was to resolve event coreference within the same document, whereas our scenario yields to a crossdocument/video transcript setting, so we remove some improper and invalid features. We also investigated the approaches by (Lee et al., 2012) and (Bejan and Harabagiu, 2010), but the confidence estimation results from these alternative methods are not reliable. Moreover, the input of event coreference are automatic results from event extraction instead of gold standard, so the noise and errors significantly impact the corefer202 ence p</context>
<context position="14953" citStr="Chen et al., 2009" startWordPosition="2409" endWordPosition="2412">r-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system performance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event mentions, remove the anchor frames and calculate the visual similarity between the videos. Our final dataset consists of 85 videos, 207 events and 848 event pairs, where 47 pairs are considered coreferential. We adopt the MaxEnt-based coreference resolution system from (Chen and Ji, 2009b; Chen et al., 2009) as our baseline, and use ACE 2005 English Corpus as the training set for the model. A 5-fold cross-validation is conducted on the training set and the average f-score is 56%. It is lower than results from (Chen and Ji, 2009a) since we remove some features which are not available for the cross-document scenario. 3.2 Results The peak F-score for the baseline system is 44.23% while our cross-media method boosts it to 51.43%. Figure 2 shows the improvement after incorporating the visual information. We adopt Wilcoxon signed-rank test to determine the significance between the pairs of precision sc</context>
</contexts>
<marker>Chen, Ji, Haralick, 2009</marker>
<rawString>Zheng Chen, Heng Ji, and Robert Haralick. 2009. A pairwise event coreference model, feature impact and evaluation for event coreference resolution. In Proceedings of the Workshop on Events in Emerging Text Types, eETTs ’09, pages 17–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<contexts>
<context position="8883" citStr="Deng et al., 2009" startWordPosition="1366" endWordPosition="1369"> objects, and activities in the visual channel. Coherence of such visual content has been used in grouping multiple video shots into the same video story (Hsu et al., 2003), but it has not been used for event coreference resolution. Recent work in computer vision has demonstrated tremendous progress in large-scale visual content recognition. In this work, we adopt the state-of-the-art techniques (Krizhevsky et al., 2012) and (Simonyan and Zisserman, 2014) that train robust convolutional neural networks (CNN) over millions of web images to detect 20,000 semantic categories defined in ImageNet (Deng et al., 2009) from each image. The 2nd to the last layer features from such deep network can be considered as high-level visual representation that can be used to discriminate various semantic classes (scenes, objects, activity). It has been found effective in computing visual similarity between images, by directly computing the L2 distance of such features or through further metric learning. To compute the similarity between videos associated with two candidate event mentions, we sample multiple frames from each video and aggregate the similarity scores of the few most similar image pairs between the vide</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2009., pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winston Hsu</author>
<author>Shih-Fu Chang</author>
<author>Chih-Wei Huang</author>
<author>Lyndon Kennedy</author>
<author>Ching-Yung Lin</author>
<author>Giridharan Iyengar</author>
</authors>
<title>Discovery and fusion of salient multimodal features toward news story segmentation. In Electronic Imaging</title>
<date>2003</date>
<pages>244--258</pages>
<contexts>
<context position="8437" citStr="Hsu et al., 2003" startWordPosition="1299" endWordPosition="1302">cially for unsupervised approaches (Bejan and Harabagiu, 2010). Nevertheless, we still incorporate features from the aforementioned methods. Table 1 shows the features that constitute the input of the MaxEnt model. 2.3 Visual Similarity Visual content provides useful cues complementary with those used in text-based approach in event coreference resolution. For example, two coreferential events typically show similar or even duplicate scenes, objects, and activities in the visual channel. Coherence of such visual content has been used in grouping multiple video shots into the same video story (Hsu et al., 2003), but it has not been used for event coreference resolution. Recent work in computer vision has demonstrated tremendous progress in large-scale visual content recognition. In this work, we adopt the state-of-the-art techniques (Krizhevsky et al., 2012) and (Simonyan and Zisserman, 2014) that train robust convolutional neural networks (CNN) over millions of web images to detect 20,000 semantic categories defined in ImageNet (Deng et al., 2009) from each image. The 2nd to the last layer features from such deep network can be considered as high-level visual representation that can be used to disc</context>
<context position="10661" citStr="Hsu et al., 2003" startWordPosition="1685" endWordPosition="1688">ir (VZ, Vj) is computed as 1 E ¯DZj = k ∗ Dmn (2) (f.,fn) , where (fm, fn) is the top k of most similar frame pairs. In our experiment, we use k = 3. Such aggregation method among the top matches is intended to capture similarity between videos that share only partially overlapped content. Each news video story typically starts with an introduction by an anchor person followed by news footages showing the visual scenes or activities of the event. Therefore, when computing visual similarity, it’s important to exclude the anchor shot and focus on the story-related clips. Anchor frame detection (Hsu et al., 2003) is a well studied problem. In order to detect anchor frames automatically, a face detector is applied to all I-frames of a video. We can obtain the location and size of each detected face. After checking the temporal consistency of the detected faces within each shot, we get a set of candidate anchor faces. The detected face regions are further extended to regions of interest that may include hair and upper body. All the candidate faces detected from the same video are clustered based on their HSV color histogram. It is reasonable to assume that the most frequent face cluster is the one corre</context>
</contexts>
<marker>Hsu, Chang, Huang, Kennedy, Lin, Iyengar, 2003</marker>
<rawString>Winston Hsu, Shih-Fu Chang, Chih-Wei Huang, Lyndon Kennedy, Ching-Yung Lin, and Giridharan Iyengar. 2003. Discovery and fusion of salient multimodal features toward news story segmentation. In Electronic Imaging 2004, pages 244–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Huang</author>
<author>Winston Hsu</author>
<author>Shin-Fu Chang</author>
</authors>
<title>Automatic closed caption alignment based on speech recognition transcripts. Rapport technique,</title>
<date>2003</date>
<location>Columbia.</location>
<contexts>
<context position="13852" citStr="Huang et al., 2003" startWordPosition="2228" endWordPosition="2231">ith large ones. An alternative way for setting the alpha parameter is through cross validation over separate data partitions. 3 Experiments 3.1 Data and Setting We establish a system that actively monitors over 100 U.S. major broadcast TV channels such as ABC, CNN and FOX, and crawls newscasts from these channels for more than two years (Li et al., 2013a). With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of “ISIS”2. This system also temporally aligns the CC text with the transcribed text from automatic speech recognition following the methods in (Huang et al., 2003). This provides accurate time alignment between the CC text and the video frames. As CC consists of capitalized letters, we apply the true-casing tool from Standford CoreNLP (Manning et al., 2014) on CC. Then we apply a state-of-the-art event extraction system (Li et al., 2013b; Li et al., 2014) to extract event mentions from CC. We asked two human annotators to investigate all event pairs and annotate coreferential pairs as the ground truth. Kappa coefficient for measuring inter-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system pe</context>
</contexts>
<marker>Huang, Hsu, Chang, 2003</marker>
<rawString>Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang. 2003. Automatic closed caption alignment based on speech recognition transcripts. Rapport technique, Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Kong</author>
<author>Dahua Lin</author>
<author>Mohit Bansal</author>
<author>Raquel Urtasun</author>
<author>Sanja Fidler</author>
</authors>
<title>What are you talking about? text-to-image coreference.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>3558--3565</pages>
<contexts>
<context position="4504" citStr="Kong et al., 2014" startWordPosition="673" endWordPosition="676">y of “Jordanian pilot”. Fortunately, videos often illustrate brief descriptions by vivid visual contents. Moreover, diverse anchors, reporters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases. Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity. In this example, the visual similarity between the corresponding video frames is high because both of them show the scene of the Jordanian pilot. Similar work such as (Kong et al., 2014), (Ramanathan et al., 2014), (Motwani and Mooney, 2012) and (Ramanathan et al., 2013) have explored methods of linking visual materials with texts. However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distinguish entity and event in the documents since the definition of visual concepts often require both of them. Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information t</context>
</contexts>
<marker>Kong, Lin, Bansal, Urtasun, Fidler, 2014</marker>
<rawString>Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, and Sanja Fidler. 2014. What are you talking about? text-to-image coreference. In Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 3558–3565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1097--1105</pages>
<contexts>
<context position="8689" citStr="Krizhevsky et al., 2012" startWordPosition="1336" endWordPosition="1339">ontent provides useful cues complementary with those used in text-based approach in event coreference resolution. For example, two coreferential events typically show similar or even duplicate scenes, objects, and activities in the visual channel. Coherence of such visual content has been used in grouping multiple video shots into the same video story (Hsu et al., 2003), but it has not been used for event coreference resolution. Recent work in computer vision has demonstrated tremendous progress in large-scale visual content recognition. In this work, we adopt the state-of-the-art techniques (Krizhevsky et al., 2012) and (Simonyan and Zisserman, 2014) that train robust convolutional neural networks (CNN) over millions of web images to detect 20,000 semantic categories defined in ImageNet (Deng et al., 2009) from each image. The 2nd to the last layer features from such deep network can be considered as high-level visual representation that can be used to discriminate various semantic classes (scenes, objects, activity). It has been found effective in computing visual similarity between images, by directly computing the L2 distance of such features or through further metric learning. To compute the similari</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>489--500</pages>
<contexts>
<context position="2502" citStr="Lee et al., 2012" startWordPosition="370" endWordPosition="373">tomatic speech recognition (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: “It ’s not clear when it was killed.”; “Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot.” instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as (Chen and Ji, 2009b; Chen et al., 2009; Lee et al., 2012; Bejan and Harabagiu, 2010). However, they only focused on formally written newswire articles and utilized textual features. Such approaches do not perform well on CC due to (1). the propagated errors from upper stream components (e.g., automatic speech/stenotype recognition and event extraction); (2). the incompleteness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journalists are trained and expected to organize reports 201 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Proce</context>
<context position="7506" citStr="Lee et al., 2012" startWordPosition="1158" endWordPosition="1161">nd they do not always cover all arguments or attributes. To tackle these challenges, we adopt a Maximum Entropy (MaxEnt) model as in (Chen and Ji, 2009b). We consider every pair of event mentions which share the same event type as a candidate and exploit features proposed in (Chen and Ji, 2009b; Chen et al., 2009). Note that the goal in (Chen and Ji, 2009b; Chen et al., 2009) was to resolve event coreference within the same document, whereas our scenario yields to a crossdocument/video transcript setting, so we remove some improper and invalid features. We also investigated the approaches by (Lee et al., 2012) and (Bejan and Harabagiu, 2010), but the confidence estimation results from these alternative methods are not reliable. Moreover, the input of event coreference are automatic results from event extraction instead of gold standard, so the noise and errors significantly impact the corefer202 ence performance, especially for unsupervised approaches (Bejan and Harabagiu, 2010). Nevertheless, we still incorporate features from the aforementioned methods. Table 1 shows the features that constitute the input of the MaxEnt model. 2.3 Visual Similarity Visual content provides useful cues complementary</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 489–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongzhi Li</author>
<author>Brendan Jou</author>
<author>Jospeh G Ellis</author>
<author>Daniel Morozoff</author>
<author>Shih-Fu Chang</author>
</authors>
<title>News rover: Exploring topical structures and serendipity in heterogeneous multimedia news.</title>
<date>2013</date>
<booktitle>In Proceedings of the 21st ACM international conference on Multimedia,</booktitle>
<pages>449--450</pages>
<contexts>
<context position="13588" citStr="Li et al., 2013" startWordPosition="2186" endWordPosition="2189"> and tense mod conflict, 1 if the attributes of EMi and EMS conflict pol conflict, gen conflict, ten conflict Table 1: Features for Event Coreference Resolution we generally enhance the confidence of event pairs with small visual distances and penalize those with large ones. An alternative way for setting the alpha parameter is through cross validation over separate data partitions. 3 Experiments 3.1 Data and Setting We establish a system that actively monitors over 100 U.S. major broadcast TV channels such as ABC, CNN and FOX, and crawls newscasts from these channels for more than two years (Li et al., 2013a). With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of “ISIS”2. This system also temporally aligns the CC text with the transcribed text from automatic speech recognition following the methods in (Huang et al., 2003). This provides accurate time alignment between the CC text and the video frames. As CC consists of capitalized letters, we apply the true-casing tool from Standford CoreNLP (Manning et al., 2014) on CC. Then we apply a state-of-the-art event extraction system (Li et al., 2013b; Li et al., 2014) to extract event mentions from CC. We a</context>
</contexts>
<marker>Li, Jou, Ellis, Morozoff, Chang, 2013</marker>
<rawString>Hongzhi Li, Brendan Jou, Jospeh G Ellis, Daniel Morozoff, and Shih-Fu Chang. 2013a. News rover: Exploring topical structures and serendipity in heterogeneous multimedia news. In Proceedings of the 21st ACM international conference on Multimedia, pages 449–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="13588" citStr="Li et al., 2013" startWordPosition="2186" endWordPosition="2189"> and tense mod conflict, 1 if the attributes of EMi and EMS conflict pol conflict, gen conflict, ten conflict Table 1: Features for Event Coreference Resolution we generally enhance the confidence of event pairs with small visual distances and penalize those with large ones. An alternative way for setting the alpha parameter is through cross validation over separate data partitions. 3 Experiments 3.1 Data and Setting We establish a system that actively monitors over 100 U.S. major broadcast TV channels such as ABC, CNN and FOX, and crawls newscasts from these channels for more than two years (Li et al., 2013a). With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of “ISIS”2. This system also temporally aligns the CC text with the transcribed text from automatic speech recognition following the methods in (Huang et al., 2003). This provides accurate time alignment between the CC text and the video frames. As CC consists of capitalized letters, we apply the true-casing tool from Standford CoreNLP (Manning et al., 2014) on CC. Then we apply a state-of-the-art event extraction system (Li et al., 2013b; Li et al., 2014) to extract event mentions from CC. We a</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013b. Joint event extraction via structured prediction with global features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Yu HONG</author>
<author>Sujian Li</author>
</authors>
<title>Constructing information networks using one single model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1846--1851</pages>
<contexts>
<context position="14148" citStr="Li et al., 2014" startWordPosition="2279" endWordPosition="2282"> these channels for more than two years (Li et al., 2013a). With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of “ISIS”2. This system also temporally aligns the CC text with the transcribed text from automatic speech recognition following the methods in (Huang et al., 2003). This provides accurate time alignment between the CC text and the video frames. As CC consists of capitalized letters, we apply the true-casing tool from Standford CoreNLP (Manning et al., 2014) on CC. Then we apply a state-of-the-art event extraction system (Li et al., 2013b; Li et al., 2014) to extract event mentions from CC. We asked two human annotators to investigate all event pairs and annotate coreferential pairs as the ground truth. Kappa coefficient for measuring inter-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system performance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event mentions, remove the anchor frames and calculate the visual similarity between the videos. Our final dat</context>
<context position="18145" citStr="Li et al., 2014" startWordPosition="2939" endWordPosition="2942"> video shots. For such errors, one potential solution is to expand the video frame windows to capture more events and concepts from videos. Expanding the detection range to include visual events in the temporal neighborhood can also differentiate the events. 3.4 Discussion A systematic way of choosing α in Equation 3 will be useful. One idea is to adapt the α value for different types of events, e.g., we expect some event types are more visually oriented than others and thus use a smaller α value. We also notice the impact of the errors from the upstream event extraction system. According to (Li et al., 2014) the F-score of event trigger labeling is 65.3%, and event argument labeling is 45%. Missing arguments in events is a main problem, thus the performance on automatically extracted event mentions is significantly worse. About 20 more coreferential pairs could be detected if events and arguments are perfectly extracted. 4 Conclusions and Future Work In this paper, we improved event coreference resolution on newscast speech by incorporating visual similarity. We also build a crawler that provides a benchmark dataset of videos with aligned closed captions. This system can also help create more dat</context>
</contexts>
<marker>Li, Ji, HONG, Li, 2014</marker>
<rawString>Qi Li, Heng Ji, Yu HONG, and Sujian Li. 2014. Constructing information networks using one single model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846–1851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="14048" citStr="Manning et al., 2014" startWordPosition="2261" endWordPosition="2264">ly monitors over 100 U.S. major broadcast TV channels such as ABC, CNN and FOX, and crawls newscasts from these channels for more than two years (Li et al., 2013a). With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of “ISIS”2. This system also temporally aligns the CC text with the transcribed text from automatic speech recognition following the methods in (Huang et al., 2003). This provides accurate time alignment between the CC text and the video frames. As CC consists of capitalized letters, we apply the true-casing tool from Standford CoreNLP (Manning et al., 2014) on CC. Then we apply a state-of-the-art event extraction system (Li et al., 2013b; Li et al., 2014) to extract event mentions from CC. We asked two human annotators to investigate all event pairs and annotate coreferential pairs as the ground truth. Kappa coefficient for measuring inter-annotator agreement is 2abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system performance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event ment</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12794" citStr="Miller, 1995" startWordPosition="2060" endWordPosition="2061">Remarks (EMi: the first event mention, EMS: the second event mention) Baseline type subtype pair of event type and subtype in EMi trigger pair trigger pair of EMi and EMS pos pair part-of-speech pair of triggers of EMi and EMS nominal 1 if the trigger of EMi is nominal nom number “plural” or “singular” if the trigger of EMi is nominal pronominal 1 if the trigger of EMi is pronominal exact match 1 if the trigger spelling in EMi matches that in EMS stem match 1 if the trigger stem in EMi matches that in EMS trigger sim the semantic similarity scores between triggers of EMi and EMS using WordNet(Miller, 1995) Arguments argument match 1 if arguments holding the same roles in both EMi and EMS matches Attributes mod,pol,gen,ten four event attributes in EMi: modality, polarity, genericity and tense mod conflict, 1 if the attributes of EMi and EMS conflict pol conflict, gen conflict, ten conflict Table 1: Features for Event Coreference Resolution we generally enhance the confidence of event pairs with small visual distances and penalize those with large ones. An alternative way for setting the alpha parameter is through cross validation over separate data partitions. 3 Experiments 3.1 Data and Setting </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanvi S Motwani</author>
<author>Raymond J Mooney</author>
</authors>
<title>Improving video activity recognition using object recognition and text mining.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th European Conference on Artificial Intelligence,</booktitle>
<pages>600--605</pages>
<contexts>
<context position="4559" citStr="Motwani and Mooney, 2012" startWordPosition="682" endWordPosition="685">n illustrate brief descriptions by vivid visual contents. Moreover, diverse anchors, reporters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases. Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity. In this example, the visual similarity between the corresponding video frames is high because both of them show the scene of the Jordanian pilot. Similar work such as (Kong et al., 2014), (Ramanathan et al., 2014), (Motwani and Mooney, 2012) and (Ramanathan et al., 2013) have explored methods of linking visual materials with texts. However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distinguish entity and event in the documents since the definition of visual concepts often require both of them. Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information to improve event coreference resolution. In this paper, </context>
</contexts>
<marker>Motwani, Mooney, 2012</marker>
<rawString>Tanvi S Motwani and Raymond J Mooney. 2012. Improving video activity recognition using object recognition and text mining. In Proceedings of the 20th European Conference on Artificial Intelligence, pages 600–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ace 2005 evaluation plan.</title>
<date>2005</date>
<note>http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.</note>
<contexts>
<context position="5808" citStr="NIST, 2005" startWordPosition="870" endWordPosition="871">ures from both speech (textual) and video (visual) channels for the first time. We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions. With the crawler, we created a benchmark dataset which is fully annotated with crossdocument coreferential events 1. 1Dataset can be found at http://www.ee.columbia.edu/dvmm/newDownloads.htm 2 Approach 2.1 Event Extraction Given unstructured transcribed CC, we extract entities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005): • Entity: an object or set of objects in the world, such as person, organization and facility. • Entity mention: words or phrases in the texts that mention an entity. • Event: a specific occurrence involving participants. • Event trigger: the word that most clearly expresses an event’s occurrence. • Event argument: an entity, or a temporal expression or a value that has a certain role (e.g., TimeWithin, Place) in an event. • Event mention: a sentence (or a text span extent) that mentions an event, including a distinct trigger and arguments involved. 2.2 Text based Event Coreference Resolutio</context>
</contexts>
<marker>NIST, 2005</marker>
<rawString>NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vignesh Ramanathan</author>
<author>Percy Liang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Video event understanding using natural language descriptions.</title>
<date>2013</date>
<booktitle>In Proceedings of 2013 IEEE International Conference on Computer Vision,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="4589" citStr="Ramanathan et al., 2013" startWordPosition="687" endWordPosition="690"> by vivid visual contents. Moreover, diverse anchors, reporters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases. Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity. In this example, the visual similarity between the corresponding video frames is high because both of them show the scene of the Jordanian pilot. Similar work such as (Kong et al., 2014), (Ramanathan et al., 2014), (Motwani and Mooney, 2012) and (Ramanathan et al., 2013) have explored methods of linking visual materials with texts. However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distinguish entity and event in the documents since the definition of visual concepts often require both of them. Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information to improve event coreference resolution. In this paper, we propose to jointly incorpor</context>
</contexts>
<marker>Ramanathan, Liang, Fei-Fei, 2013</marker>
<rawString>Vignesh Ramanathan, Percy Liang, and Li Fei-Fei. 2013. Video event understanding using natural language descriptions. In Proceedings of 2013 IEEE International Conference on Computer Vision, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vignesh Ramanathan</author>
<author>Armand Joulin</author>
<author>Percy Liang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Linking people in videos with their names using coreference resolution. In Computer Vision–ECCV</title>
<date>2014</date>
<pages>95--110</pages>
<contexts>
<context position="4531" citStr="Ramanathan et al., 2014" startWordPosition="677" endWordPosition="681">”. Fortunately, videos often illustrate brief descriptions by vivid visual contents. Moreover, diverse anchors, reporters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases. Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity. In this example, the visual similarity between the corresponding video frames is high because both of them show the scene of the Jordanian pilot. Similar work such as (Kong et al., 2014), (Ramanathan et al., 2014), (Motwani and Mooney, 2012) and (Ramanathan et al., 2013) have explored methods of linking visual materials with texts. However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distinguish entity and event in the documents since the definition of visual concepts often require both of them. Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information to improve event coreference</context>
</contexts>
<marker>Ramanathan, Joulin, Liang, Fei-Fei, 2014</marker>
<rawString>Vignesh Ramanathan, Armand Joulin, Percy Liang, and Li Fei-Fei. 2014. Linking people in videos with their names using coreference resolution. In Computer Vision–ECCV 2014, pages 95–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Simonyan</author>
<author>Andrew Zisserman</author>
</authors>
<title>Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</title>
<date>2014</date>
<contexts>
<context position="8724" citStr="Simonyan and Zisserman, 2014" startWordPosition="1341" endWordPosition="1344">mplementary with those used in text-based approach in event coreference resolution. For example, two coreferential events typically show similar or even duplicate scenes, objects, and activities in the visual channel. Coherence of such visual content has been used in grouping multiple video shots into the same video story (Hsu et al., 2003), but it has not been used for event coreference resolution. Recent work in computer vision has demonstrated tremendous progress in large-scale visual content recognition. In this work, we adopt the state-of-the-art techniques (Krizhevsky et al., 2012) and (Simonyan and Zisserman, 2014) that train robust convolutional neural networks (CNN) over millions of web images to detect 20,000 semantic categories defined in ImageNet (Deng et al., 2009) from each image. The 2nd to the last layer features from such deep network can be considered as high-level visual representation that can be used to discriminate various semantic classes (scenes, objects, activity). It has been found effective in computing visual similarity between images, by directly computing the L2 distance of such features or through further metric learning. To compute the similarity between videos associated with t</context>
</contexts>
<marker>Simonyan, Zisserman, 2014</marker>
<rawString>Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
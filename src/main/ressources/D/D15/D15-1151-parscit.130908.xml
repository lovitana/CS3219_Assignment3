<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041826">
<title confidence="0.986222">
An Improved Tag Dictionary for Faster Part-of-Speech Tagging
</title>
<author confidence="0.953774">
Robert C. Moore
</author>
<affiliation confidence="0.86072">
Google Inc.
</affiliation>
<email confidence="0.989575">
bobmoore@google.com
</email>
<sectionHeader confidence="0.993573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998864833333334">
Ratnaparkhi (1996) introduced a method
of inferring a tag dictionary from anno-
tated data to speed up part-of-speech tag-
ging by limiting the set of possible tags for
each word. While Ratnaparkhi’s tag dic-
tionary makes tagging faster but less accu-
rate, an alternative tag dictionary that we
recently proposed (Moore, 2014) makes
tagging as fast as with Ratnaparkhi’s tag
dictionary, but with no decrease in accu-
racy. In this paper, we show that a very
simple semi-supervised variant of Ratna-
parkhi’s method results in a much tighter
tag dictionary than either Ratnaparkhi’s
or our previous method, with accuracy
as high as with our previous tag dictio-
nary but much faster tagging—more than
100,000 tokens per second in Perl.
</bodyText>
<sectionHeader confidence="0.995501" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.9999126875">
In this paper, we present a new method of con-
structing tag dictionaries for part-of-speech (POS)
tagging. A tag dictionary is simply a list of words1
along with a set of possible tags for each word
listed, plus one additional set of possible tags for
all words not listed. Tag dictionaries are com-
monly used to speed up POS-tag inference by re-
stricting the tags considered for a particular word
to those specified by the dictionary.
Early work on POS tagging generally relied
heavily on manually constructed tag dictionaries,
sometimes agumented with tag statistics derived
from an annotated corpus (Leech et al., 1983;
Church, 1988; Cutting et al., 1992). Merialdo
(1994) relied only on a tag dictionary extracted
from annotated data, but he used the annotated
</bodyText>
<footnote confidence="0.9784455">
1According to the conventions of the field, POS tags are
assigned to all tokens in a tokenized text, including punctu-
ation marks and other non-word tokens. In this paper, all of
these will be covered by the term word.
</footnote>
<bodyText confidence="0.999780142857143">
tags from his test data as well as his training data to
construct his tag dictionary, so his evaluation was
not really fair.2 Ratnaparkhi (1996) seems to have
been the first to use a tag dictionary automatically
extracted only from training data.
Ratnaparkhi’s method of constructing a tag dic-
tionary substantially speeds up tagging compared
to considering every possible tag for every word,
but it noticeably degrades accuracy when used
with a current state-of-the-art tagging model. We
recently presented (Moore, 2014) a new method of
constructing a tag dictionary that produces a tag-
ging speed-up comparable to Ratnaparkhi’s, but
with no decrease in tagging accuracy. In this pa-
per, we show that a very simple semi-supervised
variant of Ratnaparkhi’s method results in a much
tighter tag dictionary than either Ratnaparkhi’s or
our previous method, with accuracy as high as we
previously obtained, while allowing much faster
tagging—more than 100,000 tokens per second
even in a Perl implementation.
</bodyText>
<subsectionHeader confidence="0.999762">
1.1 Tag Dictionaries and Tagging Speed
</subsectionHeader>
<bodyText confidence="0.9998234">
A typical modern POS tagger applies a statistical
model to compute a score for a sequence of tags
ti, ... , tn given a sequence of words wi, ... , wn.
The tag sequence assigned the highest score by the
model for a given word sequence is selected as the
tagging for the word sequence. If T is the set of
possible tags, and there are no restrictions on the
form of the model, then the time to find the highest
scoring tag sequence is potentially O(njT jn) or
worse, which would be intractable.
To make tagging practical, models are normally
defined to be factorable in a way that reduces the
time complexity to O(njT jk), for some small in-
teger k. For models in which all tagging deci-
sions are independent, or for higher-order mod-
</bodyText>
<footnote confidence="0.99968725">
2Merialdo (1994, p. 161) acknowledged this: “In some
sense this is an optimal dictionary for this data, since a word
will not have all its possible tags (in the language), but only
the tags it actually had within the text.”
</footnote>
<page confidence="0.60012">
1303
</page>
<note confidence="0.7289855">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1303–1308,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999901571428572">
els pruned by fixed-width beam search, k = 1, so
the time to find the highest scoring tag sequence is
O(nITI). But this linear dependence on the size of
the tag set means that reducing the average number
of tags considered per token should further speed
up tagging, whatever the underlying model or tag-
ger may be.
</bodyText>
<subsectionHeader confidence="0.945321">
1.2 Ratnaparkhi’s Method
</subsectionHeader>
<bodyText confidence="0.99994180952381">
For each word observed in an annotated training
set, Ratnaparkhi’s tag dictionary includes all tags
observed with that word in the training set, with all
possible tags allowed for all other words. Ratna-
parkhi reported that using this tag dictionary im-
proved per-tag accuracy from 96.31% to 96.43%
on his Penn Treebank (Marcus et al., 1993) Wall
Street Journal (WSJ) development set, compared
to considering all tags for all words.
With a more accurate model, however, we found
(Moore, 2014) that while Ratnaparkhi’s tag dictio-
nary decreased the average number of tags per to-
ken from 45 to 3.7 on the current standard WSJ de-
velopment set, it also decreased per-tag accuracy
from 97.31% to 97.19%. This loss of accuracy
can be explained by the fact that 0.5% of the de-
velopment set tokens are known words with a tag
not seen in the training set, for which our model
achieved 44.5% accuracy with all word/tag pairs
permitted. With Ratnaparkhi’s dictionary, accu-
racy for these tokens is necessarily 0%.
</bodyText>
<subsectionHeader confidence="0.982952">
1.3 Our Previous Method
</subsectionHeader>
<bodyText confidence="0.99992046875">
We previously presented (Moore, 2014) a tag dic-
tionary constructed by using the annotated train-
ing set to compute a smoothed probability estimate
for any possible tag given any possible word, and
for each word in the training set, including in the
dictionary the tags having an estimated probabil-
ity greater than a fixed threshold T. In this ap-
proach, the probability p(tIw) of tag t given word
w is computed by interpolating a discounted rel-
ative frequency estimate of p(tlw) with an esti-
mate of p(t) based on “diversity counts”, taking
the count of a tag t to be the number of distinct
words ever observed with that tag. The distribu-
tion p(t) is also used to estimate tag probabilities
for unknown words, so the set of possible tags for
any word not explicitly listed is Itjp(t) &gt; T}.
If we think of w followed by t as a word bi-
gram, this is exactly like a bigram language model
estimated by the interpolated Kneser-Ney (KN)
method described by Chen and Goodman (1999).
The way tag diversity counts are used has the de-
sirable property that closed-class tags receive a
very low estimated probability of being assigned
to a rare or unknown word, even though they oc-
cur very often with a small number of frequent
words. A single value for discounting the count
of all observed word/tag pairs is set to maximize
the estimated probability of the reference tagging
of the development set. When T was chosen to
be the highest threshold that preserves our model’s
97.31% per tag WSJ development set accuracy, we
obtained an average of 3.5 tags per token.
</bodyText>
<subsectionHeader confidence="0.991546">
1.4 Our New Approach
</subsectionHeader>
<bodyText confidence="0.999978918918919">
We now present a new method that reduces the av-
erage number of tags per token to about 1.5, with
no loss of tagging accuracy. We apply a simple
variant of Ratnaparkhi’s method, with a training
set more than 4,000 times larger than the Penn
Treebank WSJ training set. Since no such hand-
annotated corpus exists, we create the training set
automatically by running a version of our tagger
on the LDC English Gigaword corpus. We thus
describe our approach as a semi-supervised vari-
ant of Ratnaparkhi’s method. Our method can be
viewed as an instance of the well-known technique
of self-training (e.g., McClosky et al., 2006), but
ours is the first use of self-training we know of for
learning inference-time search-space pruning.
We introduce two additional modifications of
Ratnaparki’s approach. First, with such a large
training corpus, we find it unnecessary to keep in
the dictionary every tag observed with every word
in the automatically-annotated data. So, we esti-
mate a probability distribution over tags for each
word in the dictionary according to unsmoothed
relative tag frequencies, and include for each word
in the dictionary only tags whose probability given
the word is greater than a fixed threshold.
Second, since our tokenized version of the En-
glish Gigaword corpus contains more than 6 mil-
lion unique words, we reduce the vocabulary of the
dictionary to the approximately 1 million words
having 10 or more occurrences in the corpus. We
treat all other tokens as instances of unknown
words, and we use their combined unsmoothed
relative tag frequencies to estimate a tag probabil-
ity distribution for unknown words. We use the
same threshold on this distribution as we do for
words explicitly listed in the dictionary, to obtain
a set of possible tags for unknown words.
</bodyText>
<page confidence="0.993883">
1304
</page>
<sectionHeader confidence="0.995858" genericHeader="introduction">
2 Experimental Details
</sectionHeader>
<bodyText confidence="0.999983583333333">
In our experiments, we use the WSJ corpus from
Penn Treebank-3, split into the standard training
(sections 0–18), development (sections 19–21),
and test (sections 22-24) sets for POS tagging.
The tagging model we use has the property that
all digits are treated as indistinguishable for all
features. We therefore also make all digits in-
distinguishable in constructing tag dictionaries (by
internally replacing all digits by “9”), since it does
not seem sensible to give two different dictionary
entries based on containing different digits, when
the tagging model assigns them the same features.
</bodyText>
<subsectionHeader confidence="0.907606">
2.1 The Tagging Model
</subsectionHeader>
<bodyText confidence="0.999988791666667">
The model structure, feature set, and learning
method we use for POS tagging are essentially the
same as those in our earlier work, treating POS
tagging as a single-token independent multiclass
classification task. Word-class-sequence features
obtained by supervised clustering of the annotated
training set replace the hidden tag-sequence fea-
tures frequently used for POS tagging, and ad-
ditional word-class features obtained by unsuper-
vised clustering of a very large unannotated corpus
provide information about words not occurring in
the training set. For full details of the feature set,
see our previous paper (Moore, 2014).
The model is trained by optimizing the mul-
ticlass SVM hinge loss objective (Crammer and
Singer, 2001), using stochastic subgradient de-
scent as described by Zhang (2004), with early
stopping and averaging. The only difference from
our previous training procedure is that we now use
a tag dictionary to speed up training, while we pre-
viously used tag dictionaries only at test time.
Our training procedure makes multiple passes
through the training data considering each train-
ing example in turn, comparing the current model
score of the correct tag for the example to that of
the highest scoring incorrect tag and updating the
model if the score of the correct tag does not ex-
ceed the score of the highest scoring incorrect tag
by a specified margin. In our new version of this
procedure, we use the KN-smoothed tag dictio-
nary described in Section 1.3. to speed up finding
the highest scoring incorrect tag.
Recall that the KN-smoothed tag dictionary es-
timates a non-zero probability p(t|w) for every
possible word/tag pair, and that the possible tags
for a given word are determinted by setting a
threshold T on this probability. In each pass
through the training set, we use the same probabil-
ity distribution p(t|w) determined from the statis-
tics of the annotated training data, but we employ
an adaptive method to determine what threshold T
to use in each pass.
For the first pass through the training set, we
set an initial threshold To to the highest value
such that for every token in the development set,
p(t|w) ≥ To, where t is the correct tag for the to-
ken and w is the word for the token. At the end
of each training pass i, while evaluating the cur-
rent model on the development set for early stop-
ping using threshold Ti−1, we also find the high-
est probability threshold Ti such that choosing a
lower threshold would not enable any additional
correct taggings on the development set using the
current model. This threshold will normally be
higher than To, because we disregard tokens in the
development set for which the correct tag would
not be selected by the model resulting from the
previous pass at any threshold. Ti is then used as
the threshold for training pass i + 1. Whenever
the selected threshold leaves only one tag remain-
ing for a particular training example, we skip that
example in training.
On the first pass through the training set, use of
this method resulted in consideration of an aver-
age of 31.36 tags per token, compared to 45 to-
tal possible tags. On the second and all subse-
quent passes, an average of 10.48 tags were con-
sidered per token. This sped up training by a fac-
tor of 3.7 compared to considering all tags for all
tokens, with no loss of tagging accuracy when a
development-set-optimized KN-smoothed tag dic-
tionary is also used at test time.
</bodyText>
<subsectionHeader confidence="0.999175">
2.2 Tagging the Gigaword Corpus
</subsectionHeader>
<bodyText confidence="0.993382">
To construct our new tag dictionary, we need
an automatically-tagged corpus several orders of
magnitude larger than the hand-tagged WSJ train-
ing set. To obtain this corpus we ran a POS
tagger on the LDC English Gigaword Fifth Edi-
tion3 corpus, which consists of more than 4 bil-
lion words of English text from seven newswire
sources. We first removed all SGML mark-up, and
performed sentence-breaking and tokenization us-
ing the Stanford CoreNLP toolkit (Manning et al,
2014). This produced 4,471,025,373 tokens of
</bodyText>
<footnote confidence="0.959461">
3https://catalog.ldc.upenn.edu/
LDC2011T07
</footnote>
<page confidence="0.956138">
1305
</page>
<table confidence="0.99943325">
Tag Dictionary Accuracy Tags/Token Unambig Tokens/Sec
Pruned KN-smoothed 97.31% 3.48 45.3% 69k
Unpruned semi-supervised 97.31% 1.97 51.7% 82k
Pruned semi-supervised 97.31% 1.51 66.8% 103k
</table>
<tableCaption confidence="0.999631">
Table 1: WSJ development set token accuracy and tagging speed for different tag dictionaries
</tableCaption>
<bodyText confidence="0.9971909">
6,616,812 unique words. We tagged this corpus
using the model described in Section 2.1 and a
KN-smoothed tag dictionary as described in Sec-
tion 1.3, with a threshold T = 0.0005. The tagger
we used is based on the fastest of the methods de-
scribed in our previous work (Moore, 2014, Sec-
tion 3.1). Tagging took about 26 hours using a
single-threaded implementation in Perl on a Linux
workstation equipped with Intel Xeon X5550 2.67
GHz processors.
</bodyText>
<subsectionHeader confidence="0.998116">
2.3 Extracting the Tag Dictionary
</subsectionHeader>
<bodyText confidence="0.999984333333333">
We extracted a Ratnaparkhi-like tag dictionary for
the 957,819 words with 10 or more occurrences
in our corpus. Tokens of all other words in the
corpus were treated as unknown word tokens and
used to define a set of 24 tags4 to be used for words
not explicitly listed in the dictionary. To allow
pruning the dictionary as described in Section 1.4,
for each word (including the unknown word), we
computed a probability distribution p(tjw) using
unsmoothed relative frequencies. As noted above,
we treated all digits as indistinguishable in con-
structing and applying the dictionary.
</bodyText>
<sectionHeader confidence="0.992359" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999818571428571">
Tagging the WSJ development set with an un-
pruned semi-supervised tag dictionary obtained
from the automatic tagging of the English Gi-
gaword corpus produced the same tagging accu-
racy as allowing all tags for all tokens or using
the pruned KN-smoothed tag dictionary used in
tagging the Gigaword corpus. Additional exper-
iments showed that we could prune this dictionary
with a threshold on p(tjw) as high as T = 0.0024
without decreasing development set accuracy. In
addition to applying this threshold to the tag prob-
abilities for all listed words, we also applied it to
the tag probabilities for unknown words, leaving
13 possible tags5 for those.
</bodyText>
<footnote confidence="0.9705178">
4CC, CD, DT, FW, IN, JJ, JJR, JJS, MD, NN, NNP,
NNPS, NNS, PRP, RB, RBR, RP, UH, VB, VBD, VBG,
VBN, VBP, and VBZ
5CD, FW, JJ, NN, NNP, NNPS, NNS, RB, VB, VBD,
VBG, VBN, and VBZ
</footnote>
<bodyText confidence="0.99995725">
Tagging the WSJ development set with these
two dictionaries is compared in Table 1 to tag-
ging with our previous pruned KN-smoothed dic-
tionary. The second column shows the accuracy
per tag, which is 97.31% for all three dictionaries.
The third column shows the mean number of tags
per token allowed by each dictionary. The fourth
column shows the percentage of tokens with only
one tag allowed, which is significant since the tag-
ger need not apply the model for such tokens—it
can simply output the single possible tag.
The last column shows the tagging speed in
tokens per second for each of the three tag dic-
tionaries, using the fast tagging method we pre-
viously described (Moore, 2014), in a single-
threaded implementation in Perl on a Linux work-
station equipped with Intel Xeon X5550 2.67 GHz
processors. Speed is rounded to the nearest 1,000
tokens per second, because we measured times to
a precision of only about one part in one hundred.
For the pruned KN-smoothed dictionary, we pre-
viously reported a speed of 49,000 tokens per sec-
ond under similar conditions. Our current faster
speed of 69,000 tokens per second is due to an
improved low-level implementation for computing
the model scores for permitted tags, and a slightly
faster version of Perl (v5.18.2).
The most restrictive tag dictionary, the pruned
semi-supervised dictionary, allows only 1.51 tags
per token, and our implementation runs at 103,000
tokens per second on the WSJ development set.
For our final experiments, we tested our tagger
with this dictionary on the standard Penn Treebank
WSJ test set and on the Penn Treebank-3 parsed
Brown corpus subset, as an out-of-domain eval-
uation. For comparison, we tested our previous
tagger and the fast version (english-left3words-
distsim) of the Stanford tagger (Toutanova et al.,
2003; Manning, 2011) recommended for prac-
tical use on the Stanford tagger website, which
we found to be by far the fastest of the six pub-
licly available taggers tested in our previous work
(Moore, 2014). The results of these tests are
shown in Table 2.6
</bodyText>
<footnote confidence="0.943559">
6In Table 2, “OOV” has the standard meaning of a token
</footnote>
<page confidence="0.923904">
1306
</page>
<table confidence="0.997382">
Tagger WSJ All WSJ OOV WSJ Brown All Brown OOV Brown
Tokens/Sec Accuracy Accuracy Tokens/Sec Accuracy Accuracy
This work 102k 97.36% 91.09% 96k 96.55% 89.25%
Our previous 51k/54k/69k 97.34% 90.98% 40k/43k/56k 96.54% 89.36%
Stanford fast 80k 96.87% 89.69% 50k 95.53% 87.38%
</table>
<tableCaption confidence="0.999602">
Table 2: WSJ test set and Brown corpus tagging speeds and token accuracies
</tableCaption>
<bodyText confidence="0.999974823529412">
For our previous tagger, we give three speeds:
the speed we reported earlier, a speed for a dupli-
cate of the earlier experiment using the faster ver-
sion of Perl that we use here, and a third measure-
ment including both the faster version of Perl and
our improved low-level tagger implementation.
With the pruned semi-supervised dictionary, our
new tagger has slightly higher all-token accuracy
than our previous tagger on both the WSJ test set
and Brown corpus set, and it is much more accu-
rate than the fast Stanford tagger. The accuracy
on the standard WSJ test set is 97.36%, one of the
highest ever reported. The new tagger is also much
faster than either of the other taggers, achieving a
speed of more than 100,000 tokens per second on
the WSJ test set, and almost 100,000 tokens per
second on the out-of-domain Brown corpus data.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999947727272727">
Our method of constructing a tag dictionary is
technically very simple, but remarkably effective.
It reduces the mean number of possible tags per
token by 57% and increases the number of un-
ambiguous tokens by by 47%, compared to the
previous state of the art (Moore, 2014) for a tag
dictionary that does not degrade tagging accuracy.
When combined with our previous work on fast
high-accuracy POS tagging, this tag dictionary
produces by far the fastest POS tagger reported
with anything close to comparable accuracy.
</bodyText>
<sectionHeader confidence="0.972615" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.607603277777778">
Stanley F. Chen and Joshua T. Goodman. 1999.
An empirical study of smoothing techniques for
language modeling. Computer Speech and Lan-
guage, 13(4):359–393.
Kenneth Ward Church. 1988. A stochastic parts
program and noun phrase parser for unrestricted
text. In Proceedings of the Second Confer-
ence on Applied Natural Language Processing
of a word not occurring in the annotated training set. Many
of these tokens do match words in our large semi-supervised
tag dictionary, however.
of the Association for Computational Linguis-
tics, February 9–12, Austin, Texas, USA, 136–
143.
Koby Crammer and Yoram Singer. 2001. On
the algorithmic implementation of multiclass
kernel-based vector machines. Journal of Ma-
chine Learning Research, 2:265–292.
</bodyText>
<reference confidence="0.999500696969697">
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Siburn. 1992. A practical part-of-
speech tagger. In Proceedings of the Third Con-
ference on Applied Natural Language Process-
ing of the Association for Computational Lin-
guistics, March 31–April 3, Trento, Italy, 133–
140.
Geoffrey Leech, Roger Garside, and Eric Atwell.
1983. The automatic tagging of the LOB cor-
pus. ICAME Journal: International Computer
Archive of Modern and Medieval English Jour-
nal, 7:13–33.
Christopher D. Manning. 2011. Part-of-speech
tagging from 97% to 100%: Is it time for
some linguistics? In Alexander Gelbukh (ed.),
Computational Linguistics and Intelligent Text
Processing, 12th International Conference, CI-
CLing 2011, Proceedings, Part I. Lecture Notes
in Computer Science 6608, Springer, 171–189.
Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard, and
David McClosky. 2014. The Stanford CoreNLP
natural language processing toolkit. In Pro-
ceedings of 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics: System
Demonstrations, June 23–24, Baltimore, Mary-
land, USA, 55–60.
Mitchell P. Marcus, Beatrice Santorini, and Mary
A. Marcinkiewicz. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Effective Self-Training for
</reference>
<page confidence="0.836162">
1307
</page>
<reference confidence="0.99977171875">
Parsing. In Proceedings of the 2006 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics, June 4–9, New York,
New York, USA, 152-159.
Bernard Merialdo. 1994. Tagging English text
with a probabalistic model. Computational Lin-
guistics, 20(2):155–171.
Robert C. Moore. 2014. Fast high-accuracy part-
of-speech tagging by independent classifiers. In
Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguis-
tics: Technical Papers, August 23–29, Dublin,
Ireland, 1165–1176.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, May 17–18,
Philadelphia, Pennsylvania, USA, 133–142.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-
rich part-of-speech tagging with a cyclic depen-
dency network. In Proceedings of the 2003 Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, May 27–June 1, Ed-
monton, Alberta, Canada, 173–180.
Tong Zhang. 2004. Solving large scale linear pre-
diction problems using stochastic gradient de-
scent algorithms. In Proceedings of the 21st In-
ternational Conference on Machine Learning,
July 4–8, Banff, Alberta, Canada, 919–926.
</reference>
<page confidence="0.993911">
1308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999922">An Improved Tag Dictionary for Faster Part-of-Speech Tagging</title>
<author confidence="0.999772">C Robert</author>
<affiliation confidence="0.999072">Google Inc.</affiliation>
<email confidence="0.999598">bobmoore@google.com</email>
<abstract confidence="0.985258222560976">Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi’s tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi’s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging—more than 100,000 tokens per second in Perl. 1 Overview In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) A tag dictionary is simply a list of along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of will be covered by the term tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was really (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi’s, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging—more than 100,000 tokens per second even in a Perl implementation. 1.1 Tag Dictionaries and Tagging Speed A typical modern POS tagger applies a statistical model to compute a score for a sequence of tags ... , a sequence of words ... , The tag sequence assigned the highest score by the model for a given word sequence is selected as the for the word sequence. If the set of possible tags, and there are no restrictions on the form of the model, then the time to find the highest tag sequence is potentially worse, which would be intractable. To make tagging practical, models are normally defined to be factorable in a way that reduces the complexity to for some small in- For models in which all tagging deciare independent, or for higher-order mod- (1994, p. 161) acknowledged this: “In some sense this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags it actually had within the text.” 1303 of the 2015 Conference on Empirical Methods in Natural Language pages Portugal, 17-21 September 2015. Association for Computational Linguistics. pruned by fixed-width beam search, so the time to find the highest scoring tag sequence is But this linear dependence on the size of the tag set means that reducing the average number of tags considered per token should further speed up tagging, whatever the underlying model or tagger may be. 1.2 Ratnaparkhi’s Method For each word observed in an annotated training set, Ratnaparkhi’s tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found (Moore, 2014) that while Ratnaparkhi’s tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi’s dictionary, accuracy for these tokens is necessarily 0%. 1.3 Our Previous Method We previously presented (Moore, 2014) a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probabilgreater than a fixed threshold In this apthe probability tag word computed by interpolating a discounted relfrequency estimate of an estiof on “diversity counts”, taking count of a tag be the number of distinct words ever observed with that tag. The distribualso used to estimate tag probabilities for unknown words, so the set of possible tags for word not explicitly listed is we think of by a word bigram, this is exactly like a bigram language model estimated by the interpolated Kneser-Ney (KN) method described by Chen and Goodman (1999). The way tag diversity counts are used has the desirable property that closed-class tags receive a very low estimated probability of being assigned to a rare or unknown word, even though they occur very often with a small number of frequent words. A single value for discounting the count of all observed word/tag pairs is set to maximize the estimated probability of the reference tagging the development set. When chosen to be the highest threshold that preserves our model’s 97.31% per tag WSJ development set accuracy, we obtained an average of 3.5 tags per token. 1.4 Our New Approach We now present a new method that reduces the average number of tags per token to about 1.5, with no loss of tagging accuracy. We apply a simple variant of Ratnaparkhi’s method, with a training set more than 4,000 times larger than the Penn Treebank WSJ training set. Since no such handannotated corpus exists, we create the training set automatically by running a version of our tagger on the LDC English Gigaword corpus. We thus describe our approach as a semi-supervised variant of Ratnaparkhi’s method. Our method can be viewed as an instance of the well-known technique of self-training (e.g., McClosky et al., 2006), but ours is the first use of self-training we know of for learning inference-time search-space pruning. We introduce two additional modifications of Ratnaparki’s approach. First, with such a large training corpus, we find it unnecessary to keep in the dictionary every tag observed with every word in the automatically-annotated data. So, we estimate a probability distribution over tags for each word in the dictionary according to unsmoothed relative tag frequencies, and include for each word in the dictionary only tags whose probability given the word is greater than a fixed threshold. Second, since our tokenized version of the English Gigaword corpus contains more than 6 million unique words, we reduce the vocabulary of the dictionary to the approximately 1 million words having 10 or more occurrences in the corpus. We treat all other tokens as instances of unknown words, and we use their combined unsmoothed relative tag frequencies to estimate a tag probability distribution for unknown words. We use the same threshold on this distribution as we do for words explicitly listed in the dictionary, to obtain a set of possible tags for unknown words. 1304 2 Experimental Details In our experiments, we use the WSJ corpus from Penn Treebank-3, split into the standard training (sections 0–18), development (sections 19–21), and test (sections 22-24) sets for POS tagging. The tagging model we use has the property that all digits are treated as indistinguishable for all features. We therefore also make all digits indistinguishable in constructing tag dictionaries (by internally replacing all digits by “9”), since it does not seem sensible to give two different dictionary entries based on containing different digits, when the tagging model assigns them the same features. 2.1 The Tagging Model The model structure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper (Moore, 2014). The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001), using stochastic subgradient descent as described by Zhang (2004), with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time. Our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not exceed the score of the highest scoring incorrect tag by a specified margin. In our new version of this procedure, we use the KN-smoothed tag dictionary described in Section 1.3. to speed up finding the highest scoring incorrect tag. Recall that the KN-smoothed tag dictionary esa non-zero probability every possible word/tag pair, and that the possible tags for a given word are determinted by setting a this probability. In each pass through the training set, we use the same probabildistribution from the statistics of the annotated training data, but we employ adaptive method to determine what threshold to use in each pass. For the first pass through the training set, we an initial threshold the highest value such that for every token in the development set, where the correct tag for the toand the word for the token. At the end each training pass while evaluating the current model on the development set for early stopusing threshold we also find the highprobability threshold that choosing a lower threshold would not enable any additional correct taggings on the development set using the current model. This threshold will normally be than because we disregard tokens in the development set for which the correct tag would not be selected by the model resulting from the pass at any threshold. then used as threshold for training pass Whenever the selected threshold leaves only one tag remaining for a particular training example, we skip that example in training. On the first pass through the training set, use of this method resulted in consideration of an average of 31.36 tags per token, compared to 45 total possible tags. On the second and all subsequent passes, an average of 10.48 tags were considered per token. This sped up training by a factor of 3.7 compared to considering all tags for all tokens, with no loss of tagging accuracy when a development-set-optimized KN-smoothed tag dictionary is also used at test time. 2.2 Tagging the Gigaword Corpus To construct our new tag dictionary, we need an automatically-tagged corpus several orders of magnitude larger than the hand-tagged WSJ training set. To obtain this corpus we ran a POS tagger on the LDC English Gigaword Fifth Ediwhich consists of more than 4 billion words of English text from seven newswire sources. We first removed all SGML mark-up, and performed sentence-breaking and tokenization using the Stanford CoreNLP toolkit (Manning et al, 2014). This produced 4,471,025,373 tokens of LDC2011T07 1305 Tag Dictionary Accuracy Tags/Token Unambig Tokens/Sec Pruned KN-smoothed 97.31% 3.48 45.3% 69k Unpruned semi-supervised 97.31% 1.97 51.7% 82k Pruned semi-supervised 97.31% 1.51 66.8% 103k Table 1: WSJ development set token accuracy and tagging speed for different tag dictionaries 6,616,812 unique words. We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Sec- 1.3, with a threshold The tagger we used is based on the fastest of the methods described in our previous work (Moore, 2014, Section 3.1). Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. 2.3 Extracting the Tag Dictionary We extracted a Ratnaparkhi-like tag dictionary for the 957,819 words with 10 or more occurrences in our corpus. Tokens of all other words in the corpus were treated as unknown word tokens and to define a set of 24 to be used for words not explicitly listed in the dictionary. To allow pruning the dictionary as described in Section 1.4, for each word (including the unknown word), we a probability distribution using unsmoothed relative frequencies. As noted above, we treated all digits as indistinguishable in constructing and applying the dictionary. 3 Experimental Results Tagging the WSJ development set with an unpruned semi-supervised tag dictionary obtained from the automatic tagging of the English Gigaword corpus produced the same tagging accuracy as allowing all tags for all tokens or using the pruned KN-smoothed tag dictionary used in tagging the Gigaword corpus. Additional experiments showed that we could prune this dictionary a threshold on as high as without decreasing development set accuracy. In addition to applying this threshold to the tag probabilities for all listed words, we also applied it to the tag probabilities for unknown words, leaving possible for those.</abstract>
<keyword confidence="0.8920846">CD, DT, FW, IN, JJ, JJR, JJS, MD, NN, NNP, NNPS, NNS, PRP, RB, RBR, RP, UH, VB, VBD, VBG, VBN, VBP, and VBZ FW, JJ, NN, NNP, NNPS, NNS, RB, VB, VBD, VBG, VBN, and VBZ</keyword>
<abstract confidence="0.959201520833334">Tagging the WSJ development set with these two dictionaries is compared in Table 1 to tagging with our previous pruned KN-smoothed dictionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens—it can simply output the single possible tag. The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described (Moore, 2014), in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2). The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3wordsdistsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work (Moore, 2014). The results of these tests are in Table Table 2, “OOV” has the standard meaning of a token 1306 Tagger WSJ All OOV WSJ Accuracy Tokens/Sec All Brown Accuracy OOV Brown Accuracy Tokens/Sec Accuracy</abstract>
<note confidence="0.794908">This work 102k 97.36% 91.09% 96k 96.55% 89.25% Our previous 51k/54k/69k 97.34% 90.98% 40k/43k/56k 96.54% 89.36% Stanford fast 80k 96.87% 89.69% 50k 95.53% 87.38% Table 2: WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds:</note>
<abstract confidence="0.9318561">the speed we reported earlier, a speed for a duplicate of the earlier experiment using the faster version of Perl that we use here, and a third measurement including both the faster version of Perl and our improved low-level tagger implementation. With the pruned semi-supervised dictionary, our new tagger has slightly higher all-token accuracy than our previous tagger on both the WSJ test set and Brown corpus set, and it is much more accurate than the fast Stanford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported. The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data. 4 Conclusions Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art (Moore, 2014) for a tag dictionary that does not degrade tagging accuracy. When combined with our previous work on fast high-accuracy POS tagging, this tag dictionary produces by far the fastest POS tagger reported with anything close to comparable accuracy. References Stanley F. Chen and Joshua T. Goodman. 1999. An empirical study of smoothing techniques for modeling. Speech and Lan- 13(4):359–393. Kenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted In of the Second Conference on Applied Natural Language Processing of a word not occurring in the annotated training set. Many of these tokens do match words in our large semi-supervised tag dictionary, however.</abstract>
<note confidence="0.7901740625">of the Association for Computational Linguis- February 9–12, Austin, Texas, USA, 136– 143. Koby Crammer and Yoram Singer. 2001. On the algorithmic implementation of multiclass vector machines. of Ma- Learning 2:265–292. Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Siburn. 1992. A practical part-oftagger. In of the Third Conference on Applied Natural Language Processing of the Association for Computational Lin- March 31–April 3, Trento, Italy, 133– 140. Geoffrey Leech, Roger Garside, and Eric Atwell. 1983. The automatic tagging of the LOB cor- Journal: International Computer Archive of Modern and Medieval English Jour- 7:13–33. Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Alexander Gelbukh (ed.), Computational Linguistics and Intelligent Text Processing, 12th International Conference, CI- CLing 2011, Proceedings, Part I. Lecture Notes Computer Science Springer, 171–189. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System June 23–24, Baltimore, Maryland, USA, 55–60. Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. 19(2):313–330. David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for 1307 In of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Com- June 4–9, New York, New York, USA, 152-159. Bernard Merialdo. 1994. Tagging English text a probabalistic model. Lin- 20(2):155–171. Robert C. Moore. 2014. Fast high-accuracy partof-speech tagging by independent classifiers. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguis- Technical August 23–29, Dublin, Ireland, 1165–1176. Adwait Ratnaparkhi. 1996. A maximum entropy for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods Natural Language May 17–18, Philadelphia, Pennsylvania, USA, 133–142. Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Featurerich part-of-speech tagging with a cyclic depennetwork. In of the 2003 Hu-</note>
<title confidence="0.224182">man Language Technology Conference of the</title>
<author confidence="0.751">North American Chapter of the Association for</author>
<date confidence="0.694447">May 27–June 1, Ed-</date>
<note confidence="0.847776166666667">monton, Alberta, Canada, 173–180. Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient dealgorithms. In of the 21st In- Conference on Machine July 4–8, Banff, Alberta, Canada, 919–926.</note>
<intro confidence="0.35298">1308</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Siburn</author>
</authors>
<title>A practical part-ofspeech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing of the Association for Computational Linguistics, March 31–April 3,</booktitle>
<volume>133</volume>
<pages>140</pages>
<location>Trento,</location>
<contexts>
<context position="1507" citStr="Cutting et al., 1992" startWordPosition="241" endWordPosition="244">thod of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of cons</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Siburn, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Siburn. 1992. A practical part-ofspeech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing of the Association for Computational Linguistics, March 31–April 3, Trento, Italy, 133– 140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Eric Atwell</author>
</authors>
<title>The automatic tagging of the LOB corpus.</title>
<date>1983</date>
<journal>ICAME Journal: International Computer Archive of Modern and Medieval English Journal,</journal>
<pages>7--13</pages>
<contexts>
<context position="1470" citStr="Leech et al., 1983" startWordPosition="235" endWordPosition="238">In this paper, we present a new method of constructing tag dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from traini</context>
</contexts>
<marker>Leech, Garside, Atwell, 1983</marker>
<rawString>Geoffrey Leech, Roger Garside, and Eric Atwell. 1983. The automatic tagging of the LOB corpus. ICAME Journal: International Computer Archive of Modern and Medieval English Journal, 7:13–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Alexander Gelbukh (ed.), Computational Linguistics and Intelligent Text Processing, 12th International Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science 6608, Springer,</booktitle>
<pages>171--189</pages>
<contexts>
<context position="17403" citStr="Manning, 2011" startWordPosition="2910" endWordPosition="2911">ermitted tags, and a slightly faster version of Perl (v5.18.2). The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3wordsdistsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work (Moore, 2014). The results of these tests are shown in Table 2.6 6In Table 2, “OOV” has the standard meaning of a token 1306 Tagger WSJ All WSJ OOV WSJ Brown All Brown OOV Brown Tokens/Sec Accuracy Accuracy Tokens/Sec Accuracy Accuracy This work 102k 97.36% 91.09% 96k 96.55% 89.25% Our previous 51k/54k/69k 97.34% 90.98% 40k/43k/56k 96.54% 89.36% Stanford fast 80k 96.87% 89.69% 50k 95.53% 87.38% Table 2: WSJ test set and Brown c</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Alexander Gelbukh (ed.), Computational Linguistics and Intelligent Text Processing, 12th International Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science 6608, Springer, 171–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="13308" citStr="Manning et al, 2014" startWordPosition="2235" endWordPosition="2238"> no loss of tagging accuracy when a development-set-optimized KN-smoothed tag dictionary is also used at test time. 2.2 Tagging the Gigaword Corpus To construct our new tag dictionary, we need an automatically-tagged corpus several orders of magnitude larger than the hand-tagged WSJ training set. To obtain this corpus we ran a POS tagger on the LDC English Gigaword Fifth Edition3 corpus, which consists of more than 4 billion words of English text from seven newswire sources. We first removed all SGML mark-up, and performed sentence-breaking and tokenization using the Stanford CoreNLP toolkit (Manning et al, 2014). This produced 4,471,025,373 tokens of 3https://catalog.ldc.upenn.edu/ LDC2011T07 1305 Tag Dictionary Accuracy Tags/Token Unambig Tokens/Sec Pruned KN-smoothed 97.31% 3.48 45.3% 69k Unpruned semi-supervised 97.31% 1.97 51.7% 82k Pruned semi-supervised 97.31% 1.51 66.8% 103k Table 1: WSJ development set token accuracy and tagging speed for different tag dictionaries 6,616,812 unique words. We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Section 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the met</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, June 23–24, Baltimore, Maryland, USA, 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4704" citStr="Marcus et al., 1993" startWordPosition="777" endWordPosition="780">to find the highest scoring tag sequence is O(nITI). But this linear dependence on the size of the tag set means that reducing the average number of tags considered per token should further speed up tagging, whatever the underlying model or tagger may be. 1.2 Ratnaparkhi’s Method For each word observed in an annotated training set, Ratnaparkhi’s tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found (Moore, 2014) that while Ratnaparkhi’s tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>152--159</pages>
<location>New York, New York, USA,</location>
<contexts>
<context position="7578" citStr="McClosky et al., 2006" startWordPosition="1279" endWordPosition="1282">h We now present a new method that reduces the average number of tags per token to about 1.5, with no loss of tagging accuracy. We apply a simple variant of Ratnaparkhi’s method, with a training set more than 4,000 times larger than the Penn Treebank WSJ training set. Since no such handannotated corpus exists, we create the training set automatically by running a version of our tagger on the LDC English Gigaword corpus. We thus describe our approach as a semi-supervised variant of Ratnaparkhi’s method. Our method can be viewed as an instance of the well-known technique of self-training (e.g., McClosky et al., 2006), but ours is the first use of self-training we know of for learning inference-time search-space pruning. We introduce two additional modifications of Ratnaparki’s approach. First, with such a large training corpus, we find it unnecessary to keep in the dictionary every tag observed with every word in the automatically-annotated data. So, we estimate a probability distribution over tags for each word in the dictionary according to unsmoothed relative tag frequencies, and include for each word in the dictionary only tags whose probability given the word is greater than a fixed threshold. Second</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, June 4–9, New York, New York, USA, 152-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabalistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1524" citStr="Merialdo (1994)" startWordPosition="245" endWordPosition="246">g dictionaries for part-of-speech (POS) tagging. A tag dictionary is simply a list of words1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed. Tag dictionaries are commonly used to speed up POS-tag inference by restricting the tags considered for a particular word to those specified by the dictionary. Early work on POS tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of constructing a tag di</context>
<context position="3617" citStr="Merialdo (1994" startWordPosition="599" endWordPosition="601">f words wi, ... , wn. The tag sequence assigned the highest score by the model for a given word sequence is selected as the tagging for the word sequence. If T is the set of possible tags, and there are no restrictions on the form of the model, then the time to find the highest scoring tag sequence is potentially O(njT jn) or worse, which would be intractable. To make tagging practical, models are normally defined to be factorable in a way that reduces the time complexity to O(njT jk), for some small integer k. For models in which all tagging decisions are independent, or for higher-order mod2Merialdo (1994, p. 161) acknowledged this: “In some sense this is an optimal dictionary for this data, since a word will not have all its possible tags (in the language), but only the tags it actually had within the text.” 1303 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1303–1308, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. els pruned by fixed-width beam search, k = 1, so the time to find the highest scoring tag sequence is O(nITI). But this linear dependence on the size of the tag set means that reducing the av</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabalistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast high-accuracy partof-speech tagging by independent classifiers.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1165--1176</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2352" citStr="Moore, 2014" startWordPosition="380" endWordPosition="381">ks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi’s, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging—more than 100,000 tokens per second even in a Perl implementation. 1.1 Tag Dictionaries and Tagging Speed A typical modern POS tagger applies a statistical model to compute a score for</context>
<context position="4855" citStr="Moore, 2014" startWordPosition="803" endWordPosition="804">dered per token should further speed up tagging, whatever the underlying model or tagger may be. 1.2 Ratnaparkhi’s Method For each word observed in an annotated training set, Ratnaparkhi’s tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words. Ratnaparkhi reported that using this tag dictionary improved per-tag accuracy from 96.31% to 96.43% on his Penn Treebank (Marcus et al., 1993) Wall Street Journal (WSJ) development set, compared to considering all tags for all words. With a more accurate model, however, we found (Moore, 2014) that while Ratnaparkhi’s tag dictionary decreased the average number of tags per token from 45 to 3.7 on the current standard WSJ development set, it also decreased per-tag accuracy from 97.31% to 97.19%. This loss of accuracy can be explained by the fact that 0.5% of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44.5% accuracy with all word/tag pairs permitted. With Ratnaparkhi’s dictionary, accuracy for these tokens is necessarily 0%. 1.3 Our Previous Method We previously presented (Moore, 2014) a tag dictionary constructed </context>
<context position="10018" citStr="Moore, 2014" startWordPosition="1664" endWordPosition="1665">ure, feature set, and learning method we use for POS tagging are essentially the same as those in our earlier work, treating POS tagging as a single-token independent multiclass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper (Moore, 2014). The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001), using stochastic subgradient descent as described by Zhang (2004), with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time. Our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring inc</context>
<context position="13956" citStr="Moore, 2014" startWordPosition="2334" endWordPosition="2335">of 3https://catalog.ldc.upenn.edu/ LDC2011T07 1305 Tag Dictionary Accuracy Tags/Token Unambig Tokens/Sec Pruned KN-smoothed 97.31% 3.48 45.3% 69k Unpruned semi-supervised 97.31% 1.97 51.7% 82k Pruned semi-supervised 97.31% 1.51 66.8% 103k Table 1: WSJ development set token accuracy and tagging speed for different tag dictionaries 6,616,812 unique words. We tagged this corpus using the model described in Section 2.1 and a KN-smoothed tag dictionary as described in Section 1.3, with a threshold T = 0.0005. The tagger we used is based on the fastest of the methods described in our previous work (Moore, 2014, Section 3.1). Tagging took about 26 hours using a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. 2.3 Extracting the Tag Dictionary We extracted a Ratnaparkhi-like tag dictionary for the 957,819 words with 10 or more occurrences in our corpus. Tokens of all other words in the corpus were treated as unknown word tokens and used to define a set of 24 tags4 to be used for words not explicitly listed in the dictionary. To allow pruning the dictionary as described in Section 1.4, for each word (including the unknown word), we compu</context>
<context position="16272" citStr="Moore, 2014" startWordPosition="2728" endWordPosition="2729">Table 1 to tagging with our previous pruned KN-smoothed dictionary. The second column shows the accuracy per tag, which is 97.31% for all three dictionaries. The third column shows the mean number of tags per token allowed by each dictionary. The fourth column shows the percentage of tokens with only one tag allowed, which is significant since the tagger need not apply the model for such tokens—it can simply output the single possible tag. The last column shows the tagging speed in tokens per second for each of the three tag dictionaries, using the fast tagging method we previously described (Moore, 2014), in a singlethreaded implementation in Perl on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz processors. Speed is rounded to the nearest 1,000 tokens per second, because we measured times to a precision of only about one part in one hundred. For the pruned KN-smoothed dictionary, we previously reported a speed of 49,000 tokens per second under similar conditions. Our current faster speed of 69,000 tokens per second is due to an improved low-level implementation for computing the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2). The most restrictiv</context>
<context position="17585" citStr="Moore, 2014" startWordPosition="2943" endWordPosition="2944">lementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3wordsdistsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work (Moore, 2014). The results of these tests are shown in Table 2.6 6In Table 2, “OOV” has the standard meaning of a token 1306 Tagger WSJ All WSJ OOV WSJ Brown All Brown OOV Brown Tokens/Sec Accuracy Accuracy Tokens/Sec Accuracy Accuracy This work 102k 97.36% 91.09% 96k 96.55% 89.25% Our previous 51k/54k/69k 97.34% 90.98% 40k/43k/56k 96.54% 89.36% Stanford fast 80k 96.87% 89.69% 50k 95.53% 87.38% Table 2: WSJ test set and Brown corpus tagging speeds and token accuracies For our previous tagger, we give three speeds: the speed we reported earlier, a speed for a duplicate of the earlier experiment using the fa</context>
<context position="19167" citStr="Moore, 2014" startWordPosition="3215" endWordPosition="3216">anford tagger. The accuracy on the standard WSJ test set is 97.36%, one of the highest ever reported. The new tagger is also much faster than either of the other taggers, achieving a speed of more than 100,000 tokens per second on the WSJ test set, and almost 100,000 tokens per second on the out-of-domain Brown corpus data. 4 Conclusions Our method of constructing a tag dictionary is technically very simple, but remarkably effective. It reduces the mean number of possible tags per token by 57% and increases the number of unambiguous tokens by by 47%, compared to the previous state of the art (Moore, 2014) for a tag dictionary that does not degrade tagging accuracy. When combined with our previous work on fast high-accuracy POS tagging, this tag dictionary produces by far the fastest POS tagger reported with anything close to comparable accuracy. References Stanley F. Chen and Joshua T. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13(4):359–393. Kenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing of a wor</context>
</contexts>
<marker>Moore, 2014</marker>
<rawString>Robert C. Moore. 2014. Fast high-accuracy partof-speech tagging by independent classifiers. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, August 23–29, Dublin, Ireland, 1165–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1976" citStr="Ratnaparkhi (1996)" startWordPosition="324" endWordPosition="325">d tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus (Leech et al., 1983; Church, 1988; Cutting et al., 1992). Merialdo (1994) relied only on a tag dictionary extracted from annotated data, but he used the annotated 1According to the conventions of the field, POS tags are assigned to all tokens in a tokenized text, including punctuation marks and other non-word tokens. In this paper, all of these will be covered by the term word. tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.2 Ratnaparkhi (1996) seems to have been the first to use a tag dictionary automatically extracted only from training data. Ratnaparkhi’s method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state-of-the-art tagging model. We recently presented (Moore, 2014) a new method of constructing a tag dictionary that produces a tagging speed-up comparable to Ratnaparkhi’s, but with no decrease in tagging accuracy. In this paper, we show that a very simple semi-supervised variant of Ratn</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, May 17–18, Philadelphia, Pennsylvania, USA, 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Featurerich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, May 27–June 1,</booktitle>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="17387" citStr="Toutanova et al., 2003" startWordPosition="2906" endWordPosition="2909">g the model scores for permitted tags, and a slightly faster version of Perl (v5.18.2). The most restrictive tag dictionary, the pruned semi-supervised dictionary, allows only 1.51 tags per token, and our implementation runs at 103,000 tokens per second on the WSJ development set. For our final experiments, we tested our tagger with this dictionary on the standard Penn Treebank WSJ test set and on the Penn Treebank-3 parsed Brown corpus subset, as an out-of-domain evaluation. For comparison, we tested our previous tagger and the fast version (english-left3wordsdistsim) of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommended for practical use on the Stanford tagger website, which we found to be by far the fastest of the six publicly available taggers tested in our previous work (Moore, 2014). The results of these tests are shown in Table 2.6 6In Table 2, “OOV” has the standard meaning of a token 1306 Tagger WSJ All WSJ OOV WSJ Brown All Brown OOV Brown Tokens/Sec Accuracy Accuracy Tokens/Sec Accuracy Accuracy This work 102k 97.36% 91.09% 96k 96.55% 89.25% Our previous 51k/54k/69k 97.34% 90.98% 40k/43k/56k 96.54% 89.36% Stanford fast 80k 96.87% 89.69% 50k 95.53% 87.38% Table 2: WSJ test</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Featurerich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, May 27–June 1, Edmonton, Alberta, Canada, 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning, July 4–8,</booktitle>
<pages>919--926</pages>
<location>Banff, Alberta, Canada,</location>
<contexts>
<context position="10188" citStr="Zhang (2004)" startWordPosition="1691" endWordPosition="1692">ticlass classification task. Word-class-sequence features obtained by supervised clustering of the annotated training set replace the hidden tag-sequence features frequently used for POS tagging, and additional word-class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set. For full details of the feature set, see our previous paper (Moore, 2014). The model is trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer, 2001), using stochastic subgradient descent as described by Zhang (2004), with early stopping and averaging. The only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time. Our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not exceed the score of the highest scoring incorrect tag by a specified margin. In our new version</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Tong Zhang. 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the 21st International Conference on Machine Learning, July 4–8, Banff, Alberta, Canada, 919–926.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984267">
A large annotated corpus for learning natural language inference
</title>
<author confidence="0.975788">
Samuel R. Bowman*† Gabor Angeli†$
</author>
<email confidence="0.98033">
sbowman@stanford.edu angeli@stanford.edu
</email>
<author confidence="0.864536">
Christopher Potts* Christopher D. Manning*†$
</author>
<email confidence="0.973134">
cgpotts@stanford.edu manning@stanford.edu
</email>
<author confidence="0.819599">
*Stanford Linguistics †Stanford NLP Group $Stanford Computer Science
</author>
<sectionHeader confidence="0.95345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916227272728">
Understanding entailment and contradic-
tion is fundamental to understanding nat-
ural language, and inference about entail-
ment and contradiction is a valuable test-
ing ground for the development of seman-
tic representations. However, machine
learning research in this area has been dra-
matically limited by the lack of large-scale
resources. To address this, we introduce
the Stanford Natural Language Inference
corpus, a new, freely available collection
of labeled sentence pairs, written by hu-
mans doing a novel grounded task based
on image captioning. At 570K pairs, it
is two orders of magnitude larger than
all other resources of its type. This in-
crease in scale allows lexicalized classi-
fiers to outperform some sophisticated ex-
isting entailment models, and it allows a
neural network-based model to perform
competitively on natural language infer-
ence benchmarks for the first time.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999536285714286">
The semantic concepts of entailment and contra-
diction are central to all aspects of natural lan-
guage meaning (Katz, 1972; van Benthem, 2008),
from the lexicon to the content of entire texts.
Thus, natural language inference (NLI) — charac-
terizing and using these relations in computational
systems (Fyodorov et al., 2000; Condoravdi et al.,
2003; Bos and Markert, 2005; Dagan et al., 2006;
MacCartney and Manning, 2009) — is essential in
tasks ranging from information retrieval to seman-
tic parsing to commonsense reasoning.
NLI has been addressed using a variety of tech-
niques, including those based on symbolic logic,
knowledge bases, and neural networks. In recent
years, it has become an important testing ground
for approaches employing distributed word and
phrase representations. Distributed representa-
tions excel at capturing relations based in similar-
ity, and have proven effective at modeling simple
dimensions of meaning like evaluative sentiment
(e.g., Socher et al. 2013), but it is less clear that
they can be trained to support the full range of
logical and commonsense inferences required for
NLI (Bowman et al., 2015; Weston et al., 2015b;
Weston et al., 2015a). In a SemEval 2014 task
aimed at evaluating distributed representations for
NLI, the best-performing systems relied heavily
on additional features and reasoning capabilities
(Marelli et al., 2014a).
Our ultimate objective is to provide an empiri-
cal evaluation of learning-centered approaches to
NLI, advancing the case for NLI as a tool for
the evaluation of domain-general approaches to
semantic representation. However, in our view,
existing NLI corpora do not permit such an as-
sessment. They are generally too small for train-
ing modern data-intensive, wide-coverage models,
many contain sentences that were algorithmically
generated, and they are often beset with indeter-
minacies of event and entity coreference that sig-
nificantly impact annotation quality.
To address this, this paper introduces the Stan-
ford Natural Language Inference (SNLI) corpus,
a collection of sentence pairs labeled for entail-
ment, contradiction, and semantic independence.
At 570,152 sentence pairs, SNLI is two orders of
magnitude larger than all other resources of its
type. And, in contrast to many such resources,
all of its sentences and labels were written by hu-
mans in a grounded, naturalistic context. In a sepa-
rate validation phase, we collected four additional
judgments for each label for 56,941 of the exam-
ples. Of these, 98% of cases emerge with a three-
annotator consensus, and 58% see a unanimous
consensus from all five annotators.
In this paper, we use this corpus to evaluate
</bodyText>
<page confidence="0.961199">
632
</page>
<note confidence="0.99176">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.781477333333333">
A man inspects the uniform of a figure in some East
Asian country.
A black race car starts up in front of a crowd of
people.
contradiction
C C C C C
contradiction
C C C C C
The man is sleeping
Two men are smiling and laughing at the cats play-
ing on the floor.
A man is driving down a lonely road.
Some men are playing a sport.
A happy woman in a fairy costume holds an um-
brella.
An older and younger man smiling. neutral
NNENN
A soccer game with multiple males playing. entailment
EEEEE
A smiling costumed woman is holding an um- neutral
brella. NNECN
</figure>
<tableCaption confidence="0.971399">
Table 1: Randomly chosen examples from the development section of our new corpus, shown with both
</tableCaption>
<bodyText confidence="0.982442533333333">
the selected gold labels and the full set of labels (abbreviated) from the individual annotators, including
(in the first position) the label used by the initial author of the pair.
a variety of models for natural language infer-
ence, including rule-based systems, simple lin-
ear classifiers, and neural network-based models.
We find that two models achieve comparable per-
formance: a feature-rich classifier model and a
neural network model centered around a Long
Short-Term Memory network (LSTM; Hochreiter
and Schmidhuber 1997). We further evaluate the
LSTM model by taking advantage of its ready sup-
port for transfer learning, and show that it can be
adapted to an existing NLI challenge task, yielding
the best reported performance by a neural network
model and approaching the overall state of the art.
</bodyText>
<sectionHeader confidence="0.68386" genericHeader="method">
2 A new corpus for NLI
</sectionHeader>
<bodyText confidence="0.999557">
To date, the primary sources of annotated NLI cor-
pora have been the Recognizing Textual Entail-
ment (RTE) challenge tasks.1 These are generally
high-quality, hand-labeled data sets, and they have
stimulated innovative logical and statistical mod-
els of natural language reasoning, but their small
size (fewer than a thousand examples each) limits
their utility as a testbed for learned distributed rep-
resentations. The data for the SemEval 2014 task
called Sentences Involving Compositional Knowl-
edge (SICK) is a step up in terms of size, but
only to 4,500 training examples, and its partly
automatic construction introduced some spurious
patterns into the data (Marelli et al. 2014a, §6).
The Denotation Graph entailment set (Young et
al., 2014) contains millions of examples of en-
tailments between sentences and artificially con-
structed short phrases, but it was labeled using
fully automatic methods, and is noisy enough that
it is probably suitable only as a source of sup-
</bodyText>
<footnote confidence="0.921622">
1http://aclweb.org/aclwiki/index.php?
title=Textual—Entailment—Resource—Pool
</footnote>
<bodyText confidence="0.999825243243244">
plementary training data. Outside the domain of
sentence-level entailment, Levy et al. (2014) intro-
duce a large corpus of semi-automatically anno-
tated entailment examples between subject–verb–
object relation triples, and the second release of
the Paraphrase Database (Pavlick et al., 2015) in-
cludes automatically generated entailment anno-
tations over a large corpus of pairs of words and
short phrases.
Existing resources suffer from a subtler issue
that impacts even projects using only human-
provided annotations: indeterminacies of event
and entity coreference lead to insurmountable in-
determinacy concerning the correct semantic la-
bel (de Marneffe et al. 2008 §4.3; Marelli et al.
2014b). For an example of the pitfalls surround-
ing entity coreference, consider the sentence pair
A boat sank in the Pacific Ocean and A boat sank
in the Atlantic Ocean. The pair could be labeled
as a contradiction if one assumes that the two sen-
tences refer to the same single event, but could
also be reasonably labeled as neutral if that as-
sumption is not made. In order to ensure that our
labeling scheme assigns a single correct label to
every pair, we must select one of these approaches
across the board, but both choices present prob-
lems. If we opt not to assume that events are
coreferent, then we will only ever find contradic-
tions between sentences that make broad univer-
sal assertions, but if we opt to assume coreference,
new counterintuitive predictions emerge. For ex-
ample, Ruth Bader Ginsburg was appointed to the
US Supreme Court and I had a sandwich for lunch
today would unintuitively be labeled as a contra-
diction, rather than neutral, under this assumption.
Entity coreference presents a similar kind of in-
determinacy, as in the pair A tourist visited New
</bodyText>
<page confidence="0.998756">
633
</page>
<bodyText confidence="0.999695230769231">
York and A tourist visited the city. Assuming
coreference between New York and the city justi-
fies labeling the pair as an entailment, but with-
out that assumption the city could be taken to refer
to a specific unknown city, leaving the pair neu-
tral. This kind of indeterminacy of label can be re-
solved only once the questions of coreference are
resolved.
With SNLI, we sought to address the issues of
size, quality, and indeterminacy. To do this, we
employed a crowdsourcing framework with the
following crucial innovations. First, the exam-
ples were grounded in specific scenarios, and the
premise and hypothesis sentences in each exam-
ple were constrained to describe that scenario from
the same perspective, which helps greatly in con-
trolling event and entity coreference.2 Second, the
prompt gave participants the freedom to produce
entirely novel sentences within the task setting,
which led to richer examples than we see with the
more proscribed string-editing techniques of ear-
lier approaches, without sacrificing consistency.
Third, a subset of the resulting sentences were sent
to a validation task aimed at providing a highly re-
liable set of annotations over the same data, and at
identifying areas of inferential uncertainty.
</bodyText>
<subsectionHeader confidence="0.996061">
2.1 Data collection
</subsectionHeader>
<bodyText confidence="0.90637262962963">
We used Amazon Mechanical Turk for data col-
lection. In each individual task (each HIT), a
worker was presented with premise scene descrip-
tions from a pre-existing corpus, and asked to
supply hypotheses for each of our three labels—
entailment, neutral, and contradiction—forcing
the data to be balanced among these classes.
The instructions that we provided to the work-
ers are shown in Figure 1. Below the instructions
were three fields for each of three requested sen-
tences, corresponding to our entailment, neutral,
and contradiction labels, a fourth field (marked
optional) for reporting problems, and a link to an
FAQ page. That FAQ grew over the course of
data collection. It warned about disallowed tech-
niques (e.g., reusing the same sentence for many
different prompts, which we saw in a few cases),
provided guidance concerning sentence length and
2 Issues of coreference are not completely solved, but
greatly mitigated. For example, with the premise sentence
A dog is lying in the grass, a worker could safely assume that
the dog is the most prominent thing in the photo, and very
likely the only dog, and build contradicting sentences assum-
ing reference to the same dog.
We will show you the caption for a photo. We will not
show you the photo. Using only the caption and what
you know about the world:
</bodyText>
<listItem confidence="0.86328925">
• Write one alternate caption that is definitely a
true description of the photo. Example: For the
caption “Two dogs are running through a field.”
you could write “There are animals outdoors.”
• Write one alternate caption that might be a true
description of the photo. Example: For the cap-
tion “Two dogs are running through a field.” you
could write “Some puppies are running to catch a
stick.”
• Write one alternate caption that is definitely a
false description of the photo. Example: For the
caption “Two dogs are running through a field.”
you could write “The pets are sitting on a couch.”
This is different from the maybe correct category
because it’s impossible for the dogs to be both
running and sitting.
</listItem>
<figureCaption confidence="0.884444">
Figure 1: The instructions used on Mechanical
Turk for data collection.
</figureCaption>
<bodyText confidence="0.993278666666667">
complexity (we did not enforce a minimum length,
and we allowed bare NPs as well as full sen-
tences), and reviewed logistical issues around pay-
ment timing. About 2,500 workers contributed.
For the premises, we used captions from the
Flickr30k corpus (Young et al., 2014), a collection
of approximately 160k captions (corresponding to
about 30k images) collected in an earlier crowd-
sourced effort.3 The captions were not authored
by the photographers who took the source images,
and they tend to contain relatively literal scene de-
scriptions that are suited to our approach, rather
than those typically associated with personal pho-
tographs (as in their example: Our trip to the
Olympic Peninsula). In order to ensure that the la-
bel for each sentence pair can be recovered solely
based on the available text, we did not use the im-
ages at all during corpus collection.
Table 2 reports some key statistics about the col-
lected corpus, and Figure 2 shows the distributions
of sentence lengths for both our source hypotheses
and our newly collected premises. We observed
that while premise sentences varied considerably
in length, hypothesis sentences tended to be as
3 We additionally include about 4k sentence pairs from
a pilot study in which the premise sentences were instead
drawn from the VisualGenome corpus (under construction;
visualgenome.org). These examples appear only in the
training set, and have pair identifiers prefixed with vg in our
corpus.
</bodyText>
<page confidence="0.989662">
634
</page>
<figure confidence="0.925987297297297">
Data set sizes:
228
Training pairs
15994 550,152
13
Development pairs
11047 1410,000
Test pairs
760 1510,000
Sentence length:
32
Premise mean token count 14.1
2631
Hypothesis mean token count 8.3
1878
Parseroutput:
1325
Premise‘S’-rooted parses
911 2174.0%
Hypothesis ‘S’-rooted parses 642 2288.9%
Distinct words (ignoring case) 37,026
449 23
198
Table 2: Key statistics for the raw sentence pairs
7 2
07 1 2
in SNLI. Since the two halves of each pair were
3 38 7
438
collected separately, we report some statistics for
84 28
631
both.
short as possible while still providing enough in-
4343132
086
formation to yield a clear judgment, clustering at
23 33
912
around seven words. We also observed that the
6 34
897
bulk of the sentences from both sources were syn-
19 35
774
tactically complete rather than fragments, and the
8 36
453
frequency with which the parser produces a parse
12 37
618
rooted with an‘S’ (sentence) node attests to this.
4 38
330
2.2 Data validation
2
2 In order to measure the quality of our corpus,
0
and in order to construct maximally useful test-
225 1 43
162
ing and development sets, we performed an addi-
144
108
tional round of validation for about 10% of our
1 48
data. This validation phase followed the same
87 1 5
basic form as the Mechanical Turk labeling task
60 2 55
used to label the SICK entailment data: we pre-
36 1 56
sented workers with pairs of sentences in batches
90 1 60
</figure>
<bodyText confidence="0.871407444444445">
of five, and asked them to choose a single label
21 1 62
for each pair. We supplied each pair to four an-
notators, yielding five labels per pair including the
label used by the original author. The instructions
36
were similar to the instructions for initial data col-
24
lection shown in Figure 1, and linked to a similar
</bodyText>
<page confidence="0.691219">
63
18
</page>
<bodyText confidence="0.9728695">
FAQ. Though we initially used a very restrictive
qualification (based on past approval rate) to se-
</bodyText>
<figure confidence="0.371376636363636">
6
lect workers for the validation task, we nonethe-
27
less discovered (and deleted) some instances of
6
random guessing in an early batch of work, and
3
subsequently instituted a fully closed qualification
3
restricted to about 30 trusted workers.
3
</figure>
<footnote confidence="0.7736975">
6For each pair that we validated, we assigned a
gold label. If any one of the three labels was cho-
</footnote>
<page confidence="0.788383">
3
</page>
<bodyText confidence="0.570464">
sen by at least three of the five annotators, it was
</bodyText>
<page confidence="0.920689">
3
</page>
<figure confidence="0.993676">
0 5 10 15 20 25 30 35 40
Sentence length (tokens)
</figure>
<figureCaption confidence="0.999968">
Figure 2: The distribution of sentence length.
</figureCaption>
<bodyText confidence="0.999968038461539">
chosen as the gold label. If there was no such con-
sensus, which occurred in about 2% of cases, we
assigned the placeholder label ‘-’. While these un-
labeled examples are included in the corpus dis-
tribution, they are unlikely to be helpful for the
standard NLI classification task, and we do not in-
clude them in either training or evaluation in the
experiments that we discuss in this paper.
The results of this validation process are sum-
marized in Table 3. Nearly all of the examples
received a majority label, indicating broad con-
sensus about the nature of the data and categories.
The gold-labeled examples are very nearly evenly
distributed across the three labels. The Fleiss
κ scores (computed over every example with a
full five annotations) are likely to be conservative
given our large and unevenly distributed pool of
annotators, but they still provide insights about the
levels of disagreement across the three semantic
classes. This disagreement likely reflects not just
the limitations of large crowdsourcing efforts but
also the uncertainty inherent in naturalistic NLI.
Regardless, the overall rate of agreement is ex-
tremely high, suggesting that the corpus is suffi-
ciently high quality to pose a challenging but real-
istic machine learning task.
</bodyText>
<subsectionHeader confidence="0.997935">
2.3 The distributed corpus
</subsectionHeader>
<bodyText confidence="0.999941666666667">
Table 1 shows a set of randomly chosen validated
examples from the development set with their la-
bels. Qualitatively, we find the data that we col-
lected draws fairly extensively on commonsense
knowledge, and that hypothesis and premise sen-
tences often differ structurally in significant ways,
suggesting that there is room for improvement be-
yond superficial word alignment models. We also
find the sentences that we collected to be largely
</bodyText>
<figure confidence="0.998983">
Number of sentences
100,000
70,000
40,000
90,000
80,000
60,000
50,000
30,000
20,000
10,000
0
Premise Hypothesis
</figure>
<page confidence="0.941997">
635
</page>
<table confidence="0.980436066666667">
General:
Validated pairs 56,951
Pairs w/ unanimous gold label 58.3%
Individual annotator label agreement:
Individual label = gold label 89.0%
Individual label = author’s label 85.8%
Gold label/author’s label agreement:
Gold label = author’s label 91.2%
Gold label =� author’s label 6.8%
No gold label (no 3 labels match) 2.0%
Fleiss κ:
contradiction 0.77
entailment 0.72
neutral 0.60
Overall 0.70
</table>
<tableCaption confidence="0.97901">
Table 3: Statistics for the validated pairs. The au-
</tableCaption>
<bodyText confidence="0.995522615384615">
thor’s label is the label used by the worker who
wrote the premise to create the sentence pair. A
gold label reflects a consensus of three votes from
among the author and the four annotators.
fluent, correctly spelled English, with a mix of
full sentences and caption-style noun phrase frag-
ments, though punctuation and capitalization are
often omitted.
The corpus is available under a CreativeCom-
mons Attribution-ShareAlike license, the same li-
cense used for the Flickr30k source captions. It
can be downloaded at:
nlp.stanford.edu/projects/snli/
Partition We distribute the corpus with a pre-
specified train/test/development split. The test
and development sets contain 10k examples each.
Each original ImageFlickr caption occurs in only
one of the three sets, and all of the examples in the
test and development sets have been validated.
Parses The distributed corpus includes parses
produced by the Stanford PCFG Parser 3.5.2
(Klein and Manning, 2003), trained on the stan-
dard training set as well as on the Brown Corpus
(Francis and Kucera 1979), which we found to im-
prove the parse quality of the descriptive sentences
and noun phrases found in the descriptions.
</bodyText>
<sectionHeader confidence="0.89376" genericHeader="method">
3 Our data as a platform for evaluation
</sectionHeader>
<bodyText confidence="0.997528">
The most immediate application for our corpus is
in developing models for the task of NLI. In par-
</bodyText>
<table confidence="0.99982325">
System SNLI SICK RTE-3
Edit Distance Based 71.9 65.4 61.9
Classifier Based 72.2 71.4 61.5
+ Lexical Resources 75.0 78.8 63.6
</table>
<tableCaption confidence="0.7904685">
Table 4: 2-class test accuracy for two simple
baseline systems included in the Excitement Open
</tableCaption>
<bodyText confidence="0.999167571428571">
Platform, as well as SICK and RTE results for a
model making use of more sophisticated lexical
resources.
ticular, since it is dramatically larger than any ex-
isting corpus of comparable quality, we expect it to
be suitable for training parameter-rich models like
neural networks, which have not previously been
competitive at this task. Our ability to evaluate
standard classifier-base NLI models, however, was
limited to those which were designed to scale to
SNLI’s size without modification, so a more com-
plete comparison of approaches will have to wait
for future work. In this section, we explore the per-
formance of three classes of models which could
scale readily: (i) models from a well-known NLI
system, the Excitement Open Platform; (ii) vari-
ants of a strong but simple feature-based classi-
fier model, which makes use of both unlexicalized
and lexicalized features, and (iii) distributed repre-
sentation models, including a baseline model and
neural network sequence models.
</bodyText>
<subsectionHeader confidence="0.997378">
3.1 Excitement Open Platform models
</subsectionHeader>
<bodyText confidence="0.999959210526316">
The first class of models is from the Excitement
Open Platform (EOP, Pad´o et al. 2014; Magnini
et al. 2014)—an open source platform for RTE re-
search. EOP is a tool for quickly developing NLI
systems while sharing components such as com-
mon lexical resources and evaluation sets. We
evaluate on two algorithms included in the dis-
tribution: a simple edit-distance based algorithm
and a classifier-based algorithm, the latter both in
a bare form and augmented with EOP’s full suite
of lexical resources.
Our initial goal was to better understand the dif-
ficulty of the task of classifying SNLI corpus in-
ferences, rather than necessarily the performance
of a state-of-the-art RTE system. We approached
this by running the same system on several data
sets: our own test set, the SICK test data, and the
standard RTE-3 test set (Giampiccolo et al., 2007).
We report results in Table 4. Each of the models
</bodyText>
<page confidence="0.99817">
636
</page>
<bodyText confidence="0.999755789473684">
was separately trained on the training set of each
corpus. All models are evaluated only on 2-class
entailment. To convert 3-class problems like SICK
and SNLI to this setting, all instances of contradic-
tion and unknown are converted to nonentailment.
This yields a most-frequent-class baseline accu-
racy of 66% on SNLI, and 71% on SICK. This is
intended primarily to demonstrate the difficulty of
the task, rather than necessarily the performance
of a state-of-the-art RTE system. The edit dis-
tance algorithm tunes the weight of the three case-
insensitive edit distance operations on the train-
ing set, after removing stop words. In addition
to the base classifier-based system distributed with
the platform, we train a variant which includes in-
formation from WordNet (Miller, 1995) and Verb-
Ocean (Chklovski and Pantel, 2004), and makes
use of features based on tree patterns and depen-
dency tree skeletons (Wang and Neumann, 2007).
</bodyText>
<subsectionHeader confidence="0.999005">
3.2 Lexicalized Classifier
</subsectionHeader>
<bodyText confidence="0.999981142857143">
Unlike the RTE datasets, SNLI’s size supports ap-
proaches which make use of rich lexicalized fea-
tures. We evaluate a simple lexicalized classifier
to explore the ability of non-specialized models to
exploit these features in lieu of more involved lan-
guage understanding. Our classifier implements 6
feature types; 3 unlexicalized and 3 lexicalized:
</bodyText>
<listItem confidence="0.92716895">
1. The BLEU score of the hypothesis with re-
spect to the premise, using an n-gram length
between 1 and 4.
2. The length difference between the hypothesis
and the premise, as a real-valued feature.
3. The overlap between words in the premise
and hypothesis, both as an absolute count and
a percentage of possible overlap, and both
over all words and over just nouns, verbs, ad-
jectives, and adverbs.
4. An indicator for every unigram and bigram in
the hypothesis.
5. Cross-unigrams: for every pair of words
across the premise and hypothesis which
share a POS tag, an indicator feature over the
two words.
6. Cross-bigrams: for every pair of bigrams
across the premise and hypothesis which
share a POS tag on the second word, an in-
dicator feature over the two bigrams.
</listItem>
<bodyText confidence="0.998257">
We report results in Table 5, along with abla-
tion studies for removing the cross-bigram fea-
tures (leaving only the cross-unigram feature) and
</bodyText>
<table confidence="0.8210102">
System SNLI SICK
Train Test Train Test
Lexicalized 99.7 78.2 90.4 77.8
Unigrams Only 93.1 71.6 88.1 77.0
Unlexicalized 49.4 50.4 69.9 69.6
</table>
<tableCaption confidence="0.92733">
Table 5: 3-class accuracy, training on either our
</tableCaption>
<bodyText confidence="0.989820333333334">
data or SICK, including models lacking cross-
bigram features (Feature 6), and lacking all lexical
features (Features 4–6). We report results both on
the test set and the training set to judge overfitting.
for removing all lexicalized features. On our large
corpus in particular, there is a substantial jump in
accuracy from using lexicalized features, and an-
other from using the very sparse cross-bigram fea-
tures. The latter result suggests that there is value
in letting the classifier automatically learn to rec-
ognize structures like explicit negations and adjec-
tive modification. A similar result was shown in
Wang and Manning (2012) for bigram features in
sentiment analysis.
It is surprising that the classifier performs as
well as it does without any notion of alignment
or tree transformations. Although we expect that
richer models would perform better, the results
suggest that given enough data, cross bigrams with
the noisy part-of-speech overlap constraint can
produce an effective model.
</bodyText>
<subsectionHeader confidence="0.999833">
3.3 Sentence embeddings and NLI
</subsectionHeader>
<bodyText confidence="0.999981736842105">
SNLI is suitably large and diverse to make it pos-
sible to train neural network models that produce
distributed representations of sentence meaning.
In this section, we compare the performance of
three such models on the corpus. To focus specif-
ically on the strengths of these models at produc-
ing informative sentence representations, we use
sentence embedding as an intermediate step in the
NLI classification task: each model must produce
a vector representation of each of the two sen-
tences without using any context from the other
sentence, and the two resulting vectors are then
passed to a neural network classifier which pre-
dicts the label for the pair. This choice allows us to
focus on existing models for sentence embedding,
and it allows us to evaluate the ability of those
models to learn useful representations of mean-
ing (which may be independently useful for sub-
sequent tasks), at the cost of excluding from con-
</bodyText>
<page confidence="0.992892">
637
</page>
<table confidence="0.787516444444444">
Sentence model Train Test
100d Sum of words 79.3 75.3
100d RNN 73.1 72.2
100d LSTM RNN 84.8 77.6
3-way softmax classifier
200d tanh layer
200d tanh layer
200d tanh layer
100d premise 100d hypothesis
</table>
<tableCaption confidence="0.8359925">
Table 6: Accuracy in 3-class classification on our
training and test sets for each model.
sentence model sentence model
with premise input with hypothesis input
</tableCaption>
<figureCaption confidence="0.97936">
Figure 3: The neural network classification archi-
</figureCaption>
<bodyText confidence="0.993046177419355">
tecture: for each sentence embedding model eval-
uated in Tables 6 and 7, two identical copies of
the model are run with the two sentences as input,
and their outputs are used as the two 100d inputs
shown here.
sideration possible strong neural models for NLI
that directly compare the two inputs at the word or
phrase level.
Our neural network classifier, depicted in Fig-
ure 3 (and based on a one-layer model in Bow-
man et al. 2015), is simply a stack of three 200d
tanh layers, with the bottom layer taking the con-
catenated sentence representations as input and the
top layer feeding a softmax classifier, all trained
jointly with the sentence embedding model itself.
We test three sentence embedding models, each
set to use 100d phrase and sentence embeddings.
Our baseline sentence embedding model simply
sums the embeddings of the words in each sen-
tence. In addition, we experiment with two simple
sequence embedding models: a plain RNN and an
LSTM RNN (Hochreiter and Schmidhuber, 1997).
The word embeddings for all of the models are
initialized with the 300d reference GloVe vectors
(840B token version, Pennington et al. 2014) and
fine-tuned as part of training. In addition, all
of the models use an additional tanh neural net-
work layer to map these 300d embeddings into
the lower-dimensional phrase and sentence em-
bedding space. All of the models are randomly
initialized using standard techniques and trained
using AdaDelta (Zeiler, 2012) minibatch SGD un-
til performance on the development set stops im-
proving. We applied L2 regularization to all mod-
els, manually tuning the strength coefficient λ for
each, and additionally applied dropout (Srivastava
et al., 2014) to the inputs and outputs of the sen-
tence embedding models (though not to its internal
connections) with a fixed dropout rate. All mod-
els were implemented in a common framework for
this paper, and the implementations will be made
available at publication time.
The results are shown in Table 6. The sum
of words model performed slightly worse than
the fundamentally similar lexicalized classifier—
while the sum of words model can use pretrained
word embeddings to better handle rare words, it
lacks even the rudimentary sensitivity to word or-
der that the lexicalized model’s bigram features
provide. Of the two RNN models, the LSTM’s
more robust ability to learn long-term dependen-
cies serves it well, giving it a substantial advan-
tage over the plain RNN, and resulting in perfor-
mance that is essentially equivalent to the lexical-
ized classifier on the test set (LSTM performance
near the stopping iteration varies by up to 0.5%
between evaluation steps). While the lexicalized
model fits the training set almost perfectly, the gap
between train and test set accuracy is relatively
small for all three neural network models, suggest-
ing that research into significantly higher capacity
versions of these models would be productive.
</bodyText>
<subsectionHeader confidence="0.999098">
3.4 Analysis and discussion
</subsectionHeader>
<bodyText confidence="0.999967866666667">
Figure 4 shows a learning curve for the LSTM and
the lexicalized and unlexicalized feature-based
models. It shows that the large size of the corpus
is crucial to both the LSTM and the lexicalized
model, and suggests that additional data would
yield still better performance for both. In addi-
tion, though the LSTM and the lexicalized model
show similar performance when trained on the cur-
rent full corpus, the somewhat steeper slope for
the LSTM hints that its ability to learn arbitrar-
ily structured representations of sentence mean-
ing may give it an advantage over the more con-
strained lexicalized model on still larger datasets.
We were struck by the speed with which the
lexicalized classifier outperforms its unlexicalized
</bodyText>
<page confidence="0.984942">
638
</page>
<figure confidence="0.993246222222222">
Unlexicalized Lexicalized LSTM
80
70
60
50
40
30
1 10 100 1,000 10,000 100,000 1,000,000
Training pairs used (log scale)
</figure>
<figureCaption confidence="0.8080675">
Figure 4: A learning curve showing how the
baseline classifiers and the LSTM perform when
trained to convergence on varied amounts of train-
ing data. The y-axis starts near a random-chance
</figureCaption>
<bodyText confidence="0.99902085074627">
accuracy of 33%. The minibatch size of 64 that
we used to tune the LSTM sets a lower bound on
data for that model.
counterpart. With only 100 training examples, the
cross-bigram classifier is already performing bet-
ter. Empirically, we find that the top weighted
features for the classifier trained on 100 examples
tend to be high precision entailments; e.g., playing
→ outside (most scenes are outdoors), a banana
→ person eating. If relatively few spurious entail-
ments get high weight—as it appears is the case—
then it makes sense that, when these do fire, they
boost accuracy in identifying entailments.
There are revealing patterns in the errors com-
mon to all the models considered here. Despite
the large size of the training corpus and the distri-
butional information captured by GloVe initializa-
tion, many lexical relationships are still misana-
lyzed, leading to incorrect predictions of indepen-
dent, even for pairs that are common in the train-
ing corpus like beach/surf and sprinter/runner.
Semantic mistakes at the phrasal level (e.g., pre-
dicting contradiction for A male is placing an
order in a deli/A man buying a sandwich at a
deli) indicate that additional attention to composi-
tional semantics would pay off. However, many of
the persistent problems run deeper, to inferences
that depend on world knowledge and context-
specific inferences, as in the entailment pair A race
car driver leaps from a burning car/A race car
driver escaping danger, for which both the lex-
icalized classifier and the LSTM predict neutral.
In other cases, the models’ attempts to shortcut
this kind of inference through lexical cues can lead
them astray. Some of these examples have quali-
ties reminiscent of Winograd schemas (Winograd,
1972; Levesque, 2013). For example, all the mod-
els wrongly predict entailment for A young girl
throws sand toward the ocean/A girl can’t stand
the ocean, presumably because of distributional
associations between throws and can’t stand.
Analysis of the models’ predictions also yields
insights into the extent to which they grapple with
event and entity coreference. For the most part, the
original image prompts contained a focal element
that the caption writer identified with a syntac-
tic subject, following information structuring con-
ventions associating subjects and topics in English
(Ward and Birner, 2004). Our annotators generally
followed suit, writing sentences that, while struc-
turally diverse, share topic/focus (theme/rheme)
structure with their premises. This promotes a
coherent, situation-specific construal of each sen-
tence pair. This is information that our models
can easily take advantage of, but it can lead them
astray. For instance, all of them stumble with the
amusingly simple case A woman prepares ingre-
dients for a bowl of soup/A soup bowl prepares a
woman, in which prior expectations about paral-
lelism are not met. Another headline example of
this type is A man wearing padded arm protec-
tion is being bitten by a German shepherd dog/A
man bit a dog, which all the models wrongly di-
agnose as entailment, though the sentences report
two very different stories. A model with access
to explicit information about syntactic or semantic
structure should perform better on cases like these.
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="method">
4 Transfer learning with SICK
</sectionHeader>
<bodyText confidence="0.999907333333333">
To the extent that successfully training a neural
network model like our LSTM on SNLI forces that
model to encode broadly accurate representations
of English scene descriptions and to build an en-
tailment classifier over those relations, we should
expect it to be readily possible to adapt the trained
model for use on other NLI tasks. In this section,
we evaluate on the SICK entailment task using a
simple transfer learning method (Pratt et al., 1991)
and achieve competitive results.
To perform transfer, we take the parameters of
the LSTM RNN model trained on SNLI and use
them to initialize a new model, which is trained
from that point only on the training portion of
SICK. The only newly initialized parameters are
</bodyText>
<sectionHeader confidence="0.272848" genericHeader="method">
% Accuracy
</sectionHeader>
<page confidence="0.905079">
639
</page>
<table confidence="0.985938">
Training sets Train Test
Our data only 42.0 46.7
SICK only 100.0 71.3
Our data and SICK (transfer) 99.9 80.8
</table>
<tableCaption confidence="0.991767">
Table 7: LSTM 3-class accuracy on the SICK
train and test sets under three training regimes.
</tableCaption>
<bodyText confidence="0.99985625">
softmax layer parameters and the embeddings for
words that appear in SICK, but not in SNLI (which
are populated with GloVe embeddings as above).
We use the same model hyperparameters that were
used to train the original model, with the excep-
tion of the L2 regularization strength, which is
re-tuned. We additionally transfer the accumula-
tors that are used by AdaDelta to set the learn-
ing rates. This lowers the starting learning rates,
and is intended to ensure that the model does not
learn too quickly in its first few epochs after trans-
fer and destroy the knowledge accumulated in the
pre-transfer phase of training.
The results are shown in Table 7. Training
on SICK alone yields poor performance, and the
model trained on SNLI fails when tested on SICK
data, labeling more neutral examples as contradic-
tions than correctly, possibly as a result of subtle
differences in how the labeling task was presented.
In contrast, transferring SNLI representations to
SICK yields the best performance yet reported for
an unaugmented neural network model, surpasses
the available EOP models, and approaches both
the overall state of the art at 84.6% (Lai and Hock-
enmaier, 2014) and the 84% level of interannota-
tor agreement, which likely represents an approx-
imate performance ceiling. This suggests that the
introduction of a large high-quality corpus makes
it possible to train representation-learning models
for sentence meaning that are competitive with the
best hand-engineered models on inference tasks.
We attempted to apply this same transfer evalu-
ation technique to the RTE-3 challenge, but found
that the small training set (800 examples) did not
allow the model to adapt to the unfamiliar genre of
text used in that corpus, such that no training con-
figuration yielded competitive performance. Fur-
ther research on effective transfer learning on
small data sets with neural models might facilitate
improvements here.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991181818182">
Natural languages are powerful vehicles for rea-
soning, and nearly all questions about meaning-
fulness in language can be reduced to questions of
entailment and contradiction in context. This sug-
gests that NLI is an ideal testing ground for the-
ories of semantic representation, and that training
for NLI tasks can provide rich domain-general se-
mantic representations. To date, however, it has
not been possible to fully realize this potential due
to the limited nature of existing NLI resources.
This paper sought to remedy this with a new, large-
scale, naturalistic corpus of sentence pairs labeled
for entailment, contradiction, and independence.
We used this corpus to evaluate a range of models,
and found that both simple lexicalized models and
neural network models perform well, and that the
representations learned by a neural network model
on our corpus can be used to dramatically improve
performance on a standard challenge dataset. We
hope that SNLI presents valuable training data and
a challenging testbed for the continued application
of machine learning to semantic representation.
</bodyText>
<sectionHeader confidence="0.997311" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997065625">
We gratefully acknowledge support from a Google
Faculty Research Award, a gift from Bloomberg
L.P., the Defense Advanced Research Projects
Agency (DARPA) Deep Exploration and Filter-
ing of Text (DEFT) Program under Air Force Re-
search Laboratory (AFRL) contract no. FA8750-
13-2-0040, the National Science Foundation un-
der grant no. IIS 1159679, and the Department
of the Navy, Office of Naval Research, under
grant no. N00014-10-1-0109. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of Google,
Bloomberg L.P., DARPA, AFRL NSF, ONR, or
the US government. We also thank our many ex-
cellent Mechanical Turk contributors.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996525">
Johan Bos and Katja Markert. 2005. Recognising
textual entailment with logical inference. In Proc.
EMNLP.
Samuel R. Bowman, Christopher Potts, and Christo-
pher D. Manning. 2015. Recursive neural networks
can learn logical semantics. In Proc. of the 3rd
Workshop on Continuous Vector Space Models and
their Compositionality.
</reference>
<page confidence="0.972051">
640
</page>
<reference confidence="0.999734452830188">
Timothy Chklovski and Patrick Pantel. 2004. Verb-
Ocean: Mining the web for fine-grained semantic
verb relations. In Proc. EMNLP.
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. En-
tailment, intensionality and text understanding. In
Proc. of the HLT-NAACL 2003 Workshop on Text
Meaning.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising tectual entailment, pages 177–
190. Springer.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proc. ACL.
W. Nelson Francis and Henry Kucera. 1979. Brown
corpus manual. Brown University.
Yaroslav Fyodorov, Yoad Winter, and Nissim Francez.
2000. A natural logic inference system. In Proc.
of the 2nd Workshop on Inference in Computational
Semantics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL workshop on textual entailment and
paraphrasing.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Jerrold J. Katz. 1972. Semantic Theory. Harper &amp;
Row, New York.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. ACL.
Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. In Proc. SemEval.
Hector J. Levesque. 2013. On our best behaviour. In
Proc. AAAI.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014.
Focused entailment graphs for open IE propositions.
In Proc. CoNLL.
Bill MacCartney and Christopher D Manning. 2009.
An extended model of natural logic. In Proc. of the
Eighth International Conference on Computational
Semantics.
Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin
Eichler, G¨unter Neumann, Tae-Gil Noh, Sebastian
Pado, Asher Stern, and Omer Levy. 2014. The Ex-
citement Open Platform for textual inferences. Proc.
ACL.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proc. SemEval.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A SICK cure for the evaluation
of compositional distributional semantic models. In
Proc. LREC.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realization of
a modular architecture for textual entailment. Jour-
nal of Natural Language Engineering.
Ellie Pavlick, Johan Bos, Malvina Nissim, Charley
Beller, Ben Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. In Proc. ACL.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
representation. In Proc. EMNLP.
Lorien Y Pratt, Jack Mostow, Candace A Kamm, and
Ace A Kamm. 1991. Direct transfer of learned in-
formation among neural networks. In Proc. AAAI.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. EMNLP.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. JMLR.
Johan van Benthem. 2008. A brief history of natural
logic. In M. Chakraborty, B. L¨owe, M. Nath Mi-
tra, and S. Sarukki, editors, Logic, Navya-Nyaya
and Applications: Homage to Bimal Matilal. Col-
lege Publications.
Sida I. Wang and Christopher D. Manning. 2012.
Baselines and bigrams: Simple, good sentiment and
topic classification. In Proc. ACL.
Rui Wang and G¨unter Neumann. 2007. Recognizing
textual entailment using sentence similarity based on
dependency tree skeletons. In ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing.
Gregory Ward and Betty Birner. 2004. Information
structure and non-canonical syntax. In Laurence R.
Horn and Gregory Ward, editors, Handbook of Prag-
matics, pages 153–174. Blackwell, Oxford.
</reference>
<page confidence="0.978394">
641
</page>
<reference confidence="0.99904325">
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015a. Towards AI-complete
question answering: A set of prerequisite toy tasks.
arXiv:1502.05698.
Jason Weston, Sumit Chopra, and Antoine Bordes.
2015b. Memory networks. In Proc. ICLR.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology, 3(1):1–191.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to vi-
sual denotations: New similarity metrics for seman-
tic inference over event descriptions. TACL, 2:67–
78.
Matthew D. Zeiler. 2012. ADADELTA: an
adaptive learning rate method. arXiv preprint
arXiv:1212.5701.
</reference>
<page confidence="0.997987">
642
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.080535">
<title confidence="0.955678">A large annotated corpus for learning natural language inference</title>
<author confidence="0.828439">R</author>
<email confidence="0.975066">sbowman@stanford.eduangeli@stanford.edu</email>
<affiliation confidence="0.223087">D.</affiliation>
<email confidence="0.969239">cgpotts@stanford.edumanning@stanford.edu</email>
<affiliation confidence="0.433433">Linguistics NLP Group Computer Science</affiliation>
<abstract confidence="0.998339086956522">Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1571" citStr="Bos and Markert, 2005" startWordPosition="226" endWordPosition="229">ncrease in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Soc</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
<author>Christopher Potts</author>
<author>Christopher D Manning</author>
</authors>
<title>Recursive neural networks can learn logical semantics.</title>
<date>2015</date>
<booktitle>In Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.</booktitle>
<contexts>
<context position="2336" citStr="Bowman et al., 2015" startWordPosition="345" endWordPosition="348">reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive,</context>
<context position="26431" citStr="Bowman et al. 2015" startWordPosition="4319" endWordPosition="4323">3-class classification on our training and test sets for each model. sentence model sentence model with premise input with hypothesis input Figure 3: The neural network classification architecture: for each sentence embedding model evaluated in Tables 6 and 7, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here. sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level. Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the mode</context>
</contexts>
<marker>Bowman, Potts, Manning, 2015</marker>
<rawString>Samuel R. Bowman, Christopher Potts, and Christopher D. Manning. 2015. Recursive neural networks can learn logical semantics. In Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="22053" citStr="Chklovski and Pantel, 2004" startWordPosition="3595" endWordPosition="3598">ntradiction and unknown are converted to nonentailment. This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words. In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007). 3.2 Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding. Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized: 1. The BLEU score of the hypothesis with respect to the premise, using an n-gram length between 1 and 4. 2. The length</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cleo Condoravdi</author>
<author>Dick Crouch</author>
<author>Valeria de Paiva</author>
<author>Reinhard Stolle</author>
<author>Daniel G Bobrow</author>
</authors>
<title>Entailment, intensionality and text understanding.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL 2003 Workshop on Text Meaning.</booktitle>
<marker>Condoravdi, Crouch, de Paiva, Stolle, Bobrow, 2003</marker>
<rawString>Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Reinhard Stolle, and Daniel G. Bobrow. 2003. Entailment, intensionality and text understanding. In Proc. of the HLT-NAACL 2003 Workshop on Text Meaning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1591" citStr="Dagan et al., 2006" startWordPosition="230" endWordPosition="233"> lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), bu</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177– 190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<date>1979</date>
<institution>Brown University.</institution>
<note>Brown corpus manual.</note>
<contexts>
<context position="18834" citStr="Francis and Kucera 1979" startWordPosition="3068" endWordPosition="3071">license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/ Partition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated. Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions. 3 Our data as a platform for evaluation The most immediate application for our corpus is in developing models for the task of NLI. In parSystem SNLI SICK RTE-3 Edit Distance Based 71.9 65.4 61.9 Classifier Based 72.2 71.4 61.5 + Lexical Resources 75.0 78.8 63.6 Table 4: 2-class test accuracy for two simple baseline systems included in the Excitement Open Platform, as well as SICK and RTE results for a model making use of more sophisticated lexical resources. ticular, since it </context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. Nelson Francis and Henry Kucera. 1979. Brown corpus manual. Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaroslav Fyodorov</author>
<author>Yoad Winter</author>
<author>Nissim Francez</author>
</authors>
<title>A natural logic inference system.</title>
<date>2000</date>
<booktitle>In Proc. of the 2nd Workshop on Inference in Computational Semantics.</booktitle>
<contexts>
<context position="1523" citStr="Fyodorov et al., 2000" startWordPosition="218" endWordPosition="221">ger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions</context>
</contexts>
<marker>Fyodorov, Winter, Francez, 2000</marker>
<rawString>Yaroslav Fyodorov, Yoad Winter, and Nissim Francez. 2000. A natural logic inference system. In Proc. of the 2nd Workshop on Inference in Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing.</booktitle>
<contexts>
<context position="21176" citStr="Giampiccolo et al., 2007" startWordPosition="3453" endWordPosition="3456">ts such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models 636 was separately trained on the training set of each corpus. All models are evaluated only on 2-class entailment. To convert 3-class problems like SICK and SNLI to this setting, all instances of contradiction and unknown are converted to nonentailment. This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit distance algorithm tunes the weight of the three caseins</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="5228" citStr="Hochreiter and Schmidhuber 1997" startWordPosition="812" endWordPosition="815"> NNECN Table 1: Randomly chosen examples from the development section of our new corpus, shown with both the selected gold labels and the full set of labels (abbreviated) from the individual annotators, including (in the first position) the label used by the initial author of the pair. a variety of models for natural language inference, including rule-based systems, simple linear classifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art. 2 A new corpus for NLI To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small siz</context>
<context position="26990" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="4410" endWordPosition="4413">epicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the streng</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
</authors>
<title>Semantic Theory.</title>
<date>1972</date>
<publisher>Harper &amp; Row,</publisher>
<location>New York.</location>
<contexts>
<context position="1323" citStr="Katz, 1972" startWordPosition="189" endWordPosition="190">rpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for</context>
</contexts>
<marker>Katz, 1972</marker>
<rawString>Jerrold J. Katz. 1972. Semantic Theory. Harper &amp; Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="18739" citStr="Klein and Manning, 2003" startWordPosition="3050" endWordPosition="3053">tion are often omitted. The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/ Partition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated. Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of the descriptive sentences and noun phrases found in the descriptions. 3 Our data as a platform for evaluation The most immediate application for our corpus is in developing models for the task of NLI. In parSystem SNLI SICK RTE-3 Edit Distance Based 71.9 65.4 61.9 Classifier Based 72.2 71.4 61.5 + Lexical Resources 75.0 78.8 63.6 Table 4: 2-class test accuracy for two simple baseline systems included in the Excitement Open Platform, as well as SICK and</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Lai</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Illinois-LH: A denotational and distributional approach to semantics. In</title>
<date>2014</date>
<booktitle>Proc. SemEval.</booktitle>
<contexts>
<context position="35349" citStr="Lai and Hockenmaier, 2014" startWordPosition="5774" endWordPosition="5778">nd destroy the knowledge accumulated in the pre-transfer phase of training. The results are shown in Table 7. Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labeling more neutral examples as contradictions than correctly, possibly as a result of subtle differences in how the labeling task was presented. In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling. This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks. We attempted to apply this same transfer evaluation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training configuration yielded</context>
</contexts>
<marker>Lai, Hockenmaier, 2014</marker>
<rawString>Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A denotational and distributional approach to semantics. In Proc. SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector J Levesque</author>
</authors>
<title>On our best behaviour.</title>
<date>2013</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="31720" citStr="Levesque, 2013" startWordPosition="5182" endWordPosition="5183">ich at a deli) indicate that additional attention to compositional semantics would pay off. However, many of the persistent problems run deeper, to inferences that depend on world knowledge and contextspecific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lexicalized classifier and the LSTM predict neutral. In other cases, the models’ attempts to shortcut this kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner, 2004). Our anno</context>
</contexts>
<marker>Levesque, 2013</marker>
<rawString>Hector J. Levesque. 2013. On our best behaviour. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Focused entailment graphs for open IE propositions.</title>
<date>2014</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="6674" citStr="Levy et al. (2014)" startWordPosition="1035" endWordPosition="1038">s of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1http://aclweb.org/aclwiki/index.php? title=Textual—Entailment—Resource—Pool plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). </context>
</contexts>
<marker>Levy, Dagan, Goldberger, 2014</marker>
<rawString>Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open IE propositions. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>An extended model of natural logic.</title>
<date>2009</date>
<booktitle>In Proc. of the Eighth International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="1622" citStr="MacCartney and Manning, 2009" startWordPosition="234" endWordPosition="237">iers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they ca</context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>Bill MacCartney and Christopher D Manning. 2009. An extended model of natural logic. In Proc. of the Eighth International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Roberto Zanoli</author>
<author>Ido Dagan</author>
<author>Kathrin Eichler</author>
<author>G¨unter Neumann</author>
<author>Tae-Gil Noh</author>
<author>Sebastian Pado</author>
<author>Asher Stern</author>
<author>Omer Levy</author>
</authors>
<title>The Excitement Open Platform for textual inferences.</title>
<date>2014</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="20437" citStr="Magnini et al. 2014" startWordPosition="3331" endWordPosition="3334">plete comparison of approaches will have to wait for future work. In this section, we explore the performance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) variants of a strong but simple feature-based classifier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed representation models, including a baseline model and neural network sequence models. 3.1 Excitement Open Platform models The first class of models is from the Excitement Open Platform (EOP, Pad´o et al. 2014; Magnini et al. 2014)—an open source platform for RTE research. EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by run</context>
</contexts>
<marker>Magnini, Zanoli, Dagan, Eichler, Neumann, Noh, Pado, Stern, Levy, 2014</marker>
<rawString>Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin Eichler, G¨unter Neumann, Tae-Gil Noh, Sebastian Pado, Asher Stern, and Omer Levy. 2014. The Excitement Open Platform for textual inferences. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proc. SemEval.</booktitle>
<contexts>
<context position="2577" citStr="Marelli et al., 2014" startWordPosition="380" endWordPosition="383">word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality. To address this, this paper introd</context>
<context position="6210" citStr="Marelli et al. 2014" startWordPosition="972" endWordPosition="975">en the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1http://aclweb.org/aclwiki/index.php? title=Textual—Entailment—Resource—Pool plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the sec</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014a. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proc. SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A SICK cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="2577" citStr="Marelli et al., 2014" startWordPosition="380" endWordPosition="383">word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality. To address this, this paper introd</context>
<context position="6210" citStr="Marelli et al. 2014" startWordPosition="972" endWordPosition="975">en the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1http://aclweb.org/aclwiki/index.php? title=Textual—Entailment—Resource—Pool plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the sec</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014b. A SICK cure for the evaluation of compositional distributional semantic models. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="22010" citStr="Miller, 1995" startWordPosition="3590" endWordPosition="3591"> setting, all instances of contradiction and unknown are converted to nonentailment. This yields a most-frequent-class baseline accuracy of 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words. In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007). 3.2 Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding. Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized: 1. The BLEU score of the hypothesis with respect to the premise, using an n</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Tae-Gil Noh</author>
<author>Asher Stern</author>
<author>Rui Wang</author>
<author>Roberto Zanoli</author>
</authors>
<title>Design and realization of a modular architecture for textual entailment.</title>
<date>2014</date>
<journal>Journal of Natural Language Engineering.</journal>
<marker>Pad´o, Noh, Stern, Wang, Zanoli, 2014</marker>
<rawString>Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang, and Roberto Zanoli. 2014. Design and realization of a modular architecture for textual entailment. Journal of Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellie Pavlick</author>
<author>Johan Bos</author>
<author>Malvina Nissim</author>
<author>Charley Beller</author>
<author>Ben Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification.</title>
<date>2015</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Pavlick, Bos, Nissim, Beller, Van Durme, Callison-Burch, 2015</marker>
<rawString>Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller, Ben Van Durme, and Chris Callison-Burch. 2015. PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation. In</title>
<date>2014</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="27132" citStr="Pennington et al. 2014" startWordPosition="4433" endWordPosition="4436">e concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sentence embedding mode</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorien Y Pratt</author>
<author>Jack Mostow</author>
</authors>
<title>Candace A Kamm, and Ace A Kamm.</title>
<date>1991</date>
<booktitle>In Proc. AAAI.</booktitle>
<marker>Pratt, Mostow, 1991</marker>
<rawString>Lorien Y Pratt, Jack Mostow, Candace A Kamm, and Ace A Kamm. 1991. Direct transfer of learned information among neural networks. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank. In</title>
<date>2013</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="2187" citStr="Socher et al. 2013" startWordPosition="318" endWordPosition="321">005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic represen</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<publisher>JMLR.</publisher>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan van Benthem</author>
</authors>
<title>A brief history of natural logic.</title>
<date>2008</date>
<booktitle>Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal.</booktitle>
<editor>In M. Chakraborty, B. L¨owe, M. Nath Mitra, and S. Sarukki, editors,</editor>
<publisher>College Publications.</publisher>
<marker>van Benthem, 2008</marker>
<rawString>Johan van Benthem. 2008. A brief history of natural logic. In M. Chakraborty, B. L¨owe, M. Nath Mitra, and S. Sarukki, editors, Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida I Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="24266" citStr="Wang and Manning (2012)" startWordPosition="3958" endWordPosition="3961">, including models lacking crossbigram features (Feature 6), and lacking all lexical features (Features 4–6). We report results both on the test set and the training set to judge overfitting. for removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features. The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification. A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis. It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model. 3.3 Sentence embeddings and NLI SNLI is suitably large and diverse to make it possible to train neural network models that produce distributed representations of sentence meaning. In this section, we compare the performance of three su</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida I. Wang and Christopher D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>G¨unter Neumann</author>
</authors>
<title>Recognizing textual entailment using sentence similarity based on dependency tree skeletons.</title>
<date>2007</date>
<booktitle>In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="22158" citStr="Wang and Neumann, 2007" startWordPosition="3613" endWordPosition="3616">f 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit distance algorithm tunes the weight of the three caseinsensitive edit distance operations on the training set, after removing stop words. In addition to the base classifier-based system distributed with the platform, we train a variant which includes information from WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004), and makes use of features based on tree patterns and dependency tree skeletons (Wang and Neumann, 2007). 3.2 Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding. Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized: 1. The BLEU score of the hypothesis with respect to the premise, using an n-gram length between 1 and 4. 2. The length difference between the hypothesis and the premise, as a real-valued feature. 3. The overlap between word</context>
</contexts>
<marker>Wang, Neumann, 2007</marker>
<rawString>Rui Wang and G¨unter Neumann. 2007. Recognizing textual entailment using sentence similarity based on dependency tree skeletons. In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Ward</author>
<author>Betty Birner</author>
</authors>
<title>Information structure and non-canonical syntax.</title>
<date>2004</date>
<booktitle>Handbook of Pragmatics,</booktitle>
<pages>153--174</pages>
<editor>In Laurence R. Horn and Gregory Ward, editors,</editor>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="32310" citStr="Ward and Birner, 2004" startWordPosition="5270" endWordPosition="5273">(Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner, 2004). Our annotators generally followed suit, writing sentences that, while structurally diverse, share topic/focus (theme/rheme) structure with their premises. This promotes a coherent, situation-specific construal of each sentence pair. This is information that our models can easily take advantage of, but it can lead them astray. For instance, all of them stumble with the amusingly simple case A woman prepares ingredients for a bowl of soup/A soup bowl prepares a woman, in which prior expectations about parallelism are not met. Another headline example of this type is A man wearing padded arm pr</context>
</contexts>
<marker>Ward, Birner, 2004</marker>
<rawString>Gregory Ward and Betty Birner. 2004. Information structure and non-canonical syntax. In Laurence R. Horn and Gregory Ward, editors, Handbook of Pragmatics, pages 153–174. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Tomas Mikolov</author>
</authors>
<title>Towards AI-complete question answering: A set of prerequisite toy tasks.</title>
<date>2015</date>
<pages>1502--05698</pages>
<contexts>
<context position="2357" citStr="Weston et al., 2015" startWordPosition="349" endWordPosition="352">en addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models</context>
</contexts>
<marker>Weston, Bordes, Chopra, Mikolov, 2015</marker>
<rawString>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015a. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv:1502.05698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Sumit Chopra</author>
<author>Antoine Bordes</author>
</authors>
<title>Memory networks.</title>
<date>2015</date>
<booktitle>In Proc. ICLR.</booktitle>
<contexts>
<context position="2357" citStr="Weston et al., 2015" startWordPosition="349" endWordPosition="352">en addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; Weston et al., 2015a). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a). Our ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models</context>
</contexts>
<marker>Weston, Chopra, Bordes, 2015</marker>
<rawString>Jason Weston, Sumit Chopra, and Antoine Bordes. 2015b. Memory networks. In Proc. ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="31703" citStr="Winograd, 1972" startWordPosition="5180" endWordPosition="5181">n buying a sandwich at a deli) indicate that additional attention to compositional semantics would pay off. However, many of the persistent problems run deeper, to inferences that depend on world knowledge and contextspecific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lexicalized classifier and the LSTM predict neutral. In other cases, the models’ attempts to shortcut this kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive Psychology, 3(1):1–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="6274" citStr="Young et al., 2014" startWordPosition="982" endWordPosition="985">ese are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1http://aclweb.org/aclwiki/index.php? title=Textual—Entailment—Resource—Pool plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) in</context>
<context position="11971" citStr="Young et al., 2014" startWordPosition="1916" endWordPosition="1919">finitely a false description of the photo. Example: For the caption “Two dogs are running through a field.” you could write “The pets are sitting on a couch.” This is different from the maybe correct category because it’s impossible for the dogs to be both running and sitting. Figure 1: The instructions used on Mechanical Turk for data collection. complexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sentences), and reviewed logistical issues around payment timing. About 2,500 workers contributed. For the premises, we used captions from the Flickr30k corpus (Young et al., 2014), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowdsourced effort.3 The captions were not authored by the photographers who took the source images, and they tend to contain relatively literal scene descriptions that are suited to our approach, rather than those typically associated with personal photographs (as in their example: Our trip to the Olympic Peninsula). In order to ensure that the label for each sentence pair can be recovered solely based on the available text, we did not use the images at all during corpus collection. Tabl</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</title>
<date>2012</date>
<contexts>
<context position="27447" citStr="Zeiler, 2012" startWordPosition="4485" endWordPosition="4486">words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sentence embedding models (though not to its internal connections) with a fixed dropout rate. All models were implemented in a common framework for this paper, and the implementations will be made available at publication time. The results are shown in Table 6. The sum of words model performed slightly worse than the fundamentally simil</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
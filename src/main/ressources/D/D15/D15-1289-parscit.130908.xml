<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000135">
<title confidence="0.9968635">
ERSOM: A Structural Ontology Matching Approach Using Automatically
Learned Entity Representation
</title>
<author confidence="0.99881">
Chuncheng Xiang, Tingsong Jiang, Baobao Chang, Zhifang Sui
</author>
<affiliation confidence="0.937743333333333">
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China
</affiliation>
<email confidence="0.996482">
{ccxiang,tingsong,chbb,szf}@pku.edu.cn
</email>
<sectionHeader confidence="0.997355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971848484849">
As a key representation model of knowl-
edge, ontology has been widely used in
a lot of NLP related tasks, such as se-
mantic parsing, information extraction and
text mining etc. In this paper, we study
the task of ontology matching, which con-
centrates on finding semantically related
entities between different ontologies that
describe the same domain, to solve the
semantic heterogeneity problem. Previ-
ous works exploit different kinds of de-
scriptions of an entity in ontology di-
rectly and separately to find the corre-
spondences without considering the high-
er level correlations between the descrip-
tions. Besides, the structural informa-
tion of ontology haven’t been utilized ad-
equately for ontology matching. We pro-
pose in this paper an ontology matching
approach, named ERSOM, which main-
ly includes an unsupervised representation
learning method based on the deep neural
networks to learn the general representa-
tion of the entities and an iterative sim-
ilarity propagation method that takes ad-
vantage of more abundant structure infor-
mation of the ontology to discover more
mappings. The experimental results on the
datasets from Ontology Alignment Eval-
uation Initiative (OAEI1) show that ER-
SOM achieves a competitive performance
compared to the state-of-the-art ontology
matching systems.
</bodyText>
<footnote confidence="0.769663166666667">
1The OAEI is an international initiative organizing annual
campaigns for evaluating ontology matching systems. All of
the ontologies provided by OAEI are described in OWL-DL
language, and like most of the other participates our ERSOM
also manages the OWL ontology in its current version. OAEI:
http://oaei.ontologymatching.org/
</footnote>
<sectionHeader confidence="0.996086" genericHeader="keywords">
1 Introductions
</sectionHeader>
<bodyText confidence="0.999944307692308">
In the recent years, it becomes evident that one
of the most important directions of improvement
in natural language processing (NLP) tasks, like
word sense disambiguation, coreference resolu-
tion, relation extraction, and other tasks related
to knowledge extraction, is by exploiting seman-
tics resources (Bryl et al., 2010). Nowadays, the
Semantic Web made available a large amount of
logically encoded information (e.g. ontologies,
RDF(S)-data, linked data, etc.), which constitutes
a valuable source of semantics. However, extend-
ing the state-of-the-art natural language applica-
tions to use these resources is not a trivial task
mainly due to the heterogeneity and the ambiguity
of the schemes adopted by the different resources
of the Semantic Web. How to utilize these re-
sources in NLP tasks comprehensively rather than
choose just one of them has attracted much atten-
tion in recent years.
An effective solution to the ontology hetero-
geneity problem is ontology matching (Euzenat
et al., 2007; Shvaiko and Euzenat, 2013), whose
main task is to establish semantic correspondences
between entities (i.e., classes, properties or in-
stances) from different ontologies.
Ontology matching is usually done by measur-
ing the similarity between two entities from two
different ontologies. To effectively calculate the
similarities, almost all types of descriptions of an
entity should be used. In previous works, given the
different nature of different kinds of descriptions,
similarities are normally measured separately with
different methods and then aggregated with some
kind of combination strategy to compute the final
similarity score. For example, Mao et al. (2010)
defined three single similarities (i.e., Name simi-
larity, Profile similarity and Structural similarity)
based on the descriptions of an entity, then they
employed a harmony-based method to aggregate
</bodyText>
<page confidence="0.950917">
2419
</page>
<note confidence="0.984691">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2419–2429,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999938075">
the single similarities to get a final similarity for
extracting the final mappings. However, treating
different kinds of descriptions of an entity sepa-
rately suffers from two limitations. First, it lim-
its the capacity of modeling the interactions be-
tween different descriptions. For example, entity’s
label is always a specific substitution of its ID; en-
tity’s comment is a semantic definition for its ID;
a class can be characterized with its related prop-
erties, and a property is usually restricted by its
domain and range. These potential correlations of
the descriptions are very important to measure the
similarity between entities since they can be treat-
ed as some potential features describing an entity.
Second, it is difficult to estimate how many and
which kind of single similarities are needed for an
aggregation method to get a satisfactory result.
On the other hand, in order to find more map-
pings, many structural ontology matching meth-
ods are proposed. To the best of our knowledge,
previous structural methods are either local meth-
ods (Le et al., 2004; Sunna and Cruz, 2007) or
global (i.e. iterative) methods but only use part
of the structure information of the ontology (Li
et al., 2009; Ngo and Bellahsene, 2012). For ex-
ample, the ontology matching system YAM++ (N-
go and Bellahsene, 2012) utilizes a global struc-
tural method but it only uses the structure informa-
tion of classes and properties to create the propa-
gation graph to find mappings between classes and
properties. A large amount of instances and their
relations to classes and properties in the ontology
haven’t been exploited in this system.
To overcome the existing limitations, we pro-
pose in this paper a representation learning method
to capture the interactions among entity’s descrip-
tions; then we present our global structural method
which exploits more abundant structure informa-
tion of the ontology. We summarize our contribu-
tions as follows.
</bodyText>
<listItem confidence="0.994284545454545">
• We propose to use the deep neural network
model to learn the high-level abstract repre-
sentations of classes and properties from their
descriptions to acquire the potential corre-
lations for the computing of the similarities
between classes and properties. Moreover,
there is no need to select and aggregate differ-
ent single similarities in the similarity com-
putation.
• We propose a global similarity propagation
method that utilizes more abundant structure
</listItem>
<bodyText confidence="0.997792">
information including all kinds of entities and
their relations in the ontology, to find more
mappings.
To evaluate the effectiveness of our approach,
we conduct experiments on the public datasets
from OAEI campaign (We select the OAEI data
sets mainly because evaluation metrics have been
well defined on these data sets and comparision
can be easily made). The experimental result-
s show that our matching approach can achieve
a competitive matching performance compared to
the state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.923749" genericHeader="introduction">
2 Problem Statement
</sectionHeader>
<bodyText confidence="0.9996225">
Ontology is a formal, explicit specification of a
shared conceptualization in terms of classes, prop-
erties and relations (Euzenat et al., 2004). The pro-
cess of ontology matching is to find mappings (or
correspondences) between entities (classes, prop-
erties or individuals) from two ontologies. A map-
ping is defined as a four-tuple as written in Eq.(1),
where e1 and e2 represent the entity in ontology
O1 and O2 respectively, r is a kind of matching
relation (e.g., equivalent, subsume) and k → [0, 1]
is the degree of confidence of matching relation
between e1 and e2 (Mao et al., 2010).
</bodyText>
<equation confidence="0.905931">
m =&lt; e1, e2, r, k &gt; (1)
</equation>
<bodyText confidence="0.999660857142857">
Similar with most of the OAEI systems (Li et al.,
2009; Ngo and Bellahsene, 2012; Cheatham and
Hitzler, 2013b), we focus on discovering only e-
quivalent mappings between classes and proper-
ties with cardinality 1:1. That is, one class (prop-
erty) in ontology O1 can be matched to at most one
class (property) in ontology O2 and vise versa.
</bodyText>
<sectionHeader confidence="0.989186" genericHeader="method">
3 ERSOM: Entity Representation and
</sectionHeader>
<subsectionHeader confidence="0.740383">
Structure based Ontology Matching
</subsectionHeader>
<bodyText confidence="0.999889166666667">
In this paper, we propose a structural ontology
matching approach using automatically learned
entity representation, which we call ERSOM.
Fig.1 shows the architecture of our approach. The
details of its major modules are given in the fol-
lowing sections.
</bodyText>
<subsectionHeader confidence="0.999845">
3.1 Learning the representation of entity
</subsectionHeader>
<bodyText confidence="0.999969">
In this section, we present how to learn the high-
level abstract representations for ontology entities.
The motivations are: 1) we regard different kinds
of the descriptions of an entity as a whole to avoid
</bodyText>
<page confidence="0.99403">
2420
</page>
<figureCaption confidence="0.996135">
Figure 1: The architecture of ERSOM. Given the
</figureCaption>
<bodyText confidence="0.980017769230769">
two to-be-matched ontologies, we first extract the
descriptive information for each entity, then learn
the entity’s abstract representation based on its de-
scriptions, and finally utilize the representations to
compute the similarities between entities to initial-
ize the similarity propagation method to find final
mappings.
separatively calculating the similarities and aggre-
gating them later with a combination method; 2)
the learned representation can not only express the
meaning of the original descriptions of an entity
but also captures the interactions among different
descriptions.
</bodyText>
<subsectionHeader confidence="0.952246">
3.1.1 Creating term vector for entity
</subsectionHeader>
<bodyText confidence="0.9999405625">
We first generate a combination of the entity’s de-
scriptions (CDs for short) and then create a term
vector for each entity. In particular, the CDs of
a class = the class’s ID + label + comments + it-
s properties’ descriptions + its instances’ descrip-
tions. The CDs of a property = the property’s ID
+ label + its domain + its range (or its textual val-
ue when the property is a datatype property). And
the CDs of an instance = the instance’s ID + la-
bel + its properties’ values. A binary term vector
is created for each entity with the pre-processing
that consists of tokenizing, removing stop words,
stemming and deleting superfluous words. In the
binary term vectors, element 1 and 0 refer to the
existence and inexistence of a specific word, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.955918">
3.1.2 Learning entity representations
</subsectionHeader>
<bodyText confidence="0.999975333333333">
In the ontology matching area, training data usual-
ly refers to a pair of ontologies with correct map-
pings created by domain experts between their en-
tities. The acquisition of such dataset is time-
consuming and laborious. We state in this sec-
tion how to learn the abstract representations for
entities from their binary term vectors with an un-
supervised way. The deep neural network (DNN)
(Hinton et al., 2006; Bengio et al., 2007) is a multi-
layer learning model. It is mainly used for learning
the high-level abstract representations of original
input data. Given the generalization and the ab-
straction introduced in the representation learning
procedure, DNN allows us to better model the in-
teractions among different kinds of input features,
and measure the similarity at a more general lev-
el. Inspired by the work in (Hinton, 2007; Bengio
et al., 2012; He et al., 2013; Cui et al., 2014), we
use auto-encoder (Bourlard and Kamp, 1988; Hin-
ton and Zemel, 1994) to learn the representations
for classes and properties. The auto-encoder is one
of the neural network variants that can automati-
cally discover interesting abstractions to represent
an unlabeled dataset.
</bodyText>
<figureCaption confidence="0.996485">
Figure 2: Unsupervised representation learning.
</figureCaption>
<bodyText confidence="0.999636615384615">
As shown in Fig.2, the input to the auto-encoder
is denoted as x, which indicates a binary term vec-
tor of a class or a property. Auto-encoder tries
to learn an approximation to the identity function
h(x), so as to output x that is similar to x. More
specifically, the auto-encoder consists of an encod-
ing process h(x) = f(Wx + b1) and a decod-
ing process g(h(x)) = f(W&apos;h(x) + b2), where
f is a activation function like the sigmoid func-
tion f(x) = 1/(1 + exp(−x)), W is the weight
matrix and b is the bias term. The goal of the
auto-encoder is to minimize the reconstruction er-
ror L(x, g(h(x))) = |x−g(h(x))|2, thus retaining
</bodyText>
<page confidence="0.921845">
2421
</page>
<bodyText confidence="0.998861571428572">
maximum information. Through the combination
and transformation, auto-encoder learns the ab-
stract representation h(x) of the input binary term
vector. The representation is a real vector with val-
ues between 0 and 1.
In consideration of the large number of units in
the hidden layer (as marked in Fig.2), a sparsity
constraint is imposed on the hidden units to hold
the capacity to discover interesting structure in the
data. We use sparse auto-encoder (Coates et al.,
2011) to learn the correlations between descrip-
tions from their binary term vectors. The sparse
auto-encoder attempts to minimize the following
loss function:
</bodyText>
<equation confidence="0.9910292">
� � �
x(i) − bx(i)� �
λ (kW1kF + kW2kF) + β X KL (ρkbρh)
h∈H
(2)
</equation>
<bodyText confidence="0.999630166666667">
where x(i) is the binary term vector of the ith en-
tity (a class or a property), bx(i) is the reconstruc-
tion of x(i), λ is a regularization parameter, orig-
inal k · kF is the Frobenius norm, β controls the
weight of the sparsity penalty term, H is the set of
hidden units, and ρ is the sparsity parameter. For-
</bodyText>
<equation confidence="0.702891">
mally, KL (ρkbρh) = ρ log �yh ρ + (1− ρ) log 1−ρ
1−�yh
</equation>
<bodyText confidence="0.863766666666667">
is the Kullback-Leibler (KL) divergence between
a Bernoulli random variable with mean ρ and a
Bernoulli random variable with mean byh.
</bodyText>
<figureCaption confidence="0.971606">
Figure 3: Learning higher level representations.
</figureCaption>
<subsectionHeader confidence="0.980516">
3.1.3 Learning higher level representations
</subsectionHeader>
<bodyText confidence="0.999944083333333">
The auto-encoder which only has one hidden layer
may not be enough to learn the complex interac-
tions between input features. Inspired by the work
of Vincent et al. (2010) and He et al. (2013), we
build multi-layer model to learn more abstract en-
tity representations. To achieve this, we repeatedly
stack new sparse auto-encoder on top of the previ-
ously learned h(x) (i.e., the higher level represen-
tations are formed by combination of lower lev-
el representations). This model is called Stacked
Auto-Encoder (SAE) by Bengio et al. (2007). In
this way, when we input the binary term vector
to the network, we can get its abstract represen-
tations in different levels. In other words, with the
layer-by-layer learning, we obtain different level-
s of representations. The top-level representation,
which models the final interactions of the original
descriptions, can be used to measure the similarity
between classes and properties.
The prototype of Stacked Auto-Encoder (SAE)
is shown in Fig.3, where f(h(x))(m) denotes the
final representation learned by the top-level hidden
layer and superscript m means the SAE consists of
m sparse auto-encoders.
</bodyText>
<subsectionHeader confidence="0.999793">
3.2 Optimizing with the ontology structure
</subsectionHeader>
<bodyText confidence="0.999993870967742">
The above method can only consider the local de-
scriptions (such as ID, label and comments etc.)
of entities in ontology. According to the study
in (Melnik et al., 2002), we present our struc-
tural method or called Similarity Propagation (SP)
method, which exploits more abundant structure
information of the ontology to discover more map-
pings globally. The intuition behind the propaga-
tion idea is that the similarity between two entities
is not only about their neighbor entities, but it is
about all other entities (neighbor entities’ neigh-
bor entities) in the ontologies. This idea has also
been used in the ontology matching systems Ri-
MOM(Li et al., 2009) and YAM++(Ngo and Bel-
lahsene, 2012) in order to find mappings between
classes and properties. But the nodes in their
propagation graph are just limited to class pairs
and property pairs, and the propagation edges are
transformed from relations between two classes,
two properties or a class and a property. The dif-
ference of our SP method is that we consider the
instances and its relations with classes and prop-
erties when creating the propagation graph even if
we also only find mappings between classes and
properties. This is because (1) the similar degree
of two classes will be increased if they have some
of similar instances; (2) the similar degree of two
properties will be increased if the instances that
own these properties are similar. The propagation
graph in our SP method will be much more com-
plete compared with the previous ones.
</bodyText>
<equation confidence="0.98621125">
J(W, b) = Xm
i=1
2
+
</equation>
<page confidence="0.924887">
2422
</page>
<bodyText confidence="0.998942538461538">
Algorithm 1 presents the procedures of our SP
method. In the first two steps of it, we repre-
sent each to-be-matched ontology to a Directed
Labeled Graph (DLG). Each edge in the DLG
has format (s, p, o), where s and o are the source
and target nodes (each node represents a class, a
property or an instance), and the edge’s label p
comes from one of the seven ontology relation-
s including HasSubClass, HasSubProperty, Ha-
sObjectProperty, HasDataProperty, HasInstance,
HasDomain, HasRange. Then we create a Pair-
wise Connectivity Graph (PCG) from two DLGs
by merging edges having the same labels.
</bodyText>
<construct confidence="0.386679">
Algorithm 1: Our SP Algorithm
</construct>
<bodyText confidence="0.97323225">
Input: The to-be-matched ontologies, OR
and OT; The initial similarity matrix,
M0; The edges’ weight matrix, W;
Output: The updated similarity matrix, M1;
</bodyText>
<equation confidence="0.9894756">
1 DLG1 Transform(OR);
2 DLG2 Transform(OT);
3 PCG Merge(DLG1, DLG2);
4 IPG Initiate(PCG, M0, W);
5 M1 Propagation(IPG, Normalized);
</equation>
<bodyText confidence="0.999974625">
In the fourth step of Algorithm 1, for a PCG,
we assign weight values to edges as the inverse
of the number of out-linking relationships of it-
s source node (Melnik et al., 2002). For the n-
odes that consist of two classes or two properties,
we assign them values calculated with the cosine
similarity between their representations learned in
section 2.1.3. For the node consisting of two in-
stances, the similarity value assigned to it is mea-
sured with the ScaledLevenstein2 between the IDs
of instances. In this way, we construct an Induced
Propagation Graph (IPG) on which the propaga-
tion algorithm will run iteratively. Let σ(x, y) de-
notes the similarity score between entities x and y
for node (x, y) in the IPG. At the (i + 1)th itera-
tion, the similarity score is updated as follows:
</bodyText>
<equation confidence="0.96700725">
σi+1 = 1 (σ0 + σi + cp(σ0 + σi)), (3)
z
(σ0 + σi� wj (4)
j
</equation>
<footnote confidence="0.78664525">
2http://sourceforge.net/projects/
secondstring/. ScaledLevenstein is a good method for
computing string similarity (Cheatham and Hitzler, 2013a),
of course, it can be replaced by other methods.
</footnote>
<bodyText confidence="0.93447">
where z is the normalization factor defined as
</bodyText>
<equation confidence="0.868564">
�σi+1�. σ0 and σi are similarities at
z = max
x&apos;∈IPG
</equation>
<bodyText confidence="0.999721166666666">
the initial time and ith iterations, respectively. cp()
is the function to compute the similarities propa-
gated from the adjacent node σij connected to n-
ode (x, y) in the (i + 1)th iteration. And wj is the
weight of edge between the node (x, y) and its jth
neighboring node.
During each iteration in the final step of Algo-
rithm 1, only the similarity value between two en-
tities in the node will be updated. At the end of
each iteration, all similarity values are normalized
by a Normalized function to all in range [0, 1]. The
iteration stops after a predefined number of steps.
</bodyText>
<subsectionHeader confidence="0.999581">
3.3 Mapping selection
</subsectionHeader>
<bodyText confidence="0.999908416666667">
Similar with the work in (Wang and Xu, 2007;
Huang et al., 2007; Ngo and Bellahsene, 2012), we
use the Stable Marriage (SM) algorithm (Melnik
et al., 2002) to choose the 1:1 mappings from the
M rows and N columns similarity matrix, where
M and N is the number of classes and properties
in ontologies O1 and O2, respectively. In addition,
before we run the SM algorithm we set the value
of cell [i, j] of the similarity matrix to zero if i and
j correspond to different types of entities. Thus,
we remove lots of redundant data and only find
the mappings between classes or properties.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999932">
4.1 Data sets and evaluation criteria
</subsectionHeader>
<bodyText confidence="0.999970368421052">
The annual OAEI campaign is an authoritative
contest in the area of ontology matching, we
choose the data from OAEI in our experiments,
because the evaluation metrics have been well de-
fined and the comparision can be easily made.
We observe strong structure similarities lies be-
tween OAEI ontologies and ontologies used in
NLP tasks, such as WordNet and HowNet for WS-
D (Li et al., 1995; Agirre et al., 2009), and Free-
base, YAGO, and knowledge graph for IE, text
mining and QA (Yao and Van Durme, 2014; Yao
et al., 2014), both describe entities and their rela-
tions with class, properties and instances.
Development dataset: the Standard Bench-
mark 2012 dataset that OAEI provides for devel-
opers to test their system before participating in
the competition is used as the development dataset
in our experiments. This dataset contains one ref-
erence ontology and 109 target ontologies. We use
</bodyText>
<equation confidence="0.976389">
cp (σ0 + σi) = Xm
j=1
</equation>
<page confidence="0.67793">
2423
</page>
<bodyText confidence="0.999907235294118">
this dataset to test various values for the parame-
ters in our ERSOM and apply the best ones to the
experiments on the testing datasets.
Testing dataset: (1) the Benchmark-Biblio
2012 dataset which contains one reference ontol-
ogy and 94 target ontologies; (2) the Benchmark-
Biblioc 2013 dataset which has five sub-datasets
and there are one reference ontology and 93 target
ontologies in each sub-dataset. We use these two
datasets to evaluate the performance of our ER-
SOM approach.
In the matching scenario, each target ontolo-
gy should be mapped to the reference ontology.
We followed the standard evaluation criteria from
the OAEI, calculating the precision, recall and f-
measure over each test. The version computed
here is the harmonic mean of precision and recall.
</bodyText>
<subsectionHeader confidence="0.962471">
4.2 Experimental design and results
</subsectionHeader>
<subsubsectionHeader confidence="0.904444">
4.2.1 Evaluation for representation learning
</subsubsectionHeader>
<bodyText confidence="0.999847333333333">
We first use Jena3 parsing the ontologies and ex-
tract descriptions for entities according to the de-
scription in section 2.1.1, then we create a vocabu-
lary based on the dataset and denote each class and
property as a binary term vector. We apply the L-
BFGS algorithm (Ngiam et al., 2011) to train the
stacked auto-encoder described in section 2.1.3.
The size of the input layer is equals to the length
of the vocabulary created from the dataset. We fix
the parameters A = 1e − 4, Q = 3 and p = 0.25 in
Eq.2, and set the size of the first and second hidden
layer of the stacked auto-encoder to 200 and 100,
respectively, by experience. The number of itera-
tions of the L-BFGS algorithm is set to 500. We
use the learned representations to measure the sim-
ilarities between classes and properties and apply
the strategy presented in section 2.3 to extract final
mappings. The matching results of our Unsuper-
vised Representation Learning (URL) method on
the development dataset and testing datasets are
shown in Table 1 and Table 2, respectively.
</bodyText>
<table confidence="0.9916605">
Method Prec. Rec. F-m.
TV 0.841 0.715 0.748
URL(1) 0.819 0.822 0.820
URL(2) 0.843 0.846 0.844
</table>
<tableCaption confidence="0.95565">
Table 1: Representation learning on dev. dataset.
In Table 1, TV denotes the matcher in which
</tableCaption>
<footnote confidence="0.814278">
3https://jena.apache.org/
</footnote>
<bodyText confidence="0.99951852631579">
the similarities are calculated between binary ter-
m vectors of classes and properties by using co-
sine measure. URL(i), where i E {1, 2}, repre-
sents that we use the representations learned by
the ith hidden layer of the stacked auto-encoder to
measure the similarities between classes and prop-
erties to find mappings. Table 1 shows that on
the development dataset, the F-measure of TV is
0.748 and it is improved 9.6% and 12.8% when
we use the representations learned by the single-
layer and double-layer auto-encoder to find the
mappings, respectively. It illustrates that we have
learned some useful information from the term
vectors, which can be explained as the interactions
between descriptions of entities. From the last two
rows in Table 1, we can find that the F-measure im-
proved by 2.9% when we use the representations
learned by the second hidden layer (i.e., URL(2))
to measure the similarities.
</bodyText>
<table confidence="0.9994546">
Benchmark-Biblio 2012 Benchmark-Biblioc 2013
Psec. Rec. F-m. Psec. Rec. F-m.
TV 0.870 0.719 0.761 0.865 0.715 0.757
URL(1) 0.805 0.808 0.806 0.780 0.783 0.786
URL(2) 0.814 0.817 0.815 0.787 0.790 0.793
</table>
<tableCaption confidence="0.999434">
Table 2: Representation learning on test datasets.
</tableCaption>
<bodyText confidence="0.999938941176471">
From Table 2 we can see that the F-measures are
increased on both of the testing datasets when we
use the learned representations to measure simi-
larities between classes and properties compared
with using term vectors, but the amount of im-
provements are less than that on the development
dataset. This is because we estimate the parame-
ters of the representation learning model on the de-
velopment dataset and then apply them on the test
tasks directly. The precision is reduced when we
use URL method, this may be due to the learned
representations of entities are too general. In ad-
dition, in the parameter adjustment process, we
try to make the F value maximization, but not to
care about mapping precision. This is because we
usually compare the performance of the systems
based on their matching F values.
</bodyText>
<subsubsectionHeader confidence="0.551631">
4.2.2 Comparison with aggregation methods
</subsubsectionHeader>
<bodyText confidence="0.999804428571428">
Aggregating different similarities is pervasive in
ontology matching systems that contain mul-
tiple single matchers, for example, Falcon-
AO(Qu et al., 2006), RiMOM(Li et al., 2009),
YAM++(Ngo and Bellahsene, 2012), etc. Since
our representation learning method also combines
all descriptions of an entity together in an unsu-
</bodyText>
<page confidence="0.982264">
2424
</page>
<bodyText confidence="0.999398066666667">
pervised way, we compare it with previous unsu-
pervised aggregation strategies, that is, Max, Av-
erage, Sigmoid, Weighted(Cruz et al., 2010) and
Harmony(Mao et al., 2008, 2010). As the work
in (Mao et al., 2010; Ngo and Bellahsene, 2012),
we first define three context profiles including in-
dividual profile, semantic profile and external pro-
file for each class and property (this equivalent to
divide the collection of descriptions of a class or
a property into three different parts). Then we ap-
ply a vector space model with TFIDF weighting
scheme and cosine similarity measure to compute
similarity scores between profiles. And finally, we
aggregate these three single similarities using dif-
ferent aggregation methods.
</bodyText>
<table confidence="0.9995161">
Dev. Tes.1 Tes.2
Individual Profile 0.668 0.612 0.611
Semantic Profile 0.434 0.472 0.477
External Profile 0.222 0.224 0.224
MAX 0.739 0.705 0.712
Average 0.792 0.786 0.786
Sigmoid 0.763 0.753 0.757
Weighted 0.755 0.716 0.728
Harmony 0.794 0.789 0.785
URL 0.844 0.815 0.793
</table>
<tableCaption confidence="0.999992">
Table 3: Comparison with aggregation methods.
</tableCaption>
<bodyText confidence="0.9979979">
Table 3 shows the F-measure of the single
matchers and aggregation methods on the devel-
opment dataset (Dev. for short) and two test-
ing datasets (i.e., Tes.1 and Tes.2, which refer
to the Benchmark-Biblio 2012 dataset and the
Benchmark-Biblioc 2013 dataset, respectively).
First, the performance of single matcher is poor,
the highest F-measures are 0.668, 0.612 and 0.611
on the datasets Dev., Tes.1 and Tes.2, respective-
ly. And when we use external profile to calcu-
late the similarities, the F-measures are reduced
to 22%. Second, the performance is dramatical-
ly boosted by aggregation methods and they al-
l achieve F-measures higher than 0.7, so the ag-
gregation methods are very effective in improving
the performance of mapping approaches that re-
ly on measuring multiple similarities. And finally,
our Unsupervised Representation Learning (URL)
method holds the highest F-measure both on the
development dataset and on the testing datasets.
</bodyText>
<subsubsectionHeader confidence="0.642851">
4.2.3 Evaluation for our structural method
</subsubsectionHeader>
<bodyText confidence="0.994284466666667">
In this experiment, we compare our Similarity
Propagation (SP) method to other structure based
methods, that is, ADJACENTS and ASCOPATH
in (Le et al., 2004); DSI and SSC in (Sunna and
Cruz, 2007); Li’s SP (Li et al., 2009) and Ngo’s
SP (Ngo and Bellahsene, 2012). We first use enti-
ty’s ID to compute the similarity between classes
and properties to provide an unified initial simi-
larity matrix as input (or initialization) for our SP
and other structural methods. Then, a new similar-
ity matrix will be created and updated by consid-
ering the initial similarities and different structure
information. And finally, we extract the mappings
from the newly created similarity matrix with the
strategy described in section 2.3.
</bodyText>
<table confidence="0.9996425">
Dev. Tes.1 Tes.2
Initial Matcher 0.616 0.524 0.523
ADJACENTS 0.622 0.569 0.570
ASCOPATH 0.604 0.540 0.552
DSI 0.641 0.576 0.575
SSC 0.642 0.569 0.568
Li’s SP 0.747 0.769 0.772
Ngo’s SP 0.751 0.768 0.764
Our SP 0.810 0.834 0.839
Our SP with URL 0.903 0.865 0.866
</table>
<tableCaption confidence="0.999979">
Table 4: Comparison with structural methods.
</tableCaption>
<bodyText confidence="0.999927863636364">
In ADJACENTS method, the parameter Wk,
where k ∈ {1, 2, 3}, is set to 1/3. The parameter
MCP in the methods DSI and SSC is set to 0.75 as
reported in their work. The iterative times to SP al-
gorithm are fixed to 50. Table 4 reports the match-
ing F-measures of these structure based methods
on the development dataset (Dev. for short) and
testing datasets (Tes.1, Tes.2 for short).
From table 4 we can see that the local-structure
based methods (i.e., ADJACENTS, ASCOPATH,
DSI and SSC) provide low matching quality. It
means that these methods did not discover enough
additional correct mappings or even find some in-
correct mappings. For example, the F-measure
even reduced on the development dataset when use
ASCOPATH method. This is because if two enti-
ties don’t have any common entity in their ances-
tors, their similarity is equal to 0. Whereas, Li’s
SP and Ngo’s SP are global-structure based meth-
ods, and they seem to work well. The F-measure
has even improved by 21.9% when using the N-
go’s SP compared with the initial matcher. This
</bodyText>
<page confidence="0.965721">
2425
</page>
<bodyText confidence="0.999747941176471">
is because in the SP method, the similarity score
of a pair of entities depends on not only their cur-
rent status but also on the status of the other pairs.
That explains why SP outperforms all other local
based structural methods. In our SP, all instances
and their relations to other entities in ontology are
exploited to help find the mappings between class-
es and properties, therefore the matching quality is
distinctly improved.
The last two rows of Table 4 shows that
when we use the learned representations to cre-
ate the initial similarity matrix to initialize our SP
method, the matching quality is significantly im-
proved. For example, the F-measure is improved
from 0.810 to 0.903 on the development dataset.
This illustrates that the initialization step is very
important to the SP method.
</bodyText>
<subsectionHeader confidence="0.86011">
4.2.4 Comparison with other ontology
matching systems
</subsectionHeader>
<bodyText confidence="0.999995269230769">
We compare our ontology matching approach,
called ERSOM, with other multi-strategy match-
ing systems on the testing datasets. Fig.4 lists the
results of top five matching systems according to
their F-measures on the Benchmark-Biblio 2012
dataset and Benchmark-Biblioc 2013 dataset.
As shown in Fig.4, ERSOM outperforms most
of the participates except the systems YAM++ and
CroMatcher whose F-measures are 0.89 and 0.88
in 2013, respectively. CroMatcher achieves the
same level of recall as YAM++ but with consis-
tently lower precision (Grau et al., 2014). Un-
like MapSSS, our approach does not use any exter-
nal resources such as Google queries in its current
version. In YAM++ approach, the gold standard
datasets that taken from Benchmark dataset pub-
lished in OAEI 2009 are used to generate training
data to train a decision tree classifier. And in the
classifying phase, each pair of elements from two
to-be-matched ontologies is predicted as matched
or not according to its attributes. However, ER-
SOM is an unsupervised approach, but it does not
exclude using external resources and training da-
ta to help learning the representations of entities
and provide the initial similarity matrix for the SP
method to further improve the performance.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.92649675">
There are many studies on Ontology Matching
(Euzenat et al., 2007; Shvaiko and Euzenat, 2013).
Currently, almost all ontology matching system-
s exploit various kinds of information provided in
</bodyText>
<figureCaption confidence="0.990465">
Figure 4: Comparison with other OAEI systems.
</figureCaption>
<bodyText confidence="0.999939783783784">
ontologies to get better performance. To aggre-
gate the different similarity matrixes, various ap-
proaches have been proposed.
Pirr´o and Talia (2010) is a generic schema
matching system. It exploits Max, Min, Average
and Weighted strategies for the combination. The
weighted method assigns a relative weight to each
similarity matrix, and calculates a weighted sum
of similarity for all similarity matrixes. The Aver-
age method is a special case of Weighted, which
considers each similarity matrix equally important
in the combination. Max and Min are two extreme
cases that return the highest and lowest similari-
ties in all similarity matrixes respectively. Ji et al.
(2011) use the Ordered Weighted Average (OWA)
to combine different matchers. It is a kind of
ontology-independent combination method which
can assign weights to the entity level, i.e., it use a
specific ordered position rather than a weight as-
sociated with a specific similarity matrix to aggre-
gate multiple matchers. Jean-Mary et al. (2009)
combines different matchers by using a weighted
sum strategy that adjusts weights empirically, or
based on some static rules. This approach cannot
automatically combine different matchers in vari-
ous matching tasks.
There are several works which exploit the su-
pervised machine learning techniques for ontolo-
gy matching. Eckert et al. (2009), string-based,
linguistic and structural measures (in total 23 fea-
tures) were used as input to train a SVM clas-
sifier to align ontologies. CSR (Classification-
based learning of Subsumption Relations) is a
generic method for automatic ontology matching
between concepts based on supervised machine
learning (Spiliopoulos et al., 2010). It specifically
focusses on discovering subsumption correspon-
</bodyText>
<page confidence="0.956265">
2426
</page>
<bodyText confidence="0.999784307692308">
dences. SMB (Schema Matcher Boosting) is an
approach to combining matchers into ensembles
(Gal, 2011). It is based on a machine learning
technique called boosting, that is able to selec-
t (presumably the most appropriate) matchers that
participate in an ensemble.
The difference of our work is that the textual
descriptions are not been directly used to measure
the similarities between entities. We learn a rep-
resentation for each ontology entity in an unsuper-
vised way to capture the interactions among the
descriptions, which avoid the problem of selecting
and aggregating different individual similarities.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997875">
The successful ontology matching is very impor-
tant to link heterogeneous ontologies for NLP. In
this paper, we have proposed an ontology match-
ing approach, ERSOM, which describes the class-
es and properties in ontology with abstract repre-
sentations learned from their descriptions and im-
proves the overall matching quality using an it-
erative Similarity Propagation (SP) method based
on more abundant structure information. Experi-
mental results on the datasets from OAEI demon-
strate that our approach performs better than most
of the participants and achieves a competitive per-
formance. In our future work, we will consid-
er to use the ontology matching approach to the
matching between different NLP-oriented ontolo-
gies such as wordnet, Freebase, YAGO, etc.
</bodyText>
<sectionHeader confidence="0.998632" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999348666666667">
This research is supported by National Key Basic
Research Program of China (No.2014CB340504)
and National Natural Science Foundation of China
(No.61375074,61273318). The corresponding au-
thors of this paper are Baobao Chang and Zhifang
Sui.
</bodyText>
<sectionHeader confidence="0.979583" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.701511">
Agirre, E., De Lacalle, O. L., Soroa, A., and
Fakultatea, I. (2009). Knowledge-based wsd
and specific domains: Performing better than
generic supervised wsd. In IJCAI, pages 1501–
1506. Citeseer.
Bengio, Y., Courville, A. C., and Vincent, P.
(2012). Unsupervised feature learning and deep
learning: A review and new perspectives. CoRR
abs/1206.5538.
Bengio, Y., Lamblin, P., Popovici, D., and
Larochelle, H. (2007). Greedy layer-wise train-
ing of deep networks. Advances in neural infor-
mation processing systems, 19:153.
Bourlard, H. and Kamp, Y. (1988). Auto-
association by multilayer perceptrons and sin-
gular value decomposition. Biological cyber-
netics, 59(4-5):291–294.
Bryl, V., Giuliano, C., Serafini, L., and Ty-
moshenko, K. (2010). Supporting natural lan-
guage processing with background knowledge:
Coreference resolution case. In The Semantic
Web–ISWC 2010, pages 80–95. Springer.
Cheatham, M. and Hitzler, P. (2013a). String
similarity metrics for ontology alignment. In
The Semantic Web–ISWC 2013, pages 294–309.
Springer.
Cheatham, M. and Hitzler, P. (2013b). Stringsauto
and mapsss results for oaei 2013. Ontology
Matching, page 146.
</bodyText>
<reference confidence="0.991508178571429">
Coates, A., Ng, A. Y., and Lee, H. (2011). An
analysis of single-layer networks in unsuper-
vised feature learning. In International Con-
ference on Artificial Intelligence and Statistics,
pages 215–223.
Cruz, I. F., Stroe, C., Caci, M., Caimi, F., Pal-
monari, M., Antonelli, F. P., and Keles, U. C.
(2010). Using agreementmaker to align on-
tologies for oaei 2010. In ISWC Interna-
tional Workshop on Ontology Matching (OM).
CEUR Workshop Proceedings, volume 689,
pages 118–125.
Cui, L., Zhang, D., Liu, S., Chen, Q., Li, M.,
Zhou, M., and Yang, M. (2014). Learning top-
ic representation for smt with neural networks.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics,
volume 1, pages 133–143.
Eckert, K., Meilicke, C., and Stuckenschmidt,
H. (2009). Improving ontology matching us-
ing meta-level learning. In The Semantic We-
b: Research and Applications, pages 158–172.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al. (2007).
Ontology matching. Springer.
Euzenat, J., Le Bach, T., Barrasa, J., Bouquet,
P., De Bo, J., Dieng-Kuntz, R., Ehrig, M.,
Hauswirth, M., Jarrar, M., Lara, R., et al.
</reference>
<page confidence="0.757036">
2427
</page>
<reference confidence="0.998997551020408">
(2004). State of the art on ontology alignment.
Knowledge Web Deliverable D, 2:2–3.
Gal, A. (2011). Uncertain schema matching. Syn-
thesis Lectures on Data Management, 3(1):1–
97.
Grau, B. C., Dragisic, Z., Eckert, K., Euzenat, J.,
Ferrara, A., Granada, R., Ivanova, V., Jim´enez-
Ruiz, E., Kempf, A., Lambrix, P., et al. (2014).
Results of the ontology alignment evaluation
initiative 2013. In International Workshop on
Ontology Matching, collocated with the 12th
International Semantic Web Conference-ISWC
2013, pages pp–61.
He, Z., Liu, S., Li, M., Zhou, M., Zhang, L., and
Wang, H. (2013). Learning entity representation
for entity disambiguation. In ACL (2), pages
30–34.
Hinton, G. E. (2007). Learning multiple layers
of representation. Trends in cognitive sciences,
11(10):428–434.
Hinton, G. E. and Zemel, R. S. (1994). Au-
toencoders, minimum description length, and
helmholtz free energy. Advances in neural in-
formation processing systems, pages 3–3.
Huang, J., Dang, J., Vidal, J. M., and Huhns, M. N.
(2007). Ontology matching using an artificial
neural network to learn weights. In Proc. IJCAI
Workshop on Semantic Web for Collaborative
Knowledge Acquisition (SWeCKa-07), India.
Jean-Mary, Y. R., Shironoshita, E. P., and Kabuka,
M. R. (2009). Ontology matching with semantic
verification. Web Semantics: Science, Services
and Agents on the World Wide Web, 7(3):235–
251.
Ji, Q., Haase, P., and Qi, G. (2011). Combination
of similarity measures in ontology matching us-
ing the owa operator. In Recent Developments
in the Ordered Weighted Averaging Operators:
Theory and Practice, pages 281–295. Springer.
Le, B. T., Dieng-Kuntz, R., and Gandon, F. (2004).
On ontology matching problems. ICEIS (4),
pages 236–243.
Li, J., Tang, J., Li, Y., and Luo, Q. (2009). Ri-
mom: A dynamic multistrategy ontology align-
ment framework. Knowledge and Data En-
gineering, IEEE Transactions on, 21(8):1218–
1232.
Li, X., Szpakowicz, S., and Matwin, S. (1995). A
wordnet-based algorithm for word sense disam-
biguation. In IJCAI, volume 95, pages 1368–
1374. Citeseer.
Mao, M., Peng, Y., and Spring, M. (2008). A
harmony based adaptive ontology mapping ap-
proach. In SWWS, pages 336–342.
Mao, M., Peng, Y., and Spring, M. (2010). An
adaptive ontology mapping approach with neu-
ral network based constraint satisfaction. Web
Semantics: Science, Services and Agents on the
World Wide Web, 8(1):14–25.
Melnik, S., Garcia-Molina, H., and Rahm, E.
(2002). Similarity flooding: A versatile
graph matching algorithm and its application to
schema matching. In Data Engineering, 2002.
Proceedings. 18th International Conference on,
pages 117–128. IEEE.
Ngiam, J., Coates, A., Lahiri, A., Prochnow, B.,
Le, Q. V., and Ng, A. Y. (2011). On optimiza-
tion methods for deep learning. In Proceedings
of the 28th International Conference on Ma-
chine Learning (ICML-11), pages 265–272.
Ngo, D. and Bellahsene, Z. (2012). Yam++:
a multi-strategy based approach for ontology
matching task. In Knowledge Engineering
and Knowledge Management, pages 421–425.
Springer.
Pirr´o, G. and Talia, D. (2010). Ufome: An on-
tology mapping system with strategy prediction
capabilities. Data &amp; Knowledge Engineering,
69(5):444–471.
Qu, Y., Hu, W., and Cheng, G. (2006). Construct-
ing virtual documents for ontology matching.
In Proceedings of the 15th international confer-
ence on World Wide Web, pages 23–31. ACM.
Shvaiko, P. and Euzenat, J. (2013). Ontology
matching: state of the art and future challenges.
Knowledge and Data Engineering, IEEE Trans-
actions on, 25(1):158–176.
Spiliopoulos, V., Vouros, G. A., and Karkaletsis,
V. (2010). On the discovery of subsumption re-
lations for the alignment of ontologies. Web Se-
mantics: Science, Services and Agents on the
World Wide Web, 8(1):69–88.
Sunna, W. and Cruz, I. F. (2007). Structure-based
methods to enhance geospatial ontology align-
ment. In GeoSpatial Semantics, pages 82–97.
Springer.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y.,
and Manzagol, P.-A. (2010). Stacked denoising
</reference>
<page confidence="0.794999">
2428
</page>
<reference confidence="0.952932466666667">
autoencoders: Learning useful representations
in a deep network with a local denoising criteri-
on. The Journal of Machine Learning Research,
11:3371–3408.
Wang, P. and Xu, B. (2007). Lily: the results for
the ontology alignment contest oaei 2007. In
Proceedings of the Second International Work-
shop on Ontology Matching, pages 179–187.
Citeseer.
Yao, X., Berant, J., and Van Durme, B. (2014).
Freebase qa: Information extraction or semantic
parsing? ACL 2014, page 82.
Yao, X. and Van Durme, B. (2014). Information
extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
</reference>
<page confidence="0.994475">
2429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363501">
<title confidence="0.9826735">ERSOM: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation</title>
<author confidence="0.6468255">Chuncheng Xiang</author>
<author confidence="0.6468255">Tingsong Jiang</author>
<author confidence="0.6468255">Baobao Chang</author>
<author confidence="0.6468255">Zhifang Key Laboratory of Computational Linguistics</author>
<author confidence="0.6468255">Ministry of</author>
<affiliation confidence="0.993438">School of Electronics Engineering and Computer Science, Peking</affiliation>
<address confidence="0.984225">Collaborative Innovation Center for Language Ability, Xuzhou 221009</address>
<abstract confidence="0.99872058974359">As a key representation model of knowledge, ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc. In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem. Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions. Besides, the structural information of ontology haven’t been utilized adequately for ontology matching. We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings. The experimental results on the datasets from Ontology Alignment Eval- Initiative show that ER- SOM achieves a competitive performance compared to the state-of-the-art ontology matching systems. OAEI is an international initiative organizing annual campaigns for evaluating ontology matching systems. All of the ontologies provided by OAEI are described in OWL-DL language, and like most of the other participates our ERSOM also manages the OWL ontology in its current version. OAEI:</abstract>
<web confidence="0.974166">http://oaei.ontologymatching.org/</web>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Coates</author>
<author>A Y Ng</author>
<author>H Lee</author>
</authors>
<title>An analysis of single-layer networks in unsupervised feature learning.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>215--223</pages>
<contexts>
<context position="12346" citStr="Coates et al., 2011" startWordPosition="1948" endWordPosition="1951">rix and b is the bias term. The goal of the auto-encoder is to minimize the reconstruction error L(x, g(h(x))) = |x−g(h(x))|2, thus retaining 2421 maximum information. Through the combination and transformation, auto-encoder learns the abstract representation h(x) of the input binary term vector. The representation is a real vector with values between 0 and 1. In consideration of the large number of units in the hidden layer (as marked in Fig.2), a sparsity constraint is imposed on the hidden units to hold the capacity to discover interesting structure in the data. We use sparse auto-encoder (Coates et al., 2011) to learn the correlations between descriptions from their binary term vectors. The sparse auto-encoder attempts to minimize the following loss function: � � � x(i) − bx(i)� � λ (kW1kF + kW2kF) + β X KL (ρkbρh) h∈H (2) where x(i) is the binary term vector of the ith entity (a class or a property), bx(i) is the reconstruction of x(i), λ is a regularization parameter, original k · kF is the Frobenius norm, β controls the weight of the sparsity penalty term, H is the set of hidden units, and ρ is the sparsity parameter. Formally, KL (ρkbρh) = ρ log �yh ρ + (1− ρ) log 1−ρ 1−�yh is the Kullback-Lei</context>
</contexts>
<marker>Coates, Ng, Lee, 2011</marker>
<rawString>Coates, A., Ng, A. Y., and Lee, H. (2011). An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, pages 215–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I F Cruz</author>
<author>C Stroe</author>
<author>M Caci</author>
<author>F Caimi</author>
<author>M Palmonari</author>
<author>F P Antonelli</author>
<author>U C Keles</author>
</authors>
<title>Using agreementmaker to align ontologies for oaei 2010.</title>
<date>2010</date>
<booktitle>In ISWC International Workshop on Ontology Matching (OM). CEUR Workshop Proceedings,</booktitle>
<volume>689</volume>
<pages>118--125</pages>
<contexts>
<context position="24657" citStr="Cruz et al., 2010" startWordPosition="4030" endWordPosition="4033">precision. This is because we usually compare the performance of the systems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptions of a class or a property into three different parts). Then we apply a vector space model with TFIDF weighting scheme and cosine similarity measure to compute similarity scores between profiles. And finally, we aggregate these three single similarities using different aggregation methods. Dev. Tes.1 Tes.2 In</context>
</contexts>
<marker>Cruz, Stroe, Caci, Caimi, Palmonari, Antonelli, Keles, 2010</marker>
<rawString>Cruz, I. F., Stroe, C., Caci, M., Caimi, F., Palmonari, M., Antonelli, F. P., and Keles, U. C. (2010). Using agreementmaker to align ontologies for oaei 2010. In ISWC International Workshop on Ontology Matching (OM). CEUR Workshop Proceedings, volume 689, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cui</author>
<author>D Zhang</author>
<author>S Liu</author>
<author>Q Chen</author>
<author>M Li</author>
<author>M Zhou</author>
<author>M Yang</author>
</authors>
<title>Learning topic representation for smt with neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>133--143</pages>
<contexts>
<context position="10920" citStr="Cui et al., 2014" startWordPosition="1707" endWordPosition="1710">epresentations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. Figure 2: Unsupervised representation learning. As shown in Fig.2, the input to the auto-encoder is denoted as x, which indicates a binary term vector of a class or a property. Auto-encoder tries to learn an approximation to the identity function h(x), so as to output x that is similar to x. More specifically, the auto-enc</context>
</contexts>
<marker>Cui, Zhang, Liu, Chen, Li, Zhou, Yang, 2014</marker>
<rawString>Cui, L., Zhang, D., Liu, S., Chen, Q., Li, M., Zhou, M., and Yang, M. (2014). Learning topic representation for smt with neural networks. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 133–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Eckert</author>
<author>C Meilicke</author>
<author>H Stuckenschmidt</author>
</authors>
<title>Improving ontology matching using meta-level learning.</title>
<date>2009</date>
<booktitle>In The Semantic Web: Research and Applications,</booktitle>
<pages>158--172</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="32308" citStr="Eckert et al. (2009)" startWordPosition="5265" endWordPosition="5268">erent matchers. It is a kind of ontology-independent combination method which can assign weights to the entity level, i.e., it use a specific ordered position rather than a weight associated with a specific similarity matrix to aggregate multiple matchers. Jean-Mary et al. (2009) combines different matchers by using a weighted sum strategy that adjusts weights empirically, or based on some static rules. This approach cannot automatically combine different matchers in various matching tasks. There are several works which exploit the supervised machine learning techniques for ontology matching. Eckert et al. (2009), string-based, linguistic and structural measures (in total 23 features) were used as input to train a SVM classifier to align ontologies. CSR (Classificationbased learning of Subsumption Relations) is a generic method for automatic ontology matching between concepts based on supervised machine learning (Spiliopoulos et al., 2010). It specifically focusses on discovering subsumption correspon2426 dences. SMB (Schema Matcher Boosting) is an approach to combining matchers into ensembles (Gal, 2011). It is based on a machine learning technique called boosting, that is able to select (presumably </context>
</contexts>
<marker>Eckert, Meilicke, Stuckenschmidt, 2009</marker>
<rawString>Eckert, K., Meilicke, C., and Stuckenschmidt, H. (2009). Improving ontology matching using meta-level learning. In The Semantic Web: Research and Applications, pages 158–172. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Euzenat</author>
<author>J Euzenat</author>
<author>P Shvaiko</author>
</authors>
<title>Ontology matching.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3046" citStr="Euzenat et al., 2007" startWordPosition="444" endWordPosition="447">mount of logically encoded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitutes a valuable source of semantics. However, extending the state-of-the-art natural language applications to use these resources is not a trivial task mainly due to the heterogeneity and the ambiguity of the schemes adopted by the different resources of the Semantic Web. How to utilize these resources in NLP tasks comprehensively rather than choose just one of them has attracted much attention in recent years. An effective solution to the ontology heterogeneity problem is ontology matching (Euzenat et al., 2007; Shvaiko and Euzenat, 2013), whose main task is to establish semantic correspondences between entities (i.e., classes, properties or instances) from different ontologies. Ontology matching is usually done by measuring the similarity between two entities from two different ontologies. To effectively calculate the similarities, almost all types of descriptions of an entity should be used. In previous works, given the different nature of different kinds of descriptions, similarities are normally measured separately with different methods and then aggregated with some kind of combination strategy</context>
<context position="30787" citStr="Euzenat et al., 2007" startWordPosition="5031" endWordPosition="5034">datasets that taken from Benchmark dataset published in OAEI 2009 are used to generate training data to train a decision tree classifier. And in the classifying phase, each pair of elements from two to-be-matched ontologies is predicted as matched or not according to its attributes. However, ERSOM is an unsupervised approach, but it does not exclude using external resources and training data to help learning the representations of entities and provide the initial similarity matrix for the SP method to further improve the performance. 5 Related work There are many studies on Ontology Matching (Euzenat et al., 2007; Shvaiko and Euzenat, 2013). Currently, almost all ontology matching systems exploit various kinds of information provided in Figure 4: Comparison with other OAEI systems. ontologies to get better performance. To aggregate the different similarity matrixes, various approaches have been proposed. Pirr´o and Talia (2010) is a generic schema matching system. It exploits Max, Min, Average and Weighted strategies for the combination. The weighted method assigns a relative weight to each similarity matrix, and calculates a weighted sum of similarity for all similarity matrixes. The Average method i</context>
</contexts>
<marker>Euzenat, Euzenat, Shvaiko, 2007</marker>
<rawString>Euzenat, J., Euzenat, J., Shvaiko, P., et al. (2007). Ontology matching. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Euzenat</author>
<author>T Le Bach</author>
<author>J Barrasa</author>
<author>P Bouquet</author>
<author>J De Bo</author>
<author>R Dieng-Kuntz</author>
<author>M Ehrig</author>
<author>M Hauswirth</author>
<author>M Jarrar</author>
<author>R Lara</author>
</authors>
<marker>Euzenat, Le Bach, Barrasa, Bouquet, De Bo, Dieng-Kuntz, Ehrig, Hauswirth, Jarrar, Lara, </marker>
<rawString>Euzenat, J., Le Bach, T., Barrasa, J., Bouquet, P., De Bo, J., Dieng-Kuntz, R., Ehrig, M., Hauswirth, M., Jarrar, M., Lara, R., et al.</rawString>
</citation>
<citation valid="true">
<title>State of the art on ontology alignment. Knowledge Web Deliverable D,</title>
<date>2004</date>
<pages>2--2</pages>
<marker>2004</marker>
<rawString>(2004). State of the art on ontology alignment. Knowledge Web Deliverable D, 2:2–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gal</author>
</authors>
<title>Uncertain schema matching.</title>
<date>2011</date>
<journal>Synthesis Lectures on Data Management,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>97</pages>
<contexts>
<context position="32810" citStr="Gal, 2011" startWordPosition="5339" endWordPosition="5340">l works which exploit the supervised machine learning techniques for ontology matching. Eckert et al. (2009), string-based, linguistic and structural measures (in total 23 features) were used as input to train a SVM classifier to align ontologies. CSR (Classificationbased learning of Subsumption Relations) is a generic method for automatic ontology matching between concepts based on supervised machine learning (Spiliopoulos et al., 2010). It specifically focusses on discovering subsumption correspon2426 dences. SMB (Schema Matcher Boosting) is an approach to combining matchers into ensembles (Gal, 2011). It is based on a machine learning technique called boosting, that is able to select (presumably the most appropriate) matchers that participate in an ensemble. The difference of our work is that the textual descriptions are not been directly used to measure the similarities between entities. We learn a representation for each ontology entity in an unsupervised way to capture the interactions among the descriptions, which avoid the problem of selecting and aggregating different individual similarities. 6 Conclusions The successful ontology matching is very important to link heterogeneous onto</context>
</contexts>
<marker>Gal, 2011</marker>
<rawString>Gal, A. (2011). Uncertain schema matching. Synthesis Lectures on Data Management, 3(1):1– 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Grau</author>
<author>Z Dragisic</author>
<author>K Eckert</author>
<author>J Euzenat</author>
<author>A Ferrara</author>
<author>R Granada</author>
<author>V Ivanova</author>
<author>E Jim´enezRuiz</author>
<author>A Kempf</author>
<author>P Lambrix</author>
</authors>
<title>Results of the ontology alignment evaluation initiative 2013.</title>
<date>2014</date>
<booktitle>In International Workshop on Ontology Matching, collocated with the 12th International Semantic Web Conference-ISWC</booktitle>
<pages>61</pages>
<marker>Grau, Dragisic, Eckert, Euzenat, Ferrara, Granada, Ivanova, Jim´enezRuiz, Kempf, Lambrix, 2014</marker>
<rawString>Grau, B. C., Dragisic, Z., Eckert, K., Euzenat, J., Ferrara, A., Granada, R., Ivanova, V., Jim´enezRuiz, E., Kempf, A., Lambrix, P., et al. (2014). Results of the ontology alignment evaluation initiative 2013. In International Workshop on Ontology Matching, collocated with the 12th International Semantic Web Conference-ISWC 2013, pages pp–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z He</author>
<author>S Liu</author>
<author>M Li</author>
<author>M Zhou</author>
<author>L Zhang</author>
<author>H Wang</author>
</authors>
<title>Learning entity representation for entity disambiguation.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>30--34</pages>
<contexts>
<context position="10901" citStr="He et al., 2013" startWordPosition="1703" endWordPosition="1706">rn the abstract representations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. Figure 2: Unsupervised representation learning. As shown in Fig.2, the input to the auto-encoder is denoted as x, which indicates a binary term vector of a class or a property. Auto-encoder tries to learn an approximation to the identity function h(x), so as to output x that is similar to x. More specifi</context>
<context position="13346" citStr="He et al. (2013)" startWordPosition="2130" endWordPosition="2133"> is the Frobenius norm, β controls the weight of the sparsity penalty term, H is the set of hidden units, and ρ is the sparsity parameter. Formally, KL (ρkbρh) = ρ log �yh ρ + (1− ρ) log 1−ρ 1−�yh is the Kullback-Leibler (KL) divergence between a Bernoulli random variable with mean ρ and a Bernoulli random variable with mean byh. Figure 3: Learning higher level representations. 3.1.3 Learning higher level representations The auto-encoder which only has one hidden layer may not be enough to learn the complex interactions between input features. Inspired by the work of Vincent et al. (2010) and He et al. (2013), we build multi-layer model to learn more abstract entity representations. To achieve this, we repeatedly stack new sparse auto-encoder on top of the previously learned h(x) (i.e., the higher level representations are formed by combination of lower level representations). This model is called Stacked Auto-Encoder (SAE) by Bengio et al. (2007). In this way, when we input the binary term vector to the network, we can get its abstract representations in different levels. In other words, with the layer-by-layer learning, we obtain different levels of representations. The top-level representation,</context>
</contexts>
<marker>He, Liu, Li, Zhou, Zhang, Wang, 2013</marker>
<rawString>He, Z., Liu, S., Li, M., Zhou, M., Zhang, L., and Wang, H. (2013). Learning entity representation for entity disambiguation. In ACL (2), pages 30–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Learning multiple layers of representation. Trends in cognitive sciences,</title>
<date>2007</date>
<pages>11--10</pages>
<contexts>
<context position="10863" citStr="Hinton, 2007" startWordPosition="1697" endWordPosition="1698">We state in this section how to learn the abstract representations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. Figure 2: Unsupervised representation learning. As shown in Fig.2, the input to the auto-encoder is denoted as x, which indicates a binary term vector of a class or a property. Auto-encoder tries to learn an approximation to the identity function h(x), so as to outpu</context>
</contexts>
<marker>Hinton, 2007</marker>
<rawString>Hinton, G. E. (2007). Learning multiple layers of representation. Trends in cognitive sciences, 11(10):428–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R S Zemel</author>
</authors>
<title>Autoencoders, minimum description length, and helmholtz free energy. Advances in neural information processing systems,</title>
<date>1994</date>
<pages>3--3</pages>
<contexts>
<context position="10991" citStr="Hinton and Zemel, 1994" startWordPosition="1718" endWordPosition="1722">n unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. Figure 2: Unsupervised representation learning. As shown in Fig.2, the input to the auto-encoder is denoted as x, which indicates a binary term vector of a class or a property. Auto-encoder tries to learn an approximation to the identity function h(x), so as to output x that is similar to x. More specifically, the auto-encoder consists of an encoding process h(x) = f(Wx + b1) and a decoding p</context>
</contexts>
<marker>Hinton, Zemel, 1994</marker>
<rawString>Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and helmholtz free energy. Advances in neural information processing systems, pages 3–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>J Dang</author>
<author>J M Vidal</author>
<author>M N Huhns</author>
</authors>
<title>Ontology matching using an artificial neural network to learn weights.</title>
<date>2007</date>
<booktitle>In Proc. IJCAI Workshop on Semantic Web for Collaborative Knowledge Acquisition (SWeCKa-07),</booktitle>
<contexts>
<context position="18591" citStr="Huang et al., 2007" startWordPosition="3015" endWordPosition="3018">vely. cp() is the function to compute the similarities propagated from the adjacent node σij connected to node (x, y) in the (i + 1)th iteration. And wj is the weight of edge between the node (x, y) and its jth neighboring node. During each iteration in the final step of Algorithm 1, only the similarity value between two entities in the node will be updated. At the end of each iteration, all similarity values are normalized by a Normalized function to all in range [0, 1]. The iteration stops after a predefined number of steps. 3.3 Mapping selection Similar with the work in (Wang and Xu, 2007; Huang et al., 2007; Ngo and Bellahsene, 2012), we use the Stable Marriage (SM) algorithm (Melnik et al., 2002) to choose the 1:1 mappings from the M rows and N columns similarity matrix, where M and N is the number of classes and properties in ontologies O1 and O2, respectively. In addition, before we run the SM algorithm we set the value of cell [i, j] of the similarity matrix to zero if i and j correspond to different types of entities. Thus, we remove lots of redundant data and only find the mappings between classes or properties. 4 Experiments 4.1 Data sets and evaluation criteria The annual OAEI campaign i</context>
</contexts>
<marker>Huang, Dang, Vidal, Huhns, 2007</marker>
<rawString>Huang, J., Dang, J., Vidal, J. M., and Huhns, M. N. (2007). Ontology matching using an artificial neural network to learn weights. In Proc. IJCAI Workshop on Semantic Web for Collaborative Knowledge Acquisition (SWeCKa-07), India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y R Jean-Mary</author>
<author>E P Shironoshita</author>
<author>M R Kabuka</author>
</authors>
<title>Ontology matching with semantic verification. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2009</date>
<volume>7</volume>
<issue>3</issue>
<pages>251</pages>
<contexts>
<context position="31968" citStr="Jean-Mary et al. (2009)" startWordPosition="5214" endWordPosition="5217"> similarity matrixes. The Average method is a special case of Weighted, which considers each similarity matrix equally important in the combination. Max and Min are two extreme cases that return the highest and lowest similarities in all similarity matrixes respectively. Ji et al. (2011) use the Ordered Weighted Average (OWA) to combine different matchers. It is a kind of ontology-independent combination method which can assign weights to the entity level, i.e., it use a specific ordered position rather than a weight associated with a specific similarity matrix to aggregate multiple matchers. Jean-Mary et al. (2009) combines different matchers by using a weighted sum strategy that adjusts weights empirically, or based on some static rules. This approach cannot automatically combine different matchers in various matching tasks. There are several works which exploit the supervised machine learning techniques for ontology matching. Eckert et al. (2009), string-based, linguistic and structural measures (in total 23 features) were used as input to train a SVM classifier to align ontologies. CSR (Classificationbased learning of Subsumption Relations) is a generic method for automatic ontology matching between </context>
</contexts>
<marker>Jean-Mary, Shironoshita, Kabuka, 2009</marker>
<rawString>Jean-Mary, Y. R., Shironoshita, E. P., and Kabuka, M. R. (2009). Ontology matching with semantic verification. Web Semantics: Science, Services and Agents on the World Wide Web, 7(3):235– 251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Ji</author>
<author>P Haase</author>
<author>G Qi</author>
</authors>
<title>Combination of similarity measures in ontology matching using the owa operator. In Recent Developments in the Ordered Weighted Averaging Operators: Theory and Practice,</title>
<date>2011</date>
<pages>281--295</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="31633" citStr="Ji et al. (2011)" startWordPosition="5161" endWordPosition="5164">fferent similarity matrixes, various approaches have been proposed. Pirr´o and Talia (2010) is a generic schema matching system. It exploits Max, Min, Average and Weighted strategies for the combination. The weighted method assigns a relative weight to each similarity matrix, and calculates a weighted sum of similarity for all similarity matrixes. The Average method is a special case of Weighted, which considers each similarity matrix equally important in the combination. Max and Min are two extreme cases that return the highest and lowest similarities in all similarity matrixes respectively. Ji et al. (2011) use the Ordered Weighted Average (OWA) to combine different matchers. It is a kind of ontology-independent combination method which can assign weights to the entity level, i.e., it use a specific ordered position rather than a weight associated with a specific similarity matrix to aggregate multiple matchers. Jean-Mary et al. (2009) combines different matchers by using a weighted sum strategy that adjusts weights empirically, or based on some static rules. This approach cannot automatically combine different matchers in various matching tasks. There are several works which exploit the supervi</context>
</contexts>
<marker>Ji, Haase, Qi, 2011</marker>
<rawString>Ji, Q., Haase, P., and Qi, G. (2011). Combination of similarity measures in ontology matching using the owa operator. In Recent Developments in the Ordered Weighted Averaging Operators: Theory and Practice, pages 281–295. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Le</author>
<author>R Dieng-Kuntz</author>
<author>F Gandon</author>
</authors>
<title>On ontology matching problems.</title>
<date>2004</date>
<journal>ICEIS</journal>
<volume>4</volume>
<pages>236--243</pages>
<contexts>
<context position="5185" citStr="Le et al., 2004" startWordPosition="769" endWordPosition="772">roperties, and a property is usually restricted by its domain and range. These potential correlations of the descriptions are very important to measure the similarity between entities since they can be treated as some potential features describing an entity. Second, it is difficult to estimate how many and which kind of single similarities are needed for an aggregation method to get a satisfactory result. On the other hand, in order to find more mappings, many structural ontology matching methods are proposed. To the best of our knowledge, previous structural methods are either local methods (Le et al., 2004; Sunna and Cruz, 2007) or global (i.e. iterative) methods but only use part of the structure information of the ontology (Li et al., 2009; Ngo and Bellahsene, 2012). For example, the ontology matching system YAM++ (Ngo and Bellahsene, 2012) utilizes a global structural method but it only uses the structure information of classes and properties to create the propagation graph to find mappings between classes and properties. A large amount of instances and their relations to classes and properties in the ontology haven’t been exploited in this system. To overcome the existing limitations, we pr</context>
<context position="26699" citStr="Le et al., 2004" startWordPosition="4348" endWordPosition="4351">cond, the performance is dramatically boosted by aggregation methods and they all achieve F-measures higher than 0.7, so the aggregation methods are very effective in improving the performance of mapping approaches that rely on measuring multiple similarities. And finally, our Unsupervised Representation Learning (URL) method holds the highest F-measure both on the development dataset and on the testing datasets. 4.2.3 Evaluation for our structural method In this experiment, we compare our Similarity Propagation (SP) method to other structure based methods, that is, ADJACENTS and ASCOPATH in (Le et al., 2004); DSI and SSC in (Sunna and Cruz, 2007); Li’s SP (Li et al., 2009) and Ngo’s SP (Ngo and Bellahsene, 2012). We first use entity’s ID to compute the similarity between classes and properties to provide an unified initial similarity matrix as input (or initialization) for our SP and other structural methods. Then, a new similarity matrix will be created and updated by considering the initial similarities and different structure information. And finally, we extract the mappings from the newly created similarity matrix with the strategy described in section 2.3. Dev. Tes.1 Tes.2 Initial Matcher 0.</context>
</contexts>
<marker>Le, Dieng-Kuntz, Gandon, 2004</marker>
<rawString>Le, B. T., Dieng-Kuntz, R., and Gandon, F. (2004). On ontology matching problems. ICEIS (4), pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>J Tang</author>
<author>Y Li</author>
<author>Q Luo</author>
</authors>
<title>Rimom: A dynamic multistrategy ontology alignment framework.</title>
<date>2009</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>21</volume>
<issue>8</issue>
<pages>1232</pages>
<contexts>
<context position="5323" citStr="Li et al., 2009" startWordPosition="793" endWordPosition="796">nt to measure the similarity between entities since they can be treated as some potential features describing an entity. Second, it is difficult to estimate how many and which kind of single similarities are needed for an aggregation method to get a satisfactory result. On the other hand, in order to find more mappings, many structural ontology matching methods are proposed. To the best of our knowledge, previous structural methods are either local methods (Le et al., 2004; Sunna and Cruz, 2007) or global (i.e. iterative) methods but only use part of the structure information of the ontology (Li et al., 2009; Ngo and Bellahsene, 2012). For example, the ontology matching system YAM++ (Ngo and Bellahsene, 2012) utilizes a global structural method but it only uses the structure information of classes and properties to create the propagation graph to find mappings between classes and properties. A large amount of instances and their relations to classes and properties in the ontology haven’t been exploited in this system. To overcome the existing limitations, we propose in this paper a representation learning method to capture the interactions among entity’s descriptions; then we present our global s</context>
<context position="7702" citStr="Li et al., 2009" startWordPosition="1179" endWordPosition="1182">ared conceptualization in terms of classes, properties and relations (Euzenat et al., 2004). The process of ontology matching is to find mappings (or correspondences) between entities (classes, properties or individuals) from two ontologies. A mapping is defined as a four-tuple as written in Eq.(1), where e1 and e2 represent the entity in ontology O1 and O2 respectively, r is a kind of matching relation (e.g., equivalent, subsume) and k → [0, 1] is the degree of confidence of matching relation between e1 and e2 (Mao et al., 2010). m =&lt; e1, e2, r, k &gt; (1) Similar with most of the OAEI systems (Li et al., 2009; Ngo and Bellahsene, 2012; Cheatham and Hitzler, 2013b), we focus on discovering only equivalent mappings between classes and properties with cardinality 1:1. That is, one class (property) in ontology O1 can be matched to at most one class (property) in ontology O2 and vise versa. 3 ERSOM: Entity Representation and Structure based Ontology Matching In this paper, we propose a structural ontology matching approach using automatically learned entity representation, which we call ERSOM. Fig.1 shows the architecture of our approach. The details of its major modules are given in the following sect</context>
<context position="15003" citStr="Li et al., 2009" startWordPosition="2392" endWordPosition="2396">al descriptions (such as ID, label and comments etc.) of entities in ontology. According to the study in (Melnik et al., 2002), we present our structural method or called Similarity Propagation (SP) method, which exploits more abundant structure information of the ontology to discover more mappings globally. The intuition behind the propagation idea is that the similarity between two entities is not only about their neighbor entities, but it is about all other entities (neighbor entities’ neighbor entities) in the ontologies. This idea has also been used in the ontology matching systems RiMOM(Li et al., 2009) and YAM++(Ngo and Bellahsene, 2012) in order to find mappings between classes and properties. But the nodes in their propagation graph are just limited to class pairs and property pairs, and the propagation edges are transformed from relations between two classes, two properties or a class and a property. The difference of our SP method is that we consider the instances and its relations with classes and properties when creating the propagation graph even if we also only find mappings between classes and properties. This is because (1) the similar degree of two classes will be increased if th</context>
<context position="24370" citStr="Li et al., 2009" startWordPosition="3987" endWordPosition="3990">apply them on the test tasks directly. The precision is reduced when we use URL method, this may be due to the learned representations of entities are too general. In addition, in the parameter adjustment process, we try to make the F value maximization, but not to care about mapping precision. This is because we usually compare the performance of the systems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptions of a class or a prope</context>
<context position="26765" citStr="Li et al., 2009" startWordPosition="4362" endWordPosition="4365">ds and they all achieve F-measures higher than 0.7, so the aggregation methods are very effective in improving the performance of mapping approaches that rely on measuring multiple similarities. And finally, our Unsupervised Representation Learning (URL) method holds the highest F-measure both on the development dataset and on the testing datasets. 4.2.3 Evaluation for our structural method In this experiment, we compare our Similarity Propagation (SP) method to other structure based methods, that is, ADJACENTS and ASCOPATH in (Le et al., 2004); DSI and SSC in (Sunna and Cruz, 2007); Li’s SP (Li et al., 2009) and Ngo’s SP (Ngo and Bellahsene, 2012). We first use entity’s ID to compute the similarity between classes and properties to provide an unified initial similarity matrix as input (or initialization) for our SP and other structural methods. Then, a new similarity matrix will be created and updated by considering the initial similarities and different structure information. And finally, we extract the mappings from the newly created similarity matrix with the strategy described in section 2.3. Dev. Tes.1 Tes.2 Initial Matcher 0.616 0.524 0.523 ADJACENTS 0.622 0.569 0.570 ASCOPATH 0.604 0.540 0</context>
</contexts>
<marker>Li, Tang, Li, Luo, 2009</marker>
<rawString>Li, J., Tang, J., Li, Y., and Luo, Q. (2009). Rimom: A dynamic multistrategy ontology alignment framework. Knowledge and Data Engineering, IEEE Transactions on, 21(8):1218– 1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>S Szpakowicz</author>
<author>S Matwin</author>
</authors>
<title>A wordnet-based algorithm for word sense disambiguation.</title>
<date>1995</date>
<booktitle>In IJCAI,</booktitle>
<volume>95</volume>
<pages>1368--1374</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="19550" citStr="Li et al., 1995" startWordPosition="3184" endWordPosition="3187"> similarity matrix to zero if i and j correspond to different types of entities. Thus, we remove lots of redundant data and only find the mappings between classes or properties. 4 Experiments 4.1 Data sets and evaluation criteria The annual OAEI campaign is an authoritative contest in the area of ontology matching, we choose the data from OAEI in our experiments, because the evaluation metrics have been well defined and the comparision can be easily made. We observe strong structure similarities lies between OAEI ontologies and ontologies used in NLP tasks, such as WordNet and HowNet for WSD (Li et al., 1995; Agirre et al., 2009), and Freebase, YAGO, and knowledge graph for IE, text mining and QA (Yao and Van Durme, 2014; Yao et al., 2014), both describe entities and their relations with class, properties and instances. Development dataset: the Standard Benchmark 2012 dataset that OAEI provides for developers to test their system before participating in the competition is used as the development dataset in our experiments. This dataset contains one reference ontology and 109 target ontologies. We use cp (σ0 + σi) = Xm j=1 2423 this dataset to test various values for the parameters in our ERSOM an</context>
</contexts>
<marker>Li, Szpakowicz, Matwin, 1995</marker>
<rawString>Li, X., Szpakowicz, S., and Matwin, S. (1995). A wordnet-based algorithm for word sense disambiguation. In IJCAI, volume 95, pages 1368– 1374. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mao</author>
<author>Y Peng</author>
<author>M Spring</author>
</authors>
<title>A harmony based adaptive ontology mapping approach.</title>
<date>2008</date>
<booktitle>In SWWS,</booktitle>
<pages>336--342</pages>
<contexts>
<context position="24686" citStr="Mao et al., 2008" startWordPosition="4035" endWordPosition="4038">sually compare the performance of the systems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptions of a class or a property into three different parts). Then we apply a vector space model with TFIDF weighting scheme and cosine similarity measure to compute similarity scores between profiles. And finally, we aggregate these three single similarities using different aggregation methods. Dev. Tes.1 Tes.2 Individual Profile 0.668 0.612 </context>
</contexts>
<marker>Mao, Peng, Spring, 2008</marker>
<rawString>Mao, M., Peng, Y., and Spring, M. (2008). A harmony based adaptive ontology mapping approach. In SWWS, pages 336–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mao</author>
<author>Y Peng</author>
<author>M Spring</author>
</authors>
<title>An adaptive ontology mapping approach with neural network based constraint satisfaction. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2010</date>
<contexts>
<context position="3716" citStr="Mao et al. (2010)" startWordPosition="541" endWordPosition="544">stablish semantic correspondences between entities (i.e., classes, properties or instances) from different ontologies. Ontology matching is usually done by measuring the similarity between two entities from two different ontologies. To effectively calculate the similarities, almost all types of descriptions of an entity should be used. In previous works, given the different nature of different kinds of descriptions, similarities are normally measured separately with different methods and then aggregated with some kind of combination strategy to compute the final similarity score. For example, Mao et al. (2010) defined three single similarities (i.e., Name similarity, Profile similarity and Structural similarity) based on the descriptions of an entity, then they employed a harmony-based method to aggregate 2419 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2419–2429, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. the single similarities to get a final similarity for extracting the final mappings. However, treating different kinds of descriptions of an entity separately suffers from two limitations. First, it l</context>
<context position="7622" citStr="Mao et al., 2010" startWordPosition="1160" endWordPosition="1163"> systems. 2 Problem Statement Ontology is a formal, explicit specification of a shared conceptualization in terms of classes, properties and relations (Euzenat et al., 2004). The process of ontology matching is to find mappings (or correspondences) between entities (classes, properties or individuals) from two ontologies. A mapping is defined as a four-tuple as written in Eq.(1), where e1 and e2 represent the entity in ontology O1 and O2 respectively, r is a kind of matching relation (e.g., equivalent, subsume) and k → [0, 1] is the degree of confidence of matching relation between e1 and e2 (Mao et al., 2010). m =&lt; e1, e2, r, k &gt; (1) Similar with most of the OAEI systems (Li et al., 2009; Ngo and Bellahsene, 2012; Cheatham and Hitzler, 2013b), we focus on discovering only equivalent mappings between classes and properties with cardinality 1:1. That is, one class (property) in ontology O1 can be matched to at most one class (property) in ontology O2 and vise versa. 3 ERSOM: Entity Representation and Structure based Ontology Matching In this paper, we propose a structural ontology matching approach using automatically learned entity representation, which we call ERSOM. Fig.1 shows the architecture o</context>
<context position="24727" citStr="Mao et al., 2010" startWordPosition="4044" endWordPosition="4047">tems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptions of a class or a property into three different parts). Then we apply a vector space model with TFIDF weighting scheme and cosine similarity measure to compute similarity scores between profiles. And finally, we aggregate these three single similarities using different aggregation methods. Dev. Tes.1 Tes.2 Individual Profile 0.668 0.612 0.611 Semantic Profile 0.434 0.472 0.477 </context>
</contexts>
<marker>Mao, Peng, Spring, 2010</marker>
<rawString>Mao, M., Peng, Y., and Spring, M. (2010). An adaptive ontology mapping approach with neural network based constraint satisfaction. Web Semantics: Science, Services and Agents on the World Wide Web, 8(1):14–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Melnik</author>
<author>H Garcia-Molina</author>
<author>E Rahm</author>
</authors>
<title>Similarity flooding: A versatile graph matching algorithm and its application to schema matching.</title>
<date>2002</date>
<booktitle>In Data Engineering,</booktitle>
<pages>117--128</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="14513" citStr="Melnik et al., 2002" startWordPosition="2314" endWordPosition="2317">els of representations. The top-level representation, which models the final interactions of the original descriptions, can be used to measure the similarity between classes and properties. The prototype of Stacked Auto-Encoder (SAE) is shown in Fig.3, where f(h(x))(m) denotes the final representation learned by the top-level hidden layer and superscript m means the SAE consists of m sparse auto-encoders. 3.2 Optimizing with the ontology structure The above method can only consider the local descriptions (such as ID, label and comments etc.) of entities in ontology. According to the study in (Melnik et al., 2002), we present our structural method or called Similarity Propagation (SP) method, which exploits more abundant structure information of the ontology to discover more mappings globally. The intuition behind the propagation idea is that the similarity between two entities is not only about their neighbor entities, but it is about all other entities (neighbor entities’ neighbor entities) in the ontologies. This idea has also been used in the ontology matching systems RiMOM(Li et al., 2009) and YAM++(Ngo and Bellahsene, 2012) in order to find mappings between classes and properties. But the nodes i</context>
<context position="16965" citStr="Melnik et al., 2002" startWordPosition="2726" endWordPosition="2729">asDomain, HasRange. Then we create a Pairwise Connectivity Graph (PCG) from two DLGs by merging edges having the same labels. Algorithm 1: Our SP Algorithm Input: The to-be-matched ontologies, OR and OT; The initial similarity matrix, M0; The edges’ weight matrix, W; Output: The updated similarity matrix, M1; 1 DLG1 Transform(OR); 2 DLG2 Transform(OT); 3 PCG Merge(DLG1, DLG2); 4 IPG Initiate(PCG, M0, W); 5 M1 Propagation(IPG, Normalized); In the fourth step of Algorithm 1, for a PCG, we assign weight values to edges as the inverse of the number of out-linking relationships of its source node (Melnik et al., 2002). For the nodes that consist of two classes or two properties, we assign them values calculated with the cosine similarity between their representations learned in section 2.1.3. For the node consisting of two instances, the similarity value assigned to it is measured with the ScaledLevenstein2 between the IDs of instances. In this way, we construct an Induced Propagation Graph (IPG) on which the propagation algorithm will run iteratively. Let σ(x, y) denotes the similarity score between entities x and y for node (x, y) in the IPG. At the (i + 1)th iteration, the similarity score is updated as</context>
<context position="18683" citStr="Melnik et al., 2002" startWordPosition="3030" endWordPosition="3033">ij connected to node (x, y) in the (i + 1)th iteration. And wj is the weight of edge between the node (x, y) and its jth neighboring node. During each iteration in the final step of Algorithm 1, only the similarity value between two entities in the node will be updated. At the end of each iteration, all similarity values are normalized by a Normalized function to all in range [0, 1]. The iteration stops after a predefined number of steps. 3.3 Mapping selection Similar with the work in (Wang and Xu, 2007; Huang et al., 2007; Ngo and Bellahsene, 2012), we use the Stable Marriage (SM) algorithm (Melnik et al., 2002) to choose the 1:1 mappings from the M rows and N columns similarity matrix, where M and N is the number of classes and properties in ontologies O1 and O2, respectively. In addition, before we run the SM algorithm we set the value of cell [i, j] of the similarity matrix to zero if i and j correspond to different types of entities. Thus, we remove lots of redundant data and only find the mappings between classes or properties. 4 Experiments 4.1 Data sets and evaluation criteria The annual OAEI campaign is an authoritative contest in the area of ontology matching, we choose the data from OAEI in</context>
</contexts>
<marker>Melnik, Garcia-Molina, Rahm, 2002</marker>
<rawString>Melnik, S., Garcia-Molina, H., and Rahm, E. (2002). Similarity flooding: A versatile graph matching algorithm and its application to schema matching. In Data Engineering, 2002. Proceedings. 18th International Conference on, pages 117–128. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ngiam</author>
<author>A Coates</author>
<author>A Lahiri</author>
<author>B Prochnow</author>
<author>Q V Le</author>
<author>A Y Ng</author>
</authors>
<title>On optimization methods for deep learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>265--272</pages>
<contexts>
<context position="21213" citStr="Ngiam et al., 2011" startWordPosition="3462" endWordPosition="3465">rget ontology should be mapped to the reference ontology. We followed the standard evaluation criteria from the OAEI, calculating the precision, recall and fmeasure over each test. The version computed here is the harmonic mean of precision and recall. 4.2 Experimental design and results 4.2.1 Evaluation for representation learning We first use Jena3 parsing the ontologies and extract descriptions for entities according to the description in section 2.1.1, then we create a vocabulary based on the dataset and denote each class and property as a binary term vector. We apply the LBFGS algorithm (Ngiam et al., 2011) to train the stacked auto-encoder described in section 2.1.3. The size of the input layer is equals to the length of the vocabulary created from the dataset. We fix the parameters A = 1e − 4, Q = 3 and p = 0.25 in Eq.2, and set the size of the first and second hidden layer of the stacked auto-encoder to 200 and 100, respectively, by experience. The number of iterations of the L-BFGS algorithm is set to 500. We use the learned representations to measure the similarities between classes and properties and apply the strategy presented in section 2.3 to extract final mappings. The matching result</context>
</contexts>
<marker>Ngiam, Coates, Lahiri, Prochnow, Le, Ng, 2011</marker>
<rawString>Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., Le, Q. V., and Ng, A. Y. (2011). On optimization methods for deep learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ngo</author>
<author>Z Bellahsene</author>
</authors>
<title>Yam++: a multi-strategy based approach for ontology matching task.</title>
<date>2012</date>
<booktitle>In Knowledge Engineering and Knowledge Management,</booktitle>
<pages>421--425</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5350" citStr="Ngo and Bellahsene, 2012" startWordPosition="797" endWordPosition="800"> similarity between entities since they can be treated as some potential features describing an entity. Second, it is difficult to estimate how many and which kind of single similarities are needed for an aggregation method to get a satisfactory result. On the other hand, in order to find more mappings, many structural ontology matching methods are proposed. To the best of our knowledge, previous structural methods are either local methods (Le et al., 2004; Sunna and Cruz, 2007) or global (i.e. iterative) methods but only use part of the structure information of the ontology (Li et al., 2009; Ngo and Bellahsene, 2012). For example, the ontology matching system YAM++ (Ngo and Bellahsene, 2012) utilizes a global structural method but it only uses the structure information of classes and properties to create the propagation graph to find mappings between classes and properties. A large amount of instances and their relations to classes and properties in the ontology haven’t been exploited in this system. To overcome the existing limitations, we propose in this paper a representation learning method to capture the interactions among entity’s descriptions; then we present our global structural method which expl</context>
<context position="7728" citStr="Ngo and Bellahsene, 2012" startWordPosition="1183" endWordPosition="1186">ation in terms of classes, properties and relations (Euzenat et al., 2004). The process of ontology matching is to find mappings (or correspondences) between entities (classes, properties or individuals) from two ontologies. A mapping is defined as a four-tuple as written in Eq.(1), where e1 and e2 represent the entity in ontology O1 and O2 respectively, r is a kind of matching relation (e.g., equivalent, subsume) and k → [0, 1] is the degree of confidence of matching relation between e1 and e2 (Mao et al., 2010). m =&lt; e1, e2, r, k &gt; (1) Similar with most of the OAEI systems (Li et al., 2009; Ngo and Bellahsene, 2012; Cheatham and Hitzler, 2013b), we focus on discovering only equivalent mappings between classes and properties with cardinality 1:1. That is, one class (property) in ontology O1 can be matched to at most one class (property) in ontology O2 and vise versa. 3 ERSOM: Entity Representation and Structure based Ontology Matching In this paper, we propose a structural ontology matching approach using automatically learned entity representation, which we call ERSOM. Fig.1 shows the architecture of our approach. The details of its major modules are given in the following sections. 3.1 Learning the rep</context>
<context position="15039" citStr="Ngo and Bellahsene, 2012" startWordPosition="2398" endWordPosition="2402">, label and comments etc.) of entities in ontology. According to the study in (Melnik et al., 2002), we present our structural method or called Similarity Propagation (SP) method, which exploits more abundant structure information of the ontology to discover more mappings globally. The intuition behind the propagation idea is that the similarity between two entities is not only about their neighbor entities, but it is about all other entities (neighbor entities’ neighbor entities) in the ontologies. This idea has also been used in the ontology matching systems RiMOM(Li et al., 2009) and YAM++(Ngo and Bellahsene, 2012) in order to find mappings between classes and properties. But the nodes in their propagation graph are just limited to class pairs and property pairs, and the propagation edges are transformed from relations between two classes, two properties or a class and a property. The difference of our SP method is that we consider the instances and its relations with classes and properties when creating the propagation graph even if we also only find mappings between classes and properties. This is because (1) the similar degree of two classes will be increased if they have some of similar instances; (</context>
<context position="18618" citStr="Ngo and Bellahsene, 2012" startWordPosition="3019" endWordPosition="3022">nction to compute the similarities propagated from the adjacent node σij connected to node (x, y) in the (i + 1)th iteration. And wj is the weight of edge between the node (x, y) and its jth neighboring node. During each iteration in the final step of Algorithm 1, only the similarity value between two entities in the node will be updated. At the end of each iteration, all similarity values are normalized by a Normalized function to all in range [0, 1]. The iteration stops after a predefined number of steps. 3.3 Mapping selection Similar with the work in (Wang and Xu, 2007; Huang et al., 2007; Ngo and Bellahsene, 2012), we use the Stable Marriage (SM) algorithm (Melnik et al., 2002) to choose the 1:1 mappings from the M rows and N columns similarity matrix, where M and N is the number of classes and properties in ontologies O1 and O2, respectively. In addition, before we run the SM algorithm we set the value of cell [i, j] of the similarity matrix to zero if i and j correspond to different types of entities. Thus, we remove lots of redundant data and only find the mappings between classes or properties. 4 Experiments 4.1 Data sets and evaluation criteria The annual OAEI campaign is an authoritative contest </context>
<context position="24403" citStr="Ngo and Bellahsene, 2012" startWordPosition="3991" endWordPosition="3994">asks directly. The precision is reduced when we use URL method, this may be due to the learned representations of entities are too general. In addition, in the parameter adjustment process, we try to make the F value maximization, but not to care about mapping precision. This is because we usually compare the performance of the systems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptions of a class or a property into three different parts). </context>
<context position="26805" citStr="Ngo and Bellahsene, 2012" startWordPosition="4369" endWordPosition="4372">res higher than 0.7, so the aggregation methods are very effective in improving the performance of mapping approaches that rely on measuring multiple similarities. And finally, our Unsupervised Representation Learning (URL) method holds the highest F-measure both on the development dataset and on the testing datasets. 4.2.3 Evaluation for our structural method In this experiment, we compare our Similarity Propagation (SP) method to other structure based methods, that is, ADJACENTS and ASCOPATH in (Le et al., 2004); DSI and SSC in (Sunna and Cruz, 2007); Li’s SP (Li et al., 2009) and Ngo’s SP (Ngo and Bellahsene, 2012). We first use entity’s ID to compute the similarity between classes and properties to provide an unified initial similarity matrix as input (or initialization) for our SP and other structural methods. Then, a new similarity matrix will be created and updated by considering the initial similarities and different structure information. And finally, we extract the mappings from the newly created similarity matrix with the strategy described in section 2.3. Dev. Tes.1 Tes.2 Initial Matcher 0.616 0.524 0.523 ADJACENTS 0.622 0.569 0.570 ASCOPATH 0.604 0.540 0.552 DSI 0.641 0.576 0.575 SSC 0.642 0.5</context>
</contexts>
<marker>Ngo, Bellahsene, 2012</marker>
<rawString>Ngo, D. and Bellahsene, Z. (2012). Yam++: a multi-strategy based approach for ontology matching task. In Knowledge Engineering and Knowledge Management, pages 421–425. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pirr´o</author>
<author>D Talia</author>
</authors>
<title>Ufome: An ontology mapping system with strategy prediction capabilities.</title>
<date>2010</date>
<journal>Data &amp; Knowledge Engineering,</journal>
<volume>69</volume>
<issue>5</issue>
<marker>Pirr´o, Talia, 2010</marker>
<rawString>Pirr´o, G. and Talia, D. (2010). Ufome: An ontology mapping system with strategy prediction capabilities. Data &amp; Knowledge Engineering, 69(5):444–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Qu</author>
<author>W Hu</author>
<author>G Cheng</author>
</authors>
<title>Constructing virtual documents for ontology matching.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>23--31</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24346" citStr="Qu et al., 2006" startWordPosition="3982" endWordPosition="3986">opment dataset and then apply them on the test tasks directly. The precision is reduced when we use URL method, this may be due to the learned representations of entities are too general. In addition, in the parameter adjustment process, we try to make the F value maximization, but not to care about mapping precision. This is because we usually compare the performance of the systems based on their matching F values. 4.2.2 Comparison with aggregation methods Aggregating different similarities is pervasive in ontology matching systems that contain multiple single matchers, for example, FalconAO(Qu et al., 2006), RiMOM(Li et al., 2009), YAM++(Ngo and Bellahsene, 2012), etc. Since our representation learning method also combines all descriptions of an entity together in an unsu2424 pervised way, we compare it with previous unsupervised aggregation strategies, that is, Max, Average, Sigmoid, Weighted(Cruz et al., 2010) and Harmony(Mao et al., 2008, 2010). As the work in (Mao et al., 2010; Ngo and Bellahsene, 2012), we first define three context profiles including individual profile, semantic profile and external profile for each class and property (this equivalent to divide the collection of descriptio</context>
</contexts>
<marker>Qu, Hu, Cheng, 2006</marker>
<rawString>Qu, Y., Hu, W., and Cheng, G. (2006). Constructing virtual documents for ontology matching. In Proceedings of the 15th international conference on World Wide Web, pages 23–31. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Shvaiko</author>
<author>J Euzenat</author>
</authors>
<title>Ontology matching: state of the art and future challenges. Knowledge and Data Engineering,</title>
<date>2013</date>
<journal>IEEE Transactions on,</journal>
<pages>25--1</pages>
<contexts>
<context position="3074" citStr="Shvaiko and Euzenat, 2013" startWordPosition="448" endWordPosition="451">oded information (e.g. ontologies, RDF(S)-data, linked data, etc.), which constitutes a valuable source of semantics. However, extending the state-of-the-art natural language applications to use these resources is not a trivial task mainly due to the heterogeneity and the ambiguity of the schemes adopted by the different resources of the Semantic Web. How to utilize these resources in NLP tasks comprehensively rather than choose just one of them has attracted much attention in recent years. An effective solution to the ontology heterogeneity problem is ontology matching (Euzenat et al., 2007; Shvaiko and Euzenat, 2013), whose main task is to establish semantic correspondences between entities (i.e., classes, properties or instances) from different ontologies. Ontology matching is usually done by measuring the similarity between two entities from two different ontologies. To effectively calculate the similarities, almost all types of descriptions of an entity should be used. In previous works, given the different nature of different kinds of descriptions, similarities are normally measured separately with different methods and then aggregated with some kind of combination strategy to compute the final simila</context>
<context position="30815" citStr="Shvaiko and Euzenat, 2013" startWordPosition="5035" endWordPosition="5038">om Benchmark dataset published in OAEI 2009 are used to generate training data to train a decision tree classifier. And in the classifying phase, each pair of elements from two to-be-matched ontologies is predicted as matched or not according to its attributes. However, ERSOM is an unsupervised approach, but it does not exclude using external resources and training data to help learning the representations of entities and provide the initial similarity matrix for the SP method to further improve the performance. 5 Related work There are many studies on Ontology Matching (Euzenat et al., 2007; Shvaiko and Euzenat, 2013). Currently, almost all ontology matching systems exploit various kinds of information provided in Figure 4: Comparison with other OAEI systems. ontologies to get better performance. To aggregate the different similarity matrixes, various approaches have been proposed. Pirr´o and Talia (2010) is a generic schema matching system. It exploits Max, Min, Average and Weighted strategies for the combination. The weighted method assigns a relative weight to each similarity matrix, and calculates a weighted sum of similarity for all similarity matrixes. The Average method is a special case of Weighted</context>
</contexts>
<marker>Shvaiko, Euzenat, 2013</marker>
<rawString>Shvaiko, P. and Euzenat, J. (2013). Ontology matching: state of the art and future challenges. Knowledge and Data Engineering, IEEE Transactions on, 25(1):158–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Spiliopoulos</author>
<author>G A Vouros</author>
<author>V Karkaletsis</author>
</authors>
<title>On the discovery of subsumption relations for the alignment of ontologies. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2010</date>
<contexts>
<context position="32641" citStr="Spiliopoulos et al., 2010" startWordPosition="5314" endWordPosition="5317">d sum strategy that adjusts weights empirically, or based on some static rules. This approach cannot automatically combine different matchers in various matching tasks. There are several works which exploit the supervised machine learning techniques for ontology matching. Eckert et al. (2009), string-based, linguistic and structural measures (in total 23 features) were used as input to train a SVM classifier to align ontologies. CSR (Classificationbased learning of Subsumption Relations) is a generic method for automatic ontology matching between concepts based on supervised machine learning (Spiliopoulos et al., 2010). It specifically focusses on discovering subsumption correspon2426 dences. SMB (Schema Matcher Boosting) is an approach to combining matchers into ensembles (Gal, 2011). It is based on a machine learning technique called boosting, that is able to select (presumably the most appropriate) matchers that participate in an ensemble. The difference of our work is that the textual descriptions are not been directly used to measure the similarities between entities. We learn a representation for each ontology entity in an unsupervised way to capture the interactions among the descriptions, which avoi</context>
</contexts>
<marker>Spiliopoulos, Vouros, Karkaletsis, 2010</marker>
<rawString>Spiliopoulos, V., Vouros, G. A., and Karkaletsis, V. (2010). On the discovery of subsumption relations for the alignment of ontologies. Web Semantics: Science, Services and Agents on the World Wide Web, 8(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sunna</author>
<author>I F Cruz</author>
</authors>
<title>Structure-based methods to enhance geospatial ontology alignment.</title>
<date>2007</date>
<booktitle>In GeoSpatial Semantics,</booktitle>
<pages>82--97</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5208" citStr="Sunna and Cruz, 2007" startWordPosition="773" endWordPosition="776">property is usually restricted by its domain and range. These potential correlations of the descriptions are very important to measure the similarity between entities since they can be treated as some potential features describing an entity. Second, it is difficult to estimate how many and which kind of single similarities are needed for an aggregation method to get a satisfactory result. On the other hand, in order to find more mappings, many structural ontology matching methods are proposed. To the best of our knowledge, previous structural methods are either local methods (Le et al., 2004; Sunna and Cruz, 2007) or global (i.e. iterative) methods but only use part of the structure information of the ontology (Li et al., 2009; Ngo and Bellahsene, 2012). For example, the ontology matching system YAM++ (Ngo and Bellahsene, 2012) utilizes a global structural method but it only uses the structure information of classes and properties to create the propagation graph to find mappings between classes and properties. A large amount of instances and their relations to classes and properties in the ontology haven’t been exploited in this system. To overcome the existing limitations, we propose in this paper a r</context>
<context position="26738" citStr="Sunna and Cruz, 2007" startWordPosition="4356" endWordPosition="4359">lly boosted by aggregation methods and they all achieve F-measures higher than 0.7, so the aggregation methods are very effective in improving the performance of mapping approaches that rely on measuring multiple similarities. And finally, our Unsupervised Representation Learning (URL) method holds the highest F-measure both on the development dataset and on the testing datasets. 4.2.3 Evaluation for our structural method In this experiment, we compare our Similarity Propagation (SP) method to other structure based methods, that is, ADJACENTS and ASCOPATH in (Le et al., 2004); DSI and SSC in (Sunna and Cruz, 2007); Li’s SP (Li et al., 2009) and Ngo’s SP (Ngo and Bellahsene, 2012). We first use entity’s ID to compute the similarity between classes and properties to provide an unified initial similarity matrix as input (or initialization) for our SP and other structural methods. Then, a new similarity matrix will be created and updated by considering the initial similarities and different structure information. And finally, we extract the mappings from the newly created similarity matrix with the strategy described in section 2.3. Dev. Tes.1 Tes.2 Initial Matcher 0.616 0.524 0.523 ADJACENTS 0.622 0.569 0</context>
</contexts>
<marker>Sunna, Cruz, 2007</marker>
<rawString>Sunna, W. and Cruz, I. F. (2007). Structure-based methods to enhance geospatial ontology alignment. In GeoSpatial Semantics, pages 82–97. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vincent</author>
<author>H Larochelle</author>
<author>I Lajoie</author>
<author>Y Bengio</author>
<author>P-A Manzagol</author>
</authors>
<title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--3371</pages>
<contexts>
<context position="13325" citStr="Vincent et al. (2010)" startWordPosition="2125" endWordPosition="2128">parameter, original k · kF is the Frobenius norm, β controls the weight of the sparsity penalty term, H is the set of hidden units, and ρ is the sparsity parameter. Formally, KL (ρkbρh) = ρ log �yh ρ + (1− ρ) log 1−ρ 1−�yh is the Kullback-Leibler (KL) divergence between a Bernoulli random variable with mean ρ and a Bernoulli random variable with mean byh. Figure 3: Learning higher level representations. 3.1.3 Learning higher level representations The auto-encoder which only has one hidden layer may not be enough to learn the complex interactions between input features. Inspired by the work of Vincent et al. (2010) and He et al. (2013), we build multi-layer model to learn more abstract entity representations. To achieve this, we repeatedly stack new sparse auto-encoder on top of the previously learned h(x) (i.e., the higher level representations are formed by combination of lower level representations). This model is called Stacked Auto-Encoder (SAE) by Bengio et al. (2007). In this way, when we input the binary term vector to the network, we can get its abstract representations in different levels. In other words, with the layer-by-layer learning, we obtain different levels of representations. The top-</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wang</author>
<author>B Xu</author>
</authors>
<title>Lily: the results for the ontology alignment contest oaei</title>
<date>2007</date>
<booktitle>In Proceedings of the Second International Workshop on Ontology Matching,</booktitle>
<pages>179--187</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="18571" citStr="Wang and Xu, 2007" startWordPosition="3011" endWordPosition="3014">terations, respectively. cp() is the function to compute the similarities propagated from the adjacent node σij connected to node (x, y) in the (i + 1)th iteration. And wj is the weight of edge between the node (x, y) and its jth neighboring node. During each iteration in the final step of Algorithm 1, only the similarity value between two entities in the node will be updated. At the end of each iteration, all similarity values are normalized by a Normalized function to all in range [0, 1]. The iteration stops after a predefined number of steps. 3.3 Mapping selection Similar with the work in (Wang and Xu, 2007; Huang et al., 2007; Ngo and Bellahsene, 2012), we use the Stable Marriage (SM) algorithm (Melnik et al., 2002) to choose the 1:1 mappings from the M rows and N columns similarity matrix, where M and N is the number of classes and properties in ontologies O1 and O2, respectively. In addition, before we run the SM algorithm we set the value of cell [i, j] of the similarity matrix to zero if i and j correspond to different types of entities. Thus, we remove lots of redundant data and only find the mappings between classes or properties. 4 Experiments 4.1 Data sets and evaluation criteria The an</context>
</contexts>
<marker>Wang, Xu, 2007</marker>
<rawString>Wang, P. and Xu, B. (2007). Lily: the results for the ontology alignment contest oaei 2007. In Proceedings of the Second International Workshop on Ontology Matching, pages 179–187. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>J Berant</author>
<author>B Van Durme</author>
</authors>
<title>Freebase qa: Information extraction or semantic parsing? ACL</title>
<date>2014</date>
<pages>82</pages>
<marker>Yao, Berant, Van Durme, 2014</marker>
<rawString>Yao, X., Berant, J., and Van Durme, B. (2014). Freebase qa: Information extraction or semantic parsing? ACL 2014, page 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>B Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Yao, X. and Van Durme, B. (2014). Information extraction over structured data: Question answering with freebase. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
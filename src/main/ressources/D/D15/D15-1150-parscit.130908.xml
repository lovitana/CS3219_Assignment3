<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<title confidence="0.96659">
Part-of-speech Taggers for Low-resource Languages using CCA Features
</title>
<author confidence="0.997561">
Young-Bum Kim† Benjamin Snyder$ Ruhi Sarikaya†
</author>
<affiliation confidence="0.99436">
†Microsoft Corporation, Redmond, WA
$University of Wisconsin-Madison, Madison, WI
</affiliation>
<email confidence="0.98735">
{ybkim, ruhi.sarikaya}@microsoft.com
bsnyder@cs.wisc.edu
</email>
<sectionHeader confidence="0.994738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765333333334">
In this paper, we address the challenge
of creating accurate and robust part-
of-speech taggers for low-resource lan-
guages. We propose a method that lever-
ages existing parallel data between the tar-
get language and a large set of resource-
rich languages without ancillary resources
such as tag dictionaries. Crucially, we
use CCA to induce latent word represen-
tations that incorporate cross-genre distri-
butional cues, as well as projected tags
from a full array of resource-rich lan-
guages. We develop a probability-based
confidence model to identify words with
highly likely tag projections and use these
words to train a multi-class SVM using
the CCA features. Our method yields
average performance of 85% accuracy
for languages with almost no resources,
outperforming a state-of-the-art partially-
observed CRF model.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983647058824">
We address the challenge of creating accurate
and robust part-of-speech taggers for low-resource
languages. We aim to apply our methods to the
hundreds, and potentially thousands, of languages
with meager electronic resources. We do not as-
sume the existence of a tag dictionary, or any other
sort of prior knowledge of the target language. In-
stead, we base our methods entirely on the exis-
tence of parallel data between the target language
and a set of resource-rich languages.
Fortunately, such parallel data exists for just
about every written language, in the form of Bible
translations. Around 2,500 languages have at least
partial Bible translations, and somewhere between
500 and 1,000 languages have complete transla-
tions. We have collected such electronic Bible
translations for 650 languages. Figure 1 breaks
down the number of languages in our collection
according to their token count. The majority of our
languages have at least 200,000 tokens of Bible
translations.
While previous studies (T¨ackstr¨om et al., 2013;
Ganchev and Das, 2013) have addressed this gen-
eral setting, they have typically assumed the exis-
tence of a partial tag dictionary as well as large
quantities of non-parallel data in the target lan-
guage. These assumptions are quite reasonable for
the dozen most popular languages in the world, but
are inadequate for the creation of a truly world-
wide repository of NLP tools and linguistic data.
In fact, we argue that such ancillary sources of
information are not really necessary once we take
into account the vastly multilingual nature of our
parallel data. Annotations projected from individ-
ual resource-rich languages are often noisy and
unreliable, due to systematic differences between
the languages in question, as well as word align-
ment errors. We can thus think of these languages
as very lazy and unreliable annotators of our tar-
get language. Despite their incompetence, as the
number of such annotators increases, their com-
bined efforts converge upon the truth, as idiosyn-
cratic biases and random noise are washed away.
Our assumption throughout will be that we have
in our possession a single multilingual corpus
(the Bible) consisting of about 200,000 tokens
for several hundred languages languages, as well
as reasonably accurate POS taggers for about ten
“resource-rich” languages. We will tag the Bible
data for the resource-rich languages, word-align
them to one another, and also word-align them to
</bodyText>
<page confidence="0.942238">
1292
</page>
<note confidence="0.9854045">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.996542111111111">
the remaining several hundred target languages.
Of course, our goal is not to produce a tagger
restricted to the Biblical lexicon. We therefore
assume a small unannotated monolingual sample
of the target language in an entirely unrelated
genre (e.g. newswire). We use this sample trans-
ductively to adapt our learned taggers from the
Biblical genre. In our experiments, we use the
CoNLL 2006 and 2007 shared-task test data for
this purpose. Of course tagged data does not exist
for truly resource-poor languages, so we evaluate
our methodology on the resource-rich languages.
Each such language takes a turn playing the role
of the target language for testing purposes.
The goal of the paper is to introduce a gen-
eral “recipe” for successful cross-lingual induction
of accurate taggers using meager resources. We
faced three major technical challenges:
</bodyText>
<listItem confidence="0.957987111111111">
• First, word alignments across languages are
incomplete, and often do not preserve part-
of-speech due to language differences.
• Second, when using multiple resource-rich
languages, we need to resolve conflicting
projections.
• Third, the parallel data at our disposal is of an
idiosyncratic genre (the Bible), and we wish
to induce a general-purpose tagger.
</listItem>
<bodyText confidence="0.999711526315789">
To address these challenges, we forgo the typi-
cal sequence-based learning technique of HMM’s
and CRF’s and instead adopt an instance-learning
approach using latent distributional features. To
induce these features, we introduced a new method
using Canonical Correlation Analysis (CCA) to
generalize the aligned information to new words.
This method views each word position as consist-
ing of three fundamental views: (1) the token view
(word context), (2) the type view, and (3) the pro-
jected tags in the local vicinity. We perform a
CCA to induce latent continuous vector represen-
tations of each view that maximizes their correla-
tions to one another. On the test data, a simple
multi-class classifier then suffices to predict accu-
rate tags, even for novel words. This approach out-
perform a state-of-the-art baseline (T¨ackstr¨om et
al., 2013) to achieve average tag accuracy of 85%
on newswire text.
</bodyText>
<figureCaption confidence="0.707085">
Figure 1: The breakdown of languages by the
</figureCaption>
<bodyText confidence="0.9722845">
number of tokens in their available Bible trans-
lations. The horizontal axis gives the number of
tokens, and the vertical axis gives the number of
languages in each token range.
</bodyText>
<sectionHeader confidence="0.999938" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.993645">
2.1 Multilingual Projection
</subsectionHeader>
<bodyText confidence="0.999673366666667">
The idea of projecting annotated resources across
languages using parallel data was first proposed
by Yarowsky et al. (2001). This early work
recognized the noisy nature of automatic word
alignments and engineered smoothing and filter-
ing methods to mitigate the effects of cross- lin-
gual variation and alignment errors. More recent
work in this vein has dealt with this by instead
transferring information at the word type or model
structure level, rather than on a token-by-token ba-
sis (Das and Petrov, 2011; Durrett et al., 2012).
Current state-of-the-art results for indirectly su-
pervised POS performance use a combination of
token constraints as well as type constraints mined
from Wiktionary (Li et al., 2012; T¨ackstr¨om et al.,
2013; Ganchev and Das, 2013). As we argued
above, the only widely available source of infor-
mation for most low-resource languages is in fact
their Bible translation. Perhaps surprisingly, our
experiments show that this data source suffices to
achieve state-of-the-art results.
Several previous authors have considered the
advantage of using more than one resource-rich
language to alleviate alignment noise.
Fossum and Abney (2005) found that using two
source languages project-sources gave better re-
sults than simply using more data from one lan-
guage. McDonald et al. (2011) also found advan-
tages to using multiple language sources for pro-
jecting parsing constraints. In more of an unsu-
</bodyText>
<page confidence="0.970875">
1293
</page>
<bodyText confidence="0.99995525">
pervised context (but using small tag dictionaries),
adding more languages to the mix has been shown
to improve part-of-speech performance across all
component languages (Naseem et al., 2009).
In our own previous multilingual work, we have
developed the idea that supervised knowledge of
some number of languages can help guide the un-
supervised induction of linguistic structure, even
in the absence of parallel text (Kim et al., 2011;
Kim and Snyder, 2012; Kim and Snyder, 2013a;
Kim and Snyder, 2013b). We have showed that
cross-lingual supervised learning leads to signif-
icant performance gains over monolingual mod-
els. We point out that the previous tasks have con-
sidered as word-level structural analyses and our
present case as a sentence-level analysis.
</bodyText>
<subsectionHeader confidence="0.998146">
2.2 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999954583333333">
Most of the papers surveyed above rely on auto-
matic word alignments to guide the cross-lingual
transfer of information. Given our desire to use
highly multilingual information to improve pro-
jection accuracy, the question of word alignment
performance becomes crucial. Our hypothesis is
that multiple language projections are beneficial
not only in weeding out random errors and id-
iosyncratic variations, but also in improving the
linguistic consistency of the alignments them-
selves. Instead of simply aligning each source lan-
guage to the target language in isolation, we will
instead use a confidence model to synthesize in-
formation from multiple sources.
While there are not many well-known papers
that have explored word alignment on a multilin-
gual scale1, there have been related efforts to sym-
metrize bilingual alignment models, using a vari-
ety of techniques ranging from modifications of
EM (Liang et al., 2006), posterior-regularized ob-
jective function (Ganchev et al., 2010), and by
considering relaxations of the hard combinato-
rial assignment problem (DeNero and Macherey,
2011).
</bodyText>
<subsectionHeader confidence="0.998236">
2.3 Canonical Correlation Analysis (CCA)
</subsectionHeader>
<bodyText confidence="0.9989904">
Our method for generalizing the projections to
unseen words and contexts is based on Canoni-
cal Correlation Analysis (CCA), a dimensionality
reduction technique first introduced by Hotelling
(1936). The key idea is to consider two groups
</bodyText>
<footnote confidence="0.878074">
1Mayer and Cysouw (2012) used multilingual word align-
ment to compare languages
</footnote>
<bodyText confidence="0.999882388888889">
of random variables with corresponding observa-
tions and to find linear subspaces with highest cor-
relation between the two views. This can be seen
as a kind of supervised version of Principal Com-
ponents Analysis (PCA), where each view is pro-
viding supervision for the other. In fact, it can be
shown that CCA directly generalizes both multi-
ple linear regression and Fisher’s Latent Discrimi-
native Analysis (LDA) (Glahn, 1968).
From a learning theory perspective, CCA is in-
teresting in that it allows us to prove regret-based
learning bounds that depend on the “intrinsic” di-
mensionality of the problem rather than the ap-
parent dimensionality (Kakade and Foster, 2007).
This seems especially relevant to natural language
processing scenarios, where the ambient dimen-
sion is extremely large and sparse, but reductions
to dense lower-dimensional spaces may preserve
nearly all the relevant semantic and syntactic in-
formation. In fact, CCA has recently been adapted
to learning latent word representations in an inter-
esting way: by dividing each word position into
a token view (which only sees surrounding con-
text) and a type view (which only sees the word
itself) and performing a CCA between these two
views (Dhillon et al., 2012; Kim et al., 2014;
Stratos et al., 2014; Stratos et al., 2015; Kim et
al., 2015c). CCA is also used to induce label rep-
resentations (Kim et al., 2015d) and lexicon repre-
sentations (Kim et al., 2015b).
Our technique will extend this idea by addition-
ally considering a third projected tag view. Cru-
cially, it is this view which pushes the latent repre-
sentations into coherent part-of-speech categories,
allowing us to simply apply multi-class SVM for
unseen words in our test set.
</bodyText>
<sectionHeader confidence="0.9087595" genericHeader="method">
3 Tag projection from resource-rich
languages
</sectionHeader>
<bodyText confidence="0.999909727272727">
In this section, we describe two methods for incor-
porating transferred tags from resource-rich lan-
guages: sequence-based learning (T¨ackstr¨om et
al., 2013; Kim et al., 2015a) and instance-based
learning. In the former, the transferred tags are
used to train a partially-observed CRF (PO-CRF)
by maximizing the probability of a constrained lat-
tice. In contrast, instance-based learning views
each word token as an independent classifica-
tion task, but uses latent distributional information
gleaned from surrounding words as features.
</bodyText>
<page confidence="0.963022">
1294
</page>
<subsectionHeader confidence="0.9469585">
3.1 A sequence learning example of partially
observed CRF (PO-CRF)
</subsectionHeader>
<bodyText confidence="0.998426">
A first-order CRF parametrized by θ ∈ Rd de-
fines a conditional probability of a label sequence
</bodyText>
<equation confidence="0.95269">
y = y1 ... yn given an observation sequence x =
x1 ... xn as follows:
exp(θ&gt;Φ(x, y))
pθ(y|x) = Ey/∈Y(x) exp(θ&gt;Φ(x,y0))
</equation>
<bodyText confidence="0.996835333333333">
where Y(x) is the set of all possible label se-
quences for x and Φ(x, y) ∈ Rd is a global fea-
ture function that decomposes into local feature
functions Φ(x, y) = En j=1 φ(x, j, yj−1, yj) by
the first-order Markovian assumption. Given fully
labeled sequences {(x(i), y(i))}Ni=1, the standard
training method is to find θ that maximizes the log
likelihood of the label sequences under the model
with l2-regularization:
</bodyText>
<equation confidence="0.913715">
log pθ(y(i)|x(i)) − λ2 ||θ||2
</equation>
<bodyText confidence="0.9326286">
We used an l2 penalty weight λ of 1. Unfortu-
nately, in our setting, we do not have fully labeled
sequences. Instead, for each token xj in sequence
x1 ... xn we have the following two sources of la-
bel information:
</bodyText>
<listItem confidence="0.9872448">
• A set of allowed label types Y(xj). (Label
dictionary, type constraints)
• Labels ˜yj transferred from resource rich
languages. (transferred labels, token con-
straints)
</listItem>
<bodyText confidence="0.8753208">
Following previous work of T¨ackstr¨om et al.
(2013), we first define a constrained lattice
Y(x, ˜y) = Y(x1, ˜y1) × ... × Y(xn, ˜yn) where at
each position j a set of allowed label types is given
as:
</bodyText>
<equation confidence="0.876195">
Y(xj, yj) = { Y(xj) otherwise
en
</equation>
<bodyText confidence="0.976816333333333">
And then we can define a conditional probabil-
ity over label lattices for a given observation se-
quence x:
</bodyText>
<equation confidence="0.9550485">
pθ(Y(x, ˜y)|x) = � pθ(y|x)
y∈Y(x,˜y)
</equation>
<bodyText confidence="0.999301">
Given a label dictionary Y(xj) for every token
type xj and training sequences {(x(i), ˜y(i))}Ni=1
where ˜y(i) is transferred labels for x(i) and, the
new training method is to find θ that maximizes
the log likelihood of the label lattices:
</bodyText>
<equation confidence="0.99627525">
N
θ∗ = arg max log pθ(Y(x(i), ˜y(i))|x(i))
θ∈Rd i=1
λ2 ||θ||2
</equation>
<bodyText confidence="0.99873625">
Since this objective is non-convex, we find a
local optimum with a gradient-based algorithm.
The gradient of this objective at each example
(x(i), ˜y(i)) takes an intuitive form:
</bodyText>
<equation confidence="0.9994565">
∂θ log pθ(Y(x(i), ˜y(i))|x(i)) − λ
∂ 2 ||θ||2
�= pθ(y|x(i))Φ(x(i), y)
y∈Y(x(z),˜y)
�− pθ(y|x(i))Φ(x(i), y) − λθ
y∈Y(x(z))
</equation>
<bodyText confidence="0.999922538461538">
This is the same as the standard CRF train-
ing except the first term where the gold features
Φ(x(i), y(i)) are replaced by the expected value of
features in the constrained lattice Y(x(i), ˜y).
An important distinction in our setting is that
our token and type constraints are generated by
only using the transferred tags whereas T¨ackstr¨om
et al. (2013) generate type constraints induced
from Wiktionary. Our setting is more realistic for
at least two reasons; 1) Wiktionary is not always
available. 2) transferable information is not lim-
ited, but Wiktionary is (e.g., semantic role and
named entity).
</bodyText>
<subsectionHeader confidence="0.998599">
3.2 Cross-lingual instance-based learning
</subsectionHeader>
<bodyText confidence="0.9998045">
The proposed method for cross-lingual instance-
based learning has three steps:
</bodyText>
<listItem confidence="0.975522571428572">
1. Select training tokens based on the confi-
dence of the projected tag information.
2. Induce distributional features over these
words that incorporate all projected tags.
3. Train a multi-class classifier with these in-
duced features to make local predictions for
individual tokens.
</listItem>
<bodyText confidence="0.995145">
We will describe each step below.
</bodyText>
<subsectionHeader confidence="0.959075">
3.2.1 Selecting training words
</subsectionHeader>
<bodyText confidence="0.999939333333333">
Since transferred tags are not always reliable, all
words in the parallel data are not necessary help-
ful in training. Since this method trains on words
</bodyText>
<equation confidence="0.99252125">
N
θ∗ = arg max
θ∈Rd i=1
−
</equation>
<page confidence="0.944212">
1295
</page>
<figureCaption confidence="0.9143675">
Figure 2: Graphical representation of the confi-
dence model. Unobserved variable y denotes the
</figureCaption>
<bodyText confidence="0.982770894736842">
true target-language tag for a token. Each of the
L resource-rich languages displays a project of y,
as y`, with an indicator variable z` determining the
fidelity of the projection.
instead of sequences, it is easy to discard words
which have unreliable or highly conflicting pro-
jections from different resource-rich languages.
To select our set of training tokens, we define a
simple probability-based confidence model, illus-
trated in Figure 2. Suppose we have L resource-
rich languages with alignments to the word in
question. If the true tag is y, we assume that the
projected tag for language ` will be identical to y
with probability 1 − E`, where E` is a language-
specific corruption probability. With probability
E`, the projection will instead be chosen randomly
(uniformly).
To make this explicit, we introduce a corruption
indicator variable z` with:
</bodyText>
<equation confidence="0.99866">
P(z` = 1) = E`
</equation>
<bodyText confidence="0.9660785">
Given z`, the probability of the projected tag y` is
given by:
</bodyText>
<equation confidence="0.990606666666667">
1 if z = 0 and y = y`,
P(y`|y, z`) =
0 otherwise.
</equation>
<bodyText confidence="0.9999535">
where m is the total number of possible tags. We
can now compute a conditional distribution over
the unknown tag y, marginalizing out the unknown
corruption variables for each language:
</bodyText>
<equation confidence="0.9981396">
p(y|y1,...,yn)
Qn ��� m + (1 − �`)δ(y,y`)�
`=1
mn−1 P Y Qn=1 L��
1m + (1 − E`)δ(y,, y`)]
</equation>
<bodyText confidence="0.999921">
where Y is all possible tags. For simplicity, we
simply set all E` to 0.1 and use y as a training label
when the conditional probability of the most likely
value is greater than 0.9.
</bodyText>
<subsectionHeader confidence="0.878987">
3.2.2 Inducing distributional features
</subsectionHeader>
<bodyText confidence="0.9999805">
In this section we discuss our approach for deriv-
ing latent distributional features. Canonical Cor-
relation Analysis (CCA) is a general method for
inducing new representations for a pair of vari-
ables X and Y (Hotelling, 1936). To derive word
embeddings using CCA, a natural approach is to
define X to represent a word and Y to represent
the relevant information about a word, typically
context words (Dhillon et al., 2012; Kim et al.,
2015c). When they are defined as one-hot encod-
ings, the CCA computation reduces to performing
an SVD of the matrix Ω where each entry is
</bodyText>
<equation confidence="0.997088">
count(w, c)
Ωw,c =
p
count(w)count(c)
</equation>
<bodyText confidence="0.9956775">
where count(w, c) denotes co-occurrence count
of word w and context c in the given corpus,
</bodyText>
<equation confidence="0.985994">
count(w) = Pc count(w, c), and count(c) =
P
w count(w, c).
</equation>
<bodyText confidence="0.9999776">
The resulting word representation is given by
UTX where U is a matrix of the scaled left singu-
lar vectors of Ω (See Figure 3). In our work, we
use a slightly modified version of this definition by
taking square-root of each count:
</bodyText>
<equation confidence="0.930923333333333">
√ count(w, c)1/2 Ωw,c =
p
count(w)1/2count(c)1/2
</equation>
<bodyText confidence="0.999592181818182">
This has an effect of stabilizing the variance of
each term in the matrix, leading to a more effi-
cient estimator. The square-root transformation
also transforms the distribution of the count data
to look more Gaussian (Bartlett, 1936): since an
interpretation of CCA is a latent-variable with
normal distributions (Bach and Jordan, 2005), it
makes the data more suitable for CCA. It has been
observed in past works (e.g., Dhillon et al. (2012))
to significantly improve the quality of the resulting
representations.
</bodyText>
<subsectionHeader confidence="0.997092">
3.3 Feature Induction Algorithm
</subsectionHeader>
<bodyText confidence="0.9998705">
We now describe our algorithm for inducing la-
tent distributional features both on the multilin-
gual parallel corpus, as well as the monolingual,
newswire test data. This algorithm is described
</bodyText>
<equation confidence="0.583326">
⎧
⎨⎪
⎪⎩
1
m
if z = 1,
</equation>
<page confidence="0.9197">
1296
</page>
<bodyText confidence="0.999949482758621">
in detail in Figure 4. The key idea is to per-
form two CCA steps. The first step incorporates
word-distributional information over both the mul-
tilingual corpus (the Bible) as well as the exter-
nal domain monolingual corpus (CONLL data)2.
This provides us with word representations that
are general, and not overly specific to any single
genre. However, it does not incorporate any pro-
jected tag information. We truncate this first SVD
to the first 100 dimensions3.
After this CCA step is performed, we then re-
place the words in the multilingual Bible data with
their latent representations. We then perform a
second CCA between these word representations
and vectors representing the projected tags from
all resource-rich languages. This step effectively
adapts the first latent representation to the infor-
mation contained in the tag projections. We trun-
cate this second SVD to the first 50 dimensions.
We now have word embeddings that can be ap-
plied to any corpus, and are designed to maximize
correlation both with typical surrounding word
context, as well as typical projected tag context.
These embeddings serve as our primary feature
vectors for training the POS classifier (described
in the next section). We concatenate this primary
feature vector with the embeddings of the previous
and subsequent words, in order to provide context-
sensitive POS predictions.
</bodyText>
<subsectionHeader confidence="0.852151">
3.3.1 Multi-class classifier
</subsectionHeader>
<bodyText confidence="0.999914111111111">
To train our POS tagger, we use a linear multi-
class SVM (Crammer and Singer, 2002). It has
a parameter wy ∈ Rd for every tag y ∈ T
and defines a linear score function s(x, j, y) :=
wy Φ(x, j). Given any sentence x and a position
j, it predicts arg maxy∈T s(x, j, y) as the tag of
xj. We use the implementation of Fan et al. (2008)
with the default hyperparameter configurations for
training.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996842">
4.1 Datasets and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999978">
There are more than 4,000 living languages in the
world, and one of the most prevalently translated
books is the Bible. We now describe the Bible
dataset we collected.
</bodyText>
<footnote confidence="0.99864175">
2For context words, we use 5 words before and after the
word occurrence.
3Embedding dimension was empirically determined by
the singular values.
</footnote>
<figureCaption confidence="0.9977785">
Figure 3: Algorithm for deriving CCA projections
from samples of two variables.
</figureCaption>
<table confidence="0.999761909090909">
Lang Tagger Accuracy
BG Treetagger 0.9909
CS Hunpos 0.8969
DA Hunpos 0.9756
DE Hunpos 0.9855
EN Hunpos 0.9854
ES Treetagger 0.8785
IT Treetagger 0.9059
NL Treetagger 0.8781
PT Hunpos 0.9770
AVG - 0.9415
</table>
<tableCaption confidence="0.999883">
Table 1: Tagger accuracy on CoNLL data.
</tableCaption>
<bodyText confidence="0.993811294117647">
We first collect 893 bible volumes span-
ning several hundred languages that are freely
available from three resources (www.bible.is,
www.crosswire.org, www.biblegateway.com) and
changed to UTF-8 format. The distribution of to-
ken in each bible in the unit of a language is in
Figure 1.
Note that the Bible scripts are not exactly trans-
lated by sentences but by verses. We thus assume
that each verse in a chapter has the same meaning
if the number of verses is exactly same in a same
chapter. We also assume that the whole chapters
have the same meaning if the number of chap-
ters in a book are exactly the same. In the same
manner, we also assume the volumes that have the
same number of chapters are the same. That is,
their volume size should be as similar as possible
</bodyText>
<figure confidence="0.988267142857143">
CCA-PROJ-SPARSE
Input: samples (x(1), y(1)) ... (x(n), y(n)) E {0, 1}d x
{0, 1}d&apos;, dimension k
Output: projections A E Rdxk and B E Rd&apos;xk
• Calculate B E Rdxd&apos;, u E Rd, and v E Rd&apos;:
Bi,j = n [[x(l) i= 1]][[y(l)
l=1 j = 1]]
ui = n [[x(l) n [[y(l)
l=1 i = 1]] vi = l=1 i = 1]]
• Define Ωˆ = diag(u)−1/2Bdiag(v)−1/2.
• Calculate rank-k SVD ˆΩ. Let U E Rdxk (V E
Rd&apos;xk) be a matrix of the left (right) singular vector
corresponding to the largest k singular values.
• Let A = diag(u)−1/2U and B = diag(v)−1/2V .
</figure>
<page confidence="0.800449">
1297
</page>
<bodyText confidence="0.581689">
Input:
</bodyText>
<listItem confidence="0.9451083">
• N “labeled” tokens in the Bible domain: word w(i) E
V, corresponding context C(w(i)) ⊂ V and (projected)
tag set P(i) ⊂ T for i = 1 ... N
• N&apos; tokens in data in the test domain: word v(i) E V&apos;
and corresponding context C(v(i)) ⊂ V&apos; for i =
1 ... N&apos;
• CCA dimensions k1, k2
Output: embedding e(w) E Rk2 for each word w E V U V&apos;
1. Combine the observed tokens and their context from
the Bible and data in the test domain:
</listItem>
<equation confidence="0.996689">
(w : w E
(w(i))N
i=1 U (v(i))N&apos;i=1)
C1 := (C(w) : w E (w(i))Ni= 1 U (v(i))N&apos;1)
</equation>
<listItem confidence="0.986310428571429">
2. Perform rank-k1 CCA-PROJ-SPARSE on (W1, C1)
to derive a word projection matrix ΦW1 and a context
projection matrix ΦC1.
3. Project all word examples in the Bible domain using
ΦW1. Denote these projected words and the corre-
sponding projected tag sets from all resource-rich lan-
guages by
</listItem>
<equation confidence="0.9858515">
( )
W2 := ΦW1(w(i)) : i = 1 ... N
( )
P2 := P(i) : i = 1 ... N
</equation>
<listItem confidence="0.946949">
4. Perform rank-k2 CCA on (W2, P2) to derive a word
projection matrix ΦW2 and a tag projection matrix
ΦP2.
5. Set the embedding e(w) for each word w E V U V&apos; as
</listItem>
<equation confidence="0.500402">
e(w) = ΦW2(ΦW1(w))
</equation>
<figureCaption confidence="0.950037333333333">
Figure 4: Algorithm for deriving word vectors for
the (unannotated) test data that use the projected
tags in the Bible data.
</figureCaption>
<bodyText confidence="0.998989466666667">
with the respect to the number of verses, chapters,
and books.
Based upon these assumptions, we choose the
best translation in a language based on a compar-
ison to a reference Bible, the Modern King James
Version (MKJV) in English. We choose the trans-
lation for each language that best matches this ref-
erence version in terms of chapter and verse num-
bering.
There are other factors considered if there are
more than one candidates satisfying this matching.
We focus on the contents of the bible such as the
publication time. For instance, 1599 Geneva Bible
in English contains old vocabulary with different
spelling systems, causing unexpected errors when
tagged by POS annotation tools. Also, some of
volumes such as Amplified Bible (AMP) contains
extraneous comments on verses themselves, caus-
ing errors for word alignments.
After the choice of the best volume, we finally
select the 10 resource rich languages4. The two
criteria to select resource rich languages are hav-
ing i) the matched bible scripts both on the Old and
New testament and ii) reliable parts-of-speech an-
notation tools. If these two requirements are satis-
fied, we can freely add more languages as resource
rich languages in the future research. We use Hun-
pos tagger for CS, DA, DE, EN, and PT, Treetag-
ger for BG, ES, IT, and NL, and Meltparser for
FR.
</bodyText>
<subsectionHeader confidence="0.999421">
4.2 Test Data
</subsectionHeader>
<bodyText confidence="0.995625666666667">
We use CoNLL parts-of-speech tagged data
(selected resource-rich languages), plus Basque
(EU), Hungarian (HU) and Turkish (TR)) as our
test data. It consists of 5,000-6,000 hand-labeled
tokens. The accuracy of each supervised tagger on
this data is about 94% on average. Since there is
no French tagged CoNLL data, we exclude French
on testing but still use it in Training. The accuracy
of each supervised tagger on this data is shown in
Table 1.
The tag definitions used in CoNLL data are not
exactly matched the ones used in the taggers when
converted to universal POS tags. For instance in
Spanish, we initially follow mapping of Petrov
et al. (2011) for CoNLL data. The ‘dp’ tag for
words sus, su, mi are mapped to DET but they
are mapped to PRON in the bible data because of
the Treetagger definitions. Whenever we find this
kind of issues, we analyze them and choose the
one of mappings for compatibility. For the ‘dp’
tag, we choose to map PRON.
</bodyText>
<subsectionHeader confidence="0.999455">
4.3 Alignments
</subsectionHeader>
<bodyText confidence="0.9999185">
We perform two kinds of alignments in our data
sets; (i) the verse alignment and (ii) the word
alignment. When the tagged bible volumes are
prepared, we align verses across all resource rich
languages. For verse alignments, we pre-process
to remove extraneous information such as in-
line reference (e.g. [REV 4:16]) and HTML
tags. These alignments between two languages
</bodyText>
<footnote confidence="0.976525">
4Bulgarian (BG), Czech(CS), Danish (DA), English(EN),
German (DE), French (FR), Spanish (ES), Italian (IT), Dutch
(NL), and Portuguese (PT)
</footnote>
<page confidence="0.994313">
1298
</page>
<bodyText confidence="0.99998125">
occurred only when volumes have the exact same
number of chapters and verses. For instance, Mark
must have 16 chapters and the first chapter of the
Mark must have 45 verses in our criteria. The cor-
rect number of chapters and verses are pre-defined
on MKJV volume, and the number of matched
verses on each volume is greater than 30,500.
After performing verse alignments, we then per-
form word alignments. The quality of tags in
resource poor languages is highly dependent on
the quality of word alignments because parts-of-
speech tags will be projected through this align-
ment path. First, we use GIZA++ for initial one-
to-many alignments and we symmetrize by taking
their intersection. This ensures that the resulting
alignments are of high quality.
</bodyText>
<subsectionHeader confidence="0.487384">
4.4 Results
</subsectionHeader>
<table confidence="0.997439692307692">
majority union confident
BG 0.8123 0.8167 0.8235
CS 0.8013 0.8094 0.8142
DA 0.8412 0.8497 0.8492
DE 0.8532 0.8611 0.8721
ES 0.8278 0.8345 0.8385
EU 0.8326 0.8413 0.8472
HU 0.7741 0.7789 0.7953
IT 0.8486 0.8445 0.8481
NL 0.7864 0.7876 0.7884
PT 0.8022 0.8081 0.8110
TR 0.6803 0.6739 0.6935
AVG 0.8055 0.8097 0.8165
</table>
<tableCaption confidence="0.8058675">
Table 2: Baseline model CONLL performance de-
pending on criterion for selecting tag projection.
</tableCaption>
<bodyText confidence="0.999851923076923">
In all experiments, we hold out the tags of the
test language. EU, HU and TR used projected tags
from 10 resource-rich languages, 9 resource-rich
languages are used for the remaining languages.
In our first experiment, we consider the state-of-
the-art PO-CRF baseline. This model trains a par-
tially observed CRF based on a single projected
tag for each token. We experiment with different
methods of choosing the projected tags. The re-
sults are shown in Table 2. The majority method is
to choose the most common tag from the projected
tags of the current token. We then experiment with
taking the union of all projected tags (i.e. only
constraining the lattice based on unanimity of the
resource-rich languages). Finally, we considered
choosing the high confidence tags, based on our
confidence model. The confident tags are defined
by a method described in Section 3.2.1 If this ratio
is greater than 0.9, we assume that this token has
high confidence. As the results indicate, this final
method yielded the best tagging performance on
the CONLL test data, achieving average accuracy
of 82%.
In the remaining experiments we will adopt the
confidence-based selection criterion for both the
baseline as well as our method.
</bodyText>
<table confidence="0.999605384615385">
PO-CRF CCA+SVM
BG 0.8450 0.8686
CS 0.8359 0.8442
DA 0.8727 0.8826
DE 0.8862 0.9025
ES 0.8523 0.8816
EU 0.8506 0.8927
HU 0.8461 0.8495
IT 0.8705 0.8911
NL 0.8115 0.8345
PT 0.8346 0.8410
TR 0.7064 0.7389
AVG 0.8375 0.8570
</table>
<tableCaption confidence="0.999927">
Table 3: Performance on multilingual Bible data
</tableCaption>
<bodyText confidence="0.99989419047619">
In order to isolate the errors due to projection
mismatch versus domain variation, we first test
both models on the Bible data itself. To do so,
we assume that the tags produced by the test-
language’s supervised tagger are in fact the ground
truth. This experiment allows us to compare to
tag projection models using (1) PO-CRF and (2)
CCA+SVM. Results are given in Table 3. Unsur-
prisingly, PO-CRF performs better on the multi-
lingual corpus than on the CONLL data, due to
the beneficial constraint of the projected tags. Per-
haps interestingly, the CCA+SVM method, which
is a simple instance-based classifier using cleverly
constructed features, outperforms the sequence la-
beller, achieving accuracy of nearly 86%5 .
In our third experiment we use CoNLL test data
and compare the PO-CRF models with different
settings. See Table 4. This experiment is to show
the effects of suffix and Brown cluster features on
PO-CRF to relieve the unseen words issue. We
also show that the more projecting languages are
</bodyText>
<footnote confidence="0.98889975">
5Note that some previous researches (Liang et al., 2008;
Wisniewski et al., 2014; Moore, 2014) also pointed out that
POS tagging does not necessarily require a sequence model
for strong performance.
</footnote>
<page confidence="0.970396">
1299
</page>
<table confidence="0.999892846153846">
1 lang (EN) (A) 9/10 langs (W) 9/10 langs (no S/C) 9/10 langs (A)
BG 0.7883 0.7144 0.8094 0.8478
CS 0.6601 0.5589 0.6535 0.7868
DA 0.7820 0.7765 0.8016 0.8227
DE 0.8323 0.6956 0.7589 0.8500
ES 0.7893 0.7608 0.8279 0.8665
EU 0.7764 0.7543 0.8035 0.8661
HU 0.6429 0.6378 0.7834 0.8119
IT 0.8444 0.7588 0.8136 0.8921
NL 0.7887 0.6825 0.7751 0.8214
PT 0.8476 0.7797 0.8464 0.8656
TR 0.6306 0.5727 0.6719 0.7143
AVG 0.7621 0.6993 0.7768 0.8314
</table>
<tableCaption confidence="0.997382">
Table 4: Accuracy of the PO-CRF models on CoNLL data. A, W, no S/C means: all, word, all but no
</tableCaption>
<bodyText confidence="0.992922724137931">
suffix and cluster features are used, respectively.
included the better the results gets.
For the features, we used word identity, suffixes
of up to length 3, Brown cluster and three indi-
cators of (1) capitalization for the first character,
(2) containing a hyphen or (3) a digit. Especially,
Brown clusters was induced from more than 2 mil-
lion line documents, making the setting unrealistic
for resource-poor language.
With just the word features, the averaged per-
formance is 0.6993 and other indicator features
increase the performance to 0.7768. Also note
that the suffix and Brown cluster features increase
the performance from 0.7768 to 0.8314. As re-
ported, PO-CRF mitigates the adverse effects of
the unseen word issues and almost meets the per-
formance in the previous experiment (0.8375) of
T¨ackstr¨om et al. (2013) by using these features.
In fourth and final experiment, we used the
same features for PO-CRF, with Brown clusters
induced on a realistically obtainable sized (3k)
corpus for a low resource language. We compare
directly to our CCA+SVM model (which does not
use Brown clustering features at all). We achieved
0.7983 on PO-CRF with all features and our cor-
responding model on CCA achieved about 0.8474,
shown in Table 5. As reported, our model outper-
forms the PO-CRF with the realistic settings for
resource poor languages.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9974265">
We addressed the challenge of POS tagging low-
resource languages. Our key idea is to use a mas-
sively multilingual corpus. Instead of relying on a
single resource-rich language, we leverage the full
</bodyText>
<table confidence="0.999210142857143">
PO-CRF CCA+SVM
3k Brown
BG 0.8318 0.8815
CS 0.7635 0.8232
DA 0.7335 0.8911
DE 0.8296 0.8543
ES 0.8319 0.8713
EU 0.8376 0.8734
HU 0.7817 0.8372
IT 0.8451 0.8474
NL 0.7626 0.8245
PT 0.8768 0.8823
TR 0.6874 0.7354
AVG 0.7983 0.8474
</table>
<tableCaption confidence="0.898861">
Table 5: Performances on our test data, CoNLL
document.
</tableCaption>
<bodyText confidence="0.999796642857143">
array of currently available POS taggers. This re-
moves alignment-mismatch noise and identifies a
subset of words with highly confident tags. We
then use a CCA procedure to induce latent fea-
ture representations across domains, incorporating
word contexts as well as projected tags. We then
train an SVM to predict tags.
Experimentally, we show that this procedure
yields accuracy of about 85% for languages with
nearly no resources available, beating a state-of-
the-art partially observed CRF formulation. In the
near future, this technique will enable us to re-
lease a suite of POS taggers for hundreds of low-
resource languages.
</bodyText>
<page confidence="0.986221">
1300
</page>
<sectionHeader confidence="0.977769" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990819638888888">
Francis R Bach and Michael I Jordan. 2005. A proba-
bilistic interpretation of canonical correlation analy-
sis.
MSo Bartlett. 1936. The square root transformation in
analysis of variance. Supplement to the Journal of
the Royal Statistical Society, pages 68–78.
Koby Crammer and Yoram Singer. 2002. On the learn-
ability and design of output codes for multiclass
problems. Machine Learning, 47(2-3):201–233.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 600–609. Association for Computational Lin-
guistics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 420–
429. Association for Computational Linguistics.
Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,
and Lyle H. Ungar. 2012. Two Step CCA: A
new spectral method for estimating vector models
of words. In Proceedings of the 29th International
Conference on Machine learning, ICML’12.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Victoria Fossum and Steven Abney. 2005. Automat-
ically inducing a part-of-speech tagger by project-
ing from multiple source languages across aligned
corpora. In Natural Language Processing–IJCNLP
2005, pages 862–873. Springer.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence mod-
els with posterior regularization. In Proceedings
of EMNLP. Association for Computational Linguis-
tics, October.
Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001–2049.
Harry R Glahn. 1968. Canonical correlation and
its relationship to discriminant analysis and multi-
ple regression. Journal of the atmospheric sciences,
25(1):23–31.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3-4):321–377.
Sham M Kakade and Dean P Foster. 2007. Multi-
view regression via canonical correlation analysis.
In Learning Theory, pages 82–96. Springer.
Young-Bum Kim and Benjamin Snyder. 2012. Uni-
versal grapheme-to-phoneme prediction over latin
alphabets. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 332–343. Association for Compu-
tational Linguistics.
Young-Bum Kim and Benjamin Snyder. 2013a. Opti-
mal data set selection: An application to grapheme-
to-phoneme conversion. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 1196–
1205. Association for Computational Linguistics.
Young-Bum Kim and Benjamin Snyder. 2013b. Unsu-
pervised consonant-vowel prediction over hundreds
of languages. In Proceedings of the Association
for Computational Linguistics (ACL), pages 1527–
1536. Association for Computational Linguistics.
Young-Bum Kim, Jo˜ao V Grac¸a, and Benjamin Snyder.
2011. Universal morphological analysis using struc-
tured nearest neighbor prediction. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 322–332.
Association for Computational Linguistics.
Young-Bum Kim, Heemoon Chae, Benjamin Snyder,
and Yu-Seop Kim. 2014. Training a korean srl sys-
tem with rich morphological features. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), pages 637–642. Association for Compu-
tational Linguistics.
Young-Bum Kim, Minwoo Jeong, Karl Stratos, and
Ruhi Sarikaya. 2015a. Weakly supervised slot
tagging with partially labeled sequences from web
search click logs. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 84–92. Associ-
ation for Computational Linguistics.
Young-Bum Kim, Karl Stratos, Xiaohu Liu, and Ruhi
Sarikaya. 2015b. Compact lexicon selection with
spectral methods. In Proceedings of Association for
Computational Linguistics (ACL), pages 806–811.
Association for Computational Linguistics.
Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2015c. Pre-training of hidden-unit crfs. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), pages 192–198. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.800846">
1301
</page>
<reference confidence="0.999073589041096">
Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015d. New transfer learning tech-
niques for disparate label sets. In Proceedings of the
Association for Computational Linguistics (ACL),
pages 473–482. Association for Computational Lin-
guistics.
Shen Li, Joao V Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1389–1398. Pro-
ceedings of Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592–599. ACM.
Thomas Mayer and Michael Cysouw. 2012. Language
comparison through sparse multilingual word align-
ment. In Proceedings of the EACL 2012 Joint Work-
shop of LINGVIS &amp; UNCLH, pages 54–62. Associ-
ation for Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62–72. Association for Computational Lin-
guistics.
Robert C Moore. 2014. Fast high-accuracy part-of-
speech tagging by independent classifiers. In Pro-
ceedings of COLING, pages 1165–1176.
Tahira Naseem, Benjamin Snyder, Jacob Eisen-
stein, and Regina Barzilay. 2009. Multilin-
gual part-of-speech tagging: Two unsupervised ap-
proaches. Journal of Artificial Intelligence Re-
search, 36(1):341–385.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Karl Stratos, Do-kyum Kim, Michael Collins, and
Daniel Hsu. 2014. A spectral algorithm for learn-
ing class-based n-gram models of natural language.
Proceedings of the Association for Uncertainty in
Artificial Intelligence.
Karl Stratos, Michael Collins, and Daniel Hsu. 2015.
Model-based word embeddings from decomposi-
tions of count matrices. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, pages 1282–1291. Association for Com-
putational Linguistics, July.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.
Guillaume Wisniewski, Nicolas P´echeux, Souhir
Gahbiche-Braham, and Franc¸ois Yvon. 2014.
Cross-lingual part-of-speech tagging through am-
biguous learning. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1779–1785.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1–
8. Association for Computational Linguistics.
</reference>
<page confidence="0.996443">
1302
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.282998">
<title confidence="0.969856">Part-of-speech Taggers for Low-resource Languages using CCA Features</title>
<address confidence="0.4894575">Corporation, Redmond, of Wisconsin-Madison, Madison,</address>
<email confidence="0.999827">bsnyder@cs.wisc.edu</email>
<abstract confidence="0.990532681818182">In this paper, we address the challenge of creating accurate and robust partof-speech taggers for low-resource languages. We propose a method that leverages existing parallel data between the target language and a large set of resourcerich languages without ancillary resources such as tag dictionaries. Crucially, we use CCA to induce latent word representations that incorporate cross-genre distributional cues, as well as projected tags from a full array of resource-rich languages. We develop a probability-based confidence model to identify words with highly likely tag projections and use these words to train a multi-class SVM using the CCA features. Our method yields average performance of 85% accuracy for languages with almost no resources, outperforming a state-of-the-art partiallyobserved CRF model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis R Bach</author>
<author>Michael I Jordan</author>
</authors>
<title>A probabilistic interpretation of canonical correlation analysis.</title>
<date>2005</date>
<contexts>
<context position="18354" citStr="Bach and Jordan, 2005" startWordPosition="2942" endWordPosition="2945"> c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2 This has an effect of stabilizing the variance of each term in the matrix, leading to a more efficient estimator. The square-root transformation also transforms the distribution of the count data to look more Gaussian (Bartlett, 1936): since an interpretation of CCA is a latent-variable with normal distributions (Bach and Jordan, 2005), it makes the data more suitable for CCA. It has been observed in past works (e.g., Dhillon et al. (2012)) to significantly improve the quality of the resulting representations. 3.3 Feature Induction Algorithm We now describe our algorithm for inducing latent distributional features both on the multilingual parallel corpus, as well as the monolingual, newswire test data. This algorithm is described ⎧ ⎨⎪ ⎪⎩ 1 m if z = 1, 1296 in detail in Figure 4. The key idea is to perform two CCA steps. The first step incorporates word-distributional information over both the multilingual corpus (the Bible)</context>
</contexts>
<marker>Bach, Jordan, 2005</marker>
<rawString>Francis R Bach and Michael I Jordan. 2005. A probabilistic interpretation of canonical correlation analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MSo Bartlett</author>
</authors>
<title>The square root transformation in analysis of variance. Supplement to the</title>
<date>1936</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>68--78</pages>
<contexts>
<context position="18251" citStr="Bartlett, 1936" startWordPosition="2929" endWordPosition="2930">word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2 This has an effect of stabilizing the variance of each term in the matrix, leading to a more efficient estimator. The square-root transformation also transforms the distribution of the count data to look more Gaussian (Bartlett, 1936): since an interpretation of CCA is a latent-variable with normal distributions (Bach and Jordan, 2005), it makes the data more suitable for CCA. It has been observed in past works (e.g., Dhillon et al. (2012)) to significantly improve the quality of the resulting representations. 3.3 Feature Induction Algorithm We now describe our algorithm for inducing latent distributional features both on the multilingual parallel corpus, as well as the monolingual, newswire test data. This algorithm is described ⎧ ⎨⎪ ⎪⎩ 1 m if z = 1, 1296 in detail in Figure 4. The key idea is to perform two CCA steps. Th</context>
</contexts>
<marker>Bartlett, 1936</marker>
<rawString>MSo Bartlett. 1936. The square root transformation in analysis of variance. Supplement to the Journal of the Royal Statistical Society, pages 68–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the learnability and design of output codes for multiclass problems.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>47--2</pages>
<contexts>
<context position="20260" citStr="Crammer and Singer, 2002" startWordPosition="3253" endWordPosition="3256">ncate this second SVD to the first 50 dimensions. We now have word embeddings that can be applied to any corpus, and are designed to maximize correlation both with typical surrounding word context, as well as typical projected tag context. These embeddings serve as our primary feature vectors for training the POS classifier (described in the next section). We concatenate this primary feature vector with the embeddings of the previous and subsequent words, in order to provide contextsensitive POS predictions. 3.3.1 Multi-class classifier To train our POS tagger, we use a linear multiclass SVM (Crammer and Singer, 2002). It has a parameter wy ∈ Rd for every tag y ∈ T and defines a linear score function s(x, j, y) := wy Φ(x, j). Given any sentence x and a position j, it predicts arg maxy∈T s(x, j, y) as the tag of xj. We use the implementation of Fan et al. (2008) with the default hyperparameter configurations for training. 4 Experiments 4.1 Datasets and Experimental Setup There are more than 4,000 living languages in the world, and one of the most prevalently translated books is the Bible. We now describe the Bible dataset we collected. 2For context words, we use 5 words before and after the word occurrence.</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the learnability and design of output codes for multiclass problems. Machine Learning, 47(2-3):201–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graphbased projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6622" citStr="Das and Petrov, 2011" startWordPosition="1023" endWordPosition="1026">the vertical axis gives the number of languages in each token range. 2 Related Work 2.1 Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich lang</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graphbased projections. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 600–609. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Klaus Macherey</author>
</authors>
<title>Modelbased aligner combination using dual decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>420--429</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9407" citStr="DeNero and Macherey, 2011" startWordPosition="1452" endWordPosition="1455">alignments themselves. Instead of simply aligning each source language to the target language in isolation, we will instead use a confidence model to synthesize information from multiple sources. While there are not many well-known papers that have explored word alignment on a multilingual scale1, there have been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is prov</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>John DeNero and Klaus Macherey. 2011. Modelbased aligner combination using dual decomposition. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 420– 429. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Jordan Rodu</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Two Step CCA: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine learning, ICML’12.</booktitle>
<contexts>
<context position="10993" citStr="Dhillon et al., 2012" startWordPosition="1704" endWordPosition="1707"> than the apparent dimensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tag</context>
<context position="17398" citStr="Dhillon et al., 2012" startWordPosition="2777" endWordPosition="2780">e tags. For simplicity, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p coun</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster, and Lyle H. Ungar. 2012. Two Step CCA: A new spectral method for estimating vector models of words. In Proceedings of the 29th International Conference on Machine learning, ICML’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6645" citStr="Durrett et al., 2012" startWordPosition="1027" endWordPosition="1030">s the number of languages in each token range. 2 Related Work 2.1 Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate align</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="20508" citStr="Fan et al. (2008)" startWordPosition="3308" endWordPosition="3311"> serve as our primary feature vectors for training the POS classifier (described in the next section). We concatenate this primary feature vector with the embeddings of the previous and subsequent words, in order to provide contextsensitive POS predictions. 3.3.1 Multi-class classifier To train our POS tagger, we use a linear multiclass SVM (Crammer and Singer, 2002). It has a parameter wy ∈ Rd for every tag y ∈ T and defines a linear score function s(x, j, y) := wy Φ(x, j). Given any sentence x and a position j, it predicts arg maxy∈T s(x, j, y) as the tag of xj. We use the implementation of Fan et al. (2008) with the default hyperparameter configurations for training. 4 Experiments 4.1 Datasets and Experimental Setup There are more than 4,000 living languages in the world, and one of the most prevalently translated books is the Bible. We now describe the Bible dataset we collected. 2For context words, we use 5 words before and after the word occurrence. 3Embedding dimension was empirically determined by the singular values. Figure 3: Algorithm for deriving CCA projections from samples of two variables. Lang Tagger Accuracy BG Treetagger 0.9909 CS Hunpos 0.8969 DA Hunpos 0.9756 DE Hunpos 0.9855 EN</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Steven Abney</author>
</authors>
<title>Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora. In Natural Language Processing–IJCNLP</title>
<date>2005</date>
<pages>862--873</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7280" citStr="Fossum and Abney (2005)" startWordPosition="1120" endWordPosition="1123">ate-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsuper</context>
</contexts>
<marker>Fossum, Abney, 2005</marker>
<rawString>Victoria Fossum and Steven Abney. 2005. Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora. In Natural Language Processing–IJCNLP 2005, pages 862–873. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Dipanjan Das</author>
</authors>
<title>Crosslingual discriminative learning of sequence models with posterior regularization.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2144" citStr="Ganchev and Das, 2013" startWordPosition="316" endWordPosition="319">t language and a set of resource-rich languages. Fortunately, such parallel data exists for just about every written language, in the form of Bible translations. Around 2,500 languages have at least partial Bible translations, and somewhere between 500 and 1,000 languages have complete translations. We have collected such electronic Bible translations for 650 languages. Figure 1 breaks down the number of languages in our collection according to their token count. The majority of our languages have at least 200,000 tokens of Bible translations. While previous studies (T¨ackstr¨om et al., 2013; Ganchev and Das, 2013) have addressed this general setting, they have typically assumed the existence of a partial tag dictionary as well as large quantities of non-parallel data in the target language. These assumptions are quite reasonable for the dozen most popular languages in the world, but are inadequate for the creation of a truly worldwide repository of NLP tools and linguistic data. In fact, we argue that such ancillary sources of information are not really necessary once we take into account the vastly multilingual nature of our parallel data. Annotations projected from individual resource-rich languages </context>
<context position="6877" citStr="Ganchev and Das, 2013" startWordPosition="1062" endWordPosition="1065"> recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sou</context>
</contexts>
<marker>Ganchev, Das, 2013</marker>
<rawString>Kuzman Ganchev and Dipanjan Das. 2013. Crosslingual discriminative learning of sequence models with posterior regularization. In Proceedings of EMNLP. Association for Computational Linguistics, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Joao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry R Glahn</author>
</authors>
<title>Canonical correlation and its relationship to discriminant analysis and multiple regression.</title>
<date>1968</date>
<journal>Journal of the atmospheric sciences,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="10190" citStr="Glahn, 1968" startWordPosition="1577" endWordPosition="1578">ensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is providing supervision for the other. In fact, it can be shown that CCA directly generalizes both multiple linear regression and Fisher’s Latent Discriminative Analysis (LDA) (Glahn, 1968). From a learning theory perspective, CCA is interesting in that it allows us to prove regret-based learning bounds that depend on the “intrinsic” dimensionality of the problem rather than the apparent dimensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: </context>
</contexts>
<marker>Glahn, 1968</marker>
<rawString>Harry R Glahn. 1968. Canonical correlation and its relationship to discriminant analysis and multiple regression. Journal of the atmospheric sciences, 25(1):23–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="9646" citStr="Hotelling (1936)" startWordPosition="1487" endWordPosition="1488">explored word alignment on a multilingual scale1, there have been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is providing supervision for the other. In fact, it can be shown that CCA directly generalizes both multiple linear regression and Fisher’s Latent Discriminative Analysis (LDA) (Glahn, 1968). From a learning theory perspective, CCA is interesting</context>
<context position="17202" citStr="Hotelling, 1936" startWordPosition="2745" endWordPosition="2746">own tag y, marginalizing out the unknown corruption variables for each language: p(y|y1,...,yn) Qn ��� m + (1 − �`)δ(y,y`)� `=1 mn−1 P Y Qn=1 L�� 1m + (1 − E`)δ(y,, y`)] where Y is all possible tags. For simplicity, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a ma</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3-4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sham M Kakade</author>
<author>Dean P Foster</author>
</authors>
<title>Multiview regression via canonical correlation analysis.</title>
<date>2007</date>
<booktitle>In Learning Theory,</booktitle>
<pages>82--96</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10432" citStr="Kakade and Foster, 2007" startWordPosition="1614" endWordPosition="1617">ervations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is providing supervision for the other. In fact, it can be shown that CCA directly generalizes both multiple linear regression and Fisher’s Latent Discriminative Analysis (LDA) (Glahn, 1968). From a learning theory perspective, CCA is interesting in that it allows us to prove regret-based learning bounds that depend on the “intrinsic” dimensionality of the problem rather than the apparent dimensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 201</context>
</contexts>
<marker>Kakade, Foster, 2007</marker>
<rawString>Sham M Kakade and Dean P Foster. 2007. Multiview regression via canonical correlation analysis. In Learning Theory, pages 82–96. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal grapheme-to-phoneme prediction over latin alphabets.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>332--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7997" citStr="Kim and Snyder, 2012" startWordPosition="1236" endWordPosition="1239"> data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypo</context>
</contexts>
<marker>Kim, Snyder, 2012</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 332–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Optimal data set selection: An application to graphemeto-phoneme conversion.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>1196--1205</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="8019" citStr="Kim and Snyder, 2013" startWordPosition="1240" endWordPosition="1243">e. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multipl</context>
</contexts>
<marker>Kim, Snyder, 2013</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2013a. Optimal data set selection: An application to graphemeto-phoneme conversion. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1196– 1205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Unsupervised consonant-vowel prediction over hundreds of languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1527--1536</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="8019" citStr="Kim and Snyder, 2013" startWordPosition="1240" endWordPosition="1243">e. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multipl</context>
</contexts>
<marker>Kim, Snyder, 2013</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2013b. Unsupervised consonant-vowel prediction over hundreds of languages. In Proceedings of the Association for Computational Linguistics (ACL), pages 1527– 1536. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Jo˜ao V Grac¸a</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal morphological analysis using structured nearest neighbor prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>322--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Kim, Grac¸a, Snyder, 2011</marker>
<rawString>Young-Bum Kim, Jo˜ao V Grac¸a, and Benjamin Snyder. 2011. Universal morphological analysis using structured nearest neighbor prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 322–332. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Heemoon Chae</author>
<author>Benjamin Snyder</author>
<author>Yu-Seop Kim</author>
</authors>
<title>Training a korean srl system with rich morphological features.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>637--642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11011" citStr="Kim et al., 2014" startWordPosition="1708" endWordPosition="1711">ensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-ri</context>
</contexts>
<marker>Kim, Chae, Snyder, Kim, 2014</marker>
<rawString>Young-Bum Kim, Heemoon Chae, Benjamin Snyder, and Yu-Seop Kim. 2014. Training a korean srl system with rich morphological features. In Proceedings of the Association for Computational Linguistics (ACL), pages 637–642. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Minwoo Jeong</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Weakly supervised slot tagging with partially labeled sequences from web search click logs.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>84--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11073" citStr="Kim et al., 2015" startWordPosition="1720" endWordPosition="1723">elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201</context>
<context position="17416" citStr="Kim et al., 2015" startWordPosition="2781" endWordPosition="2784">, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2</context>
</contexts>
<marker>Kim, Jeong, Stratos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Minwoo Jeong, Karl Stratos, and Ruhi Sarikaya. 2015a. Weakly supervised slot tagging with partially labeled sequences from web search click logs. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 84–92. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Xiaohu Liu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Compact lexicon selection with spectral methods.</title>
<date>2015</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL),</booktitle>
<pages>806--811</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11073" citStr="Kim et al., 2015" startWordPosition="1720" endWordPosition="1723">elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201</context>
<context position="17416" citStr="Kim et al., 2015" startWordPosition="2781" endWordPosition="2784">, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2</context>
</contexts>
<marker>Kim, Stratos, Liu, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, Xiaohu Liu, and Ruhi Sarikaya. 2015b. Compact lexicon selection with spectral methods. In Proceedings of Association for Computational Linguistics (ACL), pages 806–811. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Pre-training of hidden-unit crfs.</title>
<date>2015</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>192--198</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11073" citStr="Kim et al., 2015" startWordPosition="1720" endWordPosition="1723">elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201</context>
<context position="17416" citStr="Kim et al., 2015" startWordPosition="2781" endWordPosition="2784">, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2</context>
</contexts>
<marker>Kim, Stratos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya. 2015c. Pre-training of hidden-unit crfs. In Proceedings of the Association for Computational Linguistics (ACL), pages 192–198. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
<author>Minwoo Jeong</author>
</authors>
<title>New transfer learning techniques for disparate label sets.</title>
<date>2015</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>473--482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11073" citStr="Kim et al., 2015" startWordPosition="1720" endWordPosition="1723">elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201</context>
<context position="17416" citStr="Kim et al., 2015" startWordPosition="2781" endWordPosition="2784">, we simply set all E` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriving latent distributional features. Canonical Correlation Analysis (CCA) is a general method for inducing new representations for a pair of variables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encodings, the CCA computation reduces to performing an SVD of the matrix Ω where each entry is count(w, c) Ωw,c = p count(w)count(c) where count(w, c) denotes co-occurrence count of word w and context c in the given corpus, count(w) = Pc count(w, c), and count(c) = P w count(w, c). The resulting word representation is given by UTX where U is a matrix of the scaled left singular vectors of Ω (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: √ count(w, c)1/2 Ωw,c = p count(w)1/2count(c)1/2</context>
</contexts>
<marker>Kim, Stratos, Sarikaya, Jeong, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Minwoo Jeong. 2015d. New transfer learning techniques for disparate label sets. In Proceedings of the Association for Computational Linguistics (ACL), pages 473–482. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Joao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1389--1398</pages>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Joao V Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1389–1398. Proceedings of Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9237" citStr="Liang et al., 2006" startWordPosition="1429" endWordPosition="1432">le language projections are beneficial not only in weeding out random errors and idiosyncratic variations, but also in improving the linguistic consistency of the alignments themselves. Instead of simply aligning each source language to the target language in isolation, we will instead use a confidence model to synthesize information from multiple sources. While there are not many well-known papers that have explored word alignment on a multilingual scale1, there have been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>592--599</pages>
<publisher>ACM.</publisher>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: trading structure for features. In Proceedings of the 25th international conference on Machine learning, pages 592–599. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mayer</author>
<author>Michael Cysouw</author>
</authors>
<title>Language comparison through sparse multilingual word alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9711" citStr="Mayer and Cysouw (2012)" startWordPosition="1497" endWordPosition="1500">ve been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is providing supervision for the other. In fact, it can be shown that CCA directly generalizes both multiple linear regression and Fisher’s Latent Discriminative Analysis (LDA) (Glahn, 1968). From a learning theory perspective, CCA is interesting in that it allows us to prove regret-based learning bounds that </context>
</contexts>
<marker>Mayer, Cysouw, 2012</marker>
<rawString>Thomas Mayer and Michael Cysouw. 2012. Language comparison through sparse multilingual word alignment. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH, pages 54–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>62--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7424" citStr="McDonald et al. (2011)" startWordPosition="1144" endWordPosition="1147">ktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Ki</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 62–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast high-accuracy part-ofspeech tagging by independent classifiers.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1165--1176</pages>
<contexts>
<context position="30275" citStr="Moore, 2014" startWordPosition="5005" endWordPosition="5006"> constraint of the projected tags. Perhaps interestingly, the CCA+SVM method, which is a simple instance-based classifier using cleverly constructed features, outperforms the sequence labeller, achieving accuracy of nearly 86%5 . In our third experiment we use CoNLL test data and compare the PO-CRF models with different settings. See Table 4. This experiment is to show the effects of suffix and Brown cluster features on PO-CRF to relieve the unseen words issue. We also show that the more projecting languages are 5Note that some previous researches (Liang et al., 2008; Wisniewski et al., 2014; Moore, 2014) also pointed out that POS tagging does not necessarily require a sequence model for strong performance. 1299 1 lang (EN) (A) 9/10 langs (W) 9/10 langs (no S/C) 9/10 langs (A) BG 0.7883 0.7144 0.8094 0.8478 CS 0.6601 0.5589 0.6535 0.7868 DA 0.7820 0.7765 0.8016 0.8227 DE 0.8323 0.6956 0.7589 0.8500 ES 0.7893 0.7608 0.8279 0.8665 EU 0.7764 0.7543 0.8035 0.8661 HU 0.6429 0.6378 0.7834 0.8119 IT 0.8444 0.7588 0.8136 0.8921 NL 0.7887 0.6825 0.7751 0.8214 PT 0.8476 0.7797 0.8464 0.8656 TR 0.6306 0.5727 0.6719 0.7143 AVG 0.7621 0.6993 0.7768 0.8314 Table 4: Accuracy of the PO-CRF models on CoNLL dat</context>
</contexts>
<marker>Moore, 2014</marker>
<rawString>Robert C Moore. 2014. Fast high-accuracy part-ofspeech tagging by independent classifiers. In Proceedings of COLING, pages 1165–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="7732" citStr="Naseem et al., 2009" startWordPosition="1192" endWordPosition="1195">-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the</context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="25692" citStr="Petrov et al. (2011)" startWordPosition="4245" endWordPosition="4248">ed data (selected resource-rich languages), plus Basque (EU), Hungarian (HU) and Turkish (TR)) as our test data. It consists of 5,000-6,000 hand-labeled tokens. The accuracy of each supervised tagger on this data is about 94% on average. Since there is no French tagged CoNLL data, we exclude French on testing but still use it in Training. The accuracy of each supervised tagger on this data is shown in Table 1. The tag definitions used in CoNLL data are not exactly matched the ones used in the taggers when converted to universal POS tags. For instance in Spanish, we initially follow mapping of Petrov et al. (2011) for CoNLL data. The ‘dp’ tag for words sus, su, mi are mapped to DET but they are mapped to PRON in the bible data because of the Treetagger definitions. Whenever we find this kind of issues, we analyze them and choose the one of mappings for compatibility. For the ‘dp’ tag, we choose to map PRON. 4.3 Alignments We perform two kinds of alignments in our data sets; (i) the verse alignment and (ii) the word alignment. When the tagged bible volumes are prepared, we align verses across all resource rich languages. For verse alignments, we pre-process to remove extraneous information such as inlin</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Do-kyum Kim</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>A spectral algorithm for learning class-based n-gram models of natural language.</title>
<date>2014</date>
<booktitle>Proceedings of the Association for Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="11033" citStr="Stratos et al., 2014" startWordPosition="1712" endWordPosition="1715">e and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence</context>
</contexts>
<marker>Stratos, Kim, Collins, Hsu, 2014</marker>
<rawString>Karl Stratos, Do-kyum Kim, Michael Collins, and Daniel Hsu. 2014. A spectral algorithm for learning class-based n-gram models of natural language. Proceedings of the Association for Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>Model-based word embeddings from decompositions of count matrices.</title>
<date>2015</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1282--1291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="11055" citStr="Stratos et al., 2015" startWordPosition="1716" endWordPosition="1719">his seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ack</context>
</contexts>
<marker>Stratos, Collins, Hsu, 2015</marker>
<rawString>Karl Stratos, Michael Collins, and Daniel Hsu. 2015. Model-based word embeddings from decompositions of count matrices. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1282–1291. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Nicolas P´echeux</author>
<author>Souhir Gahbiche-Braham</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Cross-lingual part-of-speech tagging through ambiguous learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1779--1785</pages>
<marker>Wisniewski, P´echeux, Gahbiche-Braham, Yvon, 2014</marker>
<rawString>Guillaume Wisniewski, Nicolas P´echeux, Souhir Gahbiche-Braham, and Franc¸ois Yvon. 2014. Cross-lingual part-of-speech tagging through ambiguous learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1779–1785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research,</booktitle>
<pages>pages</pages>
<contexts>
<context position="6238" citStr="Yarowsky et al. (2001)" startWordPosition="960" endWordPosition="963">a, a simple multi-class classifier then suffices to predict accurate tags, even for novel words. This approach outperform a state-of-the-art baseline (T¨ackstr¨om et al., 2013) to achieve average tag accuracy of 85% on newswire text. Figure 1: The breakdown of languages by the number of tokens in their available Bible translations. The horizontal axis gives the number of tokens, and the vertical axis gives the number of languages in each token range. 2 Related Work 2.1 Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the first international conference on Human language technology research, pages 1– 8. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000426">
<title confidence="0.97937">
Conversation Trees: A Grammar Model for Topic Structure in Forums
</title>
<author confidence="0.984705">
Annie Louis and Shay B. Cohen
</author>
<affiliation confidence="0.9979685">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.984949">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999262">
{alouis,scohen}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866809523809">
Online forum discussions proceed differ-
ently from face-to-face conversations and
any single thread on an online forum con-
tains posts on different subtopics. This
work aims to characterize the content of
a forum thread as a conversation tree of
topics. We present models that jointly per-
form two tasks: segment a thread into sub-
parts, and assign a topic to each part. Our
core idea is a definition of topic struc-
ture using probabilistic grammars. By
leveraging the flexibility of two grammar
formalisms, Context-Free Grammars and
Linear Context-Free Rewriting Systems,
our models create desirable structures for
forum threads: our topic segmentation is
hierarchical, links non-adjacent segments
on the same topic, and jointly labels the
topic during segmentation. We show that
our models outperform a number of tree
generation baselines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998886875">
Online forums are commonplace today and used
for various purposes: product support and trou-
bleshooting, opining about events and people, and
student interaction on online course platforms.
Threads in these forums become long, involve
posts from multiple users, and the chronological
order of the posts in a thread does not represent
a continuous flow of dialog. Adding structure to
these threads is important for tasks such as infor-
mation extraction, search, and summarization.
One such aspect of structure is topic. Figure 1
shows a computer-troubleshooting related thread
with six posts. The first post is the troubleshoot-
ing question and the remaining posts can be seen
as focusing on either of two topics, the driver soft-
ware (posts p1, p2, p5) or the speaker hardware
</bodyText>
<note confidence="0.49769">
p0 Bob: When I play a recorded video on my camera, it
</note>
<tableCaption confidence="0.715592333333333">
looks and sounds fine. On my computer, it plays
at a really fast rate and sounds like Alvin and the
Chipmunks!
</tableCaption>
<note confidence="0.7464134">
p1 Kate: I’d find and install the latest audio driver.
p2 Mary: The motherboard supplies the clocks for audio
feedback. So update the audio and motherboard
drivers.
p3 Chris: Another fine mess in audio is volume and speaker
settings. You checked these?
p4 Jane: Yes, under speaker settings, look for hardware ac-
celeration. Turning it off worked for me.
p5 Matt: Audio drivers are at this link. Rather than just
audio drivers, I would also just do all drivers.
</note>
<tableCaption confidence="0.998382">
Table 1: Example forum thread conversation
</tableCaption>
<bodyText confidence="0.999147535714286">
(p3, p4). By categorizing posts into such topics,
we can provide a useful division of content in a
thread and even across multiple threads. Note that
the driver topic is not a contiguous sequence but
present in non-adjacent parts, (p1, p2) and (p5).
We tackle the problem of joint topic segmenta-
tion and topic labeling of forum threads. Given
a thread’s posts in chronological order (the order
in which they were posted), we create a phrase
structure tree indicating how the posts are grouped
hierarchically into subtopics and super-topics. In
these conversation trees, leaves span entire posts.
Each non-terminal identifies the topic character-
izing the posts in its span. Topics are concepts or
themes which summarize the content of a group of
posts. Specifically, a topic is a set of words which
frequently co-occur in posts which are similar in
content and other conversation regularities.
Our key insight in this work is to formalize
topic structure using probabilistic grammars. We
define a base grammar for topic structure of fo-
rum threads and refine it to represent finer topics
and subtrees. We learn to predict trees under our
grammar based on two formalisms: Probabilis-
tic Context-Free Grammars (PCFG) and Proba-
bilistic Linear Context-Free Rewriting Systems
(PLCFRS). In the PCFG model, a non-terminal
spans a contiguous sequence of posts. In the
</bodyText>
<page confidence="0.892987">
1543
</page>
<note confidence="0.9856935">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1543–1553,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.985426285714286">
PLCFRS model, non-terminals are allowed to
span discontinuous segments of posts. We lever-
age algorithms from probabilistic parsing of nat-
ural language sentences and modify them for our
domain. We show that our model performs well
and sidesteps a number of limitations of prior
topic segmentation approaches. In particular:
</bodyText>
<listItem confidence="0.986763">
• Our models perform joint topic segmentation
and topic labeling while most existing models
identify unlabeled segments. Labeling topics on
segments creates richer annotation, and links non-
adjacent segments on the same topic.
• Our grammar-based probabilistic models have
two key benefits. They naturally create tree struc-
</listItem>
<bodyText confidence="0.9742136">
tures which are considered linguistically suitable
for topic segmentation but were difficult to create
under previous approaches. Second, the flexibility
of grammars such as PLCFRS allow our models
to seamlessly learn to produce trees where non-
adjacent segments on the same topic are explicitly
linked, an issue that was not addressed before.
We present large-scale experiments on a col-
lection of forum threads from the computer-
troubleshooting domain.1 We show that our gram-
mar models achieve a good balance between iden-
tifying when posts should be in the same topic ver-
sus a different topic. These grammar models out-
perform other tree generation baselines by a sig-
nificant margin especially on short threads.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999935666666667">
The ideas in this paper are related to three areas of
prior research.
Forum thread analysis. Finding structure in fo-
rum threads has been previously addressed in two
ways. The first is reply structure prediction where
a parent post is linked to its children (replies)
which were posted later in time (Wang et al.,
2008; Cong et al., 2008). Reply links are some-
times augmented with a dialog act label indicat-
ing whether the child post is a question, answer, or
confirmation to the parent post (Kim et al., 2010;
Wang et al., 2011). The second set of methods
partition sentences in emails or blog comments
into topical clusters and then show salient words
per cluster as topic tags (Joty et al., 2013).
We focus on producing rich hierarchical seg-
mentation going beyond clusters which do not
contain any cluster-internal structure. We also be-
</bodyText>
<footnote confidence="0.940947333333333">
1Our corpus is available from http://
kinloch.inf.ed.ac.uk/public/CTREES/
ConversationTrees.html
</footnote>
<bodyText confidence="0.999910745098039">
lieve that topic structure is complementary to di-
alog act and reply link annotations. Tasks on fo-
rum data such as user expertise (Lui and Baldwin,
2009) and post quality prediction (Agichtein et al.,
2008), and automatic summarization (Nenkova
and Bagga, 2003) can be carried out on a fine-
grained level using topic information.
Conversation disentanglement. A related prob-
lem of clustering utterances is defined specifically
for Internet Relay Chat (IRC) and speech conver-
sations. In this case, multiple conversations, on
different topics, are mixed in and systems extract
threads which separate out individual conversa-
tions (Shen et al., 2006; Adams and Martell, 2008;
Elsner and Charniak, 2010).
Disentanglement is typically applied for the
coarse-level problem of identifying a coherent
conversation. In addition, these methods do not
create any structure upon the clustered utterances
in contrast to the focus of this work.
Topic Segmentation. is a task which directly fo-
cuses on the topic aspect of text and speech. This
task is of greater importance for speech which
lacks explicit structure such as paragraphs and
sections. Many approaches perform linear seg-
mentation where boundaries are inserted in the
text or speech to divide it into a flat set of topic
segments (Hearst, 1994; Utiyama and Isahara,
2001; Galley et al., 2003; Malioutov and Barzilay,
2006; Eisenstein and Barzilay, 2008). Very few
methods recursively combine smaller segments
into larger ones. Such hierarchical models (Eisen-
stein, 2009) have been applied at a coarse level
for segmenting very long texts such as books into
sections. Other work has focused on linear seg-
mentation for documents within the same domain
and having a regular structure (Chen et al., 2009;
Jeong and Titov, 2010; Du et al., 2015). These
latter approaches rely on three assumptions: that
the documents contain a regular set of topics, that
these topics are discussed in a fairly regular con-
sensus order, and that the same topic does not re-
cur in the same document.
Our models address two deficiencies in these
approaches. First, text is commonly understood
to have a hierarchical structure (Grosz and Sidner,
1986) and our grammar model is an ideal frame-
work for this goal. Tree structures also have other
advantages, for example, we do not predefine the
number of expected topic segments in a conversa-
tion tree, a requirement posed by many prior seg-
</bodyText>
<page confidence="0.992714">
1544
</page>
<bodyText confidence="0.999780357142857">
mentation algorithms. The second limitation of
prior studies is assuming that topics do not recur
in the same document. But linguistic theories al-
low for non-adjacent utterances to belong to the
same topic segment (Grosz and Sidner, 1986) and
this fact is empirically true in chat and forum con-
versations (Elsner and Charniak, 2010; Wang et
al., 2011). Our models can flexibly handle and
link recurring topics within and across threads.
As a final note, because of the annotations re-
quired, most prior work on forums or IRC chats
have typically used few hundred threads. We
present a heuristically derived large corpus of
topic structure on which we evaluate our models.
</bodyText>
<sectionHeader confidence="0.997934" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.9961025">
Our topic discovery methods are based on two
constituency grammar formalisms.
</bodyText>
<subsectionHeader confidence="0.998694">
3.1 Probabilistic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.99851895">
A PCFG is defined by a 5-tuple GC =(N, T,
P, 5, D) where N is a set of non-terminal sym-
bols, T a set of terminal symbols, and 5 is the
start symbol. P is a set of production rules of
the form A —* Q where A is a non-terminal and
Q E IN U T}*. D is a function that associates
each production rule with a conditional probabil-
ity of the form p(A —* QJA). This probability in-
dicates how often the non-terminal A expands into
Q. The probabilities of all the rules conditioned on
a particular non-terminal should sum to 1.
The joint probability of a tree T with yield Y ,
P(T, Y ), is the product of the probabilities of all
the productions used to construct T. The parsing
problem is to find the tree Tˆ which is most likely
given the yield Y . Tˆ = arg maxT P(T JY ) =
arg maxT P(T, Y ).
Given training trees, we can enumerate the pro-
ductions and compute their probabilities using
maximum likelihood estimates (MLE):
</bodyText>
<equation confidence="0.979512666666667">
= count (A —* Q)
p(A QJA )
count(A)
</equation>
<bodyText confidence="0.999828866666667">
which is the fraction of the times the non-terminal
A expands into Q.
The most likely parse tree can be found using
a number of algorithms. In this work, we use
the CYK algorithm for PCFGs in Chomsky Nor-
mal Form. This algorithm has complexity O(n3)
where n is the length of the yield.
PCFGs do not capture a frequently occurring
property of forum threads, discontinuous seg-
ments on the same topic. Indirectly however, a
PCFG may assign the same non-terminal for each
of these segments. To model these discontinuities
more directly, we present a second model based
on PLCFRS where non-terminals are allowed to
span discontinuous yield strings.
</bodyText>
<subsectionHeader confidence="0.9993825">
3.2 Probabilistic Linear Context-Free
Rewriting Systems
</subsectionHeader>
<bodyText confidence="0.981355404761905">
LCFRS grammars (Vijay-Shanker et al., 1987)
generalize CFGs, where non-terminals can span
discontinuous constituents. Formally, the span
of an LCFRS non-terminal is a tuple, with size
k &gt; 1, of strings, where k is the non-terminal
“fan-out”. As such, the fan-out of a CFG non-
terminal is 1.
An LCFRS GL =(N, T, P, 5, V ) where N is
the set of non-terminals, T the terminals and 5 is
the start symbol. A function f : N —* N gives
the fan-out of each non-terminal. P is the set of
productions or otherwise called rewriting rules of
the LCFRS. V is a set of variables used to indicate
the spans of each non-terminal in these rules. A
rewriting rule has the form:
A(α1, α2, ... , αf(A)) —*
A1(x1 1 ...,x1f(A1)),...,Am(xm1 ,...,xmf(A.))
Here A,A1,...,Am E N. Since there are m
non-terminals on the RHS, this rule has rank m.
xij E V for 1 G i G m and 1 G j G f(Ai) indi-
cate the f(Ai) discontinuous spans dominated by
Ai. αi E (T U V )*, 1 G i G f(A) are the spans
of the LHS non-terminal A.
A rewriting rule explains how the left-hand
side (LHS) non-terminal’s span can be com-
posed from the yields of the right-hand side
(RHS) non-terminals. For example, in the rule
A(x1x2, x3) —* B(x1)C(x2, x3), A and C have
fan-out 2, B has fan-out 1. The two spans of A,
x1x2 and x3, are composed from the spans of B
and C. For comparison, the productions of a CFG
take the single spans of each non-terminal on the
RHS and concatenate them in the same order to
yield a single span of the LHS non-terminal.
A Probabilistic LCFRS (PLCFRS) (Levy,
2005) also contains D, a function which assigns
conditional probabilities p(A(x) —* �JA(x)) to
the rules. The probabilities conditioned on a par-
ticular non-terminal and span configuration, A(x)
should sum to 1. Given a training corpus, LCFRS
rules can be read out and probabilities computed
similar to CFG rules.
</bodyText>
<page confidence="0.958958">
1545
</page>
<bodyText confidence="0.9999815">
To find the most likely parse tree, we use the
parsing algorithm proposed by Kallmeyer and
Maier (2013) for binary PLCFRS. The approach
uses weighted deduction rules (Shieber et al.,
1995; Nederhof, 2003), which specify how to
compute a new item from other existing items.
Each item is of the form [A, pj where A is a
non-terminal and ρ�is a vector indicating the spans
dominated by A. A weight w is attached to each
item which gives the |log |of the Viterbi inside
probability of the subtree under that item. A set
of goal items specify the form of complete parse
trees. By using the Knuth’s generalization (Knuth,
1977) of the shortest paths algorithm, the most
likely tree can be found without exhaustive pars-
ing as in Viterbi parsing of CFGs. The complexity
of parsing is O(n3k) where k is the fan-out of the
grammar (the maximum fan-out of its rules).
</bodyText>
<sectionHeader confidence="0.991281" genericHeader="method">
4 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999941833333334">
Given a thread consisting of a sequence of posts
(p1, p2, . . . , pn) in chronological order, the task
is to produce a constituency tree with yield (p1,
p2 . . .pn). A leaf in this tree spans an entire
post. Non-terminals identify the topic of the posts
within their span. Non-terminals at higher levels
of the tree represent coarser topics in the conver-
sation (the span covered by these nodes contain
more posts) than those lower in the tree. The root
topic node indicates the overall topic of the thread.
Below we define a Context-Free Grammar
(CFG) for such trees.
</bodyText>
<subsectionHeader confidence="0.998513">
4.1 A Grammar for Conversation Trees
</subsectionHeader>
<bodyText confidence="0.997734055555556">
GB is our base grammar which is context-free
and has four non-terminals {5, X, T, C}. Each
post p in the corpus is a terminal symbol (i.e. a ter-
minal symbol is a bag of words). The productions
in GB are: 5 — *T X*, X —* T X* and T — *p.
GB generates trees with the following structure.
A root-level topic 5 characterizes the content of
the entire thread. Thread-starting rules are of the
form, 5 —* T X*, where X* indicates a sequence
of zero or more X non-terminals. T nodes are
pre-terminals analogous to part-of-speech tags in
the case of syntactic parsing. In our grammar,
the T —* p rule generates a post in the thread.
In the thread-starting rules, T generates the first
post of the thread which poses the query or com-
ment that elicits the rest of the conversation. The
X* sequence denotes topic branches, the subtree
under each X is assumed to correspond to a dif-
</bodyText>
<figureCaption confidence="0.817487">
Figure 1: Example conversation tree for the thread
in Table 1
</figureCaption>
<bodyText confidence="0.988016658536585">
ferent topic. These X’s characterize all but the
first post of the thread. The continuation rules,
X —* T X*, recursively subdivide the X subtrees
into topics spanning fewer posts. In each case, the
T node on the right-hand side of these rules gener-
ates the first post (in terms of posting time) in that
subtree. Therefore posts made earlier in time al-
ways dominate (in the tree structure) those which
come later in the thread. We define the head of a
non-terminal as the first post as per the chronolog-
ical order of posts in the span of the non-terminal.
This grammar does not generate binary trees.
We binarize the tree with C nodes to obtain an
equivalent grammar in Chomsky Normal Form
(CNF) (CNF yields parsing algorithms with lower
complexity)2: 5 —* T C  |T, X —* T C  |T,
T —* p and C —* X  |X X |X C. The C nodes
can be collapsed and its daughters attached to the
parent of C to revert back to the non-binary tree.
While this CFG defines the structure of conver-
sation trees, by itself this grammar is insufficient
for our task. In particular, it contains a single non-
terminal of each type (5, X, T, C) and so does
not distinguish between topics. We extend this
grammar to create GE which has a set of non-
terminals corresponding to each non-terminal in
GB, these fine-grained non-terminals correspond
to different topics. GE is created using latent an-
notations (Matsuzaki et al., 2005) on the X, T,
5 and C non-terminals from GB. The resulting
non-terminals for GE are 5[i], X[j], T [k] and
C[l], such that 1 &lt; i &lt; NS, 1 &lt; j &lt; NX,
1 &lt; k &lt; NT, 1 &lt; l &lt; NC. i, j, k and l identify
specific topics attached to a particular node type.
Our output trees are created with GE to depict
the topic segmentation of the thread and are non-
binary. The binary trees produced by our algo-
rithms are converted by collapsing the C. As a
result, conversation trees have 5[i], X[j] and T [k]
2Any context-free grammar can be converted to an equiv-
alent CNF grammar. Our algorithms support unary rules.
</bodyText>
<figure confidence="0.995109333333333">
P2
P4
S[11]
T[6]
P1
X[8]
T[7]
X[12]
X[7]
T[4]
X[5]
T[6]
P5
T[3]
P0
X[5]
T[2]
P3
</figure>
<page confidence="0.958142">
1546
</page>
<bodyText confidence="0.98965175">
nodes but no C[l] nodes.
An example conversation tree for the thread in
Table 1 is shown in Figure 1. At level 1, T[3] de-
scribes the topic of the first post while the remain-
ing posts are under X[5] which may indicate a
driver topic, and X[12], a speaker hardware topic.
Note how X[5] may re-occur in the conversation
to accommodate post p5 on the driver topic.
</bodyText>
<subsectionHeader confidence="0.988711">
4.2 Supervised learning framework
</subsectionHeader>
<bodyText confidence="0.999997222222222">
We use a supervised framework for learning the
models. We assume that we have training trees
according to the base grammar, GB. The follow-
ing section describes our data and how we obtain
these GB-based trees. In Section 6, we present
a method for creating GE-type trees with non-
terminal refinements. Estimates of rule probabili-
ties from this augmented training corpus are used
to develop the parsers for topic segmentation.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.999985115384615">
We collected 13,352 computer-troubleshooting
related threads from http://forums.cnet.
com/. The number of posts per thread varies
greatly between 1 and 394, and the average is
around 5 posts. We divide these threads into train-
ing, development and test sets. The most frequent
100 words from the training set are used as stop-
words. After filtering stopwords, a post contains
39 tokens on average and the vocabulary size of
our corpus is 81,707. For development and test-
ing, we only keep threads with a minimum of 3
posts (so that the problem is non-trivial) and a
maximum of 50 posts (due to complexity of pars-
ing). We have 9,243 training threads, 2,014 for
development, and 2,071 for testing.
A particular feature of the forums on cnet.
com is the explicit reply structure present in the
threads. The forum interface elicits these reply re-
lationships as users develop a thread. When a user
replies in a particular thread, she has to choose
(only) one of the earlier posts in the thread (in-
cluding the question post) to attach her reply to. In
this way, each post is linked to a unique post ear-
lier in time in the same thread. This reply structure
forms a dependency tree. Figure 2 (a) is a possible
reply tree for the thread in Table 1.
</bodyText>
<subsectionHeader confidence="0.994392">
5.1 Deriving conversation trees
</subsectionHeader>
<bodyText confidence="0.999449">
Next we convert these reply-link trees into phrase-
structure conversation trees. We developed a de-
terministic conversion method that uses the gen-
</bodyText>
<equation confidence="0.949273714285714">
(a) p0 p1 p2 p3 p4 p5
(b) S
T X X
p0 T X T X
p1 T T T X
p2 p3 p4 T
p5
</equation>
<figureCaption confidence="0.6913735">
Figure 2: (a) A reply structure tree for the thread
in Table 1 and (b) the derived conversation tree
</figureCaption>
<bodyText confidence="0.999592424242424">
erative process defined by the base grammar GB.
The key idea is to track when the conversation
branches into sub-topics and when the replies are
proceeding within the same topic.
The algorithm traverses the nodes of the depen-
dency tree D in breadth-first order, starting at the
root (first) post. We create a root S node in the
phrase structure tree H. Then the thread-starting
rule from GB, S → T X∗, is used to create one
T and k X nodes as children of S. The first post
p0 is attached as a child of the T node. k is equal
to the number of replies to p0 (children of p0 in
D). For each of these k X nodes, we instanti-
ate a X → T X∗ rule in H. The k replies of p0
are attached one each as a child of the T nodes
in these rules. Any set of children are always in-
stantiated in chronological order. So the span of a
non-terminal in H always contains posts in non-
decreasing time order. We continue the procedure
with the next post from D in the traversal order.
This procedure converts the reply tree of Figure
2 (a) into the conversation tree (b). Note that (a) is
a possible reply structure for our example thread
in Table 1. The conversation tree (b) derived ac-
cording to this reply structure has a non-projective
structure where p1, p2 and p5 are linked under one
X node (at level 1). Such a tree can be produced
by our LCFRS model. The ideal PCFG tree will
repeat the topic branch as in Figure 1.
The derived trees at this stage follow GB and
contain only the S, X, T non-terminals (without
any latent annotations). This tree is converted into
Chomsky Normal Form using C nodes.
</bodyText>
<subsectionHeader confidence="0.995132">
5.2 Discontinuous topic segments
</subsectionHeader>
<bodyText confidence="0.999681">
As in our example above, non-projective edges
in the reply structure are rather frequent. Of the
</bodyText>
<page confidence="0.978627">
1547
</page>
<bodyText confidence="0.998955363636363">
total threads in our corpus 14.5% contain a non-
projective edge. A thread should have a mini-
mum of four posts to have the possibility of non-
projective edges. Among the 7,691 threads with at
least four posts, the percentage of non-projective
trees is even higher, 25%. This finding suggests
that in any thread of reasonable size which we
wish to summarize or categorize, non-projective
edges will be common. Hence a direct approach
for addressing discontinuous segments such as our
PLCFRS model is important for this domain.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="method">
6 Parsers for Conversation Trees
</sectionHeader>
<bodyText confidence="0.999833">
The training data are conversation trees with rules
from GB. We refine the non-terminals to cre-
ate GE, extract PCFG or PLCFRS rules from the
training trees, and build a CYK parser that pre-
dicts the most likely tree according to GE.
</bodyText>
<subsectionHeader confidence="0.999227">
6.1 Refining the non-terminals
</subsectionHeader>
<bodyText confidence="0.998796875">
We use a clustering approach, akin to the spectral
algorithm of Cohen et al. (2013) and Narayan and
Cohen (2015),3 to create finer grained categories
corresponding to GB’s non-terminals: 5, X, C
and T. Each node in each tree in the training data
is associated with a feature vector, which is a func-
tion of the tree and the anchor node. These vectors
are clustered (for each of the non-terminals sepa-
rately) and then, each node is annotated with the
corresponding cluster. This process gives us the
non-terminals 5[i], X[j], T [k] and C[l] of GE.
The features for a node nl are: depth of nl in the
tree, root is at depth 0; maximum depth of the sub-
tree under nl; number of siblings of nl; number of
children of nl; number of posts in the span of nl;
average length (in terms of tokens) of the posts in
the span of nl; average similarity of the span of nl
with the span of nl’s siblings4; similarity of nl’s
span with the span of its left-most sibling; elapsed
time between the first and last posts in nl’s span.
We use CLUTO toolkit (Karypis, 2002) to per-
form clustering. The algorithm maximizes the
pairwise cosine similarity between the feature
vectors of nodes within the same cluster. The
</bodyText>
<footnote confidence="0.9799768">
3The main difference between our algorithm and the al-
gorithm by Narayan and Cohen (2015) is that we do not de-
compose the trees into “inside” trees and “outside” trees, or
use a singular value decomposition step before clustering the
features.
4The span of nl and that of a sibling are each represented
by binary vectors indicating the presence and absence of a
term in the span. The similarity value is computing using
cosine overlap between the vectors and the average across all
siblings is recorded.
</footnote>
<bodyText confidence="0.998904333333333">
best number of clusters for the four non-terminal
node types are tuned jointly to give the best per-
formance on our final topic segmentation task.
</bodyText>
<subsectionHeader confidence="0.99959">
6.2 Learning rule probabilities
</subsectionHeader>
<bodyText confidence="0.995410285714286">
As mentioned previously, each terminal in our
grammar is an entire post’s text. For the pre-
terminal to terminal productions in our grammar
T[j] —* pi, we compute p(T[j] —* pi|T[j]) as
the probability under a unigram language model
Lj which is trained on the collection of the posts
from the training corpus which are dominated by
</bodyText>
<equation confidence="0.5632185">
T [j] nodes. p(T [j] —* pi|T [j]) = �Npi
k=1 Lj(wik)
</equation>
<bodyText confidence="0.9992084">
where wi1, wi2...wiNpi are the tokens in post pi.
The rest of the production probabilities are
learned using MLE on the training trees. In the
case of LCFRS rules, the gap information is also
obtained during the extraction.
</bodyText>
<subsectionHeader confidence="0.996827">
6.3 CYK parsing
</subsectionHeader>
<bodyText confidence="0.999946714285714">
For both PCFG and LCFRS we use CYK style
algorithms, as outlined in §3, to obtain the most
likely tree. For the more computationally com-
plex LCFRS model, we make a number of addi-
tions to improve speed. First, we restrict the fan-
out of the grammar to 2, i.e. any non-terminal
can only span a maximum of two discontinuous
segments. 97% of the productions in fact have
only non-terminals with fan-out &lt; 2. Second, we
use A∗ search (Maier et al., 2012) to prioritize
our agenda. Last, we reduce the number of items
added to the agenda. An item has the form [A,
p-1, A is a non-terminal and ρ� is the spans cov-
ered by A. For every span, we only keep the top 5
non-terminal items according to the score. In ad-
dition, we only allow spans with a gap of at most
2 since 77% of all gaps (dominated by fan-out &lt;
2) non-terminals are &lt; 2 posts. Moreover, after
a certain number of items (10,000) are added to
the chart, we only allow the creation of new items
which have a contiguous span.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="method">
7 Systems for comparison
</sectionHeader>
<bodyText confidence="0.9841465">
We compare our models to two types of systems.
7.1 STRUCTURE ONLY
The first type generate tree structures without con-
sidering the content of the threads.
Right-branching tree (RBT). produces a
strictly right branching tree where each post is
dominated by the immediately previous (accord-
ing to time) post in the thread. It uses the grammar
</bodyText>
<page confidence="0.986686">
1548
</page>
<bodyText confidence="0.99995975">
with the rules {S → TX, X → TX, X → T,
T → p}. This method does not perform use-
ful topic segmentation as it produces only a single
topic branch containing all the posts.
Attach-to-root tree (ART). attaches each post
to the root of the tree. The grammar rules are
{S → TX1...Xn, X → T, T → p}, where n is
the number of posts in the thread. This approach
assumes each post belongs to a different topic in
the thread. In contrast to RBT, ART contains too
many topic branches, one per post in the thread.
Random tree (RAND). mixes decisions to cre-
ate a new topic branch or continue in the same
branch. The generation process is top down, at
each step, the algorithm chooses a certain num-
ber of topic branches (Xs) to create (≤ number of
posts left to add to the tree). Then, the number
of posts under each branch is sampled (such that
each branch has at least one post). This process is
then recursively done at the new topic branches.
</bodyText>
<subsectionHeader confidence="0.992291">
7.2 STRUCTURE AND CONTENT
</subsectionHeader>
<bodyText confidence="0.999955921568628">
These approaches produce tree structures in-
formed by content. We build these parsers by
modifying prior models for chat disentanglement
and linear topic segmentation of documents.
Similarity tree (SIM). produces trees by at-
taching each post as a child of the most similar of
the previous (by time) posts (Wang et al., 2008).
We use cosine similarity between vector represen-
tations of two posts in order to compute similar-
ity. When the similarity exceeds a threshold value,
the post is added under the topic branch of the
prior post. Otherwise the post is under a new topic
branch attached to the root of the tree. A thresh-
old of 0.15 was chosen after tuning on the devel-
opment data.
Cluster tree (CLUS). uses an approach re-
lated to chat disentanglement (Elsner and Char-
niak, 2010). The posts within each thread are clus-
tered separately into kl clusters where kl = l/h
depends on the number of posts in the thread, l.
h = 6 was chosen by tuning. The posts in each
cluster are ordered by time and a right branching
tree is created over them. These kl cluster-level
trees are then attached as children of a new node
to create a thread-level tree. The cluster-trees are
ordered left to right in the thread-tree according to
the time of the earliest post in each cluster.
Linear segmentation tree (LSEG). is based
on postprocessing the output of a Bayesian linear
topic segmentation model (Eisenstein and Barzi-
lay, 2008). Each post’s content is treated as a sen-
tence and a document is created for each thread by
appending its posts in their time order. The model
is then used to group consecutive sentences into kl
segments. For each thread of length l, kl = l/h,
h = 6 was chosen by tuning. For each segment, a
right branching tree is created and these segment-
level trees are made siblings in a thread-level tree.
The segments are added left to right in the thread-
tree as per their order in the text.
All STRUCTURE trees contain thread structure
but no topic labels. In other words, they have
coarse non-terminals (X, T and S) only. The
STRUCTURE AND CONTENT trees, LSEG and
CLUS contain topics or groups but only at one top
level, and further the number and labels of these
topics are different per thread. Hence there is no
linking across threads. Within a thread, the SIM
and CLUS tree can link non-adjacent posts under
the same topic. These links are also not available
from a LSEG tree.
</bodyText>
<sectionHeader confidence="0.987219" genericHeader="method">
8 Evaluation metrics
</sectionHeader>
<bodyText confidence="0.998761571428571">
To evaluate the topic segmentation, we develop a
node-governance based measure. Our score com-
pares two conversation trees g and h, where g is
the gold-standard tree and h is the hypothesized
one. We assume that g and h are in dependency
format, reversing the transformation from §5.1.
We break g (and h) into a set of pairs, for each
pair of nodes in the tree (each node is a post in the
thread). For each such pair, p and q, we find their
least common ancestor, f(p, q|g) (or f(p, q|h). If
these nodes are in a governing relation (p domi-
nates q or vice versa), then f(p, q) is the dominat-
ing node. We then define the following sets and
quantities for · E {g, h}:
</bodyText>
<listItem confidence="0.99950175">
• S1(·) = {(p, q, f(p, q))  |f(p, q|·) E {p, q}}.
• S2(·) = {(p, q, f(p, q))  |f(p, q|·) E/ {p, q}}.
• n1(g, h) = |S1(g) n S1(h)|.
• n2(g, h) = |S2(g) n S2(h)|.
</listItem>
<bodyText confidence="0.9461456">
s1(·) and s2(·) are defined as the size of
S1(·) and S2(·), repsectively. Let g1, ... , gn and
h1, ... , hn be a corpus of gold-standard conver-
sation trees and their corresponding hypothesized
conversation trees. Then the evaluation metric
we compute is the harmonic mean (Fscore) of the
micro-average of the precision for governing (G-
p) and non-governing (NG-p) pairs, and recall for
governing (G-r) and non-governing (NG-r) pairs.
For example, G-p is calculated as
</bodyText>
<page confidence="0.89269">
1549
</page>
<equation confidence="0.9715395">
G-p = �i= n1(gi, hi)
Eni=1 s1(hi) .
</equation>
<bodyText confidence="0.999955">
Traditional parsing evaluation measures such as
constituency bracketting and dependency attach-
ment scores were too local for our purpose. For
example, if a long chain of posts is placed in a dif-
ferent topic but their local dependencies are main-
tained, we only penalize one constituent and one
node’s parent in the constituency and dependency
scores respectively. But the topic segmentation
created by this change has several posts placed in
the wrong topic branch. Our scores overcome this
problem by considering the relationship between
all pairs of posts and also dividing the relationship
in the pair as governing or non-governing.
</bodyText>
<sectionHeader confidence="0.996597" genericHeader="evaluation">
9 Results and discussion
</sectionHeader>
<bodyText confidence="0.975107034482759">
We tune the number of latent topic annotations for
the non-terminals using grid search on the devel-
opment set. The best settings are 40 5, 100 X, 20
C, 80 T clusters for PCFG and 10 5, 5 X, 15 C,
40 T for LCFRS.
Below we show an example non-projective tree
created by our LCFRS parser. The topics are indi-
cated with the most frequent 5 words in the match-
ing cluster.
S[9]: problem, time,
windows, pc, could
Here post p4 though later in posting time is pre-
dicted to be on the same topic as p1.
The non-terminals in our trees enable useful
topic segmentation and we found that perfor-
mance is extremely sensitive to the number of
non-terminals of each type 5, X, C and T. Cur-
rently, we do not have a direct method to evaluate
the non-terminals in our tree but we plan to use the
information in other applications as an evaluation.
Table 2 and 3 shows the segmentation perfor-
mance of the models (as percentages). The per-
formance varied greatly depending on the length
of the threads and hence we show the results sepa-
rately for threads with up to 15 posts (SHORT) and
those with 16 to 50 posts (LONG). The results are
divided into sections based on the subset of test
data on which the evaluation is performed. The
first section (R1.) is performance on all threads,
(R2.) only on the projective threads in the test
data, and (R3.) only on the non-projective threads.
Among the baselines, the Right-branching trees
(RBT) or Attaching to the root (ART) have some
advantages: the RBT receives 100% recall of the
governing pairs and the ART tree has high recall
of the non-governing pairs. However, their Fs-
cores are 0. Recall that the RBT contains a single
topic branch and hence no useful segmentation is
done; ART is the other extreme where every post
is put in a separate topic branch. RAND is the av-
erage performance of 3 randomly generated trees
for each thread. This method has a better balance
between branching and depth leading to 33.4 Fs-
core for SHORT and 21.5 for LONG threads.
The PCFG and the LCFRS models clearly out-
perform these baselines. The Fscore improves up
to 15% over RAND on SHORT and LONG threads.
The grammar models also consistently outperform
SIM systems.
With regard to CLUS and LSEG, there is a
difference in performance between SHORT and
LONG threads and based on whether the desired
structure was projective or non-projective. On
SHORT threads, the grammar models outperform
LSEG and CLUS particularly on the projective
threads (the LCFRS model has a 22% higher Fs-
core). On the longer threads however, the CLUS
and LSEG models perform best overall and for
non-projective threads. CLUS and LSEG directly
model the content similarity of posts while the
grammar models make many decisions at level of
topic nodes. Remember that the clustering is done
per thread in CLUS and LSEG compared to using
a common set of topics across all threads. Making
such fine-grained similarity comparison appears
to be helpful especially for longer threads and
even though LSEG does not make non-projective
decisions, its accuracy is high on the attachments
it makes leading to good performance on non-
projective threads too. In future work, we plan
to explore how we can combine the advantages of
direct similarity with the grammar models.
Between the two grammar models, the LCFRS
model is better than PCFG, even on projective
threads, and can produce non-projective trees.
Part of this improvement on projective trees could
be due to more data being available in the LCFRS
model since all the data can be used for training it.
T[1]:thank, time, i’ll,
really, try
X[4]:power, time, go,
problem, same
X[3]:drive, try, windows,
hard, problem
T[6]: cd, drive,
windows,
problem, dvd
</bodyText>
<figure confidence="0.991387083333333">
P2
X[3]
T[6]
P3
T[17]:printer,
ink, hp,
printers, print
P1
X[4]
T[17]
P4
PQ
</figure>
<page confidence="0.924966">
1550
</page>
<table confidence="0.999704862068965">
Model Ex G-p G-r NG-p NG-r F
R1. On all gold threads
(1,971 threads, 24,620 post pairs)
RBT 20.4 50.8 100.0 100.0 0.0 0.0
ART 5.6 100.0 0.0 42.3 86.0 0.0
RAND 5.2 55.5 19.4 39.2 65.5 33.4
SIM 5.7 68.2 13.3 43.1 79.0 27.9
CLUS 20.2 52.9 85.5 47.5 17.2 42.8
LSEG 20.2 53.0 88.2 52.2 16.5 42.8
PCFG 9.7 52.7 60.4 41.0 34.9 48.3
LCFRS 11.4 53.3 62.5 43.6 35.9 49.9
R2. On projective gold threads only
RBT 24.4 59.8 100.0 100.0 0.0 0.0
ART 6.7 100.0 0.0 35.1 87.4 0.0
RAND 6.2 62.0 22.0 32.4 63.5 35.3
SIM 5.9 73.8 13.9 36.0 79.6 28.4
CLUS 24.2 59.8 90.6 31.3 7.3 26.8
LSEG 24.2 60.1 91.9 35.6 7.6 27.9
PCFG 11.7 61.2 60.5 35.8 36.4 49.5
LCFRS 13.5 62.0 64.5 37.6 35.3 50.8
R3. On non-projective gold threads only
RBT 0.0 39.1 100.0 100.0 0.0 0.0
ART 0.0 100.0 0.0 51.7 84.8 0.0
RAND 0.0 42.0 14.3 47.3 67.3 26.9
SIM 4.3 58.1 12.1 52.1 78.5 26.0
CLUS 0.0 41.2 75.1 54.4 25.8 45.4
LSEG 0.0 41.8 80.7 59.8 24.1 45.9
PCFG 0.0 41.1 60.1 47.6 33.5 45.2
LCFRS 0.3 40.8 58.5 50.4 36.4 46.0
</table>
<tableCaption confidence="0.994881">
Table 2: Results on threads with up to 15 posts:
</tableCaption>
<bodyText confidence="0.99933105882353">
for the grammar models (PCFG and LCFRS) and
comparison systems (See Section 7). ‘Ex’ is per-
centage of fully correct trees and other scores are
from Section 8. Top two Fscores are in bold.
For the PCFG model, only the projective data can
be used for training.
Overall, the LCFRS model is powerful on pro-
jective threads and SHORT non-projective threads.
Compared to PCFG, the LCFRS model has a num-
ber of advantages: we can use more data, can pre-
dict non-projective trees. Some of the constraints
we imposed on the LCFRS parser, such as restrict-
ing the gap degree are likely to have limited the
ability of the model to generate more flexible non-
projective edges. We believe that as we figure out
how to make these parsers faster, we will see even
more improvements from the LCFRS models.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="conclusions">
10 Conclusions
</sectionHeader>
<bodyText confidence="0.996835333333333">
This work represents a first approach to learn dis-
course structure of forum threads within an ex-
plicit grammar framework. We show that a coarse
</bodyText>
<table confidence="0.999890103448276">
Model Ex G-p G-r NG-p NG-r F
R1. On all gold threads
(100 threads, 27,590 post pairs)
RBT 0.0 21.2 100.0 100.0 0.0 0.0
ART 0.0 100.0 0.0 69.8 88.6 0.0
RAND 0.0 38.9 10.0 65.4 78.4 21.5
SIM 0.0 37.0 8.9 67.2 80.9 19.6
CLUS 0.0 32.6 37.3 73.3 70.5 42.0
LSEG 0.0 35.4 50.4 76.8 68.0 48.4
PCFG 0.0 24.6 54.1 54.3 36.8 36.6
LCFRS 0.0 22.8 71.4 68.7 29.4 36.5
R2. On projective gold threads only
RBT 0.0 36.7 100.0 100.0 0.0 0.0
ART 0.0 100.0 0.0 57.1 90.3 0.0
RAND 0.0 59.9 11.0 56.0 82.6 24.3
SIM 0.0 45.9 8.3 54.4 80.2 19.1
CLUS 0.0 42.0 38.1 60.0 63.2 45.3
LSEG 0.0 51.3 55.0 68.0 65.1 56.9
PCFG 0.0 42.2 66.6 34.5 22.9 39.9
LCFRS 0.0 49.0 65.3 51.6 41.7 52.3
R3. On non-projective gold threads only
RBT 0.0 19.6 100.0 100.0 0.0 0.0
ART 0.0 100.0 0.0 71.1 88.5 0.0
RAND 0.0 36.1 9.9 66.3 78.1 20.9
SIM 0.0 35.8 9.0 68.5 81.0 19.7
CLUS 0.0 31.2 37.1 74.6 71.1 41.3
LSEG 0.0 33.2 49.6 77.6 68.2 46.8
PCFG 0.0 22.3 51.6 55.8 37.9 34.8
LCFRS 0.0 20.9 72.6 71.6 28.4 34.8
</table>
<tableCaption confidence="0.999951">
Table 3: Results on threads with &gt; 15 posts
</tableCaption>
<bodyText confidence="0.999863545454545">
grammar for structure can be refined using latent
annotations to indicate the finer topic differences.
Our trees have good segmentation performance
and provide useful summaries of the thread con-
tent at the non-terminal nodes. A main goal for
future work is to incorporate further domain spe-
cific constraints on the models to improve parsing
speed and at the same time allow more flexible
trees. We also plan to evaluate the usefulness of
conversation trees in tasks such as predicting if a
thread is resolved, and user expertise.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998625">
We thank the anonymous reviewers for their sug-
gestions. We also thank Bonnie Webber, Adam
Lopez and other members of the Probabilistic
Models of Language reading group at the Univer-
sity of Edinburgh for helpful discussions. The first
author was supported by a Newton International
Fellowship (NF120479) from the Royal Society
and the British Academy.
</bodyText>
<page confidence="0.990709">
1551
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944098039216">
P. H. Adams and C. H. Martell. 2008. Topic detection
and extraction in chat. In Proceedings of the IEEE
International Conference on Semantic Computing,
pages 581–588.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media. In Proceedings of WSDM, pages 183–
194.
H. Chen, S. R. K. Branavan, R. Barzilay, and D. R.
Karger. 2009. Content modeling using latent per-
mutations. Journal of Artificial Intelligence Re-
search, 36(1):129–163.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2013. Experiments with spectral learn-
ing of latent-variable PCFGs. In Proceedings of
NAACL.
G. Cong, L. Wang, C. Lin, Y. Song, and Y. Sun. 2008.
Finding question-answer pairs from online forums.
In Proceedings of SIGIR, pages 467–474.
L. Du, J. K. Pate, and M. Johnson. 2015. Topic seg-
mentation with an ordering-based topic model. In
Proceedings of AAAI, pages 2232–2238.
J. Eisenstein and R. Barzilay. 2008. Bayesian un-
supervised topic segmentation. In Proceedings of
EMNLP, pages 334–343.
J. Eisenstein. 2009. Hierarchical text segmentation
from multi-scale lexical cohesion. In Proceedings
of HLT:NAACL, pages 353–361.
M. Elsner and E. Charniak. 2010. Disentangling chat.
Computational Linguistics, 36(3):389–409.
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of ACL, pages
562–569.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computational
Linguistics, 12(3):175–204, July.
M.A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings ofACL, pages 9–16.
M. Jeong and I. Titov. 2010. Unsupervised discourse
segmentation of documents with inherently paral-
lel structure. In Proceedings of ACL: Short papers,
pages 151–155.
S. Joty, G. Carenini, and R. T. Ng. 2013. Topic seg-
mentation and labeling in asynchronous conversa-
tions. Journal of Artificial Intelligence Research,
47(1):521–573.
L. Kallmeyer and W. Maier. 2013. Data-driven pars-
ing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1):87–119.
G. Karypis. 2002. Cluto - a clustering toolkit. Techni-
cal Report TR-02-017, Dept. of Computer Science,
University of Minnesota.
S. Kim, L. Cavedon, and T. Baldwin. 2010. Clas-
sifying dialogue acts in one-on-one live chats. In
Proceedings of EMNLP, pages 862–871.
D. E. Knuth. 1977. A generalization of dijkstra’s al-
gorithm. Information Processing Letters, 6(1):1–5.
R. Levy. 2005. Probabilistic models of word order
and syntactic discontinuity. Ph.D. thesis, Stanford
University.
M. Lui and T. Baldwin. 2009. Classifying user forum
participants: Separating the gurus from the hacks,
and other tales of the internet. In Proceedings of the
2010 Australasian Language Technology Workshop,
pages 49–57.
W. Maier, M. Kaeshammer, and L. Kallmeyer. 2012.
Plcfrs parsing revisited: Restricting the fan-out to
two. In Proceedings of the 11th International Work-
shop on Tree Adjoining Grammar and Related For-
malisms.
I. Malioutov and R. Barzilay. 2006. Minimum cut
model for spoken lecture segmentation. In Proceed-
ings of COLING-ACL, pages 25–32.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In Proceedings
ofACL, pages 75–82.
S. Narayan and S. B. Cohen. 2015. Diversity in spec-
tral learning for natural language parsing. In Pro-
ceedings of EMNLP.
M. Nederhof. 2003. Weighted deductive parsing
and knuth’s algorithm. Computational Linguistics,
29(1):135–143.
A. Nenkova and A. Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP.
D. Shen, Q. Yang, J. Sun, and Z. Chen. 2006. Thread
detection in dynamic text message streams. In Pro-
ceedings of SIGIR, pages 35–42.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1&amp;2):3–36.
M. Utiyama and H. Isahara. 2001. A statistical model
for domain-independent text segmentation. In Pro-
ceedings of ACL, pages 499–506.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
ACL, pages 104–111.
Y. Wang, M. Joshi, W. Cohen, and C. P. Ros´e. 2008.
Recovering implicit thread structure in newsgroup
style conversations. In Proceedings of ICWSM.
</reference>
<page confidence="0.874588">
1552
</page>
<reference confidence="0.99894175">
L. Wang, M. Lui, S. Kim, J. Nivre, and T. Baldwin.
2011. Predicting thread discourse structure over
technical web forums. In Proceedings of EMNLP,
pages 13–25.
</reference>
<page confidence="0.982172">
1553
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496535">
<title confidence="0.998355">Conversation Trees: A Grammar Model for Topic Structure in Forums</title>
<author confidence="0.998068">Annie Louis</author>
<author confidence="0.998068">B Shay</author>
<affiliation confidence="0.994889">School of University of</affiliation>
<address confidence="0.50835">Edinburgh, EH8 9AB,</address>
<abstract confidence="0.999056590909091">Online forum discussions proceed differently from face-to-face conversations and any single thread on an online forum contains posts on different subtopics. This work aims to characterize the content of forum thread as a tree topics. We present models that jointly perform two tasks: segment a thread into subparts, and assign a topic to each part. Our core idea is a definition of topic structure using probabilistic grammars. By leveraging the flexibility of two grammar formalisms, Context-Free Grammars and Linear Context-Free Rewriting Systems, our models create desirable structures for forum threads: our topic segmentation is hierarchical, links non-adjacent segments on the same topic, and jointly labels the topic during segmentation. We show that our models outperform a number of tree generation baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P H Adams</author>
<author>C H Martell</author>
</authors>
<title>Topic detection and extraction in chat.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE International Conference on Semantic Computing,</booktitle>
<pages>581--588</pages>
<contexts>
<context position="7026" citStr="Adams and Martell, 2008" startWordPosition="1109" endWordPosition="1112">o dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic seg</context>
</contexts>
<marker>Adams, Martell, 2008</marker>
<rawString>P. H. Adams and C. H. Martell. 2008. Topic detection and extraction in chat. In Proceedings of the IEEE International Conference on Semantic Computing, pages 581–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
<author>A Gionis</author>
<author>G Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>183--194</pages>
<contexts>
<context position="6563" citStr="Agichtein et al., 2008" startWordPosition="1040" endWordPosition="1043"> al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. I</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. 2008. Finding high-quality content in social media. In Proceedings of WSDM, pages 183– 194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>S R K Branavan</author>
<author>R Barzilay</author>
<author>D R Karger</author>
</authors>
<title>Content modeling using latent permutations.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="8101" citStr="Chen et al., 2009" startWordPosition="1280" endWordPosition="1283">ons. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments </context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>H. Chen, S. R. K. Branavan, R. Barzilay, and D. R. Karger. 2009. Content modeling using latent permutations. Journal of Artificial Intelligence Research, 36(1):129–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D P Foster</author>
<author>L Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable PCFGs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="22574" citStr="Cohen et al. (2013)" startWordPosition="3915" endWordPosition="3918">gests that in any thread of reasonable size which we wish to summarize or categorize, non-projective edges will be common. Hence a direct approach for addressing discontinuous segments such as our PLCFRS model is important for this domain. 6 Parsers for Conversation Trees The training data are conversation trees with rules from GB. We refine the non-terminals to create GE, extract PCFG or PLCFRS rules from the training trees, and build a CYK parser that predicts the most likely tree according to GE. 6.1 Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB’s non-terminals: 5, X, C and T. Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals 5[i], X[j], T [k] and C[l] of GE. The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl; number of siblings of</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cong</author>
<author>L Wang</author>
<author>C Lin</author>
<author>Y Song</author>
<author>Y Sun</author>
</authors>
<title>Finding question-answer pairs from online forums.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="5760" citStr="Cong et al., 2008" startWordPosition="912" endWordPosition="915">domain.1 We show that our grammar models achieve a good balance between identifying when posts should be in the same topic versus a different topic. These grammar models outperform other tree generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html li</context>
</contexts>
<marker>Cong, Wang, Lin, Song, Sun, 2008</marker>
<rawString>G. Cong, L. Wang, C. Lin, Y. Song, and Y. Sun. 2008. Finding question-answer pairs from online forums. In Proceedings of SIGIR, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Du</author>
<author>J K Pate</author>
<author>M Johnson</author>
</authors>
<title>Topic segmentation with an ordering-based topic model.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>2232--2238</pages>
<contexts>
<context position="8142" citStr="Du et al., 2015" startWordPosition="1288" endWordPosition="1291">tation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement pos</context>
</contexts>
<marker>Du, Pate, Johnson, 2015</marker>
<rawString>L. Du, J. K. Pate, and M. Johnson. 2015. Topic segmentation with an ordering-based topic model. In Proceedings of AAAI, pages 2232–2238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="7755" citStr="Eisenstein and Barzilay, 2008" startWordPosition="1224" endWordPosition="1227">identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur </context>
<context position="28708" citStr="Eisenstein and Barzilay, 2008" startWordPosition="5021" endWordPosition="5025"> posts within each thread are clustered separately into kl clusters where kl = l/h depends on the number of posts in the thread, l. h = 6 was chosen by tuning. The posts in each cluster are ordered by time and a right branching tree is created over them. These kl cluster-level trees are then attached as children of a new node to create a thread-level tree. The cluster-trees are ordered left to right in the thread-tree according to the time of the earliest post in each cluster. Linear segmentation tree (LSEG). is based on postprocessing the output of a Bayesian linear topic segmentation model (Eisenstein and Barzilay, 2008). Each post’s content is treated as a sentence and a document is created for each thread by appending its posts in their time order. The model is then used to group consecutive sentences into kl segments. For each thread of length l, kl = l/h, h = 6 was chosen by tuning. For each segment, a right branching tree is created and these segmentlevel trees are made siblings in a thread-level tree. The segments are added left to right in the threadtree as per their order in the text. All STRUCTURE trees contain thread structure but no topic labels. In other words, they have coarse non-terminals (X, T</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
</authors>
<title>Hierarchical text segmentation from multi-scale lexical cohesion.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT:NAACL,</booktitle>
<pages>353--361</pages>
<contexts>
<context position="7872" citStr="Eisenstein, 2009" startWordPosition="1241" endWordPosition="1243">ast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to </context>
</contexts>
<marker>Eisenstein, 2009</marker>
<rawString>J. Eisenstein. 2009. Hierarchical text segmentation from multi-scale lexical cohesion. In Proceedings of HLT:NAACL, pages 353–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>E Charniak</author>
</authors>
<title>Disentangling chat.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="7054" citStr="Elsner and Charniak, 2010" startWordPosition="1113" endWordPosition="1116">nk annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama</context>
<context position="9098" citStr="Elsner and Charniak, 2010" startWordPosition="1448" endWordPosition="1451">ave a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement posed by many prior seg1544 mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories allow for non-adjacent utterances to belong to the same topic segment (Grosz and Sidner, 1986) and this fact is empirically true in chat and forum conversations (Elsner and Charniak, 2010; Wang et al., 2011). Our models can flexibly handle and link recurring topics within and across threads. As a final note, because of the annotations required, most prior work on forums or IRC chats have typically used few hundred threads. We present a heuristically derived large corpus of topic structure on which we evaluate our models. 3 Background Our topic discovery methods are based on two constituency grammar formalisms. 3.1 Probabilistic Context-Free Grammars A PCFG is defined by a 5-tuple GC =(N, T, P, 5, D) where N is a set of non-terminal symbols, T a set of terminal symbols, and 5 i</context>
<context position="28073" citStr="Elsner and Charniak, 2010" startWordPosition="4909" endWordPosition="4913">egmentation of documents. Similarity tree (SIM). produces trees by attaching each post as a child of the most similar of the previous (by time) posts (Wang et al., 2008). We use cosine similarity between vector representations of two posts in order to compute similarity. When the similarity exceeds a threshold value, the post is added under the topic branch of the prior post. Otherwise the post is under a new topic branch attached to the root of the tree. A threshold of 0.15 was chosen after tuning on the development data. Cluster tree (CLUS). uses an approach related to chat disentanglement (Elsner and Charniak, 2010). The posts within each thread are clustered separately into kl clusters where kl = l/h depends on the number of posts in the thread, l. h = 6 was chosen by tuning. The posts in each cluster are ordered by time and a right branching tree is created over them. These kl cluster-level trees are then attached as children of a new node to create a thread-level tree. The cluster-trees are ordered left to right in the thread-tree according to the time of the earliest post in each cluster. Linear segmentation tree (LSEG). is based on postprocessing the output of a Bayesian linear topic segmentation mo</context>
</contexts>
<marker>Elsner, Charniak, 2010</marker>
<rawString>M. Elsner and E. Charniak. 2010. Disentangling chat. Computational Linguistics, 36(3):389–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multiparty conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="7693" citStr="Galley et al., 2003" startWordPosition="1216" endWordPosition="1219"> typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly re</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003. Discourse segmentation of multiparty conversation. In Proceedings of ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="8526" citStr="Grosz and Sidner, 1986" startWordPosition="1351" endWordPosition="1354">level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement posed by many prior seg1544 mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories allow for non-adjacent utterances to belong to the same topic segment (Grosz and Sidner, 1986) and this fact is empirically true in chat and forum conversations (Elsner and Charniak, 2010; Wang et al., 2011). Our mo</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. J. Grosz and C. L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="7645" citStr="Hearst, 1994" startWordPosition="1210" endWordPosition="1211">r and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>M.A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings ofACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jeong</author>
<author>I Titov</author>
</authors>
<title>Unsupervised discourse segmentation of documents with inherently parallel structure.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL: Short papers,</booktitle>
<pages>151--155</pages>
<contexts>
<context position="8124" citStr="Jeong and Titov, 2010" startWordPosition="1284" endWordPosition="1287">s perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree,</context>
</contexts>
<marker>Jeong, Titov, 2010</marker>
<rawString>M. Jeong and I. Titov. 2010. Unsupervised discourse segmentation of documents with inherently parallel structure. In Proceedings of ACL: Short papers, pages 151–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Joty</author>
<author>G Carenini</author>
<author>R T Ng</author>
</authors>
<title>Topic segmentation and labeling in asynchronous conversations.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="6121" citStr="Joty et al., 2013" startWordPosition="976" endWordPosition="979">ad analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A rel</context>
</contexts>
<marker>Joty, Carenini, Ng, 2013</marker>
<rawString>S. Joty, G. Carenini, and R. T. Ng. 2013. Topic segmentation and labeling in asynchronous conversations. Journal of Artificial Intelligence Research, 47(1):521–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kallmeyer</author>
<author>W Maier</author>
</authors>
<title>Data-driven parsing using probabilistic linear context-free rewriting systems.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="13164" citStr="Kallmeyer and Maier (2013)" startWordPosition="2188" endWordPosition="2191"> the productions of a CFG take the single spans of each non-terminal on the RHS and concatenate them in the same order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns conditional probabilities p(A(x) —* �JA(x)) to the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, pj where A is a non-terminal and ρ�is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaus</context>
</contexts>
<marker>Kallmeyer, Maier, 2013</marker>
<rawString>L. Kallmeyer and W. Maier. 2013. Data-driven parsing using probabilistic linear context-free rewriting systems. Computational Linguistics, 39(1):87–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Karypis</author>
</authors>
<title>Cluto - a clustering toolkit.</title>
<date>2002</date>
<tech>Technical Report TR-02-017,</tech>
<institution>Dept. of Computer Science, University of Minnesota.</institution>
<contexts>
<context position="23538" citStr="Karypis, 2002" startWordPosition="4097" endWordPosition="4098">otated with the corresponding cluster. This process gives us the non-terminals 5[i], X[j], T [k] and C[l] of GE. The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl; number of siblings of nl; number of children of nl; number of posts in the span of nl; average length (in terms of tokens) of the posts in the span of nl; average similarity of the span of nl with the span of nl’s siblings4; similarity of nl’s span with the span of its left-most sibling; elapsed time between the first and last posts in nl’s span. We use CLUTO toolkit (Karypis, 2002) to perform clustering. The algorithm maximizes the pairwise cosine similarity between the feature vectors of nodes within the same cluster. The 3The main difference between our algorithm and the algorithm by Narayan and Cohen (2015) is that we do not decompose the trees into “inside” trees and “outside” trees, or use a singular value decomposition step before clustering the features. 4The span of nl and that of a sibling are each represented by binary vectors indicating the presence and absence of a term in the span. The similarity value is computing using cosine overlap between the vectors a</context>
</contexts>
<marker>Karypis, 2002</marker>
<rawString>G. Karypis. 2002. Cluto - a clustering toolkit. Technical Report TR-02-017, Dept. of Computer Science, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>L Cavedon</author>
<author>T Baldwin</author>
</authors>
<title>Classifying dialogue acts in one-on-one live chats.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>862--871</pages>
<contexts>
<context position="5931" citStr="Kim et al., 2010" startWordPosition="943" endWordPosition="946">perform other tree generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality pre</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>S. Kim, L. Cavedon, and T. Baldwin. 2010. Classifying dialogue acts in one-on-one live chats. In Proceedings of EMNLP, pages 862–871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>A generalization of dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="13682" citStr="Knuth, 1977" startWordPosition="2282" endWordPosition="2283">he most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, pj where A is a non-terminal and ρ�is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaustive parsing as in Viterbi parsing of CFGs. The complexity of parsing is O(n3k) where k is the fan-out of the grammar (the maximum fan-out of its rules). 4 Problem Formulation Given a thread consisting of a sequence of posts (p1, p2, . . . , pn) in chronological order, the task is to produce a constituency tree with yield (p1, p2 . . .pn). A leaf in this tree spans an entire post. Non-terminals identify the topic of the posts within their span. Non-terminals at higher levels of the tree represent coarser topics i</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>D. E. Knuth. 1977. A generalization of dijkstra’s algorithm. Information Processing Letters, 6(1):1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Probabilistic models of word order and syntactic discontinuity.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="12748" citStr="Levy, 2005" startWordPosition="2123" endWordPosition="2124">(T U V )*, 1 G i G f(A) are the spans of the LHS non-terminal A. A rewriting rule explains how the left-hand side (LHS) non-terminal’s span can be composed from the yields of the right-hand side (RHS) non-terminals. For example, in the rule A(x1x2, x3) —* B(x1)C(x2, x3), A and C have fan-out 2, B has fan-out 1. The two spans of A, x1x2 and x3, are composed from the spans of B and C. For comparison, the productions of a CFG take the single spans of each non-terminal on the RHS and concatenate them in the same order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns conditional probabilities p(A(x) —* �JA(x)) to the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is o</context>
</contexts>
<marker>Levy, 2005</marker>
<rawString>R. Levy. 2005. Probabilistic models of word order and syntactic discontinuity. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lui</author>
<author>T Baldwin</author>
</authors>
<title>Classifying user forum participants: Separating the gurus from the hacks, and other tales of the internet.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2010 Australasian Language Technology Workshop,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="6510" citStr="Lui and Baldwin, 2009" startWordPosition="1032" endWordPosition="1035">mation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-le</context>
</contexts>
<marker>Lui, Baldwin, 2009</marker>
<rawString>M. Lui and T. Baldwin. 2009. Classifying user forum participants: Separating the gurus from the hacks, and other tales of the internet. In Proceedings of the 2010 Australasian Language Technology Workshop, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Maier</author>
<author>M Kaeshammer</author>
<author>L Kallmeyer</author>
</authors>
<title>Plcfrs parsing revisited: Restricting the fan-out to two.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms.</booktitle>
<contexts>
<context position="25435" citStr="Maier et al., 2012" startWordPosition="4425" endWordPosition="4428">duction probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction. 6.3 CYK parsing For both PCFG and LCFRS we use CYK style algorithms, as outlined in §3, to obtain the most likely tree. For the more computationally complex LCFRS model, we make a number of additions to improve speed. First, we restrict the fanout of the grammar to 2, i.e. any non-terminal can only span a maximum of two discontinuous segments. 97% of the productions in fact have only non-terminals with fan-out &lt; 2. Second, we use A∗ search (Maier et al., 2012) to prioritize our agenda. Last, we reduce the number of items added to the agenda. An item has the form [A, p-1, A is a non-terminal and ρ� is the spans covered by A. For every span, we only keep the top 5 non-terminal items according to the score. In addition, we only allow spans with a gap of at most 2 since 77% of all gaps (dominated by fan-out &lt; 2) non-terminals are &lt; 2 posts. Moreover, after a certain number of items (10,000) are added to the chart, we only allow the creation of new items which have a contiguous span. 7 Systems for comparison We compare our models to two types of systems</context>
</contexts>
<marker>Maier, Kaeshammer, Kallmeyer, 2012</marker>
<rawString>W. Maier, M. Kaeshammer, and L. Kallmeyer. 2012. Plcfrs parsing revisited: Restricting the fan-out to two. In Proceedings of the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>R Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="7723" citStr="Malioutov and Barzilay, 2006" startWordPosition="1220" endWordPosition="1223">r the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and tha</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>I. Malioutov and R. Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of COLING-ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="16867" citStr="Matsuzaki et al., 2005" startWordPosition="2863" endWordPosition="2866">—* T C |T, T —* p and C —* X |X X |X C. The C nodes can be collapsed and its daughters attached to the parent of C to revert back to the non-binary tree. While this CFG defines the structure of conversation trees, by itself this grammar is insufficient for our task. In particular, it contains a single nonterminal of each type (5, X, T, C) and so does not distinguish between topics. We extend this grammar to create GE which has a set of nonterminals corresponding to each non-terminal in GB, these fine-grained non-terminals correspond to different topics. GE is created using latent annotations (Matsuzaki et al., 2005) on the X, T, 5 and C non-terminals from GB. The resulting non-terminals for GE are 5[i], X[j], T [k] and C[l], such that 1 &lt; i &lt; NS, 1 &lt; j &lt; NX, 1 &lt; k &lt; NT, 1 &lt; l &lt; NC. i, j, k and l identify specific topics attached to a particular node type. Our output trees are created with GE to depict the topic segmentation of the thread and are nonbinary. The binary trees produced by our algorithms are converted by collapsing the C. As a result, conversation trees have 5[i], X[j] and T [k] 2Any context-free grammar can be converted to an equivalent CNF grammar. Our algorithms support unary rules. P2 P4 </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg with latent annotations. In Proceedings ofACL, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayan</author>
<author>S B Cohen</author>
</authors>
<title>Diversity in spectral learning for natural language parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="22603" citStr="Narayan and Cohen (2015)" startWordPosition="3920" endWordPosition="3923"> of reasonable size which we wish to summarize or categorize, non-projective edges will be common. Hence a direct approach for addressing discontinuous segments such as our PLCFRS model is important for this domain. 6 Parsers for Conversation Trees The training data are conversation trees with rules from GB. We refine the non-terminals to create GE, extract PCFG or PLCFRS rules from the training trees, and build a CYK parser that predicts the most likely tree according to GE. 6.1 Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB’s non-terminals: 5, X, C and T. Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals 5[i], X[j], T [k] and C[l] of GE. The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl; number of siblings of nl; number of children of nl</context>
</contexts>
<marker>Narayan, Cohen, 2015</marker>
<rawString>S. Narayan and S. B. Cohen. 2015. Diversity in spectral learning for natural language parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nederhof</author>
</authors>
<title>Weighted deductive parsing and knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13265" citStr="Nederhof, 2003" startWordPosition="2205" endWordPosition="2206"> order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns conditional probabilities p(A(x) —* �JA(x)) to the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, pj where A is a non-terminal and ρ�is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaustive parsing as in Viterbi parsing of CFGs. The complexity of parsing is O(n3k) where k is the fan-ou</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M. Nederhof. 2003. Weighted deductive parsing and knuth’s algorithm. Computational Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>A Bagga</author>
</authors>
<title>Facilitating email thread access by extractive summary generation.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="6618" citStr="Nenkova and Bagga, 2003" startWordPosition="1047" endWordPosition="1050">nces in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure u</context>
</contexts>
<marker>Nenkova, Bagga, 2003</marker>
<rawString>A. Nenkova and A. Bagga. 2003. Facilitating email thread access by extractive summary generation. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>Q Yang</author>
<author>J Sun</author>
<author>Z Chen</author>
</authors>
<title>Thread detection in dynamic text message streams.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="7001" citStr="Shen et al., 2006" startWordPosition="1105" endWordPosition="1108"> is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it int</context>
</contexts>
<marker>Shen, Yang, Sun, Chen, 2006</marker>
<rawString>D. Shen, Q. Yang, J. Sun, and Z. Chen. 2006. Thread detection in dynamic text message streams. In Proceedings of SIGIR, pages 35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>Y Schabes</author>
<author>F C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="13248" citStr="Shieber et al., 1995" startWordPosition="2201" endWordPosition="2204">enate them in the same order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns conditional probabilities p(A(x) —* �JA(x)) to the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, pj where A is a non-terminal and ρ�is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaustive parsing as in Viterbi parsing of CFGs. The complexity of parsing is O(n3k) wher</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1&amp;2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>499--506</pages>
<contexts>
<context position="7672" citStr="Utiyama and Isahara, 2001" startWordPosition="1212" endWordPosition="1215">, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are dis</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>M. Utiyama and H. Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL, pages 499–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="11260" citStr="Vijay-Shanker et al., 1987" startWordPosition="1834" endWordPosition="1837"> a number of algorithms. In this work, we use the CYK algorithm for PCFGs in Chomsky Normal Form. This algorithm has complexity O(n3) where n is the length of the yield. PCFGs do not capture a frequently occurring property of forum threads, discontinuous segments on the same topic. Indirectly however, a PCFG may assign the same non-terminal for each of these segments. To model these discontinuities more directly, we present a second model based on PLCFRS where non-terminals are allowed to span discontinuous yield strings. 3.2 Probabilistic Linear Context-Free Rewriting Systems LCFRS grammars (Vijay-Shanker et al., 1987) generalize CFGs, where non-terminals can span discontinuous constituents. Formally, the span of an LCFRS non-terminal is a tuple, with size k &gt; 1, of strings, where k is the non-terminal “fan-out”. As such, the fan-out of a CFG nonterminal is 1. An LCFRS GL =(N, T, P, 5, V ) where N is the set of non-terminals, T the terminals and 5 is the start symbol. A function f : N —* N gives the fan-out of each non-terminal. P is the set of productions or otherwise called rewriting rules of the LCFRS. V is a set of variables used to indicate the spans of each non-terminal in these rules. A rewriting rul</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>M Joshi</author>
<author>W Cohen</author>
<author>C P Ros´e</author>
</authors>
<title>Recovering implicit thread structure in newsgroup style conversations.</title>
<date>2008</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<marker>Wang, Joshi, Cohen, Ros´e, 2008</marker>
<rawString>Y. Wang, M. Joshi, W. Cohen, and C. P. Ros´e. 2008. Recovering implicit thread structure in newsgroup style conversations. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>M Lui</author>
<author>S Kim</author>
<author>J Nivre</author>
<author>T Baldwin</author>
</authors>
<title>Predicting thread discourse structure over technical web forums.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>13--25</pages>
<contexts>
<context position="5951" citStr="Wang et al., 2011" startWordPosition="947" endWordPosition="950"> generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein e</context>
<context position="9118" citStr="Wang et al., 2011" startWordPosition="1452" endWordPosition="1455">e (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement posed by many prior seg1544 mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories allow for non-adjacent utterances to belong to the same topic segment (Grosz and Sidner, 1986) and this fact is empirically true in chat and forum conversations (Elsner and Charniak, 2010; Wang et al., 2011). Our models can flexibly handle and link recurring topics within and across threads. As a final note, because of the annotations required, most prior work on forums or IRC chats have typically used few hundred threads. We present a heuristically derived large corpus of topic structure on which we evaluate our models. 3 Background Our topic discovery methods are based on two constituency grammar formalisms. 3.1 Probabilistic Context-Free Grammars A PCFG is defined by a 5-tuple GC =(N, T, P, 5, D) where N is a set of non-terminal symbols, T a set of terminal symbols, and 5 is the start symbol. </context>
</contexts>
<marker>Wang, Lui, Kim, Nivre, Baldwin, 2011</marker>
<rawString>L. Wang, M. Lui, S. Kim, J. Nivre, and T. Baldwin. 2011. Predicting thread discourse structure over technical web forums. In Proceedings of EMNLP, pages 13–25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
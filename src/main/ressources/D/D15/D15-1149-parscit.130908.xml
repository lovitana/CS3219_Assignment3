<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996948">
Search-Aware Tuning for Hierarchical Phrase-based Decoding
</title>
<author confidence="0.986112">
Feifei Zhai
</author>
<affiliation confidence="0.9879465">
Queens College
City University of New York
</affiliation>
<email confidence="0.96191">
ffzhai2012@gmail.com
</email>
<sectionHeader confidence="0.981258" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965565217391">
Parameter tuning is a key problem for sta-
tistical machine translation (SMT). Most
popular parameter tuning algorithms for
SMT are agnostic of decoding, result-
ing in parameters vulnerable to search er-
rors in decoding. The recent research of
“search-aware tuning” (Liu and Huang,
2014) addresses this problem by consid-
ering the partial derivations in every de-
coding step so that the promising ones
are more likely to survive the inexact de-
coding beam. We extend this approach
from phrase-based translation to syntax-
based translation by generalizing the eval-
uation metrics for partial translations to
handle tree-structured derivations in a way
inspired by inside-outside algorithm. Our
approach is simple to use and can be ap-
plied to most of the conventional parame-
ter tuning methods as a plugin. Extensive
experiments on Chinese-to-English trans-
lation show significant BLEU improve-
ments on MERT, MIRA and PRO.
</bodyText>
<sectionHeader confidence="0.995048" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997239875">
Efforts in parameter tuning algorithms for SMT,
such as MERT (Och, 2003; Galley et al., 2013),
MIRA (Watanabe et al., 2007; Chiang et al., 2009;
Cherry and Foster, 2012; Chiang, 2012), and
PRO (Hopkins and May, 2011) have improved
the translation quality considerably in the past
decade. These tuning algorithms share the same
characteristic that they treat the decoder as a black
box. This decoding insensitiveness has two ef-
fects: 1) the parameter tuning algorithm can be
more general to choose the most effective decod-
ing paradigm for different language pairs; 2) how-
ever, it also means that the learned parameters may
not fit the decoding algorithm well, so that promis-
ing partial translations might be pruned out due to
the beam search decoding.
</bodyText>
<author confidence="0.934895">
Liang Huang Kai Zhao
</author>
<affiliation confidence="0.965366">
Queens College &amp; Graduate Center
City University of New York
</affiliation>
<email confidence="0.889167">
{lianghuang.sh,kzhao.hf}@gmail.com
</email>
<bodyText confidence="0.984735418604651">
Recent researches reveal that the parameter tun-
ing algorithms tailored for specific decoding al-
gorithms can be beneficial in general structured
prediction problems (Huang et al., 2012), and in
machine translation (Yu et al., 2013; Zhao et al.,
2014; Liu and Huang, 2014). Particularly, Liu and
Huang (2014) show that by requiring the conven-
tional parameter tuning algorithms to consider the
final decoding results (full translations) as well as
the intermediate decoding states (partial transla-
tions) at the same time, the inexact decoding can
be significantly improved so that correct interme-
diate partial translations are more likely to survive
the beam. However, the underlying phrase-based
decoding model suffers from limited distortion,
and thus, may not be flexible enough for translat-
ing language pairs that are syntactically different,
which require long distance reordering.
In order to better handle long distance reorder-
ing which beyonds the capability of phrase-based
MT, we extend the search-aware tuning frame-
work from phrase-based MT to syntax-based MT,
in particular the hierarchical phrase-based transla-
tion model (HIERO) (Chiang, 2007).
One key advantage of search-aware tuning for
previous phrase-based MT is the minimal change
to existing parameter tuning algorithms, which
is achieved by defining BLEU-like metrics for
the intermediate decoding states with sequence-
structured derivations. To keep our approach
simple, we generalize these BLEU-like metrics
to handle intermediate decoding states with tree-
structured derivations in HIERO, which are cal-
culated by dynamic programming algorithms in-
spired by the inside-outside algorithm.
We make the following contributions:
1. We extend the framework of search-aware
tuning methods from phrase-based transla-
tion to syntax-based translation. This exten-
sion is simple to be applied to most conven-
tional parameter tuning methods, requiring
minimal extra changes to existing algorithms.
1282
</bodyText>
<note confidence="0.9956695">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1282–1291,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<listItem confidence="0.6163332">
2. We propose two BLEU metrics and their vari-
ants to evaluate partial derivations. In order
to efficiently compute the new BLEU metrics,
we investigate dynamic programming based
algorithms to recover the good partial deriva-
tions for search-aware tuning.
3. Our method obtains significant improve-
ments on large-scale Chinese-to-English
translation on top of MERT, MIRA and PRO
baselines.
</listItem>
<sectionHeader confidence="0.913169" genericHeader="method">
2 Hierarchical Phrase-based Decoding
</sectionHeader>
<bodyText confidence="0.99997525">
We first briefly describe the hierarchical phrase-
based translation (Chiang, 2007), HIERO, by a
Chinese-English translation example (Figure 1).
A HIERO decoder translates a source sentence
by using a synchronous grammar (Figure 1 (a)) to
parse it into a bilingual derivation tree, as shown
in Figure 1 (b) and Figure 1 (c). An example rule
of the synchronous grammar is
</bodyText>
<equation confidence="0.995768">
r8 : X —* (X1 qian X2, X2 before X1),
</equation>
<bodyText confidence="0.998885846153846">
where the left-hand-side is a non-terminal symbol,
and the right-hand-side is a pair of source and tar-
get strings, each of which consists a sequence of
lexical terminals and non-terminals. Specifically,
the same subscript on both sides denotes a one-to-
one correspondence between their non-terminals.
We use |r |to denote the arity of rule r, i.e., the
number of non-terminals. Usually, the rule used in
HIERO system has a maximum arity 2.
Let (x, y) be a Chinese-English sentence pair
in tuning set, the HIERO decoder will generate a
derivation tree for it (Figure 1 (b)), which can be
defined recursively in a functional way:
</bodyText>
<equation confidence="0.999936333333333">
r |r |= 0
r(d1) |r |= 1 (1)
r(d1, d2) |r |= 2 ,
</equation>
<bodyText confidence="0.997227533333333">
where d is a (partial) derivation, i.e., the (sub-)
derivation tree. When r is a fully lexicalized rule
(|r |= 0), the decoder generates a tree node di-
rectly (e.g., X[3,5] in Figure 1 (b)). If |r |&gt; 0, a
new (partial) derivation will be created by apply-
ing r to its children nodes. In our notation, we
denote this process as applying a function of rule
r to its arguments. For example, node X[1,5] in
Figure 1(b) is created by applying rule r8 in Fig-
ure 1(a), where the arguments are d1 = X[1,2] and
d2 = X[3,5] respectively.
Practically, we organize the partial derivations
based on spans in HIERO decoder, and use a beam
B[i,j] to keep the k-best partial derivations for each
span [i, j]:
</bodyText>
<equation confidence="0.88516">
B[i,j] = topkw (D[i,j]),
</equation>
<bodyText confidence="0.999874">
where D[i,j] is the set of all possible partial deriva-
tions for span [i, j], and topk w(�) returns the top k
ones according to the current model w. Figure 2
shows an example of the HIERO beam-search de-
coding.
</bodyText>
<sectionHeader confidence="0.898717" genericHeader="method">
3 Search-Aware Tuning for HIERO
</sectionHeader>
<bodyText confidence="0.99995584">
As we discussed in Section 1, current tuning meth-
ods for HIERO system (MERT, MIRA, or PRO) are
mostly search-agnostic. They only consider the
complete translations in final beam B[1,|x|], but ig-
nore the partial ones in the intermediate beams.
However, because MT decoding is inexact (beam-
search), many potentially promising derivations
might be pruned before reaching the final beam
(e.g., the partial derivation X[1,5] of Figure 1(b) is
pruned in beam B[1,5] in Figure 2). Consequently,
once we lose these good partial derivations in de-
coding, it is hard to promote them by these search-
agnostic tuning methods.
In order to address this problem, search-aware
tuning (Liu and Huang, 2014) aims to promote not
only the accurate complete translations in the final
beam, but more importantly those promising par-
tial derivations in non-final beams.
In this section, we apply search-aware tuning to
HIERO system. Similar to the phrase-based one,
the key challenge here is also the evaluation of par-
tial derivations. Following (Liu and Huang, 2014),
we design two different evaluation metrics, partial
BLEU and potential BLEU respectively for HIERO
decoding.
</bodyText>
<subsectionHeader confidence="0.994158">
3.1 Partial BLEU
</subsectionHeader>
<bodyText confidence="0.9999359">
Given a partial derivation d of span [i, j], partial
BLEU evaluates it by comparing its partial trans-
lation e(d) directly with the (full) reference. We
explore two different partial BLEU measures here:
full partial BLEU and span partial BLEU.
Full partial BLEU is similar to the one used in
(Liu and Huang, 2014), which compares e(d) with
the full reference y and computes BLEU score us-
ing an adjusted effective reference length propor-
tional to the length of the source span [i, j] (i.e.,
</bodyText>
<figure confidence="0.96696345">
⎧
⎨
⎩
d =
1283
id rule S[0,5]
r0
S → hX1, X1i
before getting married
X
X
Berlin
X
in li
S
qian 3 zhuzai 4
bolin
S[0,1]
X[0,1]
0 Aimi 1
</figure>
<equation confidence="0.9522118">
X[1,2]
jiehun 2
X[1,5]
X
S → hS1X1, S1X1i
X → hAimi, Amyi
X → hAimi, Amy hasi
X → hzhuzai bolin, lived in Berlini
X → hzhuzai bolin, living in Berlini
X → hqian, beforei
X → hqian, formeri
X → hjiehun, got marriedi
X → hjiehun, getting marriedi
X → hX1 qian X2,X2 before X1i
X → hX1 qian X2,X1 before X2i
1
r
r2
[3 ,5]
r2
r3
r3
r4
r5
r6
r7
r8
r9
t 5
1284
</equation>
<bodyText confidence="0.999887625">
and Huang (2014) introduce a future string f(d)
to denote the translation of the untranslated part,
and get the potential of d by concatenating e(d)
and f(d).
Different from their work, we define an outside
string for d in HIERO system. Suppose a complete
translation generated from d is ¯e(d), it can be de-
composed as follows:
</bodyText>
<equation confidence="0.994432">
¯e(d) = η(d) o e(d) o ξ(d) (2)
</equation>
<bodyText confidence="0.9979368">
where o is an operator that concatenates strings
monotonically, η(d) and ξ(d) are the left and right
parts of ¯e(d) apart from e(d), which we call left
and right future strings of d. The pair of η(d) and
ξ(d) is defined as the outside string of d:
</bodyText>
<equation confidence="0.985439">
O(d) = (η(d), ξ(d))
</equation>
<bodyText confidence="0.957289142857143">
An example of the outside string is shown in Fig-
ure 3. Assume we are evaluating the partial deriva-
tion which translates Chinese word “qian” to En-
glish word “before”, and get a complete transla-
tion “Amy lived in Berlin before getting married”
from it, then its outside string would be:
(Amy lived in Berlin, getting married)
</bodyText>
<figure confidence="0.478413666666667">
jiehun qian zhuzai bolin
lived in Berlin before getting married
⌘(d) e(d) ⇠(d)
</figure>
<figureCaption confidence="0.843848">
Figure 3: Example of outside string when we
</figureCaption>
<bodyText confidence="0.98107">
use“Amy lived in Berlin before getting married” as
the potential for the partial derivation which trans-
lates word “qian” to “before”.
Theoretically, different partial derivations of the
same span could have different outside strings. To
simplify the problem, we use the same outside
string for all partial derivations of the same span.
The outside string for span [i, j] is defined as:
</bodyText>
<equation confidence="0.987215">
O([i,j]) = (η([i,j]), ξ([i,j]))
</equation>
<bodyText confidence="0.999873571428571">
For a specific partial derivation d of span [i, j],
by combining O([i, j]) and e(d), we can compute
its potential BLEU against the reference. Appar-
ently, different outside string will lead to different
potential BLEU score. In the rest of this section,
we will explore three different methods to gener-
ate outside strings.
</bodyText>
<subsectionHeader confidence="0.923838">
3.2.1 Concatenation
</subsectionHeader>
<bodyText confidence="0.999992571428571">
In order to generate outside string, one simple and
straightforward way is concatenation. For a spe-
cific span [i, j], we first get the best translation
of its adjacent spans [0, i] and [j, |x|] (e([0, i]) and
e([j, |x|]) respectively).3 Then for a partial deriva-
tion d of span [i, j], we generate the outside string
of d by concatenation
</bodyText>
<equation confidence="0.999864333333333">
η([i,j]) = e([0,i])
ξ([i,j]) = e([j, |x|])
¯ex(d) = e([0, i]) o e(d) o e([j, |x|])
</equation>
<bodyText confidence="0.998315444444445">
Also take the example of Figure 3, if we per-
form concatenation on it, the outside string of the
corresponding partial derivation is:
(Amy getting married, lived in Berlin).
Obviously, this outside string is not good, since it
does not consider the reordering between spans. In
order to incorporate reordering into outside string,
we propose a new top-down algorithm in the next
subsection.
</bodyText>
<subsectionHeader confidence="0.967122">
3.2.2 Top-Down
</subsectionHeader>
<bodyText confidence="0.9512468">
Top-down method (Algorithm 1) is defined over
the derivation tree, and takes the complete transla-
tions in the final beam as the potential for comput-
ing potential BLEU.
Suppose we have a partial derivation d =
r(d1, d2) as shown in Formula 1, and the target
side string of rule r is:
w1 ··· wp X1 wq ··· wm X2 wn ··· wl
The corresponding (partial) translation e(d) of d
could be generated by applying an r-based func-
tion er(·) with d1 and d2 as the arguments:
er(d1, d2) = w1···wp e(d1) wq···wm e(d2) wn···wl
where e(d1) and e(d2) are the partial translations
of d1 and d2. We can further decompose er(d1, d2)
based on e(d1):
</bodyText>
<equation confidence="0.9936544">
 |{z }
w1 ··· wp e(d1) wq ··· wm e(d2) wn ··· wl
 |{z } .
ξd(d1)
ηd(d1)
</equation>
<bodyText confidence="0.6837392">
where we call ηd(d1) and ξd(d1) the partial left
and right future strings of d1 under d. Similar
3For the spans not having translations, we compute a best
possible translation for each of them by concatenating the
translations of its children spans monotonically.
</bodyText>
<figure confidence="0.772262944444445">
Aimi
Amy
r
{
:
&lt;
r
{
:
1285
Algorithm 1 Top-Down Outside String.
1: function TOPDOWN(d)
2: r ← the rule that generates d
3: for each non-terminal x in rule r do
4: 77(dx) = 77(d) ◦ 77d(dx)
5: ξ(dx) = ξd(dx) ◦ ξ(d)
6: TOPDOWN(dx)
7: return
</figure>
<bodyText confidence="0.997572333333333">
to the outside cost of inside-outside algorithm, we
compute the outside string (η(d1), ξ(d1)) for d1
based on the decomposition:
</bodyText>
<equation confidence="0.959806142857143">
η(d1) = η(d) o ηd(d1)
ξ(d1) = ξd(d1) o ξ(d).
Similarly, if we decompose er(d1, d2) based on
e(d2) we have:
w1 ··· wp e(d1) wQ ··· wm
� �� �
ηd(d2)
</equation>
<bodyText confidence="0.960338">
and the outside string (η(d2), ξ(d2)) for d2 is:
</bodyText>
<equation confidence="0.9993445">
η(d2) = η(d) o ηd(d2)
ξ(d2) = ξd(d2) o ξ(d).
</equation>
<bodyText confidence="0.994512631578947">
The top-down method works based on the above
decompositions. For a complete translation in the
final beam B[0,|x|], the algorithm tracebacks on its
derivation tree, and gets the outside string for all
spans in the derivation. Taking the span [1, 2] in
Figure 1(b) as an example, if we do top-down, its
outside string would be:
(Amy lived in Berlin before, E).
where c denotes the empty string. Compared to
the concatenation method, top-down is able to
consider the reordering between spans, and thus
would be much better. However, since it bases on
the k-best list in the final beam, it can only handle
the spans appearing in the final k-best derivations.
In our experiments, top-down algorithm only cov-
ers about 30% of all spans.4 In order to incorpo-
rate more spans into search-aware tuning, we en-
hance the top-down algorithm and propose guided
backtrace algorithm in the next subsection.
</bodyText>
<subsectionHeader confidence="0.797441">
3.2.3 Guided Backtrace
</subsectionHeader>
<bodyText confidence="0.981070288461538">
The guided backtrace algorithm is a variation of
top-down method. In guided backtrace, we first
4We do not consider the hypothesis recombination in top-
down algorithm.
introduce a container s[i,j] to keep a best partial
derivation for each split point of [i, j] during de-
coding.5 Hence, there will be at most j − i par-
tial derivations in s[i,j].6 For instance, suppose the
span is [2, 5], we will keep three partial derivations
in s[i,j] for split point 2, 3, and 4 respectively. Dif-
ferent from the similar k-best list in the decoding
beam, s[i j] introduces more diverse partial deriva-
tions for backtracing, and thus could help to cover
more spans.
After decoding, we employ algorithm 2 to do
backtrace. The algorithm starts from the best
translation in the final beam. At first, we get the
corresponding span of the input partial derivation
d (line 2), and the outside string for this span
(line 5-6). We then sort the partial derivations
in s[i,j] based on their potential Bleu+1 (line 12).
Thus, the sorting will guide us to first backtrace
the partial derivations with better potential Bleu+1
scores. Then we traverse all these partial deriva-
tions (line 13-14), and do guided backtrace recur-
sively on its child partial derivations (line 16-19).
In this process, about 90% of all spans are visited
in our experiments. We demand each span will
only be visited once (line 3-4),
During the above process, guided backtrace will
collect good partial derivations which have bet-
ter potential Bleu+1 scores than the best Bleu+1
score MaxSenBleu of the final beam (line 10-11)
into goodcands. Meanwhile, we also collect the
good partial derivations from descendant nodes
(line 19), and apply rule r to them to form good
partial derivations for span [i, j] (line 20-21). At
last, we add the top 50 good partial derivations to
the beam B[i,j] for tuning (line 22).
The purpose of adding good partial derivations
for tuning is to recover the good ones pruned out
of the beam. For example, the partial derivation
X[1,5] in Figure 1 has been pruned out of the beam
B[1,5] in Figure 2. If we only consider the partial
derivations in the beam, it is still hard to promote
it. After adding good partial derivations, we will
have more good targets to do better tuning.
From Algorithm 2, we can also get a new way
to compute oracle BLEU score of a translation sys-
tem. We memorize the string that has the maxi-
mum potential Bleu+1 score of all strings (line 9).
We then collect the best string of all source sen-
</bodyText>
<footnote confidence="0.707892">
5If the partial derivation is generated by HIERO rules, we
set the split point as the last position of the first non-terminal.
6Some split points might not have corresponding partial
</footnote>
<figure confidence="0.5249508">
derivations.
e(d2) wn ··· w:
v
ξd(d2)
,
1286
Algorithm 2 Guided Backtrace Outside String.
1: function GUIDED(d)
2: [i, j] +— the source span that d belongs to
3: if [i, j] has been visited then
4: return 0
5: 77([i, j]) = 77(d)
6: ξ([i, j]) = ξ(d)
7: goodcands +— 0
8: for each partial derivation d in B[i,j] do
9: bleu = Bleu+1(77([i, j]) o e(d) o ξ([i, j]))
10: if bleu &gt; MaxSenBleu then
11: add d to goodcands
12: sort s[i,j] based on potential Bleu+1
13: for each partial derivation d of span [i, j] do
14: r +— the rule that generates d
15: Dd +— the set of partial derivations r use to form d
16: for each non-terminal x in rule r do
17: 77(dx) = 77([i, j]) o 77d(dx)
18: ξ(dx) = ξd(dx) o ξ([i, j])
19: cands +— GUIDED(dx)
20: for each cand d9 in cands do
21: add r(d9, Dk\dx) to goodcands
22: add top 50 partial derivations of goodcands to B[i,j]
23: return goodcands
</figure>
<bodyText confidence="0.99969225">
tences in the tuning or test set, and use them to
compute BLEU score of the set. This can be seen
as an approximation upper bound of the current
model and decoder, which we call guided oracle.
</bodyText>
<subsectionHeader confidence="0.994163">
3.3 Implementation
</subsectionHeader>
<bodyText confidence="0.99999255">
The major process of search-aware tuning is sim-
ilar to traditional tuning pipeline. We first decode
the source sentence, and then output both the com-
plete translations in the final beam and the partial
derivations from the shorter spans as training in-
stances. For potential BLEU, the outputs of par-
tial derivations would be the corresponding com-
plete translations generated by Equation (2). If we
use partial BLEU, the outputs are the correspond-
ing partial translations. Each span will serve as
a single tuning instance. Different from (Liu and
Huang, 2014), we only use the features from par-
tial derivations for tuning.
For the spans that top-down or guided backtrace
algorithm cannot get outside strings, we use con-
catenation for them to maintain consistent number
of tuning instances between different tuning iter-
ations. However, since we do not want to spent
much effort on them, we only use the one-best par-
tial derivation for each of them.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999994">
To evaluate our method, we conduct experiments
on Chinese-to-English translation. The train-
ing data includes 1.8M bilingual sentence pairs,
with about 40M Chinese words and 48M English
words. We generate symmetric word alignment
using GIZA++ and the grow-diag-final-and strat-
egy. We train a 4-gram language model on the
Xinhua portion of English Gigaword corpus by
SRILM toolkit (Stolcke, 2002). We use BLEU
4 with “average reference length” to evaluate the
translation performance for all experiments.
We use the NIST MT 2002 evaluation data
(878 sentences) as the tuning set, and adopt NIST
MT04 (1,789 sentences), MT05 (1,082 sentences),
MT06 (616 sentences from new portion) and
MT08 (691 sentences from new portion) data as
the test set.
Our baseline system is an in-house hierarchical
phrase-based system. The translation rules are ex-
tracted with Moses toolkit (Koehn et al., 2007) by
default settings. For the decoder, we set the beam
size to 30, nbest list to 50, and 20 as the maximum
length of spans for using non-glue rules.
The baseline tuning methods are batch tun-
ing methods based on k-best translations, includ-
ing MERT (Och, 2003), MIRA (Cherry and Fos-
ter, 2012) and PRO (Hopkins and May, 2011)
from Moses. Another baseline tuning method is
hypergraph-MERT from cdec toolkit (Dyer et al.,
2010). To guarantee the tuning efficiency, we con-
strain the minimum length of spans for search-
aware tuning to restrict the number of training in-
stances. For the sentences with less than 20 words,
we only use the spans longer than 0.75 times sen-
tence length. For the ones with more than 20
words, the minimum span length is set to 18.7 All
results are achieved by averaging three indepen-
dent runs for fair comparison (Clark et al., 2011).
</bodyText>
<subsectionHeader confidence="0.996912">
4.1 Translation Results
</subsectionHeader>
<bodyText confidence="0.979845">
Table 1 compares the main results of our MERT-
based search-aware tuning with traditional tuning:
MERT and hypergraph-MERT.
From the results, we can see that hypergraph-
MERT is better than MERT by 0.5 BLEU points,
verifying the result of (Kumar et al., 2009). For
search-aware tuning, partial BLEU (both full and
span one) only gets comparable results with base-
line tuning method, confirming our previous anal-
ysis in section 3.1, and the results are also consis-
tent with (Liu and Huang, 2014).
7As the decoder demands that spans longer than 20 can
only be translated by glue rule, for these spans, we only need
to consider the ones beginning with 0 in search-aware tuning.
</bodyText>
<table confidence="0.9713111">
1287
nist03 nist04 nist05 nist06 nist08 avg.
MERT 34.4 35.9 34.2 31.9 28.2 32.9
MERT-S 34.3 36.1 34.3 32.3 28.5 33.1
hypergraph-MERT 34.5 36.4 34.4 33.0 28.9 33.4
full partial BLEU 34.3 35.9 34.1 32.4 28.1 33.0
span partial BLEU 34.1 36.1 34.5 32.5 28.4 33.1
concatenation 34.3 36.4 34.6 33.1 28.5 33.4
top-down 34.5 36.6 34.5 33.1 28.9 33.5
guided backtrace 34.9 36.9 35.2 33.7 29.1 34.0
</table>
<tableCaption confidence="0.967617666666667">
Table 1: MERT results: BLEU scores the test sets (nist03, nist04, nist05, nist06, and nist08). MERT-S is
an enhanced version of MERT, which uses a beam size 200 and nbest list 200 for tuning, and beam size
30 for testing.
</tableCaption>
<table confidence="0.999826857142857">
methods nist02 nist04
1-best MERT 36.1 35.9
guided backtrace +0.5 +1.0
k-best Oracle MERT 43.3 42.8
guided backtrace +1.1 +1.2
Guided Oracle MERT 48.0 47.2
guided backtrace +1.4 +1.4
</table>
<tableCaption confidence="0.993518">
Table 2: The oracle BLEU comparison between
</tableCaption>
<bodyText confidence="0.936641125">
baseline MERT and guided backtrace.
Compared to partial BLEU, potential BLEU is
more helpful. Both concatenation and top-down
method are better than MERT on all five test sets.
Guided backtrace gets the best performance over
all methods. It outperforms traditional MERT
by 1.1 BLEU points on average, and better than
hypergraph-MERT by 0.6 BLEU points.
</bodyText>
<subsectionHeader confidence="0.998457">
4.2 Analysis
</subsectionHeader>
<bodyText confidence="0.997435357142857">
In this section, we analyze the results of search-
aware tuning by comparing MERT-based baseline
and guided backtrace in detail.
Oracle BLEU We first compare the oracle
BLEU scores of baseline and guided backtrace in
Table 2. In order to get the k-best oracle, we first
look for the best Bleu+1 translation in the k-best
list for each source sentence, and then use these
best translations to compute the BLEU score of the
entire set. To get the guided oracle, we use the
weights from baseline MERT to run Algorithm 2
on tuning set (nist02) and nist04 test set, and gen-
erate the best oracle translation (section 3.2.3) for
each source sentence for evaluation.
</bodyText>
<tableCaption confidence="0.618868">
The final oracle BLEU comparison is shown in
Table 2. On both nist02 tuning set and nist04 test
set, guided backtrace method gains at lease 1.0
</tableCaption>
<table confidence="0.804812666666667">
nist02 nist04
MERT 0.3960 0.3791
guided backtrace 0.4098 0.3932
</table>
<tableCaption confidence="0.901294">
Table 3: The diversity comparison based on k-best
</tableCaption>
<bodyText confidence="0.996059095238095">
list in the final beam on both tuning set (nist02)
and nist04 test set. The higher the value is, the
more diverse the k-best list is.
BLEU points improvements over traditional MERT
on both k-best oracle and guided oracle. More-
over, k-best and guided oracle get more improve-
ments than 1-best, indicating that by search-aware
tuning, the decoder could generate a better k-best
list, and has a higher upper bound.
Diversity As shown in (Gimpel et al., 2013;
Liu and Huang, 2014), diversity is important for
MT tuning. An k-best list with higher diversity
can better represent the entire decoding space, and
thus tuning on it will lead to a better performance.
Similar as (Liu and Huang, 2014), our method
encourages the diversity of partial translations in
each beam, including the ones in the final beam.
We use the metric in (Liu and Huang, 2014) to
compare the diversity of traditional MERT and
guided backtrace. The metric is based on the n-
gram matches between two sentences y and y&apos;:
</bodyText>
<equation confidence="0.999122666666667">
(yi:i+q = y&apos;j:j+q)
2 x Δ(y, y&apos;)
d(y, y&apos;) = 1 − Δ(y, y) + Δ(y&apos;, y&apos;),
</equation>
<bodyText confidence="0.97873275">
where q = n − 1 and (x) equals to 1 if x is true, 0
otherwise. The final diversity results are shown in
Table 3. We can see guided backtrace gets a better
diversity than traditional MERT.
</bodyText>
<figure confidence="0.93957456">
Δ(y, y&apos;) = −
|y|−q
i=1
|y&apos;|−q
�
j=1
1288
BLEU
2 4 8 16 30
Beam Size
0.75L 0.5L 0.25L 1
Minimum Span Length
37
36
35
34
33
Guided Backtrace
MERT
Guided Backtrace
MERT
BLEU 38
37
36
35
</figure>
<figureCaption confidence="0.793314">
Figure 4: The BLEU comparison between MERT
and guided backtrace on nist04 test set over differ-
ent beam sizes.
</figureCaption>
<bodyText confidence="0.996469105263158">
Beam Size As we discussed before, search-
aware tuning helps to accommodate search errors
in decoding, and promotes good partial deriva-
tions. Thus, we believe that even with a small
beam, these good partial derivations can still sur-
vive with search-aware tuning, resulting in a good
translation quality. Figure 4 compares the results
of different beam sizes (2, 4, 8, 16, 30) between
traditional MERT and guided backtrace. The com-
parison shows that guided backtrace achieves bet-
ter result than baseline MERT, and when the beam
is smaller, the improvement is bigger. More-
over, guided backtrace method with a beam size
8 could achieve comparable BLEU score to tradi-
tional MERT with beam size 30.
Span Size For a big tuning set, in order to make
the tuning tractable, we constrain the length of
spans for search-aware tuning. Intuitively, towards
search-aware tuning, more spans will get better re-
sults because we have more training instances to
guide the tuning process, and accommodate search
errors in early decoding stage (shorter spans). To
verify this intuition, we perform experiments on
a small tuning set, which is generated by select-
ing the sentences with less than 20 words from the
original NIST MT 02 tuning set.
Figure 5 compares the results of traditional
MERT and guided backtrace over different mini-
mum span length. The curve in the figure shows
that by using more spans for search-aware tuning,
we can achieve better translation performance.
Figure 5: The BLEU comparison between MERT
and guided backtrace on nist04 test set over dif-
ferent span sizes. L denotes the sentence length.
The value of x-axis refers to the minimum length
of spans used for search-aware tuning. More spans
will be used in tuning with smaller minimum span
length.
</bodyText>
<subsectionHeader confidence="0.975663">
4.3 MIRA and PRO Results
</subsectionHeader>
<bodyText confidence="0.999879285714286">
Table 4 and Table 5 shows the final results of
MIRA and PRO for traditional tuning and search-
aware tuning using potential BLEU. We can see
that potential BLEU is helpful for tuning. Guided
backtrace is also the best one, which outperforms
the baseline MIRA and PRO by 0.9 and 0.8 BLEU
points on average.
</bodyText>
<subsectionHeader confidence="0.960313">
4.4 Efficiency
</subsectionHeader>
<bodyText confidence="0.999858882352941">
Table 6 shows the training time comparisons be-
tween search-aware tuning and traditional tuning.
From this Table, we can see that search-aware tun-
ing slows down the training speed.8 The slow
training is due to three reasons.
Similar to (Liu and Huang, 2014), as search-
aware tuning considers partial translations of
spans besides the complete translations, it has
much more training instances than traditional tun-
ing. In our experiments, although we have con-
strained the length of spans, we get a total of
38,539 instances for search-aware tuning, while it
is 878 for traditional tuning.
Another time consuming factor is the compu-
tation of Bleu+1 scores. In guided backtrace set-
ting, we need to compute the Bleu+1 scores for all
candidates of spans we consider. This is why the
</bodyText>
<footnote confidence="0.895212">
8Since the tuning set is different for traditional PRO tuning
and search-aware tuning, we do not compare them here.
</footnote>
<table confidence="0.978856333333333">
1289
nist03 nist04 nist05 nist06 nist08 avg.
MIRA 34.5 36.1 34.4 32.1 28.5 33.1
concatenation 34.5 36.5 34.6 33.6 28.9 33.6
top-down 34.4 36.6 34.6 33.5 28.8 33.6
guided backtrace 35.0 36.9 35.1 33.7 29.1 34.0
</table>
<tableCaption confidence="0.986959">
Table 4: MIRA results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08).
</tableCaption>
<table confidence="0.999905">
nist03 nist04 nist05 nist06 nist08 avg.
PRO 34.2 36.1 33.9 32.3 28.7 33.1
concatenation 34.6 36.2 34.7 32.3 28.5 33.2
top-down 34.8 36.4 34.7 32.7 29.0 33.5
guided backtrace 35.0 36.8 34.7 33.4 29.6 33.9
</table>
<tableCaption confidence="0.53330975">
Table 5: PRO results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). Similar to (Liu
and Huang, 2014), there is also an monster phenomenon (Nakov et al., 2013) in our search-aware tuning
setting. Therefore, here we perform search-aware tuning on only 109 short sentences (with less than 10
words) from nist02.
</tableCaption>
<table confidence="0.997546833333333">
MERT MIRA
Total Optim. Total Optim.
baseline 17 2 17 2
concatenation 59 44 38 23
top-down 31 14 23 8
guided backtrace 108 45 69 22
</table>
<tableCaption confidence="0.989464">
Table 6: Comparison on training efficiency. The
</tableCaption>
<bodyText confidence="0.997556642857143">
time (in minutes) is measured at the last iteration
of tuning. Column “Total” refers to the time for an
entire iteration, while “Optim.” is the time of op-
timization. We use a parallelized MERT for tuning
by 24 cores.
decoding of guided backtrace is much slower than
baseline or top-down.
Finally, adding good candidates enlarges the k-
best lists of training instances, which further slows
down the tuning process of guided backtrace.
It should be noted that although search-aware
tuning is slower than traditional tuning method,
since the decoding is all the same for them in test-
ing time, it does not slow down the testing speed.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999983384615385">
We have presented an extension of “search-aware
tuning” to accommodate search errors in hier-
archical phrase-based decoding and promote the
promising partial derivations so that they are more
likely to survive in the inexact beam search. In
order to handle the tree-structured derivations for
HIERO system, we generalize the BLEU metrics
and propose corresponding partial BLEU and po-
tential BLEU to evaluate partial derivations. Our
approach can be used as a plugin for most popu-
lar parameter tuning algorithms including MERT,
MIRA and PRO. Extensive experiments confirmed
substantial BLEU gains from our method.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9998252">
We thank the three anonymous reviewers for the
valuable suggestions. This project was supported
in part by DARPA FA8750-13-2-0041 (DEFT),
NSF IIS-1449278, and a Google Faculty Research
Award.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991668125">
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montr´eal, Canada, June. Association for
Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–208.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159–1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
1290
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL.
Michel Galley, Chris Quirk, Colin Cherry, and Kristina
Toutanova. 2013. Regularized minimum error rate
training. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1948–1959, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory
Shakhnarovich. 2013. A systematic exploration of
diversity in machine translation. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1100–1111, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL: Demonstrations.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 163–171. Association for
Computational Linguistics.
Lemao Liu and Liang Huang. 2014. Search-aware
tuning for machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1942–
1952, Doha, Qatar, October. Association for Com-
putational Linguistics.
Preslav Nakov, Francisco Guzm´an, and Stephan Vogel.
2013. A tale about pro and monsters. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 12–17, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901–904.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
EMNLP.
Kai Zhao, Liang Huang, Haitao Mi, and Abe Itty-
cheriah. 2014. Hierarchical mt training using max-
violation perceptron. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
785–790, Baltimore, Maryland, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.724511">
1291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858903">
<title confidence="0.999847">Search-Aware Tuning for Hierarchical Phrase-based Decoding</title>
<author confidence="0.998066">Feifei Zhai</author>
<affiliation confidence="0.97689">Queens College City University of New York</affiliation>
<email confidence="0.999509">ffzhai2012@gmail.com</email>
<abstract confidence="0.999897086956522">Parameter tuning is a key problem for statistical machine translation (SMT). Most popular parameter tuning algorithms for SMT are agnostic of decoding, resulting in parameters vulnerable to search errors in decoding. The recent research of “search-aware tuning” (Liu and Huang, 2014) addresses this problem by considering the partial derivations in every decoding step so that the promising ones are more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English transshow significant improve-</abstract>
<intro confidence="0.904746">on</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1236" citStr="Cherry and Foster, 2012" startWordPosition="187" endWordPosition="190">oach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Z</context>
<context position="19739" citStr="Cherry and Foster, 2012" startWordPosition="3361" endWordPosition="3365">nces) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 com</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="1211" citStr="Chiang et al., 2009" startWordPosition="183" endWordPosition="186">. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search de</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3089" citStr="Chiang, 2007" startWordPosition="471" endWordPosition="472">can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, which require long distance reordering. In order to better handle long distance reordering which beyonds the capability of phrase-based MT, we extend the search-aware tuning framework from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2007). One key advantage of search-aware tuning for previous phrase-based MT is the minimal change to existing parameter tuning algorithms, which is achieved by defining BLEU-like metrics for the intermediate decoding states with sequencestructured derivations. To keep our approach simple, we generalize these BLEU-like metrics to handle intermediate decoding states with treestructured derivations in HIERO, which are calculated by dynamic programming algorithms inspired by the inside-outside algorithm. We make the following contributions: 1. We extend the framework of search-aware tuning methods fro</context>
<context position="4592" citStr="Chiang, 2007" startWordPosition="682" endWordPosition="683"> pages 1282–1291, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2. We propose two BLEU metrics and their variants to evaluate partial derivations. In order to efficiently compute the new BLEU metrics, we investigate dynamic programming based algorithms to recover the good partial derivations for search-aware tuning. 3. Our method obtains significant improvements on large-scale Chinese-to-English translation on top of MERT, MIRA and PRO baselines. 2 Hierarchical Phrase-based Decoding We first briefly describe the hierarchical phrasebased translation (Chiang, 2007), HIERO, by a Chinese-English translation example (Figure 1). A HIERO decoder translates a source sentence by using a synchronous grammar (Figure 1 (a)) to parse it into a bilingual derivation tree, as shown in Figure 1 (b) and Figure 1 (c). An example rule of the synchronous grammar is r8 : X —* (X1 qian X2, X2 before X1), where the left-hand-side is a non-terminal symbol, and the right-hand-side is a pair of source and target strings, each of which consists a sequence of lexical terminals and non-terminals. Specifically, the same subscript on both sides denotes a one-toone correspondence bet</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>J. Machine Learning Research (JMLR),</journal>
<pages>13--1159</pages>
<contexts>
<context position="1251" citStr="Chiang, 2012" startWordPosition="191" endWordPosition="192">anslation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens Coll</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. J. Machine Learning Research (JMLR), 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational</booktitle>
<pages>1290</pages>
<contexts>
<context position="20302" citStr="Clark et al., 2011" startWordPosition="3461" endWordPosition="3464">cluding MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our MERTbased search-aware tuning with traditional tuning: MERT and hypergraph-MERT. From the results, we can see that hypergraphMERT is better than MERT by 0.5 BLEU points, verifying the result of (Kumar et al., 2009). For search-aware tuning, partial BLEU (both full and span one) only gets comparable results with baseline tuning method, confirming our previous analysis in section 3.1, and the results are also consistent with (Liu and Huang, 2014). 7As the decoder demands that spans longer than 20 can only be translated by glue ru</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational 1290</rawString>
</citation>
<citation valid="true">
<title>Linguistics: Human Language Technologies,</title>
<date></date>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker></marker>
<rawString>Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="19871" citStr="Dyer et al., 2010" startWordPosition="3383" endWordPosition="3386">1 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our MERTbased search-aware tuning with traditional tuning: MERT and hypergraph-MERT. From the results, we </context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Regularized minimum error rate training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1948--1959</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1161" citStr="Galley et al., 2013" startWordPosition="174" endWordPosition="177">re more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translat</context>
</contexts>
<marker>Galley, Quirk, Cherry, Toutanova, 2013</marker>
<rawString>Michel Galley, Chris Quirk, Colin Cherry, and Kristina Toutanova. 2013. Regularized minimum error rate training. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1948–1959, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Dhruv Batra</author>
<author>Chris Dyer</author>
<author>Gregory Shakhnarovich</author>
</authors>
<title>A systematic exploration of diversity in machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1100--1111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="23570" citStr="Gimpel et al., 2013" startWordPosition="4014" endWordPosition="4017">est set, guided backtrace method gains at lease 1.0 nist02 nist04 MERT 0.3960 0.3791 guided backtrace 0.4098 0.3932 Table 3: The diversity comparison based on k-best list in the final beam on both tuning set (nist02) and nist04 test set. The higher the value is, the more diverse the k-best list is. BLEU points improvements over traditional MERT on both k-best oracle and guided oracle. Moreover, k-best and guided oracle get more improvements than 1-best, indicating that by search-aware tuning, the decoder could generate a better k-best list, and has a higher upper bound. Diversity As shown in (Gimpel et al., 2013; Liu and Huang, 2014), diversity is important for MT tuning. An k-best list with higher diversity can better represent the entire decoding space, and thus tuning on it will lead to a better performance. Similar as (Liu and Huang, 2014), our method encourages the diversity of partial translations in each beam, including the ones in the final beam. We use the metric in (Liu and Huang, 2014) to compare the diversity of traditional MERT and guided backtrace. The metric is based on the ngram matches between two sentences y and y&apos;: (yi:i+q = y&apos;j:j+q) 2 x Δ(y, y&apos;) d(y, y&apos;) = 1 − Δ(y, y) + Δ(y&apos;, y&apos;),</context>
</contexts>
<marker>Gimpel, Batra, Dyer, Shakhnarovich, 2013</marker>
<rawString>Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory Shakhnarovich. 2013. A systematic exploration of diversity in machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100–1111, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1284" citStr="Hopkins and May, 2011" startWordPosition="195" endWordPosition="198"> translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens College &amp; Graduate Center City Univer</context>
<context position="19771" citStr="Hopkins and May, 2011" startWordPosition="3368" endWordPosition="3371"> NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our ME</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2120" citStr="Huang et al., 2012" startWordPosition="324" endWordPosition="327">1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens College &amp; Graduate Center City University of New York {lianghuang.sh,kzhao.hf}@gmail.com Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for t</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL:</booktitle>
<publisher>Demonstrations.</publisher>
<contexts>
<context position="19449" citStr="Koehn et al., 2007" startWordPosition="3309" endWordPosition="3312">d strategy. We train a 4-gram language model on the Xinhua portion of English Gigaword corpus by SRILM toolkit (Stolcke, 2002). We use BLEU 4 with “average reference length” to evaluate the translation performance for all experiments. We use the NIST MT 2002 evaluation data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less th</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of ACL: Demonstrations.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20583" citStr="Kumar et al., 2009" startWordPosition="3507" endWordPosition="3510">e tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our MERTbased search-aware tuning with traditional tuning: MERT and hypergraph-MERT. From the results, we can see that hypergraphMERT is better than MERT by 0.5 BLEU points, verifying the result of (Kumar et al., 2009). For search-aware tuning, partial BLEU (both full and span one) only gets comparable results with baseline tuning method, confirming our previous analysis in section 3.1, and the results are also consistent with (Liu and Huang, 2014). 7As the decoder demands that spans longer than 20 can only be translated by glue rule, for these spans, we only need to consider the ones beginning with 0 in search-aware tuning. 1287 nist03 nist04 nist05 nist06 nist08 avg. MERT 34.4 35.9 34.2 31.9 28.2 32.9 MERT-S 34.3 36.1 34.3 32.3 28.5 33.1 hypergraph-MERT 34.5 36.4 34.4 33.0 28.9 33.4 full partial BLEU 34.3</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 163–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Liang Huang</author>
</authors>
<title>Search-aware tuning for machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="2206" citStr="Liu and Huang, 2014" startWordPosition="340" endWordPosition="343">coding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens College &amp; Graduate Center City University of New York {lianghuang.sh,kzhao.hf}@gmail.com Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, which require long distanc</context>
<context position="7237" citStr="Liu and Huang, 2014" startWordPosition="1147" endWordPosition="1150">em (MERT, MIRA, or PRO) are mostly search-agnostic. They only consider the complete translations in final beam B[1,|x|], but ignore the partial ones in the intermediate beams. However, because MT decoding is inexact (beamsearch), many potentially promising derivations might be pruned before reaching the final beam (e.g., the partial derivation X[1,5] of Figure 1(b) is pruned in beam B[1,5] in Figure 2). Consequently, once we lose these good partial derivations in decoding, it is hard to promote them by these searchagnostic tuning methods. In order to address this problem, search-aware tuning (Liu and Huang, 2014) aims to promote not only the accurate complete translations in the final beam, but more importantly those promising partial derivations in non-final beams. In this section, we apply search-aware tuning to HIERO system. Similar to the phrase-based one, the key challenge here is also the evaluation of partial derivations. Following (Liu and Huang, 2014), we design two different evaluation metrics, partial BLEU and potential BLEU respectively for HIERO decoding. 3.1 Partial BLEU Given a partial derivation d of span [i, j], partial BLEU evaluates it by comparing its partial translation e(d) direc</context>
<context position="18159" citStr="Liu and Huang, 2014" startWordPosition="3103" endWordPosition="3106">l and decoder, which we call guided oracle. 3.3 Implementation The major process of search-aware tuning is similar to traditional tuning pipeline. We first decode the source sentence, and then output both the complete translations in the final beam and the partial derivations from the shorter spans as training instances. For potential BLEU, the outputs of partial derivations would be the corresponding complete translations generated by Equation (2). If we use partial BLEU, the outputs are the corresponding partial translations. Each span will serve as a single tuning instance. Different from (Liu and Huang, 2014), we only use the features from partial derivations for tuning. For the spans that top-down or guided backtrace algorithm cannot get outside strings, we use concatenation for them to maintain consistent number of tuning instances between different tuning iterations. However, since we do not want to spent much effort on them, we only use the one-best partial derivation for each of them. 4 Experiments To evaluate our method, we conduct experiments on Chinese-to-English translation. The training data includes 1.8M bilingual sentence pairs, with about 40M Chinese words and 48M English words. We ge</context>
<context position="20817" citStr="Liu and Huang, 2014" startWordPosition="3546" endWordPosition="3549">8.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1 Translation Results Table 1 compares the main results of our MERTbased search-aware tuning with traditional tuning: MERT and hypergraph-MERT. From the results, we can see that hypergraphMERT is better than MERT by 0.5 BLEU points, verifying the result of (Kumar et al., 2009). For search-aware tuning, partial BLEU (both full and span one) only gets comparable results with baseline tuning method, confirming our previous analysis in section 3.1, and the results are also consistent with (Liu and Huang, 2014). 7As the decoder demands that spans longer than 20 can only be translated by glue rule, for these spans, we only need to consider the ones beginning with 0 in search-aware tuning. 1287 nist03 nist04 nist05 nist06 nist08 avg. MERT 34.4 35.9 34.2 31.9 28.2 32.9 MERT-S 34.3 36.1 34.3 32.3 28.5 33.1 hypergraph-MERT 34.5 36.4 34.4 33.0 28.9 33.4 full partial BLEU 34.3 35.9 34.1 32.4 28.1 33.0 span partial BLEU 34.1 36.1 34.5 32.5 28.4 33.1 concatenation 34.3 36.4 34.6 33.1 28.5 33.4 top-down 34.5 36.6 34.5 33.1 28.9 33.5 guided backtrace 34.9 36.9 35.2 33.7 29.1 34.0 Table 1: MERT results: BLEU sc</context>
<context position="23592" citStr="Liu and Huang, 2014" startWordPosition="4018" endWordPosition="4021">race method gains at lease 1.0 nist02 nist04 MERT 0.3960 0.3791 guided backtrace 0.4098 0.3932 Table 3: The diversity comparison based on k-best list in the final beam on both tuning set (nist02) and nist04 test set. The higher the value is, the more diverse the k-best list is. BLEU points improvements over traditional MERT on both k-best oracle and guided oracle. Moreover, k-best and guided oracle get more improvements than 1-best, indicating that by search-aware tuning, the decoder could generate a better k-best list, and has a higher upper bound. Diversity As shown in (Gimpel et al., 2013; Liu and Huang, 2014), diversity is important for MT tuning. An k-best list with higher diversity can better represent the entire decoding space, and thus tuning on it will lead to a better performance. Similar as (Liu and Huang, 2014), our method encourages the diversity of partial translations in each beam, including the ones in the final beam. We use the metric in (Liu and Huang, 2014) to compare the diversity of traditional MERT and guided backtrace. The metric is based on the ngram matches between two sentences y and y&apos;: (yi:i+q = y&apos;j:j+q) 2 x Δ(y, y&apos;) d(y, y&apos;) = 1 − Δ(y, y) + Δ(y&apos;, y&apos;), where q = n − 1 and (</context>
<context position="27004" citStr="Liu and Huang, 2014" startWordPosition="4617" endWordPosition="4620">inimum span length. 4.3 MIRA and PRO Results Table 4 and Table 5 shows the final results of MIRA and PRO for traditional tuning and searchaware tuning using potential BLEU. We can see that potential BLEU is helpful for tuning. Guided backtrace is also the best one, which outperforms the baseline MIRA and PRO by 0.9 and 0.8 BLEU points on average. 4.4 Efficiency Table 6 shows the training time comparisons between search-aware tuning and traditional tuning. From this Table, we can see that search-aware tuning slows down the training speed.8 The slow training is due to three reasons. Similar to (Liu and Huang, 2014), as searchaware tuning considers partial translations of spans besides the complete translations, it has much more training instances than traditional tuning. In our experiments, although we have constrained the length of spans, we get a total of 38,539 instances for search-aware tuning, while it is 878 for traditional tuning. Another time consuming factor is the computation of Bleu+1 scores. In guided backtrace setting, we need to compute the Bleu+1 scores for all candidates of spans we consider. This is why the 8Since the tuning set is different for traditional PRO tuning and search-aware t</context>
<context position="28273" citStr="Liu and Huang, 2014" startWordPosition="4829" endWordPosition="4832">ist04 nist05 nist06 nist08 avg. MIRA 34.5 36.1 34.4 32.1 28.5 33.1 concatenation 34.5 36.5 34.6 33.6 28.9 33.6 top-down 34.4 36.6 34.6 33.5 28.8 33.6 guided backtrace 35.0 36.9 35.1 33.7 29.1 34.0 Table 4: MIRA results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). nist03 nist04 nist05 nist06 nist08 avg. PRO 34.2 36.1 33.9 32.3 28.7 33.1 concatenation 34.6 36.2 34.7 32.3 28.5 33.2 top-down 34.8 36.4 34.7 32.7 29.0 33.5 guided backtrace 35.0 36.8 34.7 33.4 29.6 33.9 Table 5: PRO results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). Similar to (Liu and Huang, 2014), there is also an monster phenomenon (Nakov et al., 2013) in our search-aware tuning setting. Therefore, here we perform search-aware tuning on only 109 short sentences (with less than 10 words) from nist02. MERT MIRA Total Optim. Total Optim. baseline 17 2 17 2 concatenation 59 44 38 23 top-down 31 14 23 8 guided backtrace 108 45 69 22 Table 6: Comparison on training efficiency. The time (in minutes) is measured at the last iteration of tuning. Column “Total” refers to the time for an entire iteration, while “Optim.” is the time of optimization. We use a parallelized MERT for tuning by 24 co</context>
</contexts>
<marker>Liu, Huang, 2014</marker>
<rawString>Lemao Liu and Liang Huang. 2014. Search-aware tuning for machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1942– 1952, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzm´an</author>
<author>Stephan Vogel</author>
</authors>
<title>A tale about pro and monsters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>12--17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Nakov, Guzm´an, Vogel, 2013</marker>
<rawString>Preslav Nakov, Francisco Guzm´an, and Stephan Vogel. 2013. A tale about pro and monsters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 12–17, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1139" citStr="Och, 2003" startWordPosition="172" endWordPosition="173">sing ones are more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that prom</context>
<context position="19707" citStr="Och, 2003" startWordPosition="3358" endWordPosition="3359">on data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules. The baseline tuning methods are batch tuning methods based on k-best translations, including MERT (Och, 2003), MIRA (Cherry and Foster, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit (Dyer et al., 2010). To guarantee the tuning efficiency, we constrain the minimum length of spans for searchaware tuning to restrict the number of training instances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sentence length. For the ones with more than 20 words, the minimum span length is set to 18.7 All results are achieved by averaging three independent runs for fair comparison (Clark et al., 2011). 4.1</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="18956" citStr="Stolcke, 2002" startWordPosition="3232" endWordPosition="3233">maintain consistent number of tuning instances between different tuning iterations. However, since we do not want to spent much effort on them, we only use the one-best partial derivation for each of them. 4 Experiments To evaluate our method, we conduct experiments on Chinese-to-English translation. The training data includes 1.8M bilingual sentence pairs, with about 40M Chinese words and 48M English words. We generate symmetric word alignment using GIZA++ and the grow-diag-final-and strategy. We train a 4-gram language model on the Xinhua portion of English Gigaword corpus by SRILM toolkit (Stolcke, 2002). We use BLEU 4 with “average reference length” to evaluate the translation performance for all experiments. We use the NIST MT 2002 evaluation data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set. Our baseline system is an in-house hierarchical phrase-based system. The translation rules are extracted with Moses toolkit (Koehn et al., 2007) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1190" citStr="Watanabe et al., 2007" startWordPosition="179" endWordPosition="182">e inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO. 1 Introduction Efforts in parameter tuning algorithms for SMT, such as MERT (Och, 2003; Galley et al., 2013), MIRA (Watanabe et al., 2007; Chiang et al., 2009; Cherry and Foster, 2012; Chiang, 2012), and PRO (Hopkins and May, 2011) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two effects: 1) the parameter tuning algorithm can be more general to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2165" citStr="Yu et al., 2013" startWordPosition="332" endWordPosition="335">eral to choose the most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens College &amp; Graduate Center City University of New York {lianghuang.sh,kzhao.hf}@gmail.com Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactica</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Abe Ittycheriah</author>
</authors>
<title>Hierarchical mt training using maxviolation perceptron.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>785--790</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2184" citStr="Zhao et al., 2014" startWordPosition="336" endWordPosition="339">e most effective decoding paradigm for different language pairs; 2) however, it also means that the learned parameters may not fit the decoding algorithm well, so that promising partial translations might be pruned out due to the beam search decoding. Liang Huang Kai Zhao Queens College &amp; Graduate Center City University of New York {lianghuang.sh,kzhao.hf}@gmail.com Recent researches reveal that the parameter tuning algorithms tailored for specific decoding algorithms can be beneficial in general structured prediction problems (Huang et al., 2012), and in machine translation (Yu et al., 2013; Zhao et al., 2014; Liu and Huang, 2014). Particularly, Liu and Huang (2014) show that by requiring the conventional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial translations) at the same time, the inexact decoding can be significantly improved so that correct intermediate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translating language pairs that are syntactically different, whic</context>
</contexts>
<marker>Zhao, Huang, Mi, Ittycheriah, 2014</marker>
<rawString>Kai Zhao, Liang Huang, Haitao Mi, and Abe Ittycheriah. 2014. Hierarchical mt training using maxviolation perceptron. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 785–790, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002488">
<title confidence="0.994974">
Semi-Supervised Bootstrapping of Relationship Extractors with
Distributional Semantics
</title>
<author confidence="0.948659">
David S. Batista Bruno Martins M´ario J. Silva
</author>
<affiliation confidence="0.878287">
INESC-ID, Instituto Superior T´ecnico, Universidade de Lisboa
</affiliation>
<email confidence="0.98203">
{david.batista,bruno.g.martins,mario.gaspar.silva}@ist.utl.pt
</email>
<sectionHeader confidence="0.997164" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998159307692308">
Semi-supervised bootstrapping techniques
for relationship extraction from text iter-
atively expand a set of initial seed rela-
tionships while limiting the semantic drift.
We research bootstrapping for relationship
extraction using word embeddings to find
similar relationships. Experimental results
show that relying on word embeddings
achieves a better performance on the task
of extracting four types of relationships
from a collection of newswire documents
when compared with a baseline using TF-
IDF to find similar relationships.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999796444444445">
Relationship Extraction (RE) transforms unstruc-
tured text into relational triples, each represent-
ing a relationship between two named-entities. A
bootstrapping system for RE starts with a collec-
tion of documents and a few seed instances. The
system scans the document collection, collecting
occurrence contexts for the seed instances. Then,
based on these contexts, the system generates ex-
traction patterns. The documents are scanned
again using the patterns to match new relation-
ship instances. These newly extracted instances
are then added to the seed set, and the process is
repeated until a certain stop criteria is met.
The objective of bootstrapping is thus to expand
the seed set with new relationship instances, while
limiting the semantic drift, i.e. the progressive de-
viation of the semantics for the extracted relation-
ships from the semantics of the seed relationships.
State-of-the-art approaches rely on word vec-
tor representations with TF-IDF weights (Salton
and Buckley, 1988). However expanding the seed
set by relying on TF-IDF representations to find
similar instances has limitations, since the similar-
ity between any two relationship instance vectors
of TF-IDF weights is only positive when the in-
stances share at least one term. For instance, the
phrases was founded by and is the co-founder of
do not have any common words, but they have the
same semantics. Stemming techniques can aid in
these cases, but only for variations of the same root
word (Porter, 1980).
We propose to address this challenge with an
approach based on word embeddings (Mikolov et
al., 2013a). By relying on word embeddings, the
similarity of two phrases can be captured even
if no common words exist. The word embed-
dings for co-founder and founded should be sim-
ilar, since these words tend to occur in the same
contexts. Word embeddings can nonetheless also
introduce semantic drift. When using word em-
beddings, phrases like studied history at can, for
instance, have a high similarity with phrases like
history professor at. In our approach, we control
the semantic drift by ranking the extracted rela-
tionship instances, and by scoring the generated
extraction patterns.
We implemented these ideas in BREDS, a boot-
strapping system for RE based on word embed-
dings. BREDS was evaluated with a collection of
1.2 million sentences from news articles. The ex-
perimental results show that our method outper-
forms a baseline bootstrapping system based on
the ideas of Agichtein and Gravano (2000) which
relies on TF-IDF representations.
</bodyText>
<sectionHeader confidence="0.879043" genericHeader="method">
2 Bootstrapping Relationship Extractors
</sectionHeader>
<bodyText confidence="0.999870888888889">
Brin (1999) developed DIPRE, the first system to
apply bootstrapping for RE, which represents the
occurrences of seeds as three contexts of strings:
words before the first entity (BEF), words between
the two entities (BET), and words after the second
entity (AFT). DIPRE generates extraction patterns
by grouping contexts based on string matching,
and controls semantic drift by limiting the number
of instances a pattern can extract.
</bodyText>
<page confidence="0.988633">
499
</page>
<note confidence="0.6603075">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 499–504,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997715">
Agichtein and Gravano (2000) developed
Snowball, which is inspired on DIPRE’s method
of collecting three contexts for each occurrence,
but computing a TF-IDF representation for each
context. The seed contexts are clustered with a
single-pass algorithm based on the cosine similar-
ity between contexts using the three vector repre-
sentations:
</bodyText>
<equation confidence="0.994540666666667">
Sim(Sn, Sj) = α · cos(BEFi, BEFj)
+ Q · cos(BETi, BETj) (1)
+ γ · cos(AFTi, AFTj)
</equation>
<bodyText confidence="0.99995884">
In the formula, the parameters α, Q and γ weight
each vector. An extraction pattern is represented
by the centroid of the vectors that form a cluster.
The patterns are used to scan the text again, and
for each segment of text where any pair of enti-
ties with the same semantic types as the seeds co-
occur, three vectors are generated. If the similarity
from the context vectors towards an extraction pat-
tern is greater than a threshold τaim, the instance
is extracted.
Snowball scores the patterns and ranks the ex-
tracted instances to control the semantic drift. A
pattern is scored based on the instances that it ex-
tracted, which can be included in three sets: P,
N, and U. If an extracted instance contains an
entity e1, which is part of a seed, and if the asso-
ciated entity e2 in the instance is the same as in
in the seed, then the extraction is considered pos-
itive (included in set P). If the relationship con-
tradicts a relationship in the seed set (i.e., e2 does
not match), then the extraction is considered neg-
ative (included in a set N). If the relationship is
not part of the seed set, the extraction is consid-
ered unknown (included in a set U). A score is
assigned to each pattern p according to:
</bodyText>
<equation confidence="0.999979">
Confp(p) = |P |+Wngt · |N|+Wunk · |U|
|P  |(2)
</equation>
<bodyText confidence="0.9903116">
Wngt and Wunk are weights associated to the neg-
ative and unknown extractions, respectively. The
confidence of an instance is calculated based on
the similarity scores towards the patterns that ex-
tracted it, weighted by the pattern’s confidence:
</bodyText>
<equation confidence="0.988673">
Confι(i) = 1 − � ISI (1 − Confp(ξj) × Sim(Ci, ξj))
j=0
(3)
</equation>
<bodyText confidence="0.99947725">
where, ξ is the set of patterns that extracted i, and
Ci is the textual context where i occurred. In-
stances with a confidence above a threshold τt are
used as seeds in the next iteration.
</bodyText>
<sectionHeader confidence="0.9154715" genericHeader="method">
3 Bootstrapping Relationship Extractors
with Word Embeddings
</sectionHeader>
<bodyText confidence="0.999894428571429">
BREDS follows the architecture of Snowball,
having the same processing phases: find seed
matches, generating extraction patterns, finding
relationship instances, and detecting semantic
drift. It differs, however, in that it attempts to find
similar relationships using word embeddings, in-
stead of relying on TF-IDF representations.
</bodyText>
<subsectionHeader confidence="0.999477">
3.1 Find Seed Matches
</subsectionHeader>
<bodyText confidence="0.984359903225807">
BREDS scans the document collection and, if both
entities of a seed instance co-occur in a text seg-
ment within a sentence, then that segment is con-
sidered and BREDS extracts the three textual con-
texts as in Snowball: BEF, BET, and AFT.
In the BET context, BREDS tries to identify
a relational pattern based on a shallow heuristic
originally proposed in ReVerb (Fader et al., 2011).
The pattern limits a relation context to a verb (e.g.,
invented), a verb followed by a preposition (e.g.,
located in), or a verb followed by nouns, adjec-
tives, or adverbs ending in a preposition (e.g., has
atomic weight of). These patterns will nonetheless
only consider verb mediated relationships. If no
verbs exist between two entities, BREDS extracts
all the words between the two entities, to build the
representations for the BET context.
Each context is transformed into a single vec-
tor by a simple compositional function that starts
by removing stop-words and adjectives and then
sums the word embedding vectors of each indi-
vidual word. Representing small phrases by sum-
ming each individual word’s embedding results in
good representations for the semantics in small
phrases (Mikolov et al., 2013b).
A relationship instance i is represented by three
embedding vectors: VBEF, VBET, and VAFT.
Considering the sentence:
The tech company Soundcloud is based in Berlin,
capital of Germany.
BREDS generates the relationship instance with:
</bodyText>
<equation confidence="0.997325666666667">
VBEF = E(“tech”) + E(“company”)
VBET = E(“is”) + E(“based”)
VAFT = E(“capital”)
</equation>
<page confidence="0.870009">
500
</page>
<bodyText confidence="0.9995949375">
where, E(x) is the word embedding for word x.
BREDS also tries to identify the passive voice
using part-of-speech (PoS) tags, which can help
to detect the correct order of the entities in a rela-
tional triple. BREDS identifies the presence of the
passive voice by considering any form of the verb
to be, followed by a verb in the past tense or the
past participle, and ending in the word by.
For instance, the seed &lt;Google, owns,
DoubleClick&gt; states that the organisation
Google owns the organisation DoubleClick.
Using this seed, if BREDS detects a pattern like
agreed to be acquired by it will swap the order of
the entities when producing a relational triple, out-
putting the triple &lt;ORG2, owns, ORG1&gt;, instead
of the triple &lt;ORG1, owns, ORG2&gt;.
</bodyText>
<subsectionHeader confidence="0.998961">
3.2 Extraction Patterns Generation
</subsectionHeader>
<bodyText confidence="0.999936">
As Snowball, BREDS generates extraction pat-
terns by applying a single-pass clustering algo-
rithm to the relationship instances gathered in the
previous step. Each resulting cluster contains a
set of relationship instances, represented by their
three context vectors.
Algorithm 1 describes the clustering approach
taken by BREDS, which takes as input a list of
relationship instances and assigns the first instance
to a new empty cluster. Next, it iterates through the
list of instances, computing the similarity between
an instance in and every cluster Cly. The instance
in is assigned to the first cluster whose similarity
is higher or equal to a threshold τsim. If all the
clusters have a similarity lower than a threshold
τsim, a new cluster Cm is created, containing the
instance in.
The similarity function Sim(in, Cly), between
an instance in and a cluster Cly, returns the max-
imum of the similarities between an instance in
and any of the instances in a cluster Cly, if the
majority of the similarity scores is higher than a
threshold τsim. A value of zero is returned oth-
erwise. The similarity between two instances is
computed according to Formula (1). As a result,
clustering in Algorithm 1 differs from the original
Snowball method, which instead computes simi-
larities towards cluster centroids.
</bodyText>
<subsectionHeader confidence="0.994887">
3.3 Find Relationship Instances
</subsectionHeader>
<bodyText confidence="0.80227625">
After the generation of extraction patterns,
BREDS finds relationship instances with Algo-
rithm 2. It scans the documents once again, col-
lecting all segments of text containing entity pairs
</bodyText>
<table confidence="0.645090375">
Algorithm 1: Single-Pass Clustering.
Input: Instances = {i1, i2, i3, ..., in }
Output: Patterns = {}
Cl1 = {i1}
Patterns = {Cl1}
for in ∈ Instances do
for Cly ∈ Patterns do
if Sim(in, Cly) &gt;= τsim then
</table>
<equation confidence="0.8044005">
Cly = Cly ∪ {in}
else
Clm = {in}
Patterns = Patterns ∪ {Clm}
</equation>
<bodyText confidence="0.999766705882353">
whose semantic types are the same as those in the
seed instances. For each segment, an instance i
is generated as described in Section 3.1, and the
similarity towards all previously generated extrac-
tion patterns (i.e., clusters) is computed. If the
similarity between i and a pattern Cly is equal
or above τsim, then i is considered a candidate
instance, and the confidence score of the pattern
is updated, according to Formula (2). The pat-
tern which has the highest similarity (patternbest)
is associated with i, along with the correspond-
ing similarity score (simbest). This information
is kept in a history of Candidates. Note that the
histories of Candidates and Patterns are kept
through all the bootstrap iterations, and new pat-
terns or instances can be added, or the scores of
existing patterns or instances can change.
</bodyText>
<figure confidence="0.81771225">
Algorithm 2: Find Relationship Instances.
Input: Sentences = {s1, s2, s3, ..., sn }
Input: Patterns = {Cl1, Cl2, ..., Cln }
Output: Candidates
for si ∈ Sentences do
i = create instance(si)
simbest = 0
pbest = None
for Cli ∈ Patterns do
sim = Sim(i, Cli)
if sim &gt;= τsim then
Confp(Ci)
if sim &gt;= simbest then
simbest = sim
Pbest = Cli
Candidates[i].patterns[pbest] = simbest
</figure>
<page confidence="0.96319">
501
</page>
<subsectionHeader confidence="0.848477">
3.4 Semantic Drift Detection
</subsectionHeader>
<bodyText confidence="0.999887666666667">
As Snowball, BREDS ranks the candidate in-
stances at the end of each iteration, based on the
scores computed with Formula (3). Instances with
a score equal or above the threshold Tt are added
to the seed set, for use in the next iteration of the
bootstrapping algorithm.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999554243243243">
In our evaluation we used a set of 5.5 million news
articles from AFP and APW (Parker et al., 2011).
Our pre-processing pipeline is based on the
models provided by the NLTK toolkit (Bird et
al., 2009): sentence segmentation1, tokenisa-
tion2, PoS-tagging3 and named-entity recognition
(NER). The NER module in NLTK is a wrapper
over the Stanford NER toolkit (Finkel et al., 2005).
We performed weak entity-linking by matching
entity names in sentences with FreebaseEasy (Bast
et al., 2014). FreebaseEasy is a processed version
of Freebase (Bollacker et al., 2008), which con-
tains a unique meaningful name for every entity,
together with canonical binary relations. For our
experiments, we selected only the sentences con-
taining at least two entities linked to FreebaseEasy,
which corresponded to 1.2 million sentences.
With the full articles set, we computed word
embeddings with the skip-gram model4 using
the word2vec5 implementation from Mikolov et.
al. (2013a). The TF-IDF representations used by
Snowball were calculated over the same articles
set. We adopted a previously proposed framework
for the evaluation of large-scale RE systems by
Bronzi et al. (2012), to estimate precision and re-
call, using FreebaseEasy as the knowledge base.
We considered entity pairs no further away than
6 tokens, and a window of 2 tokens for the BEF
and AFT contexts, ignoring the remaining of the
sentence. We discarded the clusters with only one
relationship instances, and ran a maximum of 4
bootstrapping iterations. The Wunk and Wngt pa-
rameters were set to 0.1 and 2, respectively, based
on the results reported by Yu et al. (2003).
We compared BREDS against Snowball in four
relationship types, shown in Table 1. For each re-
lationship type we considered several bootstrap-
</bodyText>
<footnote confidence="0.999371">
1nltk.tokenize.punkt.PunktSentenceTokenizer
2nltk.tokenize.treebank.TreebankWordTokenizer
3taggers/maxent treebank pos tagger/english.pickle
4skip length of 5 tokens and vectors of 200 dimensions
5https://code.google.com/p/word2vec/
</footnote>
<figure confidence="0.771502166666667">
Relationship Seeds
{Adidas, Reebok}
acquired
{Google, DoubleClick}
{CNN, Ted Turner}
founder-of
{Amazon, Jeff Bezos}
{Nokia, Espoo}
headquartered
{Pfizer, New York}
affiliation {Google, Marissa Mayer}
{Xerox, Ursula Burns}
</figure>
<tableCaption confidence="0.99634">
Table 1: Relationship types and used seeds.
</tableCaption>
<bodyText confidence="0.9999026">
ping configurations by combining different values
for the Taim and Tt thresholds, all within the inter-
val [0.5,1.0].
We bootstrapped each relationship with two
context weighting configurations in Formula (1):
</bodyText>
<listItem confidence="0.9999145">
• Conf1: α = 0.0, Q = 1.0, -y = 0.0
• Conf2: α = 0.2, Q = 0.6, -y = 0.2
</listItem>
<bodyText confidence="0.999782064516129">
where Conf1 only considers the BET context and
Conf2 uses the three contexts, while giving more
importance to the BET context.
Table 2 shows, for each relationship type, the
best F1 score and the corresponding precision and
recall, for all combinations of Taim and Tt val-
ues, and considering only extracted relationship
instances with confidence scores equal or above
0.5. Table 2a shows the results for the BREDS sys-
tem, while Table 2b shows the results for Snow-
ball (ReVerb), a modified Snowball in which a re-
lational pattern based on ReVerb is used to select
the words for the BET context. Finally, Table 2c
shows the results for Snowball, implemented as
described in the original paper.
Overall, BREDS achieves better F1 scores than
both versions of Snowball. The F1 score of
BREDS is higher, mainly as a consequence of
much higher recall scores, which we believe to be
due to the relaxed semantic matching caused by
using the word embeddings. For some relation-
ship types, the recall more than doubles when us-
ing word embeddings instead of TF-IDF. For the
acquired relationship, when considering Conf1,
the precision of BREDS drops compared with the
other versions of Snowball, but without affecting
the F1 score, since the higher recall compensates
for the small loss in precision.
Regarding the context weighting configura-
tions, Conf2 produces a lower recall when com-
pared to Conf1. This might be caused by the
</bodyText>
<page confidence="0.993367">
502
</page>
<bodyText confidence="0.999958952380952">
sparsity of both BEF and AFT, which contain
many different words that do not contribute to
capture the relationship between the two enti-
ties. Although, sometimes, the phrase or word
that indicates a relationship occurs on the BEF or
AFT contexts, it is more often the case that these
phrases or words occur in the BET context.
The performance results of Snowball (Clas-
sic) and Snowball (ReVerb) suggest that selecting
words based on a relational pattern to represent the
BET context, instead of using all the words, works
better for TF-IDF representations.
The results also show that word embeddings
can generate more extraction patterns. For in-
stance, for the founder-of relationship, BREDS
learns patterns based on words such as founder, co-
founder, co-founders or founded, while Snowball
only learns patterns that have the word founder,
like CEO and founder or founder and chairman.
The implementations of BREDS and Snowball,
as described in this paper, are available on-line6.
</bodyText>
<sectionHeader confidence="0.99263" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999890076923077">
This paper reports on a novel bootstrapping sys-
tem for relation extraction based on word em-
beddings. In our experiments, bootstrapped RE
achieved better results when using word embed-
dings to find similar relationships than with simi-
larities between TF-IDF weighted vectors.
We have identified two main sources of errors:
NER problems and incorrect relational patterns
extraction due to the use of a shallow heuristic that
only captures local relationships.
In future work, more robust entity-linking ap-
proaches, as proposed by Hoffart et al. (2011),
could be included in our pre-processing pipeline.
This could alleviate NER errors and enable exper-
imentation with other relationship types.
Gabbard et al. (2011) have shown that co-
reference resolution can increase bootstrapping
RE performance, and the method of Durrett and
Klein (2014) could also be included in our pre-
processing pipeline.
Finally, we could explore richer compositional
functions, combining word embeddings with syn-
tactic dependencies (SD) (Yu et al., 2014). The
shortest path between two entities in an SD tree
supports the extraction of local and long-distance
relationships (Bunescu and Mooney, 2005).
</bodyText>
<footnote confidence="0.977208">
6https://github.com/davidsbatista/
BREDS
</footnote>
<table confidence="0.977075690476191">
BREDS
Relationship Precision Recall F1
Conf1
acquired 0.73 0.77 0.75
founder-of 0.98 0.86 0.91
headquartered 0.63 0.69 0.66
affiliation 0.85 0.91 0.88
Conf2
acquired 1.00 0.15 0.26
founder-of 0.97 0.79 0.87
headquartered 0.64 0.61 0.62
affiliation 0.84 0.60 0.70
(a) Precision, Recall and F1 results obtained with different
configurations of BREDS.
Snowball (ReVerb)
Relationship Precision Recall F1
Conf1
acquired 0.83 0.61 0.70
founder-of 0.96 0.77 0.86
headquartered 0.48 0.63 0.55
affiliation 0.52 0.29 0.37
Conf2
acquired 0.73 0.22 0.34
founder-of 0.97 0.75 0.85
headquartered 0.55 0.42 0.47
affiliation 0.36 0.05 0.08
(b) Precision, Recall and F1 results obtained with different
configurations of Snowball (ReVerb).
Snowball (Classic)
Relationship Precision Recall F1
Conf1
acquired 0.87 0.54 0.67
founder-of 0.97 0.76 0.85
headquartered 0.52 0.61 0.57
affiliation 0.49 0.29 0.36
Conf2
acquired 0.77 0.54 0.63
founder-of 0.98 0.73 0.84
headquartered 0.53 0.54 0.54
affiliation 0.42 0.08 0.13
(c) Precision, Recall and F1 results obtained with different
configurations of Snowball (Classic).
</table>
<tableCaption confidence="0.962454">
Table 2: Precision, Recall and F1 scores over the
four relationships for the three different systems.
</tableCaption>
<page confidence="0.996673">
503
</page>
<sectionHeader confidence="0.838983" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9118989">
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
ACL’05.
This work was supported by Fundac¸˜ao para
a Ciˆencia e a Tecnologia, under grants
SFRH/BD/70478/2010, UID/CEC/50021/2013
and EXCL/EEI- ESS/0257/2012 (DataStorm).
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999549037037037">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting Relations from Large Plain-Text Collec-
tions. In Proceedings of the ACM Conference on
Digital libraries, DL’00.
Hannah Bast, Florian B¨aurle, Bj¨orn Buchhold, and El-
mar Haußmann. 2014. Easy Access to the Freebase
Dataset. In Companion Publication of the 23rd In-
ternational Conference on World Wide Web, WWW
Companion ’14.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database for Structur-
ing Human Knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08.
Sergey Brin. 1999. Extracting Patterns and Relations
from the World Wide Web. In Selected Papers from
the International Workshop on The World Wide Web
and Databases, WebDB’98.
Mirko Bronzi, Zhaochen Guo, Filipe Mesquita, De-
nilson Barbosa, and Paolo Merialdo. 2012. Auto-
matic Evaluation of Relation Extraction Systems on
Large-scale. In Proceedings of the Joint Workshop
on Automatic Knowledge Base Construction and
Web-scale Knowledge Extraction, AKBC-WEKEX
’12.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation ex-
traction. In Proceedings of the Human Language
Technology Conference and the Conference on Em-
pirical Methods in Natural Language Processing,
HLT/EMNLP’05.
Greg Durrett and Dan Klein. 2014. A Joint Model for
Entity Analysis: Coreference, Typing, and Linking.
Transactions of the Association for Computational
Linguistics, 2.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11.
Ryan Gabbard, Marjorie Freedman, and Ralph M.
Weischedel. 2011. Coreference for Learning to Ex-
tract Relations: Yes Virginia, Coreference Matters.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL’11.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust Disambiguation of Named
Entities in Text. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Rep-
resentations in Vector Space. In Proceedings of the
Workshop at the International Conference on Learn-
ing Representations, ICLR’13.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Proceedings of the Conference on Neural
Information Processing Systems, NIPS’13.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion LDC2011T07.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5).
Hong Yu and Eugene Agichtein. 2003. Extracting
Synonymous Gene and Protein Terms from Biolog-
ical Literature. Bioinformatics, 19(suppl 1).
Mo Yu, Matthew R. Gormley, and Mark Dredze. 2014.
Factor-based Compositional Embedding Models. In
Proceedings of the Conference on Neural Informa-
tion Processing Systems, NIPS’14.
</reference>
<page confidence="0.998341">
504
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792852">
<title confidence="0.995045">Semi-Supervised Bootstrapping of Relationship Extractors Distributional Semantics</title>
<author confidence="0.99999">David S Batista Bruno Martins M´ario J Silva</author>
<affiliation confidence="0.889579">INESC-ID, Instituto Superior T´ecnico, Universidade de</affiliation>
<abstract confidence="0.992613214285715">Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. We research bootstrapping for relationship extraction using word embeddings to find similar relationships. Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TF- IDF to find similar relationships.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting Relations from Large Plain-Text Collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACM Conference on Digital libraries, DL’00.</booktitle>
<contexts>
<context position="3299" citStr="Agichtein and Gravano (2000)" startWordPosition="496" endWordPosition="499">troduce semantic drift. When using word embeddings, phrases like studied history at can, for instance, have a high similarity with phrases like history professor at. In our approach, we control the semantic drift by ranking the extracted relationship instances, and by scoring the generated extraction patterns. We implemented these ideas in BREDS, a bootstrapping system for RE based on word embeddings. BREDS was evaluated with a collection of 1.2 million sentences from news articles. The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations. 2 Bootstrapping Relationship Extractors Brin (1999) developed DIPRE, the first system to apply bootstrapping for RE, which represents the occurrences of seeds as three contexts of strings: words before the first entity (BEF), words between the two entities (BET), and words after the second entity (AFT). DIPRE generates extraction patterns by grouping contexts based on string matching, and controls semantic drift by limiting the number of instances a pattern can extract. 499 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Proc</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. In Proceedings of the ACM Conference on Digital libraries, DL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannah Bast</author>
<author>Florian B¨aurle</author>
<author>Bj¨orn Buchhold</author>
<author>Elmar Haußmann</author>
</authors>
<title>Easy Access to the Freebase Dataset.</title>
<date>2014</date>
<booktitle>In Companion Publication of the 23rd International Conference on World Wide Web, WWW Companion ’14.</booktitle>
<marker>Bast, B¨aurle, Buchhold, Haußmann, 2014</marker>
<rawString>Hannah Bast, Florian B¨aurle, Bj¨orn Buchhold, and Elmar Haußmann. 2014. Easy Access to the Freebase Dataset. In Companion Publication of the 23rd International Conference on World Wide Web, WWW Companion ’14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python.</booktitle>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="12386" citStr="Bird et al., 2009" startWordPosition="2015" endWordPosition="2018">en Confp(Ci) if sim &gt;= simbest then simbest = sim Pbest = Cli Candidates[i].patterns[pbest] = simbest 501 3.4 Semantic Drift Detection As Snowball, BREDS ranks the candidate instances at the end of each iteration, based on the scores computed with Formula (3). Instances with a score equal or above the threshold Tt are added to the seed set, for use in the next iteration of the bootstrapping algorithm. 4 Evaluation In our evaluation we used a set of 5.5 million news articles from AFP and APW (Parker et al., 2011). Our pre-processing pipeline is based on the models provided by the NLTK toolkit (Bird et al., 2009): sentence segmentation1, tokenisation2, PoS-tagging3 and named-entity recognition (NER). The NER module in NLTK is a wrapper over the Stanford NER toolkit (Finkel et al., 2005). We performed weak entity-linking by matching entity names in sentences with FreebaseEasy (Bast et al., 2014). FreebaseEasy is a processed version of Freebase (Bollacker et al., 2008), which contains a unique meaningful name for every entity, together with canonical binary relations. For our experiments, we selected only the sentences containing at least two entities linked to FreebaseEasy, which corresponded to 1.2 mi</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08.</booktitle>
<contexts>
<context position="12747" citStr="Bollacker et al., 2008" startWordPosition="2068" endWordPosition="2071">e next iteration of the bootstrapping algorithm. 4 Evaluation In our evaluation we used a set of 5.5 million news articles from AFP and APW (Parker et al., 2011). Our pre-processing pipeline is based on the models provided by the NLTK toolkit (Bird et al., 2009): sentence segmentation1, tokenisation2, PoS-tagging3 and named-entity recognition (NER). The NER module in NLTK is a wrapper over the Stanford NER toolkit (Finkel et al., 2005). We performed weak entity-linking by matching entity names in sentences with FreebaseEasy (Bast et al., 2014). FreebaseEasy is a processed version of Freebase (Bollacker et al., 2008), which contains a unique meaningful name for every entity, together with canonical binary relations. For our experiments, we selected only the sentences containing at least two entities linked to FreebaseEasy, which corresponded to 1.2 million sentences. With the full articles set, we computed word embeddings with the skip-gram model4 using the word2vec5 implementation from Mikolov et. al. (2013a). The TF-IDF representations used by Snowball were calculated over the same articles set. We adopted a previously proposed framework for the evaluation of large-scale RE systems by Bronzi et al. (201</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting Patterns and Relations from the World Wide Web. In</title>
<date>1999</date>
<booktitle>Selected Papers from the International Workshop on The World Wide Web and Databases, WebDB’98.</booktitle>
<contexts>
<context position="3391" citStr="Brin (1999)" startWordPosition="509" endWordPosition="510"> high similarity with phrases like history professor at. In our approach, we control the semantic drift by ranking the extracted relationship instances, and by scoring the generated extraction patterns. We implemented these ideas in BREDS, a bootstrapping system for RE based on word embeddings. BREDS was evaluated with a collection of 1.2 million sentences from news articles. The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations. 2 Bootstrapping Relationship Extractors Brin (1999) developed DIPRE, the first system to apply bootstrapping for RE, which represents the occurrences of seeds as three contexts of strings: words before the first entity (BEF), words between the two entities (BET), and words after the second entity (AFT). DIPRE generates extraction patterns by grouping contexts based on string matching, and controls semantic drift by limiting the number of instances a pattern can extract. 499 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 499–504, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Comput</context>
</contexts>
<marker>Brin, 1999</marker>
<rawString>Sergey Brin. 1999. Extracting Patterns and Relations from the World Wide Web. In Selected Papers from the International Workshop on The World Wide Web and Databases, WebDB’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirko Bronzi</author>
<author>Zhaochen Guo</author>
<author>Filipe Mesquita</author>
<author>Denilson Barbosa</author>
<author>Paolo Merialdo</author>
</authors>
<title>Automatic Evaluation of Relation Extraction Systems on Large-scale.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12.</booktitle>
<contexts>
<context position="13349" citStr="Bronzi et al. (2012)" startWordPosition="2159" endWordPosition="2162">cker et al., 2008), which contains a unique meaningful name for every entity, together with canonical binary relations. For our experiments, we selected only the sentences containing at least two entities linked to FreebaseEasy, which corresponded to 1.2 million sentences. With the full articles set, we computed word embeddings with the skip-gram model4 using the word2vec5 implementation from Mikolov et. al. (2013a). The TF-IDF representations used by Snowball were calculated over the same articles set. We adopted a previously proposed framework for the evaluation of large-scale RE systems by Bronzi et al. (2012), to estimate precision and recall, using FreebaseEasy as the knowledge base. We considered entity pairs no further away than 6 tokens, and a window of 2 tokens for the BEF and AFT contexts, ignoring the remaining of the sentence. We discarded the clusters with only one relationship instances, and ran a maximum of 4 bootstrapping iterations. The Wunk and Wngt parameters were set to 0.1 and 2, respectively, based on the results reported by Yu et al. (2003). We compared BREDS against Snowball in four relationship types, shown in Table 1. For each relationship type we considered several bootstrap</context>
</contexts>
<marker>Bronzi, Guo, Mesquita, Barbosa, Merialdo, 2012</marker>
<rawString>Mirko Bronzi, Zhaochen Guo, Filipe Mesquita, Denilson Barbosa, and Paolo Merialdo. 2012. Automatic Evaluation of Relation Extraction Systems on Large-scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP’05.</booktitle>
<contexts>
<context position="18332" citStr="Bunescu and Mooney, 2005" startWordPosition="2942" endWordPosition="2945">1), could be included in our pre-processing pipeline. This could alleviate NER errors and enable experimentation with other relationship types. Gabbard et al. (2011) have shown that coreference resolution can increase bootstrapping RE performance, and the method of Durrett and Klein (2014) could also be included in our preprocessing pipeline. Finally, we could explore richer compositional functions, combining word embeddings with syntactic dependencies (SD) (Yu et al., 2014). The shortest path between two entities in an SD tree supports the extraction of local and long-distance relationships (Bunescu and Mooney, 2005). 6https://github.com/davidsbatista/ BREDS BREDS Relationship Precision Recall F1 Conf1 acquired 0.73 0.77 0.75 founder-of 0.98 0.86 0.91 headquartered 0.63 0.69 0.66 affiliation 0.85 0.91 0.88 Conf2 acquired 1.00 0.15 0.26 founder-of 0.97 0.79 0.87 headquartered 0.64 0.61 0.62 affiliation 0.84 0.60 0.70 (a) Precision, Recall and F1 results obtained with different configurations of BREDS. Snowball (ReVerb) Relationship Precision Recall F1 Conf1 acquired 0.83 0.61 0.70 founder-of 0.96 0.77 0.86 headquartered 0.48 0.63 0.55 affiliation 0.52 0.29 0.37 Conf2 acquired 0.73 0.22 0.34 founder-of 0.97</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>A Joint Model for Entity Analysis: Coreference, Typing, and Linking.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="17997" citStr="Durrett and Klein (2014)" startWordPosition="2892" endWordPosition="2895">ships than with similarities between TF-IDF weighted vectors. We have identified two main sources of errors: NER problems and incorrect relational patterns extraction due to the use of a shallow heuristic that only captures local relationships. In future work, more robust entity-linking approaches, as proposed by Hoffart et al. (2011), could be included in our pre-processing pipeline. This could alleviate NER errors and enable experimentation with other relationship types. Gabbard et al. (2011) have shown that coreference resolution can increase bootstrapping RE performance, and the method of Durrett and Klein (2014) could also be included in our preprocessing pipeline. Finally, we could explore richer compositional functions, combining word embeddings with syntactic dependencies (SD) (Yu et al., 2014). The shortest path between two entities in an SD tree supports the extraction of local and long-distance relationships (Bunescu and Mooney, 2005). 6https://github.com/davidsbatista/ BREDS BREDS Relationship Precision Recall F1 Conf1 acquired 0.73 0.77 0.75 founder-of 0.98 0.86 0.91 headquartered 0.63 0.69 0.66 affiliation 0.85 0.91 0.88 Conf2 acquired 1.00 0.15 0.26 founder-of 0.97 0.79 0.87 headquartered 0</context>
</contexts>
<marker>Durrett, Klein, 2014</marker>
<rawString>Greg Durrett and Dan Klein. 2014. A Joint Model for Entity Analysis: Coreference, Typing, and Linking. Transactions of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</booktitle>
<contexts>
<context position="6973" citStr="Fader et al., 2011" startWordPosition="1110" endWordPosition="1113">tion patterns, finding relationship instances, and detecting semantic drift. It differs, however, in that it attempts to find similar relationships using word embeddings, instead of relying on TF-IDF representations. 3.1 Find Seed Matches BREDS scans the document collection and, if both entities of a seed instance co-occur in a text segment within a sentence, then that segment is considered and BREDS extracts the three textual contexts as in Snowball: BEF, BET, and AFT. In the BET context, BREDS tries to identify a relational pattern based on a shallow heuristic originally proposed in ReVerb (Fader et al., 2011). The pattern limits a relation context to a verb (e.g., invented), a verb followed by a preposition (e.g., located in), or a verb followed by nouns, adjectives, or adverbs ending in a preposition (e.g., has atomic weight of). These patterns will nonetheless only consider verb mediated relationships. If no verbs exist between two entities, BREDS extracts all the words between the two entities, to build the representations for the BET context. Each context is transformed into a single vector by a simple compositional function that starts by removing stop-words and adjectives and then sums the w</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Marjorie Freedman</author>
<author>Ralph M Weischedel</author>
</authors>
<title>Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL’11.</booktitle>
<contexts>
<context position="17872" citStr="Gabbard et al. (2011)" startWordPosition="2873" endWordPosition="2876">mbeddings. In our experiments, bootstrapped RE achieved better results when using word embeddings to find similar relationships than with similarities between TF-IDF weighted vectors. We have identified two main sources of errors: NER problems and incorrect relational patterns extraction due to the use of a shallow heuristic that only captures local relationships. In future work, more robust entity-linking approaches, as proposed by Hoffart et al. (2011), could be included in our pre-processing pipeline. This could alleviate NER errors and enable experimentation with other relationship types. Gabbard et al. (2011) have shown that coreference resolution can increase bootstrapping RE performance, and the method of Durrett and Klein (2014) could also be included in our preprocessing pipeline. Finally, we could explore richer compositional functions, combining word embeddings with syntactic dependencies (SD) (Yu et al., 2014). The shortest path between two entities in an SD tree supports the extraction of local and long-distance relationships (Bunescu and Mooney, 2005). 6https://github.com/davidsbatista/ BREDS BREDS Relationship Precision Recall F1 Conf1 acquired 0.73 0.77 0.75 founder-of 0.98 0.86 0.91 he</context>
</contexts>
<marker>Gabbard, Freedman, Weischedel, 2011</marker>
<rawString>Ryan Gabbard, Marjorie Freedman, and Ralph M. Weischedel. 2011. Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust Disambiguation of Named Entities in Text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</booktitle>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust Disambiguation of Named Entities in Text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop at the International Conference on Learning Representations, ICLR’13.</booktitle>
<contexts>
<context position="2400" citStr="Mikolov et al., 2013" startWordPosition="349" endWordPosition="352">Salton and Buckley, 1988). However expanding the seed set by relying on TF-IDF representations to find similar instances has limitations, since the similarity between any two relationship instance vectors of TF-IDF weights is only positive when the instances share at least one term. For instance, the phrases was founded by and is the co-founder of do not have any common words, but they have the same semantics. Stemming techniques can aid in these cases, but only for variations of the same root word (Porter, 1980). We propose to address this challenge with an approach based on word embeddings (Mikolov et al., 2013a). By relying on word embeddings, the similarity of two phrases can be captured even if no common words exist. The word embeddings for co-founder and founded should be similar, since these words tend to occur in the same contexts. Word embeddings can nonetheless also introduce semantic drift. When using word embeddings, phrases like studied history at can, for instance, have a high similarity with phrases like history professor at. In our approach, we control the semantic drift by ranking the extracted relationship instances, and by scoring the generated extraction patterns. We implemented th</context>
<context position="7779" citStr="Mikolov et al., 2013" startWordPosition="1239" endWordPosition="1242">reposition (e.g., has atomic weight of). These patterns will nonetheless only consider verb mediated relationships. If no verbs exist between two entities, BREDS extracts all the words between the two entities, to build the representations for the BET context. Each context is transformed into a single vector by a simple compositional function that starts by removing stop-words and adjectives and then sums the word embedding vectors of each individual word. Representing small phrases by summing each individual word’s embedding results in good representations for the semantics in small phrases (Mikolov et al., 2013b). A relationship instance i is represented by three embedding vectors: VBEF, VBET, and VAFT. Considering the sentence: The tech company Soundcloud is based in Berlin, capital of Germany. BREDS generates the relationship instance with: VBEF = E(“tech”) + E(“company”) VBET = E(“is”) + E(“based”) VAFT = E(“capital”) 500 where, E(x) is the word embedding for word x. BREDS also tries to identify the passive voice using part-of-speech (PoS) tags, which can help to detect the correct order of the entities in a relational triple. BREDS identifies the presence of the passive voice by considering any </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In Proceedings of the Workshop at the International Conference on Learning Representations, ICLR’13.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of the Conference on Neural Information Processing Systems, NIPS’13.</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of the Conference on Neural Information Processing Systems, NIPS’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2011</date>
<booktitle>English Gigaword Fifth Edition LDC2011T07.</booktitle>
<contexts>
<context position="12285" citStr="Parker et al., 2011" startWordPosition="1998" endWordPosition="2001"> create instance(si) simbest = 0 pbest = None for Cli ∈ Patterns do sim = Sim(i, Cli) if sim &gt;= τsim then Confp(Ci) if sim &gt;= simbest then simbest = sim Pbest = Cli Candidates[i].patterns[pbest] = simbest 501 3.4 Semantic Drift Detection As Snowball, BREDS ranks the candidate instances at the end of each iteration, based on the scores computed with Formula (3). Instances with a score equal or above the threshold Tt are added to the seed set, for use in the next iteration of the bootstrapping algorithm. 4 Evaluation In our evaluation we used a set of 5.5 million news articles from AFP and APW (Parker et al., 2011). Our pre-processing pipeline is based on the models provided by the NLTK toolkit (Bird et al., 2009): sentence segmentation1, tokenisation2, PoS-tagging3 and named-entity recognition (NER). The NER module in NLTK is a wrapper over the Stanford NER toolkit (Finkel et al., 2005). We performed weak entity-linking by matching entity names in sentences with FreebaseEasy (Bast et al., 2014). FreebaseEasy is a processed version of Freebase (Bollacker et al., 2008), which contains a unique meaningful name for every entity, together with canonical binary relations. For our experiments, we selected onl</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edition LDC2011T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2298" citStr="Porter, 1980" startWordPosition="334" endWordPosition="335">ionships. State-of-the-art approaches rely on word vector representations with TF-IDF weights (Salton and Buckley, 1988). However expanding the seed set by relying on TF-IDF representations to find similar instances has limitations, since the similarity between any two relationship instance vectors of TF-IDF weights is only positive when the instances share at least one term. For instance, the phrases was founded by and is the co-founder of do not have any common words, but they have the same semantics. Stemming techniques can aid in these cases, but only for variations of the same root word (Porter, 1980). We propose to address this challenge with an approach based on word embeddings (Mikolov et al., 2013a). By relying on word embeddings, the similarity of two phrases can be captured even if no common words exist. The word embeddings for co-founder and founded should be similar, since these words tend to occur in the same contexts. Word embeddings can nonetheless also introduce semantic drift. When using word embeddings, phrases like studied history at can, for instance, have a high similarity with phrases like history professor at. In our approach, we control the semantic drift by ranking the</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="1805" citStr="Salton and Buckley, 1988" startWordPosition="249" endWordPosition="252">he system generates extraction patterns. The documents are scanned again using the patterns to match new relationship instances. These newly extracted instances are then added to the seed set, and the process is repeated until a certain stop criteria is met. The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships. State-of-the-art approaches rely on word vector representations with TF-IDF weights (Salton and Buckley, 1988). However expanding the seed set by relying on TF-IDF representations to find similar instances has limitations, since the similarity between any two relationship instance vectors of TF-IDF weights is only positive when the instances share at least one term. For instance, the phrases was founded by and is the co-founder of do not have any common words, but they have the same semantics. Stemming techniques can aid in these cases, but only for variations of the same root word (Porter, 1980). We propose to address this challenge with an approach based on word embeddings (Mikolov et al., 2013a). B</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing and Management, 24(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Eugene Agichtein</author>
</authors>
<date>2003</date>
<booktitle>Extracting Synonymous Gene and Protein Terms from Biological Literature. Bioinformatics, 19(suppl 1).</booktitle>
<marker>Yu, Agichtein, 2003</marker>
<rawString>Hong Yu and Eugene Agichtein. 2003. Extracting Synonymous Gene and Protein Terms from Biological Literature. Bioinformatics, 19(suppl 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew R Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based Compositional Embedding Models.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Neural Information Processing Systems, NIPS’14.</booktitle>
<contexts>
<context position="18186" citStr="Yu et al., 2014" startWordPosition="2920" endWordPosition="2923">uristic that only captures local relationships. In future work, more robust entity-linking approaches, as proposed by Hoffart et al. (2011), could be included in our pre-processing pipeline. This could alleviate NER errors and enable experimentation with other relationship types. Gabbard et al. (2011) have shown that coreference resolution can increase bootstrapping RE performance, and the method of Durrett and Klein (2014) could also be included in our preprocessing pipeline. Finally, we could explore richer compositional functions, combining word embeddings with syntactic dependencies (SD) (Yu et al., 2014). The shortest path between two entities in an SD tree supports the extraction of local and long-distance relationships (Bunescu and Mooney, 2005). 6https://github.com/davidsbatista/ BREDS BREDS Relationship Precision Recall F1 Conf1 acquired 0.73 0.77 0.75 founder-of 0.98 0.86 0.91 headquartered 0.63 0.69 0.66 affiliation 0.85 0.91 0.88 Conf2 acquired 1.00 0.15 0.26 founder-of 0.97 0.79 0.87 headquartered 0.64 0.61 0.62 affiliation 0.84 0.60 0.70 (a) Precision, Recall and F1 results obtained with different configurations of BREDS. Snowball (ReVerb) Relationship Precision Recall F1 Conf1 acqui</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew R. Gormley, and Mark Dredze. 2014. Factor-based Compositional Embedding Models. In Proceedings of the Conference on Neural Information Processing Systems, NIPS’14.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000515">
<title confidence="0.895826">
Learning Timeline Difference for Text Categorization
</title>
<author confidence="0.981092">
Fumiyo Fukumoto
</author>
<affiliation confidence="0.990303333333333">
Graduate Faculty of
Interdisciplinary Research
Univ. of Yamanashi, Japan
</affiliation>
<email confidence="0.99782">
fukumoto@yamanashi.ac.jp
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902583333333">
This paper addresses text categorization
problem that training data may derive
from a different time period from the test
data. We present a learning framework
which extends a boosting technique to
learn accurate model for timeline adapta-
tion. The results showed that the method
was comparable to the current state-of-the-
art biased-SVM method, especially the
method is effective when the creation time
period of the test data differs greatly from
the training data.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999746769230769">
Text categorization supports and improves several
tasks such as creating digital libraries, informa-
tion retrieval, and even helping users to interact
with search engines (Mourao et al., 2008). A
growing number of machine learning (ML) tech-
niques have been applied to the text categorization
task (Xue et al., 2008; Gopal and Yang, 2010).
Each document is represented using a vector of
features/terms (Yang and Pedersen, 1997; Hassan
et al., 2007). Then, the documents with category
label are used to train classifiers. Once category
models are trained, each test document is classi-
fied by using these models. A basic assumption
in the categorization task is that the distributions
of terms between training and test documents are
identical. When the assumption does not hold,
the classification accuracy is worse. However, it
is often the case that the term distribution in the
training data is different from that of the test data
when the training data may drive from a different
time period from the test data. Manual annotation
of tagged new data is very expensive and time-
consuming. The methodology for accurate clas-
sification of the new test data by making the max-
imum use of tagged old data is needed in learning
techniques.
</bodyText>
<author confidence="0.775787">
Yoshimi Suzuki
</author>
<affiliation confidence="0.948602">
Graduate Faculty of
Interdisciplinary Research
Univ. of Yamanashi, Japan
</affiliation>
<email confidence="0.987078">
ysuzuki@yamanashi.ac.jp
</email>
<bodyText confidence="0.999880705882353">
In this paper, we present a method for text cat-
egorization that minimizes the impact of tempo-
ral effects. Our approach extends a boosting tech-
nique to learn accurate model for timeline adap-
tation. We used two types of labeled training
data: One is the same creation time period with
the test data. Another is different creation time
period with the test data. We call the former same-
period training, and the latter diff-period train-
ing data. For the same-period training data, the
learner shows the same behavior as the boosting.
In contrast, for diff-period training data, once they
are wrongly predicted by the learned model, these
data would be useless to classify test data. We
decreased the weights of these data by applying
Gaussian function in order to weaken their im-
pacts.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.992443636363637">
The analysis of temporal aspects is a practical
problem as well as the process of large-scale
heterogeneous data since the World-Wide Web
(WWW) is widely used by various sorts of peo-
ple. It is widely studied in many text process-
ing tasks. One attempt is concept or topic drift
dealing with temporal effects (Klinkenberg and
Joachims, 2000; Kleinberg, 2002; Lazarescu et
al., 2004; Folino et al., 2007; Song et al., 2014).
Wang et al. developed the continuous time dy-
namic topic model (cDTM) (Wang et al., 2008).
He et al. proposed a method to find bursts, peri-
ods of elevated occurrence of events as a dynamic
phenomenon instead of focusing on arrival rates
(He and Parker, 2010). They used Moving Aver-
age Convergence/Divergence (MACD) histogram
which was used in technical stock market analysis
(Murphy, 1999) to detect bursts.
Another attempt is domain adaptation. The goal
of this attempt is to develop learning algorithms
that can be easily ported from one domain to an-
other (Daum´e III, 2007; Sparinnapakorn and Ku-
</bodyText>
<page confidence="0.977877">
799
</page>
<note confidence="0.6638925">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 799–804,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999505204081633">
bat, 2007; Glorot et al., 2011; Siao and Guo,
2013). Domain adaptation is particularly inter-
esting in Natural Language Processing (NLP) be-
cause it is often the case that we have a collec-
tion of labeled data in one domain but truly de-
sire a model that can work well for another do-
main. Lots of studies addressed domain adapta-
tion in NLP tasks such as part-of-speech tagging
(Siao and Guo, 2013), named-entity (Daum´e III,
2007), and sentiment classification (Glorot et al.,
2011) are presented. One approach to domain
adaptation is to use transfer learning. The transfer
learning is a learning technique that retains and ap-
plies the knowledge learned in one or more tasks
to efficiently develop an effective hypothesis for a
new task. The earliest discussion is done by ML
community in a NIPS-95 workshop1, and more re-
cently, transfer learning techniques have been suc-
cessfully applied in many applications. Blitzer
et al. proposed a method for sentiment classi-
fication using structural correspondence learning
that makes use of the unlabeled data from the tar-
get domain to extract some relevant features that
may reduce the difference between the domains
(Blitzer et al., 2006). Several authors have at-
tempted to learn classifiers across domains us-
ing transfer learning in the text classification task
(Raina et al., 2006; Dai et al., 2007; Sparinna-
pakorn and Kubat, 2007). Raina et al. proposed
a transfer learning algorithm that constructs an in-
formative Bayesian prior for a given text classi-
fication task (Raina et al., 2006). They reported
that a 20 to 40% test error reduction over a com-
monly used prior in the binary text classification
task. Dai et al. presented a method called TrAd-
aBoost which extends boosting-based learning al-
gorithms (Dai et al., 2007). Their experimental
results show that TrAdaBoost allows knowledge
to be effectively transferred from the old data to
the new one. All of these approaches aimed at
utilizing a small amount of newly labeled data to
leverage the old data to construct a high-quality
classification model for the new data. However,
the temporal effects are not explicitly incorporated
into their models.
To our knowledge, there have been only a few
previous work on temporal-based text categoriza-
tion. Mourao et al. investigated the impact of
temporal evolution of document collections on
</bodyText>
<footnote confidence="0.994346">
1http://socrates.acadiau.ca/courses/comp/dsilver/
NIPS95 LTL/transfer.workshop.1995.html.
</footnote>
<bodyText confidence="0.999806619047619">
the document classification (Mourao et al., 2008).
Salles et al. presented an approach to classify doc-
uments in scenarios where the method uses infor-
mation about both the past and the future, and this
information may change over time (Salles et al.,
2010). They address the drawbacks of which in-
stances to select by approximating the Temporal
Weighting Function (TWF) using a mixture of two
Gaussians. However, their method needs tagged
training data across full temporal range of training
documents to construct TWF.
There are three novel aspects in our method.
Firstly, we propose a method for text categoriza-
tion that minimizes the impact of temporal effects
in a learning technique. Secondly, from manual
annotation of data perspective, the method allows
users to annotate only a limited number of newly
training data. Finally, from the perspective of ro-
bustness, the method is automated, and can be
applied easily to a new domain, or different lan-
guages, given sufficient old labeled documents.
</bodyText>
<sectionHeader confidence="0.911961" genericHeader="method">
3 Learning Timeline Difference
</sectionHeader>
<bodyText confidence="0.999948142857143">
Our learning model, Timeline Adaptation by
Boosting (TABoost) is based on AdaBoost (Fre-
und and Schapire, 1997). AdaBoost aims to boost
the accuracy of a weak learner by adjusting the
weights of training instances and learn a classi-
fier accordingly. The TABoost uses two types of
training data, same-period and diff-period train-
ing data. The assumption is that the quantity of
the same-period data is limited, while diff-period
training data is abundant. The TABoost aims at
utilizing the diff-period training data to make up
the deficit of a small amount of the same-period
to construct a high-quality classification model for
the test data. Similar to the TrAdaBoost presented
by (Dai et al., 2007), TABoost is the same behav-
ior as boosting for the same-period training data.
In contrast, once diff-period training instances are
wrongly predicted, we assume that these instances
do not contribute to the accurate test data classifi-
cation, and the weights of these instances decrease
in order to weaken their impacts. The difference
between TrAdaBoost and TABoost is a weight-
ing manner, i.e. TABoost is a continuous time-
line model, and it weights these instances by ap-
plying Gaussian function in order to weaken their
impacts. TABoost is illustrated in Figure 1.
The training data set Tr is partitioned into two
labeled sets Trdp, and Trsp. Trdp in Figure 1
</bodyText>
<page confidence="0.991965">
800
</page>
<figureCaption confidence="0.99973">
Figure 1: Flow of the algorithm
</figureCaption>
<bodyText confidence="0.920316882352941">
shows the diff-period training data that Trdp =
{(xdp
i , c(xdp
i ))}, where xdp
i ∈ Xdp (i = 1, · · ·, n),
and Xdp refers to the diff-period instance space.
Similarly, Trsp represents the same-period train-
ing data that Trsp = {(xsp
i , c(xsp
i ))}, where xsp
i ∈
Xsp (i = 1, · · ·, m), and Xsp refers to the same-
period instance space. n and m are the number of
documents in Trdp and Trsp, respectively. c(xi)
returns a label for the input instance xi. The com-
bined training set Tr = {(xi,c(xi))} is given by:
{ xdp
</bodyText>
<equation confidence="0.9115">
i i = 1,···,n xsp
i i = n + 1, · ·
</equation>
<bodyText confidence="0.974914692307692">
In each iteration round shown in Figure 1, if a
diff-period training instance is wrongly predicted,
the instance may be useless to classify test data
correctly. We decrease its training weight to re-
duce the effect. To do this, we assume a standard
lognormal distribution (Crow, 1988), i.e. F(S)
=1√2π exp(−δ22). S in F(S) represents time dif-
ference between diff-period training and test data.
For instance, if the training data is 1999, and test
data is 2000, S equals to 1. Similarly, if the train-
ing data is 2000, and test data is 1999, S is −1.
The greater the time difference value, the smaller
the training weight. We fit the model according
to the temporal range of the data. As shown in
Figure 1, we decrease its training weight wti to re-
duce its effect through multiplying its weight by
expF(δ)αt|ht(xi)−c(xi) |.
We used the Support Vector Machines (SVM)
as a learner. We represented each training and test
document as a vector, each dimension of a vec-
tor is a noun word appeared in the document, and
each element of the dimension is a term frequency.
We applied the algorithm shown in Figure 1. Af-
ter several iterations, a learner model is created by
linearly combining weak learners, and a test doc-
ument is classified by using a learner.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999814">
We evaluated our TABoost by using the Mainichi
Japanese newspaper documents.
</bodyText>
<subsectionHeader confidence="0.965339">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99987105882353">
We choose the Mainichi Japanese newspaper
corpus from 1991 to 2012. The corpus con-
sists of 2,883,623 documents organized into 16
categories. We selected 8 categories, “Inter-
national(Int)”, “Economy(Eco)”, “Home”, “Cul-
ture”, “Reading”, “Arts”, “Sports”, and “Local
news(Local)”, each of which has sufficient num-
ber of documents. All documents were tagged
by using a morphological analyzer Chasen (Mat-
sumoto et al., 2000) and selected noun words. The
total number of documents assigned to these cate-
gories are 787,518. For each category within each
year, we divided documents into three folds: 2%
of documents are used as the same-period training
data, 50% of documents are the diff-period train-
ing data, and the remains are used to test our clas-
sification method.2
</bodyText>
<footnote confidence="0.831478333333333">
2When the creation time period of the training data is the
same as the test data, we used only the same-period training
data.
</footnote>
<figure confidence="0.770474361111111">
Input {
The diff-period data Trdp, the same-
period data Trsp, and the maximum
number of iterations N.
}
Output {
hf(x) = ENt=1 αtht(xi).
}
Initialization {
w1 = (w1 1, · · · ,w1 n+m).
}
TABoost {
For t = 1,· · ·,N
1. Set Pt = wt/ (�n+m
k=1 wti).
2. Train a weak learner on the combined
training set Trdp and Trsp with the
distribution Pt, and create weak hy-
pothesis ht: X → {−1, +1}
3. Calculate the error of ht on the com-
bined training set Trdp and Trsp:
Et = �n+m wt i·|ht(xi)−c(xi)|
�n+m
i=1 i=1 wt i .
4. Chose αt = 1 2In(1−�t
~t )
5. Update the new weight vector:
wt+1 = ⎧ wtiexpF(δ)αt|ht(xi)−c(xi)|,
i ⎨⎪⎪⎪⎪⎪ 1 ≤ i ≤ n
⎪⎪⎪⎪⎪⎩ wtiexp−αt|ht(xi)−c(xi)|,
n + 1 ≤ i ≤ n + m
}
xi =
·,n + m
801
Error Rate
</figure>
<tableCaption confidence="0.998867">
Table 1: The error rates across categories
</tableCaption>
<table confidence="0.9956252">
Cat TAB s SVM TrAdaB b-SVM TAB
Int 0.409 0.467 0.326 0.253 0.329
Eco 0.368 0.429 0.243 0.228 0.208
Home 0.475 0.649 0.312 0.460 0.172
Culture 0.468 0.848 0.440 0.559 0.196
Reading 0.358 0.520 0.298 0.357 0.337
Arts 0.402 0.684 0.330 0.588 0.331
Sports 0.226 0.212 0.107 0.075 0.123
Local 0.586 0.305 0.400 0.156 0.303
M-Avg 0.411 0.514 0.307 0.334 0.257
</table>
<figure confidence="0.9688085">
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
&amp;quot;SVM&amp;quot;
&amp;quot;TrAdaB&amp;quot;
&amp;quot;bSVM&amp;quot;
&amp;quot;TAB&amp;quot;
-25 -20 -15 -10 -5 0 5 10 15 20 25
Temporal Distance
</figure>
<bodyText confidence="0.999701714285714">
We used LIBLINEAR (Fan et al., 2008) as a
basic learner in the experiments. We compared
our method, TABoost with four baselines: (1)
TABoost with the same-period training data only
(TAB s), (2) SVM, (3) TrAdaBoost (Dai et al.,
2007), and (4) biased-SVM (Liu et al., 2003) by
SVM-light (Joachims, 1998). TAB s is the same
behavior as boosting. TrAdaBoost (TrAdaB) is
presented by (Dai et al., 2007). Biased-SVM
(b-SVM) is known as the state-of-the-art SVMs
method, and often used for comparison (Elkan and
Noto, 2008). Similar to SVM, for biased-SVM,
we used the first two folds as a training data, and
classified test documents directly, i.e. we used
closed data. We empirically selected values of
two parameters, “c” (trade-off between training er-
ror and margin) and “j”, i.e. cost (cost-factor, by
which training errors on positive instances) that
optimized result obtained by classification of test
documents. Similar to (Liu et al., 2003), “c” is
searched in steps of 0.02 from 0.01 to 0.61. “j”
is searched in steps of 5 from 1 to 200. As a re-
sult, we set c and j to 0.01 and 10, respectively. To
make comparisons fair, all five methods including
our method are based on linear kernel. Throughout
the experiments, the number of iterations is set to
100. We used error rate as an evaluation measure
(Dai et al., 2007).
</bodyText>
<sectionHeader confidence="0.611745" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.984648777777778">
Categorization results for 8 categories (48% of
the test documents, i.e. 378,008 documents) are
shown in Table 1. Each value in Table 1 shows
macro-averaged error rate across 22 years. “M-
Avg” refers to macro-averaged error rate across
categories. The results obtained by biased-SVM
show minimum error rate obtained by varying the
parameters, “c” and “j”.
As can be seen clearly from Table 1, the overall
</bodyText>
<figureCaption confidence="0.995233">
Figure 2: Performance against temporal distance
</figureCaption>
<bodyText confidence="0.999797361111111">
performance obtained by TAB was the best among
the five methods. The macro average error rates
with TrAdaB and TAB were lower to those ob-
tained by b-SVM, although b-SVM in Table 1 was
the result obtained by using the closed data. In
contrast, SVM did not work well. This demon-
strates that once the training data drive from a dif-
ferent time period from the test data, the distri-
butions of terms between training and test docu-
ments are not identical. The results obtained by
TAB s were worse than those obtained by TrAd-
aBoost, b-SVM, and TAB. This shows that (i) the
same-period training data we used is not sufficient
to train a model alone, and (ii) TAB demonstrates
a good transfer ability.
Figure 2 illustrates error rate against the tempo-
ral difference between diff-period training and test
data. Both training and test data are the documents
from 1991 to 2012. For instance, “10” of the x-
axis in Figure 2 indicates that the test documents
are created 10 years later than the training docu-
ments. We can see from Figure 2 that the result
obtained by TAB was the best in all of the tempo-
ral distances. There are no significant differences
among three methods, bSVM, TrAdaB, and TAB
when the test and training data are the same time
period. The performance of these methods includ-
ing SVM drops when the creation time of the test
data differs greatly from the diff-period training
data. However, the performance of TAB was still
better to those obtained by other methods. This
demonstrates that the algorithm with continuous
timeline model works well for categorization.
Figure 3 shows the error rate against the number
of iterations. Each curve shows averaged error rate
under time period between same-period and diff-
</bodyText>
<page confidence="0.990499">
802
</page>
<figure confidence="0.9942425">
0 10 20 30 40 50 60 70 80 90 100
Number of Iterations
</figure>
<figureCaption confidence="0.999954">
Figure 3: Iteration curves by temporal distance
</figureCaption>
<bodyText confidence="0.999960555555556">
period training data. For example, “diff 10” indi-
cates that the difference time period between same
and diff training data is ± 10 years. We can see
from Figure 3 that all curves except for “diff 20”
drop rapidly and converge around 10 iterations.
“diff 20” converges around 20 iterations. This was
the same behaviour as TrAdaBoost, i.e. TrAd-
aBoost converges around 20 iterations. The fast
convergence is not particularly surprising because
we used a small number of same-period (2%) and
a large number of diff-period (50%) training data.
It is necessary to examine how the ratio between
same-period and diff-period training data affects
overall performance for further quantitative eval-
uation, although a main contribution of TAB is in
situations using by both of a small amount of la-
beled new data which is not sufficient to train a
model alone, and a large amount of old data.
</bodyText>
<sectionHeader confidence="0.996463" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999976157894737">
We have presented a method for text catego-
rizaiton that minimizes the impact of temporal ef-
fects. The results using Japanese Mainichi News-
paper corpus show that it works well for cate-
gorization, especially when the creation time of
the test data differs greatly from the training data.
There are a number of interesting directions for
future work. The rate of convergence of TAB
(O(�/Inn/N)) is slow which can also be found in
(Dai et al., 2007). Here, n is the number of train-
ing data, and N is the number of iterations. In
the future, we will try to extend the framework to
address this issue. We used Japanese newspaper
documents in the experiments. For quantitative
evaluation, we need to apply our mehtod to other
data such as ACM-DL and a large, heterogeneous
collection of web content in addition to the experi-
ment to examine the performance agasint the ratio
between same-period and diff-period training data.
</bodyText>
<sectionHeader confidence="0.9961" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998701044444444">
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 120–
128.
S. K. Crow. 1988. Log-normal Distributions: Theory
and Application. NewYork: Dekker.
W. Dai, Q. Yang, G.R. Xue, and Y. Yu. 2007. Boost-
ing for Transfer Learning. In Proc. of the 24th In-
ternational Conference on Machine Learning, pages
193–200.
H. Daum´e III. 2007. Frustratingly Easy Domain
Adaptation. In Proc. of the 45th Annual Meeting of
the Association of computational Linguistics, pages
256–263.
C. Elkan and K. Noto. 2008. Learning Classifiers from
Only Positive and Unlabeled Data. In Proc. of the
KDD’08, pages 213–220.
F. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and
C. J. Lin. 2008. LIBLINEAR: A Library for Large
Linear Classification. Machine Learning, 9:1871–
1874.
G. Folino, C. Pizzuti, and G. Spezzano. 2007. An
Adaptive Distributed Ensemble Approach to Mine
Concept-drifting Data Streams. In Proc. of the 19th
IEEE International Conference on Tools with Artifi-
cial Intelligence, pages 183–188.
Y. Freund and R. E. Schapire. 1997. A Decision-
Theoretic Generalization of On-Line Learning and
an Application to Boosting. Journal of Computer
and System Sciences, 55(1):119–139.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Do-
main Adaptation for Large-Scale Sentiment Classifi-
cation: A Deep Learning Approach. In Proc. of the
28th International Conference on Machine Learn-
ing, pages 97–110.
S. Gopal and Y. Yang. 2010. Multilabel Classification
with Meta-level Features. In Proc. of the 33rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 315–322.
S. Hassan, R. Mihalcea, and C. Nanea. 2007. Random-
Walk Term Weighting for Improved Text Classifica-
tion. In Proc. of the IEEE International Conference
on Semantic Computing, pages 242–249.
</reference>
<figure confidence="0.9992672">
&amp;quot;diff_1&amp;quot;
&amp;quot;diff_5&amp;quot;
&amp;quot;diff_10&amp;quot;
&amp;quot;diff_15&amp;quot;
&amp;quot;diff_21&amp;quot;
Error Rate 0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
</figure>
<page confidence="0.990979">
803
</page>
<reference confidence="0.9998042">
D. He and D. S. Parker. 2010. Topic Dynamics: An
Alternative Model of Bursts in Streams of Topics.
In Proc. of the 16th ACM SIGKDD Conference on
Knowledge discovery and Data Mining, pages 443–
452.
T. Joachims. 1998. SVM Light Support Vector Ma-
chine. In Dept. of Computer Science Cornell Uni-
versity.
M. Kleinberg. 2002. Bursty and Hierarchical Structure
in Streams. In Proc. of the Eighth ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 91–101.
R. Klinkenberg and T. Joachims. 2000. Detecting
Concept Drift with Support Vector Machines. In
Proc. of the 17th International Conference on Ma-
chine Learning, pages 487–494.
M. M. Lazarescu, S. Venkatesh, and H. H. Bui. 2004.
Using Multiple Windows to Track Concept Drift.
Intelligent Data Analysis, 8(1):29–59.
B. Liu, Y. dai, X. Li, W. S. Lee, and P. S. Yu. 2003.
Building Text Classifiers using Positive and Unla-
beled Examples. In Proc. of the ICDM’03, pages
179–188.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano,
Y. Matsuda, K. Takaoka, and M. Asahara. 2000.
Japanese Morphological Analysis System Chasen
Version 2.2.1. In Naist Technical Report.
F. Mourao, L. Rocha, R. Araujo, T. Couto,
M. Goncalves, and W. M. Jr. 2008. Understanding
Temporal Aspects in Document Classification. In
Proc. of the 1st ACM International Conference on
Web Search and Data Mining, pages 159–169.
J. Murphy. 1999. Technical Analysis of the Financial
Markets. Prentice Hall.
R. Raina, A. Y. Ng, and D. Koller. 2006. Construct-
ing Informative Priors using Transfer Learning. In
Proc. of the 23rd International Conference on Ma-
chine Learning, pages 713–720.
T. Salles, L. Rocha, and G. L. Pappa. 2010.
Temporally-aware Algorithms for Document Clas-
sification. In Proc. of the 33rd Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 307–314.
M. Siao and Y. Guo. 2013. Domain Adaptation for
Sequence Labeling Tasks with a Probabilistic Lan-
guage Adaptation Model. In Proc. of the 30th In-
ternational Conference on Machine Learning, pages
293–301.
M. Song, G. E. Heo, and S. Y. Kim. 2014. Analyzing
topic evolution in bioinformatics: Investigation of
dynamics of the field with conference data in dblp.
Scientometrics, 101(1):397–428.
K. Sparinnapakorn and M. Kubat. 2007. Combining
Subclassifiers in Text Categorization: A DST-based
Solution and a Case Study. In Proc. of the 13th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 210–219.
C. Wang, D. Blei, and D. Heckerman. 2008. Contin-
uous Time Dynamic Topic Models. In Proc. of the
24th Conference on Uncertainty in Artificial Intelli-
gence, pages 579–586.
G. R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topic-
bridged PLSA for Cross-Domain Text Classifica-
tion. In Proc. of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 627–634.
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
In Proc. of the 14th International Conference on Ma-
chine Learning, pages 412–420.
</reference>
<page confidence="0.99878">
804
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.878650">
<title confidence="0.999988">Learning Timeline Difference for Text Categorization</title>
<author confidence="0.94779">Fumiyo Fukumoto</author>
<affiliation confidence="0.965521333333333">Graduate Faculty of Interdisciplinary Research Univ. of Yamanashi, Japan</affiliation>
<email confidence="0.985523">fukumoto@yamanashi.ac.jp</email>
<abstract confidence="0.999389384615385">This paper addresses text categorization problem that training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="5191" citStr="Blitzer et al., 2006" startWordPosition="830" endWordPosition="833"> The transfer learning is a learning technique that retains and applies the knowledge learned in one or more tasks to efficiently develop an effective hypothesis for a new task. The earliest discussion is done by ML community in a NIPS-95 workshop1, and more recently, transfer learning techniques have been successfully applied in many applications. Blitzer et al. proposed a method for sentiment classification using structural correspondence learning that makes use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains (Blitzer et al., 2006). Several authors have attempted to learn classifiers across domains using transfer learning in the text classification task (Raina et al., 2006; Dai et al., 2007; Sparinnapakorn and Kubat, 2007). Raina et al. proposed a transfer learning algorithm that constructs an informative Bayesian prior for a given text classification task (Raina et al., 2006). They reported that a 20 to 40% test error reduction over a commonly used prior in the binary text classification task. Dai et al. presented a method called TrAdaBoost which extends boosting-based learning algorithms (Dai et al., 2007). Their expe</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 120– 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Crow</author>
</authors>
<title>Log-normal Distributions: Theory and Application.</title>
<date>1988</date>
<publisher>NewYork: Dekker.</publisher>
<contexts>
<context position="9697" citStr="Crow, 1988" startWordPosition="1584" endWordPosition="1585">p = {(xsp i , c(xsp i ))}, where xsp i ∈ Xsp (i = 1, · · ·, m), and Xsp refers to the sameperiod instance space. n and m are the number of documents in Trdp and Trsp, respectively. c(xi) returns a label for the input instance xi. The combined training set Tr = {(xi,c(xi))} is given by: { xdp i i = 1,···,n xsp i i = n + 1, · · In each iteration round shown in Figure 1, if a diff-period training instance is wrongly predicted, the instance may be useless to classify test data correctly. We decrease its training weight to reduce the effect. To do this, we assume a standard lognormal distribution (Crow, 1988), i.e. F(S) =1√2π exp(−δ22). S in F(S) represents time difference between diff-period training and test data. For instance, if the training data is 1999, and test data is 2000, S equals to 1. Similarly, if the training data is 2000, and test data is 1999, S is −1. The greater the time difference value, the smaller the training weight. We fit the model according to the temporal range of the data. As shown in Figure 1, we decrease its training weight wti to reduce its effect through multiplying its weight by expF(δ)αt|ht(xi)−c(xi) |. We used the Support Vector Machines (SVM) as a learner. We rep</context>
</contexts>
<marker>Crow, 1988</marker>
<rawString>S. K. Crow. 1988. Log-normal Distributions: Theory and Application. NewYork: Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dai</author>
<author>Q Yang</author>
<author>G R Xue</author>
<author>Y Yu</author>
</authors>
<title>Boosting for Transfer Learning.</title>
<date>2007</date>
<booktitle>In Proc. of the 24th International Conference on Machine Learning,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="5353" citStr="Dai et al., 2007" startWordPosition="857" endWordPosition="860"> new task. The earliest discussion is done by ML community in a NIPS-95 workshop1, and more recently, transfer learning techniques have been successfully applied in many applications. Blitzer et al. proposed a method for sentiment classification using structural correspondence learning that makes use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains (Blitzer et al., 2006). Several authors have attempted to learn classifiers across domains using transfer learning in the text classification task (Raina et al., 2006; Dai et al., 2007; Sparinnapakorn and Kubat, 2007). Raina et al. proposed a transfer learning algorithm that constructs an informative Bayesian prior for a given text classification task (Raina et al., 2006). They reported that a 20 to 40% test error reduction over a commonly used prior in the binary text classification task. Dai et al. presented a method called TrAdaBoost which extends boosting-based learning algorithms (Dai et al., 2007). Their experimental results show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new one. All of these approaches aimed at utilizing </context>
<context position="8168" citStr="Dai et al., 2007" startWordPosition="1299" endWordPosition="1302"> (Freund and Schapire, 1997). AdaBoost aims to boost the accuracy of a weak learner by adjusting the weights of training instances and learn a classifier accordingly. The TABoost uses two types of training data, same-period and diff-period training data. The assumption is that the quantity of the same-period data is limited, while diff-period training data is abundant. The TABoost aims at utilizing the diff-period training data to make up the deficit of a small amount of the same-period to construct a high-quality classification model for the test data. Similar to the TrAdaBoost presented by (Dai et al., 2007), TABoost is the same behavior as boosting for the same-period training data. In contrast, once diff-period training instances are wrongly predicted, we assume that these instances do not contribute to the accurate test data classification, and the weights of these instances decrease in order to weaken their impacts. The difference between TrAdaBoost and TABoost is a weighting manner, i.e. TABoost is a continuous timeline model, and it weights these instances by applying Gaussian function in order to weaken their impacts. TABoost is illustrated in Figure 1. The training data set Tr is partitio</context>
<context position="13134" citStr="Dai et al., 2007" startWordPosition="2203" endWordPosition="2206">0.475 0.649 0.312 0.460 0.172 Culture 0.468 0.848 0.440 0.559 0.196 Reading 0.358 0.520 0.298 0.357 0.337 Arts 0.402 0.684 0.330 0.588 0.331 Sports 0.226 0.212 0.107 0.075 0.123 Local 0.586 0.305 0.400 0.156 0.303 M-Avg 0.411 0.514 0.307 0.334 0.257 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 &amp;quot;SVM&amp;quot; &amp;quot;TrAdaB&amp;quot; &amp;quot;bSVM&amp;quot; &amp;quot;TAB&amp;quot; -25 -20 -15 -10 -5 0 5 10 15 20 25 Temporal Distance We used LIBLINEAR (Fan et al., 2008) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (Dai et al., 2007), and (4) biased-SVM (Liu et al., 2003) by SVM-light (Joachims, 1998). TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by (Dai et al., 2007). Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison (Elkan and Noto, 2008). Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e. cost (cost-factor, by which training errors on positiv</context>
</contexts>
<marker>Dai, Yang, Xue, Yu, 2007</marker>
<rawString>W. Dai, Q. Yang, G.R. Xue, and Y. Yu. 2007. Boosting for Transfer Learning. In Proc. of the 24th International Conference on Machine Learning, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association of computational Linguistics,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proc. of the 45th Annual Meeting of the Association of computational Linguistics, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Elkan</author>
<author>K Noto</author>
</authors>
<title>Learning Classifiers from Only Positive and Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proc. of the KDD’08,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="13418" citStr="Elkan and Noto, 2008" startWordPosition="2249" endWordPosition="2252"> 0.35 0.3 0.25 0.2 0.15 &amp;quot;SVM&amp;quot; &amp;quot;TrAdaB&amp;quot; &amp;quot;bSVM&amp;quot; &amp;quot;TAB&amp;quot; -25 -20 -15 -10 -5 0 5 10 15 20 25 Temporal Distance We used LIBLINEAR (Fan et al., 2008) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (Dai et al., 2007), and (4) biased-SVM (Liu et al., 2003) by SVM-light (Joachims, 1998). TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by (Dai et al., 2007). Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison (Elkan and Noto, 2008). Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e. cost (cost-factor, by which training errors on positive instances) that optimized result obtained by classification of test documents. Similar to (Liu et al., 2003), “c” is searched in steps of 0.02 from 0.01 to 0.61. “j” is searched in steps of 5 from 1 to 200. As a result, we set c and j to 0.01 and 10, respectively. To make compariso</context>
</contexts>
<marker>Elkan, Noto, 2008</marker>
<rawString>C. Elkan and K. Noto. 2008. Learning Classifiers from Only Positive and Unlabeled Data. In Proc. of the KDD’08, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F E Fan</author>
<author>K W Chang</author>
<author>C J Hsieh</author>
<author>X R Wang</author>
<author>C J Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<booktitle>Machine Learning,</booktitle>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="12938" citStr="Fan et al., 2008" startWordPosition="2170" endWordPosition="2173"> n + 1 ≤ i ≤ n + m } xi = ·,n + m 801 Error Rate Table 1: The error rates across categories Cat TAB s SVM TrAdaB b-SVM TAB Int 0.409 0.467 0.326 0.253 0.329 Eco 0.368 0.429 0.243 0.228 0.208 Home 0.475 0.649 0.312 0.460 0.172 Culture 0.468 0.848 0.440 0.559 0.196 Reading 0.358 0.520 0.298 0.357 0.337 Arts 0.402 0.684 0.330 0.588 0.331 Sports 0.226 0.212 0.107 0.075 0.123 Local 0.586 0.305 0.400 0.156 0.303 M-Avg 0.411 0.514 0.307 0.334 0.257 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 &amp;quot;SVM&amp;quot; &amp;quot;TrAdaB&amp;quot; &amp;quot;bSVM&amp;quot; &amp;quot;TAB&amp;quot; -25 -20 -15 -10 -5 0 5 10 15 20 25 Temporal Distance We used LIBLINEAR (Fan et al., 2008) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (Dai et al., 2007), and (4) biased-SVM (Liu et al., 2003) by SVM-light (Joachims, 1998). TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by (Dai et al., 2007). Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison (Elkan and Noto, 2008). Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>F. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and C. J. Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Machine Learning, 9:1871– 1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Folino</author>
<author>C Pizzuti</author>
<author>G Spezzano</author>
</authors>
<title>An Adaptive Distributed Ensemble Approach to Mine Concept-drifting Data Streams.</title>
<date>2007</date>
<booktitle>In Proc. of the 19th IEEE International Conference on Tools with Artificial Intelligence,</booktitle>
<pages>183--188</pages>
<contexts>
<context position="3198" citStr="Folino et al., 2007" startWordPosition="507" endWordPosition="510"> they are wrongly predicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sp</context>
</contexts>
<marker>Folino, Pizzuti, Spezzano, 2007</marker>
<rawString>G. Folino, C. Pizzuti, and G. Spezzano. 2007. An Adaptive Distributed Ensemble Approach to Mine Concept-drifting Data Streams. In Proc. of the 19th IEEE International Conference on Tools with Artificial Intelligence, pages 183–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A DecisionTheoretic Generalization of On-Line Learning and an Application to Boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="7579" citStr="Freund and Schapire, 1997" startWordPosition="1202" endWordPosition="1206">F. There are three novel aspects in our method. Firstly, we propose a method for text categorization that minimizes the impact of temporal effects in a learning technique. Secondly, from manual annotation of data perspective, the method allows users to annotate only a limited number of newly training data. Finally, from the perspective of robustness, the method is automated, and can be applied easily to a new domain, or different languages, given sufficient old labeled documents. 3 Learning Timeline Difference Our learning model, Timeline Adaptation by Boosting (TABoost) is based on AdaBoost (Freund and Schapire, 1997). AdaBoost aims to boost the accuracy of a weak learner by adjusting the weights of training instances and learn a classifier accordingly. The TABoost uses two types of training data, same-period and diff-period training data. The assumption is that the quantity of the same-period data is limited, while diff-period training data is abundant. The TABoost aims at utilizing the diff-period training data to make up the deficit of a small amount of the same-period to construct a high-quality classification model for the test data. Similar to the TrAdaBoost presented by (Dai et al., 2007), TABoost i</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Y. Freund and R. E. Schapire. 1997. A DecisionTheoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1):119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Glorot</author>
<author>A Bordes</author>
<author>Y Bengio</author>
</authors>
<title>Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach.</title>
<date>2011</date>
<booktitle>In Proc. of the 28th International Conference on Machine Learning,</booktitle>
<pages>97--110</pages>
<contexts>
<context position="4044" citStr="Glorot et al., 2011" startWordPosition="641" endWordPosition="644">d of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku799 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 799–804, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. bat, 2007; Glorot et al., 2011; Siao and Guo, 2013). Domain adaptation is particularly interesting in Natural Language Processing (NLP) because it is often the case that we have a collection of labeled data in one domain but truly desire a model that can work well for another domain. Lots of studies addressed domain adaptation in NLP tasks such as part-of-speech tagging (Siao and Guo, 2013), named-entity (Daum´e III, 2007), and sentiment classification (Glorot et al., 2011) are presented. One approach to domain adaptation is to use transfer learning. The transfer learning is a learning technique that retains and applies th</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach. In Proc. of the 28th International Conference on Machine Learning, pages 97–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gopal</author>
<author>Y Yang</author>
</authors>
<title>Multilabel Classification with Meta-level Features.</title>
<date>2010</date>
<booktitle>In Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="993" citStr="Gopal and Yang, 2010" startWordPosition="143" endWordPosition="146">o learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 1 Introduction Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (Mourao et al., 2008). A growing number of machine learning (ML) techniques have been applied to the text categorization task (Xue et al., 2008; Gopal and Yang, 2010). Each document is represented using a vector of features/terms (Yang and Pedersen, 1997; Hassan et al., 2007). Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classified by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classification accuracy is worse. However, it is often the case that the term distribution in the training data is different from that of the test data when</context>
</contexts>
<marker>Gopal, Yang, 2010</marker>
<rawString>S. Gopal and Y. Yang. 2010. Multilabel Classification with Meta-level Features. In Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>R Mihalcea</author>
<author>C Nanea</author>
</authors>
<title>RandomWalk Term Weighting for Improved Text Classification.</title>
<date>2007</date>
<booktitle>In Proc. of the IEEE International Conference on Semantic Computing,</booktitle>
<pages>242--249</pages>
<contexts>
<context position="1103" citStr="Hassan et al., 2007" startWordPosition="160" endWordPosition="163">t state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 1 Introduction Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (Mourao et al., 2008). A growing number of machine learning (ML) techniques have been applied to the text categorization task (Xue et al., 2008; Gopal and Yang, 2010). Each document is represented using a vector of features/terms (Yang and Pedersen, 1997; Hassan et al., 2007). Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classified by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classification accuracy is worse. However, it is often the case that the term distribution in the training data is different from that of the test data when the training data may drive from a different time period from the test data. Manual annotation of tagged new </context>
</contexts>
<marker>Hassan, Mihalcea, Nanea, 2007</marker>
<rawString>S. Hassan, R. Mihalcea, and C. Nanea. 2007. RandomWalk Term Weighting for Improved Text Classification. In Proc. of the IEEE International Conference on Semantic Computing, pages 242–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D He</author>
<author>D S Parker</author>
</authors>
<title>Topic Dynamics: An Alternative Model of Bursts in Streams of Topics.</title>
<date>2010</date>
<booktitle>In Proc. of the 16th ACM SIGKDD Conference on Knowledge discovery and Data Mining,</booktitle>
<pages>443--452</pages>
<contexts>
<context position="3477" citStr="He and Parker, 2010" startWordPosition="557" endWordPosition="560">well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku799 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 799–804, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. bat, 2007; Glorot et al., 2011; Siao and Guo, 2013). Domain ada</context>
</contexts>
<marker>He, Parker, 2010</marker>
<rawString>D. He and D. S. Parker. 2010. Topic Dynamics: An Alternative Model of Bursts in Streams of Topics. In Proc. of the 16th ACM SIGKDD Conference on Knowledge discovery and Data Mining, pages 443– 452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>SVM Light Support Vector Machine. In</title>
<date>1998</date>
<institution>Dept. of Computer Science Cornell University.</institution>
<contexts>
<context position="13203" citStr="Joachims, 1998" startWordPosition="2216" endWordPosition="2217">ding 0.358 0.520 0.298 0.357 0.337 Arts 0.402 0.684 0.330 0.588 0.331 Sports 0.226 0.212 0.107 0.075 0.123 Local 0.586 0.305 0.400 0.156 0.303 M-Avg 0.411 0.514 0.307 0.334 0.257 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 &amp;quot;SVM&amp;quot; &amp;quot;TrAdaB&amp;quot; &amp;quot;bSVM&amp;quot; &amp;quot;TAB&amp;quot; -25 -20 -15 -10 -5 0 5 10 15 20 25 Temporal Distance We used LIBLINEAR (Fan et al., 2008) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (Dai et al., 2007), and (4) biased-SVM (Liu et al., 2003) by SVM-light (Joachims, 1998). TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by (Dai et al., 2007). Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison (Elkan and Noto, 2008). Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e. cost (cost-factor, by which training errors on positive instances) that optimized result obtained by classification of test</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. SVM Light Support Vector Machine. In Dept. of Computer Science Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kleinberg</author>
</authors>
<title>Bursty and Hierarchical Structure in Streams.</title>
<date>2002</date>
<booktitle>In Proc. of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>91--101</pages>
<contexts>
<context position="3153" citStr="Kleinberg, 2002" startWordPosition="501" endWordPosition="502">rast, for diff-period training data, once they are wrongly predicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported fro</context>
</contexts>
<marker>Kleinberg, 2002</marker>
<rawString>M. Kleinberg. 2002. Bursty and Hierarchical Structure in Streams. In Proc. of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 91–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Klinkenberg</author>
<author>T Joachims</author>
</authors>
<title>Detecting Concept Drift with Support Vector Machines.</title>
<date>2000</date>
<booktitle>In Proc. of the 17th International Conference on Machine Learning,</booktitle>
<pages>487--494</pages>
<contexts>
<context position="3136" citStr="Klinkenberg and Joachims, 2000" startWordPosition="497" endWordPosition="500">ehavior as the boosting. In contrast, for diff-period training data, once they are wrongly predicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be </context>
</contexts>
<marker>Klinkenberg, Joachims, 2000</marker>
<rawString>R. Klinkenberg and T. Joachims. 2000. Detecting Concept Drift with Support Vector Machines. In Proc. of the 17th International Conference on Machine Learning, pages 487–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Lazarescu</author>
<author>S Venkatesh</author>
<author>H H Bui</author>
</authors>
<title>Using Multiple Windows to Track Concept Drift. Intelligent Data Analysis,</title>
<date>2004</date>
<contexts>
<context position="3177" citStr="Lazarescu et al., 2004" startWordPosition="503" endWordPosition="506">riod training data, once they are wrongly predicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another </context>
</contexts>
<marker>Lazarescu, Venkatesh, Bui, 2004</marker>
<rawString>M. M. Lazarescu, S. Venkatesh, and H. H. Bui. 2004. Using Multiple Windows to Track Concept Drift. Intelligent Data Analysis, 8(1):29–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>Y dai</author>
<author>X Li</author>
<author>W S Lee</author>
<author>P S Yu</author>
</authors>
<title>Building Text Classifiers using Positive and Unlabeled Examples.</title>
<date>2003</date>
<booktitle>In Proc. of the ICDM’03,</booktitle>
<pages>179--188</pages>
<contexts>
<context position="13173" citStr="Liu et al., 2003" startWordPosition="2210" endWordPosition="2213">.468 0.848 0.440 0.559 0.196 Reading 0.358 0.520 0.298 0.357 0.337 Arts 0.402 0.684 0.330 0.588 0.331 Sports 0.226 0.212 0.107 0.075 0.123 Local 0.586 0.305 0.400 0.156 0.303 M-Avg 0.411 0.514 0.307 0.334 0.257 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 &amp;quot;SVM&amp;quot; &amp;quot;TrAdaB&amp;quot; &amp;quot;bSVM&amp;quot; &amp;quot;TAB&amp;quot; -25 -20 -15 -10 -5 0 5 10 15 20 25 Temporal Distance We used LIBLINEAR (Fan et al., 2008) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost (Dai et al., 2007), and (4) biased-SVM (Liu et al., 2003) by SVM-light (Joachims, 1998). TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by (Dai et al., 2007). Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison (Elkan and Noto, 2008). Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data. We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i.e. cost (cost-factor, by which training errors on positive instances) that optimized result obta</context>
</contexts>
<marker>Liu, dai, Li, Lee, Yu, 2003</marker>
<rawString>B. Liu, Y. dai, X. Li, W. S. Lee, and P. S. Yu. 2003. Building Text Classifiers using Positive and Unlabeled Examples. In Proc. of the ICDM’03, pages 179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A Kitauchi</author>
<author>T Yamashita</author>
<author>Y Hirano</author>
<author>Y Matsuda</author>
<author>K Takaoka</author>
<author>M Asahara</author>
</authors>
<title>Japanese Morphological Analysis System Chasen Version 2.2.1. In Naist</title>
<date>2000</date>
<tech>Technical Report.</tech>
<contexts>
<context position="11195" citStr="Matsumoto et al., 2000" startWordPosition="1832" endWordPosition="1836">y linearly combining weak learners, and a test document is classified by using a learner. 4 Experiments We evaluated our TABoost by using the Mainichi Japanese newspaper documents. 4.1 Experimental setup We choose the Mainichi Japanese newspaper corpus from 1991 to 2012. The corpus consists of 2,883,623 documents organized into 16 categories. We selected 8 categories, “International(Int)”, “Economy(Eco)”, “Home”, “Culture”, “Reading”, “Arts”, “Sports”, and “Local news(Local)”, each of which has sufficient number of documents. All documents were tagged by using a morphological analyzer Chasen (Matsumoto et al., 2000) and selected noun words. The total number of documents assigned to these categories are 787,518. For each category within each year, we divided documents into three folds: 2% of documents are used as the same-period training data, 50% of documents are the diff-period training data, and the remains are used to test our classification method.2 2When the creation time period of the training data is the same as the test data, we used only the same-period training data. Input { The diff-period data Trdp, the sameperiod data Trsp, and the maximum number of iterations N. } Output { hf(x) = ENt=1 αth</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Takaoka, Asahara, 2000</marker>
<rawString>Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, Y. Matsuda, K. Takaoka, and M. Asahara. 2000. Japanese Morphological Analysis System Chasen Version 2.2.1. In Naist Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mourao</author>
<author>L Rocha</author>
<author>R Araujo</author>
<author>T Couto</author>
<author>M Goncalves</author>
<author>W M Jr</author>
</authors>
<title>Understanding Temporal Aspects in Document Classification.</title>
<date>2008</date>
<booktitle>In Proc. of the 1st ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>159--169</pages>
<contexts>
<context position="848" citStr="Mourao et al., 2008" startWordPosition="118" endWordPosition="121">t training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 1 Introduction Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (Mourao et al., 2008). A growing number of machine learning (ML) techniques have been applied to the text categorization task (Xue et al., 2008; Gopal and Yang, 2010). Each document is represented using a vector of features/terms (Yang and Pedersen, 1997; Hassan et al., 2007). Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classified by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classificat</context>
<context position="6487" citStr="Mourao et al., 2008" startWordPosition="1028" endWordPosition="1031">nsferred from the old data to the new one. All of these approaches aimed at utilizing a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. However, the temporal effects are not explicitly incorporated into their models. To our knowledge, there have been only a few previous work on temporal-based text categorization. Mourao et al. investigated the impact of temporal evolution of document collections on 1http://socrates.acadiau.ca/courses/comp/dsilver/ NIPS95 LTL/transfer.workshop.1995.html. the document classification (Mourao et al., 2008). Salles et al. presented an approach to classify documents in scenarios where the method uses information about both the past and the future, and this information may change over time (Salles et al., 2010). They address the drawbacks of which instances to select by approximating the Temporal Weighting Function (TWF) using a mixture of two Gaussians. However, their method needs tagged training data across full temporal range of training documents to construct TWF. There are three novel aspects in our method. Firstly, we propose a method for text categorization that minimizes the impact of temp</context>
</contexts>
<marker>Mourao, Rocha, Araujo, Couto, Goncalves, Jr, 2008</marker>
<rawString>F. Mourao, L. Rocha, R. Araujo, T. Couto, M. Goncalves, and W. M. Jr. 2008. Understanding Temporal Aspects in Document Classification. In Proc. of the 1st ACM International Conference on Web Search and Data Mining, pages 159–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Murphy</author>
</authors>
<title>Technical Analysis of the Financial Markets.</title>
<date>1999</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="3608" citStr="Murphy, 1999" startWordPosition="577" endWordPosition="578">ly studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku799 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 799–804, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. bat, 2007; Glorot et al., 2011; Siao and Guo, 2013). Domain adaptation is particularly interesting in Natural Language Processing (NLP) because it is often the case that we have a collection of </context>
</contexts>
<marker>Murphy, 1999</marker>
<rawString>J. Murphy. 1999. Technical Analysis of the Financial Markets. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Raina</author>
<author>A Y Ng</author>
<author>D Koller</author>
</authors>
<title>Constructing Informative Priors using Transfer Learning.</title>
<date>2006</date>
<booktitle>In Proc. of the 23rd International Conference on Machine Learning,</booktitle>
<pages>713--720</pages>
<contexts>
<context position="5335" citStr="Raina et al., 2006" startWordPosition="853" endWordPosition="856">ive hypothesis for a new task. The earliest discussion is done by ML community in a NIPS-95 workshop1, and more recently, transfer learning techniques have been successfully applied in many applications. Blitzer et al. proposed a method for sentiment classification using structural correspondence learning that makes use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains (Blitzer et al., 2006). Several authors have attempted to learn classifiers across domains using transfer learning in the text classification task (Raina et al., 2006; Dai et al., 2007; Sparinnapakorn and Kubat, 2007). Raina et al. proposed a transfer learning algorithm that constructs an informative Bayesian prior for a given text classification task (Raina et al., 2006). They reported that a 20 to 40% test error reduction over a commonly used prior in the binary text classification task. Dai et al. presented a method called TrAdaBoost which extends boosting-based learning algorithms (Dai et al., 2007). Their experimental results show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new one. All of these approaches a</context>
</contexts>
<marker>Raina, Ng, Koller, 2006</marker>
<rawString>R. Raina, A. Y. Ng, and D. Koller. 2006. Constructing Informative Priors using Transfer Learning. In Proc. of the 23rd International Conference on Machine Learning, pages 713–720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Salles</author>
<author>L Rocha</author>
<author>G L Pappa</author>
</authors>
<title>Temporally-aware Algorithms for Document Classification.</title>
<date>2010</date>
<booktitle>In Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>307--314</pages>
<contexts>
<context position="6693" citStr="Salles et al., 2010" startWordPosition="1064" endWordPosition="1067">ew data. However, the temporal effects are not explicitly incorporated into their models. To our knowledge, there have been only a few previous work on temporal-based text categorization. Mourao et al. investigated the impact of temporal evolution of document collections on 1http://socrates.acadiau.ca/courses/comp/dsilver/ NIPS95 LTL/transfer.workshop.1995.html. the document classification (Mourao et al., 2008). Salles et al. presented an approach to classify documents in scenarios where the method uses information about both the past and the future, and this information may change over time (Salles et al., 2010). They address the drawbacks of which instances to select by approximating the Temporal Weighting Function (TWF) using a mixture of two Gaussians. However, their method needs tagged training data across full temporal range of training documents to construct TWF. There are three novel aspects in our method. Firstly, we propose a method for text categorization that minimizes the impact of temporal effects in a learning technique. Secondly, from manual annotation of data perspective, the method allows users to annotate only a limited number of newly training data. Finally, from the perspective of</context>
</contexts>
<marker>Salles, Rocha, Pappa, 2010</marker>
<rawString>T. Salles, L. Rocha, and G. L. Pappa. 2010. Temporally-aware Algorithms for Document Classification. In Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 307–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siao</author>
<author>Y Guo</author>
</authors>
<title>Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.</title>
<date>2013</date>
<booktitle>In Proc. of the 30th International Conference on Machine Learning,</booktitle>
<pages>293--301</pages>
<contexts>
<context position="4065" citStr="Siao and Guo, 2013" startWordPosition="645" endWordPosition="648">val rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku799 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 799–804, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. bat, 2007; Glorot et al., 2011; Siao and Guo, 2013). Domain adaptation is particularly interesting in Natural Language Processing (NLP) because it is often the case that we have a collection of labeled data in one domain but truly desire a model that can work well for another domain. Lots of studies addressed domain adaptation in NLP tasks such as part-of-speech tagging (Siao and Guo, 2013), named-entity (Daum´e III, 2007), and sentiment classification (Glorot et al., 2011) are presented. One approach to domain adaptation is to use transfer learning. The transfer learning is a learning technique that retains and applies the knowledge learned i</context>
</contexts>
<marker>Siao, Guo, 2013</marker>
<rawString>M. Siao and Y. Guo. 2013. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model. In Proc. of the 30th International Conference on Machine Learning, pages 293–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Song</author>
<author>G E Heo</author>
<author>S Y Kim</author>
</authors>
<title>Analyzing topic evolution in bioinformatics: Investigation of dynamics of the field with conference data in dblp.</title>
<date>2014</date>
<journal>Scientometrics,</journal>
<volume>101</volume>
<issue>1</issue>
<contexts>
<context position="3218" citStr="Song et al., 2014" startWordPosition="511" endWordPosition="514">dicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku7</context>
</contexts>
<marker>Song, Heo, Kim, 2014</marker>
<rawString>M. Song, G. E. Heo, and S. Y. Kim. 2014. Analyzing topic evolution in bioinformatics: Investigation of dynamics of the field with conference data in dblp. Scientometrics, 101(1):397–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparinnapakorn</author>
<author>M Kubat</author>
</authors>
<title>Combining Subclassifiers in Text Categorization: A DST-based Solution and a Case Study.</title>
<date>2007</date>
<booktitle>In Proc. of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>210--219</pages>
<contexts>
<context position="5386" citStr="Sparinnapakorn and Kubat, 2007" startWordPosition="861" endWordPosition="865">liest discussion is done by ML community in a NIPS-95 workshop1, and more recently, transfer learning techniques have been successfully applied in many applications. Blitzer et al. proposed a method for sentiment classification using structural correspondence learning that makes use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains (Blitzer et al., 2006). Several authors have attempted to learn classifiers across domains using transfer learning in the text classification task (Raina et al., 2006; Dai et al., 2007; Sparinnapakorn and Kubat, 2007). Raina et al. proposed a transfer learning algorithm that constructs an informative Bayesian prior for a given text classification task (Raina et al., 2006). They reported that a 20 to 40% test error reduction over a commonly used prior in the binary text classification task. Dai et al. presented a method called TrAdaBoost which extends boosting-based learning algorithms (Dai et al., 2007). Their experimental results show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new one. All of these approaches aimed at utilizing a small amount of newly labeled d</context>
</contexts>
<marker>Sparinnapakorn, Kubat, 2007</marker>
<rawString>K. Sparinnapakorn and M. Kubat. 2007. Combining Subclassifiers in Text Categorization: A DST-based Solution and a Case Study. In Proc. of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 210–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D Blei</author>
<author>D Heckerman</author>
</authors>
<title>Continuous Time Dynamic Topic Models.</title>
<date>2008</date>
<booktitle>In Proc. of the 24th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="3308" citStr="Wang et al., 2008" startWordPosition="527" endWordPosition="530">ed the weights of these data by applying Gaussian function in order to weaken their impacts. 2 Related Work The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of people. It is widely studied in many text processing tasks. One attempt is concept or topic drift dealing with temporal effects (Klinkenberg and Joachims, 2000; Kleinberg, 2002; Lazarescu et al., 2004; Folino et al., 2007; Song et al., 2014). Wang et al. developed the continuous time dynamic topic model (cDTM) (Wang et al., 2008). He et al. proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates (He and Parker, 2010). They used Moving Average Convergence/Divergence (MACD) histogram which was used in technical stock market analysis (Murphy, 1999) to detect bursts. Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to another (Daum´e III, 2007; Sparinnapakorn and Ku799 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,</context>
</contexts>
<marker>Wang, Blei, Heckerman, 2008</marker>
<rawString>C. Wang, D. Blei, and D. Heckerman. 2008. Continuous Time Dynamic Topic Models. In Proc. of the 24th Conference on Uncertainty in Artificial Intelligence, pages 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Xue</author>
<author>W Dai</author>
<author>Q Yang</author>
<author>Y Yu</author>
</authors>
<title>Topicbridged PLSA for Cross-Domain Text Classification.</title>
<date>2008</date>
<booktitle>In Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>627--634</pages>
<contexts>
<context position="970" citStr="Xue et al., 2008" startWordPosition="139" endWordPosition="142">osting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 1 Introduction Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (Mourao et al., 2008). A growing number of machine learning (ML) techniques have been applied to the text categorization task (Xue et al., 2008; Gopal and Yang, 2010). Each document is represented using a vector of features/terms (Yang and Pedersen, 1997; Hassan et al., 2007). Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classified by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classification accuracy is worse. However, it is often the case that the term distribution in the training data is different from tha</context>
</contexts>
<marker>Xue, Dai, Yang, Yu, 2008</marker>
<rawString>G. R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topicbridged PLSA for Cross-Domain Text Classification. In Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 627–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization.</title>
<date>1997</date>
<booktitle>In Proc. of the 14th International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<contexts>
<context position="1081" citStr="Yang and Pedersen, 1997" startWordPosition="156" endWordPosition="159"> comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 1 Introduction Text categorization supports and improves several tasks such as creating digital libraries, information retrieval, and even helping users to interact with search engines (Mourao et al., 2008). A growing number of machine learning (ML) techniques have been applied to the text categorization task (Xue et al., 2008; Gopal and Yang, 2010). Each document is represented using a vector of features/terms (Yang and Pedersen, 1997; Hassan et al., 2007). Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classified by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classification accuracy is worse. However, it is often the case that the term distribution in the training data is different from that of the test data when the training data may drive from a different time period from the test data. Manual ann</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. O. Pedersen. 1997. A Comparative Study on Feature Selection in Text Categorization. In Proc. of the 14th International Conference on Machine Learning, pages 412–420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.9969175">
Building a shared world:
Mapping distributional to model-theoretic semantic spaces
</title>
<author confidence="0.905392">
Aur´elie Herbelot
</author>
<affiliation confidence="0.835835">
Universit¨at Stuttgart
</affiliation>
<address confidence="0.67356">
Institut f¨ur Maschinelle Sprachverarbeitung
Stuttgart, Germany
</address>
<email confidence="0.996512">
aurelie.herbelot@cantab.net
</email>
<author confidence="0.991992">
Eva Maria Vecchi
</author>
<affiliation confidence="0.9773705">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.805567">
Cambridge, UK
</address>
<email confidence="0.99903">
eva.vecchi@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908222222222">
In this paper, we introduce an approach to au-
tomatically map a standard distributional se-
mantic space onto a set-theoretic model. We
predict that there is a functional relationship
between distributional information and vecto-
rial concept representations in which dimen-
sions are predicates and weights are gener-
alised quantifiers. In order to test our pre-
diction, we learn a model of such relation-
ship over a publicly available dataset of feature
norms annotated with natural language quan-
tifiers. Our initial experimental results show
that, at least for domain-specific data, we can
indeed map between formalisms, and generate
high-quality vector representations which en-
capsulate set overlap information. We further
investigate the generation of natural language
quantifiers from such vectors.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9974617">
In recent years, the complementarity of distributional
and formal semantics has become increasingly evi-
dent. While distributional semantics (Turney and Pan-
tel, 2010; Clark, 2012; Erk, 2012) has proved very suc-
cessful in modelling lexical effects such as graded sim-
ilarity and polysemy, it clearly has difficulties account-
ing for logical phenomena which are well covered by
model-theoretic semantics (Grefenstette, 2013).
A number of proposals have emerged from these
considerations, suggesting that an overarching seman-
tics integrating both distributional and formal aspects
would be desirable (Coecke et al., 2011; Bernardi et al.,
2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette
et al., 2013; Beltagy et al., 2013; Lewis and Steedman,
2013). We will use the term ‘Formal Distributional Se-
mantics’ (FDS) to refer to such proposals. This paper
follows this line of work, focusing on one central ques-
tion: the formalisation of the systematic dependencies
between lexical and set-theoretic levels.
Let us consider the following examples.
</bodyText>
<listItem confidence="0.9760515">
1. Kim writes books.
2. Kim likes books.
</listItem>
<bodyText confidence="0.999575">
The preferred reading of 1 has a logical form where
the object is treated as an existential, while the object
in 2 has a generic reading:
</bodyText>
<listItem confidence="0.99888">
• ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)]
• GENx[book&apos;(x) → like&apos;(Kim,x)]
</listItem>
<bodyText confidence="0.997574848484848">
with x* indicating a plurality and GEN the generic
quantifier.
It is generally accepted that the appropriate choice
of quantifier for an ambiguous bare plural object de-
pends, amongst other things, on the lexical semantics
of the verb (e.g. Glasbey (2006)). This type of inter-
action implies the existence of systematic influences of
the lexicon over logic, which could in principle be for-
malised.
A model of the lexicon/logic interface would be de-
sirable to explain how speakers resolve standard cases
of ambiguity like the bare plural in 1 and 2, but more
generally, it could be the basis for answering a more
fundamental question: how do speakers construct a
model of a sentence for which they have no prior per-
ceptual data?
People can make complex inferences about state-
ments without having access to their real-world ref-
erence. As an example, consider the sentence The
kouprey is a mammal. English speakers have no
problem ascertaining that if x is a kouprey, x is a
mammal (which set-theoretic semantics would express
as dx[kouprey&apos;(x) → mammal&apos;(x)]), regardless of
whether they have ever encountered a kouprey. The in-
ference is supported by the lexical semantics of mam-
mal, which applies a property (being a mammal) to all
instances of a class. Much more complex inferences
are routinely performed by speakers, down to estimat-
ing the cardinality of the entities involved in a partic-
ular situation. Compare e.g. The cats are on the sofa
(2 / a few cats?), I picked pears today (a few / a few
dozen?) and The protesters were blocking the entire
avenue (hundreds/thousands of protesters?).
</bodyText>
<page confidence="0.985347">
22
</page>
<note confidence="0.985003">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 22–32,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999874027777778">
Understanding how this process works would not
only give us an insight into a complex cognitive pro-
cess, but also make a crucial contribution to NLP tasks
relying on inference (e.g. the Recognising Textual En-
tailment challenge, RTE: Dagan et al. (2009)). In-
deed, while systems have successfully been developed
to model entailment between quantifiers, ranging from
natural logic approaches (MacCartney and Manning,
2008) to distributional semantics solutions (Baroni et
al., 2012), they rely on an explicit representation of
quantification. That is, they can model the entailment
All koupreys are mammals |= This kouprey is a mam-
mal, but not Koupreys are mammals |= This kouprey is
a mammal.
In this work, we assume the existence of a mapping
between language (distributional models) and world
(set-theoretic models), or to be more precise, between
language and a shared set of beliefs about the world, as
negotiated by a group of speakers. To operationalise
this mapping, we propose that set-theoretic models,
like distributions, can be expressed in terms of vec-
tors – giving us a common representation across for-
malisms. Using a publicly available dataset of feature
norms annotated with quantifiers1 (Herbelot and Vec-
chi, 2015), we show that human-like intuitions about
the quantification of simple subject/predicate pairs can
be induced from standard distributional data.
This paper is structured as follows. §2 reviews re-
lated work, focusing in turn on approaches to formal
distributional semantics, computational work on quan-
tification, and mapping between semantic spaces. In
§3, we describe our dataset. §4 and §5 describe our
experiments, reporting correlation against human an-
notations. We discuss our results in §6 and end with an
attempt at generating natural language quantifiers from
our mapped vectors (§7).
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.979077">
2.1 Formal Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.9999575">
The relation between distributional and formal seman-
tics has been the object of a number of studies in re-
cent years. Proposals for a FDS, i.e. a combination
of both formalisms, roughly fall into two groups: a)
the fully distributional approaches, which redefine the
concepts of formal semantics in distributional terms
(Coecke et al., 2011; Bernardi et al., 2013; Grefen-
stette, 2013; Hermann et al., 2013; Baroni et al., 2014a;
Clarke, 2012); b) the hybrid approaches, which try to
keep the set-theoretic apparatus for function words and
integrate distributions as content words representations
(Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013;
Lewis and Steedman, 2013). This paper follows the hy-
brid frameworks in that we fully preserve the principles
of set theory and do not attempt to give a distributional
interpretation to phenomena traditionally catered for by
</bodyText>
<footnote confidence="0.9871745">
1Data available at http://www.cl.cam.ac.uk/
˜ah433/mcrae-quantified-majority.txt
</footnote>
<bodyText confidence="0.999434380952381">
formal semantics such as quantification or negation.
Our account is also similar to that proposed by Erk
(2015). Erk suggests that distributional data influences
semantic ‘knowledge’2: specifically, while a speaker
may not know the extension of the word alligator, they
maintain an information state which models properties
of alligators (for instance, that they are animals). This
information state is described in terms of probabilistic
logic, which accounts for an agent’s uncertainty about
what the world is like. The probability of a sentence
is the summed probability of the possible worlds that
make it true. Similarly, we assume a systematic relation
between distributional information and world knowl-
edge, expressed set-theoretically. The knowledge rep-
resentation we derive is not a model proper: it cannot
be said to be a description of a world – either the real
one or a speaker’s set of beliefs (c.f. §4 for more de-
tails). But it is a good approximation of the shared in-
tuitions people have about the world, in the way that
distributional representations are an averaged represen-
tation of how a group of speakers use their language.
</bodyText>
<subsectionHeader confidence="0.993336">
2.2 Generalised quantifiers
</subsectionHeader>
<bodyText confidence="0.999726181818182">
Computational semantics has traditionally focused on
very specific aspects of quantification. There is a large
literature on the computational formalisation of quan-
tifiers as automata, starting with Van Benthem (1986).
In parallel to this work, much research has been done
on drawing inferences from explicitly quantified state-
ments – i.e. statements quantified with determiners
such as some/most/all, which give information about
the set overlap of a subject-predicate pair (Cooper et
al., 1996; Alshawi and Crouch, 1992; MacCartney and
Manning, 2008). Recent work in this area has even
shown that entailment between explicit quantifiers can
be modelled distributionally (Baroni et al., 2012). A
complementary object of focus, actively pursued in the
1990s, has been inference between generic statements
(Bacchus, 1989; Vogel, 1995).
Beside those efforts, computational approaches have
been developed to convert arbitrary text into logical
forms. The techniques range from completely super-
vised (Baldwin et al., 2004; Bos, 2008) to lightly su-
pervised (Zettlemoyer and Collins, 2005). Such work
has shown that it was possible to automatically give
complex formal semantics analyses to large amounts
of data. But the formalisation of quantifiers in those
systems either remains very much underspecified (e.g.
bare plurals are not resolved into either existentials or
generics) or relies on some grounded information, for
example in the form of a database.
To the best of our knowledge, no existing system is
able to universally predict the generalised quantifica-
tion of noun phrases, including those introduced by the
(in)definite singulars a/the and definite plurals the. The
closest attempt is Herbelot (2013), who suggests that
</bodyText>
<footnote confidence="0.9780135">
2We use the term knowledge loosely, to refer to a
speaker’s beliefs about the world or a state of affairs.
</footnote>
<page confidence="0.998526">
23
</page>
<figure confidence="0.983723">
Concept
ALL
MOST
SOME
FEW
ALL
tricycle
FEW
NO
</figure>
<tableCaption confidence="0.995986">
Table 1: Example annotations for concepts.
</tableCaption>
<bodyText confidence="0.999478055555555">
‘model-theoretic vectors’ can be built out of distribu-
tional vectors supplemented with manually annotated
training data. The proposed implementation, however,
fails to validate the theory.
Our work follows the intuition that distributions can
be translated into set-theoretic equivalents. But it im-
plements the mapping as a systematic linear transfor-
mation. Our approach is similar to Gupta et al. (2015),
who predict numerical attributes for unseen concepts
(countries and cities) from distributional vectors, get-
ting comparably accurate estimates for features such as
the GDP or CO2 emissions of a country. We comple-
ment such research by providing a more formal inter-
pretation of the mapping between language and world
knowledge. In particular, we offer a) a vectorial repre-
sentation of set-theoretic models; b) a mechanism for
predicting the application of generalised quantifiers to
the sets in a model.
</bodyText>
<subsectionHeader confidence="0.99989">
2.3 Mapping between Semantic Spaces
</subsectionHeader>
<bodyText confidence="0.999935761904762">
The mapping between different semantic modalities or
semantic spaces has been explored in various aspects.
In cognitive science, research by Riordan and Jones
(2011) and Andrews et al. (2009) show that models that
map between and integrate perceptual and linguistic in-
formation perform better at fitting human semantic in-
tuition. In NLP, Mikolov et al. (2013b) show that a
linear mapping between vector spaces of different lan-
guages can be learned to infer missing dictionary en-
tries by relying on a small amount of bilingual infor-
mation. Frome et al. (2013) learn a linear regression
to transform vector-based image representations onto
vectors representing the same concepts in a linguistic
semantic space, and Lazaridou et al. (2014) explore
mapping techniques to learn a cross-modal mapping
between text and images with promising performance.
We follow the basic intuition introduced by these pre-
vious studies: a simple linear function can map be-
tween semantic spaces, in this case between a linguistic
(distributional) semantic space and a model-theoretic
space.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="method">
3 Annotated datasets
</sectionHeader>
<subsectionHeader confidence="0.999816">
3.1 The quantified McRae norms
</subsectionHeader>
<bodyText confidence="0.999978666666666">
The McRae norms (McRae et al., 2005) are a set of
feature norms elicited from 725 human participants for
541 concepts covering living and non-living entities
(e.g. alligator, chair, accordion). The annotators were
given concepts and asked to provide features for them,
covering physical, functional and other properties. The
result is a set of 7257 concept-feature pairs such as air-
plane used-for-passengers or bear is-brown.
In our work, we use the annotation layer pro-
duced by Herbelot and Vecchi (2015) for the McRae
norms (henceforth QMR): for each concept-feature
pair (C, f), the annotation provides a natural language
quantifier expressing the ratio of instances of C having
the feature f, as elicited by three coders. The quan-
tifiers in use are NO, FEW, SOME, MOST, ALL. Ta-
ble 1 provides example annotations for concept-feature
pairs (reproduced from the original paper). An addi-
tional label, KIND, was introduced for usages of the
concept as a kind, where quantification does not ap-
ply (e.g. beaver symbol-of-Canada). A subset of the
annotation layer is available for training computational
models, corresponding to all instances with a majority
label (i.e. those where two or three coders agreed on a
label). The reported average weighted Cohen kappa on
this data is rc = 0.59.
In the following, we use a derived gold standard in-
cluding all 5 quantified classes in QMR (removing the
KIND items), with the annotation set to majority opin-
ion (6156 instances). The natural language quantifiers
are converted to a numerical format (see §4 for details).
Using the numerical data, we can calculate the mean
Spearman rank correlation between the three annota-
tors, which comes to 0.63.
</bodyText>
<subsectionHeader confidence="0.999959">
3.2 Additional animal data
</subsectionHeader>
<bodyText confidence="0.9999805">
QMR gives us an average of 11 features per con-
cept. This results in fairly sparse vectors in the model-
theoretic semantic space (see §4). In order to remedy
data sparsity, we consider the use of additional data in
the form of the animal dataset from Herbelot (2013)
(henceforth AD). AD3 is a set of 72 animal concepts
with quantification annotations along 54 features. The
main differences between QMR and AD are as follows:
</bodyText>
<listItem confidence="0.964456285714286">
• Nature of features: the features in AD are not hu-
man elicited norms, but linguistic predicates ob-
tained from a corpus analysis.
• Comprehensiveness of annotation: the 72 con-
cepts were annotated along all 54 features. This
ensures the availability of a large number of nega-
tively quantified pairs (e.g. cat is-fish).
</listItem>
<bodyText confidence="0.99792925">
We manually align the AD concepts and features to
the QMR format, changing e.g. bat to bat (animal).
The QMR and AD sets have an overlap of 39 concepts
and 33 features.
</bodyText>
<footnote confidence="0.930948666666667">
3Data available at http://www.cl.cam.ac.uk/
˜ah433/material/herbelot_iwcs13_data.
txt.
</footnote>
<figure confidence="0.994802615384615">
ape
Feature
is muscular
is wooly
lives on coasts
is blind
has 3 wheels
used by children
is small
used for transportation
a bike
MOST
SOME
</figure>
<page confidence="0.987497">
24
</page>
<sectionHeader confidence="0.976953" genericHeader="method">
4 Semantic spaces
</sectionHeader>
<bodyText confidence="0.9999675">
We construct two distinct semantic spaces (distribu-
tional and model-theoretic), as described below.
</bodyText>
<subsectionHeader confidence="0.998264">
4.1 The distributional semantic space
</subsectionHeader>
<bodyText confidence="0.999986458333333">
We consider two distributional semantic space archi-
tectures which have each shown to have considerable
success in a number of semantic tasks. First, we build
a co-occurrence based space (DScooc), in which a word
is represented by co-occurrence counts with content
words (nouns, verbs, adjectives and adverbs). As a
source corpus, we use a concatenation of the ukWaC,
a 2009 dump of the English Wikipedia and the BNC4,
which consists of about 2.8 billion tokens. We select
the top 10K content words for the contexts, using a bag-
of-words approach and counting co-occurrences within
a sentence. We then apply positive Pointwise Mutual
Information to the raw counts, and reduce the dimen-
sions to 300 through Singular Value Decomposition.5
Next we consider the context-predicting vectors
(DSMikolov) available as part of the word2vec6 project
(Mikolov et al., 2013a). We use the publicly avail-
able vectors which were trained on a Google News
dataset of circa 100 billion tokens. Baroni et al. (2014b)
showed that vectors constructed under this architecture
outperform the classic count-based approaches across
many semantic tasks, and we therefore explore this op-
tion as a valid distributional representation of a word’s
semantics.
</bodyText>
<subsectionHeader confidence="0.997967">
4.2 The model-theoretic space
</subsectionHeader>
<bodyText confidence="0.999948190476191">
Our ‘model-theoretic space’ differs in a couple of
important respects from traditional formal semantics
models. So it may be helpful to first come back to
the standard definition of a model, which relies on two
components: an ontology and a denotation function
(Cann, 1993). The ontology describes a world (which
can be a simple situation or ‘state of affairs’), with ev-
erything that is contained in that world. Ontologies can
be represented in various ways, but in this paper, we
assume they are formalised in terms of sets of entities.
The denotation function associates words with their ex-
tensions in the model, i.e. the sets they refer to. Thanks
to the availability of the ontology, it is possible to define
a truth function for sentences, which computes whether
a particular statement corresponds to the model or not.
In our account, we do not have an a priori model of
the world: we wish to infer it from our observation of
language data. We believe this to be an advantage over
traditional formal semantics, which requires full onto-
logical data to be available in order to account for refer-
ence and truth conditions, but never spells out how this
</bodyText>
<footnote confidence="0.993888666666667">
4http://wacky.sslmit.unibo.it, http:
//www.natcorp.ox.ac.uk
5All semantic spaces, both distributional and model-
theoretic, were built using the DISSECT toolkit (Dinu et al.,
2013).
6https://code.google.com/p/word2vec
</footnote>
<bodyText confidence="0.99995032">
data comes into being. This however implies that our
produced ontology will necessarily be partial: we can
only model what can be inferred from language use.
This has consequences for the denotation function.
Let’s imagine a world with three cats and two horses.
In model theory, the word horse has an extension in that
world which is the set of horses, with a cardinality of
two. This can be trivially derived because the world is
fully described in the ontology. In our approach, how-
ever, it is unlikely we might be able to learn the cardi-
nality of any set in any world. And in fact, it is clear
that ‘in real life’, speakers do miss this information for
many sets (how many horses are there in the world?)
Note that we do not in principle reject the possibility
to learn cardinalities from distributional data (for an
example of this, see Gupta et al. (2015)). We simply
remark that this will not always possible, or even desir-
able from a cognitive point of view. By extension, this
means that a model built from distributional data does
not support denotation in the standard way, and thus
precludes the definition of a truth function: we cannot
verify the truth of the sentence There are 25,957 white
horses in the world. Our ‘model-theoretic’ space may
then be described as an underspecified set-theoretic
representation of some shared beliefs about the world.
Our ‘ontology’ can be defined as follows. To each
word wk in vocabulary V = w1...m corresponds
a set w0k with underspecified cardinality. A num-
ber of predicates p01...n are similarly defined as sets
with an unknown number of elements. Our claim
is that this very underspecified model can be fur-
ther specified by learning a function F from dis-
tributions to generalised quantifiers. Specifically,
F( wk) = {Q1(w0 k,p0 1),Q2(w0 k,p0 2)...Qn(w0k,p0 n)},
where wk is the distribution of wk and Q1...Qn E
{no, few, some, most, all} . That is, F takes a dis-
tribution wk and returns a quantifier for each predicate
in the model, corresponding to the set overlap between
w0k and p01...n. Note that we focus here on 5 quanti-
fiers only, but as mentioned above, we do not preclude
the possibility of learning others (including cardinals in
appropriate cases).
F( wk) lives in a model-theoretic space which
broadly follows the representation suggested by Her-
belot (2013). We assume a space with n dimensions
d1...dn which correspond to predicates p01...n (e.g. is
fluffy, used for transportation). In that space, F( wk) is
weighted along the dimension dm in proportion to the
set overlap w0k ∩p0 m.7 The following shows a toy vector
with only four dimensions for the concept horse.
</bodyText>
<figure confidence="0.7466465">
a mammal 1
has four legs 0.95
is brown 0.35
is scaly 0
</figure>
<footnote confidence="0.997941">
7In Herbelot (2013), weights are taken to be probabilities,
but we prefer to talk of quantifiers, as the notion models our
data more directly.
</footnote>
<page confidence="0.997949">
25
</page>
<bodyText confidence="0.999941">
This vector tells us that the set of horses includes
the set of mammals (the number of horses that are also
mammals divided by the number of horses comes to 1,
i.e. all horses are mammals), and that the set of horses
and the set of things that are scaly are disjoint (no horse
is scaly). We also learn that a great majority of horses
have four legs and that some are brown.
In the following, we experiment with 3 model-
theoretic spaces built from the McRae and AD datasets
described in §3. As both datasets are annotated with
natural language quantifiers rather than cardinality ra-
tios, we convert the annotation into a numerical for-
mat, where ALL → 1, MOST → 0.95, SOME → 0.35,
FEW → 0.05, and NO → 0. These values correspond
to the weights giving the best inter-annotator agree-
ment in Herbelot and Vecchi (2015), when calculating
weighted Cohen’s kappa on QMR.
In each model-theoretic space, a concept is repre-
sented as a vector in which the dimensions are features
(has buttons, is green), and the values of the vectors
along each dimension are quantifiers (in numerical for-
mat). When a feature does not occur with a concept
in QMR, the concept’s vector receives a weight of 0
on the corresponding dimension.8 We define 3 spaces
as follows. The McRae-based model-theoretic space
(MTQMR) contains 541 concepts, as described in §3.1.
The second space is constructed specifically for the ad-
ditional animal data from §3.2 (MTAD). Finally, we
merge the two into a single space of 555 unique con-
cepts (MTQMR+AD).
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992059">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999966142857143">
To map from one semantic representation to another,
we learn a function f : DS → MT that transforms
a distributional semantic vector for a concept to its
model-theoretic equivalent.
Following previous research showing that similari-
ties amongst word representations can be maintained
within linear transformations (Mikolov et al., 2013b;
Frome et al., 2013), we learn the mapping as a linear
relationship between the distributional representation
of a word and its model-theoretic representation. We
estimate the coefficients of the function using (multi-
variate) partial least squares regression (PLSR) as im-
plemented in the R pls package (Mevik and Wehrens,
2007).
We learn a function from the distributional space to
each of the model-theoretic spaces (c.f. §4). The dis-
tribution of training and test items is outlined in Ta-
ble 2, expressed as a number of concept vectors. We
also include the number of quantified instances in the
test set (i.e. the number of actual concept-feature pairs
that were explicitly annotated in QMR/AD and that
</bodyText>
<footnote confidence="0.985757">
8No transformations or dimensionality reductions were
performed on the MT spaces.
</footnote>
<table confidence="0.9894872">
Space # train # test # dims # test
vec. vec. inst.
MTQMR 400 141 2172 1570
MTAD 60 12 54 648
MTQMR+AD 410 145 2193 1595
</table>
<tableCaption confidence="0.971043">
Table 2: Distribution of training/test items for each
</tableCaption>
<bodyText confidence="0.9353552">
model-theoretic semantic space. We also provide the
number of dimensions for each space, and the actual
number of concept-feature instances tested on.
we can thus evaluate – this is a portion of each concept
vector in the spaces including QMR data).
</bodyText>
<subsectionHeader confidence="0.703863">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999973425">
We first consider a preliminary quantitative analysis to
better understand the behavior of the transformations,
while a more qualitative analysis is provided in §6. The
results in Table 3 show the degree to which predicted
values for each dimension in a model-theoretic space
correlate with the gold annotations, operationalised as
the Spearman p (rank-order correlation). Wherever ap-
propriate, we also report the mean Spearman correla-
tion between the three human annotators for the par-
ticular test set under consideration, showing how much
they agreed on their judgements.9 These figures pro-
vide an upper bound performance for the system, i.e.
we will consider having reached human performance if
the correlation between system and gold standard is in
the same range as the agreement between humans. For
each mapping tested, Table 3 provides details about the
training data used to learn the mapping function and
the test data for the respective results. Also for each
mapping, results are reported when learned from either
the co-occurrence distributional space (DS,oo,) or the
context-predicting distributional space (DSMikolov).
The top section of the table reports results for the
QMR and AD dataset taken separately, as well as their
concatenation. Performance on the domain-specific
AD is very promising, at 0.641 correlation, calculated
over 648 test instances. The results when trained on
just the QMR features (MTQMR) are much lower (0.35
over 1570 test instances), which we put down to the
wider variety of concepts in that dataset; we however
observe a substantial increase in performance when
we train and test over the two datasets (MTQMR+AD:
0.569 over 1595 instances).
We investigate whether merging the datasets gen-
erally benefits QMR concepts or just the animals
(see middle section in Table 3). The result on the
MTanimals test set, which includes animals from the
AD and QMR datasets, shows that this category fares
indeed very well, at p = 0.663. But while augment-
ing the training data with category-specific datapoints
benefits that category, it does not improve the results
</bodyText>
<footnote confidence="0.766908">
9These figures are only available for the QMR dataset, as
AD only contains one annotation per subject-predicate pair.
</footnote>
<page confidence="0.976405">
26
</page>
<table confidence="0.999865">
Model-Theoretic Distributional human
train test DScooc DSMikolov
MTQMR MTQMR 0.350 0.346 0.624
MTAD MTAD 0.641 0.634 –
MTQMR+AD MTQMR+AD 0.569 0.523 –
MTQMR+AD MTanimals 0.663 0.612 –
MTQMR+AD MTno-animals 0.353 0.341 –
MTQMR MTQMRanimals 0.419 0.405 –
MTQMR+AD MTQMRanimals 0.666 0.600 0.663
</table>
<tableCaption confidence="0.8342055">
Table 3: (Spearman) correlations of mapped dimensions with gold annotations for all test items. The table reports
results (p) when mapped from a distributional space (DScooc or DSMikolov) to each MT space, as well as the
</tableCaption>
<bodyText confidence="0.987839361111111">
correlation with human annotations when available. The train/test data for the mappings is specified in Table 2.
For further analysis we report the results when tested only on animal test items (animals), or on all test items but
animals (no-animals). MTanimals contains test items from both AD and the animal section of the McRae norms.
See text for more details.
for concepts of other classes (c.f. compare MTanimals
with MTno-animals).
Finally, we quantify the specific improvement to the
QMR animal concepts by comparing the correlation
obtained on MTQMRanimals (a test set consisting only
of QMR animal features) after training on a) the QMR
data alone and b) the merged dataset (third section of
Table 3). Performance increases from 0.419 to 0.666 on
that specific set. This is in line with the inter-annotator
agreement (0.663).
To summarise, we find that the best correlations
with the gold annotations are seen when we in-
clude the animal-only dataset in training (MTAD
and MTQMR+AD) and test on just animal concepts
(MTAD, MTanimals and MTQMRanimals). As one
might expect, category-specific training data yields
high performance when tested on the same category.
Although this expectation seems intuitive, it is worth
noting that our system produces promisingly high cor-
relations, reaching human-performance on a subset of
our data. The assumption we can draw from these
results is that, given a reasonable amount of training
data for a category, we can proficiently generate model-
theoretic representations for concept-feature pairs from
a distributional space. The empirical question remains
whether this can be generalized for all categories in the
QMR dataset.
It is important to keep in mind that the MT spaces
are not full matrices, meaning that we have ‘miss-
ing values’ for various dimensions when a concept
is converted into a vector. For example, the feature
has a tail is not among the annotated features for bear
in QMR and has a weight of 0, even though most bears
have a tail. This is a consequence of the original McRae
dataset, rather than the design of our approach. But
it follows that in this quantitative analysis, we are not
able to confirm the accuracy of the predicted values
on dimensions for which we do not have gold anno-
tations. This may also affect the performance of the
system by including ‘false’ 0 weights in the training
% of gold in...
top 5 neighbours 19% (27/145)
top 10 neighbours 29% (42/145)
top 20 neighbours 46% (67/145)
Table 4: Percentage of gold vectors found in the top
neighbours of the mapped concepts, shown for the
DScooc → MTQMR+AD transformation.
data. Although this does not affect our reported cor-
relation results – we test the correlations on those val-
ues for which we have gold annotations only – it does
open the door to a natural next step in the evaluation.
In order to judge the performance of the system on the
missing gold dimensions, we need a manual analysis
to assess the quality of the whole vectors, which goes
hand-in-hand with obtaining additional annotations for
the missing dimensions. It seems, therefore, that an ac-
tive learning strategy would allow us to not only eval-
uate the model-theoretic vectors more fully, but also
improve the system by capturing new data.10
In this analysis, we focused primarily on the com-
parison between transformations using various truth-
theoretic datasets for training and generation. We leave
it to further work to extensively compare the effect of
varying the type of the distributional space. Our re-
sults show, however, that the Mikolov model performs
slightly worse than the co-occurrence space (cooc), dis-
proving the idea that predictive models always outper-
form count-based models.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.965572571428571">
To further assess the quality of the produced space, we
perform a nearest-neighbour analysis of our results to
evaluate the coherence of the estimated vectors: for
10As suggested by a reviewer, one could also treat the miss-
ing entries as latent dimensions and define the loss function
on only the known entries. We leave it to future work to test
this promising option to resolve the issue of data sparsity.
</bodyText>
<page confidence="0.995517">
27
</page>
<bodyText confidence="0.981239125">
axe hatchet
a tool a tool
is sharp is sharp
has a handle has a handle
used for cutting used for cutting
has a metal blade made of metal
a weapon an axe
has a head is small
</bodyText>
<table confidence="0.6058175">
used for chopping –
has a blade –
is dangerous –
is heavy –
used by lumberjacks –
used for killing –
</table>
<tableCaption confidence="0.98824">
Table 5: McRae feature norms for axe and hatchet
</tableCaption>
<bodyText confidence="0.986579557377049">
each concept in our test set, we return its nearest neigh-
bours from the gold dataset, as given by the cosine sim-
ilarity measure, hoping to find that the estimated vector
is close to its ideal representation (see F˘ag˘ar˘as¸an et al.
(2015) for a similar evaluation on McRae norms). Re-
sults are shown in Table 4. We find that the gold vector
is among the top 5 nearest neighbours to the predicted
equivalent in nearly 20% of concepts, with the percent-
age of gold items in the top neighbours improving as
we increase the size of the neighbourhood. We per-
form a more in-depth analysis of the neighbourhoods
for each concept to gain a better understanding of their
behaviour and quality.
We discover that, in many cases, the mapped vector
is close to a similar concept in the gold standard, but not
−−−−−−→
to itself. So for instance, alligatormapped is very close
to crocodilegold, but not to alligatorgold. Similar find-
ings are made for church/cathedral, axe/hatchet, dish-
washer/fridge, etc. A further investigation show that in
the gold standard itself, those pairs are not as close to
each other as they should be. Here are some relevant
cosine similarities:
alligator − crocodile 0.47
church − cathedral 0.45
axe − hatchet 0.50
dishwasher − fridge 0.21
Two reasons can be identified for these compara-
tively low11 similarities. First, the McRae norms do not
make for a consistent semantic space because a feature
that – from an extensional point of view – seems rele-
vant to two concepts may only have been produced by
the annotators for one of them. As an example of this,
see Table 5, which shows the feature norms for axe and
hatchet after processing (§3). Although the concepts
share 4 features, they also differ quite strongly, an axe
being seen as a weapon with a blade, while the hatchet
is itself referred to as an axe. Extensionally, of course,
there is no reason to think that a hatchet does not have
11Compare with e.g. ape - monkey, Sim = 0.97.
a blade or might not be dangerous, but those features
do not appear in the norms for the concept. This re-
sults in the two vectors being clearly separated in the
set-theoretic space. This means that the distribution of
axe may well be mapped to a region close to hatchet,
but thereby ends up separated from the gold axe vector.
The second, related issue is that the animal con-
cepts in the McRae norms are annotated along fewer
dimensions than in AD. For example, alligator – which
only appears in the McRae set – has 13 features, while
crocodile (in both sets) has 70. Given that features
which are not mentioned for a concept receive a weight
of 0, this also results in very different vectors.
In Table 6, we provide the top weighted features for
a small set of concepts. As expected, the animal repre-
sentations (bear, housefly) have higher quality than the
other two (plum, cottage). But overall, the ranking of
dimensions is sensible. We see also that these represen-
tations have ‘learnt’ features for which we do not have
values in our gold data – thereby correcting some of the
0 values in the training vectors.
</bodyText>
<sectionHeader confidence="0.688824" genericHeader="method">
7 Generating natural language
quantifiers
</sectionHeader>
<bodyText confidence="0.999930647058823">
In a last experiment, we attempt to map the set-
theoretic vectors obtained in §5 back to natural lan-
guage quantifiers. This last step completes our
pipeline, giving us a system that produces quantified
statements of the type All dogs are mammals or Some
bears are brown from distributional data.
For each mapped vector F( ~wk) = ~vk and a set of di-
mensions d1...n corresponding to properties p&apos;1...n, the
value of ~vk along each dimension is indicative of the
proportion of instances of w&apos;k having the property sig-
nalled by the dimension. The smaller the value, the
smaller the overlap between the set of instances of w&apos;k
and the set of things having the property. Deriving
natural language quantifiers from these values involves
setting four thresholds tall, tmost, tsome and tfew so
that for instance, if the value of ~vk along dm is more
than tall, it is the case that all instances of ~wk have
property pm, and similarly for the other quantifiers (no
has a special status as it is not entailed by any of the
other quantifiers under consideration). We set the t-
thresholds by a systematic search on a training set (see
below).
To evaluate this step, we propose a function that cal-
culates precision while taking into account the two fol-
lowing factors: a) some errors are worse than others:
the system shouldn’t be overly penalised for classifying
a property as MOST rather than ALL, but much more for
classifying a gold standard ALL as SOME; b) errors that
are conducive to false inferences should be strongly pe-
nalised, e.g. generating all dogs are black is more seri-
ous than some dogs are mammals, because the former
might lead to incorrect inferences with respect to indi-
vidual dogs while the latter is true, even though it is
pragmatically odd.
</bodyText>
<page confidence="0.99643">
28
</page>
<bodyText confidence="0.99951219047619">
bear housefly plum cottage
an animal an insect a fruit has a roof
a mammal is small grows on trees used for shelter*
has eyes flies tastes sweet has doors*
is muscular is slender* is edible a house
has a head crawls* is round has windows
has 4 legs stings* is small is small
has a heart has legs has skin a building*
is terrestrial is large* is juicy used for living in
has hair a bug* tastes good made of wood*
is brown has wings has seeds* made by humans*
walks is black is green* worn on feet*
is wooly is terrestrial* has peel* has rooms*
has a tail* hibernates* is orange* used for storing farm equipment*
a carnivore has a heart* is citrus* found on farms*
is large has eyes is yellow* found in the country
a predator has antennae* has vitamin C* an appliance*
is furry* bites* has leaves* has tenants*
roosts jumps* has a pit has a bathroom*
is stout has a head* has a stem* requires rent*
hunted by people is grey* grows in warm climates* requires a landlord*
</bodyText>
<tableCaption confidence="0.707992333333333">
Table 6: Example of 20 most weighted contexts in the predicted model-theoretic vectors for 4 test concepts, shown
for the DScooc → MTMcRae+AD transformation. Features marked with an asterisk (∗) are not among the concept’s
features in the gold data.
</tableCaption>
<table confidence="0.818838625">
Gold
no few some most all
Mapped no 0 -0.05 -0.35 -0.95 -1
few -0.05 0 0.2 0.9 0.95
some -0.35 -0.2 0 0.6 0.65
most -0.95 -0.9 -0.6 0 0.05
all -1 -0.95 -0.65 -0.05 0
Table 7: Distance matrix for the evaluation of the natu-
ral language quantifiers generation step.
Gold
no few some most all
Mapped no 238 66 20 4 2
few 53 45 30 19 12
some 6 1 2 3 2
most 4 6 4 16 56
all 0 0 0 2 3
</table>
<tableCaption confidence="0.8275775">
Table 8: Confusion matrix for the results of the natural
language quantifiers generation.
</tableCaption>
<bodyText confidence="0.999958333333333">
We set a distance matrix, which we will use for pe-
nalising errors. This matrix, shown in Table 7, is ba-
sically equivalent to the matrix used by Herbelot and
Vecchi (2015) to calculate weighted kappa between
annotators, with the difference that all errors involv-
ing NO cause incorrect inferences and receive special
treatment. Cases where the gold quantifier entails the
mapped quantifier (all cats |= some cats) have posi-
tive distances, while cases where the entailment doesn’t
hold have negative distances. Using the distance ma-
trix, we give a score to each instance in our test data as
follows:
</bodyText>
<equation confidence="0.906398666666667">
�
_ 1 − d if d ≥ 0
s (1)
</equation>
<bodyText confidence="0.983734129032258">
d if d &lt; 0
where d is obtained from the distance matrix.
This has the effect that when the mapped quantifier
equals the gold quantifier, the system scores 1; when
the mapped value deviates from the gold standard but
produces a true sentence (some dogs are mammals), the
system gets a partial score proportional to the distance
between its output and the gold data; when the map-
ping results in a false sentence (all dogs are black), the
system is penalised with minus points.
In what follows, we report the average performance
� sm
of the system as P = N where sm is the score
assigned to a particular test instance, and N is the
number of test instances. We evaluate on the 648 test
instances of MTAD, as this is the only dataset con-
taining a fair number of negatively quantified concept-
predicate pairs. We perform 5-fold cross-evaluation on
this data, using 4 folds to set the t thresholds, and test-
ing on one fold. We obtain an average P of 0.61. Infer-
ence is preserved in 73% of cases (also averaged over
the 5 folds).
Table 8 shows the confusion matrix for our results.
We note that the system classifies NO-quantified in-
stances with good accuracy (72% – most confusions
being with FEW). Because of the penalty given to
instances that violate proper entailment, the system
is conservative and prefers FEW to SOME, as well as
MOST to ALL. Table 9 shows randomly selected in-
stances, together with their mapped quantifier and the
label from the gold standard.
</bodyText>
<page confidence="0.997101">
29
</page>
<bodyText confidence="0.9554174">
Instance Mapped Gold
raven a bird most all
pigeon has hair few no
elephant has eyes most all
crab is blind few few
snail a predator no no
octopus is stout no few
turtle roosts no few
moose is yellow no no
cobra hunted by people some some
snail forages few no
chicken is nocturnal few no
moose has a heart most all
pigeon hunted by people no few
cobra bites few most
</bodyText>
<tableCaption confidence="0.902086">
Table 9: Examples of mapped concept-predicate pairs
</tableCaption>
<sectionHeader confidence="0.994542" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999989550724638">
In this paper, we introduced an approach to map from
distributional to model-theoretic semantic vectors. Us-
ing traditional distributional representations for a con-
cept, we showed that we are able to generate vecto-
rial representations that encapsulate generalised quan-
tifiers.
We found that with a relatively “cheap” linear func-
tion – cheap in that it is easy to learn and requires mod-
est training data – we can reproduce the quantifiers in
our gold annotation with high correlation, reaching hu-
man performance on a domain-specific test set. In fu-
ture work, we will however explore the effect of more
powerful functions to learn the transformations from
distributional to model-theoretic spaces.
Our qualitative analysis showed that our predicted
model-theoretic vectors sensibly model the concepts
under consideration, even for features which do not
have gold annotations. This is not only a promising
result for our approach, but it provides potential as a
next step to this work: expanding our training data with
non-zero dimensions in an active learning procedure.
We also experimented with generating natural language
quantifiers from the mapped vectorial representations,
producing ‘true’ quantified sentences with a 73% accu-
racy.
We note that our approach gives a systematic way
to disambiguate non-explicitly quantified sentences
such as generics, opening up new possibilities for im-
proved semantic parsing and recognising entailment.
Right now, many parsers give the same broad anal-
ysis to Mosquitoes are insects and Mosquitoes carry
malaria, involving an underspecified/generic quanti-
fier. This prevents inferring, for instance, that Mandie
the mosquito is definitely an insect but may or may
not carry malaria. In contrast, our system would at-
tribute the most plausible quantifiers to those sentences
(all/few), allowing us to produce correct inferences.
The focus of this paper was concept-predicate pairs
out of context. That is, we considered quantified sen-
tences where the restrictor was the entire set denoted
by a lexical item. A natural next step is to inves-
tigate the quantification of statements involving con-
textualised subsets. For instance, we should obtain a
different quantifier for taxis are yellow depending on
whether the sentence starts with In London... or In New
York... In future work, we will test our system on such
context-specific examples, using contextualised vector
representations such as the ones proposed by e.g. Erk
and Pad´o (2008) and Dinu and Lapata (2010).
We conclude by noting again that the set-theoretic
models produced in this work differ from formal se-
mantics models in important ways. They do not rep-
resent the world per se, but rather some shared beliefs
about the world, induced from an annotated dataset of
feature norms. This calls for a modified version of the
standard denotation function and for the replacement of
the truth function with a ‘plausibility’ function, which
would indicate how likely a stereotypical speaker might
be to agree with a particular sentence. While this would
be a fundamental departure from the core philosophy of
model theory, we feel that it may be a worthwhile en-
deavour, allowing us to preserve the immense benefits
of the set-theoretic apparatus in a cognitively plausible
fashion. Following this aim, we hope to expand the pre-
liminary framework presented here into a more expres-
sive vector-based interpretation of set theory, catering
for aspects not covered in this paper (e.g. cardinality,
non-intersective modification) and refining our notion
of a model, together with its relation to meaning.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999615833333333">
We thank Marco Baroni, Stephen Clark, Ann Copes-
take and Katrin Erk for their helpful comments on a
previous version of this paper, and the three anonymous
reviewers for their thorough feedback on this work.
Eva Maria Vecchi is supported by ERC Starting Grant
DisCoTex (306920).
</bodyText>
<sectionHeader confidence="0.995744" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.932556733333333">
Hiyan Alshawi and Richard Crouch. 1992. Monotonic
semantic interpretation. In Proceedings of the 30th
annual meeting on Association for Computational
Linguistics, pages 32–39. Association for Computa-
tional Linguistics.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463–498.
Fahiem Bacchus. 1989. A modest, but semantically
well founded, inheritance reasoner. In Proceedings
of the 11th International Joint Conference on Artifi-
cial Intelligence, pages 1104–1109, Detroit, MI.
Timothy Baldwin, Emily M Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
</reference>
<page confidence="0.990047">
30
</page>
<reference confidence="0.999310072072072">
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC2004), Lisbon, Portugal.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of the fifteenth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL2012), pages 23–32.
Marco Baroni, Raffaela Bernardi, and Roberto Zam-
parelli. 2014a. Frege in space: A program of com-
positional distributional semantics. Linguistic Issues
in Language Technology, 9.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014b. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL2014), pages
238–247, Baltimore, Maryland.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM2013), pages 11–21, Atlanta, Georgia, USA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (ACL2013), Sofia, Bulgaria.
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of the 2008 Conference
on Semantics in Text Processing (STEP2008), pages
277–286.
Ronnie Cann. 1993. Formal semantics. Cambridge
University Press.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics – second edi-
tion. Wiley-Blackwell.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41–71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributional model of meaning. Lin-
guistic Analysis: A Festschrift for Joachim Lambek,
36(1–4):345–384.
Robin Cooper, Dick Crouch, JV Eijckl, Chris Fox,
JV Genabith, J Japars, Hans Kamp, David Milward,
Manfred Pinkal, Massimo Poesio, et al. 1996. A
framework for computational semantics (FraCaS).
Technical report, The FraCaS Consortium.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15:459–476.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2010), pages 1162–
1172.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, Sofia, Bulgaria.
Katrin Erk and Sebastian Pad´o. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2008), pages 897–906, Honolulu, HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6:635–653.
Katrin Erk. 2013. Towards a semantics for distribu-
tional representations. In Proceedings of the Tenth
International Conference on Computational Seman-
tics (IWCS2013), Potsdam, Germany.
Katrin Erk. 2015. What do you know about an alli-
gator when you know the company it keeps? Un-
published draft. https://utexas.box.com/
s/ekznoh08afi1kpkbf0hb.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Tomas Mikolov, et al. 2013.
Devise: A deep visual-semantic embedding model.
In Advances in Neural Information Processing Sys-
tems, pages 2121–2129.
Luana F˘ag˘ar˘as¸an, Eva Maria Vecchi, and Stephen
Clark. 2015. From distributional semantics to fea-
ture norms: Grounding semantic models in human
perceptual data. In Proceedings of the 11th Inter-
national Conference on Computational Semantics
(IWCS 2015), London, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney.
2013. A formal approach to linking logical form
and vector-space lexical semantics. In Harry Bunt,
Johan Bos, and Stephen Pulman, editors, Computing
Meaning, volume 4. Springer.
Sheila Glasbey. 2006. Bare plurals in object posi-
tion: which verbs fail to give existential readings,
and why? In Liliane Tasmowski and Svetlana Vo-
geleer, editors, Non-definiteness and Plurality, pages
133–157. Amsterdam: Benjamins.
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM2013), Atlanta, GA.
</reference>
<page confidence="0.997658">
31
</page>
<reference confidence="0.999486513513513">
Abhijeet Gupta, Gemma Boleda, Marco Baroni, and
Sebastian Pad´o. 2015. Distributional vectors encode
referential attributes. In Proceedingsof the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP2015), Lisboa, Portugal.
Aur´elie Herbelot and Eva Maria Vecchi. 2015. From
concepts to models: some issues in quantifying fea-
ture norms. Linguistic Issues in Language Technol-
ogy. To appear.
Aur´elie Herbelot. 2013. What is in a text, what isn’t,
and what this has to do with lexical semantics. In
Proceedings of the Tenth International Conference
on Computational Semantics (IWCS2013), Potsdam,
Germany.
Karl Moritz Hermann, Edward Grefenstette, and Phil
Blunsom. 2013. “Not not bad” is not “bad”:
A distributional account of negation. In Pro-
ceedings of the 2013 Workshop on Continuous
Vector Space Models and their Compositionality
(ACL2013), Sofia, Bulgaria.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL2014), pages 1403–1414, Baltimore, Mary-
land.
Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING08), pages 521–528, Manch-
ester, UK.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547–559.
Bj¨orn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in R. Journal of Statistical Soft-
ware, 18(2). Published online: http://www.
jstatsoft.org/v18/i02/.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Brian Riordan and Michael N Jones. 2011. Redun-
dancy in perceptual and linguistic experience: Com-
paring feature-based and distributional models of se-
mantic representation. Topics in Cognitive Science,
3(2):303–345.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
J.F.A.K. Van Benthem. 1986. Essays in logical seman-
tics. Number 29. Reidel.
Carl M Vogel. 1995. Inheritance reasoning: Psy-
chological plausibility, proof theory and semantics.
Ph.D. thesis, University of Edinburgh. College of
Science and Engineering. School of Informatics.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in AI, pages 658–666.
</reference>
<page confidence="0.999298">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520677">
<title confidence="0.999142">Building a shared world: Mapping distributional to model-theoretic semantic spaces</title>
<author confidence="0.76513">Aur´elie</author>
<affiliation confidence="0.8999655">Universit¨at Institut f¨ur Maschinelle</affiliation>
<address confidence="0.906157">Stuttgart,</address>
<email confidence="0.977663">aurelie.herbelot@cantab.net</email>
<author confidence="0.911107">Eva Maria</author>
<affiliation confidence="0.9887365">University of Computer</affiliation>
<address confidence="0.952838">Cambridge,</address>
<email confidence="0.999802">eva.vecchi@cl.cam.ac.uk</email>
<abstract confidence="0.999435789473684">In this paper, we introduce an approach to automatically map a standard distributional semantic space onto a set-theoretic model. We predict that there is a functional relationship between distributional information and vectorial concept representations in which dimensions are predicates and weights are generalised quantifiers. In order to test our prediction, we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quantifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Richard Crouch</author>
</authors>
<title>Monotonic semantic interpretation.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>32--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8735" citStr="Alshawi and Crouch, 1992" startWordPosition="1336" endWordPosition="1339"> an averaged representation of how a group of speakers use their language. 2.2 Generalised quantifiers Computational semantics has traditionally focused on very specific aspects of quantification. There is a large literature on the computational formalisation of quantifiers as automata, starting with Van Benthem (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was poss</context>
</contexts>
<marker>Alshawi, Crouch, 1992</marker>
<rawString>Hiyan Alshawi and Richard Crouch. 1992. Monotonic semantic interpretation. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 32–39. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="11278" citStr="Andrews et al. (2009)" startWordPosition="1724" endWordPosition="1727">ting comparably accurate estimates for features such as the GDP or CO2 emissions of a country. We complement such research by providing a more formal interpretation of the mapping between language and world knowledge. In particular, we offer a) a vectorial representation of set-theoretic models; b) a mechanism for predicting the application of generalised quantifiers to the sets in a model. 2.3 Mapping between Semantic Spaces The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapp</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fahiem Bacchus</author>
</authors>
<title>A modest, but semantically well founded, inheritance reasoner.</title>
<date>1989</date>
<booktitle>In Proceedings of the 11th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1104--1109</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="9033" citStr="Bacchus, 1989" startWordPosition="1381" endWordPosition="1382">them (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for</context>
</contexts>
<marker>Bacchus, 1989</marker>
<rawString>Fahiem Bacchus. 1989. A modest, but semantically well founded, inheritance reasoner. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pages 1104–1109, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Ara Kim</author>
<author>Stephan Oepen</author>
</authors>
<title>Road-testing the English Resource Grammar over the British National Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC2004),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="9231" citStr="Baldwin et al., 2004" startWordPosition="1407" endWordPosition="1410">all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in the form of a database. To the best of our knowledge, no existing system is able to universally predict the generalised quantification of noun phrases, including those introduced by the </context>
</contexts>
<marker>Baldwin, Bender, Flickinger, Kim, Oepen, 2004</marker>
<rawString>Timothy Baldwin, Emily M Bender, Dan Flickinger, Ara Kim, and Stephan Oepen. 2004. Road-testing the English Resource Grammar over the British National Corpus. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC2004), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL2012),</booktitle>
<pages>23--32</pages>
<contexts>
<context position="4690" citStr="Baroni et al., 2012" startWordPosition="712" endWordPosition="715">thods in Natural Language Processing, pages 22–32, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Understanding how this process works would not only give us an insight into a complex cognitive process, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual Entailment challenge, RTE: Dagan et al. (2009)). Indeed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches (MacCartney and Manning, 2008) to distributional semantics solutions (Baroni et al., 2012), they rely on an explicit representation of quantification. That is, they can model the entailment All koupreys are mammals |= This kouprey is a mammal, but not Koupreys are mammals |= This kouprey is a mammal. In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vectors – giving us </context>
<context position="8907" citStr="Baroni et al., 2012" startWordPosition="1361" endWordPosition="1364">s of quantification. There is a large literature on the computational formalisation of quantifiers as automata, starting with Van Benthem (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much unders</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the fifteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL2012), pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaela Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program of compositional distributional semantics.</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<contexts>
<context position="1812" citStr="Baroni et al., 2014" startWordPosition="248" endWordPosition="251">formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → l</context>
<context position="6509" citStr="Baroni et al., 2014" startWordPosition="999" endWordPosition="1002">man annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is </context>
<context position="16210" citStr="Baroni et al. (2014" startWordPosition="2519" endWordPosition="2522">dump of the English Wikipedia and the BNC4, which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bagof-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimensions to 300 through Singular Value Decomposition.5 Next we consider the context-predicting vectors (DSMikolov) available as part of the word2vec6 project (Mikolov et al., 2013a). We use the publicly available vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this option as a valid distributional representation of a word’s semantics. 4.2 The model-theoretic space Our ‘model-theoretic space’ differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relies on two components: an ontology and a denotation function (Cann, 1993). The ontology describes a world (which can be a simple situati</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaela Bernardi, and Roberto Zamparelli. 2014a. Frege in space: A program of compositional distributional semantics. Linguistic Issues in Language Technology, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014),</booktitle>
<pages>238--247</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="1812" citStr="Baroni et al., 2014" startWordPosition="248" endWordPosition="251">formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → l</context>
<context position="6509" citStr="Baroni et al., 2014" startWordPosition="999" endWordPosition="1002">man annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is </context>
<context position="16210" citStr="Baroni et al. (2014" startWordPosition="2519" endWordPosition="2522">dump of the English Wikipedia and the BNC4, which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bagof-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimensions to 300 through Singular Value Decomposition.5 Next we consider the context-predicting vectors (DSMikolov) available as part of the word2vec6 project (Mikolov et al., 2013a). We use the publicly available vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this option as a valid distributional representation of a word’s semantics. 4.2 The model-theoretic space Our ‘model-theoretic space’ differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relies on two components: an ontology and a denotation function (Cann, 1993). The ontology describes a world (which can be a simple situati</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014b. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014), pages 238–247, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets Markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM2013),</booktitle>
<pages>11--21</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1858" citStr="Beltagy et al., 2013" startWordPosition="256" endWordPosition="259">ent. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → like&apos;(Kim,x)] with x* indicating a plurality an</context>
<context position="6734" citStr="Beltagy et al., 2013" startWordPosition="1032" endWordPosition="1035">ional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to that proposed by Erk (2015). Erk suggests that distributional data influences semantic ‘knowledge’2: specifically, while a speaker may not know the extension of the word alligator, they maintain an information</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets Markov: Deep semantics with probabilistic logical form. In Second Joint Conference on Lexical and Computational Semantics (*SEM2013), pages 11–21, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raffaella Bernardi</author>
<author>Georgiana Dinu</author>
<author>Marco Marelli</author>
<author>Marco Baroni</author>
</authors>
<title>A relatedness benchmark to test the role of determiners in compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1771" citStr="Bernardi et al., 2013" startWordPosition="242" endWordPosition="245"> the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x</context>
<context position="6446" citStr="Bernardi et al., 2013" startWordPosition="988" endWordPosition="991">and §5 describe our experiments, reporting correlation against human annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt forma</context>
</contexts>
<marker>Bernardi, Dinu, Marelli, Baroni, 2013</marker>
<rawString>Raffaella Bernardi, Georgiana Dinu, Marco Marelli, and Marco Baroni. 2013. A relatedness benchmark to test the role of determiners in compositional distributional semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Semantics in Text Processing (STEP2008),</booktitle>
<pages>277--286</pages>
<contexts>
<context position="9243" citStr="Bos, 2008" startWordPosition="1411" endWordPosition="1412">ation about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in the form of a database. To the best of our knowledge, no existing system is able to universally predict the generalised quantification of noun phrases, including those introduced by the (in)definite</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In Proceedings of the 2008 Conference on Semantics in Text Processing (STEP2008), pages 277–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie Cann</author>
</authors>
<title>Formal semantics.</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16747" citStr="Cann, 1993" startWordPosition="2601" endWordPosition="2602"> on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this option as a valid distributional representation of a word’s semantics. 4.2 The model-theoretic space Our ‘model-theoretic space’ differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relies on two components: an ontology and a denotation function (Cann, 1993). The ontology describes a world (which can be a simple situation or ‘state of affairs’), with everything that is contained in that world. Ontologies can be represented in various ways, but in this paper, we assume they are formalised in terms of sets of entities. The denotation function associates words with their extensions in the model, i.e. the sets they refer to. Thanks to the availability of the ontology, it is possible to define a truth function for sentences, which computes whether a particular statement corresponds to the model or not. In our account, we do not have an a priori model </context>
</contexts>
<marker>Cann, 1993</marker>
<rawString>Ronnie Cann. 1993. Formal semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning.</title>
<date>2012</date>
<booktitle>Handbook of Contemporary Semantics –</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="1311" citStr="Clark, 2012" startWordPosition="176" endWordPosition="177">del of such relationship over a publicly available dataset of feature norms annotated with natural language quantifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors. 1 Introduction In recent years, the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Fo</context>
</contexts>
<marker>Clark, 2012</marker>
<rawString>Stephen Clark. 2012. Vector space models of lexical meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics – second edition. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="6525" citStr="Clarke, 2012" startWordPosition="1003" endWordPosition="1004">scuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to </context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis: A Festschrift for Joachim Lambek,</title>
<date>2011</date>
<pages>36--1</pages>
<contexts>
<context position="1748" citStr="Coecke et al., 2011" startWordPosition="238" endWordPosition="241">tion In recent years, the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic</context>
<context position="6423" citStr="Coecke et al., 2011" startWordPosition="984" endWordPosition="987">ribe our dataset. §4 and §5 describe our experiments, reporting correlation against human annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quanti</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2011</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2011. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis: A Festschrift for Joachim Lambek, 36(1–4):345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Dick Crouch</author>
<author>JV Eijckl</author>
<author>Chris Fox</author>
<author>JV Genabith</author>
<author>J Japars</author>
<author>Hans Kamp</author>
<author>David Milward</author>
<author>Manfred Pinkal</author>
<author>Massimo Poesio</author>
</authors>
<title>A framework for computational semantics (FraCaS).</title>
<date>1996</date>
<tech>Technical report, The FraCaS Consortium.</tech>
<contexts>
<context position="8709" citStr="Cooper et al., 1996" startWordPosition="1332" endWordPosition="1335">l representations are an averaged representation of how a group of speakers use their language. 2.2 Generalised quantifiers Computational semantics has traditionally focused on very specific aspects of quantification. There is a large literature on the computational formalisation of quantifiers as automata, starting with Van Benthem (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work </context>
</contexts>
<marker>Cooper, Crouch, Eijckl, Fox, Genabith, Japars, Kamp, Milward, Pinkal, Poesio, 1996</marker>
<rawString>Robin Cooper, Dick Crouch, JV Eijckl, Chris Fox, JV Genabith, J Japars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, et al. 1996. A framework for computational semantics (FraCaS). Technical report, The FraCaS Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--459</pages>
<contexts>
<context position="4463" citStr="Dagan et al. (2009)" startWordPosition="682" endWordPosition="685"> cats are on the sofa (2 / a few cats?), I picked pears today (a few / a few dozen?) and The protesters were blocking the entire avenue (hundreds/thousands of protesters?). 22 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 22–32, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Understanding how this process works would not only give us an insight into a complex cognitive process, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual Entailment challenge, RTE: Dagan et al. (2009)). Indeed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches (MacCartney and Manning, 2008) to distributional semantics solutions (Baroni et al., 2012), they rely on an explicit representation of quantification. That is, they can model the entailment All koupreys are mammals |= This kouprey is a mammal, but not Koupreys are mammals |= This kouprey is a mammal. In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between lang</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15:459–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2010),</booktitle>
<pages>1162--1172</pages>
<contexts>
<context position="42530" citStr="Dinu and Lapata (2010)" startWordPosition="6984" endWordPosition="6987">his paper was concept-predicate pairs out of context. That is, we considered quantified sentences where the restrictor was the entire set denoted by a lexical item. A natural next step is to investigate the quantification of statements involving contextualised subsets. For instance, we should obtain a different quantifier for taxis are yellow depending on whether the sentence starts with In London... or In New York... In future work, we will test our system on such context-specific examples, using contextualised vector representations such as the ones proposed by e.g. Erk and Pad´o (2008) and Dinu and Lapata (2010). We conclude by noting again that the set-theoretic models produced in this work differ from formal semantics models in important ways. They do not represent the world per se, but rather some shared beliefs about the world, induced from an annotated dataset of feature norms. This calls for a modified version of the standard denotation function and for the replacement of the truth function with a ‘plausibility’ function, which would indicate how likely a stereotypical speaker might be to agree with a particular sentence. While this would be a fundamental departure from the core philosophy of m</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2010), pages 1162– 1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT: DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the System Demonstrations of ACL 2013,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="17807" citStr="Dinu et al., 2013" startWordPosition="2774" endWordPosition="2777">truth function for sentences, which computes whether a particular statement corresponds to the model or not. In our account, we do not have an a priori model of the world: we wish to infer it from our observation of language data. We believe this to be an advantage over traditional formal semantics, which requires full ontological data to be available in order to account for reference and truth conditions, but never spells out how this 4http://wacky.sslmit.unibo.it, http: //www.natcorp.ox.ac.uk 5All semantic spaces, both distributional and modeltheoretic, were built using the DISSECT toolkit (Dinu et al., 2013). 6https://code.google.com/p/word2vec data comes into being. This however implies that our produced ontology will necessarily be partial: we can only model what can be inferred from language use. This has consequences for the denotation function. Let’s imagine a world with three cats and two horses. In model theory, the word horse has an extension in that world which is the set of horses, with a cardinality of two. This can be trivially derived because the world is fully described in the ontology. In our approach, however, it is unlikely we might be able to learn the cardinality of any set in </context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT: DIStributional SEmantics Composition Toolkit. In Proceedings of the System Demonstrations of ACL 2013, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008),</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008), pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--635</pages>
<contexts>
<context position="1323" citStr="Erk, 2012" startWordPosition="178" endWordPosition="179">elationship over a publicly available dataset of feature norms annotated with natural language quantifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors. 1 Introduction In recent years, the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distrib</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass, 6:635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Towards a semantics for distributional representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013),</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="6689" citStr="Erk, 2013" startWordPosition="1026" endWordPosition="1027">ics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to that proposed by Erk (2015). Erk suggests that distributional data influences semantic ‘knowledge’2: specifically, while a speaker may not know the extension of the</context>
</contexts>
<marker>Erk, 2013</marker>
<rawString>Katrin Erk. 2013. Towards a semantics for distributional representations. In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013), Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>What do you know about an alligator when you know the company it keeps? Unpublished draft.</title>
<date>2015</date>
<note>https://utexas.box.com/ s/ekznoh08afi1kpkbf0hb.</note>
<contexts>
<context position="7152" citStr="Erk (2015)" startWordPosition="1093" endWordPosition="1094">pproaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to that proposed by Erk (2015). Erk suggests that distributional data influences semantic ‘knowledge’2: specifically, while a speaker may not know the extension of the word alligator, they maintain an information state which models properties of alligators (for instance, that they are animals). This information state is described in terms of probabilistic logic, which accounts for an agent’s uncertainty about what the world is like. The probability of a sentence is the summed probability of the possible worlds that make it true. Similarly, we assume a systematic relation between distributional information and world knowled</context>
</contexts>
<marker>Erk, 2015</marker>
<rawString>Katrin Erk. 2015. What do you know about an alligator when you know the company it keeps? Unpublished draft. https://utexas.box.com/ s/ekznoh08afi1kpkbf0hb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg S Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeff Dean</author>
<author>Tomas Mikolov</author>
</authors>
<title>Devise: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2121--2129</pages>
<contexts>
<context position="11645" citStr="Frome et al. (2013)" startWordPosition="1786" endWordPosition="1789"> to the sets in a model. 2.3 Mapping between Semantic Spaces The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these previous studies: a simple linear function can map between semantic spaces, in this case between a linguistic (distributional) semantic space and a model-theoretic space. 3 Annotated datasets 3.1 The quantified McRae norms The McRae norms (McRae et al., 2005) a</context>
<context position="22584" citStr="Frome et al., 2013" startWordPosition="3589" endWordPosition="3592">TQMR) contains 541 concepts, as described in §3.1. The second space is constructed specifically for the additional animal data from §3.2 (MTAD). Finally, we merge the two into a single space of 555 unique concepts (MTQMR+AD). 5 Experiments 5.1 Experimental setup To map from one semantic representation to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent. Following previous research showing that similarities amongst word representations can be maintained within linear transformations (Mikolov et al., 2013b; Frome et al., 2013), we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation. We estimate the coefficients of the function using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007). We learn a function from the distributional space to each of the model-theoretic spaces (c.f. §4). The distribution of training and test items is outlined in Table 2, expressed as a number of concept vectors. We also include the number of quantified instances in the test set (i.e. the numbe</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luana F˘ag˘ar˘as¸an</author>
<author>Eva Maria Vecchi</author>
<author>Stephen Clark</author>
</authors>
<title>From distributional semantics to feature norms: Grounding semantic models in human perceptual data.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015),</booktitle>
<location>London, UK.</location>
<marker>F˘ag˘ar˘as¸an, Vecchi, Clark, 2015</marker>
<rawString>Luana F˘ag˘ar˘as¸an, Eva Maria Vecchi, and Stephen Clark. 2015. From distributional semantics to feature norms: Grounding semantic models in human perceptual data. In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015), London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>A formal approach to linking logical form and vector-space lexical semantics.</title>
<date>2013</date>
<journal>Computing Meaning,</journal>
<volume>4</volume>
<editor>In Harry Bunt, Johan Bos, and Stephen Pulman, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1836" citStr="Garrette et al., 2013" startWordPosition="252" endWordPosition="255">ecome increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → like&apos;(Kim,x)] with x* ind</context>
<context position="6712" citStr="Garrette et al., 2013" startWordPosition="1028" endWordPosition="1031">ation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to that proposed by Erk (2015). Erk suggests that distributional data influences semantic ‘knowledge’2: specifically, while a speaker may not know the extension of the word alligator, they m</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2013</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. A formal approach to linking logical form and vector-space lexical semantics. In Harry Bunt, Johan Bos, and Stephen Pulman, editors, Computing Meaning, volume 4. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheila Glasbey</author>
</authors>
<title>Bare plurals in object position: which verbs fail to give existential readings, and why?</title>
<date>2006</date>
<booktitle>In Liliane Tasmowski and Svetlana Vogeleer, editors, Non-definiteness and Plurality,</booktitle>
<pages>133--157</pages>
<location>Amsterdam: Benjamins.</location>
<contexts>
<context position="2679" citStr="Glasbey (2006)" startWordPosition="391" endWordPosition="392"> of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → like&apos;(Kim,x)] with x* indicating a plurality and GEN the generic quantifier. It is generally accepted that the appropriate choice of quantifier for an ambiguous bare plural object depends, amongst other things, on the lexical semantics of the verb (e.g. Glasbey (2006)). This type of interaction implies the existence of systematic influences of the lexicon over logic, which could in principle be formalised. A model of the lexicon/logic interface would be desirable to explain how speakers resolve standard cases of ambiguity like the bare plural in 1 and 2, but more generally, it could be the basis for answering a more fundamental question: how do speakers construct a model of a sentence for which they have no prior perceptual data? People can make complex inferences about statements without having access to their real-world reference. As an example, consider</context>
</contexts>
<marker>Glasbey, 2006</marker>
<rawString>Sheila Glasbey. 2006. Bare plurals in object position: which verbs fail to give existential readings, and why? In Liliane Tasmowski and Svetlana Vogeleer, editors, Non-definiteness and Plurality, pages 133–157. Amsterdam: Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Towards a formal distributional semantics: Simulating logical calculi with tensors.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM2013),</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="1553" citStr="Grefenstette, 2013" startWordPosition="212" endWordPosition="213">ms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors. 1 Introduction In recent years, the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us conside</context>
<context position="6466" citStr="Grefenstette, 2013" startWordPosition="992" endWordPosition="994">eriments, reporting correlation against human annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as </context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Towards a formal distributional semantics: Simulating logical calculi with tensors. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM2013), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhijeet Gupta</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Distributional vectors encode referential attributes.</title>
<date>2015</date>
<booktitle>In Proceedingsof the Conference on Empirical Methods in Natural Language Processing (EMNLP2015),</booktitle>
<location>Lisboa, Portugal.</location>
<marker>Gupta, Boleda, Baroni, Pad´o, 2015</marker>
<rawString>Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pad´o. 2015. Distributional vectors encode referential attributes. In Proceedingsof the Conference on Empirical Methods in Natural Language Processing (EMNLP2015), Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie Herbelot</author>
<author>Eva Maria Vecchi</author>
</authors>
<title>From concepts to models: some issues in quantifying feature norms. Linguistic Issues in Language Technology.</title>
<date>2015</date>
<note>To appear.</note>
<contexts>
<context position="5440" citStr="Herbelot and Vecchi, 2015" startWordPosition="833" endWordPosition="837"> |= This kouprey is a mammal, but not Koupreys are mammals |= This kouprey is a mammal. In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vectors – giving us a common representation across formalisms. Using a publicly available dataset of feature norms annotated with quantifiers1 (Herbelot and Vecchi, 2015), we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data. This paper is structured as follows. §2 reviews related work, focusing in turn on approaches to formal distributional semantics, computational work on quantification, and mapping between semantic spaces. In §3, we describe our dataset. §4 and §5 describe our experiments, reporting correlation against human annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Relat</context>
<context position="12712" citStr="Herbelot and Vecchi (2015)" startWordPosition="1948" endWordPosition="1951">guistic (distributional) semantic space and a model-theoretic space. 3 Annotated datasets 3.1 The quantified McRae norms The McRae norms (McRae et al., 2005) are a set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion). The annotators were given concepts and asked to provide features for them, covering physical, functional and other properties. The result is a set of 7257 concept-feature pairs such as airplane used-for-passengers or bear is-brown. In our work, we use the annotation layer produced by Herbelot and Vecchi (2015) for the McRae norms (henceforth QMR): for each concept-feature pair (C, f), the annotation provides a natural language quantifier expressing the ratio of instances of C having the feature f, as elicited by three coders. The quantifiers in use are NO, FEW, SOME, MOST, ALL. Table 1 provides example annotations for concept-feature pairs (reproduced from the original paper). An additional label, KIND, was introduced for usages of the concept as a kind, where quantification does not apply (e.g. beaver symbol-of-Canada). A subset of the annotation layer is available for training computational model</context>
<context position="21494" citStr="Herbelot and Vecchi (2015)" startWordPosition="3415" endWordPosition="3418">s), and that the set of horses and the set of things that are scaly are disjoint (no horse is scaly). We also learn that a great majority of horses have four legs and that some are brown. In the following, we experiment with 3 modeltheoretic spaces built from the McRae and AD datasets described in §3. As both datasets are annotated with natural language quantifiers rather than cardinality ratios, we convert the annotation into a numerical format, where ALL → 1, MOST → 0.95, SOME → 0.35, FEW → 0.05, and NO → 0. These values correspond to the weights giving the best inter-annotator agreement in Herbelot and Vecchi (2015), when calculating weighted Cohen’s kappa on QMR. In each model-theoretic space, a concept is represented as a vector in which the dimensions are features (has buttons, is green), and the values of the vectors along each dimension are quantifiers (in numerical format). When a feature does not occur with a concept in QMR, the concept’s vector receives a weight of 0 on the corresponding dimension.8 We define 3 spaces as follows. The McRae-based model-theoretic space (MTQMR) contains 541 concepts, as described in §3.1. The second space is constructed specifically for the additional animal data fr</context>
<context position="37682" citStr="Herbelot and Vecchi (2015)" startWordPosition="6175" endWordPosition="6178">some most all Mapped no 0 -0.05 -0.35 -0.95 -1 few -0.05 0 0.2 0.9 0.95 some -0.35 -0.2 0 0.6 0.65 most -0.95 -0.9 -0.6 0 0.05 all -1 -0.95 -0.65 -0.05 0 Table 7: Distance matrix for the evaluation of the natural language quantifiers generation step. Gold no few some most all Mapped no 238 66 20 4 2 few 53 45 30 19 12 some 6 1 2 3 2 most 4 6 4 16 56 all 0 0 0 2 3 Table 8: Confusion matrix for the results of the natural language quantifiers generation. We set a distance matrix, which we will use for penalising errors. This matrix, shown in Table 7, is basically equivalent to the matrix used by Herbelot and Vecchi (2015) to calculate weighted kappa between annotators, with the difference that all errors involving NO cause incorrect inferences and receive special treatment. Cases where the gold quantifier entails the mapped quantifier (all cats |= some cats) have positive distances, while cases where the entailment doesn’t hold have negative distances. Using the distance matrix, we give a score to each instance in our test data as follows: � _ 1 − d if d ≥ 0 s (1) d if d &lt; 0 where d is obtained from the distance matrix. This has the effect that when the mapped quantifier equals the gold quantifier, the system </context>
</contexts>
<marker>Herbelot, Vecchi, 2015</marker>
<rawString>Aur´elie Herbelot and Eva Maria Vecchi. 2015. From concepts to models: some issues in quantifying feature norms. Linguistic Issues in Language Technology. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie Herbelot</author>
</authors>
<title>What is in a text, what isn’t, and what this has to do with lexical semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013),</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="9924" citStr="Herbelot (2013)" startWordPosition="1516" endWordPosition="1517">has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in the form of a database. To the best of our knowledge, no existing system is able to universally predict the generalised quantification of noun phrases, including those introduced by the (in)definite singulars a/the and definite plurals the. The closest attempt is Herbelot (2013), who suggests that 2We use the term knowledge loosely, to refer to a speaker’s beliefs about the world or a state of affairs. 23 Concept ALL MOST SOME FEW ALL tricycle FEW NO Table 1: Example annotations for concepts. ‘model-theoretic vectors’ can be built out of distributional vectors supplemented with manually annotated training data. The proposed implementation, however, fails to validate the theory. Our work follows the intuition that distributions can be translated into set-theoretic equivalents. But it implements the mapping as a systematic linear transformation. Our approach is similar</context>
<context position="14186" citStr="Herbelot (2013)" startWordPosition="2196" endWordPosition="2197">classes in QMR (removing the KIND items), with the annotation set to majority opinion (6156 instances). The natural language quantifiers are converted to a numerical format (see §4 for details). Using the numerical data, we can calculate the mean Spearman rank correlation between the three annotators, which comes to 0.63. 3.2 Additional animal data QMR gives us an average of 11 features per concept. This results in fairly sparse vectors in the modeltheoretic semantic space (see §4). In order to remedy data sparsity, we consider the use of additional data in the form of the animal dataset from Herbelot (2013) (henceforth AD). AD3 is a set of 72 animal concepts with quantification annotations along 54 features. The main differences between QMR and AD are as follows: • Nature of features: the features in AD are not human elicited norms, but linguistic predicates obtained from a corpus analysis. • Comprehensiveness of annotation: the 72 concepts were annotated along all 54 features. This ensures the availability of a large number of negatively quantified pairs (e.g. cat is-fish). We manually align the AD concepts and features to the QMR format, changing e.g. bat to bat (animal). The QMR and AD sets h</context>
<context position="20169" citStr="Herbelot (2013)" startWordPosition="3175" endWordPosition="3177">tributions to generalised quantifiers. Specifically, F( wk) = {Q1(w0 k,p0 1),Q2(w0 k,p0 2)...Qn(w0k,p0 n)}, where wk is the distribution of wk and Q1...Qn E {no, few, some, most, all} . That is, F takes a distribution wk and returns a quantifier for each predicate in the model, corresponding to the set overlap between w0k and p01...n. Note that we focus here on 5 quantifiers only, but as mentioned above, we do not preclude the possibility of learning others (including cardinals in appropriate cases). F( wk) lives in a model-theoretic space which broadly follows the representation suggested by Herbelot (2013). We assume a space with n dimensions d1...dn which correspond to predicates p01...n (e.g. is fluffy, used for transportation). In that space, F( wk) is weighted along the dimension dm in proportion to the set overlap w0k ∩p0 m.7 The following shows a toy vector with only four dimensions for the concept horse. a mammal 1 has four legs 0.95 is brown 0.35 is scaly 0 7In Herbelot (2013), weights are taken to be probabilities, but we prefer to talk of quantifiers, as the notion models our data more directly. 25 This vector tells us that the set of horses includes the set of mammals (the number of </context>
</contexts>
<marker>Herbelot, 2013</marker>
<rawString>Aur´elie Herbelot. 2013. What is in a text, what isn’t, and what this has to do with lexical semantics. In Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013), Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>Not not bad” is not “bad”: A distributional account of negation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality (ACL2013),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="6488" citStr="Hermann et al., 2013" startWordPosition="995" endWordPosition="998">correlation against human annotations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors (§7). 2 Related Work 2.1 Formal Distributional Semantics The relation between distributional and formal semantics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or nega</context>
</contexts>
<marker>Hermann, Grefenstette, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann, Edward Grefenstette, and Phil Blunsom. 2013. “Not not bad” is not “bad”: A distributional account of negation. In Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality (ACL2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014),</booktitle>
<pages>1403--1414</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="11823" citStr="Lazaridou et al. (2014)" startWordPosition="1811" endWordPosition="1814">nitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these previous studies: a simple linear function can map between semantic spaces, in this case between a linguistic (distributional) semantic space and a model-theoretic space. 3 Annotated datasets 3.1 The quantified McRae norms The McRae norms (McRae et al., 2005) are a set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion). The annotators were gi</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014), pages 1403–1414, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<date>2013</date>
<booktitle>Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics,</booktitle>
<pages>1--179</pages>
<contexts>
<context position="1885" citStr="Lewis and Steedman, 2013" startWordPosition="260" endWordPosition="263">nal semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use the term ‘Formal Distributional Semantics’ (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central question: the formalisation of the systematic dependencies between lexical and set-theoretic levels. Let us consider the following examples. 1. Kim writes books. 2. Kim likes books. The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading: • ∃x*[book&apos;(x*) ∧ write&apos;(Kim, x*)] • GENx[book&apos;(x) → like&apos;(Kim,x)] with x* indicating a plurality and GEN the generic quantifie</context>
<context position="6761" citStr="Lewis and Steedman, 2013" startWordPosition="1036" endWordPosition="1039">tics has been the object of a number of studies in recent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Hermann et al., 2013; Baroni et al., 2014a; Clarke, 2012); b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations (Erk, 2013; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). This paper follows the hybrid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by 1Data available at http://www.cl.cam.ac.uk/ ˜ah433/mcrae-quantified-majority.txt formal semantics such as quantification or negation. Our account is also similar to that proposed by Erk (2015). Erk suggests that distributional data influences semantic ‘knowledge’2: specifically, while a speaker may not know the extension of the word alligator, they maintain an information state which models propert</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08),</booktitle>
<pages>521--528</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="4630" citStr="MacCartney and Manning, 2008" startWordPosition="704" endWordPosition="707">f protesters?). 22 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 22–32, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Understanding how this process works would not only give us an insight into a complex cognitive process, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual Entailment challenge, RTE: Dagan et al. (2009)). Indeed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches (MacCartney and Manning, 2008) to distributional semantics solutions (Baroni et al., 2012), they rely on an explicit representation of quantification. That is, they can model the entailment All koupreys are mammals |= This kouprey is a mammal, but not Koupreys are mammals |= This kouprey is a mammal. In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like dist</context>
<context position="8766" citStr="MacCartney and Manning, 2008" startWordPosition="1340" endWordPosition="1343">n of how a group of speakers use their language. 2.2 Generalised quantifiers Computational semantics has traditionally focused on very specific aspects of quantification. There is a large literature on the computational formalisation of quantifiers as automata, starting with Van Benthem (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give comp</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>Bill MacCartney and Christopher D Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08), pages 521–528, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things. Behavior research methods,</title>
<date>2005</date>
<pages>37--4</pages>
<contexts>
<context position="12243" citStr="McRae et al., 2005" startWordPosition="1875" endWordPosition="1878">. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these previous studies: a simple linear function can map between semantic spaces, in this case between a linguistic (distributional) semantic space and a model-theoretic space. 3 Annotated datasets 3.1 The quantified McRae norms The McRae norms (McRae et al., 2005) are a set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion). The annotators were given concepts and asked to provide features for them, covering physical, functional and other properties. The result is a set of 7257 concept-feature pairs such as airplane used-for-passengers or bear is-brown. In our work, we use the annotation layer produced by Herbelot and Vecchi (2015) for the McRae norms (henceforth QMR): for each concept-feature pair (C, f), the annotation provides a natural language quantifier </context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S Cree, Mark S Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior research methods, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bj¨orn-Helge Mevik</author>
<author>Ron Wehrens</author>
</authors>
<title>The pls package: Principal component and partial least squares regression in R.</title>
<date>2007</date>
<journal>Journal of Statistical Software,</journal>
<volume>18</volume>
<issue>2</issue>
<note>Published online: http://www. jstatsoft.org/v18/i02/.</note>
<contexts>
<context position="22891" citStr="Mevik and Wehrens, 2007" startWordPosition="3635" endWordPosition="3638"> to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent. Following previous research showing that similarities amongst word representations can be maintained within linear transformations (Mikolov et al., 2013b; Frome et al., 2013), we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation. We estimate the coefficients of the function using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007). We learn a function from the distributional space to each of the model-theoretic spaces (c.f. §4). The distribution of training and test items is outlined in Table 2, expressed as a number of concept vectors. We also include the number of quantified instances in the test set (i.e. the number of actual concept-feature pairs that were explicitly annotated in QMR/AD and that 8No transformations or dimensionality reductions were performed on the MT spaces. Space # train # test # dims # test vec. vec. inst. MTQMR 400 141 2172 1570 MTAD 60 12 54 648 MTQMR+AD 410 145 2193 1595 Table 2: Distribution</context>
</contexts>
<marker>Mevik, Wehrens, 2007</marker>
<rawString>Bj¨orn-Helge Mevik and Ron Wehrens. 2007. The pls package: Principal component and partial least squares regression in R. Journal of Statistical Software, 18(2). Published online: http://www. jstatsoft.org/v18/i02/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="11445" citStr="Mikolov et al. (2013" startWordPosition="1751" endWordPosition="1754">the mapping between language and world knowledge. In particular, we offer a) a vectorial representation of set-theoretic models; b) a mechanism for predicting the application of generalised quantifiers to the sets in a model. 2.3 Mapping between Semantic Spaces The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these previous studies: a simple linear function can map between se</context>
<context position="16076" citStr="Mikolov et al., 2013" startWordPosition="2495" endWordPosition="2498">ence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC4, which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bagof-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimensions to 300 through Singular Value Decomposition.5 Next we consider the context-predicting vectors (DSMikolov) available as part of the word2vec6 project (Mikolov et al., 2013a). We use the publicly available vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this option as a valid distributional representation of a word’s semantics. 4.2 The model-theoretic space Our ‘model-theoretic space’ differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relie</context>
<context position="22562" citStr="Mikolov et al., 2013" startWordPosition="3585" endWordPosition="3588">odel-theoretic space (MTQMR) contains 541 concepts, as described in §3.1. The second space is constructed specifically for the additional animal data from §3.2 (MTAD). Finally, we merge the two into a single space of 555 unique concepts (MTQMR+AD). 5 Experiments 5.1 Experimental setup To map from one semantic representation to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent. Following previous research showing that similarities amongst word representations can be maintained within linear transformations (Mikolov et al., 2013b; Frome et al., 2013), we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation. We estimate the coefficients of the function using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007). We learn a function from the distributional space to each of the model-theoretic spaces (c.f. §4). The distribution of training and test items is outlined in Table 2, expressed as a number of concept vectors. We also include the number of quantified instances in the te</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="11445" citStr="Mikolov et al. (2013" startWordPosition="1751" endWordPosition="1754">the mapping between language and world knowledge. In particular, we offer a) a vectorial representation of set-theoretic models; b) a mechanism for predicting the application of generalised quantifiers to the sets in a model. 2.3 Mapping between Semantic Spaces The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these previous studies: a simple linear function can map between se</context>
<context position="16076" citStr="Mikolov et al., 2013" startWordPosition="2495" endWordPosition="2498">ence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC4, which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bagof-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimensions to 300 through Singular Value Decomposition.5 Next we consider the context-predicting vectors (DSMikolov) available as part of the word2vec6 project (Mikolov et al., 2013a). We use the publicly available vectors which were trained on a Google News dataset of circa 100 billion tokens. Baroni et al. (2014b) showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this option as a valid distributional representation of a word’s semantics. 4.2 The model-theoretic space Our ‘model-theoretic space’ differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relie</context>
<context position="22562" citStr="Mikolov et al., 2013" startWordPosition="3585" endWordPosition="3588">odel-theoretic space (MTQMR) contains 541 concepts, as described in §3.1. The second space is constructed specifically for the additional animal data from §3.2 (MTAD). Finally, we merge the two into a single space of 555 unique concepts (MTQMR+AD). 5 Experiments 5.1 Experimental setup To map from one semantic representation to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent. Following previous research showing that similarities amongst word representations can be maintained within linear transformations (Mikolov et al., 2013b; Frome et al., 2013), we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation. We estimate the coefficients of the function using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007). We learn a function from the distributional space to each of the model-theoretic spaces (c.f. §4). The distribution of training and test items is outlined in Table 2, expressed as a number of concept vectors. We also include the number of quantified instances in the te</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Riordan</author>
<author>Michael N Jones</author>
</authors>
<title>Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation.</title>
<date>2011</date>
<journal>Topics in Cognitive Science,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="11252" citStr="Riordan and Jones (2011)" startWordPosition="1719" endWordPosition="1722">m distributional vectors, getting comparably accurate estimates for features such as the GDP or CO2 emissions of a country. We complement such research by providing a more formal interpretation of the mapping between language and world knowledge. In particular, we offer a) a vectorial representation of set-theoretic models; b) a mechanism for predicting the application of generalised quantifiers to the sets in a model. 2.3 Mapping between Semantic Spaces The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by Riordan and Jones (2011) and Andrews et al. (2009) show that models that map between and integrate perceptual and linguistic information perform better at fitting human semantic intuition. In NLP, Mikolov et al. (2013b) show that a linear mapping between vector spaces of different languages can be learned to infer missing dictionary entries by relying on a small amount of bilingual information. Frome et al. (2013) learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and Lazaridou et al. (2014) explore mapping techniques t</context>
</contexts>
<marker>Riordan, Jones, 2011</marker>
<rawString>Brian Riordan and Michael N Jones. 2011. Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation. Topics in Cognitive Science, 3(2):303–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1298" citStr="Turney and Pantel, 2010" startWordPosition="171" endWordPosition="175">prediction, we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quantifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors. 1 Introduction In recent years, the complementarity of distributional and formal semantics has become increasingly evident. While distributional semantics (Turney and Pantel, 2010; Clark, 2012; Erk, 2012) has proved very successful in modelling lexical effects such as graded similarity and polysemy, it clearly has difficulties accounting for logical phenomena which are well covered by model-theoretic semantics (Grefenstette, 2013). A number of proposals have emerged from these considerations, suggesting that an overarching semantics integrating both distributional and formal aspects would be desirable (Coecke et al., 2011; Bernardi et al., 2013; Grefenstette, 2013; Baroni et al., 2014a; Garrette et al., 2013; Beltagy et al., 2013; Lewis and Steedman, 2013). We will use</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F A K Van Benthem</author>
</authors>
<title>Essays in logical semantics.</title>
<date>1986</date>
<journal>Number</journal>
<volume>29</volume>
<publisher>Reidel.</publisher>
<marker>Van Benthem, 1986</marker>
<rawString>J.F.A.K. Van Benthem. 1986. Essays in logical semantics. Number 29. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl M Vogel</author>
</authors>
<title>Inheritance reasoning: Psychological plausibility, proof theory and semantics.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh. College of Science and Engineering. School of Informatics.</institution>
<contexts>
<context position="9047" citStr="Vogel, 1995" startWordPosition="1383" endWordPosition="1384"> parallel to this work, much research has been done on drawing inferences from explicitly quantified statements – i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in th</context>
</contexts>
<marker>Vogel, 1995</marker>
<rawString>Carl M Vogel. 1995. Inheritance reasoning: Psychological plausibility, proof theory and semantics. Ph.D. thesis, University of Edinburgh. College of Science and Engineering. School of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in AI,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="9297" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1417" endWordPosition="1420">ubject-predicate pair (Cooper et al., 1996; Alshawi and Crouch, 1992; MacCartney and Manning, 2008). Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally (Baroni et al., 2012). A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements (Bacchus, 1989; Vogel, 1995). Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely supervised (Baldwin et al., 2004; Bos, 2008) to lightly supervised (Zettlemoyer and Collins, 2005). Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in the form of a database. To the best of our knowledge, no existing system is able to universally predict the generalised quantification of noun phrases, including those introduced by the (in)definite singulars a/the and definite plurals the. The closest</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in AI, pages 658–666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
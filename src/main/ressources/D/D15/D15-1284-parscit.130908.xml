<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000720">
<title confidence="0.97583">
Humor Recognition and Humor Anchor Extraction
</title>
<author confidence="0.993749">
Diyi Yang, Alon Lavie, Chris Dyer, Eduard Hovy
</author>
<affiliation confidence="0.9855885">
Language Technologies Institute, School of Computer Science
Carnegie Mellon University. Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.969937">
{diyiy, alavie, cdyer}@cs.cmu.edu, hovy@cmu.edu
</email>
<sectionHeader confidence="0.99718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794736842105">
Humor is an essential component in
personal communication. How to create
computational models to discover the
structures behind humor, recognize humor
and even extract humor anchors remains
a challenge. In this work, we first
identify several semantic structures behind
humor and design sets of features for
each structure, and next employ a com-
putational approach to recognize humor.
Furthermore, we develop a simple and
effective method to extract anchors that
enable humor in a sentence. Experiments
conducted on two datasets demonstrate
that our humor recognizer is effective
in automatically distinguishing between
humorous and non-humorous texts and our
extracted humor anchors correlate quite
well with human annotations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973372881356">
Humor is one of the most interesting and puzzling
research areas in the field of natural language
understanding. Recently, computers have changed
their roles from automatons that can only perform
assigned tasks to intelligent agents that dynami-
cally interact with people and learn to understand
their users. When a computer converses with a
human being, if it can figure out the humor in
human’s language, it can better understand the true
meaning of human language, and thereby make
better decisions that improve the user experience.
Developing techniques that enable computers to
understand humor in human conversations and
adapt behavior accordingly deserves particular
attention.
The task of Humor Recognition refers to
determining whether a sentence in a given context
expresses a certain degree of humor. Humor
recognition is a challenging natural language
problem (Attardo, 1994). First, a universal
definition of humor is hard to achieve, because
different people hold different understandings of
even the same sentence. Second, humor is always
situated in a broader context that sometimes
requires a lot of external knowledge to fully
understand it. For example, consider the sentence,
“The one who invented the door knocker got a
No Bell prize” and “Veni, Vidi, Visa: I came, I
saw, I did a little shopping”. One needs a larger
cultural context to figure out the subtle humorous
meaning expressed in these two sentences. Last
but not least, there are different types of humor
(Raz, 2012), such as wordplay, irony and sarcasm,
but there exist few formal taxonomies of humor
characteristics. Thus it is almost impossible to
design a general algorithm that can classify all the
different types of humor, since even human cannot
perfectly classify all of them.
Although it is impossible to understand univer-
sal humor characteristics, one can still capture the
possible latent structures behind humor (Bucaria,
2004; Binsted and Ritchie, 1997). In this work, we
uncover several latent semantic structures behind
humor, in terms of meaning incongruity, ambigu-
ity, phonetic style and personal affect. In addition
to humor recognition, identifying anchors, or
which words prompt humor in a sentence, is
essential in understanding the phenomenon of
humor in language. Here, Anchor Extraction
refers to extracting the semantic units (keywords
or phrases) that enable the humor in a given
sentence. The presence of such anchors plays
an important role in generating humor within a
sentence or phrase.
In this work, we formulate humor recognition
as a classification task in which we distinguish
between humorous and non-humorous instances.
Then we explore the semantic structure behind
humor from four perspectives: incongruity, am-
</bodyText>
<page confidence="0.921025">
2367
</page>
<note confidence="0.984535">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2367–2376,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999389333333333">
biguity, interpersonal effect and phonetic style.
For each latent structure, we design a set of
features to capture the potential indicators of
humor. With high classification accuracy, we
then extract humor anchors in sentences via a
simple and effective method. Both quantitative
and qualitative experimental results are provided
to validate the classification and anchor extraction
performance.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997559770115">
Most existing studies on humor recognition are
formulated as a binary classification problem and
try to recognize jokes via a set of linguistic
features (Purandare and Litman, 2006; Kiddon
and Brun, 2011). For example, Mihalcea and
Strapparava (2005) defined three types of humor-
specific stylistic features: Alliteration, Antonym
and Adult Slang, and trained a classifier based
on these feature representations. Similarly, Zhang
and Liu (2014) designed several categories of
humor-related features, derived from influential
humor theories, linguistic norms, and affective
dimensions, and input around fifty features into
the Gradient Boosting Regression Tree model for
humor recognition. Taylor and Mazlack (2004)
recognized wordplay jokes based on statistical lan-
guage recognition techniques, where they learned
statistical patterns of text in N-grams and provided
a heuristic focus for a location of where wordplay
may or may not occur. Similar work can also be
found in (Taylor, 2009), which described humor
detection process through Ontological Semantics
by automatically transposing the text into the
formatted text-meaning representation to detect
humor. In addition to language features, some
other studies also utilize spoken or multimodal
signals. For example, Purandare and Litman
(2006) analyzed acoustic-prosodic and linguistic
features to automatically recognize humor during
spoken conversations. However, the humor
related features in most of those works are not
systematically derived or explained.
One essential component in humor recognition
is the construction of negative data instances.
Classifiers based on negative samples that lie in
a different domain than humor positive instances
will have high classification performance, but
are not necessarily good classifiers. There
are few existing benchmark datasets for humor
recognition and most studies select negative
instances specifically. For example, Mihalcea and
Strapparava (2005) constructed the set of negative
examples by using news title from Reuters news,
proverbs and British National Corpus. (Zhang, el.
al 2014) randomly sampled 1500 tweets and then
asked annotators to filter out humorous tweets.
Compared to humor recognition, humor gener-
ation has received quite a lot attention in the past
decades(Stock and Strapparava, 2005; Ritchie,
2005; Hong and Ong, 2009). Most generation
work draws on humor theories to account for
humor factors, such as the Script-based Semantic
Theory of Humor (Raskin, 1985; Labutov and
Lipson, 2012) and employs templates to generate
jokes. For example, Ozbal and Strapparava (2012)
created humorous neologism using WordNet and
ConceptNet. In detail, their system combined
several linguistic resources to generate creative
names, more specifically neologisms based on
homophonic puns and metaphors. Stock and
Strapparava (2005) introduced HAHACRONYM,
a system (an acronym ironic re-analyzer and gen-
erator) devoted to produce humorous acronyms
mainly by exploiting incongruity theories (Stock
and Strapparava, 2003).
In contrast to research on humor recognition
and generation, there are few studies that identify
the humor anchors that trigger humorous effects in
general sentences. A certain type of jokes might
have specific structures or characteristics that
provide pointers to humor anchors. For example,
in the problem of “That’s what she said” (Kiddon
and Brun, 2011), characteristics that involves the
using of nouns that are euphemisms for sexually
explicit nouns or structures common in the erotic
domain might probably give clues to potential
humor anchors. Similarly, in the Knock Knock
jokes (Taylor and Mazlack, 2004), wordplay is
what leads to the humor. However, the wordplay
by itself is not enough to trigger the comic effect,
thus not equivalent to the humor anchors for a
joke. To address these issues, we introduce a
formal definition of humor anchors and design an
effective method to extract such anchors in this
work. To the best of our knowledge, this is the
first study on extracting humor anchors that trigger
humor in general sentences.
</bodyText>
<sectionHeader confidence="0.994352" genericHeader="method">
3 Data Preparation
</sectionHeader>
<bodyText confidence="0.9934195">
To perform automatic recognition of humor and
humor anchor extraction, a data set consisting
of both humorous (positive) and non-humorous
(negative) examples is needed. The dataset we
</bodyText>
<page confidence="0.976418">
2368
</page>
<bodyText confidence="0.999945738095238">
use to conduct our humor recognition experiments
includes two parts: Pun of the Day 1 and the 16000
One-Liner dataset (Mihalcea and Strapparava,
2005). The two data sets only contain humorous
text. In order to acquire negative samples
for the humor classification task, we sample
negative samples from four resources, including
AP News2, New York Times, Yahoo! Answer3
and Proverb4. Such datasets not only enable us
to automatically learn computational models for
humor recognition, but also provide us with the
chances to evaluate the performance of our model.
However, directly applying sentences extracted
from those four resources and simply treating
them as negative instances of humor recognition
could result in deceptively high performance
of classification, due to the domain differences
between positive and negative datasets. For
example, the humor sentences in our positive
datasets often relate to daily lives, such as “My
wife tells me I’m a skeptic, but I don’t believe
a word she says.”. Meanwhile, sentences in
news websites sometimes describe scenes related
to wars or politics, such as “Judge Thomas P.
Griesa of Federal District Court in Manhattan
stopped short of issuing sanctions”. Such domain
differences between descriptive words might make
a naive bag of words model perform quite well,
without taking into account the deeper semantic
structures behind humor. To deal with this issue,
we extract our negative instances in a way that
tries to minimize such domain differences by (1)
selecting negative instances whose words are all
contained in our positive instance word dictionary
and (2) forcing the text length of non-humorous
instances to follow the similar length restriction
as humorous examples, i.e. one sentence with
an average length of 10-30 words. Here, we
assume sentences come from the aforementioned
four resources are all non-humorous in nature.
Table 1 provides a detailed statistical description
to our datasets.
</bodyText>
<sectionHeader confidence="0.980313" genericHeader="method">
4 Latent Structures behind Humor
</sectionHeader>
<bodyText confidence="0.992664">
In this section, we explore the latent semantic
structures behind humor in four aspects: (a)
Incongruity; (b) Ambiguity; (c) Interpersonal
</bodyText>
<footnote confidence="0.9942392">
1Pun of the Day: http://www.punoftheday.
com/ This constructed dataset will be made public.
2http://hosted.ap.org/dynamic/fronts/HOME?SITE=AP
3https://answers.yahoo.com/
4Manually extracted 654 proverbs from Proverb websites
</footnote>
<table confidence="0.517634">
#Positive #Negative
2423 2403
16000 16002
</table>
<tableCaption confidence="0.996699">
Table 1: Statistics on Two Datasets
</tableCaption>
<bodyText confidence="0.998894">
Effect and (d) Phonetic Style. For each latent
structure, a set of features is designed to capture
the corresponding indicators of humor.
</bodyText>
<subsectionHeader confidence="0.998557">
4.1 Incongruity Structure
</subsectionHeader>
<bodyText confidence="0.99979147826087">
“Laughter arises from the view of two or more
inconsistent, unsuitable, or incongruous parts or
circumstances, considered as united in complex
object or assemblage, or as acquiring a sort
of mutual relation from the peculiar manner in
which the mind takes notice of them” (Lefcourt,
2001). The essence of the laughable is the
incongruous, the disconnecting of one idea from
another (Paulos, 2008). Humor sometimes relies
on a certain type of incongruity, such as opposition
or contradiction. For example, the following
‘clean desk’ and ‘cluttered desk drawer’ example
(Mihalcea and Strapparava, 2005) presents an
incongruous/contrast structure, resulting in a
comic effect.
A clean desk is a sign of a cluttered desk drawer.
Direct identification of incongruity is hard
to achieve, however, it is relatively easier to
measure the semantic disconnection in a sentence.
Taking advantage of Word2Vec5, we extract
two types of features to evaluate the meaning
distance6 between content word pairs in a sentence
(Mikolov et al., 2013):
</bodyText>
<listItem confidence="0.99571725">
• Disconnection: the maximum meaning dis-
tance of word pairs in a sentence.
• Repetition: the minimum meaning distance
of word pairs in a sentence.
</listItem>
<subsectionHeader confidence="0.999047">
4.2 Ambiguity Theory
</subsectionHeader>
<bodyText confidence="0.997806333333333">
Ambiguity (Bucaria, 2004), the disambiguation of
words with multiple meanings (Bekinschtein et
al., 2011), is a crucial component of many humor
jokes (Miller and Gurevych, 2015). Humor and
ambiguity often come together when a listener
expects one meaning, but is forced to use another
</bodyText>
<footnote confidence="0.9901426">
5https://code.google.com/p/word2vec/
6We take the generic Word2Vec vectors without training
new vectors for our specific domain. In addition, vectors
associated with senses (Kumar Jauhar et al., 2015) might be
alternative advantageous in this task.
</footnote>
<figure confidence="0.929956">
Dataset
Pun of the Day
16000 One Liners
</figure>
<page confidence="0.985935">
2369
</page>
<bodyText confidence="0.998541916666667">
meaning. Ambiguity occurs when the words of
the surface sentence structure can be grouped
in more than one way, thus yielding more than
one associated deep structures, as shown in the
example below.
Did you hear about the guy whose whole left
side was cut off? He’s all right now.
The multiple possible meanings of words
provide readers with different understandings. To
capture the ambiguity contained in a sentence, we
utilize the lexical resource WordNet (Fellbaum,
1998) and capture the ambiguity as follows:
</bodyText>
<listItem confidence="0.998783333333333">
• Sense Combination: the sense combination
in a sentence computed as follows: we first
use a POS tagger (Toutanova et al., 2003)
to identify Noun, Verb, Adj, Adv. Then we
consider the possible meanings of such words
{w1, w2 · · · wk} via WordNet and calculate
the sense combinations as log(r1ki=1 nwi).
nwi is the total number of senses of word wi.
• Sense Farmost: the largest Path Similarity7
of any word senses in a sentence.
• Sense Closest: the smallest Path Similarity of
any word senses in a sentence.
</listItem>
<subsectionHeader confidence="0.9944">
4.3 Interpersonal Effect
</subsectionHeader>
<bodyText confidence="0.987882263157895">
Besides humor theories and linguistic style mod-
eling, one important theory behind humor is
its social/hostility focus, especially regarding its
interpersonal effect on receivers. That is, humor is
essentially associated with sentiment (Zhang and
Liu, 2014) and subjectivity (Wiebe and Mihalcea,
2006). For example, a sentence is likely to
be humorous if it contains some words carrying
strong sentiment, such as ‘idiot’ as follows.
Your village called. They want their Idiot back.
Each word is associated with positive or neg-
ative sentiments and such measurements reflect
the emotion expressed by the writer. To identify
the word-associated sentiment, we use the word
association resource in the work by (Wilson et al.,
2005), which provides annotations and clues to
measure the subjectivity and sentiment associated
with words. This enables us to design the
following features.
</bodyText>
<listItem confidence="0.999329">
• Negative (Positive) Polarity: the number of
occurrences of all Negative (Positive) words.
</listItem>
<footnote confidence="0.7511705">
7Path Similarity: http://www.nltk.org/howto/
wordnet.html
</footnote>
<listItem confidence="0.9904346">
• Weak (Strong) Subjectivity: the number of
occurrences of all Weak (Strong) Subjectivity
oriented words in a sentence. It is the
linguistic expression of people’s opinions,
evaluations, beliefs or speculations.
</listItem>
<subsectionHeader confidence="0.992956">
4.4 Phonetic Style
</subsectionHeader>
<bodyText confidence="0.966297578947369">
Many humorous texts play with sounds, creating
incongruous sounds or words. Some studies
(Mihalcea and Strapparava, 2005) have shown that
the phonetic properties of humorous sentences
are at least as important as their content. Many
one-liner jokes contain linguistic phenomena such
as alliteration, word repetition and rhyme that
produce a comic effect even if the jokes are not
necessarily meant to be humorous in content.
What is the difference between a nicely dressed
man on a tricycle and a poorly dressed man on a
bicycle? A tire.
An alliteration chain refers to two or more
words beginning with the same phones. A rhyme
chain is defined as the relationship that words
end with the same syllable. To extract this
phonetic feature, we take advantage of the CMU
Pronouncing Dictionary 8 and design four features
as follows:
</bodyText>
<listItem confidence="0.9968488">
• Alliteration: the number of alliteration chains
in a sentence, and the maximum length of
alliteration chains.
• Rhyme: the number of rhyme chains and the
maximum length of rhyme chains.
</listItem>
<sectionHeader confidence="0.978376" genericHeader="method">
5 Humor Anchor Extraction
</sectionHeader>
<bodyText confidence="0.999973">
In addition to humor recognition, identifying
anchors, or which words prompt humor in a
sentence, is also essential in understanding humor
language phenomena. In this section, we first
define what humor anchors are and then describe
how to extract such semantic units that enable
humor in a given sentence.
</bodyText>
<subsectionHeader confidence="0.95692">
5.1 Humor Anchor Definition
</subsectionHeader>
<bodyText confidence="0.999859142857143">
The semantic units or humor anchors enable
humor in a given sentence, and are reflected
in the form of sentence words. However, not
every single word can be a humor anchor. For
example, I am glad that I know sign language;
it is pretty handy. In this one-liner, words such
as ‘am’ and ‘is’ are not able to enable humor
</bodyText>
<footnote confidence="0.959487">
8http://www.speech.cs.cmu.edu/cgi-bin/cmudict
</footnote>
<page confidence="0.957382">
2370
</page>
<figure confidence="0.996220423076923">
Maximal Decrement Results
0.8, i am glad that I know [sign language]; it is [pretty handy] ßBest
0.3, i am glad that I know [sign language]; it is pretty handy
0.2, i am glad that I know sign language; it is [pretty handy]
0.1, i am glad that I [know] sign language; it is [pretty handy]
Humor sentence:
i am glad that i know
sign language; it is
pretty handy
Maximal Decrement Algorithm
· f(X): the predicted score for
sentence X
· f(X \ K): the predicted score by
removing set K from X
· For Anchor Subset K (|K|&lt;=t),
calculate f(X) - f(X \ K).
· Find the K with max decrement.
Anchor Candidates
i
am
i
know
sign language
it
is
pretty handy
</figure>
<figureCaption confidence="0.999309">
Figure 1: Humor Anchor Extraction Overview. Based on the parsing output of each sentence, we
</figureCaption>
<bodyText confidence="0.965286636363636">
generate its humor anchor candidates. We then apply the Maximal Decrement algorithm to these
candidates. The humor anchor subset that gives the maximal decrement is the extracted humor anchors
for that sentence.
via themselves. Similarly, ‘sign’ or ‘language’
itself are not capable to prompt comic effect.
The possible anchors in this example should
contain both ‘sign language’ and ‘handy’; it is
the combination of these two spans that triggers
humor. Therefore, formally defined, a humor
anchor is a meaningful, complete, minimal set
of word spans in a sentence that potentially
enable one of the latent structures of Section 4
to occur. (1) Meaningful means humor anchors
are meaningful word spans, not meaningless stop
words in a sentence; (2) Completeness shows that
all possible humor anchors should be covered by
this anchor set and no individual span in this
anchor set is capable enough to enable humor; (3)
Minimal emphasizes that it is the combination of
these anchors together that prompts comic effect;
discarding any anchors from this candidate set
destroys the humorous effect.
</bodyText>
<subsectionHeader confidence="0.9976">
5.2 Anchor Extraction Method
</subsectionHeader>
<bodyText confidence="0.99893375">
Based on the humor anchor requirements listed
above, we scoped humor anchor candidates to
words or phrases that belong to the syntactic
categories of Noun, Verb, Noun Phrase, Verb
Phrase, ADVP or ADJP. Those properties are
acquired via a sentence parse tree. To generate
anchor candidates, we parsed each sentence and
selected words or phrases that satisfy one or more
of the latent structure criteria by first extracting
the minimal parse subtrees of NP, VP, ADVP and
ADJP and then adding remaining Nouns and Verbs
into candidate sets.
The above anchor generation process provides
us with all possible anchors that might enable
humor. It satisfies the Meaningful and Com-
pleteness requirements. To extract a Minimal set
of anchors, we proposed a simple and effective
method of Maximal Decrement. Its basic idea is
summarized as follows: Each complete sentence
has a predicted humor score, which is computed
via a humor recognition classifier trained on all
data points. This humor recognizer is not limited
to any specific classifiers or features as long as
it provides good classification accuracy, which
guarantees the generalization ability of our anchor
extraction method. We next enumerate a subset
of anchors from all potential anchors for this
sentence. Then, we recompute the predicted
humor score by providing the classifier with
features associated with the current sentence, after
removing that subset of anchors. Note that our
designed humor structural features are all word
order free, thereby not distinguishing between
complete and incomplete sentences. The subset
of humor anchor candidates that provides the
maximum decrement of humor predicted scores is
then returned as the extracted humor anchor set.
Mathematically, Xi is the word set of sentence
i. Let f denote the trained classifier on all data
instances. f(Xi) is the predicted humor score
</bodyText>
<page confidence="0.963965">
2371
</page>
<bodyText confidence="0.999972166666667">
for sentence i before performing any operations.
Denote Ki(Ki C Xi) as the subset of words
that we need to remove from sentence i. The
size of Ki should be smaller than a threshold t,
|Ki |G t. f(Xi/Ki) is the recomputed humor
score for sentence i after removing Ki. Our
Maximal Decrement method tries to maximize the
following objective by enumerating all possible
Kis. The subset Ki that gives the maximal
decrement is returned as our extracted humor
anchors for sentence i. The system overview is
shown in Figure 1.
</bodyText>
<equation confidence="0.8342165">
arg min f(Xi) − f(Xi/Ki) (1)
|Ki≤t|
</equation>
<sectionHeader confidence="0.991645" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.999957571428571">
In this section, we validate the performance
of different semantic structures we extracted on
humor recognition and how the combination of
the structures contributes to classification. In
addition, both qualitative and quantitative results
regarding humor anchor extraction performance
are explored.
</bodyText>
<subsectionHeader confidence="0.993449">
6.1 Humor Recognition
</subsectionHeader>
<bodyText confidence="0.99952372">
We formulate humor recognition as a traditional
text classification problem, and apply Random
Forest to perform 10 fold cross validation on
two datasets. Random Forest is an ensemble of
decision trees 9 for classification (regression) that
constructs a multitude of decision trees at training
time and outputs the class that is the mode of the
classes output by individual trees. Unlike single
decision trees, which are likely to suffer from
high variance or high bias, random forests use
averaging to find a natural balance between the
two extremes.
In addition to the four latent structures behind
humor, we also design a set of K Nearest Neighbor
(KNN) features that uses the humor classes of
the K sentences (K = 5) that are the closest
to this sentence in terms of meaning distance
in the training data. We use several methods
to act as baselines for comparison with our
classier. Bag of Words baseline is used to
capture a multiset of words in a sentence that
might differentiate humor and non-humor. Lan-
guage Model baseline assigns a humor/nonhumor
probability to words in a sentence via probability
distributions. Word2Vec baseline represents the
</bodyText>
<footnote confidence="0.6819">
9https://www.kaggle.com/wiki/RandomForests
</footnote>
<bodyText confidence="0.9698358">
meaning of sentences via Word2Vec (Mikolov
et al., 2013) distributional semantic meaning
representation. We implemented an earlier work
(Mihalcea and Strapparava, 2005) that exploits
stylistic features including alliteration, autonomy
and adult slang and ensembles with bag of words
representations, denoted as SaC Ensemble. It is
worth mentioning that our datasets are balanced in
terms of positive and negative instances, giving a
random classification accuracy of 50%.
</bodyText>
<figureCaption confidence="0.937884">
Figure 2: Different Latent Structures’ Contribu-
tion to Humor Recognition
</figureCaption>
<bodyText confidence="0.999678703703704">
We first explored how different latent semantic
structures affect humor recognition performance
and summarize the results in Figure 2. It
is evident that Incongruity performs the best
among all latent semantic structures in the context
of Pun of the Day and both Ambiguity and
Phonetic substantially contribute to recognition
performance on the 16000 One Liners dataset.
The reason behind the differences in performance
with Incongruity and with Phonetic lies in the
different nature of the corpus. Most puns are
well structured and play with contrasting or
incongruous meaning. However, humor sentences
in the 16000 One Liners often rely on the reader’s
awareness of attention-catching sounds (Mihalcea
and Strapparava, 2005). This demonstrates that
humor characteristics are expressed differently in
different contexts and datasets.
We also investigated how the combination of
such semantic structures performs compared with
our proposed baselines, as shown in Table 2.
Here, we denote the combination of four latent
structures and KNN features as Human Centric
Features (HCF). From Table 2, we found that
(1) HCF (21 features in total) has a bigger
contribution to humor recognition, compared with
Bag of Words and Language Model (LM). The
</bodyText>
<page confidence="0.981641">
2372
</page>
<table confidence="0.960745375">
Pun of the Day 16000 One Liners
Accuracy Precision Recall F1 Accuracy Precision Recall F1
HCF 0.705 0.696 0.736 0.715 0.701 0.685 0.746 0.714
Bag of Words 0.632 0.623 0.686 0.650 0.673 0.708 0.662 0.684
Language Model 0.627 0.602 0.762 0.673 0.635 0.645 0.596 0.620
Word2Vec 0.833 0.804 0.880 0.841 0.781 0.767 0.809 0.787
SaC Ensemble 0.763 0.838 0.655 0.735 0.662 0.628 0.796 0.701
Word2Vec+HCF 0.854 0.834 0.888 0.859 0.797 0.776 0.836 0.805
</table>
<tableCaption confidence="0.999907">
Table 2: Comparison of Different Methods of Humor Recognition
</tableCaption>
<bodyText confidence="0.999709642857143">
inadequacy of LM also indicates that we can
alleviate the domain differences and capture the
real humor. (2) SaC Ensemble is inferior to the
combination of Word2Vec and HCF because it
does not involve enough latent structures such as
Interpersonal Effect and distributional semantics.
(3) The combination of Word2Vec and HCF
(Word2Vec+HCF) gives the best classification
performance because it takes into account both
latent structures and semantic word meanings.
Such a conclusion is consistent across two
datasets. This indicates that our extracted latent
semantic structures are effective in capturing
humorous meaning.
</bodyText>
<subsectionHeader confidence="0.9998575">
6.2 Anchor Extraction
Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999935928571428">
The above humor recognition classifier provides
us with decent accuracy in identifying humor in
the text. To better understand which words or
semantic units enable humor in sentences, we
performed humor anchor extraction as described
in Section 5.2. We set the size of the humor anchor
set as 3, i.e. t = 3. The classifier that is used
to predict the humor score is trained on all data
instances. Then all predicted humorous instances
are collected and input into the humor anchor
extraction component. Based on the Maximal
Decrement method, a set of humor anchors is
extracted for each instance.
Table 3 presents selected extracted humor
anchor results, including both successful and
unsatisfying extractions. As we can see, extracted
humor anchors are quite reasonable in explaining
the humor causes or focuses. For example, in
the sentence “I used to be a watchmaker; it is a
great job and I made my own hours”, our method
selected ‘watchmaker’, ‘made’ and ‘hours’ as
humor anchors. It makes sense because each
word is necessary and essential to enable humor.
Deleting ‘watchmaker’ will make the combination
of ‘made’ and ‘hours’ helpless to the comic effect.
To sum up, our extracted anchor extraction works
fairly well in identifying the focus and meaning of
humor language.
</bodyText>
<subsectionHeader confidence="0.970643">
Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.996951375">
In addition to the above qualitative exploration,
we also conducted quantitative evaluations. For
each dataset, we randomly sampled 200 sentences.
Then for each sentence, 3 annotators are asked
to annotate and label the possible humor anchors.
To assess the consistency of the labeling in this
context, we introduced an Annotation Agreement
Ratio (AAR) measurement as follows:
</bodyText>
<equation confidence="0.957945">
AAR(A, B) = 1 Ns JAi n Bi
Ns JAi U BiJ
i=1
</equation>
<bodyText confidence="0.9998165">
Here, Ns is the total number of sentences. Ai
and Bi are the humor anchor sets of sentence
i provided by annotator A and B respectively.
The AARs on Pun of the Day and 16000 One
Liners datasets are 0.618 and 0.433 respectively,
computed by averaging the AAR scores between
any two different annotators, which indicate
relatively reasonable agreement.
As a further step to validate the effectiveness of
our anchor extraction method, we also introduced
two baselines. The Random Extraction baseline
selects humor anchors by sampling words in a
sentence randomly. Similarly, POS Extraction
baseline generates anchors by narrowing down all
the words in a sentence to a set of certain POS, e.g.
Noun, Verb, Noun Phrase, Verb Phrase, ADVP
and ADJP and then sampling words from this set.
To evaluate whether our extracted anchors are
consistent with human annotation, we used each
annotator’s extracted anchor list as the ground
truth, and compared with anchor list provided by
our method. To identify whether two anchors
</bodyText>
<page confidence="0.975395">
2373
</page>
<figure confidence="0.4774947">
Result Category
Representative Sentences
Did you hear about the guy who got hit in the head with a can of soda? He was
lucky it was a soft drink.
Good I was struggling to figure out how lightning works then it struck me.
The one who invented the door knocker got a No-bell prize.
I used to be a watchmaker; it is a great job and I made my own hours.
I wanted to lose weight, so I went to the paint store. I heard I could get
Bad thinner there.
I used to be a banker but I lost interest
</figure>
<tableCaption confidence="0.997259">
Table 3: Representative Extracted Humor Anchors. Highlighted parts are the extracted humor anchors in a sentence.
</tableCaption>
<bodyText confidence="0.912495090909091">
are the same, we introduce two measurements:
Exact (EX) Matching and At-Least-One (ALO)
Matching. Exact Matching requires the two
anchors to be exactly the same. For ALO, two
anchors are considered the same if they have at
least one word in common. Recall, Precision and
F1 Score are act as evaluation metrics. We then
average the three annotators’ individual scores to
get the final extraction performance.
Metrics Recall Precision F1
Pun of the Day Dataset
</bodyText>
<table confidence="0.999856692307692">
MDE EX 0.444 0.446 0.438
POS EX 0.166 0.170 0.165
Random EX 0.121 0.116 0.116
MDE ALO 0.782 0.784 0.756
POS ALO 0.364 0.371 0.360
Random ALO 0.297 0.287 0.285
16000 One Liners Dataset
MDE EX 0.314 0.281 0.288
POS EX 0.104 0.110 0.104
Random EX 0.087 0.075 0.079
MDE ALO 0.675 0.638 0.616
POS ALO 0.386 0.363 0.356
Random ALO 0.341 0.334 0.319
</table>
<tableCaption confidence="0.960384">
Table 4: Quantitative Result Comparison of
Humor Anchor Extraction
</tableCaption>
<bodyText confidence="0.999678727272727">
The quantitative evaluation results are summa-
rized in Table 4. Maximal Decrement Extraction
is denoted as MDE; POS Extraction is denoted
as POS, and Random Extraction is denoted as
Random. We report both ALO and EX results
for MED, POS and Random. From Table 4, we
found that MDE performs quite well under the
measurement of human annotation in terms of
both ALO and EX settings. This again validates
our assumption towards humor anchors and the
effectiveness of our anchor extraction method.
</bodyText>
<subsectionHeader confidence="0.93951">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999014763157895">
The above two subsections described the per-
formance of both humor recognition and humor
anchor extraction tasks. In terms of humor recog-
nition, incongruity, ambiguity, personal affect and
phonetic style are taken into consideration to
assist the identification of humorous language.
We focus on discovering generalized structures
behind humor, and did not take into account sexual
oriented words such as adult slang in modeling
humorous language. Based on our results, these
four latent structures are effective in capturing
humor characteristics and such characteristics are
expressed to different extents in different contexts.
Note that we can apply any classification methods
with our humor latent structures. Once such
structures help us acquire high recognition accu-
racy, we can perform the generalized Maximal
Decrement extraction method to identify anchors
in humorous text.
Both humor recognition and humor anchor
extraction suffer from several common issues.
(1) Phrase Meaning: For example, a humorous
sentence “How does the earth get clean? It takes
a meteor shower” is predicted as non-humorous,
because the recognizer does not fully understand
the meaning of ‘meteor shower’, let alone the
comic effect caused by ‘earth’, ‘clean’ and
‘meteor shower’. For the unsatisfying example
in Table 3 “I used to be a banker but I lost
interest”, anchor extraction would work better if
it recognizes ‘lost interest’ correctly as a basic
semantic unit. (2) External Knowledge: For
jokes that involve idioms or social phenomena,
or need some external knowledge such as “Veni,
Vidi, Visa: I came, I saw, I did a little shopping”,
both humor recognition and anchor extraction fail
because a broader and implicit comparison of this
sentence and its origin (“Veni, Vidi, Vici: I came, I
</bodyText>
<page confidence="0.965824">
2374
</page>
<bodyText confidence="0.9977155">
saw, I conquered... ”) is hard to be captured from a
sentence. (3) Humor Categorization: Moreover,
a fine granularity categorization of humor might
aid in understanding humorous language, because
humor has different types of manifestations, such
as irony, sarcasm, creativity, insult and wordplay.
Therefore, more sophisticated techniques in mod-
eling phrase meaning, external knowledge, humor
types, etc., are needed to better expose and define
humor for automatic recognition and extraction.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999990222222222">
In this work, we focus on understanding hu-
morous language through two subtasks: humor
recognition and humor anchor extraction. For
this purpose, we first designed four semantic
structures behind humor. Based on the designed
sets of features associated with each structure, we
constructed different computational classifiers to
recognize humor. Then we proposed a simple
and effective Maximal Decrement method to
automatically extract anchors that enable humor
in a sentence. Experimental results conducted
on two datasets demonstrate the effectiveness of
our proposed latent structures. The performances
of humor recognition and anchor extraction are
superior compared to several baselines. In the
future, we would like to step further into the
discovery of humor characteristics and apply our
findings to the process of humor generation.
</bodyText>
<sectionHeader confidence="0.970237" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99981875">
The authors would like to thank Li Zhou, Anna
Kasunic, the anonymous reviewers, our annotators
and all colleagues who have contributed their
valuable comments and suggestions.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999346417910448">
Salvatore Attardo. 1994. Linguistic theories of humor,
volume 1. Walter de Gruyter.
Tristan A Bekinschtein, Matthew H Davis, Jennifer M
Rodd, and Adrian M Owen. 2011. Why clowns
taste funny: the relationship between humor and
semantic ambiguity. The Journal of Neuroscience,
31(26):9665–9671.
Kim Binsted and Graeme Ritchie. 1997. Computa-
tional rules for generating punning riddles. Humor:
International Journal of Humor Research.
Chiara Bucaria. 2004. Lexical and syntactic ambiguity
as a source of humor: The case of newspaper
headlines. Humor, 17(3):279–310.
Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.
Bryan Anthony Hong and Ethel Ong. 2009. Automat-
ically extracting word relationships as templates for
pun generation. In Proceedings of the Workshop on
Computational Approaches to Linguistic Creativity,
CALC ’09, pages 24–31, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That’s what she
said: double entendre identification. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 89–94.
Association for Computational Linguistics.
Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In The 2015 Conference of the North American
Chapter of the Association for Computational
Linguistics.
Igor Labutov and Hod Lipson. 2012. Humor as
circuits in semantic networks. In Proceedings of
the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume
2, pages 150–155. Association for Computational
Linguistics.
Herbert M Lefcourt. 2001. Humor: The psychology
of living buoyantly. Springer Science &amp; Business
Media.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 531–
538. Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed
representations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems, pages 3111–3119.
Tristan Miller and Iryna Gurevych. 2015. Automatic
disambiguation of english puns. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 719–729, Beijing,
China, July. Association for Computational Linguis-
tics.
G¨ozde Ozbal and Carlo Strapparava. 2012. Computa-
tional humour for creative naming. Computational
Humor 2012, page 15.
John Allen Paulos. 2008. Mathematics and humor: A
study of the logic of humor. University of Chicago
Press.
</reference>
<page confidence="0.693817">
2375
</page>
<reference confidence="0.9997438125">
Amruta Purandare and Diane Litman. 2006. Humor:
Prosody analysis and automatic recognition for
f*r*i*e*n*d*s*. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’06, pages 208–215, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Victor Raskin. 1985. Semantic mechanisms of humor,
volume 24. Springer.
Yishay Raz. 2012. Automatic humor classification
on twitter. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies: Student Research Workshop, pages
66–70. Association for Computational Linguistics.
Graeme Ritchie. 2005. Computational mechanisms
for pun generation. In Proceedings of the 10th
European Natural Language Generation Workshop,
pages 125–132. Citeseer.
Oliviero Stock and Carlo Strapparava. 2003. Getting
serious about the development of computational
humor. In IJCAI, volume 3, pages 59–64.
Oliviero Stock and Carlo Strapparava. 2005.
Hahacronym: A computational humor system.
In Proceedings of the ACL 2005 on Interactive
poster and demonstration sessions, pages 113–116.
Association for Computational Linguistics.
J Taylor and L Mazlack. 2004. Computationally
recognizing wordplay in jokes. Proceedings of
CogSci 2004.
Julia M Taylor. 2009. Computational detection of
humor: A dream or a nightmare? the ontological
semantics approach. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology-
Volume 03, pages 429–432. IEEE Computer Society.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 173–
180, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association
for Computational Linguistics, pages 1065–1072.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 347–354, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Renxian Zhang and Naishi Liu. 2014. Recognizing
humor on twitter. In Proceedings of the 23rd
ACM International Conference on Conference on
Information and Knowledge Management, pages
889–898. ACM.
</reference>
<page confidence="0.989623">
2376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899382">
<title confidence="0.999569">Humor Recognition and Humor Anchor Extraction</title>
<author confidence="0.958822">Diyi Yang</author>
<author confidence="0.958822">Alon Lavie</author>
<author confidence="0.958822">Chris Dyer</author>
<author confidence="0.958822">Eduard</author>
<affiliation confidence="0.963498">Language Technologies Institute, School of Computer</affiliation>
<address confidence="0.952195">Carnegie Mellon University. Pittsburgh, PA, 15213,</address>
<email confidence="0.998771">alavie,hovy@cmu.edu</email>
<abstract confidence="0.9993698">Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salvatore Attardo</author>
</authors>
<title>Linguistic theories of humor,</title>
<date>1994</date>
<volume>1</volume>
<note>Walter de Gruyter.</note>
<contexts>
<context position="1895" citStr="Attardo, 1994" startWordPosition="273" endWordPosition="274">nd learn to understand their users. When a computer converses with a human being, if it can figure out the humor in human’s language, it can better understand the true meaning of human language, and thereby make better decisions that improve the user experience. Developing techniques that enable computers to understand humor in human conversations and adapt behavior accordingly deserves particular attention. The task of Humor Recognition refers to determining whether a sentence in a given context expresses a certain degree of humor. Humor recognition is a challenging natural language problem (Attardo, 1994). First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, “The one who invented the door knocker got a No Bell prize” and “Veni, Vidi, Visa: I came, I saw, I did a little shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor </context>
</contexts>
<marker>Attardo, 1994</marker>
<rawString>Salvatore Attardo. 1994. Linguistic theories of humor, volume 1. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan A Bekinschtein</author>
<author>Matthew H Davis</author>
<author>Jennifer M Rodd</author>
<author>Adrian M Owen</author>
</authors>
<title>Why clowns taste funny: the relationship between humor and semantic ambiguity.</title>
<date>2011</date>
<journal>The Journal of Neuroscience,</journal>
<volume>31</volume>
<issue>26</issue>
<contexts>
<context position="12542" citStr="Bekinschtein et al., 2011" startWordPosition="1862" endWordPosition="1865">ean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5https://code.google.com/p/word2vec/ 6We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. Dataset Pun of the Day 16000 One Liners 2369 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding </context>
</contexts>
<marker>Bekinschtein, Davis, Rodd, Owen, 2011</marker>
<rawString>Tristan A Bekinschtein, Matthew H Davis, Jennifer M Rodd, and Adrian M Owen. 2011. Why clowns taste funny: the relationship between humor and semantic ambiguity. The Journal of Neuroscience, 31(26):9665–9671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Binsted</author>
<author>Graeme Ritchie</author>
</authors>
<title>Computational rules for generating punning riddles.</title>
<date>1997</date>
<journal>Humor: International Journal of Humor Research.</journal>
<contexts>
<context position="2957" citStr="Binsted and Ritchie, 1997" startWordPosition="442" endWordPosition="445"> needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal affect. In addition to humor recognition, identifying anchors, or which words prompt humor in a sentence, is essential in understanding the phenomenon of humor in language. Here, Anchor Extraction refers to extracting the semantic units (keywords or phrases) that enable the humor in a given sentence. The presence of such anchors plays an important role in generating humor within a sentence or phrase. In this work, we formulate humor recognition as </context>
</contexts>
<marker>Binsted, Ritchie, 1997</marker>
<rawString>Kim Binsted and Graeme Ritchie. 1997. Computational rules for generating punning riddles. Humor: International Journal of Humor Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiara Bucaria</author>
</authors>
<title>Lexical and syntactic ambiguity as a source of humor: The case of newspaper headlines.</title>
<date>2004</date>
<journal>Humor,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="2929" citStr="Bucaria, 2004" startWordPosition="440" endWordPosition="441"> shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal affect. In addition to humor recognition, identifying anchors, or which words prompt humor in a sentence, is essential in understanding the phenomenon of humor in language. Here, Anchor Extraction refers to extracting the semantic units (keywords or phrases) that enable the humor in a given sentence. The presence of such anchors plays an important role in generating humor within a sentence or phrase. In this work, we for</context>
<context position="12462" citStr="Bucaria, 2004" startWordPosition="1853" endWordPosition="1854">an incongruous/contrast structure, resulting in a comic effect. A clean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5https://code.google.com/p/word2vec/ 6We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. Dataset Pun of the Day 16000 One Liners 2369 meaning. Ambiguity occurs when the words of th</context>
</contexts>
<marker>Bucaria, 2004</marker>
<rawString>Chiara Bucaria. 2004. Lexical and syntactic ambiguity as a source of humor: The case of newspaper headlines. Humor, 17(3):279–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="13490" citStr="Fellbaum, 1998" startWordPosition="2013" endWordPosition="2014">ociated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. Dataset Pun of the Day 16000 One Liners 2369 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w1, w2 · · · wk} via WordNet and calculate the sense combinations as log(r1ki=1 nwi). nwi is the total number of senses of word wi. • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Anthony Hong</author>
<author>Ethel Ong</author>
</authors>
<title>Automatically extracting word relationships as templates for pun generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09,</booktitle>
<pages>24--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6701" citStr="Hong and Ong, 2009" startWordPosition="979" endWordPosition="982">cation performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorou</context>
</contexts>
<marker>Hong, Ong, 2009</marker>
<rawString>Bryan Anthony Hong and Ethel Ong. 2009. Automatically extracting word relationships as templates for pun generation. In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09, pages 24–31, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chloe Kiddon</author>
<author>Yuriy Brun</author>
</authors>
<title>That’s what she said: double entendre identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>89--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4556" citStr="Kiddon and Brun, 2011" startWordPosition="676" endWordPosition="679">ics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they</context>
<context position="7743" citStr="Kiddon and Brun, 2011" startWordPosition="1130" endWordPosition="1133">ogisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences. A certain type of jokes might have specific structures or characteristics that provide pointers to humor anchors. For example, in the problem of “That’s what she said” (Kiddon and Brun, 2011), characteristics that involves the using of nouns that are euphemisms for sexually explicit nouns or structures common in the erotic domain might probably give clues to potential humor anchors. Similarly, in the Knock Knock jokes (Taylor and Mazlack, 2004), wordplay is what leads to the humor. However, the wordplay by itself is not enough to trigger the comic effect, thus not equivalent to the humor anchors for a joke. To address these issues, we introduce a formal definition of humor anchors and design an effective method to extract such anchors in this work. To the best of our knowledge, th</context>
</contexts>
<marker>Kiddon, Brun, 2011</marker>
<rawString>Chloe Kiddon and Yuriy Brun. 2011. That’s what she said: double entendre identification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 89–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujay Kumar Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
</authors>
<title>Ontologically grounded multi-sense representation learning for semantic vector space models.</title>
<date>2015</date>
<booktitle>In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12922" citStr="Jauhar et al., 2015" startWordPosition="1918" endWordPosition="1921">ing distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5https://code.google.com/p/word2vec/ 6We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. Dataset Pun of the Day 16000 One Liners 2369 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as fo</context>
</contexts>
<marker>Jauhar, Dyer, Hovy, 2015</marker>
<rawString>Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy. 2015. Ontologically grounded multi-sense representation learning for semantic vector space models. In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Humor as circuits in semantic networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>150--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6868" citStr="Labutov and Lipson, 2012" startWordPosition="1005" endWordPosition="1008">stances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies </context>
</contexts>
<marker>Labutov, Lipson, 2012</marker>
<rawString>Igor Labutov and Hod Lipson. 2012. Humor as circuits in semantic networks. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 150–155. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert M Lefcourt</author>
</authors>
<title>Humor: The psychology of living buoyantly.</title>
<date>2001</date>
<publisher>Springer Science &amp; Business Media.</publisher>
<contexts>
<context position="11526" citStr="Lefcourt, 2001" startWordPosition="1709" endWordPosition="1710">P 3https://answers.yahoo.com/ 4Manually extracted 654 proverbs from Proverb websites #Positive #Negative 2423 2403 16000 16002 Table 1: Statistics on Two Datasets Effect and (d) Phonetic Style. For each latent structure, a set of features is designed to capture the corresponding indicators of humor. 4.1 Incongruity Structure “Laughter arises from the view of two or more inconsistent, unsuitable, or incongruous parts or circumstances, considered as united in complex object or assemblage, or as acquiring a sort of mutual relation from the peculiar manner in which the mind takes notice of them” (Lefcourt, 2001). The essence of the laughable is the incongruous, the disconnecting of one idea from another (Paulos, 2008). Humor sometimes relies on a certain type of incongruity, such as opposition or contradiction. For example, the following ‘clean desk’ and ‘cluttered desk drawer’ example (Mihalcea and Strapparava, 2005) presents an incongruous/contrast structure, resulting in a comic effect. A clean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of </context>
</contexts>
<marker>Lefcourt, 2001</marker>
<rawString>Herbert M Lefcourt. 2001. Humor: The psychology of living buoyantly. Springer Science &amp; Business Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: Investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>531--538</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4602" citStr="Mihalcea and Strapparava (2005)" startWordPosition="682" endWordPosition="685">d phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-gra</context>
<context position="6309" citStr="Mihalcea and Strapparava (2005)" startWordPosition="918" endWordPosition="921">nd linguistic features to automatically recognize humor during spoken conversations. However, the humor related features in most of those works are not systematically derived or explained. One essential component in humor recognition is the construction of negative data instances. Classifiers based on negative samples that lie in a different domain than humor positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes.</context>
<context position="8792" citStr="Mihalcea and Strapparava, 2005" startWordPosition="1299" endWordPosition="1302">joke. To address these issues, we introduce a formal definition of humor anchors and design an effective method to extract such anchors in this work. To the best of our knowledge, this is the first study on extracting humor anchors that trigger humor in general sentences. 3 Data Preparation To perform automatic recognition of humor and humor anchor extraction, a data set consisting of both humorous (positive) and non-humorous (negative) examples is needed. The dataset we 2368 use to conduct our humor recognition experiments includes two parts: Pun of the Day 1 and the 16000 One-Liner dataset (Mihalcea and Strapparava, 2005). The two data sets only contain humorous text. In order to acquire negative samples for the humor classification task, we sample negative samples from four resources, including AP News2, New York Times, Yahoo! Answer3 and Proverb4. Such datasets not only enable us to automatically learn computational models for humor recognition, but also provide us with the chances to evaluate the performance of our model. However, directly applying sentences extracted from those four resources and simply treating them as negative instances of humor recognition could result in deceptively high performance of</context>
<context position="11838" citStr="Mihalcea and Strapparava, 2005" startWordPosition="1753" endWordPosition="1756">mor. 4.1 Incongruity Structure “Laughter arises from the view of two or more inconsistent, unsuitable, or incongruous parts or circumstances, considered as united in complex object or assemblage, or as acquiring a sort of mutual relation from the peculiar manner in which the mind takes notice of them” (Lefcourt, 2001). The essence of the laughable is the incongruous, the disconnecting of one idea from another (Paulos, 2008). Humor sometimes relies on a certain type of incongruity, such as opposition or contradiction. For example, the following ‘clean desk’ and ‘cluttered desk drawer’ example (Mihalcea and Strapparava, 2005) presents an incongruous/contrast structure, resulting in a comic effect. A clean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory A</context>
<context position="15444" citStr="Mihalcea and Strapparava, 2005" startWordPosition="2311" endWordPosition="2314">ons and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words. 7Path Similarity: http://www.nltk.org/howto/ wordnet.html • Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations. 4.4 Phonetic Style Many humorous texts play with sounds, creating incongruous sounds or words. Some studies (Mihalcea and Strapparava, 2005) have shown that the phonetic properties of humorous sentences are at least as important as their content. Many one-liner jokes contain linguistic phenomena such as alliteration, word repetition and rhyme that produce a comic effect even if the jokes are not necessarily meant to be humorous in content. What is the difference between a nicely dressed man on a tricycle and a poorly dressed man on a bicycle? A tire. An alliteration chain refers to two or more words beginning with the same phones. A rhyme chain is defined as the relationship that words end with the same syllable. To extract this p</context>
<context position="23057" citStr="Mihalcea and Strapparava, 2005" startWordPosition="3547" endWordPosition="3550">osest to this sentence in terms of meaning distance in the training data. We use several methods to act as baselines for comparison with our classier. Bag of Words baseline is used to capture a multiset of words in a sentence that might differentiate humor and non-humor. Language Model baseline assigns a humor/nonhumor probability to words in a sentence via probability distributions. Word2Vec baseline represents the 9https://www.kaggle.com/wiki/RandomForests meaning of sentences via Word2Vec (Mikolov et al., 2013) distributional semantic meaning representation. We implemented an earlier work (Mihalcea and Strapparava, 2005) that exploits stylistic features including alliteration, autonomy and adult slang and ensembles with bag of words representations, denoted as SaC Ensemble. It is worth mentioning that our datasets are balanced in terms of positive and negative instances, giving a random classification accuracy of 50%. Figure 2: Different Latent Structures’ Contribution to Humor Recognition We first explored how different latent semantic structures affect humor recognition performance and summarize the results in Figure 2. It is evident that Incongruity performs the best among all latent semantic structures in</context>
</contexts>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 531– 538. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="12267" citStr="Mikolov et al., 2013" startWordPosition="1820" endWordPosition="1823">times relies on a certain type of incongruity, such as opposition or contradiction. For example, the following ‘clean desk’ and ‘cluttered desk drawer’ example (Mihalcea and Strapparava, 2005) presents an incongruous/contrast structure, resulting in a comic effect. A clean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5https://code.google.com/p/word2vec/ 6We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vec</context>
<context position="22945" citStr="Mikolov et al., 2013" startWordPosition="3534" endWordPosition="3537">Nearest Neighbor (KNN) features that uses the humor classes of the K sentences (K = 5) that are the closest to this sentence in terms of meaning distance in the training data. We use several methods to act as baselines for comparison with our classier. Bag of Words baseline is used to capture a multiset of words in a sentence that might differentiate humor and non-humor. Language Model baseline assigns a humor/nonhumor probability to words in a sentence via probability distributions. Word2Vec baseline represents the 9https://www.kaggle.com/wiki/RandomForests meaning of sentences via Word2Vec (Mikolov et al., 2013) distributional semantic meaning representation. We implemented an earlier work (Mihalcea and Strapparava, 2005) that exploits stylistic features including alliteration, autonomy and adult slang and ensembles with bag of words representations, denoted as SaC Ensemble. It is worth mentioning that our datasets are balanced in terms of positive and negative instances, giving a random classification accuracy of 50%. Figure 2: Different Latent Structures’ Contribution to Humor Recognition We first explored how different latent semantic structures affect humor recognition performance and summarize t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Automatic disambiguation of english puns.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>719--729</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="12614" citStr="Miller and Gurevych, 2015" startWordPosition="1874" endWordPosition="1877">incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5https://code.google.com/p/word2vec/ 6We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. Dataset Pun of the Day 16000 One Liners 2369 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below.</context>
</contexts>
<marker>Miller, Gurevych, 2015</marker>
<rawString>Tristan Miller and Iryna Gurevych. 2015. Automatic disambiguation of english puns. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 719–729, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨ozde Ozbal</author>
<author>Carlo Strapparava</author>
</authors>
<title>Computational humour for creative naming. Computational Humor</title>
<date>2012</date>
<pages>15</pages>
<contexts>
<context position="6951" citStr="Ozbal and Strapparava (2012)" startWordPosition="1017" endWordPosition="1020">he set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences.</context>
</contexts>
<marker>Ozbal, Strapparava, 2012</marker>
<rawString>G¨ozde Ozbal and Carlo Strapparava. 2012. Computational humour for creative naming. Computational Humor 2012, page 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Allen Paulos</author>
</authors>
<title>Mathematics and humor: A study of the logic of humor.</title>
<date>2008</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="11634" citStr="Paulos, 2008" startWordPosition="1726" endWordPosition="1727">2403 16000 16002 Table 1: Statistics on Two Datasets Effect and (d) Phonetic Style. For each latent structure, a set of features is designed to capture the corresponding indicators of humor. 4.1 Incongruity Structure “Laughter arises from the view of two or more inconsistent, unsuitable, or incongruous parts or circumstances, considered as united in complex object or assemblage, or as acquiring a sort of mutual relation from the peculiar manner in which the mind takes notice of them” (Lefcourt, 2001). The essence of the laughable is the incongruous, the disconnecting of one idea from another (Paulos, 2008). Humor sometimes relies on a certain type of incongruity, such as opposition or contradiction. For example, the following ‘clean desk’ and ‘cluttered desk drawer’ example (Mihalcea and Strapparava, 2005) presents an incongruous/contrast structure, resulting in a comic effect. A clean desk is a sign of a cluttered desk drawer. Direct identification of incongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5, we extract two types of features to evaluate the meaning distance6 between content word pairs in </context>
</contexts>
<marker>Paulos, 2008</marker>
<rawString>John Allen Paulos. 2008. Mathematics and humor: A study of the logic of humor. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Diane Litman</author>
</authors>
<title>Humor: Prosody analysis and automatic recognition for f*r*i*e*n*d*s*.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4532" citStr="Purandare and Litman, 2006" startWordPosition="672" endWordPosition="675">n for Computational Linguistics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognitio</context>
</contexts>
<marker>Purandare, Litman, 2006</marker>
<rawString>Amruta Purandare and Diane Litman. 2006. Humor: Prosody analysis and automatic recognition for f*r*i*e*n*d*s*. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 208–215, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Raskin</author>
</authors>
<title>Semantic mechanisms of humor,</title>
<date>1985</date>
<volume>24</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="6841" citStr="Raskin, 1985" startWordPosition="1003" endWordPosition="1004">ct negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generat</context>
</contexts>
<marker>Raskin, 1985</marker>
<rawString>Victor Raskin. 1985. Semantic mechanisms of humor, volume 24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yishay Raz</author>
</authors>
<title>Automatic humor classification on twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop,</booktitle>
<pages>66--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2506" citStr="Raz, 2012" startWordPosition="375" endWordPosition="376"> First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, “The one who invented the door knocker got a No Bell prize” and “Veni, Vidi, Visa: I came, I saw, I did a little shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal a</context>
</contexts>
<marker>Raz, 2012</marker>
<rawString>Yishay Raz. 2012. Automatic humor classification on twitter. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, pages 66–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Ritchie</author>
</authors>
<title>Computational mechanisms for pun generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Natural Language Generation Workshop,</booktitle>
<pages>125--132</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6680" citStr="Ritchie, 2005" startWordPosition="977" endWordPosition="978">e high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devot</context>
</contexts>
<marker>Ritchie, 2005</marker>
<rawString>Graeme Ritchie. 2005. Computational mechanisms for pun generation. In Proceedings of the 10th European Natural Language Generation Workshop, pages 125–132. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliviero Stock</author>
<author>Carlo Strapparava</author>
</authors>
<title>Getting serious about the development of computational humor.</title>
<date>2003</date>
<booktitle>In IJCAI,</booktitle>
<volume>3</volume>
<pages>59--64</pages>
<contexts>
<context position="7383" citStr="Stock and Strapparava, 2003" startWordPosition="1074" endWordPosition="1077">unt for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences. A certain type of jokes might have specific structures or characteristics that provide pointers to humor anchors. For example, in the problem of “That’s what she said” (Kiddon and Brun, 2011), characteristics that involves the using of nouns that are euphemisms for sexually explicit nouns or structures common in the erotic domain might probably give clues to potential humor anchors. Similarly, in the Knock Knock jokes (Taylor a</context>
</contexts>
<marker>Stock, Strapparava, 2003</marker>
<rawString>Oliviero Stock and Carlo Strapparava. 2003. Getting serious about the development of computational humor. In IJCAI, volume 3, pages 59–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliviero Stock</author>
<author>Carlo Strapparava</author>
</authors>
<title>Hahacronym: A computational humor system.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions,</booktitle>
<pages>113--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6665" citStr="Stock and Strapparava, 2005" startWordPosition="973" endWordPosition="976">r positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and g</context>
</contexts>
<marker>Stock, Strapparava, 2005</marker>
<rawString>Oliviero Stock and Carlo Strapparava. 2005. Hahacronym: A computational humor system. In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, pages 113–116. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Taylor</author>
<author>L Mazlack</author>
</authors>
<title>Computationally recognizing wordplay in jokes.</title>
<date>2004</date>
<booktitle>Proceedings of CogSci</booktitle>
<contexts>
<context position="5065" citStr="Taylor and Mazlack (2004)" startWordPosition="744" endWordPosition="747">blem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-grams and provided a heuristic focus for a location of where wordplay may or may not occur. Similar work can also be found in (Taylor, 2009), which described humor detection process through Ontological Semantics by automatically transposing the text into the formatted text-meaning representation to detect humor. In addition to language features, some other studies also utilize spoken or multimodal signals. For example, Purandare and Litman (2006) analyzed acoust</context>
<context position="8000" citStr="Taylor and Mazlack, 2004" startWordPosition="1169" endWordPosition="1172">a, 2003). In contrast to research on humor recognition and generation, there are few studies that identify the humor anchors that trigger humorous effects in general sentences. A certain type of jokes might have specific structures or characteristics that provide pointers to humor anchors. For example, in the problem of “That’s what she said” (Kiddon and Brun, 2011), characteristics that involves the using of nouns that are euphemisms for sexually explicit nouns or structures common in the erotic domain might probably give clues to potential humor anchors. Similarly, in the Knock Knock jokes (Taylor and Mazlack, 2004), wordplay is what leads to the humor. However, the wordplay by itself is not enough to trigger the comic effect, thus not equivalent to the humor anchors for a joke. To address these issues, we introduce a formal definition of humor anchors and design an effective method to extract such anchors in this work. To the best of our knowledge, this is the first study on extracting humor anchors that trigger humor in general sentences. 3 Data Preparation To perform automatic recognition of humor and humor anchor extraction, a data set consisting of both humorous (positive) and non-humorous (negative</context>
</contexts>
<marker>Taylor, Mazlack, 2004</marker>
<rawString>J Taylor and L Mazlack. 2004. Computationally recognizing wordplay in jokes. Proceedings of CogSci 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia M Taylor</author>
</authors>
<title>Computational detection of humor: A dream or a nightmare? the ontological semantics approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent TechnologyVolume 03,</booktitle>
<pages>429--432</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="5339" citStr="Taylor, 2009" startWordPosition="790" endWordPosition="791">sed on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-grams and provided a heuristic focus for a location of where wordplay may or may not occur. Similar work can also be found in (Taylor, 2009), which described humor detection process through Ontological Semantics by automatically transposing the text into the formatted text-meaning representation to detect humor. In addition to language features, some other studies also utilize spoken or multimodal signals. For example, Purandare and Litman (2006) analyzed acoustic-prosodic and linguistic features to automatically recognize humor during spoken conversations. However, the humor related features in most of those works are not systematically derived or explained. One essential component in humor recognition is the construction of nega</context>
</contexts>
<marker>Taylor, 2009</marker>
<rawString>Julia M Taylor. 2009. Computational detection of humor: A dream or a nightmare? the ontological semantics approach. In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent TechnologyVolume 03, pages 429–432. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13657" citStr="Toutanova et al., 2003" startWordPosition="2039" endWordPosition="2042"> occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w1, w2 · · · wk} via WordNet and calculate the sense combinations as log(r1ki=1 nwi). nwi is the total number of senses of word wi. • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173– 180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1065--1072</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14363" citStr="Wiebe and Mihalcea, 2006" startWordPosition="2153" endWordPosition="2156"> words {w1, w2 · · · wk} via WordNet and calculate the sense combinations as log(r1ki=1 nwi). nwi is the total number of senses of word wi. • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive)</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 1065–1072. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="14788" citStr="Wilson et al., 2005" startWordPosition="2222" endWordPosition="2225">tility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words. 7Path Similarity: http://www.nltk.org/howto/ wordnet.html • Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations. 4.4 Phonetic Style Many humorous texts play with sounds, creating incongruous sounds</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renxian Zhang</author>
<author>Naishi Liu</author>
</authors>
<title>Recognizing humor on twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>889--898</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4795" citStr="Zhang and Liu (2014)" startWordPosition="709" endWordPosition="712">imple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-grams and provided a heuristic focus for a location of where wordplay may or may not occur. Similar work can also be found in (Taylor, 2009), which described humor detection process through Ontolo</context>
<context position="14319" citStr="Zhang and Liu, 2014" startWordPosition="2147" endWordPosition="2150"> consider the possible meanings of such words {w1, w2 · · · wk} via WordNet and calculate the sense combinations as log(r1ki=1 nwi). nwi is the total number of senses of word wi. • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design t</context>
</contexts>
<marker>Zhang, Liu, 2014</marker>
<rawString>Renxian Zhang and Naishi Liu. 2014. Recognizing humor on twitter. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 889–898. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
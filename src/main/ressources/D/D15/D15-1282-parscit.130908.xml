<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003233">
<title confidence="0.977733">
Social Media Text Classification under Negative Covariate Shift
</title>
<author confidence="0.996412">
Geli Fei and Bing Liu
</author>
<affiliation confidence="0.9112135">
University of Illinois at Chicago
Chicago, IL 60607, USA
</affiliation>
<email confidence="0.992419">
gfei2@uic.edu, liub@cs.uic.edu
</email>
<sectionHeader confidence="0.993705" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998906">
In a typical social media content analysis
task, the user is interested in analyzing
posts of a particular topic. Identifying
such posts is often formulated as a classi-
fication problem. However, this problem
is challenging. One key issue is covariate
shift. That is, the training data is not fully
representative of the test data. We ob-
served that the covariate shift mainly oc-
curs in the negative data because topics
discussed in social media are highly di-
verse and numerous, but the user-labeled
negative training data may cover only a
small number of topics. This paper pro-
poses a novel technique to solve the
problem. The key novelty of the tech-
nique is the transformation of document
representation from the traditional n-
gram feature space to a center-based
similarity (CBS) space. In the CBS
space, the covariate shift problem is sig-
nificantly mitigated, which enables us to
build much better classifiers. Experiment
results show that the proposed approach
markedly improves classification.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97377525">
Applications using social media data, such as
reviews, discussion posts, and (micro) blogs are
becoming increasingly popular. We observed
from our collaborations with social science and
health science researchers that in a typical appli-
cation, the researcher first need to obtain a set of
posts of a particular topic that he/she wants to
study, e.g., a political issue. Keyword search is
often used as the first step. However, that is not
sufficient due to low precision and low recall. A
post containing the keyword ‚Äúpolitics‚Äù may not
be a political post while a post that does not con-
tain the keyword may be a political post. Thus,
text classification is needed to make more so-
phisticated decisions to improve accuracy.
For classification, the user first manually la-
bels a set of relevant posts (positive data) about
the political issue and irrelevant posts (negative
data) not about the political issue and then builds
a classifier by running a learning algorithm, e.g.
SVM or na√Øve Bayes. However, the resulting
classifier may not be satisfactory. There may be
many reasons. One key reason we observed is
that the labeled negative training data is not fully
representative of the negative test data.
Let the user-interested topic be P (positive),
and the set of all other irrelevant topics discussed
in a social media source be T = {T1, T2, √â, Tn},
which forms the negative data. n is usually large.
However, due to the labor-intensive effort of
manual labeling, the user can label only a certain
number of training posts. Then the labeled nega-
tive training posts may cover only a small num-
ber of irrelevant topics S of T (S ‚äÜ T) as nega-
tive. Further, due to the highly dynamic nature of
social media, it is probably impossible to label
all possible negative topics. In testing, when
posts of other negative topics in T‚àíS show up,
their classification can be unpredictable. For ex-
ample, in an application, the training data has no
negative examples about sports. However, in
testing, some sports posts show up. These unex-
pected sports posts may be classified arbitrarily,
which results in low classification accuracy. In
this paper, we aim to solve this problem.
In machine learning, this problem is called
covariate shift, a type of sample selection bias.
In classic machine learning, it is assumed that the
training and testing data are drawn from the same
distribution. However, this assumption may not
hold in practice such as in our case above, i.e.,
the training and the test distributions are different
(Heckman 1979; Shimodaira 2000; Zadrozny
2004; Huang et al. 2007; Sugiyama et al. 2008;
Bickel et al. 2009). In general, the sample selec-
tion bias problem is not solvable because the two
</bodyText>
<page confidence="0.921228">
2347
</page>
<note confidence="0.984993">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347‚Äì2356,
Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99973275">
distributions can be arbitrarily far apart from
each other. Various assumptions were made to
solve special cases of the problem. One main
assumption was that the conditional distribution
of the class given a data instance is the same in
the training and test data sets (Shimodaira 2000;
Huang et al. 2007; Bickel et al. 2009). This gives
the covariate shift problem.
In this paper, we focus on a special case of the
covariate shift problem. We assume that the co-
variate shift problem occurs mainly in the nega-
tive training and test data, and no or minimum
covariate shift exists in the positive training and
test data. This assumption is reasonable because
the user knows the type of posts/documents that
s/he is looking for and can label many of them.
Following the notations in (Bickel et al.
2009), our special case of the covariate shift
problem can be stated formally as follows: let the
set of training examples be {(x1, y1), (x2, y2), √â,
(xk, yk)}, where xi is the data/feature vector and yi
is the class label of xi. Let the set of test cases be
{xk+1, xk+2, √â, xn}, which have no class labels.
Since we are interested in binary classification, yi
is either 1 (positive class) or -1 (negative class).
The labeled training data and the unseen test data
have the same target conditional distribution
p(y|x) and the marginal distributions of the posi-
tive data in both the training and testing are also
the same. But the marginal distributions of the
negative data in the training and testing are dif-
ferent, i.e., ùëùL(ùê±‚Äî) ‚â† ùëùAùê± ), where L, T, and ‚Äì
represent the labeled training data, test data, and
the negative class respectively.
Existing methods for addressing the covariate
shift problem basically work as follows (see the
Related Work section). First, they estimate the
bias of the training data based on the given test
data using some statistical techniques. Then, a
classifier is trained on a weighted version of the
original training set based on the estimated bias.
Requiring the test data to be available in training
is, however, a major weakness. In the social me-
dia post classification setting, the system needs
to constantly classify the incoming data. It is in-
feasible to perform training constantly.
In this paper, we propose a novel learning
technique that does not need the test data to be
available during training due to the specific na-
ture of our problem, i.e., the positive training
data does not have the covariate shift issue.
One obvious solution to this problem is one-
class classification (Sch≈°lkopf et al. 1999; Tax
and Duin, 1999a), i.e., one-class SVM. We simp-
ly discard the negative training posts/documents
completely because they have the covariate shift
problem. Although this is a valid solution, as we
will see in the evaluation section, the models
built based on one-class SVM perform poorly.
Although it is conceivable to use an unsuper-
vised method such clustering, SVD (Alter et al.,
2000) or LDA (Blei et al., 2003), supervised
learning usually give much higher accuracy.
In our proposed method, instead of perform-
ing supervised learning in the original document
space based on n-grams, we perform learning in
a similarity space. Thus, the key novelty of the
method is the transformation from the original
document space (DS) to a center-based similarity
space (CBS). In the new space, the covariate
shift problem is significantly mitigated, which
enables us to build more accurate classifiers. The
reason for this is that in CBS based learning the
vectors in the similarity space enable SVM
(which is the learning algorithm that we use) to
find a good boundary of the positive class data
based on similarity and to separate it from all
possible negative class data, including those neg-
ative data that is not represented in training. We
will explain this in greater detail in Section 3.5
after we present the proposed algorithm, which
we call CBS-L (for CBS Learning).
This paper makes three contributions: First, it
formulates a special case of the covariate shift
problem. This case occurs frequently in social
media data classification as we discussed above.
Second, it proposes a novel CBS space based
learning method, CBS-L, which avoids the co-
variate shift problem to a large extent because it
is able to find a good similarity boundary of the
positive data. Third, it experimentally demon-
strates the effectiveness of the proposed method.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999845533333333">
Traditional supervised learning assumes that the
training and test examples are drawn from the
same distribution. However, this assumption can
be violated in many applications. This is espe-
cially the case for social media data because of
the high topic diversity and constant changes of
topics. This problem is known as covariate shift,
which is a form of sample selection bias.
Sample selection bias was first introduced in
econometrics by Heckman (1979). It came into
the field of machine learning through the work of
Zadrozny (2004). The main approach in machine
learning is to first estimate the distribution bias
of the training data based on the test data, and
then learn using weighted training examples to
</bodyText>
<page confidence="0.973878">
2348
</page>
<bodyText confidence="0.999940640000001">
compensate for the bias (Bickel et al. 2009).
For example, Shimodaira (2000) and Sugiya-
ma and Muller (2005) proposed to estimate the
training and test data distributions using kernel
density estimation. The estimated density ratio is
then used to generate weighted training exam-
ples. Dudik et al. (2005) and Bickel and Scheffer
(2007) used maximum entropy density estima-
tion, while Huang et al. (2007) proposed kernel
mean matching. Sugiyama et al. (2008) and Tsu-
boi et al. (2008) estimated the weights for the
training instances by minimizing the Kullback-
Leibler divergence between the test and the
weighted training distributions. Bickel et al.
(2009) proposed an integrated model. As we dis-
cussed in the introduction, the need for the test
data at the training time is a major weakness for
social media data classification. The proposed
technique CBS-L doesn‚Äôt have this restriction.
As mentioned in the introduction, one-class
classification is a suitable approach to solve the
problem. Tax and Duin (1999a and 1999b) pro-
posed a model for one-class classification called
Support Vector Data Description (SVDD) to
seek a hyper-sphere around the positive data that
encompasses points in the data with the mini-
mum radius. In order to balance between model
over-fitting and under-fitting, Tax and Duin
(2001) proposed a method that tries to use artifi-
cially generated outliers to optimize the model
parameters. However, their experiments suggest
that the procedure to generate artificial outliers in
a hyper-sphere is only feasible for up to 30 di-
mensions. Also, as pointed out by (Khan and
Madden, 2010; 2014), one drawback of their
methods is that they often require a large dataset
and the methods become very inefficient in high
dimensional feature spaces. Since text documents
are usually represented in a much higher dimen-
sional space, these methods are less suitable for
text applications. Manevitz and Yousef (2001)
performed one-class text classification using
one-class SVM as proposed by Sch≈°lkopf et al.
(1999). The method is based on identifying outli-
er data that are representative of the second class.
Instead of assuming the origin is the only mem-
ber of the outlier class, it assumes those data
points with few non-zero entries are also outliers.
However, as reported in the paper, their methods
produce quite weak results (Sch≈°lkopf et al.,
1999; 2000). Li et al. (2003) presented an im-
proved version of one-class SVM for detecting
anomalies. Their idea is to consider all data
points that are close to the origin as outliers.
Both (Yang and Madden, 2007) and (Tian and
Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by
searching optimal parameters. Luo et al., (2007)
proposed a cost-sensitive one-class SVM algo-
rithm for intrusion detection. We will see in the
experiment section that one-class classification is
far inferior to our proposed CBS-L method.
In this work, we propose to represent docu-
ments in the similarity space and thus it is related
to works on document representation. Alternative
document representations have been proposed in
the past and have been shown to perform well in
many applications (Radev et al., 2000; He et al.,
2004; Lebanon 2006; Ranzato and Szummer,
2008, Wang and Domeniconi, 2008). In (Radev
et al., 2000), although the centroid sen-
tence/document vector was computed, it was not
transformed to a similarity space vector represen-
tation. Wang and Domeniconi (2008) proposed
to use external knowledge to build semantic ker-
nels for documents in order to improve text clas-
sification. In our problem, the main difficulty is
that testing negative documents cannot be well
covered in training. It is not clear how the en-
riched document representations could help solve
our problem.
Our work is also related to learning from posi-
tive and unlabeled examples, also known as PU
learning (Denis, 1998; Yu et al. 2002; Liu et al.
2003; Lee and Liu, 2003; Elkan and Noto, 2008;
Li et al. 2010). In this learning model, there is a
set of labeled positive training data and a set of
unlabeled data, but there is no labeled negative
training data. Clearly, their setting is different
from ours too. There is also no guarantee that the
unlabeled data has the same distribution as the
future test data.
Our problem is also very different from do-
main adaption as we work in the same domain.
Due to the use of document similarity, our meth-
od has some resemblance to learning to rank (Li,
2011; Liu, 2011). However, CBS-L is very dif-
ferent because we perform supervised classifica-
tion. Our similarity is also center-based rather
than pair-wise document similarity, which is also
used in (Qian and Liu 2013) for spam detection.
</bodyText>
<sectionHeader confidence="0.992247" genericHeader="method">
3 The Proposed CBS Learning
</sectionHeader>
<bodyText confidence="0.999971142857143">
We now formulate the proposed supervised
learning in the CBS space, called CSB-L. The
key difference between CBS learning and the
classic document space (DS) learning is in the
document representation, which applies to both
training and testing documents or posts. In the
next subsection, we first give the intuitive idea
</bodyText>
<page confidence="0.951595">
2349
</page>
<bodyText confidence="0.99997125">
and a simple example. The detailed algorithm
follows. In Section 3.5, we explain why CBS-L
is better than DS-based learning when unex-
pected negative data appear in the test set.
</bodyText>
<subsectionHeader confidence="0.999423">
3.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.999238857142857">
In the proposed CBS-L formulation, each docu-
ment d is still represented as a feature vector, but
the vector no longer represents the document d
itself based on n-grams. Instead, it represents a
set of similarity values between document d and
the center of the positive documents. Specifically,
the learning consists of the following steps:
</bodyText>
<listItem confidence="0.9846194375">
1. Each document d (in the positive or negative
class) is first represented with a set of docu-
ment representations, i.e., document space
vectors (ds-vectors) based on the document it-
self as in traditional text classification. Each
vector denotes one representation of the doc-
ument. For example, one representation may
be based on only unigrams, and another rep-
resentation may be based on only bigrams.
For simplicity, we use only one representa-
tion/vector x (e.g., unigrams) here to represent
d. Note that we use bold lower case letters to
represent vectors. Each feature in a ds-vector
is called a ds-feature.
2. A center vector c is then computed for each
document representation for the positive class
</listItem>
<bodyText confidence="0.911998780487805">
documents using the ds-vectors of all positive
and negative documents of that representation.
c is thus also a ds-vector.
3. Each document d in the positive and negative
class is then transformed to a center-based
similarity space vector sd (called a cbs-vector).
sd consists of a set of similarity values be-
tween document d‚Äôs set of ds-vectors, i.e., {x}
in our case here (since we use only one repre-
sentation), and the set of corresponding posi-
tive class center vectors, i.e., {c} in our case:
sd =Sim({x}, {c}),
where Sim is a similarity function consisting
of a set of similarity measures. Each feature in
sd is called an cbs-feature. sd still has the same
original class label as d. Let us see an actual
example. We assume that our single center
vector for the positive class has been comput-
ed (see Section 3.2) based on the unigram rep-
resentation of documents:
c: 1:1 2:1 6:2
where y:z represents a ds-feature y (e.g., a
word) and its feature value (e.g., term fre-
quency, tf). We want to transform the follow-
ing positive document d1 and negative docu-
ment d2 (ds-vectors) to their cbs-vectors (the
first number is the class):
d1: 1 1:2 2:1 3:1 d2: -1 2:2 3:1 5:2
If we use cosine as the first similarity measure
in Sim, we can generate a cbs-feature 1:0.50
for d1 (as cosine(c, d1) = 0.50) and a cbs-
feature 1:0.27 for d2 (as cosine(c, d2) = 0.27).
If we have more similarity measures, more
cbs-features will be produced. The resulting
cbs-vectors for d1 and d2 with their class la-
bels, 1 and -1, are:
d1: 1 1:0.50 ... d2: -1 1:0.27 ...
4. We now have a binary classification problem
in the CBS space. This step simply runs a
classification algorithm, e.g., SVM, to build a
classifier. We use SVM in our work.
</bodyText>
<subsectionHeader confidence="0.998941">
3.2 CBS Based Learning
</subsectionHeader>
<bodyText confidence="0.997114878787879">
We are given a binary text classification problem.
Let D = {(d1, y1), (d2, y2), ..., (dn, yn)} be the set
of training examples, where di is a document and
yi ‚àà {1, -1} is its class label. Traditional classi-
fication directly uses D to build a binary classifi-
er. However, in the CBS space, we learn a classi-
fier that returns 1 for documents that are ‚Äúclose
enough‚Äù to the center of the training positive
documents and -1 for documents elsewhere.
We now detail the proposed technique. As we
mentioned above, instead of using one single ds-
vector to represent a document di ‚ààD, we use a
set Rd of p ds-vectors Rd = {ùê±d, ùê±2, ..., ùê±p}}.
Each vector ùê±ÔøΩÔøΩ denotes one document space rep-
resentation of the document, e.g., unigram repre-
sentation. We then compute the center of positive
training documents, which is represented as a set
of ùëù centroids C = {c1, c2, ..., cp}, each of which
corresponds to one document space representa-
tion in Rd. The way to compute each center ci is
similar to that in the Rocchio relevance feedback
method in information retrieval (Rocchio, 1971;
Manning et al. 2008), which uses the correspond-
ing ds-vectors of all training positive and nega-
tive documents. The detail will be given below.
Based on Rd for document d and the center C, we
can transform a document d from its document
space representations Rd to one center-based sim-
ilarity vector cbs-v by applying a similarity func-
tion ùëÜùëñùëö on each element ùê±d of Rd and its corre-
sponding center ci. We now detail document
transformation.
Training document transformation: The train-
</bodyText>
<page confidence="0.919251">
2350
</page>
<bodyText confidence="0.987968954545455">
ing data transformation from ds-vectors to cbs-
vectors performs the following two steps:
Step 1: Compute the set C of centroids for the
positive class. Each centroid vector ci‚ààC is
for one document representation ùê±d. And it is
computed by applying the Rocchio method to
the corresponding ds-vectors of all documents
in both positive and negative training data.
where ùê∑+ is the set of documents in the posi-
tive class and |. |is the size function. ùõº and ùõΩ
are parameters, which are usually set empiri-
cally. It is reported that using tf-idf representa-
tion, ùõº = 16 and ùõΩ = 4 usually work quite
well (Buckley et al. 1994). The subtraction is
used to reduce the influence of those terms
that are not discriminative (i.e., terms appear-
ing in both positive and negative documents).
Step 2: Compute the similarity vector cbs-vd
(center-based similarity space vector) for each
document d ‚ààD based on its set of document
space vectors Rd and the corresponding cen-
troids C of the positive documents.
</bodyText>
<equation confidence="0.837914">
cbs-vd = Sim(Rd, C)
</equation>
<bodyText confidence="0.999161666666667">
Sim has a set of similarity measures, and each
measure mj is applied to p document represen-
tations ùê±d in Rd and their corresponding cen-
ters ùêúi in C to generate p similarity features
(cbs-features) in cbs-vd. We discuss the ds-
features and similarity measures for compu-
ting cbs-features in the next two subsections.
Complexity: The data transformation step is
clearly linear in the number of examples, i.e., n.
Test document transformation: For each test
document d, we can use step 2 above to produce
a cbs-vector for d.
</bodyText>
<subsectionHeader confidence="0.994435">
3.3 DS-Features
</subsectionHeader>
<bodyText confidence="0.999966727272727">
In order to compute cbs-features (center-based
similarity space features) for each document, we
need to have the ds-features of a document and
the center of the positive class. We discuss ds-
features first, which are extracted from each doc-
ument itself.
Since our task is document classification, we
use the popular unigram, bigram and trigram
with tf-idf weighting as the ds-features for a doc-
ument. These three types of ds-features also give
us three different document representations.
</bodyText>
<subsectionHeader confidence="0.978942">
3.4 CBS-Features
</subsectionHeader>
<bodyText confidence="0.999132636363636">
Ds-vectors are transformed into cbs-vectors by
applying a set of similarity measures on each
document space vector and the corresponding
center vector. In this work, we employed five
similarity measures from (Cha, 2007) to gauge
the similarity of two vectors. Based on these
measures, we produce 15 CBS features using the
unigram, bigram, and trigrams representations of
each document. The similarity measures we used
are listed in Table 1, where P and Q are two vec-
tors and d represents the dimension of P and Q.
</bodyText>
<table confidence="0.998144523809524">
d /ÔøΩ
i=1 ùëÉiQi
ùë†Cos =
d 2 d/ÔøΩ 2
i=1 ùëÉi i=1 Qi
ùë†ÔøΩÔøΩÔøΩ = 1 d ùëÉi ùëÑi
1 ‚àí ‚àí
ùëë ÔøΩ=1
d 2 d2
i=1 ùëÉi i=1 ùëÑi
d
ùë†Lor = 1 ‚àí ùëôùëõ 1 + ùëÉi ‚àí ùëÑi
i=1
d
2 i=1 ùëÉiùëÑi
ùë†ince = d 2 d 2
i=1 ùëÉi + i=1 ùëÑi
ùë†lac d
_ i=d 1 ùëÉi ùëÑi
2 d 2_ d
L=1 ùëÉi + i=1 ùëÑi i=1 ùëÉiùëÑi
</table>
<tableCaption confidence="0.999724">
Table 1: similarity measures for CBS-Features
</tableCaption>
<subsectionHeader confidence="0.86753">
3.5 Why Does CBS Space Learning Work?
</subsectionHeader>
<bodyText confidence="0.99998205">
We now try to explain why CBS learning (CBS-
L) can deal with the covariate shift problem, and
thus can perform better than document space
learning. The reason is that due to the use of sim-
ilarity features, CBS-L is essentially trying to
generate a boundary for the positive training data
because similarity is not directional and thus co-
vers all directions in a spherical shape in the
space. In classification, the negative data from
anywhere or direction outside the spherical shape
can be detected. The covariate shift problem will
not affect the classification much. Many types of
documents that are not represented in the nega-
tive training data will still be detected due to
their low similarity. For example, in Figure 1, we
want to build a SVM classifier to separate posi-
tive data represented as black squares and nega-
tive data represented as empty circles. The con-
structed CBS-L classifier would look like a circle
(in dashed line) in the original document space
</bodyText>
<figure confidence="0.994474">
ùêùùê¨ÔøΩd‚ààD+
ùê±d
ùê±d
ùõº
ùê∑+
ùêúi =
ùê±d
ùê±d
|ùê∑ ‚àí ùê∑+|
ùê±d‚ààD-D+
ùõΩ
</figure>
<page confidence="0.990804">
2351
</page>
<bodyText confidence="0.998598777777778">
covering the positive data. The size of this
(boundary) circle depends on the separation mar-
gin between the two classes. Although data
points represented by empty triangles are not
represented in the negative training data (which
has only empty circles) in building the classifier,
our classifier is able to identify them as not posi-
tive at the test time because they are outside the
boundary circle.
</bodyText>
<figureCaption confidence="0.999224">
Figure 1: CBS learning vs. DS learning.
</figureCaption>
<bodyText confidence="0.999971888888889">
If we had used the document space (DS) features
to build a SVM classifier, the classifier would be
a line (see Figure 1) between the positive data
(black squares) and the negative data (empty cir-
cles). This line unfortunately will not be able to
identify data points represented as empty trian-
gles as not positive because the triangles actually
lie on the positive side and would be classified as
positive, which is clearly wrong.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999685333333333">
In this section, we evaluate the proposed learning
in the center-based similarity space (CBS-L) and
compare it with baselines.
</bodyText>
<subsectionHeader confidence="0.966802">
4.1 Experimental Dataset
</subsectionHeader>
<bodyText confidence="0.999981777777778">
As stated at the beginning of the paper, this work
was motivated by the real-life problem of identi-
fying the right social media posts or documents
for specific applications. For an effective evalua-
tion, we need a large number of classes in the
data to reflect the topic richness and diversity of
the social media. The whole data also has to be
labeled for evaluation. Using online reviews of a
large number of products is a natural choice be-
cause there are many types of products and ser-
vices and there is no need to do manual labeling,
which is very labor intensive, time consuming,
and error prone. We obtained the Amazon review
database from the authors of (Jindal and Liu
2008), and constructed a dataset with reviews of
50 types of products, which we also call 50 top-
ics. Each topic (a type of products) have 1000
reviews. For each topic, we randomly sampled
700 reviews/documents for training and the re-
maining 300 reviews for testing. Note that alt-
hough we use this product review collection, we
do not perform sentiment classification. Instead,
we still perform the traditional topic based classi-
fication. That is, given a review, the system de-
cides what type of product the review is about. In
our experiments, we use every topic as the posi-
tive class. This gives us 50 classification results.
</bodyText>
<subsectionHeader confidence="0.98752">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.995716386363636">
We use three baselines in our evaluation.
Document space one-class SVM (ds-osvm): As
we discussed earlier, due to the covariate shift
problem in the negative training data, one solu-
tion is to drop the negative training data com-
pletely to build a one-class classifier. One-class
SVM is the state-of-the-art one-class classifica-
tion algorithm. We apply one-class SVM to the
documents in the document space as one of the
baselines. One-class SVM was first introduced
by Sch≈°lkopf et al. (1999; 2000), which is based
on the assumption that the origin is the only
member of the second class. The data is first
mapped into a transformed feature space via a
kernel and then standard two-class SVM is em-
ployed to construct a hyper-plane that separates
the data and the original with maximum margin.
As mentioned earlier, there is also the support
vector data description (SVDD) formulation for
one-class classification proposed by Tax and
Duin (1999a; 1999b). SVDD seeks to distinguish
the positive class from all other possible data in
space. It basically finds a hyper-sphere around
the positive class data that contains almost all
points in the data set with the minimum radius. It
has been shown that the use of Gaussian kernel
makes SVDD and One-class SVM equivalent,
and the results reported in (Khan and Madden,
2014) demonstrate that SVDD and One-class
SVM are comparable when the Gaussian kernel
is applied. Thus in this paper, we just use one-
class SVM, which is one of the SVM-based clas-
sification tools in the LIBSVM1 library (version
3.20) (Chang and Lin, 2011).
Center-based similarity space one-class SVM
(cbs-osvm): Instead of applying one-class SVM
to documents in the original document space, this
baseline applies it to the CBS space after the
documents are transformed to CBS vectors.
SVM: This baseline is the SVM applied in the
original document space. Although in this case,
there is covariate shift problem, we want to see
how serious the problem might be, and how the
proposed CBS-L technique can deal with the
</bodyText>
<footnote confidence="0.96629">
1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
</footnote>
<page confidence="0.957756">
2352
</page>
<table confidence="0.999870454545454">
In-training Out-of-training Combined
precision recall F1-score precision recall F1-score precision recall F1-score
10 topics are used in the training negative class
ds-osvm 0.154 0.498 0.205
cbs-osvm 0.664 0.453 0.514 0.357 0.442 0.339 0.343 0.452 0.330
SVM 0.678 0.811 0.736 0.176 0.803 0.282 0.160 0.819 0.262
CBS-L 0.796 0.766 0.776 0.384 0.768 0.491 0.368 0.754 0.481
20 topics are used in the training negative class
ds-osvm 0.154 0.498 0.205
cbs-osvm 0.561 0.477 0.466 0.430 0.445 0.390 0.364 0.457 0.344
SVM 0.566 0.753 0.643 0.304 0.753 0.422 0.254 0.758 0.371
CBS-L 0.761 0.700 0.723 0.557 0.702 0.608 0.485 0.693 0.558
30 topics are used in the training negative class
ds-osvm 0.154 0.498 0.205
cbs-osvm 0.451 0.491 0.393 0.488 0.524 0.407 0.378 0.487 0.355
SVM 0.508 0.721 0.591 0.450 0.722 0.547 0.323 0.726 0.439
CBS-L 0.723 0.650 0.678 0.721 0.644 0.667 0.569 0.649 0.598
40 topics are used in the training negative class
ds-osvm 0.154 0.498 0.205
cbs-osvm 0.423 0.482 0.379 0.590 0.511 0.444 0.372 0.486 0.347
SVM 0.456 0.689 0.544 0.641 0.685 0.658 0.374 0.695 0.481
CBS-L 0.697 0.613 0.644 0.848 0.616 0.699 0.639 0.613 0.619
</table>
<tableCaption confidence="0.998332">
Table 2: Summary results of the 50 topics
</tableCaption>
<bodyText confidence="0.944828">
problem. We use the SVM tool in LIBSVM.
</bodyText>
<subsectionHeader confidence="0.991172">
4.3 Kernels and Parameters
</subsectionHeader>
<bodyText confidence="0.999965365853659">
As Khan and Madden (2014) pointed out that
one-class SVM performs the best when Gaussian
kernel is used, we use Gaussian kernel as well.
Manevitz and Yousef (2001) applied one-class
SVM to text classification, and the authors re-
ported that one-class SVM works the best with
binary feature weighting scheme compared to tf
or tf-idf weighting schemes. Also, they reported
that a small number of features (10) with highest
document frequency performed the best with
Gaussian kernel. We also use binary representa-
tion, but found that 10 features are already too
many in our case. In fact, 5 features give the best
results. Using a small number of features is intui-
tive because to find the boundary of a very high
dimensional space is very difficult. We also tried
more features but they were poorer.
For SVM classification in the document space,
we use the linear kernel as it has been shown by
many researchers that the linear kernel performs
the best (e.g., Joachims, 1998; Colas and Brazdil,
2006). We experimented with RBF kernels ex-
tensively, but they did not perform well with the
traditional document representation. The term
weighting scheme is tf-idf (Colas and Brazdil,
2006) with no feature selection.
For our proposed method CBS-L, we use tf-idf
values of unigram, bigram and trigram to repre-
sent a document in three ways in the document
space. As mentioned earlier, five document simi-
larity functions are used to transform document
space vectors to CBS space vectors. And in order
to filter out less useful features for the center
vector of the positive class, we performed feature
selection in the document space using the classic
information gain method (Yang and Pedersen,
1997) to empirically choose the most effective
100 features for the positive class.
For all the kernels, we use the default parame-
ter settings in the LIBSVM systems. We tried to
tune the parameters, but did not get better results.
</bodyText>
<subsectionHeader confidence="0.872589">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999959615384615">
We now present the experiment results. As men-
tioned above, we treat each topic as the positive
class. This gives 50 tests. To test the effect of
covariate shift, we also vary the number of topics
in the negative class. We used 10, 20, 30, and 40
topics in the training negative class. The test set
always has 49 topics of negative data.
For each setting, we give three sets of results
for the positive class, which is the target topic
data that we are interested in obtaining through
classification. Each set of results includes the
standard measures of precision, recall, and F1-
score for the positive class. The three sets are:
</bodyText>
<listItem confidence="0.909221">
1. In-training: In this case, the test negative data
</listItem>
<page confidence="0.971645">
2353
</page>
<bodyText confidence="0.9990215">
contains only data from those topics used in
training. This is the classical supervised learn-
ing setting where the training and test data are
randomly drawn from the same distribution.
</bodyText>
<listItem confidence="0.97401375">
2. Not-in-training: In this case, the test negative
set contains only data from the other topics
not used in training. The classical setting of
supervised learning does not deal with this
problem. This represents covariate shift.
3. Combined: In this case, the test data contains
both in-training and not-in-training negative
topics. Due to the use of not-in-training test
</listItem>
<bodyText confidence="0.980801071428571">
data, this is also not the classical setting.
Due to a large number of experiment results, we
cannot report all the details. Table 2 summarizes
the results. Notice that for ds-osvm, it does not
make sense to have in-training and not-in-
training results because it does not use any train-
ing negative data. Thus, there is only one set of
results for ‚ÄúCombined,‚Äù which is duplicated in
the table for easy comparison. However, note
that cbs-osvm uses negative data for training in
order to compute the center for the positive class.
From the table, we can make the following
observations (since there are many numbers, we
only focus on F1-scores).
</bodyText>
<listItem confidence="0.9943745">
1. The proposed CBS-L method performs mark-
edly better than all baselines. For the results
</listItem>
<bodyText confidence="0.9938484">
of in-training, not-in-training, and combined,
CBS-L is consistently better in all cases than
all baselines. Even for in-training, CBS-L per-
form better than SVM. This clearly shows the
superiority of the proposed CBS-L method.
</bodyText>
<listItem confidence="0.9558392">
2. ds-osvm performs poorly. cbs-osvm is much
better because it uses the negative data in fea-
ture selection and center computation.
3. SVM in the document space performed poorly
(Combined) when only a small number of
</listItem>
<bodyText confidence="0.945246941176471">
negative topics are used in training. It gets
better than both one-class SVM baselines
when more negative topics are used in train-
ing (see the reason in the next point).
4. Finally, we can also see that with the number
of training negative topics increases, the re-
sults of the combined case of both SVM and
CBS-L improve. This is expected because
with the increased number of negative topics
for training, the number of not-in-training
negative topics for testing decreases and the
covariate shift problem gets smaller. We can
also see that cbs-osvm, SVM and CBS-L‚Äôs
F1-scores for not-in-training improve with the
increased training negative topics due to the
same reason. However, their F1-scores drop
for in-training because with more negative
</bodyText>
<table confidence="0.999945637931035">
topic ds-osvm cbs-osvm SVM CBS-L
Amplifier 0.125 0.360 0.406 0.597
Automotive 0.041 0.031 0.240 0.383
Battery 0.266 0.425 0.433 0.656
Beauty 0.079 0.401 0.470 0.618
Cable 0.131 0.028 0.231 0.500
Camera 0.376 0.361 0.433 0.523
CD Player 0.154 0.274 0.344 0.585
Clothing 0.046 0.234 0.292 0.486
Computer 0.117 0.225 0.328 0.455
Conditioner 0.075 0.195 0.381 0.519
Fan 0.408 0.581 0.581 0.724
Flashlight 0.273 0.487 0.528 0.744
Graphics Card 0.419 0.473 0.552 0.631
Headphone 0.298 0.338 0.432 0.533
Home 0.039 0.032 0.178 0.233
Improvement
Jewelry 0.362 0.579 0.632 0.800
Kindle 0.107 0.387 0.416 0.685
Kitchen 0.042 0.118 0.197 0.261
Lamp 0.091 0.249 0.374 0.487
Luggage 0.105 0.482 0.506 0.482
Magazine 0.406 0.597 0.796 0.858
Subscriptions
Mattress 0.435 0.562 0.603 0.702
Memory Card 0.134 0.256 0.367 0.508
Microphone 0.103 0.223 0.25 0.417
Microwave 0.378 0.577 0.637 0.735
Monitor 0.136 0.345 0.312 0.513
Mouse 0.493 0.580 0.552 0.779
Movies TV 0.146 0.507 0.641 0.682
Musical 0.073 0.241 0.446 0.575
Instruments
Network 0.164 0.483 0.481 0.596
Adapter
Office Products 0.040 0.193 0.327 0.346
Patio Lawn 0.043 0.226 0.295 0.483
Garden
Pet Supplies 0.098 0.447 0.524 0.584
Pillow 0.491 0.640 0.781 0.888
Printer 0.549 0.557 0.624 0.859
Projector 0.230 0.459 0.482 0.805
Rice Cooker 0.571 0.616 0.692 0.942
Shoes 0.224 0.524 0.585 0.793
Speaker 0.241 0.251 0.253 0.410
Subwoofer 0.147 0.268 0.346 0.401
Table Chair 0.141 0.496 0.571 0.703
Tablet 0.069 0.234 0.142 0.424
Telephone 0.099 0.034 0.144 0.167
Tent 0.289 0.465 0.428 0.764
Toys 0.088 0.029 0.331 0.449
Video Games 0.424 0.387 0.508 0.705
Vitamin 0.052 0.026 0.341 0.527
Supplement
Wall Clock 0.401 0.582 0.607 0.777
Watch 0.362 0.553 0.543 0.775
Webcam 0.155 0.304 0.372 0.645
Average 0.205 0.355 0.439 0.598
</table>
<tableCaption confidence="0.969341">
Table 3: F1-score for each positive topic or class
in the combined case
</tableCaption>
<page confidence="0.987976">
2354
</page>
<bodyText confidence="0.999890285714286">
topics, the data becomes more skewed, which
hurts in-training classification.
To give a flavor of the detailed results for each
topic (product), we give the full results for one
setting with 30 randomly selected topics as the
training negative data (Table 3). The results in
the table are F1-scores of the combined case.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999527">
The ability to get relevant posts accurately about
a topic from social media is a challenging prob-
lem. This paper attempted to solve this problem
by identifying and dealing with the technical is-
sue of covariate shift. The key idea of our tech-
nique is to transform document representation
from the traditional n-gram feature space to a
similarity based space. Our experimental results
show that the proposed method CBS-L outper-
formed strong baselines by large margins.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99958">
This research was partially supported by the NSF
grants IIS-1111092 and IIS-1407927, and a
Google faculty award.
</bodyText>
<sectionHeader confidence="0.983342" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.997656293333333">
Alter, O., Brown, P.O. and Bostein, D. 2000.
Singular Value Decomposition for Genome-
Wide Expression Data Processing and Model-
ing. Proc. Nat&apos;,l Academy of Science, vol. 97,
no. 18, pp. 10101-10106, Aug.
Blei, D. Ng, A. and Jordan, M., 2003. Latent di-
richlet allocation, The Journal of Machine
Learning Research, 3, p.993-1022, 3/1/2003
Buckley, C., Salton, G., Allan, J. 1994. The Ef-
fect of Adding Relevance Information in a
Relevance Feedback Environment, Proceed-
ings of SIGIR Conference.
Bickel, S., Bruckner, M., and Scheffer. 2009. T.
Discriminative learning under covariate shift.
Journal of Machine Learning Research.
Bickel S. and Scheffer T. 2007. Dirichlet-
enhanced spam filtering based on biased
samples. Advances in Neural Information
Processing Systems.
Cha, S.-H. 2007. Comprehensive Survey on Dis-
tance/Similarity Measures between Probabil-
ity Density Functions. International Journal
of Mathematical Models and Methods in Ap-
plied Sciences, 1(4):300--307.
Chang, C-C. and Lin, C-J. 2011. LIBSVM: a
library for support vector machines. ACM
Transactions on Intelligent Systems and
Technology, 2:27:1--27:27, http://www.csie.
ntu.edu. tw/~cjlin/libsvm
Colas, F. and Brazdil. P. 2006. Comparison of
SVM and some older classification algorithms
in text classification tasks. Artificial Intelli-
gence in Theory and Practice. IFIP Interna-
tional Federation for Information Processing,
pp. 169-178.
Denis, F., PAC learning from positive statistical
queries. ALT, 1998.
Radev, D., Jing, H. and Budzikowska, M. 2000.
Centroid-based summarization of multiple
documents: Sentence extraction, utility-based
evaluation, and user studies. In ANLP/NAACL
Workshop on Summarization, Seattle, April.
Dudik, M., Schapire, R., and Phillips, S. 2005.
Correcting sample selection bias in maximum
entropy density estimation. Advances in
Neural Information Processing Systems.
Elkan, C. and Noto, K. 2008. Learning classifiers
from only positive and unlabeled data. KDD,
213-220.
He, X., Cai, D., Liu, H. and Ma, W.-Y. 2004.
Locality Preserving Indexing for Document
Representation. Proc. Of SIGIR.
Heckman, J. 1979. Sample selection bias as a
specification error. Econometrica, 47:153‚Äì
161.
Huang, J., Smola, A. and Gretton, A., Borgwardt
K., and Scholkopf B. 2007. Correcting sample
selection bias by unlabeled data. Advances in
Neural Information Processing Systems.
Joachims, T. 1998. Text categorization with
support vector machines: Learning with many
relevant features. ECML.
Jindal, N. and Liu, B. 2008. Opinion Spam and
Analysis. Proceedings of the ACM Confer-
ence on Web Search and Data Mining.
Khan, S., and Madden, M. 2010. A survey of
recent trends in one class classification. Artifi-
cial Intelligence and Cognitive Science, vol-
ume 6206 of Lecture Notes in Computer Sci-
ence. 188‚Äì197.
Khan, S. and Madden, M. 2014. One-Class Clas-
sification: Taxonomy of Study and Review of
Techniques. The Knowledge Engineering Re-
view, 1-30.
Lebanon, G. 2006. Sequential document repre-
</reference>
<page confidence="0.749342">
2355
</page>
<reference confidence="0.992643216494845">
sentations and simplicial curves. UAI.
Lee, W. S. and Liu, B. 2003. Learning with Posi-
tive and Unlabeled Examples Using Weighted
Logistic Regression. ICML.
Li, H. 2011. Learning to Rank for Information
Retrieval and Natural Language Processing.
Morgan &amp; Claypool publishers.
Li, K., Huang, H., Tian, S. and Xu, W. 2003.
Improving One-class SVM for anomaly detec-
tion. Proc. of the Second International confer-
ence on Machine Learning and Cybernetics,
volume 5, pages 3077‚Äì3081.
Li, X., Liu, B. and Ng. S.-K. 2010. Negative
Training Data can be Harmful to Text Classi-
fication. EMNLP.
Liu, B, Dai, Y., Li, X., Lee, W-S. and Yu. P.
2003. Building text classifiers using positive
and unlabeled examples. ICDM.
Liu. T. 2011. Learning to Rank for Information
Retrieval. Springer.
Luo, J., Ding, L., Pan, Z., Ni, G. and Hu, G.
2007. Research on cost-sensitive learning in
one-class anomaly detection algorithms. Auto-
nomic and Trusted Computing, volume 4610
of Lecture Notes in Computer Science.
Manevitz, L. and Yousef. M. 2001. One-class
SVMs for document classification. Journal of
Machine Learning research.
Manning, C. D., Prabhakar R., and Hinrich, S.
2008. Introduction to Information Retrieval.
Cambridge University Press.
Qian, T. and Liu, B. 2013. Identifying Multiple
Userids of the Same Author. EMNLP.
Ranzato, M. and Szummer, M. 2008. Semi-
supervised learning of compact document rep-
resentations with deep networks. ICML.
Rocchio, J. 1971. Relevant feedback in infor-
mation retrieval. In G. Salton (ed.). The smart
retrieval system: experiments in automatic
document processing.
Sch≈°lkopf, B., Williamson, R., Smola, A., Tay-
lor, J. and Platt, J. 2000. Support vector meth-
od for novelty detection. Neural Information
Processing Systems, pages 582‚Äì588.
Sch≈°lkopf, B., Platt, J., Shawe-Taylor, J., Smola,
A. and Williamson, R. 1999. Estimating the
support of a high-dimensional distribution.
Technical Report, Microsoft Research, MSR-
TR-99-87.
Shimodaira, H. 2000. Improving predictive
inference under covariate shift by weighting
the log-likelihood function. Journal of
Statistical Planning and Inference, 90:227‚Äì
244.
Sugiyama, M. and Muller, K.-R. 2005. Input-
dependent estimation of generalization error
under covariate shift. Statistics and Decision,
23(4):249‚Äì279.
Sugiyama, M., Nakajima, S., Kashima, H., von
Bunau P., and Kawanabe M. 2008. Direct
importance estimation with model selection
and its application to covariate shift
adaptation. Advances in Neural Information
Processing Systems.
Tax, D. and Duin, R. 1999a. Data domain de-
scription using support vectors. Proceedings
ESAN99, Brussels. 251-256
Tax, D. and Duin, R. 1999b. Support vector do-
main description. Pattern Recognition Letters
20. 1191-1199
Tax, D. and Duin, R. 2001. Uniform object gen-
eration for optimizing one-class classifiers. J.
of Machine Learning Research, 2:155‚Äì173.
Tian, J. and Gu, H. 2010. Anomaly detection
combining one-class SVMs and particle
swarm optimization algorithms. Nonlinear
Dynamics, 61(1-2): 303‚Äì310.
Tsuboi, J., Kashima, H., Hido, S., Bickel, S., and
Sugiyama, M. 2008. Direct density ratio
estimation for large-scale covariate shift
adaptation. Proceedings of the SIAM
International Conference on Data Mining
(SDM).
Wang, P. and Domeniconi, C. 2008. Building
semantic kernels for text classification using
Wikipedia, KDD.
Yang, Y. and Pedersen, J. O. 1997. A compara-
tive study on feature selection in text catego-
rization. ICML.
Yang, L., and Madden, M. 2007. One-class sup-
port vector machine calibration using particle
swarm optimization. AICS, Dublin.
Yu, H., Han, J. and Chang, K. 2002. PEBL: Posi-
tive example based learning for Web page
classification using SVM. KDD, 239-248.
Zadrozny, B. 2004. Learning and evaluating
classifiers under s ample selection bias, ICML.
</reference>
<page confidence="0.989459">
2356
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838606">
<title confidence="0.994851">Social Media Text Classification under Negative Covariate Shift</title>
<author confidence="0.986916">Geli Fei</author>
<author confidence="0.986916">Bing</author>
<affiliation confidence="0.995271">University of Illinois at</affiliation>
<address confidence="0.860139">Chicago, IL 60607,</address>
<email confidence="0.999481">gfei2@uic.edu,liub@cs.uic.edu</email>
<abstract confidence="0.999678192307692">In a typical social media content analysis task, the user is interested in analyzing posts of a particular topic. Identifying such posts is often formulated as a classification problem. However, this problem challenging. One key issue is That is, the training data is not fully representative of the test data. We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics. This paper proposes a novel technique to solve the problem. The key novelty of the technique is the transformation of document from the traditional nfeature to a space. In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers. Experiment results show that the proposed approach markedly improves classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Alter</author>
<author>P O Brown</author>
<author>D Bostein</author>
</authors>
<title>Singular Value Decomposition for GenomeWide Expression Data Processing and Modeling.</title>
<date>2000</date>
<booktitle>Proc. Nat&apos;,l Academy of Science,</booktitle>
<volume>97</volume>
<pages>10101--10106</pages>
<contexts>
<context position="7027" citStr="Alter et al., 2000" startWordPosition="1162" endWordPosition="1165">available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue. One obvious solution to this problem is oneclass classification (Sch≈°lkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM. We simply discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy. In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a similarity space. Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS). In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers. The reason for this is that in CBS based learning the vectors in the similarity space enable </context>
</contexts>
<marker>Alter, Brown, Bostein, 2000</marker>
<rawString>Alter, O., Brown, P.O. and Bostein, D. 2000. Singular Value Decomposition for GenomeWide Expression Data Processing and Modeling. Proc. Nat&apos;,l Academy of Science, vol. 97, no. 18, pp. 10101-10106, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ng Blei</author>
<author>A</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation,</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="7054" citStr="Blei et al., 2003" startWordPosition="1168" endWordPosition="1171">e to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue. One obvious solution to this problem is oneclass classification (Sch≈°lkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM. We simply discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy. In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a similarity space. Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS). In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers. The reason for this is that in CBS based learning the vectors in the similarity space enable SVM (which is the learning </context>
</contexts>
<marker>Blei, A, Jordan, 2003</marker>
<rawString>Blei, D. Ng, A. and Jordan, M., 2003. Latent dirichlet allocation, The Journal of Machine Learning Research, 3, p.993-1022, 3/1/2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>G Salton</author>
<author>J Allan</author>
</authors>
<title>The Effect of Adding Relevance Information in a Relevance Feedback Environment,</title>
<date>1994</date>
<booktitle>Proceedings of SIGIR Conference.</booktitle>
<contexts>
<context position="19377" citStr="Buckley et al. 1994" startWordPosition="3245" endWordPosition="3248">2350 ing data transformation from ds-vectors to cbsvectors performs the following two steps: Step 1: Compute the set C of centroids for the positive class. Each centroid vector ci‚ààC is for one document representation ùê±d. And it is computed by applying the Rocchio method to the corresponding ds-vectors of all documents in both positive and negative training data. where ùê∑+ is the set of documents in the positive class and |. |is the size function. ùõº and ùõΩ are parameters, which are usually set empirically. It is reported that using tf-idf representation, ùõº = 16 and ùõΩ = 4 usually work quite well (Buckley et al. 1994). The subtraction is used to reduce the influence of those terms that are not discriminative (i.e., terms appearing in both positive and negative documents). Step 2: Compute the similarity vector cbs-vd (center-based similarity space vector) for each document d ‚ààD based on its set of document space vectors Rd and the corresponding centroids C of the positive documents. cbs-vd = Sim(Rd, C) Sim has a set of similarity measures, and each measure mj is applied to p document representations ùê±d in Rd and their corresponding centers ùêúi in C to generate p similarity features (cbs-features) in cbs-vd. </context>
</contexts>
<marker>Buckley, Salton, Allan, 1994</marker>
<rawString>Buckley, C., Salton, G., Allan, J. 1994. The Effect of Adding Relevance Information in a Relevance Feedback Environment, Proceedings of SIGIR Conference.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T</author>
</authors>
<title>Discriminative learning under covariate shift.</title>
<journal>Journal of Machine Learning Research.</journal>
<marker>T, </marker>
<rawString>Bickel, S., Bruckner, M., and Scheffer. 2009. T. Discriminative learning under covariate shift. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bickel</author>
<author>T Scheffer</author>
</authors>
<title>Dirichletenhanced spam filtering based on biased samples.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="9552" citStr="Bickel and Scheffer (2007)" startWordPosition="1573" endWordPosition="1576">onometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class cla</context>
</contexts>
<marker>Bickel, Scheffer, 2007</marker>
<rawString>Bickel S. and Scheffer T. 2007. Dirichletenhanced spam filtering based on biased samples. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-H Cha</author>
</authors>
<title>Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions.</title>
<date>2007</date>
<booktitle>International Journal of Mathematical Models and Methods in Applied Sciences,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="21029" citStr="Cha, 2007" startWordPosition="3514" endWordPosition="3515">s-features of a document and the center of the positive class. We discuss dsfeatures first, which are extracted from each document itself. Since our task is document classification, we use the popular unigram, bigram and trigram with tf-idf weighting as the ds-features for a document. These three types of ds-features also give us three different document representations. 3.4 CBS-Features Ds-vectors are transformed into cbs-vectors by applying a set of similarity measures on each document space vector and the corresponding center vector. In this work, we employed five similarity measures from (Cha, 2007) to gauge the similarity of two vectors. Based on these measures, we produce 15 CBS features using the unigram, bigram, and trigrams representations of each document. The similarity measures we used are listed in Table 1, where P and Q are two vectors and d represents the dimension of P and Q. d /ÔøΩ i=1 ùëÉiQi ùë†Cos = d 2 d/ÔøΩ 2 i=1 ùëÉi i=1 Qi ùë†ÔøΩÔøΩÔøΩ = 1 d ùëÉi ùëÑi 1 ‚àí ‚àí ùëë ÔøΩ=1 d 2 d2 i=1 ùëÉi i=1 ùëÑi d ùë†Lor = 1 ‚àí ùëôùëõ 1 + ùëÉi ‚àí ùëÑi i=1 d 2 i=1 ùëÉiùëÑi ùë†ince = d 2 d 2 i=1 ùëÉi + i=1 ùëÑi ùë†lac d _ i=d 1 ùëÉi ùëÑi 2 d 2_ d L=1 ùëÉi + i=1 ùëÑi i=1 ùëÉiùëÑi Table 1: similarity measures for CBS-Features 3.5 Why Does CBS Space Learning </context>
</contexts>
<marker>Cha, 2007</marker>
<rawString>Cha, S.-H. 2007. Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions. International Journal of Mathematical Models and Methods in Applied Sciences, 1(4):300--307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<note>http://www.csie. ntu.edu. tw/~cjlin/libsvm</note>
<contexts>
<context position="26578" citStr="Chang and Lin, 2011" startWordPosition="4496" endWordPosition="4499">999b). SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius. It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied. Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.20) (Chang and Lin, 2011). Center-based similarity space one-class SVM (cbs-osvm): Instead of applying one-class SVM to documents in the original document space, this baseline applies it to the CBS space after the documents are transformed to CBS vectors. SVM: This baseline is the SVM applied in the original document space. Although in this case, there is covariate shift problem, we want to see how serious the problem might be, and how the proposed CBS-L technique can deal with the 1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ 2352 In-training Out-of-training Combined precision recall F1-score precision recall F1-s</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chang, C-C. and Lin, C-J. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, http://www.csie. ntu.edu. tw/~cjlin/libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>Comparison of SVM and some older classification algorithms in text classification tasks.</title>
<date>2006</date>
<booktitle>Artificial Intelligence in Theory and Practice. IFIP International Federation for Information Processing,</booktitle>
<pages>169--178</pages>
<marker>P, 2006</marker>
<rawString>Colas, F. and Brazdil. P. 2006. Comparison of SVM and some older classification algorithms in text classification tasks. Artificial Intelligence in Theory and Practice. IFIP International Federation for Information Processing, pp. 169-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Denis</author>
</authors>
<title>PAC learning from positive statistical queries.</title>
<date>1998</date>
<publisher>ALT,</publisher>
<contexts>
<context position="13060" citStr="Denis, 1998" startWordPosition="2141" endWordPosition="2142">008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010). In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). How</context>
</contexts>
<marker>Denis, 1998</marker>
<rawString>Denis, F., PAC learning from positive statistical queries. ALT, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Jing</author>
<author>M Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In ANLP/NAACL Workshop on Summarization,</booktitle>
<location>Seattle,</location>
<contexts>
<context position="12367" citStr="Radev et al., 2000" startWordPosition="2026" endWordPosition="2029">as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also rel</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Radev, D., Jing, H. and Budzikowska, M. 2000. Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies. In ANLP/NAACL Workshop on Summarization, Seattle, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dudik</author>
<author>R Schapire</author>
<author>S Phillips</author>
</authors>
<title>Correcting sample selection bias in maximum entropy density estimation.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="9521" citStr="Dudik et al. (2005)" startWordPosition="1568" endWordPosition="1571">s first introduced in econometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in </context>
</contexts>
<marker>Dudik, Schapire, Phillips, 2005</marker>
<rawString>Dudik, M., Schapire, R., and Phillips, S. 2005. Correcting sample selection bias in maximum entropy density estimation. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Elkan</author>
<author>K Noto</author>
</authors>
<title>Learning classifiers from only positive and unlabeled data.</title>
<date>2008</date>
<journal>KDD,</journal>
<pages>213--220</pages>
<contexts>
<context position="13134" citStr="Elkan and Noto, 2008" startWordPosition="2155" endWordPosition="2158">ument vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010). In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is very different because we perform supervised classification</context>
</contexts>
<marker>Elkan, Noto, 2008</marker>
<rawString>Elkan, C. and Noto, K. 2008. Learning classifiers from only positive and unlabeled data. KDD, 213-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>D Cai</author>
<author>H Liu</author>
<author>W-Y Ma</author>
</authors>
<title>Locality Preserving Indexing for Document Representation.</title>
<date>2004</date>
<booktitle>Proc. Of SIGIR.</booktitle>
<contexts>
<context position="12384" citStr="He et al., 2004" startWordPosition="2030" endWordPosition="2033">ang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning </context>
</contexts>
<marker>He, Cai, Liu, Ma, 2004</marker>
<rawString>He, X., Cai, D., Liu, H. and Ma, W.-Y. 2004. Locality Preserving Indexing for Document Representation. Proc. Of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Heckman</author>
</authors>
<title>Sample selection bias as a specification error.</title>
<date>1979</date>
<journal>Econometrica,</journal>
<volume>47</volume>
<pages>161</pages>
<contexts>
<context position="3722" citStr="Heckman 1979" startWordPosition="610" endWordPosition="611">raining data has no negative examples about sports. However, in testing, some sports posts show up. These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy. In this paper, we aim to solve this problem. In machine learning, this problem is called covariate shift, a type of sample selection bias. In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution. However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009). In general, the sample selection bias problem is not solvable because the two 2347 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347‚Äì2356, Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics. distributions can be arbitrarily far apart from each other. Various assumptions were made to solve special cases of the problem. One main assumption was that the conditional distribution of the class given a data instance is t</context>
<context position="8954" citStr="Heckman (1979)" startWordPosition="1477" endWordPosition="1478">s able to find a good similarity boundary of the positive data. Third, it experimentally demonstrates the effectiveness of the proposed method. 2 Related Work Traditional supervised learning assumes that the training and test examples are drawn from the same distribution. However, this assumption can be violated in many applications. This is especially the case for social media data because of the high topic diversity and constant changes of topics. This problem is known as covariate shift, which is a form of sample selection bias. Sample selection bias was first introduced in econometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) u</context>
</contexts>
<marker>Heckman, 1979</marker>
<rawString>Heckman, J. 1979. Sample selection bias as a specification error. Econometrica, 47:153‚Äì 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>A Smola</author>
<author>A Gretton</author>
<author>K Borgwardt</author>
<author>B Scholkopf</author>
</authors>
<title>Correcting sample selection bias by unlabeled data.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="3773" citStr="Huang et al. 2007" startWordPosition="616" endWordPosition="619">orts. However, in testing, some sports posts show up. These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy. In this paper, we aim to solve this problem. In machine learning, this problem is called covariate shift, a type of sample selection bias. In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution. However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009). In general, the sample selection bias problem is not solvable because the two 2347 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347‚Äì2356, Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics. distributions can be arbitrarily far apart from each other. Various assumptions were made to solve special cases of the problem. One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets (Shimoda</context>
<context position="9619" citStr="Huang et al. (2007)" startWordPosition="1584" endWordPosition="1587">hrough the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. Tax and Du</context>
</contexts>
<marker>Huang, Smola, Gretton, Borgwardt, Scholkopf, 2007</marker>
<rawString>Huang, J., Smola, A. and Gretton, A., Borgwardt K., and Scholkopf B. 2007. Correcting sample selection bias by unlabeled data. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<publisher>ECML.</publisher>
<contexts>
<context position="29316" citStr="Joachims, 1998" startWordPosition="4945" endWordPosition="4946">they reported that a small number of features (10) with highest document frequency performed the best with Gaussian kernel. We also use binary representation, but found that 10 features are already too many in our case. In fact, 5 features give the best results. Using a small number of features is intuitive because to find the boundary of a very high dimensional space is very difficult. We also tried more features but they were poorer. For SVM classification in the document space, we use the linear kernel as it has been shown by many researchers that the linear kernel performs the best (e.g., Joachims, 1998; Colas and Brazdil, 2006). We experimented with RBF kernels extensively, but they did not perform well with the traditional document representation. The term weighting scheme is tf-idf (Colas and Brazdil, 2006) with no feature selection. For our proposed method CBS-L, we use tf-idf values of unigram, bigram and trigram to represent a document in three ways in the document space. As mentioned earlier, five document similarity functions are used to transform document space vectors to CBS space vectors. And in order to filter out less useful features for the center vector of the positive class, </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, T. 1998. Text categorization with support vector machines: Learning with many relevant features. ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion Spam and Analysis.</title>
<date>2008</date>
<booktitle>Proceedings of the ACM Conference on Web Search and Data Mining.</booktitle>
<contexts>
<context position="24377" citStr="Jindal and Liu 2008" startWordPosition="4128" endWordPosition="4131">ted by the real-life problem of identifying the right social media posts or documents for specific applications. For an effective evaluation, we need a large number of classes in the data to reflect the topic richness and diversity of the social media. The whole data also has to be labeled for evaluation. Using online reviews of a large number of products is a natural choice because there are many types of products and services and there is no need to do manual labeling, which is very labor intensive, time consuming, and error prone. We obtained the Amazon review database from the authors of (Jindal and Liu 2008), and constructed a dataset with reviews of 50 types of products, which we also call 50 topics. Each topic (a type of products) have 1000 reviews. For each topic, we randomly sampled 700 reviews/documents for training and the remaining 300 reviews for testing. Note that although we use this product review collection, we do not perform sentiment classification. Instead, we still perform the traditional topic based classification. That is, given a review, the system decides what type of product the review is about. In our experiments, we use every topic as the positive class. This gives us 50 cl</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Jindal, N. and Liu, B. 2008. Opinion Spam and Analysis. Proceedings of the ACM Conference on Web Search and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khan</author>
<author>M Madden</author>
</authors>
<title>A survey of recent trends in one class classification.</title>
<date>2010</date>
<journal>Artificial Intelligence and Cognitive Science,</journal>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>6206</volume>
<pages>188--197</pages>
<contexts>
<context position="10821" citStr="Khan and Madden, 2010" startWordPosition="1775" endWordPosition="1778">problem. Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius. In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions. Also, as pointed out by (Khan and Madden, 2010; 2014), one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces. Since text documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications. Manevitz and Yousef (2001) performed one-class text classification using one-class SVM as proposed by Sch≈°lkopf et al. (1999). The method is based on identifying outlier data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes tho</context>
</contexts>
<marker>Khan, Madden, 2010</marker>
<rawString>Khan, S., and Madden, M. 2010. A survey of recent trends in one class classification. Artificial Intelligence and Cognitive Science, volume 6206 of Lecture Notes in Computer Science. 188‚Äì197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khan</author>
<author>M Madden</author>
</authors>
<title>One-Class Classification: Taxonomy of Study and Review of Techniques. The Knowledge Engineering Review,</title>
<date>2014</date>
<pages>1--30</pages>
<contexts>
<context position="26329" citStr="Khan and Madden, 2014" startWordPosition="4453" endWordPosition="4456">mployed to construct a hyper-plane that separates the data and the original with maximum margin. As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b). SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius. It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied. Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.20) (Chang and Lin, 2011). Center-based similarity space one-class SVM (cbs-osvm): Instead of applying one-class SVM to documents in the original document space, this baseline applies it to the CBS space after the documents are transformed to CBS vectors. SVM: This baseline is the SVM applied in the original document space. Although in this case, there is covariate shift pr</context>
<context position="28371" citStr="Khan and Madden (2014)" startWordPosition="4783" endWordPosition="4786">training negative class ds-osvm 0.154 0.498 0.205 cbs-osvm 0.451 0.491 0.393 0.488 0.524 0.407 0.378 0.487 0.355 SVM 0.508 0.721 0.591 0.450 0.722 0.547 0.323 0.726 0.439 CBS-L 0.723 0.650 0.678 0.721 0.644 0.667 0.569 0.649 0.598 40 topics are used in the training negative class ds-osvm 0.154 0.498 0.205 cbs-osvm 0.423 0.482 0.379 0.590 0.511 0.444 0.372 0.486 0.347 SVM 0.456 0.689 0.544 0.641 0.685 0.658 0.374 0.695 0.481 CBS-L 0.697 0.613 0.644 0.848 0.616 0.699 0.639 0.613 0.619 Table 2: Summary results of the 50 topics problem. We use the SVM tool in LIBSVM. 4.3 Kernels and Parameters As Khan and Madden (2014) pointed out that one-class SVM performs the best when Gaussian kernel is used, we use Gaussian kernel as well. Manevitz and Yousef (2001) applied one-class SVM to text classification, and the authors reported that one-class SVM works the best with binary feature weighting scheme compared to tf or tf-idf weighting schemes. Also, they reported that a small number of features (10) with highest document frequency performed the best with Gaussian kernel. We also use binary representation, but found that 10 features are already too many in our case. In fact, 5 features give the best results. Using </context>
</contexts>
<marker>Khan, Madden, 2014</marker>
<rawString>Khan, S. and Madden, M. 2014. One-Class Classification: Taxonomy of Study and Review of Techniques. The Knowledge Engineering Review, 1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lebanon</author>
</authors>
<title>Sequential document representations and simplicial curves.</title>
<date>2006</date>
<publisher>UAI.</publisher>
<contexts>
<context position="12398" citStr="Lebanon 2006" startWordPosition="2034" endWordPosition="2035">007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive </context>
</contexts>
<marker>Lebanon, 2006</marker>
<rawString>Lebanon, G. 2006. Sequential document representations and simplicial curves. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Lee</author>
<author>B Liu</author>
</authors>
<title>Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression.</title>
<date>2003</date>
<publisher>ICML.</publisher>
<contexts>
<context position="13112" citStr="Lee and Liu, 2003" startWordPosition="2151" endWordPosition="2154">ntroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010). In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is very different because we perform sup</context>
</contexts>
<marker>Lee, Liu, 2003</marker>
<rawString>Lee, W. S. and Liu, B. 2003. Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
</authors>
<title>Learning to Rank for Information Retrieval and Natural Language Processing.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool publishers.</publisher>
<contexts>
<context position="13643" citStr="Li, 2011" startWordPosition="2250" endWordPosition="2251"> PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010). In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is very different because we perform supervised classification. Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection. 3 The Proposed CBS Learning We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts. In the next subsection, we first give the intuitive idea 2349 and a simpl</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Li, H. 2011. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan &amp; Claypool publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Li</author>
<author>H Huang</author>
<author>S Tian</author>
<author>W Xu</author>
</authors>
<title>Improving One-class SVM for anomaly detection.</title>
<date>2003</date>
<booktitle>Proc. of the Second International conference on Machine Learning and Cybernetics,</booktitle>
<volume>5</volume>
<pages>3077--3081</pages>
<contexts>
<context position="11605" citStr="Li et al. (2003)" startWordPosition="1902" endWordPosition="1905"> documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications. Manevitz and Yousef (2001) performed one-class text classification using one-class SVM as proposed by Sch≈°lkopf et al. (1999). The method is based on identifying outlier data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results (Sch≈°lkopf et al., 1999; 2000). Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on documen</context>
</contexts>
<marker>Li, Huang, Tian, Xu, 2003</marker>
<rawString>Li, K., Huang, H., Tian, S. and Xu, W. 2003. Improving One-class SVM for anomaly detection. Proc. of the Second International conference on Machine Learning and Cybernetics, volume 5, pages 3077‚Äì3081.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-K</author>
</authors>
<title>Negative Training Data can be Harmful to Text Classification.</title>
<date>2010</date>
<publisher>EMNLP.</publisher>
<marker>S-K, 2010</marker>
<rawString>Li, X., Liu, B. and Ng. S.-K. 2010. Negative Training Data can be Harmful to Text Classification. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>Building text classifiers using positive and unlabeled examples.</title>
<date>2003</date>
<publisher>ICDM.</publisher>
<marker>P, 2003</marker>
<rawString>Liu, B, Dai, Y., Li, X., Lee, W-S. and Yu. P. 2003. Building text classifiers using positive and unlabeled examples. ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2011</date>
<publisher>Springer.</publisher>
<marker>T, 2011</marker>
<rawString>Liu. T. 2011. Learning to Rank for Information Retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Luo</author>
<author>L Ding</author>
<author>Z Pan</author>
<author>G Ni</author>
<author>G Hu</author>
</authors>
<title>Research on cost-sensitive learning in one-class anomaly detection algorithms.</title>
<date>2007</date>
<journal>Autonomic and Trusted Computing,</journal>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>4610</volume>
<contexts>
<context position="11901" citStr="Luo et al., (2007)" startWordPosition="1952" endWordPosition="1955">r data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results (Sch≈°lkopf et al., 1999; 2000). Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid</context>
</contexts>
<marker>Luo, Ding, Pan, Ni, Hu, 2007</marker>
<rawString>Luo, J., Ding, L., Pan, Z., Ni, G. and Hu, G. 2007. Research on cost-sensitive learning in one-class anomaly detection algorithms. Autonomic and Trusted Computing, volume 4610 of Lecture Notes in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M</author>
</authors>
<title>One-class SVMs for document classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning research.</journal>
<marker>M, 2001</marker>
<rawString>Manevitz, L. and Yousef. M. 2001. One-class SVMs for document classification. Journal of Machine Learning research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>R Prabhakar</author>
<author>S Hinrich</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18294" citStr="Manning et al. 2008" startWordPosition="3056" endWordPosition="3059">sed technique. As we mentioned above, instead of using one single dsvector to represent a document di ‚ààD, we use a set Rd of p ds-vectors Rd = {ùê±d, ùê±2, ..., ùê±p}}. Each vector ùê±ÔøΩÔøΩ denotes one document space representation of the document, e.g., unigram representation. We then compute the center of positive training documents, which is represented as a set of ùëù centroids C = {c1, c2, ..., cp}, each of which corresponds to one document space representation in Rd. The way to compute each center ci is similar to that in the Rocchio relevance feedback method in information retrieval (Rocchio, 1971; Manning et al. 2008), which uses the corresponding ds-vectors of all training positive and negative documents. The detail will be given below. Based on Rd for document d and the center C, we can transform a document d from its document space representations Rd to one center-based similarity vector cbs-v by applying a similarity function ùëÜùëñùëö on each element ùê±d of Rd and its corresponding center ci. We now detail document transformation. Training document transformation: The train2350 ing data transformation from ds-vectors to cbsvectors performs the following two steps: Step 1: Compute the set C of centroids for t</context>
</contexts>
<marker>Manning, Prabhakar, Hinrich, 2008</marker>
<rawString>Manning, C. D., Prabhakar R., and Hinrich, S. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qian</author>
<author>B Liu</author>
</authors>
<title>Identifying Multiple Userids of the Same Author.</title>
<date>2013</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="13856" citStr="Qian and Liu 2013" startWordPosition="2281" endWordPosition="2284"> unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is very different because we perform supervised classification. Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection. 3 The Proposed CBS Learning We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts. In the next subsection, we first give the intuitive idea 2349 and a simple example. The detailed algorithm follows. In Section 3.5, we explain why CBS-L is better than DS-based learning when unexpected negative data appear in the test set. 3.1 Basic Idea In the proposed CBS-L formulati</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Qian, T. and Liu, B. 2013. Identifying Multiple Userids of the Same Author. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ranzato</author>
<author>M Szummer</author>
</authors>
<title>Semisupervised learning of compact document representations with deep networks.</title>
<date>2008</date>
<publisher>ICML.</publisher>
<contexts>
<context position="12425" citStr="Ranzato and Szummer, 2008" startWordPosition="2036" endWordPosition="2039"> and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, als</context>
</contexts>
<marker>Ranzato, Szummer, 2008</marker>
<rawString>Ranzato, M. and Szummer, M. 2008. Semisupervised learning of compact document representations with deep networks. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevant feedback in information retrieval.</title>
<date>1971</date>
<editor>In G. Salton (ed.).</editor>
<contexts>
<context position="18272" citStr="Rocchio, 1971" startWordPosition="3054" endWordPosition="3055">etail the proposed technique. As we mentioned above, instead of using one single dsvector to represent a document di ‚ààD, we use a set Rd of p ds-vectors Rd = {ùê±d, ùê±2, ..., ùê±p}}. Each vector ùê±ÔøΩÔøΩ denotes one document space representation of the document, e.g., unigram representation. We then compute the center of positive training documents, which is represented as a set of ùëù centroids C = {c1, c2, ..., cp}, each of which corresponds to one document space representation in Rd. The way to compute each center ci is similar to that in the Rocchio relevance feedback method in information retrieval (Rocchio, 1971; Manning et al. 2008), which uses the corresponding ds-vectors of all training positive and negative documents. The detail will be given below. Based on Rd for document d and the center C, we can transform a document d from its document space representations Rd to one center-based similarity vector cbs-v by applying a similarity function ùëÜùëñùëö on each element ùê±d of Rd and its corresponding center ci. We now detail document transformation. Training document transformation: The train2350 ing data transformation from ds-vectors to cbsvectors performs the following two steps: Step 1: Compute the se</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>Rocchio, J. 1971. Relevant feedback in information retrieval. In G. Salton (ed.). The smart retrieval system: experiments in automatic document processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch≈°lkopf</author>
<author>R Williamson</author>
<author>A Smola</author>
<author>J Taylor</author>
<author>J Platt</author>
</authors>
<title>Support vector method for novelty detection. Neural Information Processing Systems,</title>
<date>2000</date>
<pages>582--588</pages>
<marker>Sch≈°lkopf, Williamson, Smola, Taylor, Platt, 2000</marker>
<rawString>Sch≈°lkopf, B., Williamson, R., Smola, A., Taylor, J. and Platt, J. 2000. Support vector method for novelty detection. Neural Information Processing Systems, pages 582‚Äì588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch≈°lkopf</author>
<author>J Platt</author>
<author>J Shawe-Taylor</author>
<author>A Smola</author>
<author>R Williamson</author>
</authors>
<title>Estimating the support of a high-dimensional distribution.</title>
<date>1999</date>
<tech>Technical Report, Microsoft Research,</tech>
<pages>99--87</pages>
<contexts>
<context position="6637" citStr="Sch≈°lkopf et al. 1999" startWordPosition="1099" endWordPosition="1102">n of the original training set based on the estimated bias. Requiring the test data to be available in training is, however, a major weakness. In the social media post classification setting, the system needs to constantly classify the incoming data. It is infeasible to perform training constantly. In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue. One obvious solution to this problem is oneclass classification (Sch≈°lkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM. We simply discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy. In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform </context>
<context position="11240" citStr="Sch≈°lkopf et al. (1999)" startWordPosition="1839" endWordPosition="1842">parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions. Also, as pointed out by (Khan and Madden, 2010; 2014), one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces. Since text documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications. Manevitz and Yousef (2001) performed one-class text classification using one-class SVM as proposed by Sch≈°lkopf et al. (1999). The method is based on identifying outlier data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results (Sch≈°lkopf et al., 1999; 2000). Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf</context>
<context position="25502" citStr="Sch≈°lkopf et al. (1999" startWordPosition="4315" endWordPosition="4318">eview is about. In our experiments, we use every topic as the positive class. This gives us 50 classification results. 4.2 Baselines We use three baselines in our evaluation. Document space one-class SVM (ds-osvm): As we discussed earlier, due to the covariate shift problem in the negative training data, one solution is to drop the negative training data completely to build a one-class classifier. One-class SVM is the state-of-the-art one-class classification algorithm. We apply one-class SVM to the documents in the document space as one of the baselines. One-class SVM was first introduced by Sch≈°lkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class. The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin. As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b). SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive</context>
</contexts>
<marker>Sch≈°lkopf, Platt, Shawe-Taylor, Smola, Williamson, 1999</marker>
<rawString>Sch≈°lkopf, B., Platt, J., Shawe-Taylor, J., Smola, A. and Williamson, R. 1999. Estimating the support of a high-dimensional distribution. Technical Report, Microsoft Research, MSRTR-99-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shimodaira</author>
</authors>
<title>Improving predictive inference under covariate shift by weighting the log-likelihood function.</title>
<date>2000</date>
<journal>Journal of Statistical Planning and Inference,</journal>
<volume>90</volume>
<pages>244</pages>
<contexts>
<context position="3739" citStr="Shimodaira 2000" startWordPosition="612" endWordPosition="613">as no negative examples about sports. However, in testing, some sports posts show up. These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy. In this paper, we aim to solve this problem. In machine learning, this problem is called covariate shift, a type of sample selection bias. In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution. However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009). In general, the sample selection bias problem is not solvable because the two 2347 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347‚Äì2356, Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics. distributions can be arbitrarily far apart from each other. Various assumptions were made to solve special cases of the problem. One main assumption was that the conditional distribution of the class given a data instance is the same in the tr</context>
<context position="9294" citStr="Shimodaira (2000)" startWordPosition="1534" endWordPosition="1535">his is especially the case for social media data because of the high topic diversity and constant changes of topics. This problem is known as covariate shift, which is a form of sample selection bias. Sample selection bias was first introduced in econometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated</context>
</contexts>
<marker>Shimodaira, 2000</marker>
<rawString>Shimodaira, H. 2000. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90:227‚Äì 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sugiyama</author>
<author>K-R Muller</author>
</authors>
<title>Inputdependent estimation of generalization error under covariate shift.</title>
<date>2005</date>
<journal>Statistics and Decision,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="9325" citStr="Sugiyama and Muller (2005)" startWordPosition="1537" endWordPosition="1541">case for social media data because of the high topic diversity and constant changes of topics. This problem is known as covariate shift, which is a form of sample selection bias. Sample selection bias was first introduced in econometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the </context>
</contexts>
<marker>Sugiyama, Muller, 2005</marker>
<rawString>Sugiyama, M. and Muller, K.-R. 2005. Inputdependent estimation of generalization error under covariate shift. Statistics and Decision, 23(4):249‚Äì279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sugiyama</author>
<author>S Nakajima</author>
<author>H Kashima</author>
<author>P von Bunau</author>
<author>M Kawanabe</author>
</authors>
<title>Direct importance estimation with model selection and its application to covariate shift adaptation.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<marker>Sugiyama, Nakajima, Kashima, von Bunau, Kawanabe, 2008</marker>
<rawString>Sugiyama, M., Nakajima, S., Kashima, H., von Bunau P., and Kawanabe M. 2008. Direct importance estimation with model selection and its application to covariate shift adaptation. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>R Duin</author>
</authors>
<title>Data domain description using support vectors.</title>
<date>1999</date>
<booktitle>Proceedings ESAN99,</booktitle>
<pages>251--256</pages>
<location>Brussels.</location>
<contexts>
<context position="6657" citStr="Tax and Duin, 1999" startWordPosition="1103" endWordPosition="1106">ing set based on the estimated bias. Requiring the test data to be available in training is, however, a major weakness. In the social media post classification setting, the system needs to constantly classify the incoming data. It is infeasible to perform training constantly. In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue. One obvious solution to this problem is oneclass classification (Sch≈°lkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM. We simply discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy. In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a simila</context>
<context position="10227" citStr="Tax and Duin (1999" startWordPosition="1680" endWordPosition="1683">al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius. In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions. Also, as pointed out by (Khan and Madden, 2010; 2014</context>
<context position="25954" citStr="Tax and Duin (1999" startWordPosition="4390" endWordPosition="4393">ication algorithm. We apply one-class SVM to the documents in the document space as one of the baselines. One-class SVM was first introduced by Sch≈°lkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class. The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin. As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b). SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius. It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied. Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.2</context>
</contexts>
<marker>Tax, Duin, 1999</marker>
<rawString>Tax, D. and Duin, R. 1999a. Data domain description using support vectors. Proceedings ESAN99, Brussels. 251-256</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>R Duin</author>
</authors>
<title>Support vector domain description.</title>
<date>1999</date>
<journal>Pattern Recognition Letters</journal>
<volume>20</volume>
<pages>1191--1199</pages>
<contexts>
<context position="6657" citStr="Tax and Duin, 1999" startWordPosition="1103" endWordPosition="1106">ing set based on the estimated bias. Requiring the test data to be available in training is, however, a major weakness. In the social media post classification setting, the system needs to constantly classify the incoming data. It is infeasible to perform training constantly. In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue. One obvious solution to this problem is oneclass classification (Sch≈°lkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM. We simply discard the negative training posts/documents completely because they have the covariate shift problem. Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly. Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy. In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a simila</context>
<context position="10227" citStr="Tax and Duin (1999" startWordPosition="1680" endWordPosition="1683">al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius. In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions. Also, as pointed out by (Khan and Madden, 2010; 2014</context>
<context position="25954" citStr="Tax and Duin (1999" startWordPosition="4390" endWordPosition="4393">ication algorithm. We apply one-class SVM to the documents in the document space as one of the baselines. One-class SVM was first introduced by Sch≈°lkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class. The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin. As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b). SVDD seeks to distinguish the positive class from all other possible data in space. It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius. It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied. Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.2</context>
</contexts>
<marker>Tax, Duin, 1999</marker>
<rawString>Tax, D. and Duin, R. 1999b. Support vector domain description. Pattern Recognition Letters 20. 1191-1199</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>R Duin</author>
</authors>
<title>Uniform object generation for optimizing one-class classifiers.</title>
<date>2001</date>
<journal>J. of Machine Learning Research,</journal>
<pages>2--155</pages>
<contexts>
<context position="10526" citStr="Tax and Duin (2001)" startWordPosition="1727" endWordPosition="1730"> discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius. In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters. However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions. Also, as pointed out by (Khan and Madden, 2010; 2014), one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces. Since text documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications. Manevitz an</context>
</contexts>
<marker>Tax, Duin, 2001</marker>
<rawString>Tax, D. and Duin, R. 2001. Uniform object generation for optimizing one-class classifiers. J. of Machine Learning Research, 2:155‚Äì173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tian</author>
<author>H Gu</author>
</authors>
<title>Anomaly detection combining one-class SVMs and particle swarm optimization algorithms. Nonlinear Dynamics,</title>
<date>2010</date>
<pages>61--1</pages>
<contexts>
<context position="11814" citStr="Tian and Gu, 2010" startWordPosition="1939" endWordPosition="1942">s SVM as proposed by Sch≈°lkopf et al. (1999). The method is based on identifying outlier data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results (Sch≈°lkopf et al., 1999; 2000). Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Sz</context>
</contexts>
<marker>Tian, Gu, 2010</marker>
<rawString>Tian, J. and Gu, H. 2010. Anomaly detection combining one-class SVMs and particle swarm optimization algorithms. Nonlinear Dynamics, 61(1-2): 303‚Äì310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsuboi</author>
<author>H Kashima</author>
<author>S Hido</author>
<author>S Bickel</author>
<author>M Sugiyama</author>
</authors>
<title>Direct density ratio estimation for large-scale covariate shift adaptation.</title>
<date>2008</date>
<booktitle>Proceedings of the SIAM International Conference on Data Mining (SDM).</booktitle>
<contexts>
<context position="9698" citStr="Tsuboi et al. (2008)" startWordPosition="1597" endWordPosition="1601">o first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching. Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification. The proposed technique CBS-L doesn‚Äôt have this restriction. As mentioned in the introduction, one-class classification is a suitable approach to solve the problem. Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Suppo</context>
</contexts>
<marker>Tsuboi, Kashima, Hido, Bickel, Sugiyama, 2008</marker>
<rawString>Tsuboi, J., Kashima, H., Hido, S., Bickel, S., and Sugiyama, M. 2008. Direct density ratio estimation for large-scale covariate shift adaptation. Proceedings of the SIAM International Conference on Data Mining (SDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wang</author>
<author>C Domeniconi</author>
</authors>
<title>Building semantic kernels for text classification using Wikipedia,</title>
<date>2008</date>
<location>KDD.</location>
<contexts>
<context position="12453" citStr="Wang and Domeniconi, 2008" startWordPosition="2040" endWordPosition="2043">ine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Deni</context>
</contexts>
<marker>Wang, Domeniconi, 2008</marker>
<rawString>Wang, P. and Domeniconi, C. 2008. Building semantic kernels for text classification using Wikipedia, KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<publisher>ICML.</publisher>
<contexts>
<context position="30036" citStr="Yang and Pedersen, 1997" startWordPosition="5059" endWordPosition="5062">rm well with the traditional document representation. The term weighting scheme is tf-idf (Colas and Brazdil, 2006) with no feature selection. For our proposed method CBS-L, we use tf-idf values of unigram, bigram and trigram to represent a document in three ways in the document space. As mentioned earlier, five document similarity functions are used to transform document space vectors to CBS space vectors. And in order to filter out less useful features for the center vector of the positive class, we performed feature selection in the document space using the classic information gain method (Yang and Pedersen, 1997) to empirically choose the most effective 100 features for the positive class. For all the kernels, we use the default parameter settings in the LIBSVM systems. We tried to tune the parameters, but did not get better results. 4.4 Results We now present the experiment results. As mentioned above, we treat each topic as the positive class. This gives 50 tests. To test the effect of covariate shift, we also vary the number of topics in the negative class. We used 10, 20, 30, and 40 topics in the training negative class. The test set always has 49 topics of negative data. For each setting, we give</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yang, Y. and Pedersen, J. O. 1997. A comparative study on feature selection in text categorization. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Yang</author>
<author>M Madden</author>
</authors>
<title>One-class support vector machine calibration using particle swarm optimization.</title>
<date>2007</date>
<publisher>AICS, Dublin.</publisher>
<contexts>
<context position="11790" citStr="Yang and Madden, 2007" startWordPosition="1934" endWordPosition="1937">lassification using one-class SVM as proposed by Sch≈°lkopf et al. (1999). The method is based on identifying outlier data that are representative of the second class. Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers. However, as reported in the paper, their methods produce quite weak results (Sch≈°lkopf et al., 1999; 2000). Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies. Their idea is to consider all data points that are close to the origin as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Sch≈°lkopf‚Äôs models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Leba</context>
</contexts>
<marker>Yang, Madden, 2007</marker>
<rawString>Yang, L., and Madden, M. 2007. One-class support vector machine calibration using particle swarm optimization. AICS, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>J Han</author>
<author>K Chang</author>
</authors>
<title>PEBL: Positive example based learning for Web page classification using SVM.</title>
<date>2002</date>
<pages>239--248</pages>
<publisher>KDD,</publisher>
<contexts>
<context position="13076" citStr="Yu et al. 2002" startWordPosition="2143" endWordPosition="2146">ev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010). In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is v</context>
</contexts>
<marker>Yu, Han, Chang, 2002</marker>
<rawString>Yu, H., Han, J. and Chang, K. 2002. PEBL: Positive example based learning for Web page classification using SVM. KDD, 239-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zadrozny</author>
</authors>
<title>Learning and evaluating classifiers under s ample selection bias,</title>
<date>2004</date>
<journal>ICML.</journal>
<contexts>
<context position="3754" citStr="Zadrozny 2004" startWordPosition="614" endWordPosition="615">amples about sports. However, in testing, some sports posts show up. These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy. In this paper, we aim to solve this problem. In machine learning, this problem is called covariate shift, a type of sample selection bias. In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution. However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009). In general, the sample selection bias problem is not solvable because the two 2347 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347‚Äì2356, Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics. distributions can be arbitrarily far apart from each other. Various assumptions were made to solve special cases of the problem. One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test</context>
<context position="9034" citStr="Zadrozny (2004)" startWordPosition="1491" endWordPosition="1492">imentally demonstrates the effectiveness of the proposed method. 2 Related Work Traditional supervised learning assumes that the training and test examples are drawn from the same distribution. However, this assumption can be violated in many applications. This is especially the case for social media data because of the high topic diversity and constant changes of topics. This problem is known as covariate shift, which is a form of sample selection bias. Sample selection bias was first introduced in econometrics by Heckman (1979). It came into the field of machine learning through the work of Zadrozny (2004). The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to 2348 compensate for the bias (Bickel et al. 2009). For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation. The estimated density ratio is then used to generate weighted training examples. Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kerne</context>
</contexts>
<marker>Zadrozny, 2004</marker>
<rawString>Zadrozny, B. 2004. Learning and evaluating classifiers under s ample selection bias, ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001828">
<title confidence="0.9987795">
Joint Prediction for Entity/Event-Level Sentiment Analysis
using Probabilistic Soft Logic Models
</title>
<author confidence="0.828118">
Lingjia Deng Janyce Wiebe
</author>
<affiliation confidence="0.856934">
Intelligent Systems Program Intelligent Systems Program
University of Pittsburgh Department of Computer Science
lid29@pitt.edu University of Pittsburgh
</affiliation>
<email confidence="0.980274">
wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.993542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999142333333333">
In this work, we build an entity/event-level
sentiment analysis system, which is able to
recognize and infer both explicit and im-
plicit sentiments toward entities and events
in the text. We design Probabilistic Soft
Logic models that integrate explicit senti-
ments, inference rules, and +/-effect event
information (events that positively or neg-
atively affect entities). The experiments
show that the method is able to greatly im-
prove over baseline accuracies in recog-
nizing entity/event-level sentiments.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998812818181818">
There are increasing numbers of opinions ex-
pressed in various genres, including reviews,
newswire, editorials, and forums. While much
early work was at the document or sentence
level, to fully understand and utilize opinions, re-
searchers are increasingly carrying out more fine-
grained sentiment analysis to extract components
of opinion frames: the source (whose sentiment
is it), the polarity, and the target (what is the senti-
ment toward). Much fine-grained analysis is span
or aspect based (Yang and Cardie, 2014; Pontiki
et al., 2014). In contrast, this work contributes
to entity/event-level sentiment analysis. A sys-
tem that could recognize sentiments toward enti-
ties and events would be valuable in an application
such as Automatic Question Answering, to sup-
port answering questions such as “Who is nega-
tive/positive toward X?” (Stoyanov et al., 2005),
where X could be any entity or event.
Let us consider an example from the MPQA
opinion annotated corpus (Wiebe et al., 2005a;
Wilson, 2007; Deng and Wiebe, 2015).
</bodyText>
<note confidence="0.4030155">
Ex(1) When the Imam
( may God be satisfied with him 1)
</note>
<bodyText confidence="0.995328774193548">
issued the fatwa against 2 Salman Rushdie for
insulting 3 the Prophet ( peace be upon him 4),
the countries that are so-called 5 supporters of
human rights protested against 6 the fatwa.
There are several sentiment expressions anno-
tated in MPQA. In the first clause, the writer is
positive toward Imam and Prophet as expressed
by may God be satisfied with him (1) and peace
be upon him (4), respectively. Imam is negative
toward Salman Rushdie and the insulting event,
as revealed by the expression issued the fatwa
against (2). And Salman Rushdie is negative to-
ward Prophet, as revealed by the expression insult-
ing (3). In the second clause, the writer is negative
toward the countries, as expressed by so-called
(5). And the countries are negative toward fatwa,
as revealed by the expression protested against
(6). Using the source and the target, we summa-
rize the positive opinions above in a set P, and the
negative opinions above in another set N. Thus, P
contains {(writer, Imam), (writer, Prophet)}, and
N contains {(Imam, Rushdie), (Imam, insulting),
(Rushdie, Prophet), (writer, countries), (countries,
fatwa)}.1
An (ideal) explicit sentiment analysis system is
expected to extract the above sentiments expressed
by (1)-(6). However, there are many more sen-
timents communicated by the writer but not ex-
pressed via explicit expressions. First, Imam is
positive toward the Prophet, because Rushdie in-
sults the Prophet and Imam is angry that he does
</bodyText>
<footnote confidence="0.9881876">
1Sources in MPQA are nested, having the form (writer)
or (writer, S1, ... , S.). This work only deals with the right-
most source, writer or S.. Also, actions like issuing a fatwa
are treated the same as private states. Please see (Wiebe et
al., 2005a).
</footnote>
<page confidence="0.939902">
179
</page>
<note confidence="0.9959455">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 179–189,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.9363685">
Figure 1: Explicit and implicit sentiments in
Ex(1).
</figureCaption>
<bodyText confidence="0.994891377049181">
so. Second, the writer is negative toward Rushdie,
because the writer is positive toward the Prophet
but Rushdie insults him! Also, the writer is prob-
ably positive toward the fatwa since it is against
Rushdie. Third, the countries are probably nega-
tive toward Imam, because the countries are neg-
ative toward fatwa and it is Imam who issued
the fatwa. Thus, the set P should also contain
{(Imam, Prophet), (writer, fatwa)}, and the set
N should also contain {(writer, Rushdie), (coun-
tries, Imam)}. These opinions are not directly ex-
pressed, but are inferred by a human reader.2 The
explicit and implicit sentiments are summarized in
Figure 1, where each green line represents a posi-
tive sentiment and each red line represents a neg-
ative sentiment. The solid lines are explicit senti-
ments and the dashed lines are implicit sentiments.
In this work, we detect sentiments such as those
in P and N, where the sources are entities (or the
writer) and the targets are entities and events.
Previous work in sentiment analysis mainly fo-
cuses on detecting explicit opinions. Recently
there is emerging focus on sentiment inference,
which recognizes implicit sentiments by inferring
them from explicit sentiments via inference rules.
Current works in sentiment inference differ on
how the sentiment inference rules are defined and
how they are expressed. For example, Zhang
and Liu (2011) define linguistic templates to rec-
ognize phrases that express implicit sentiments,
while previously we (Deng et al., 2014) represent
a few simple rules as (in)equality constraints in In-
teger Linear Programming. In contrast to previous
2Note that the inferences are conversational implicatures;
they are defeasible and may not go through in context (Deng
et al., 2014; Wiebe and Deng, 2014).
work, we propose a more general set of inference
rules and encode them in a probabilistic soft logic
(PSL) framework (Bach et al., 2015). We chose
PSL because it is designed to have efficient infer-
ence and, as similar methods in Statistical Rela-
tional Learning do, it allows probabilistic models
to be specified in first-order logic, an expressive
and natural way to represent if-then rules, and it
supports joint prediction. Joint prediction is criti-
cal for our task because it involves multiple, mutu-
ally constraining ambiguities (the source, polarity,
and target).
Thus, this work aims at detecting both implicit
and explicit sentiments expressed by an entity to-
ward another entity/event (i.e., an eTarget) within
the sentence. The contributions of this work are:
(1) defining a method for entity/event-level senti-
ment analysis to provide a deeper understanding
of the text; (2) exploiting first-order logic rules to
infer such sentiments, where the source is not lim-
ited to the writer, and the target may be any entity,
event, or even another sentiment; and (3) devel-
oping a PSL model to jointly resolve explicit and
implicit sentiment ambiguities by integrating in-
ference rules.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.990828227272727">
Fined-grained sentiment analysis. Most fine-
grained sentiment analysis is span or aspect based.
Previous work differs from the entity/event-level
sentiment analysis task we address in terms of tar-
gets and sources. In terms of targets, in a span-
based sentiment analysis system, the target is a
span instead of the exact head of the phrase re-
ferring to the target. The target in a span-based
system is evaluated by measuring the overlapping
proportion of an extracted span against the gold
standard phrase (Yang and Cardie, 2013), while
the eTarget in an entity/event-level system is eval-
uated against the exact word (i.e., head of NP/VP)
in the gold standard. It is a stricter evaluation.
While the targets in aspect-based sentiment analy-
sis are often entity targets, they are mainly product
aspects, which are a predefined set.3 In contrast,
the target in the entity/event-level task may be any
noun or verb. In terms of sources, previous work in
sentiment analysis trained on review data assumes
that the source is the writer of the review (Hu and
Liu, 2004; Titov and McDonald, 2008).
</bodyText>
<footnote confidence="0.954143">
3As stated in SemEval-2014: “we annotate only aspect
terms naming particular aspects”.
</footnote>
<page confidence="0.997225">
180
</page>
<bodyText confidence="0.999820395833333">
Our work is rare in that it allows sources other
than the writer and finds sentiments toward eTar-
gets which may be any entity or event.
Sentiment Inference. There is some recent
work investigating features that directly indicate
implicit sentiments (Zhang and Liu, 2011; Feng
et al., 2013). That work assumes the source is
only the writer. Further, as it uses features to di-
rectly extract implicit sentiments, it does not per-
form general sentiment inference.
Previously, we (Deng et al., 2013; Deng and
Wiebe, 2014; Deng et al., 2014) develop rules
and models to infer sentiments related to +/-effect
events, events that positively or negatively affect
entities. That work assumes that the source is only
the writer, and the targets are limited to entities
that participate in +/-effect events. Further, our
previous models all require certain manual (ora-
cle) annotations to be input. In this work we use
an expanded set of more general rules. We al-
low sources other than the writer, and targets that
may be any entity or event. In fact, under our new
rules, the targets of sentiments may be other sen-
timents; we model such novel “sentiment toward
sentiment” structures in Section 4.3. Finally, our
method requiring no manual annotations as input
when the inference is conducted.
Previously, we also propose a set of sentiment
inference rules and develop a rule-based system to
infer sentiments (Wiebe and Deng, 2014). How-
ever, the rule-based system requires all informa-
tion regarding explicit sentiments and +/-effect
events to be provided as oracle information by
manual annotations.
Probabilistic Soft Logic. Probabilistic Soft
Logic (PSL) is a variation of Markov Logic Net-
works, which is a framework for probabilistic
logic that employs weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks) (Bach et al., 2015; Beltagy et al., 2014).
PSL is a new statistical relational learning method
that has been applied to many NLP and other ma-
chine learning tasks in recent years (Beltagy et al.,
2014; London et al., 2013; Pujara et al., 2013;
Bach et al., 2013; Huang et al., 2013; Memory et
al., 2012). Previously, PSL has not been applied
to entity/event-level sentiment analysis.
</bodyText>
<sectionHeader confidence="0.989284" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.994194117647059">
In this section, we introduce the definition of
the entity/event-level sentiment analysis task, fol-
lowed by a description of the gold standard corpus.
For each sentence s, we define a set E consist-
ing of entities, events, and the writer of s, and sets
P and N consisting of positive and negative senti-
ments, respectively. Each element in P is a tuple,
representing a positive pair of two entities, (e1,
e2) where e1, e2 ∈ E, and e1 is positive toward
e2. A positive pair (e1,e2) aggregates all the posi-
tive sentiments from e1 to e2 in the sentence. N is
the corresponding set for negative pairs.
The goal of this work is to automatically rec-
ognize a set of positive pairs (Pauto) and a set of
negative pairs (Nauto). We compare the system
output (Pauto ∪ Nauto) against the gold standard
(Pgold ∪ Ngold) for each sentence.
</bodyText>
<subsectionHeader confidence="0.998807">
3.1 Gold Standard Corpus: MPQA 3.0
</subsectionHeader>
<bodyText confidence="0.9998982">
MPQA 3.0 is a recently developed corpus with
entity/event-level sentiment annotations (Deng
and Wiebe, 2015).4 It is built on the basis of
MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007),
which includes editorials, reviews, news reports,
and scripts of interviews from different news agen-
cies, and covers a wide range of topics.
In both MPQA 2.0 and 3.0, the top-level an-
notations include direct subjectives (DS). Each
DS has a nested-source annotation. Each DS has
one or more attitude links, meaning that all of the
attitudes share the same nested source. The at-
titudes differ from one another in their attitude
types, polarities, and/or targets. Moreover, both
corpora contain expressive subjective element
(ESE) annotations, which pinpoint specific ex-
pressions used to express subjectivity. We ignore
neutral ESEs and only consider ESEs whose po-
larity is positive or negative.
MPQA 2.0 and 3.0 differ in their target annota-
tions. In 2.0, each target is a span. A target annota-
tion of an opinion captures the most important tar-
get this opinion is expressed toward. Since the ex-
act boundaries of the spans are hard to define even
for human annotators (Wiebe et al., 2005a; Yang
and Cardie, 2013), the target span in MPQA 2.0
could be a single word, an NP or VP, or a text span
covering more than one constituent. In contrast, in
MPQA 3.0, each target is anchored to the head of
an NP or VP, which is a single word. It is called an
</bodyText>
<footnote confidence="0.998959">
4Available at http://mpqa.cs.pitt.edu/corpora/
</footnote>
<page confidence="0.997322">
181
</page>
<bodyText confidence="0.999540545454545">
eTarget since it is an entity or an event. In MPQA
2.0, only attitudes have target-span annotations. In
MPQA 3.0, both attitudes and ESEs have eTarget
annotations. Importantly, the eTargets include the
targets of both explicit and implicit sentiments.
Recall Ex(1) in Section 1. Pgold = {(writer,
Imam), (writer, Prophet), (Imam, Prophet),
(writer, fatwa)}, and Ngold = {(Imam, Rushdie),
(Imam, insulting), (Rushdie, Prophet), (writer,
countries), (countries, fatwa), (writer, Rushdie),
(countries, Imam)}.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
4 PSL for Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.999902">
We need to resolve three components for an opin-
ion frame: the source, the polarity, and the eTarget.
Each of these ambiguities has several candidates.
For example in Ex(1), the eTarget of the opinion
expression insulting is an ambiguity. The candi-
dates include Prophet, countries, and so on.
In this work, we use Probabilistic Soft Logic
(PSL). A PSL model is defined using a set of
atoms to be grounded, and a set of weighted if-
then rules expressed in first-order logic. For ex-
ample, we define the atom ETARGET(y,t) to rep-
resent an opinion y having eTarget t. If y and
t are constants, then ETARGET(y,t) is a ground
atom (e.g., ETARGET(insulting, Prophet)). Each
ground atom is assigned a score by a local system.
PSL takes as input all the local scores as well as
the constraints defined by the rules among atoms,
so that it is able to jointly resolve all the ambigu-
ities. In the final output, for example, the score
ETARGET(insulting, Prophet) &gt; 0 means that PSL
considers Prophet to be an eTarget of insulting,
while ETARGET(insulting, countries) = 0 means
that PSL does not consider countries to be an eTar-
get of insulting.
In this section, we first introduce PSL in Section
4.1. We then present three PSL models in turn.
PSL1 (Section 4.2) aggregates span-based opin-
ions into Pauto and Nauto. PSL2 (Section 4.3) adds
sentiment inference rules to PSL1. For PSL3 (Sec-
tion 4.4), rules involving +/-effect events are added
to PSL2, resulting in the richest overall model.
</bodyText>
<subsectionHeader confidence="0.997061">
4.1 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.999667583333333">
PSL (Bach et al., 2015) uses logical representa-
tions to compactly define large graphical models
with continuous variables, and includes methods
for performing efficient probabilistic inference for
the resulting models (Beltagy et al., 2014). As
mentioned above, a PSL model is defined using a
set of atoms to be grounded, and a set of weighted
if-then rules in first-order logic. For example,
friend(x,y) n votesFor(y,z) ⇒ votesFor(x,z)
means that a person may vote for the same per-
son as his/her friend. Each predicate in the rule is
an atom (e.g., friend(x,y)). A ground atom is pro-
duced by replacing variables with constants (e.g.,
friend(Tom, Mary)). Each rule is associated with
a weight, indicating the importance of this rule in
the whole rule set.
A key distinguishing feature of PSL is that each
ground atom a has a soft, continuous truth value
in the interval [0, 1], denoted as I(a), rather than
a binary truth value as in Markov Logic Net-
works and most other probabilistic logic frame-
works (Beltagy et al., 2014). To compute soft
truth values for logical formulas, Lukasiewicz re-
laxations are used:
</bodyText>
<equation confidence="0.9634762">
l1 n l2 = max{0, I(l1) + I(l2) − 1}
l1 V l2 = min{I(l1) + I(l2),1}
¬l1 = 1 − I(l1)
A rule r ≡ rbody → rhead, is satisfied (i.e.
I(r) = 1) iff I(rbody) ≤ I(rhead). Other-
</equation>
<bodyText confidence="0.998964166666667">
wise, a distance to satisfaction d(r) is calculated,
which defines how far a rule r is from being satis-
fied: d(r) = max {0, I(rbody) − I(rhead)}. Us-
ing d(r), PSL defines a probability distribution
over all possible interpretations I of all ground
atoms:
</bodyText>
<equation confidence="0.991491">
1
p(I) = Z exp {−1 ∗
r∈R
</equation>
<bodyText confidence="0.9999462">
where Z is the normalization constant, λr is the
weight of rule r, R is the set of all rules, and p de-
fines loss functions. PSL seeks the interpretation
with the minimum distance d(r) and which satis-
fies all rules to the extent possible.
</bodyText>
<subsectionHeader confidence="0.59693">
4.2 PSL for Sentiment Aggregation (PSL1)
</subsectionHeader>
<bodyText confidence="0.999774111111111">
The first PSL model, PSL1, aggregates span-based
opinions into Pauto and Nauto. We call this senti-
ment aggregation because, instead of building an
entity/event-level sentiment system from scratch,
we choose to fully utilize previous work on span-
based sentiment analysis. PSL1 aggregates span-
based opinions into entity/event-level opinions.
Consistent with the task definition in Section 3,
we define two atoms in PSL:
</bodyText>
<listItem confidence="0.9996255">
(1) POSPAIR(s,t): a positive pair from s toward t
(2) NEGPAIR(s,t): a negative pair from s toward t
</listItem>
<bodyText confidence="0.537571">
λr(d(r))p}
</bodyText>
<page confidence="0.986789">
182
</page>
<bodyText confidence="0.9998602">
Both s and t are chosen from the set E. The val-
ues of ground atoms (1) and (2) are not observed
and are inferred by PSL.
Then, we define atoms to model an entity/event-
level opinion:
</bodyText>
<listItem confidence="0.99990975">
(3) POS(y): y is a positive sentiment
(4) NEG(y): y is a negative sentiment
(5) SOURCE(y,s): the source of y is s
(6) ETARGET(y,t): the eTarget of y is t
</listItem>
<bodyText confidence="0.999324373626374">
Two rules are defined to aggregate various opin-
ions extracted by span-based systems into positive
pairs and negative pairs, shown in Part 1 of Table
1 as Rules 1.1 and 1.2. Thus, under our repre-
sentation, the PSL model not only finds a set of
eTargets of an opinion (ETARGET(y,t)), but also
represents the aggregated sentiments among enti-
ties and events (POSPAIR(s,t) and NEGPAIR(s,t))
in the sentence.
Next, we turn to assigning local scores to
ground atoms (3)-(6).
POS(y) and NEG(y): We build upon three span-
based sentiment analysis systems. The first, S1
(Yang and Cardie, 2013), and the second, S2
(Yang and Cardie, 2014), are both trained on
MPQA 2.0, which does not contain any eTarget
annotations. S1 extracts triples of (source span,
opinion span, target span), but does not extract
opinion polarities. S2 extracts opinion spans and
opinion polarities, but it does not extract sources
or targets. The third system, S3 (Socher et al.,
2013), is trained on movie review data. It extracts
opinion spans and polarities. The source is always
assumed to be the writer.
We take the union set of opinions extracted by
S1, S2 and S3. For each opinion y, a ground atom
is created, depending on the polarity (POS(y) if y
is positive and NEG(y) is y is negative). The po-
larity is determined as follows. If S2 assigns a po-
larity to y, then that polarity is used. If S3 but
not S2 assigns a polarity to y, then S3’s polarity
is used. In both cases, the score assigned to the
ground atom is 1.0. If neither S2 nor S3 assigns a
polarity to y, we use the MPQA subjectivity lex-
icon to determine its polarity. The score assigned
to the ground atom is the proportion of the words
in the opinion span that are included in the subjec-
tivity lexicon.
SOURCE(y,s): S1 extracts the source of each
opinion, S2 does not extract the source, and S3 as-
sumes the source is always the writer. Thus, for
an opinion y, if the source s is assigned by S1, a
ground atom SOURCE(y,s) is created with score
1.0. Otherwise, if S3 extracts opinion y, a ground
atom SOURCE(y,writer) is created with score 1.0
(since S3 assumes the source is always the writer).
Otherwise, we run the Stanford named entity rec-
ognizer (Manning et al., 2014; Finkel et al., 2005)
to extract named entities in the sentence. The near-
est named entity to the opinion span on the depen-
dency parse graph will be treated as the source.
The score is the reciprocal of the length of the path
between the opinion span and the source span in
the dependency parse.
ETARGET(y,t): Though each eTarget is an en-
tity or event, it is difficult to determine which
nouns and verbs should be considered. Taking
into consideration the trade-off between precision
and recall, we experimented with three methods
to select eTarget candidates. For each opinion y,
a ground atom ETARGET(y,t) is created for each
eTarget candidate t.
ET1 considers all the nouns and verbs in the
sentence, to provide a full recall of eTargets.
ET2 considers all the nouns and verbs in the tar-
get spans and opinion spans that are automatically
extracted by systems S1, S2 and S3. We hypoth-
esized that ET2 would be useful because most of
the eTargets in MPQA 3.0 appear within the opin-
ion or the target spans of MPQA 2.0.
ET3 considers the heads of the target and opin-
ion spans that are automatically extracted by sys-
tems S1, S2 and S3.5 ET3 also considers the heads
of siblings of target spans and opinion spans.
Among the three methods, ET3 has the lowest re-
call but the highest precision.
In addition, for the eTarget candidate set ex-
tracted by ET2, or ET3, we run the Stanford co-
reference system (Manning et al., 2014; Recasens
et al., 2013; Lee et al., 2013) to expand the set
in two ways. First, for each eTarget candidate t,
the co-reference system extracts the entities that
co-refer with t. We add the referring entities into
the candidate set. Second, the co-reference system
extracts words which the Stanford system judges
to be entities, regardless of whether they have any
referent or not. We add this set of entities to the
candidate set as well.
We train an SVM classifier (Cortes and Vap-
nik, 1995) to assign a score to the ground atom
ETARGET(y,t). Syntactic features describing the
</bodyText>
<footnote confidence="0.880137">
5The head of a phrase is extracted by the Collins head
finder in the Stanford parser (Manning et al., 2014).
</footnote>
<page confidence="0.989656">
183
</page>
<table confidence="0.999446826086956">
Part 1. Aggregation Rules.
1.1 SOURCE(y,s) ∧ ETARGET(y,t) ∧ POS(y) ⇒ POSPAIR(s,t)
1.2 SOURCE(y,s) ∧ ETARGET(y,t) ∧ NEG(y) ⇒ NEGPAIR(s,t)
Part 2. Inference Rules.
2.1 POSPAIR(s1,y2) ∧ SOURCE(y2,s2) ⇒ POSPAIR(s1,s2)
2.2 POSPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ POS(y2) ⇒ POSPAIR(s1,t2)
2.3 POSPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ NEG(y2) ⇒ NEGPAIR(s1,t2)
2.4 NEGPAIR(s1,y2) ∧ SOURCE(y2,s2) ⇒ NEGPAIR(s1,s2)
2.5 NEGPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ POS(y2) ⇒ NEGPAIR(s1,t2)
2.6 NEGPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ NEG(y2) ⇒ POSPAIR(s1,t2)
Part 3. Inference Rules w.r.t +/-Effect Event Information.
3.1 POSPAIR(s,x) ∧ AGENT(x,a) ⇒ POSPAIR(s,a)
3.2 POSPAIR(s,x) ∧ THEME(x,h) ∧ +EFFECT(x) ⇒ POSPAIR(s,h)
3.3 POSPAIR(s,x) ∧ THEME(x,h) ∧-EFFECT(x) ⇒ NEGPAIR(s,h)
3.4 NEGPAIR(s,x) ∧ AGENT(x,a) ⇒ NEGPAIR(s,a)
3.5 NEGPAIR(s,x) ∧ THEME(x,h) ∧ +EFFECT(x) ⇒ NEGPAIR(s,h)
3.6 NEGPAIR(s,x) ∧ THEME(x,h) ∧-EFFECT(x) ⇒ POSPAIR(s,h)
3.7 POSPAIR(s,a) ∧ AGENT(x,a) ⇒ POSPAIR(s,x)
3.8 POSPAIR(s,h) ∧ THEME(x,h) ∧ +EFFECT(x) ⇒ POSPAIR(s,x)
3.9 POSPAIR(s,h) ∧ THEME(x,h) ∧-EFFECT(x) ⇒ NEGPAIR(s,x)
3.10 NEGPAIR(s,a) ∧ AGENT(x,a) ⇒ NEGPAIR(s,x)
3.11 NEGPAIR(s,h) ∧ THEME(x,h) ∧ +EFFECT(x) ⇒ NEGPAIR(s,x)
3.12 NEGPAIR(s,h) ∧ THEME(x,h) ∧-EFFECT(x) ⇒ POSPAIR(s,x)
</table>
<tableCaption confidence="0.999869">
Table 1: Rules in First-Order Logic.
</tableCaption>
<bodyText confidence="0.9737559">
relations between an eTarget and the extracted
opinion span and target span are considered, in-
cluding: (1) whether the eTarget is in the opin-
ion/target span; (2) the unigrams and bigrams on
the path from the eTarget to the opinion/target
span in the constituency parse tree; and (3) the
unigrams and bigrams on the path from the eTar-
get to the opinion/target word in the dependency
parse graph. We normalize the SVM scores into
the range of a ground atom score, [0,1].
</bodyText>
<subsectionHeader confidence="0.851366">
4.3 PSL for Sentiment Inference (PSL2)
</subsectionHeader>
<bodyText confidence="0.99967335">
The two rules defined in Section 4.2 aggregate
various opinions into positive pairs and negative
pairs, but inferences have not yet been introduced.
PSL2 is defined using the atoms and rules in
PSL1. But it also includes some rules defined
in (Wiebe and Deng, 2014), represented here in
first-order logic in Part 2 of Table 1. Let us go
through an example inference for Ex(1), in partic-
ular, the inference that Imam is positive toward the
Prophet. Rule 2.6 supports this inference. Recall
the two explicit sentiments: Imam is negative to-
ward the insulting sentiment (revealed by issued
the fatwa against), and Rushdie is negative to-
ward the Prophet (revealed by insulting). Thus,
we can instantiate Rule 2.6, where s1 is Imam, y2
is the negative sentiment (insulting), and t2 is the
Prophet. The inference is: since Imam is negative
that there is any negative opinion expressed toward
the Prophet, we infer that Imam is positive toward
the Prophet.
</bodyText>
<equation confidence="0.99316975">
NEGPAIR(Imam, insulting)
∧ ETARGET(insulting, Prophet)
∧ NEG(insulting)
⇒ POSPAIR(Imam, Prophet).
</equation>
<bodyText confidence="0.999989538461538">
The inference rules in Part 2 of Table 1 are novel
in that eTargets may be sentiments (e.g., NEG-
PAIR(Imam,insulting) means that Imam is nega-
tive toward the negative sentiment revealed by in-
sulting). The inference rules link sentiments to
sentiments and, transitively, link entities to entities
(e.g., from Imam to Rushdie to the Prophet).
To support such rules, more groundings of
ETARGET(y,t) are created in PSL2 than in PSL1.
For two opinions y1 and y2, if the target span of
y1 overlaps with the opinion span of y2, we cre-
ate ETARGET(y1,y2) as a ground atom represent-
ing that y2 is an eTarget of y1.
</bodyText>
<page confidence="0.995856">
184
</page>
<bodyText confidence="0.873355">
4.4 PSL Augmented with +/-Effect Events
(PSL3)
Finally, for PSL3, +/-effect event atoms and rules
are added to PSL2 for the inference of additional
sentiments.
According to (Deng et al., 2013), a +effect event
has positive effect on the theme (examples are
help, increase, and save), and a -effect event has
negative effect on the theme (examples are ob-
struct, decrease, and kill).6 We define the follow-
ing atoms to represent such events:
</bodyText>
<listItem confidence="0.9990665">
(7) +EFFECT(x): x is a +effect event
(8) -EFFECT(x): x is a -effect event
(9) AGENT(x,a): the agent of x is a
(10) THEME(x,h): the theme of x is h
</listItem>
<bodyText confidence="0.997419151515151">
Next we assign scores to these ground atoms.
+EFFECT(x) and -EFFECT(x): We use the
+/-effect sense-level lexicon (Choi and Wiebe,
2014)7 to extract the +/-effect events in each sen-
tence. The score of +EFFECT(x) is the fraction of
that word’s senses that are +effect senses accord-
ing to the lexicon, and the score of-EFFECT(x) is
the fraction of that word’s senses that are -effect
senses according to the lexicon. If a word does
not appear in the lexicon, we do not treat it as a +/-
effect event, and thus assign 0 to both +EFFECT(x)
and -EFFECT(x).
AGENT(x,a) and THEME(x,h): We consider all
nouns in the same or in sibling constituents of
a +/-effect event as potential agents or themes.
An SVM classifier is run to assign scores to
AGENT(x,a), and another SVM classifier is run to
assign scores to THEME(x,h). Both SVM clas-
sifiers are trained on a separate corpus, the +/-
effect corpus (Deng et al., 2013) used in (Deng et
al., 2014), which is annotated with +/-effect event,
agent, and theme spans. The features we use to
train the agent and theme classifier include uni-
gram, bigram and syntax information.
Generalizations of the inference rules used in
(Deng et al., 2014) are expressed in first-order
logic, shown in Part 3 of Table 1. Let us go
through an example inference for Ex(1), in partic-
ular, the inference that the countries are negative
toward Imam. Recall that we infer this because
the countries are negative toward the fatwa and it
is Imam who issued the fatwa. The rules support-
ing this inference are Rules 3.11 and 3.4 in Table
</bodyText>
<footnote confidence="0.999717333333333">
6In (Deng et al., 2013), such events are called good-
For/badFor events; they are later renamed as +/-effect events.
7Available at: http://mpqa.cs.pitt.edu/lexicons/effect lexicon/
</footnote>
<bodyText confidence="0.9993461">
1, where s is the countries, h is the fatwa, x is the
issue event, and a is Imam.
The application of Rule 3.11 can be explained
as follows. The countries are negative toward
the fatwa, and the issue event is a +effect event
with theme fatwa (the issue event is +effect for
the fatwa because it creates the fatwa; creation is
one type of +effect event identified in (Deng et al.,
2013)); thus, the countries are negative toward the
issue event.
</bodyText>
<equation confidence="0.780711">
NEGPAIR(countries, fatwa)
∧ THEME(issue, fatwa)
∧ +EFFECT(issue)
⇒ NEGPAIR(countries, issue) .
</equation>
<bodyText confidence="0.9997445">
The application of Rule 3.4 can be explained as
follows. The countries are negative toward the is-
sue event, and it is Imam who conducted the event;
thus, the countries are negative toward Imam.
</bodyText>
<equation confidence="0.990392333333333">
NEGPAIR(countries, issue)
∧ AGENT(issue, Imam)
⇒ NEGPAIR(countries, Imam) .
</equation>
<bodyText confidence="0.999958416666667">
Finally, to support the new inferences, more
groundings of ETARGET(y,t) are defined in PSL3.
For a +/-effect event x whose agent is a, if one of
x and a is an eTarget candidate of y, the other will
be added to the eTarget candidate set for y (senti-
ments toward both +effect and -effect events and
their agents have the same polarity according to
the rules (Deng et al., 2014)). For +effect event
x whose theme is h, if one of x and h is an eTar-
get candidate of y, the other is added to the eTar-
get candidate set for y (sentiments toward +effect
events and their themes have the same polarity).
</bodyText>
<sectionHeader confidence="0.998565" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998838">
We carry out experiments on the MPQA 3.0 cor-
pus. Currently, there are 70 documents, 1,634 sen-
tences, and 1,921 DS and ESEs in total. The to-
tal number of POSPAIR(s,t) and NEGPAIR(s,t) are
867 and 1,975, respectively. Though the PSL in-
ference does not need supervision and the SVM
classifier for agents and themes in Section 4.4 is
trained on a separate corpus, we still have to train
the eTarget SVM classifier to assign local scores
as described in Section 4.2. Thus, the experiments
are carried out using 5-fold cross validation. For
each fold test set, the eTarget classifier is trained
on the other folds. The trained classifier is then
run on the test set, and PSL inference is carried
out on the test set.
</bodyText>
<page confidence="0.997705">
185
</page>
<bodyText confidence="0.999988285714286">
In total, we have three methods for eTarget can-
didate selection (ET1, ET2, ET3) and three mod-
els for sentiment analysis (PSL1, PSL2, PSL3).
Baselines. Since each noun and verb may be an
eTarget, the first baseline (All NP/VP) regards all
the nouns and verbs as eTargets. The first baseline
estimates the difficulty of this task.
The second baseline (SVM) uses the SVM lo-
cal classification results from Section 4.2. The
score of ETARGET(y,t) is assigned by the SVM
classifier. Then it is normalized as input into PSL.
Before normalization, if the score assigned by the
SVM classifier is above 0, the SVM baseline con-
siders it as a correct eTarget.
</bodyText>
<subsectionHeader confidence="0.970589">
5.1 Evaluations
</subsectionHeader>
<bodyText confidence="0.999658181818182">
First, we examine the performance of the PSL
models on correctly recognizing eTargets of a par-
ticular opinion. This evaluation is carried out
on a subset of the corpus: we only examine the
opinions which are automatically extracted by the
span-based systems (S1, S2 and S3). If an opinion
expression in the gold standard is not extracted by
any span-based system, it is not input into PSL, so
PSL cannot possibly find its eTargets.
The second and third evaluations assess perfor-
mance of the PSL models on correctly extracting
positive and negative pairs. Note that our senti-
ment analysis system has the capability, through
inference, to recognize positive and negative pairs
even if corresponding opinion expressions are not
extracted. Thus, the second and third evaluations
are carried out on the entire corpus. The second
evaluation uses ET3, and compares PSL1, PSL2
and PSL3. The third evaluation uses PSL3 and
compares performance using ET1, ET2 and ET3.
The results for the other combinations follow the
same trends.
ETargets of An Opinion. According to the gold
standard in Section 3.1, each opinion has a set of
eTargets. But not all eTargets are equally impor-
tant. Thus, our first evaluation assesses the perfor-
mance of extracting the most important eTarget.
As introduced in Section 3.1, a span-based target
annotation of an opinion in MPQA 2.0 captures
the most important target this opinion is expressed
toward. Thus, the head of the target span can be
considered to be the most important eTarget of an
opinion. We model this as a ranking problem to
compare models. For an opinion y automatically
extracted by a span-based system, both the SVM
baseline and PSL assign scores to ETARGET(y,t).
We rank the eTargets according to the scores. Be-
cause the ALL NP/VP baseline does not assign
scores to the nouns and verbs, we do not compare
with that baseline in this ranking experiment. We
use the Precision@N evaluation metric. If the top
N eTargets of an opinion contain the head of tar-
get span, we consider it as a correct hit. The results
are in Table 2.
</bodyText>
<table confidence="0.999874">
Prec@1 Prec@3 Prec@5
SVM 0.0370 0.0556 0.0820
PSL1 0.5105 0.6905 0.7831
PSL2 0.5317 0.7486 0.7883
PSL3 0.5503 0.7434 0.8148
</table>
<tableCaption confidence="0.999324">
Table 2: Precision@N of Most Important ETarget.
</tableCaption>
<bodyText confidence="0.991293147058824">
Table 2 shows that SVM is poor at ranking
the most important eTarget. The PSL models are
much better, even PSL1, which does not include
any inference rules. This shows that SVM, which
only uses local features, cannot distinguish the
most important eTarget from the others. But the
PSL models consider all the opinions, and can rec-
ognize a true negative even if it ranks high in the
local results. The ability of PSL to rule out true
negative candidates will be repeatedly shown in
the later evaluations.
We not only evaluate the ability to recognize the
most important eTarget of a particular opinion, we
also evaluate the ability to extract all the eTargets
of that opinion. The F-measure of SVM is 0.2043,
while the F-measures of PSL1, PSL2 and PSL3
are 0.3135, 0.3239, and 0.3275, respectively. Cor-
rectly recognizing all the eTargets is difficult, but
all the PSL models are better than the baseline.
Positive Pairs and Negative Pairs. Now we
evaluate the performance in a stricter way. We
compare automatically extracted sets of sentiment
pairs: Pauto = {POSPAIR(s, t) &gt; 01 and Nauto =
{NEGPAIR(s, t) &gt; 01, against the gold standard
sets Pgold and Ngold. Table 3 shows the accura-
cies using ET3. Note that higher accuracies can
be achieved, as shown later. Here we use ET3 just
to show the trend of results.
As shown in Table 3, the low accuracy of base-
line All NP/VP shows that entity/event-level sen-
timent analysis is a difficult task. Even the SVM
baseline does not have good accuracy. Note that
the SVM baseline in Table 3 uses ET3. The base-
line classifies the heads of target spans and opin-
</bodyText>
<page confidence="0.995254">
186
</page>
<table confidence="0.9984105">
POSPAIR NEGPAIR
All NP/VP 0.1280 0.1654
SVM 0.0765 0.0670
PSL1 0.3356 0.3754
PSL2 0.3705 0.3705
PSL3 0.4315 0.3892
</table>
<tableCaption confidence="0.995411">
Table 3: Accuracy comparing PSL models (ET3
used for all)
</tableCaption>
<bodyText confidence="0.997449928571429">
ion spans, which are extracted by state-of-the-
art span-based sentiment analysis systems. This
shows the results from span-based sentiment anal-
ysis systems do not provide enough accurate in-
formation for the more fine-grained entity/event-
level sentiment analysis task. In contrast, PSL1
achieves much higher accuracy than the baselines.
PSL2 and PSL3, which add sentiment toward sen-
timent and +/-effect event inferences, give fur-
ther improvements. A reason is that SVM uses a
hard constraint to cut off many eTarget candidates,
while the PSL models take the scores as soft con-
straints.
A more critical reason is due to the definition
of accuracy: (TruePositive+TrueNegative)/All. A
significant benefit of using PSL is correctly recog-
nizing true negative eTarget candidates and elim-
inating them from the set. Interestingly, even
though both PSL2 and PSL3 introduce more eTar-
get candidates, both are able to recognize more
true negatives and improve the accuracy.
Note that F-measure does not count true nega-
tives. Precision is T P
T P +FP , and recall is T P
T P +F N ;
neither considers true negatives (TN). As shown
in Table 4, the increment of PSL model over base-
lines on F-measure is not as large as the increase
in accuracy. Comparing PSL2 and PSL3 to PSL1,
the inference rules largely increase recall but lower
precision. However, the accuracy in Table 3 keeps
growing. Thus, the biggest advantage of PSL
models is to correctly rule out true negative eTar-
gets. For the baselines, though the SVM baseline
has higher precision, it eliminates so many eTarget
candidates that the F-measure is not high.
ETarget Selection. To assess the methods for
eTarget selection, we run PSL3 (the fullest PSL
model) using each method in turn. The F-
measures and accuracies are listed in Table 5. The
F-measure of ET1 is slightly lower than the F-
measures of ET2 and ET3, while the accuracy of
</bodyText>
<table confidence="0.999949538461539">
Precision Recall F-measure
POSPAIR
All NP/VP 0.1481 0.4857 0.2270
SVM 0.3791 0.0870 0.1415
PSL1 0.2234 0.2687 0.2440
PSL2 0.1666 0.2738 0.2072
PSL3 0.1659 0.3523 0.2256
NEGPAIR
All NP/VP 0.1824 0.6408 0.2840
SVM 0.3568 0.0761 0.1254
PSL1 0.2857 0.3872 0.3288
PSL2 0.2772 0.3883 0.3235
PSL3 0.2586 0.4529 0.3292
</table>
<tableCaption confidence="0.99381">
Table 4: F-measure comparing PSL models (ET3
used for all)
</tableCaption>
<bodyText confidence="0.999603833333333">
ET1 is much better than the accuracies of ET2
and ET3. Again, this is because PSL recognizes
true negatives in the eTarget candidates. Since
ET1 considers more eTarget candidates, ET1 gives
PSL a greater opportunity to remove true nega-
tives, leading to an overall increase in accuracy.
</bodyText>
<table confidence="0.9970412">
POSPAIR NEGPAIR
F Acc. F Acc.
ET1 0.2192 0.4963 0.3157 0.4461
ET2 0.2374 0.4433 0.3261 0.3969
ET3 0.2256 0.4315 0.3295 0.3892
</table>
<tableCaption confidence="0.9899615">
Table 5: Comparison of eTarget selection methods
(PSL3 used for all)
</tableCaption>
<sectionHeader confidence="0.9981" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999940733333334">
This work builds upon state-of-the-art span-
based sentiment analysis systems to perform
entity/event-level sentiment analysis covering
both explicit and implicit sentiments expressed
among entities and events in text. Probabilis-
tic Soft Logic models incorporating explicit senti-
ments, inference rules and +/-effect event informa-
tion are able to jointly disambiguate the ambigui-
ties in the opinion frames and improve over base-
line accuracies in recognizing entity/event-level
sentiments.
Acknowledgements. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008. We thank the anonymous reviewers
for their helpful comments.
</bodyText>
<page confidence="0.996827">
187
</page>
<sectionHeader confidence="0.987361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998508198198198">
Stephen H Bach, Bert Huang, and Lise Getoor. 2013.
Learning latent groups with hinge-loss markov ran-
dom fields. In Inferning: ICML Workshop on Inter-
actions between Inference and Learning.
Stephen H. Bach, Matthias Broecheler, Bert Huang,
and Lise Getoor. 2015. Hinge-loss markov random
fields and probabilistic soft logic. arXiv:1505.04406
[cs.LG].
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1210–1219, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.
Yoonjung Choi and Janyce Wiebe. 2014. +/-
effectwordnet: Sense-level lexicon acquisition for
opinion inference. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1181–1191, Doha,
Qatar, October. Association for Computational Lin-
guistics.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 377–385, Gothenburg, Sweden, April. Asso-
ciation for Computational Linguistics.
Lingjia Deng and Janyce Wiebe. 2015. Mpqa 3.0:
An entity/event-level sentiment corpus. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1323–1328, Denver, Colorado, May–June. Associa-
tion for Computational Linguistics.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In ACL 2013 (short paper). Asso-
ciation for Computational Linguistics.
Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of im-
plicit sentiments via implicature constraints. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 79–88, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Sofia, Bulgaria, Angust. Association for Com-
putational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Bert Huang, Angelika Kimmig, Lise Getoor, and Jen-
nifer Golbeck. 2013. A flexible framework for
probabilistic models of social trust. In Social Com-
puting, Behavioral-Cultural Modeling and Predic-
tion, pages 265–273. Springer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Ben London, Sameh Khamis, Stephen H. Bach, Bert
Huang, Lise Getoor, and Larry Davis. 2013. Collec-
tive activity detection using hinge-loss Markov ran-
dom fields. In CVPR Workshop on Structured Pre-
diction: Tractability, Learning and Inference.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Alex Memory, Angelika Kimmig, Stephen Bach,
Louiqa Raschid, and Lise Getoor. 2012. Graph
summarization in annotated data using probabilistic
soft logic. In Proceedings of the 8th International
Workshop on Uncertainty Reasoning for the Seman-
tic Web (URSW 2012), volume 900, pages 75–86.
Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis,
Ion Androutsopoulos, John Pavlopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27–35.
Jay Pujara, Hui Miao, Lise Getoor, and William Cohen.
2013. Knowledge graph identification. In The Se-
mantic Web–ISWC 2013, pages 542–557. Springer.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
HLT-NAACL, pages 627–633.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
</reference>
<page confidence="0.98228">
188
</page>
<reference confidence="0.999721604651163">
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642. Citeseer.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-Perspective Question Answering using
the OpQA corpus. In Proceedings of the Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 923–930, Vancou-
ver, Canada.
Ivan Titov and Ryan T McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL, volume 8, pages 308–316. Cite-
seer.
Janyce Wiebe and Lingjia Deng. 2014. An account of
opinion implicatures. arXiv, 1404.6491[cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005a. Annotating expressions of opinions and
emotions in language. Language resources and
evaluation, 39(2-3):165–210.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005b. Annotating expressions of opinions and
emotions in language ann. Language Resources and
Evaluation, 39(2/3):164–210.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Po-
larity, and Attitudes of private states. Ph.D. the-
sis, Intelligent Systems Program, University of Pitts-
burgh.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In ACL (1),
pages 1640–1649.
Bishan Yang and Claire Cardie. 2014. Context-aware
learning for sentence-level sentiment analysis with
posterior regularization. In Proceedings of ACL.
Lei Zhang and Bing Liu. 2011. Identifying noun prod-
uct features that imply opinions. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 575–580, Portland, Oregon, USA, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.998926">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960556">
<title confidence="0.998846">Joint Prediction for Entity/Event-Level Sentiment using Probabilistic Soft Logic Models</title>
<author confidence="0.99225">Lingjia Deng Janyce</author>
<affiliation confidence="0.998159666666667">Intelligent Systems Program Intelligent Systems University of Pittsburgh lid29@pitt.edu Department of Computer University of</affiliation>
<email confidence="0.999101">wiebe@cs.pitt.edu</email>
<abstract confidence="0.997954076923077">In this work, we build an entity/event-level sentiment analysis system, which is able to recognize and infer both explicit and implicit sentiments toward entities and events in the text. We design Probabilistic Soft Logic models that integrate explicit sentiments, inference rules, and +/-effect event information (events that positively or negatively affect entities). The experiments show that the method is able to greatly improve over baseline accuracies in recognizing entity/event-level sentiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen H Bach</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>Learning latent groups with hinge-loss markov random fields.</title>
<date>2013</date>
<booktitle>In Inferning: ICML Workshop on Interactions between Inference and Learning.</booktitle>
<contexts>
<context position="10137" citStr="Bach et al., 2013" startWordPosition="1618" endWordPosition="1621">t events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of s, and sets P and N consisting of positive and negative sentiments, respectively. Each element in P is a tuple, representing a positive pair of two entities, (e1, e2) where e1, e2 ∈ E, and e1 is positive toward e2. A po</context>
</contexts>
<marker>Bach, Huang, Getoor, 2013</marker>
<rawString>Stephen H Bach, Bert Huang, and Lise Getoor. 2013. Learning latent groups with hinge-loss markov random fields. In Inferning: ICML Workshop on Interactions between Inference and Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen H Bach</author>
<author>Matthias Broecheler</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>Hinge-loss markov random fields and probabilistic soft logic.</title>
<date>2015</date>
<note>arXiv:1505.04406 [cs.LG].</note>
<contexts>
<context position="5763" citStr="Bach et al., 2015" startWordPosition="907" endWordPosition="910"> inference rules are defined and how they are expressed. For example, Zhang and Liu (2011) define linguistic templates to recognize phrases that express implicit sentiments, while previously we (Deng et al., 2014) represent a few simple rules as (in)equality constraints in Integer Linear Programming. In contrast to previous 2Note that the inferences are conversational implicatures; they are defeasible and may not go through in context (Deng et al., 2014; Wiebe and Deng, 2014). work, we propose a more general set of inference rules and encode them in a probabilistic soft logic (PSL) framework (Bach et al., 2015). We chose PSL because it is designed to have efficient inference and, as similar methods in Statistical Relational Learning do, it allows probabilistic models to be specified in first-order logic, an expressive and natural way to represent if-then rules, and it supports joint prediction. Joint prediction is critical for our task because it involves multiple, mutually constraining ambiguities (the source, polarity, and target). Thus, this work aims at detecting both implicit and explicit sentiments expressed by an entity toward another entity/event (i.e., an eTarget) within the sentence. The c</context>
<context position="9895" citStr="Bach et al., 2015" startWordPosition="1574" endWordPosition="1577">ted. Previously, we also propose a set of sentiment inference rules and develop a rule-based system to infer sentiments (Wiebe and Deng, 2014). However, the rule-based system requires all information regarding explicit sentiments and +/-effect events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, eve</context>
<context position="14679" citStr="Bach et al., 2015" startWordPosition="2384" endWordPosition="2387"> the score ETARGET(insulting, Prophet) &gt; 0 means that PSL considers Prophet to be an eTarget of insulting, while ETARGET(insulting, countries) = 0 means that PSL does not consider countries to be an eTarget of insulting. In this section, we first introduce PSL in Section 4.1. We then present three PSL models in turn. PSL1 (Section 4.2) aggregates span-based opinions into Pauto and Nauto. PSL2 (Section 4.3) adds sentiment inference rules to PSL1. For PSL3 (Section 4.4), rules involving +/-effect events are added to PSL2, resulting in the richest overall model. 4.1 Probabilistic Soft Logic PSL (Bach et al., 2015) uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models (Beltagy et al., 2014). As mentioned above, a PSL model is defined using a set of atoms to be grounded, and a set of weighted if-then rules in first-order logic. For example, friend(x,y) n votesFor(y,z) ⇒ votesFor(x,z) means that a person may vote for the same person as his/her friend. Each predicate in the rule is an atom (e.g., friend(x,y)). A ground atom is produced by replacing variables with const</context>
</contexts>
<marker>Bach, Broecheler, Huang, Getoor, 2015</marker>
<rawString>Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2015. Hinge-loss markov random fields and probabilistic soft logic. arXiv:1505.04406 [cs.LG].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Probabilistic soft logic for semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1210--1219</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="9918" citStr="Beltagy et al., 2014" startWordPosition="1578" endWordPosition="1581"> also propose a set of sentiment inference rules and develop a rule-based system to infer sentiments (Wiebe and Deng, 2014). However, the rule-based system requires all information regarding explicit sentiments and +/-effect events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of </context>
<context position="14896" citStr="Beltagy et al., 2014" startWordPosition="2413" endWordPosition="2416">lting. In this section, we first introduce PSL in Section 4.1. We then present three PSL models in turn. PSL1 (Section 4.2) aggregates span-based opinions into Pauto and Nauto. PSL2 (Section 4.3) adds sentiment inference rules to PSL1. For PSL3 (Section 4.4), rules involving +/-effect events are added to PSL2, resulting in the richest overall model. 4.1 Probabilistic Soft Logic PSL (Bach et al., 2015) uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models (Beltagy et al., 2014). As mentioned above, a PSL model is defined using a set of atoms to be grounded, and a set of weighted if-then rules in first-order logic. For example, friend(x,y) n votesFor(y,z) ⇒ votesFor(x,z) means that a person may vote for the same person as his/her friend. Each predicate in the rule is an atom (e.g., friend(x,y)). A ground atom is produced by replacing variables with constants (e.g., friend(Tom, Mary)). Each rule is associated with a weight, indicating the importance of this rule in the whole rule set. A key distinguishing feature of PSL is that each ground atom a has a soft, continuou</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014. Probabilistic soft logic for semantic textual similarity. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1210–1219, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoonjung Choi</author>
<author>Janyce Wiebe</author>
</authors>
<title>effectwordnet: Sense-level lexicon acquisition for opinion inference.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1181--1191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="25813" citStr="Choi and Wiebe, 2014" startWordPosition="4273" endWordPosition="4276">e added to PSL2 for the inference of additional sentiments. According to (Deng et al., 2013), a +effect event has positive effect on the theme (examples are help, increase, and save), and a -effect event has negative effect on the theme (examples are obstruct, decrease, and kill).6 We define the following atoms to represent such events: (7) +EFFECT(x): x is a +effect event (8) -EFFECT(x): x is a -effect event (9) AGENT(x,a): the agent of x is a (10) THEME(x,h): the theme of x is h Next we assign scores to these ground atoms. +EFFECT(x) and -EFFECT(x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014)7 to extract the +/-effect events in each sentence. The score of +EFFECT(x) is the fraction of that word’s senses that are +effect senses according to the lexicon, and the score of-EFFECT(x) is the fraction of that word’s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/- effect event, and thus assign 0 to both +EFFECT(x) and -EFFECT(x). AGENT(x,a) and THEME(x,h): We consider all nouns in the same or in sibling constituents of a +/-effect event as potential agents or themes. An SVM classifier is run to assign scores to</context>
</contexts>
<marker>Choi, Wiebe, 2014</marker>
<rawString>Yoonjung Choi and Janyce Wiebe. 2014. +/-effectwordnet: Sense-level lexicon acquisition for opinion inference. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181–1191, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="21474" citStr="Cortes and Vapnik, 1995" startWordPosition="3584" endWordPosition="3588">n. In addition, for the eTarget candidate set extracted by ET2, or ET3, we run the Stanford coreference system (Manning et al., 2014; Recasens et al., 2013; Lee et al., 2013) to expand the set in two ways. First, for each eTarget candidate t, the co-reference system extracts the entities that co-refer with t. We add the referring entities into the candidate set. Second, the co-reference system extracts words which the Stanford system judges to be entities, regardless of whether they have any referent or not. We add this set of entities to the candidate set as well. We train an SVM classifier (Cortes and Vapnik, 1995) to assign a score to the ground atom ETARGET(y,t). Syntactic features describing the 5The head of a phrase is extracted by the Collins head finder in the Stanford parser (Manning et al., 2014). 183 Part 1. Aggregation Rules. 1.1 SOURCE(y,s) ∧ ETARGET(y,t) ∧ POS(y) ⇒ POSPAIR(s,t) 1.2 SOURCE(y,s) ∧ ETARGET(y,t) ∧ NEG(y) ⇒ NEGPAIR(s,t) Part 2. Inference Rules. 2.1 POSPAIR(s1,y2) ∧ SOURCE(y2,s2) ⇒ POSPAIR(s1,s2) 2.2 POSPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ POS(y2) ⇒ POSPAIR(s1,t2) 2.3 POSPAIR(s1,y2) ∧ ETARGET(y2,t2) ∧ NEG(y2) ⇒ NEGPAIR(s1,t2) 2.4 NEGPAIR(s1,y2) ∧ SOURCE(y2,s2) ⇒ NEGPAIR(s1,s2) 2.5 NEGPA</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Sentiment propagation via implicature constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>377--385</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="8517" citStr="Deng and Wiebe, 2014" startWordPosition="1355" endWordPosition="1358">Donald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014) develop rules and models to infer sentiments related to +/-effect events, events that positively or negatively affect entities. That work assumes that the source is only the writer, and the targets are limited to entities that participate in +/-effect events. Further, our previous models all require certain manual (oracle) annotations to be input. In this work we use an expanded set of more general rules. We allow sources other than the writer, and targets that may be any entity or event. In fact, under our new rules, the targets of sentiments may be other sentiments; we m</context>
</contexts>
<marker>Deng, Wiebe, 2014</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 377–385, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Mpqa 3.0: An entity/event-level sentiment corpus.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1323--1328</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="1843" citStr="Deng and Wiebe, 2015" startWordPosition="269" endWordPosition="272">he target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa. There are several sentiment expressions annotated in MPQA. In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upon him (4), respectively. Imam is negative toward Salman Rushdie and the insulting event, as revealed by the expression issued the fatwa against (2). And Salman Rus</context>
<context position="11244" citStr="Deng and Wiebe, 2015" startWordPosition="1808" endWordPosition="1811">tuple, representing a positive pair of two entities, (e1, e2) where e1, e2 ∈ E, and e1 is positive toward e2. A positive pair (e1,e2) aggregates all the positive sentiments from e1 to e2 in the sentence. N is the corresponding set for negative pairs. The goal of this work is to automatically recognize a set of positive pairs (Pauto) and a set of negative pairs (Nauto). We compare the system output (Pauto ∪ Nauto) against the gold standard (Pgold ∪ Ngold) for each sentence. 3.1 Gold Standard Corpus: MPQA 3.0 MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015).4 It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agencies, and covers a wide range of topics. In both MPQA 2.0 and 3.0, the top-level annotations include direct subjectives (DS). Each DS has a nested-source annotation. Each DS has one or more attitude links, meaning that all of the attitudes share the same nested source. The attitudes differ from one another in their attitude types, polarities, and/or targets. Moreover, both corpora contain expressive subjective element (</context>
</contexts>
<marker>Deng, Wiebe, 2015</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2015. Mpqa 3.0: An entity/event-level sentiment corpus. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1323–1328, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Yoonjung Choi</author>
<author>Janyce Wiebe</author>
</authors>
<title>Benefactive/malefactive event and writer attitude annotation.</title>
<date>2013</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="8495" citStr="Deng et al., 2013" startWordPosition="1351" endWordPosition="1354"> 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014) develop rules and models to infer sentiments related to +/-effect events, events that positively or negatively affect entities. That work assumes that the source is only the writer, and the targets are limited to entities that participate in +/-effect events. Further, our previous models all require certain manual (oracle) annotations to be input. In this work we use an expanded set of more general rules. We allow sources other than the writer, and targets that may be any entity or event. In fact, under our new rules, the targets of sentiments may be </context>
<context position="25284" citStr="Deng et al., 2013" startWordPosition="4180" endWordPosition="4183">ed by insulting). The inference rules link sentiments to sentiments and, transitively, link entities to entities (e.g., from Imam to Rushdie to the Prophet). To support such rules, more groundings of ETARGET(y,t) are created in PSL2 than in PSL1. For two opinions y1 and y2, if the target span of y1 overlaps with the opinion span of y2, we create ETARGET(y1,y2) as a ground atom representing that y2 is an eTarget of y1. 184 4.4 PSL Augmented with +/-Effect Events (PSL3) Finally, for PSL3, +/-effect event atoms and rules are added to PSL2 for the inference of additional sentiments. According to (Deng et al., 2013), a +effect event has positive effect on the theme (examples are help, increase, and save), and a -effect event has negative effect on the theme (examples are obstruct, decrease, and kill).6 We define the following atoms to represent such events: (7) +EFFECT(x): x is a +effect event (8) -EFFECT(x): x is a -effect event (9) AGENT(x,a): the agent of x is a (10) THEME(x,h): the theme of x is h Next we assign scores to these ground atoms. +EFFECT(x) and -EFFECT(x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014)7 to extract the +/-effect events in each sentence. The score of +EFFEC</context>
<context position="26588" citStr="Deng et al., 2013" startWordPosition="4411" endWordPosition="4414">xicon, and the score of-EFFECT(x) is the fraction of that word’s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/- effect event, and thus assign 0 to both +EFFECT(x) and -EFFECT(x). AGENT(x,a) and THEME(x,h): We consider all nouns in the same or in sibling constituents of a +/-effect event as potential agents or themes. An SVM classifier is run to assign scores to AGENT(x,a), and another SVM classifier is run to assign scores to THEME(x,h). Both SVM classifiers are trained on a separate corpus, the +/- effect corpus (Deng et al., 2013) used in (Deng et al., 2014), which is annotated with +/-effect event, agent, and theme spans. The features we use to train the agent and theme classifier include unigram, bigram and syntax information. Generalizations of the inference rules used in (Deng et al., 2014) are expressed in first-order logic, shown in Part 3 of Table 1. Let us go through an example inference for Ex(1), in particular, the inference that the countries are negative toward Imam. Recall that we infer this because the countries are negative toward the fatwa and it is Imam who issued the fatwa. The rules supporting this i</context>
</contexts>
<marker>Deng, Choi, Wiebe, 2013</marker>
<rawString>Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude annotation. In ACL 2013 (short paper). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
<author>Yoonjung Choi</author>
</authors>
<title>Joint inference and disambiguation of implicit sentiments via implicature constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>79--88</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="5358" citStr="Deng et al., 2014" startWordPosition="841" endWordPosition="844">in P and N, where the sources are entities (or the writer) and the targets are entities and events. Previous work in sentiment analysis mainly focuses on detecting explicit opinions. Recently there is emerging focus on sentiment inference, which recognizes implicit sentiments by inferring them from explicit sentiments via inference rules. Current works in sentiment inference differ on how the sentiment inference rules are defined and how they are expressed. For example, Zhang and Liu (2011) define linguistic templates to recognize phrases that express implicit sentiments, while previously we (Deng et al., 2014) represent a few simple rules as (in)equality constraints in Integer Linear Programming. In contrast to previous 2Note that the inferences are conversational implicatures; they are defeasible and may not go through in context (Deng et al., 2014; Wiebe and Deng, 2014). work, we propose a more general set of inference rules and encode them in a probabilistic soft logic (PSL) framework (Bach et al., 2015). We chose PSL because it is designed to have efficient inference and, as similar methods in Statistical Relational Learning do, it allows probabilistic models to be specified in first-order logi</context>
<context position="8537" citStr="Deng et al., 2014" startWordPosition="1359" endWordPosition="1362">ted in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014) develop rules and models to infer sentiments related to +/-effect events, events that positively or negatively affect entities. That work assumes that the source is only the writer, and the targets are limited to entities that participate in +/-effect events. Further, our previous models all require certain manual (oracle) annotations to be input. In this work we use an expanded set of more general rules. We allow sources other than the writer, and targets that may be any entity or event. In fact, under our new rules, the targets of sentiments may be other sentiments; we model such novel “sen</context>
<context position="26616" citStr="Deng et al., 2014" startWordPosition="4417" endWordPosition="4420">CT(x) is the fraction of that word’s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/- effect event, and thus assign 0 to both +EFFECT(x) and -EFFECT(x). AGENT(x,a) and THEME(x,h): We consider all nouns in the same or in sibling constituents of a +/-effect event as potential agents or themes. An SVM classifier is run to assign scores to AGENT(x,a), and another SVM classifier is run to assign scores to THEME(x,h). Both SVM classifiers are trained on a separate corpus, the +/- effect corpus (Deng et al., 2013) used in (Deng et al., 2014), which is annotated with +/-effect event, agent, and theme spans. The features we use to train the agent and theme classifier include unigram, bigram and syntax information. Generalizations of the inference rules used in (Deng et al., 2014) are expressed in first-order logic, shown in Part 3 of Table 1. Let us go through an example inference for Ex(1), in particular, the inference that the countries are negative toward Imam. Recall that we infer this because the countries are negative toward the fatwa and it is Imam who issued the fatwa. The rules supporting this inference are Rules 3.11 and </context>
<context position="28592" citStr="Deng et al., 2014" startWordPosition="4752" endWordPosition="4755"> can be explained as follows. The countries are negative toward the issue event, and it is Imam who conducted the event; thus, the countries are negative toward Imam. NEGPAIR(countries, issue) ∧ AGENT(issue, Imam) ⇒ NEGPAIR(countries, Imam) . Finally, to support the new inferences, more groundings of ETARGET(y,t) are defined in PSL3. For a +/-effect event x whose agent is a, if one of x and a is an eTarget candidate of y, the other will be added to the eTarget candidate set for y (sentiments toward both +effect and -effect events and their agents have the same polarity according to the rules (Deng et al., 2014)). For +effect event x whose theme is h, if one of x and h is an eTarget candidate of y, the other is added to the eTarget candidate set for y (sentiments toward +effect events and their themes have the same polarity). 5 Experiments We carry out experiments on the MPQA 3.0 corpus. Currently, there are 70 documents, 1,634 sentences, and 1,921 DS and ESEs in total. The total number of POSPAIR(s,t) and NEGPAIR(s,t) are 867 and 1,975, respectively. Though the PSL inference does not need supervision and the SVM classifier for agents and themes in Section 4.4 is trained on a separate corpus, we stil</context>
</contexts>
<marker>Deng, Wiebe, Choi, 2014</marker>
<rawString>Lingjia Deng, Janyce Wiebe, and Yoonjung Choi. 2014. Joint inference and disambiguation of implicit sentiments via implicature constraints. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 79–88, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Sak Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>2</volume>
<institution>Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.</institution>
<contexts>
<context position="8292" citStr="Feng et al., 2013" startWordPosition="1317" endWordPosition="1320">get in the entity/event-level task may be any noun or verb. In terms of sources, previous work in sentiment analysis trained on review data assumes that the source is the writer of the review (Hu and Liu, 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014) develop rules and models to infer sentiments related to +/-effect events, events that positively or negatively affect entities. That work assumes that the source is only the writer, and the targets are limited to entities that participate in +/-effect events. Further, our previous models all require certain manual (oracle) annotations to be input. In t</context>
</contexts>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19587" citStr="Finkel et al., 2005" startWordPosition="3251" endWordPosition="3254">core assigned to the ground atom is the proportion of the words in the opinion span that are included in the subjectivity lexicon. SOURCE(y,s): S1 extracts the source of each opinion, S2 does not extract the source, and S3 assumes the source is always the writer. Thus, for an opinion y, if the source s is assigned by S1, a ground atom SOURCE(y,s) is created with score 1.0. Otherwise, if S3 extracts opinion y, a ground atom SOURCE(y,writer) is created with score 1.0 (since S3 assumes the source is always the writer). Otherwise, we run the Stanford named entity recognizer (Manning et al., 2014; Finkel et al., 2005) to extract named entities in the sentence. The nearest named entity to the opinion span on the dependency parse graph will be treated as the source. The score is the reciprocal of the length of the path between the opinion span and the source span in the dependency parse. ETARGET(y,t): Though each eTarget is an entity or event, it is difficult to determine which nouns and verbs should be considered. Taking into consideration the trade-off between precision and recall, we experimented with three methods to select eTarget candidates. For each opinion y, a ground atom ETARGET(y,t) is created for</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7883" citStr="Hu and Liu, 2004" startWordPosition="1252" endWordPosition="1255">portion of an extracted span against the gold standard phrase (Yang and Cardie, 2013), while the eTarget in an entity/event-level system is evaluated against the exact word (i.e., head of NP/VP) in the gold standard. It is a stricter evaluation. While the targets in aspect-based sentiment analysis are often entity targets, they are mainly product aspects, which are a predefined set.3 In contrast, the target in the entity/event-level task may be any noun or verb. In terms of sources, previous work in sentiment analysis trained on review data assumes that the source is the writer of the review (Hu and Liu, 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert Huang</author>
<author>Angelika Kimmig</author>
<author>Lise Getoor</author>
<author>Jennifer Golbeck</author>
</authors>
<title>A flexible framework for probabilistic models of social trust.</title>
<date>2013</date>
<booktitle>In Social Computing, Behavioral-Cultural Modeling and Prediction,</booktitle>
<pages>265--273</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10157" citStr="Huang et al., 2013" startWordPosition="1622" endWordPosition="1625">ided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of s, and sets P and N consisting of positive and negative sentiments, respectively. Each element in P is a tuple, representing a positive pair of two entities, (e1, e2) where e1, e2 ∈ E, and e1 is positive toward e2. A positive pair (e1,e2) </context>
</contexts>
<marker>Huang, Kimmig, Getoor, Golbeck, 2013</marker>
<rawString>Bert Huang, Angelika Kimmig, Lise Getoor, and Jennifer Golbeck. 2013. A flexible framework for probabilistic models of social trust. In Social Computing, Behavioral-Cultural Modeling and Prediction, pages 265–273. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="21024" citStr="Lee et al., 2013" startWordPosition="3508" endWordPosition="3511">xtracted by systems S1, S2 and S3. We hypothesized that ET2 would be useful because most of the eTargets in MPQA 3.0 appear within the opinion or the target spans of MPQA 2.0. ET3 considers the heads of the target and opinion spans that are automatically extracted by systems S1, S2 and S3.5 ET3 also considers the heads of siblings of target spans and opinion spans. Among the three methods, ET3 has the lowest recall but the highest precision. In addition, for the eTarget candidate set extracted by ET2, or ET3, we run the Stanford coreference system (Manning et al., 2014; Recasens et al., 2013; Lee et al., 2013) to expand the set in two ways. First, for each eTarget candidate t, the co-reference system extracts the entities that co-refer with t. We add the referring entities into the candidate set. Second, the co-reference system extracts words which the Stanford system judges to be entities, regardless of whether they have any referent or not. We add this set of entities to the candidate set as well. We train an SVM classifier (Cortes and Vapnik, 1995) to assign a score to the ground atom ETARGET(y,t). Syntactic features describing the 5The head of a phrase is extracted by the Collins head finder in</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben London</author>
<author>Sameh Khamis</author>
<author>Stephen H Bach</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
<author>Larry Davis</author>
</authors>
<title>Collective activity detection using hinge-loss Markov random fields.</title>
<date>2013</date>
<booktitle>In CVPR Workshop on Structured Prediction: Tractability, Learning and Inference.</booktitle>
<contexts>
<context position="10097" citStr="London et al., 2013" startWordPosition="1610" endWordPosition="1613">regarding explicit sentiments and +/-effect events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of s, and sets P and N consisting of positive and negative sentiments, respectively. Each element in P is a tuple, representing a positive pair of two entities, (e1, e2) where e1, e2</context>
</contexts>
<marker>London, Khamis, Bach, Huang, Getoor, Davis, 2013</marker>
<rawString>Ben London, Sameh Khamis, Stephen H. Bach, Bert Huang, Lise Getoor, and Larry Davis. 2013. Collective activity detection using hinge-loss Markov random fields. In CVPR Workshop on Structured Prediction: Tractability, Learning and Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="19565" citStr="Manning et al., 2014" startWordPosition="3247" endWordPosition="3250">ne its polarity. The score assigned to the ground atom is the proportion of the words in the opinion span that are included in the subjectivity lexicon. SOURCE(y,s): S1 extracts the source of each opinion, S2 does not extract the source, and S3 assumes the source is always the writer. Thus, for an opinion y, if the source s is assigned by S1, a ground atom SOURCE(y,s) is created with score 1.0. Otherwise, if S3 extracts opinion y, a ground atom SOURCE(y,writer) is created with score 1.0 (since S3 assumes the source is always the writer). Otherwise, we run the Stanford named entity recognizer (Manning et al., 2014; Finkel et al., 2005) to extract named entities in the sentence. The nearest named entity to the opinion span on the dependency parse graph will be treated as the source. The score is the reciprocal of the length of the path between the opinion span and the source span in the dependency parse. ETARGET(y,t): Though each eTarget is an entity or event, it is difficult to determine which nouns and verbs should be considered. Taking into consideration the trade-off between precision and recall, we experimented with three methods to select eTarget candidates. For each opinion y, a ground atom ETARG</context>
<context position="20982" citStr="Manning et al., 2014" startWordPosition="3500" endWordPosition="3503">ns and opinion spans that are automatically extracted by systems S1, S2 and S3. We hypothesized that ET2 would be useful because most of the eTargets in MPQA 3.0 appear within the opinion or the target spans of MPQA 2.0. ET3 considers the heads of the target and opinion spans that are automatically extracted by systems S1, S2 and S3.5 ET3 also considers the heads of siblings of target spans and opinion spans. Among the three methods, ET3 has the lowest recall but the highest precision. In addition, for the eTarget candidate set extracted by ET2, or ET3, we run the Stanford coreference system (Manning et al., 2014; Recasens et al., 2013; Lee et al., 2013) to expand the set in two ways. First, for each eTarget candidate t, the co-reference system extracts the entities that co-refer with t. We add the referring entities into the candidate set. Second, the co-reference system extracts words which the Stanford system judges to be entities, regardless of whether they have any referent or not. We add this set of entities to the candidate set as well. We train an SVM classifier (Cortes and Vapnik, 1995) to assign a score to the ground atom ETARGET(y,t). Syntactic features describing the 5The head of a phrase </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Memory</author>
<author>Angelika Kimmig</author>
<author>Stephen Bach</author>
<author>Louiqa Raschid</author>
<author>Lise Getoor</author>
</authors>
<title>Graph summarization in annotated data using probabilistic soft logic.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 2012),</booktitle>
<volume>900</volume>
<pages>75--86</pages>
<contexts>
<context position="10179" citStr="Memory et al., 2012" startWordPosition="1626" endWordPosition="1629">mation by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of s, and sets P and N consisting of positive and negative sentiments, respectively. Each element in P is a tuple, representing a positive pair of two entities, (e1, e2) where e1, e2 ∈ E, and e1 is positive toward e2. A positive pair (e1,e2) aggregates all the pos</context>
</contexts>
<marker>Memory, Kimmig, Bach, Raschid, Getoor, 2012</marker>
<rawString>Alex Memory, Angelika Kimmig, Stephen Bach, Louiqa Raschid, and Lise Getoor. 2012. Graph summarization in annotated data using probabilistic soft logic. In Proceedings of the 8th International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 2012), volume 900, pages 75–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Haris Papageorgiou</author>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
<author>John Pavlopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<contexts>
<context position="1360" citStr="Pontiki et al., 2014" startWordPosition="191" endWordPosition="194">r baseline accuracies in recognizing entity/event-level sentiments. 1 Introduction There are increasing numbers of opinions expressed in various genres, including reviews, newswire, editorials, and forums. While much early work was at the document or sentence level, to fully understand and utilize opinions, researchers are increasingly carrying out more finegrained sentiment analysis to extract components of opinion frames: the source (whose sentiment is it), the polarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 th</context>
</contexts>
<marker>Pontiki, Papageorgiou, Galanis, Androutsopoulos, Pavlopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Pujara</author>
<author>Hui Miao</author>
<author>Lise Getoor</author>
<author>William Cohen</author>
</authors>
<title>Knowledge graph identification.</title>
<date>2013</date>
<booktitle>In The Semantic Web–ISWC 2013,</booktitle>
<pages>542--557</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10118" citStr="Pujara et al., 2013" startWordPosition="1614" endWordPosition="1617">ntiments and +/-effect events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other machine learning tasks in recent years (Beltagy et al., 2014; London et al., 2013; Pujara et al., 2013; Bach et al., 2013; Huang et al., 2013; Memory et al., 2012). Previously, PSL has not been applied to entity/event-level sentiment analysis. 3 Task Definition In this section, we introduce the definition of the entity/event-level sentiment analysis task, followed by a description of the gold standard corpus. For each sentence s, we define a set E consisting of entities, events, and the writer of s, and sets P and N consisting of positive and negative sentiments, respectively. Each element in P is a tuple, representing a positive pair of two entities, (e1, e2) where e1, e2 ∈ E, and e1 is posit</context>
</contexts>
<marker>Pujara, Miao, Getoor, Cohen, 2013</marker>
<rawString>Jay Pujara, Hui Miao, Lise Getoor, and William Cohen. 2013. Knowledge graph identification. In The Semantic Web–ISWC 2013, pages 542–557. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In HLT-NAACL, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="18310" citStr="Socher et al., 2013" startWordPosition="3013" endWordPosition="3016">among entities and events (POSPAIR(s,t) and NEGPAIR(s,t)) in the sentence. Next, we turn to assigning local scores to ground atoms (3)-(6). POS(y) and NEG(y): We build upon three spanbased sentiment analysis systems. The first, S1 (Yang and Cardie, 2013), and the second, S2 (Yang and Cardie, 2014), are both trained on MPQA 2.0, which does not contain any eTarget annotations. S1 extracts triples of (source span, opinion span, target span), but does not extract opinion polarities. S2 extracts opinion spans and opinion polarities, but it does not extract sources or targets. The third system, S3 (Socher et al., 2013), is trained on movie review data. It extracts opinion spans and polarities. The source is always assumed to be the writer. We take the union set of opinions extracted by S1, S2 and S3. For each opinion y, a ground atom is created, depending on the polarity (POS(y) if y is positive and NEG(y) is y is negative). The polarity is determined as follows. If S2 assigns a polarity to y, then that polarity is used. If S3 but not S2 assigns a polarity to y, then S3’s polarity is used. In both cases, the score assigned to the ground atom is 1.0. If neither S2 nor S3 assigns a polarity to y, we use the M</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multi-Perspective Question Answering using the OpQA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005),</booktitle>
<pages>923--930</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1680" citStr="Stoyanov et al., 2005" startWordPosition="240" endWordPosition="243">s are increasingly carrying out more finegrained sentiment analysis to extract components of opinion frames: the source (whose sentiment is it), the polarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa. There are several sentiment expressions annotated in MPQA. In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upo</context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-Perspective Question Answering using the OpQA corpus. In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005), pages 923–930, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan T McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>308--316</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7910" citStr="Titov and McDonald, 2008" startWordPosition="1256" endWordPosition="1259">acted span against the gold standard phrase (Yang and Cardie, 2013), while the eTarget in an entity/event-level system is evaluated against the exact word (i.e., head of NP/VP) in the gold standard. It is a stricter evaluation. While the targets in aspect-based sentiment analysis are often entity targets, they are mainly product aspects, which are a predefined set.3 In contrast, the target in the entity/event-level task may be any noun or verb. In terms of sources, previous work in sentiment analysis trained on review data assumes that the source is the writer of the review (Hu and Liu, 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wieb</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan T McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In ACL, volume 8, pages 308–316. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Lingjia Deng</author>
</authors>
<title>An account of opinion implicatures. arXiv,</title>
<date>2014</date>
<pages>1404--6491</pages>
<contexts>
<context position="5625" citStr="Wiebe and Deng, 2014" startWordPosition="883" endWordPosition="886">t sentiments by inferring them from explicit sentiments via inference rules. Current works in sentiment inference differ on how the sentiment inference rules are defined and how they are expressed. For example, Zhang and Liu (2011) define linguistic templates to recognize phrases that express implicit sentiments, while previously we (Deng et al., 2014) represent a few simple rules as (in)equality constraints in Integer Linear Programming. In contrast to previous 2Note that the inferences are conversational implicatures; they are defeasible and may not go through in context (Deng et al., 2014; Wiebe and Deng, 2014). work, we propose a more general set of inference rules and encode them in a probabilistic soft logic (PSL) framework (Bach et al., 2015). We chose PSL because it is designed to have efficient inference and, as similar methods in Statistical Relational Learning do, it allows probabilistic models to be specified in first-order logic, an expressive and natural way to represent if-then rules, and it supports joint prediction. Joint prediction is critical for our task because it involves multiple, mutually constraining ambiguities (the source, polarity, and target). Thus, this work aims at detect</context>
<context position="9420" citStr="Wiebe and Deng, 2014" startWordPosition="1504" endWordPosition="1507">urther, our previous models all require certain manual (oracle) annotations to be input. In this work we use an expanded set of more general rules. We allow sources other than the writer, and targets that may be any entity or event. In fact, under our new rules, the targets of sentiments may be other sentiments; we model such novel “sentiment toward sentiment” structures in Section 4.3. Finally, our method requiring no manual annotations as input when the inference is conducted. Previously, we also propose a set of sentiment inference rules and develop a rule-based system to infer sentiments (Wiebe and Deng, 2014). However, the rule-based system requires all information regarding explicit sentiments and +/-effect events to be provided as oracle information by manual annotations. Probabilistic Soft Logic. Probabilistic Soft Logic (PSL) is a variation of Markov Logic Networks, which is a framework for probabilistic logic that employs weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks) (Bach et al., 2015; Beltagy et al., 2014). PSL is a new statistical relational learning method that has been applied to many NLP and other mach</context>
<context position="23703" citStr="Wiebe and Deng, 2014" startWordPosition="3918" endWordPosition="3921">span; (2) the unigrams and bigrams on the path from the eTarget to the opinion/target span in the constituency parse tree; and (3) the unigrams and bigrams on the path from the eTarget to the opinion/target word in the dependency parse graph. We normalize the SVM scores into the range of a ground atom score, [0,1]. 4.3 PSL for Sentiment Inference (PSL2) The two rules defined in Section 4.2 aggregate various opinions into positive pairs and negative pairs, but inferences have not yet been introduced. PSL2 is defined using the atoms and rules in PSL1. But it also includes some rules defined in (Wiebe and Deng, 2014), represented here in first-order logic in Part 2 of Table 1. Let us go through an example inference for Ex(1), in particular, the inference that Imam is positive toward the Prophet. Rule 2.6 supports this inference. Recall the two explicit sentiments: Imam is negative toward the insulting sentiment (revealed by issued the fatwa against), and Rushdie is negative toward the Prophet (revealed by insulting). Thus, we can instantiate Rule 2.6, where s1 is Imam, y2 is the negative sentiment (insulting), and t2 is the Prophet. The inference is: since Imam is negative that there is any negative opini</context>
</contexts>
<marker>Wiebe, Deng, 2014</marker>
<rawString>Janyce Wiebe and Lingjia Deng. 2014. An account of opinion implicatures. arXiv, 1404.6491[cs.CL].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1805" citStr="Wiebe et al., 2005" startWordPosition="263" endWordPosition="266">ntiment is it), the polarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa. There are several sentiment expressions annotated in MPQA. In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upon him (4), respectively. Imam is negative toward Salman Rushdie and the insulting event, as revealed by the expression issued</context>
<context position="3604" citStr="Wiebe et al., 2005" startWordPosition="565" endWordPosition="568">, countries), (countries, fatwa)}.1 An (ideal) explicit sentiment analysis system is expected to extract the above sentiments expressed by (1)-(6). However, there are many more sentiments communicated by the writer but not expressed via explicit expressions. First, Imam is positive toward the Prophet, because Rushdie insults the Prophet and Imam is angry that he does 1Sources in MPQA are nested, having the form (writer) or (writer, S1, ... , S.). This work only deals with the rightmost source, writer or S.. Also, actions like issuing a fatwa are treated the same as private states. Please see (Wiebe et al., 2005a). 179 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 179–189, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Explicit and implicit sentiments in Ex(1). so. Second, the writer is negative toward Rushdie, because the writer is positive toward the Prophet but Rushdie insults him! Also, the writer is probably positive toward the fatwa since it is against Rushdie. Third, the countries are probably negative toward Imam, because the countries are negative toward fatwa and it is Imam who issued the fa</context>
<context position="11303" citStr="Wiebe et al., 2005" startWordPosition="1821" endWordPosition="1824">where e1, e2 ∈ E, and e1 is positive toward e2. A positive pair (e1,e2) aggregates all the positive sentiments from e1 to e2 in the sentence. N is the corresponding set for negative pairs. The goal of this work is to automatically recognize a set of positive pairs (Pauto) and a set of negative pairs (Nauto). We compare the system output (Pauto ∪ Nauto) against the gold standard (Pgold ∪ Ngold) for each sentence. 3.1 Gold Standard Corpus: MPQA 3.0 MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015).4 It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agencies, and covers a wide range of topics. In both MPQA 2.0 and 3.0, the top-level annotations include direct subjectives (DS). Each DS has a nested-source annotation. Each DS has one or more attitude links, meaning that all of the attitudes share the same nested source. The attitudes differ from one another in their attitude types, polarities, and/or targets. Moreover, both corpora contain expressive subjective element (ESE) annotations, which pinpoint specific expressions used </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005a. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1805" citStr="Wiebe et al., 2005" startWordPosition="263" endWordPosition="266">ntiment is it), the polarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa. There are several sentiment expressions annotated in MPQA. In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upon him (4), respectively. Imam is negative toward Salman Rushdie and the insulting event, as revealed by the expression issued</context>
<context position="3604" citStr="Wiebe et al., 2005" startWordPosition="565" endWordPosition="568">, countries), (countries, fatwa)}.1 An (ideal) explicit sentiment analysis system is expected to extract the above sentiments expressed by (1)-(6). However, there are many more sentiments communicated by the writer but not expressed via explicit expressions. First, Imam is positive toward the Prophet, because Rushdie insults the Prophet and Imam is angry that he does 1Sources in MPQA are nested, having the form (writer) or (writer, S1, ... , S.). This work only deals with the rightmost source, writer or S.. Also, actions like issuing a fatwa are treated the same as private states. Please see (Wiebe et al., 2005a). 179 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 179–189, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Explicit and implicit sentiments in Ex(1). so. Second, the writer is negative toward Rushdie, because the writer is positive toward the Prophet but Rushdie insults him! Also, the writer is probably positive toward the fatwa since it is against Rushdie. Third, the countries are probably negative toward Imam, because the countries are negative toward fatwa and it is Imam who issued the fa</context>
<context position="11303" citStr="Wiebe et al., 2005" startWordPosition="1821" endWordPosition="1824">where e1, e2 ∈ E, and e1 is positive toward e2. A positive pair (e1,e2) aggregates all the positive sentiments from e1 to e2 in the sentence. N is the corresponding set for negative pairs. The goal of this work is to automatically recognize a set of positive pairs (Pauto) and a set of negative pairs (Nauto). We compare the system output (Pauto ∪ Nauto) against the gold standard (Pgold ∪ Ngold) for each sentence. 3.1 Gold Standard Corpus: MPQA 3.0 MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015).4 It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agencies, and covers a wide range of topics. In both MPQA 2.0 and 3.0, the top-level annotations include direct subjectives (DS). Each DS has a nested-source annotation. Each DS has one or more attitude links, meaning that all of the attitudes share the same nested source. The attitudes differ from one another in their attitude types, polarities, and/or targets. Moreover, both corpora contain expressive subjective element (ESE) annotations, which pinpoint specific expressions used </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005b. Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of private states.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="1820" citStr="Wilson, 2007" startWordPosition="267" endWordPosition="268">olarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rushdie for insulting 3 the Prophet ( peace be upon him 4), the countries that are so-called 5 supporters of human rights protested against 6 the fatwa. There are several sentiment expressions annotated in MPQA. In the first clause, the writer is positive toward Imam and Prophet as expressed by may God be satisfied with him (1) and peace be upon him (4), respectively. Imam is negative toward Salman Rushdie and the insulting event, as revealed by the expression issued the fatwa agai</context>
<context position="11319" citStr="Wilson, 2007" startWordPosition="1825" endWordPosition="1826"> e1 is positive toward e2. A positive pair (e1,e2) aggregates all the positive sentiments from e1 to e2 in the sentence. N is the corresponding set for negative pairs. The goal of this work is to automatically recognize a set of positive pairs (Pauto) and a set of negative pairs (Nauto). We compare the system output (Pauto ∪ Nauto) against the gold standard (Pgold ∪ Ngold) for each sentence. 3.1 Gold Standard Corpus: MPQA 3.0 MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015).4 It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agencies, and covers a wide range of topics. In both MPQA 2.0 and 3.0, the top-level annotations include direct subjectives (DS). Each DS has a nested-source annotation. Each DS has one or more attitude links, meaning that all of the attitudes share the same nested source. The attitudes differ from one another in their attitude types, polarities, and/or targets. Moreover, both corpora contain expressive subjective element (ESE) annotations, which pinpoint specific expressions used to express subje</context>
</contexts>
<marker>Wilson, 2007</marker>
<rawString>Theresa Wilson. 2007. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of private states. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>1640--1649</pages>
<contexts>
<context position="7352" citStr="Yang and Cardie, 2013" startWordPosition="1162" endWordPosition="1165">olve explicit and implicit sentiment ambiguities by integrating inference rules. 2 Related Work Fined-grained sentiment analysis. Most finegrained sentiment analysis is span or aspect based. Previous work differs from the entity/event-level sentiment analysis task we address in terms of targets and sources. In terms of targets, in a spanbased sentiment analysis system, the target is a span instead of the exact head of the phrase referring to the target. The target in a span-based system is evaluated by measuring the overlapping proportion of an extracted span against the gold standard phrase (Yang and Cardie, 2013), while the eTarget in an entity/event-level system is evaluated against the exact word (i.e., head of NP/VP) in the gold standard. It is a stricter evaluation. While the targets in aspect-based sentiment analysis are often entity targets, they are mainly product aspects, which are a predefined set.3 In contrast, the target in the entity/event-level task may be any noun or verb. In terms of sources, previous work in sentiment analysis trained on review data assumes that the source is the writer of the review (Hu and Liu, 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate</context>
<context position="12330" citStr="Yang and Cardie, 2013" startWordPosition="1990" endWordPosition="1993">fer from one another in their attitude types, polarities, and/or targets. Moreover, both corpora contain expressive subjective element (ESE) annotations, which pinpoint specific expressions used to express subjectivity. We ignore neutral ESEs and only consider ESEs whose polarity is positive or negative. MPQA 2.0 and 3.0 differ in their target annotations. In 2.0, each target is a span. A target annotation of an opinion captures the most important target this opinion is expressed toward. Since the exact boundaries of the spans are hard to define even for human annotators (Wiebe et al., 2005a; Yang and Cardie, 2013), the target span in MPQA 2.0 could be a single word, an NP or VP, or a text span covering more than one constituent. In contrast, in MPQA 3.0, each target is anchored to the head of an NP or VP, which is a single word. It is called an 4Available at http://mpqa.cs.pitt.edu/corpora/ 181 eTarget since it is an entity or an event. In MPQA 2.0, only attitudes have target-span annotations. In MPQA 3.0, both attitudes and ESEs have eTarget annotations. Importantly, the eTargets include the targets of both explicit and implicit sentiments. Recall Ex(1) in Section 1. Pgold = {(writer, Imam), (writer, </context>
<context position="17944" citStr="Yang and Cardie, 2013" startWordPosition="2953" endWordPosition="2956">(6) ETARGET(y,t): the eTarget of y is t Two rules are defined to aggregate various opinions extracted by span-based systems into positive pairs and negative pairs, shown in Part 1 of Table 1 as Rules 1.1 and 1.2. Thus, under our representation, the PSL model not only finds a set of eTargets of an opinion (ETARGET(y,t)), but also represents the aggregated sentiments among entities and events (POSPAIR(s,t) and NEGPAIR(s,t)) in the sentence. Next, we turn to assigning local scores to ground atoms (3)-(6). POS(y) and NEG(y): We build upon three spanbased sentiment analysis systems. The first, S1 (Yang and Cardie, 2013), and the second, S2 (Yang and Cardie, 2014), are both trained on MPQA 2.0, which does not contain any eTarget annotations. S1 extracts triples of (source span, opinion span, target span), but does not extract opinion polarities. S2 extracts opinion spans and opinion polarities, but it does not extract sources or targets. The third system, S3 (Socher et al., 2013), is trained on movie review data. It extracts opinion spans and polarities. The source is always assumed to be the writer. We take the union set of opinions extracted by S1, S2 and S3. For each opinion y, a ground atom is created, de</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In ACL (1), pages 1640–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Context-aware learning for sentence-level sentiment analysis with posterior regularization.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1337" citStr="Yang and Cardie, 2014" startWordPosition="187" endWordPosition="190"> to greatly improve over baseline accuracies in recognizing entity/event-level sentiments. 1 Introduction There are increasing numbers of opinions expressed in various genres, including reviews, newswire, editorials, and forums. While much early work was at the document or sentence level, to fully understand and utilize opinions, researchers are increasingly carrying out more finegrained sentiment analysis to extract components of opinion frames: the source (whose sentiment is it), the polarity, and the target (what is the sentiment toward). Much fine-grained analysis is span or aspect based (Yang and Cardie, 2014; Pontiki et al., 2014). In contrast, this work contributes to entity/event-level sentiment analysis. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Who is negative/positive toward X?” (Stoyanov et al., 2005), where X could be any entity or event. Let us consider an example from the MPQA opinion annotated corpus (Wiebe et al., 2005a; Wilson, 2007; Deng and Wiebe, 2015). Ex(1) When the Imam ( may God be satisfied with him 1) issued the fatwa against 2 Salman Rus</context>
<context position="17988" citStr="Yang and Cardie, 2014" startWordPosition="2961" endWordPosition="2964">rules are defined to aggregate various opinions extracted by span-based systems into positive pairs and negative pairs, shown in Part 1 of Table 1 as Rules 1.1 and 1.2. Thus, under our representation, the PSL model not only finds a set of eTargets of an opinion (ETARGET(y,t)), but also represents the aggregated sentiments among entities and events (POSPAIR(s,t) and NEGPAIR(s,t)) in the sentence. Next, we turn to assigning local scores to ground atoms (3)-(6). POS(y) and NEG(y): We build upon three spanbased sentiment analysis systems. The first, S1 (Yang and Cardie, 2013), and the second, S2 (Yang and Cardie, 2014), are both trained on MPQA 2.0, which does not contain any eTarget annotations. S1 extracts triples of (source span, opinion span, target span), but does not extract opinion polarities. S2 extracts opinion spans and opinion polarities, but it does not extract sources or targets. The third system, S3 (Socher et al., 2013), is trained on movie review data. It extracts opinion spans and polarities. The source is always assumed to be the writer. We take the union set of opinions extracted by S1, S2 and S3. For each opinion y, a ground atom is created, depending on the polarity (POS(y) if y is posi</context>
</contexts>
<marker>Yang, Cardie, 2014</marker>
<rawString>Bishan Yang and Claire Cardie. 2014. Context-aware learning for sentence-level sentiment analysis with posterior regularization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
</authors>
<title>Identifying noun product features that imply opinions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>575--580</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5235" citStr="Zhang and Liu (2011)" startWordPosition="823" endWordPosition="826">lines are explicit sentiments and the dashed lines are implicit sentiments. In this work, we detect sentiments such as those in P and N, where the sources are entities (or the writer) and the targets are entities and events. Previous work in sentiment analysis mainly focuses on detecting explicit opinions. Recently there is emerging focus on sentiment inference, which recognizes implicit sentiments by inferring them from explicit sentiments via inference rules. Current works in sentiment inference differ on how the sentiment inference rules are defined and how they are expressed. For example, Zhang and Liu (2011) define linguistic templates to recognize phrases that express implicit sentiments, while previously we (Deng et al., 2014) represent a few simple rules as (in)equality constraints in Integer Linear Programming. In contrast to previous 2Note that the inferences are conversational implicatures; they are defeasible and may not go through in context (Deng et al., 2014; Wiebe and Deng, 2014). work, we propose a more general set of inference rules and encode them in a probabilistic soft logic (PSL) framework (Bach et al., 2015). We chose PSL because it is designed to have efficient inference and, a</context>
<context position="8272" citStr="Zhang and Liu, 2011" startWordPosition="1313" endWordPosition="1316"> In contrast, the target in the entity/event-level task may be any noun or verb. In terms of sources, previous work in sentiment analysis trained on review data assumes that the source is the writer of the review (Hu and Liu, 2004; Titov and McDonald, 2008). 3As stated in SemEval-2014: “we annotate only aspect terms naming particular aspects”. 180 Our work is rare in that it allows sources other than the writer and finds sentiments toward eTargets which may be any entity or event. Sentiment Inference. There is some recent work investigating features that directly indicate implicit sentiments (Zhang and Liu, 2011; Feng et al., 2013). That work assumes the source is only the writer. Further, as it uses features to directly extract implicit sentiments, it does not perform general sentiment inference. Previously, we (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014) develop rules and models to infer sentiments related to +/-effect events, events that positively or negatively affect entities. That work assumes that the source is only the writer, and the targets are limited to entities that participate in +/-effect events. Further, our previous models all require certain manual (oracle) annotatio</context>
</contexts>
<marker>Zhang, Liu, 2011</marker>
<rawString>Lei Zhang and Bing Liu. 2011. Identifying noun product features that imply opinions. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 575–580, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
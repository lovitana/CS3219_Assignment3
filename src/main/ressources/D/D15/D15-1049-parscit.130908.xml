<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000440">
<title confidence="0.917557">
Flexible Domain Adaptation for Automated Essay Scoring Using
Correlated Linear Regression
</title>
<author confidence="0.990743">
Peter Phandi1 Kian Ming A. Chai2 Hwee Tou Ng1
</author>
<affiliation confidence="0.984717">
1 Department of Computer Science, National University of Singapore
</affiliation>
<email confidence="0.90813">
{peter-p,nght}@comp.nus.edu.sg
</email>
<address confidence="0.456872">
2 DSO National Laboratories
</address>
<email confidence="0.980258">
ckianmin@dso.org.sg
</email>
<sectionHeader confidence="0.9944" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99974075">
Most of the current automated essay scor-
ing (AES) systems are trained using manu-
ally graded essays from a specific prompt.
These systems experience a drop in accu-
racy when used to grade an essay from a
different prompt. Obtaining a large num-
ber of manually graded essays each time
a new prompt is introduced is costly and
not viable. We propose domain adapta-
tion as a solution to adapt an AES system
from an initial prompt to a new prompt.
We also propose a novel domain adapta-
tion technique that uses Bayesian linear
ridge regression. We evaluate our domain
adaptation technique on the publicly avail-
able Automated Student Assessment Prize
(ASAP) dataset and show that our pro-
posed technique is a competitive default
domain adaptation algorithm for the AES
task.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996001754386">
Essay writing is a common task evaluated in
schools and universities. In this task, students are
typically given a prompt or essay topic to write
about. Essay writing is included in high-stakes as-
sessments, such as Test of English as a Foreign
Language (TOEFL) and Graduate Record Exami-
nation (GRE). Manually grading all essays takes a
lot of time and effort for the graders. This is what
Automated Essay Scoring (AES) systems are try-
ing to alleviate.
Automated Essay Scoring uses computer soft-
ware to automatically evaluate an essay written in
an educational setting by giving it a score. Work
related to essay scoring can be traced back to
1966 when Ellis Page created a computer grading
software called Project Essay Grade (PEG). Re-
search on AES has continued through the years.
The recent Automated Student Assessment Prize
(ASAP) Competition1 sponsored by the Hewlett
Foundation in 2012 has renewed interest on this
topic. The agreement between the scores assigned
by state-of-the-art AES systems and the scores as-
signed by human raters has been shown to be rel-
atively high. See Shermis and Burstein (2013) for
a recent overview of AES.
AES is usually treated as a supervised machine
learning problem, either as a classification, regres-
sion, or rank preference task. Using this approach,
a training set in the form of human graded essays
is needed. However, human graded essays are not
readily available. This is perhaps why research in
this area was mostly done by commercial organi-
zations. After the ASAP competition, research in-
terest in this area has been rekindled because of
the released dataset.
Most of the recent AES related work is prompt-
specific. That is, an AES system is trained using
essays from a specific prompt and tested against
essays from the same prompt. These AES systems
will not work as well when tested against a differ-
ent prompt. Furthermore, generating the training
data each time a new prompt is introduced will be
costly and time consuming.
In this paper, we propose domain adaptation as
a solution to this problem. Instead of hiring peo-
ple to grade new essays each time a new prompt
is introduced, domain adaptation can be used to
adapt the old prompt-specific system to suit the
new prompt. This way, a smaller number of train-
ing essays from the new prompt is needed. In this
paper, we propose a novel domain adaptation tech-
nique based on Bayesian linear ridge regression.
The rest of this paper is organized as follows. In
Section 2, we give an overview of related work on
AES and domain adaptation. Section 3 describes
the AES task and the features used. Section 4
presents our novel domain adaptation algorithm.
</bodyText>
<footnote confidence="0.972104">
1http://www.kaggle.com/c/asap-aes
</footnote>
<page confidence="0.946423">
431
</page>
<note confidence="0.9852915">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 431–439,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999165666666667">
Section 5 describes our data, experimental setup,
and evaluation metric. Section 6 presents and dis-
cusses the results. We conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999977333333333">
We first introduce related work on automated es-
say scoring, followed by domain adaptation in the
context of natural language processing.
</bodyText>
<subsectionHeader confidence="0.960274">
2.1 Automated Essay Scoring
</subsectionHeader>
<bodyText confidence="0.999960325">
Since the first AES system, Project Essay Grade,
was created in 1966, a number of commercial sys-
tems have been deployed. One such system, e-
rater (Attali and Burstein, 2004), is even used as
a replacement for the second human grader in the
Test of English as a Foreign Language (TOEFL)
and Graduate Record Examination (GRE). Other
AES commercial systems also exist, such as Intel-
liMetric2 and Intelligent Essay Assessor (Foltz et
al., 1999).
AES is generally considered as a machine learn-
ing problem. Some work, such as PEG (Page,
1994) and e-rater, considers it as a regression prob-
lem. PEG uses a large number of features with re-
gression to predict the human score. e-rater uses
natural language processing (NLP) techniques to
extract a smaller number of complex features, such
as grammatical error and lexical complexity, and
uses them with stepwise linear regression (At-
tali and Burstein, 2004). Others like (Larkey,
1998) take the classification approach. (Rudner
and Liang, 2002) uses Bayesian models for clas-
sification and treats AES as a text classification
problem. Intelligent Essay Assessor uses Latent
Semantic Analysis (LSA) (Landauer et al., 1998)
as a measure of semantic similarity between es-
says. Other recent work uses the preference rank-
ing based approach (Yannakoudakis et al., 2011;
Chen and He, 2013).
In this paper, we also treat AES as a regression
problem, following PEG and e-rater. We use re-
gression because the range of scores of the essays
could be very large and a classification approach
does not work well in this case. It also allows us to
model essay scores as continuous values and scale
them easily in the case of different score ranges
between the source essay prompt and the target es-
say prompt.
The features used differ among the systems,
ranging from simple features (e.g., word length,
</bodyText>
<footnote confidence="0.839634">
2http://www.vantagelearning.com/products/intellimetric/
</footnote>
<bodyText confidence="0.999442428571429">
essay length, etc) to more complex features (e.g.,
grammatical errors). Some of these features are
generic in the sense that they could apply to all
kinds of prompts. Such features include the num-
ber of spelling errors, grammatical errors, lexical
complexity, etc. Others are prompt-specific fea-
tures such as bag of words features.
</bodyText>
<subsectionHeader confidence="0.998844">
2.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999681705882353">
The knowledge learned from a single domain
might not be directly applicable to another do-
main. For example, a named entity recognition
system trained on labeled news data might not per-
form as well on biomedical texts (Jiang and Zhai,
2007). We can solve this problem either by getting
labeled data from the other domain, which might
not be available, or by performing domain adapta-
tion.
Domain adaptation is the task of adapting
knowledge learned in a source domain to a target
domain. Various approaches to this task have been
proposed and used in the context of NLP. Some
commonly used approaches include EasyAdapt
(Daum´e III, 2007), instance weighting (IW) (Jiang
and Zhai, 2007), and structural correspondence
learning (SCL) (Blitzer et al., 2006).
We can divide the approaches of domain adapta-
tion into two categories based on the availability of
labeled target data. The case where a small num-
ber of labeled target data is available is usually re-
ferred to as supervised domain adaptation (such
as EasyAdapt and IW). The case where no la-
beled target domain data is available is usually re-
ferred to as unsupervised domain adaptation (such
as SCL). In our work, we focus on supervised do-
main adaptation.
Daum´e III (2007) described a domain adapta-
tion scheme called EasyAdapt which makes use of
feature augmentation. Suppose we have a feature
vector x in the original feature space. This scheme
will map this instance using the mapping functions
4,s(x) and 4,t(x) for the source and target domain
respectively, where
</bodyText>
<equation confidence="0.9985205">
4,s(x) _ (x, x, 0)
4,t(x) _ (x, 0, x),
</equation>
<bodyText confidence="0.9279298">
and 0 is a zero vector of length |x|. This adapta-
tion scheme is attractive because of its simplicity
and ease of use as a pre-processing step, and also
because it performs quite well despite its simplic-
ity. It has been used in various NLP tasks such
</bodyText>
<page confidence="0.997925">
432
</page>
<bodyText confidence="0.999951166666667">
as word segmentation (Monroe et al., 2014), ma-
chine translation (Green et al., 2014), word sense
disambiguation (Zhong et al., 2008), and short an-
swer scoring (Heilman and Madnani, 2013). Our
work is an extension of this scheme in the sense
that our work is a generalization of EasyAdapt.
</bodyText>
<sectionHeader confidence="0.984284" genericHeader="method">
3 Automated Essay Scoring
</sectionHeader>
<bodyText confidence="0.999717">
This section describes the Automated Essay Scor-
ing (AES) task and the features we use for the task.
</bodyText>
<subsectionHeader confidence="0.998864">
3.1 Task Description
</subsectionHeader>
<bodyText confidence="0.9999003">
In AES, the input to the system is a student es-
say, and the output is the score assigned to the es-
say. The score assigned by the AES system will
be compared against the human assigned score
to measure their agreement. Common agree-
ment measures used include Pearson’s correlation,
Spearman’s correlation, and quadratic weighted
Kappa (QWK). We use QWK in this paper, which
is also the evaluation metric in the ASAP compe-
tition.
</bodyText>
<subsectionHeader confidence="0.998091">
3.2 Features and Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.99998284">
We model the AES task as a regression problem
and use Bayesian linear ridge regression (BLRR)
as our learning algorithm. We choose BLRR as
our learning algorithm so as to use the correlated
BLRR approach which will be explained in Sec-
tion 4. We use an open source essay scoring sys-
tem, EASE (Enhanced AI Scoring Engine)3, to ex-
tract the features. EASE is created by one of the
winners of the ASAP competition so the features
they use have been proven to be robust. Table 1
gives the features used by EASE.
Useful n-grams are defined as n-grams that sep-
arate good scoring essays and bad scoring es-
says, determined using the Fisher test (Fisher,
1922). Good scoring essays are essays with a
score greater than or equal to the average score,
and the remainder are considered as bad scoring
essays. The top 201 n-grams with the highest
Fisher values are then chosen as the bag features.
We perform the calculation of useful n-grams sep-
arately for source and target domain essays, and
join them together using set union during the do-
main adaptation experiment. This is done to pre-
vent the system from choosing only n-grams from
the source domain as the useful n-grams, since the
</bodyText>
<footnote confidence="0.63075">
3https://github.com/edx/ease
</footnote>
<bodyText confidence="0.999308153846154">
number of source domain essays is much larger
than the target domain essays.
EASE uses NLTK (Bird et al., 2009) for POS
tagging and stemming, aspell for spellchecking,
and WordNet (Fellbaum, 1998) to get the syn-
onyms. Correct POS tags are generated using a
grammatically correct text (provided by EASE).
The POS tag sequences not included in the correct
POS tags are considered as bad POS. EASE uses
scikit-learn (Pedregosa et al., 2011) for extracting
unigram and bigram features. For linear regres-
sion, a constant feature of value one is appended
for the bias.
</bodyText>
<sectionHeader confidence="0.999834" genericHeader="method">
4 Correlated Bayesian Linear Ridge
Regression
</sectionHeader>
<bodyText confidence="0.99991575">
First, consider the single-task setting. Let x E RP
be the feature vector of an essay. p represents the
number of features in x. The generative model for
an observed real-valued score y is
</bodyText>
<equation confidence="0.998582333333333">
α — Γ(α1, α2), λ — Γ(λ1, λ2),
w — N(0, λ−1I), f(x) def = xTw,
y — N(f(xi), α−1).
</equation>
<bodyText confidence="0.999993666666667">
Here, α and λ are Gamma distributed hyper-
parameters of the model; w E RP is the Normal
distributed weight vector of the model; f is the
latent function that returns the “true” score of an
essay represented by x by linear combination; and
y is the noisy observed score of x.
Now, consider the two-task setting, where we
indicate the source task and the target task by su-
perscripts s and t. Given an essay with feature
vector x, we consider its observed scores ys and
yt when evaluated in task s and task t separately.
We have scale hyper-parameters α and λ sampled
as before. In addition, we have the correlation ρ
between the two tasks. The generative model re-
lating the two tasks is
</bodyText>
<equation confidence="0.997808714285714">
ρ — pp,
wt, ws — N(0, λ−1I),
ft(def
x) = xTwt,
fs(x) def = ρxTwt + (1 — ρ2)1/2xTws,
yt — N (ft(x), α−1),
ys — N(fs(x), α−1),
</equation>
<bodyText confidence="0.999784">
where pp is a chosen distribution over the correla-
tion; and wt and ws are the weight vectors of the
</bodyText>
<page confidence="0.997707">
433
</page>
<figure confidence="0.5926259">
Feature Type Feature Description
Length Number of characters
Number of words
Number of commas
Number of apostrophes
Number of sentence ending punctuation symbols ( “.”, “?”, or “!”)
Average word length
Part of speech (POS) Number of bad POS n-grams
Number of bad POS n-grams divided by the total number of words in the essay
Prompt Number of words in the essay that appears in the prompt
</figure>
<bodyText confidence="0.9810965">
Number of words in the essay that appears in the prompt divided by the total
number of words in the essay
Number of words in the essay which is a word or a synonym of a word that
appears in the prompt
Number of words in the essay which is a word or a synonym of a word that
appears in the prompt divided by the total number of words in the essay
Bag of words Count of useful unigrams and bigrams (unstemmed)
Count of stemmed and spell corrected useful unigrams and bigrams
</bodyText>
<tableCaption confidence="0.992797">
Table 1: Description of the features used by EASE.
</tableCaption>
<bodyText confidence="0.9999564">
target and the source tasks respectively, and they
are identically distributed but independent. In this
setting, it can be shown that the correlation be-
tween latent scoring functions for the target and
the source tasks is ρ. That is,
</bodyText>
<equation confidence="0.996035">
E(ft(x)fs(x�)) = λ−1ρxTx�. (1)
</equation>
<bodyText confidence="0.999963733333333">
This, in fact, is a generalization of the EasyAdapt
scheme, for which the correlation ρ is fixed at 0.5
[(Daum´e III, 2007), see eq. 3]. Two other common
values for ρ are 1 and 0; the former corresponds to
a straightforward concatenation of the source and
target data, while the latter is the shared-hyper-
parameter setting which shares α and λ between
the source and target domain. Through adjust-
ing ρ, the model traverses smoothly between these
three regimes of domain adaptation.
EasyAdapt is attractive because of its (frustrat-
ingly) ease of use via encoding the correlation
within an expanded feature representation scheme.
In the same way, the current setup can be achieved
readily by the expanded feature representation
</bodyText>
<equation confidence="0.999980333333333">
Φt(x) = * 0p) ,
D E (2)
Φs(x) = ρx, (1 − ρ2)1/2x
</equation>
<bodyText confidence="0.999571409090909">
in R2p for the target and the source tasks. Asso-
ciated with this expanded feature representation is
the weight vector w def = (wt, ws) also in R2p. As
we shall see in Section 4.1, such a representation
eases the estimation of the parameters.
The above model is related to the multi-task
Gaussian Process model that has been used for
joint emotion analysis (Beck et al., 2014). There,
the intrinsic coregionalisation model (ICM) has
been used with squared-exponential covariance
function. Here, we use the simpler linear covari-
ance function (Rasmussen and Williams, 2006),
and this leads to Bayesian linear ridge regression.
There are two reasons for this choice. The first
is that linear combination of carefully chosen fea-
tures, especially lexical ones, usually gives good
performance in NLP tasks. The second is in the
preceding paragraph: an intuitive feature expan-
sion representation of the domain adaptation pro-
cess that allows ease of parameter estimation.
The above model is derived from the Cholesky
decomposition
</bodyText>
<equation confidence="0.961155">
(1 ρ) �1 � ~1 �
0 ρ
=
ρ 1 ρ (1 − ρ2)1/2 0 (1 − ρ2)1/2
</equation>
<bodyText confidence="0.9998402">
of the desired correlation matrix that will eventu-
ally lead to equation (1). Other choices are possi-
ble, as long as equation (1) is satisfied. However,
the current choice has the desired property that the
wt portion of the combined weight vector is di-
</bodyText>
<page confidence="0.996422">
434
</page>
<bodyText confidence="0.997305">
rectly interpretable as the weights for the features
in the target domain.
</bodyText>
<subsectionHeader confidence="0.995557">
4.1 Maximum Likelihood Estimation
</subsectionHeader>
<bodyText confidence="0.999954789473684">
We estimate the parameters (α, λ, ρ) of the model
using penalized maximum likelihood. For α and
λ, the gamma distributions are used. For ρ, we
impose a distribution with density pp(ρ) = 1 +
a − 2aρ, a ∈ [−1, 1]. This distribution is sup-
ported only in [0, 1]; negative ρs are not supported
because we think that negative transfer of informa-
tion from source to target domain prompts in this
essay scoring task is improbable. In our applica-
tion, we slightly bias the correlations towards zero
with a = 1/10 in order to ameliorate spurious cor-
relations.
For the training data, let there be nt examples in
the target domain and ns in the source domain. Let
Xt (resp. Xs) be the nt-by-p (resp. ns-by-p) de-
sign matrix for the training data in the target (resp.
source) domain. Let yt and ys be the correspond-
ing observed essay scores. The expanded feature
matrix due to equation (2) is
</bodyText>
<equation confidence="0.850596">
1/2Xs) .
</equation>
<bodyText confidence="0.9702002">
Similarly, let y be the stacking of yt and ys. Let
K def = λ−1XXT + α−1I, which is also known
as the Gramian for the observations. The log
marginal likelihood of the training data is (Ras-
mussen and Williams, 2006)
</bodyText>
<equation confidence="0.990572166666667">
L = −2yTK−1y − 2 1 log |K |− nt + ns
1 2 log 2π.
This is penalized to give Lp by adding
(α1 − 1) log(α) − α2α + α1 log α2 − log F(α1)
+(λ1 − 1) log(λ) − λ2λ + λ1 log λ2 − log F(λ1)
+ log(1 + a − 2aρ).
</equation>
<bodyText confidence="0.9999874">
The estimation of these parameters is then done
by optimising Lp. In our implementation, we use
scikit-learn for estimating α and λ in an inner
loop, and we use gradient descent for estimating
ρ in the outer loop using
</bodyText>
<equation confidence="0.9475495">
1 ��γγT − K−1� ∂K � − 2a
2 tr 1 + a − 2aρ,
∂ρ
def
</equation>
<bodyText confidence="0.763346">
where γ = K−1y and
</bodyText>
<equation confidence="0.94439425">
� 0 �
∂K Xt(Xs)T
= λ−1
∂ρ Xs(Xt)T 0
</equation>
<subsectionHeader confidence="0.923284">
4.2 Prediction
</subsectionHeader>
<bodyText confidence="0.994596333333333">
We report the mean prediction as the score of
an essay. This uses the mean weight vector
w¯ = λ−1XTK−1y ∈ R2p, which may be parti-
tioned into two vectors ¯wt and ¯ws, each in Rp.
The prediction of a new essay represented by x∗
in the target domain is then given by xT∗ ¯wt.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999920666666667">
In this section, we will give a brief description
of the dataset we use, describe our experimental
setup, and explain the evaluation metric we use.
</bodyText>
<subsectionHeader confidence="0.952023">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.997900136363637">
We use the ASAP dataset4 for our domain adapta-
tion experiments. This dataset contains 8 prompts
of different genres. The average length of the es-
says differs for each prompt, ranging from 150 to
650 words. The essays were written by students
ranging in grade 7 to grade 10. All the essays were
graded by at least 2 human graders. The genres
include narrative, argumentative, or response. The
prompts also have different score ranges, as shown
in Table 2.
We pick four pairs of essay prompts to perform
our experiments. In each experiment, one of the
essay prompts from the pair will be the source do-
main and the other essay prompt will be the target
domain. The essay set pairs we choose are 1 → 2,
3 → 4, 5 → 6, and 7 → 8, where the pair 1 → 2
denotes using prompt 1 as the source domain and
prompt 2 as the target domain, for example. These
pairs are chosen based on the similarities in their
genres, score ranges, and median scores. The aim
is to have similar source and target domains for
effective domain adaptation.
</bodyText>
<subsectionHeader confidence="0.993639">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999671636363636">
We use 5-fold cross validation on the ASAP train-
ing data for evaluation. This is because the of-
ficial test data of the competition is not released
to the public. We divide the target domain data
randomly into 5 folds. One fold is used as the
test data, while the remaining four folds are col-
lected together and then sub-sampled to obtain the
target-domain training data. The sizes of the sub-
sampled target-domain training data are 10, 25, 50
and 100, with the larger sets containing the smaller
sets. All essays from the source domain are used.
</bodyText>
<equation confidence="0.8435934">
4https://www.kaggle.com/c/asap-aes/data
X def Xt 0
(ρXs (1 − ρ2)
∂Lp
∂ρ
</equation>
<page confidence="0.996004">
435
</page>
<table confidence="0.7624048">
Set # Essays Genre Avg len Score
Range Median
1 1,783 ARG 350 2–12 8
2 1,800 ARG 350 1–6 3
3 1,726 RES 150 0–3 1
4 1,772 RES 150 0–3 1
5 1,805 RES 150 0–4 2
6 1,800 RES 150 0–4 2
7 1,569 NAR 250 0–30 16
8 723 NAR 650 0–60 36
</table>
<tableCaption confidence="0.971965">
Table 2: Selected details of the ASAP data. For
</tableCaption>
<bodyText confidence="0.994299131578947">
the genre column, ARG denotes argumentative es-
says, RES denotes response essays, and NAR de-
notes narrative essays.
Our evaluation considers the following four
ways in which we train the AES model:
SourceOnly Using essays from the source do-
main only;
TargetOnly Using 10, 25, 50, and 100 sampled
essays from the target domain only;
SharedHyper Using correlated Bayesian linear
ridge regression (BLRR) with p fixed to 0
on source domain essays and sampled essays
from the target domain.
EasyAdapt As SharedHyper, but with p = 0.5;
Concat As SharedHyper, but with p fixed to 1.0;
ML-p Using correlated BLRR with p maximizing
the likelihood of the data.
Since the source and target domain may have
different score ranges, we scale the scores linearly
to range from −1 to 1. When predicting on the
test essays, the predicted scores of our system will
be linearly scaled back to the target domain score
range and rounded to the nearest integer.
We build upon scikit-learn’s implementation of
BLRR for our learning algorithm. To ameliorate
the effects of different scales of features, we nor-
malize the features: length, POS, and prompt fea-
tures are linearly scaled to range from 0 to 1 ac-
cording to the training data; and the feature values
for bag-of-words features are log(1 + count) in-
stead of the actual counts.
We use scikit-learn version 0.15.2, NLTK ver-
sion 2.0b7, and aspell version 0.60.6.1 in this ex-
periment. The BLRR code (bayes.py) in scikit-
learn is modified to obtain valid likelihoods for use
in the outer loop for estimating p. We use scikit-
learn’s default value for the parameters α1, α2, A1,
and A2 which is 10−6.
</bodyText>
<table confidence="0.9436878">
QWK scores
Set # BLRR SVM Human
1 0.761 0.781 0.721
2 0.606 0.621 0.814
3 0.621 0.630 0.769
4 0.742 0.749 0.851
5 0.784 0.782 0.753
6 0.775 0.771 0.776
7 0.730 0.727 0.721
8 0.617 0.534 0.629
</table>
<tableCaption confidence="0.999543">
Table 3: In-domain experimental results.
</tableCaption>
<subsectionHeader confidence="0.969311">
5.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.990077315789474">
Quadratic weighted Kappa (QWK) is used to mea-
sure the agreement between the human rater and
the system. We choose to use this evaluation met-
ric since it is the official evaluation metric of the
ASAP competition. Other work such as (Chen and
He, 2013) that uses the ASAP dataset also uses
this evaluation metric. QWK is calculated using
E,
i,j wi,jEi,j
where matrices O, (wi,j), and E are the matrices
of observed scores, weights, and expected scores
respectively. Matrix Oi,j corresponds to the num-
ber of essays that receive a score i by the first rater
and a score j by the second rater. The weight en-
tries are wi,j = (i − j)2/(N −1)2, where N is the
number of possible ratings. Matrix E is calculated
by taking the outer product between the score vec-
tors of the two raters, which are then normalized
to have the same sum as O.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999845076923077">
In-domain results for comparison First, we
determine indicative upper bounds on the QWK
scores using Bayesian linear ridge regression
(BLRR). To this end, we perform 5-fold cross vali-
dation by training and testing within each domain.
This is also done with linear support vector ma-
chine (SVM) regression to confirm that BLRR is
a competitive method for this task. In addition,
since the ASAP data has at least 2 human annota-
tors for each essay, we also calculate the human
agreement score. The results are shown in Ta-
ble 3. We see that the BLRR scores are close to
the the human agreement scores for prompt 1 and
</bodyText>
<equation confidence="0.872525666666667">
= 1 −
Ei,j wi,jOi,j
κ
</equation>
<page confidence="0.996761">
436
</page>
<table confidence="0.9999303125">
QWK Scores
Method nt =10 25 50 100
1 → 2
SourceOnly 0.434
TargetOnly 0.069 0.169 0.279 0.395
SharedHyper 0.158 0.218 0.332 0.390
EasyAdapt 0.425 0.422 0.442 0.467
Concat 0.484 0.507 0.529 0.545
ML-p 0.463 0.457 0.492 0.510
3 → 4
SourceOnly
0.522
TargetOnly 0.117 0.398 0.545 0.626
SharedHyper 0.113 0.350 0.487 0.575
EasyAdapt 0.461 0.541 0.589 0.628
Concat 0.594 0.611 0.617 0.638
ML-p 0.593 0.609 0.618 0.646
5 → 6
SourceOnly 0.187
TargetOnly 0.416 0.506 0.554 0.608
SharedHyper 0.380 0.500 0.544 0.600
EasyAdapt 0.553 0.621 0.652 0.698
Concat 0.649 0.689 0.708 0.722
ML-p 0.539 0.662 0.680 0.713
7 → 8
SourceOnly
0.171
TargetOnly 0.290 0.381 0.426 0.477
SharedHyper 0.302 0.383 0.444 0.484
EasyAdapt 0.594 0.616 0.605 0.610
Concat 0.332 0.362 0.396 0.463
ML-p 0.586 0.607 0.613 0.621
</table>
<tableCaption confidence="0.989605">
Table 4: QWK scores of the six methods on four
</tableCaption>
<bodyText confidence="0.99902990625">
domain adaptation experiments, ranging from us-
ing 10 target-domain essays (second column) to
100 target-domain essays (fifth column). The
scores are the averages over 5 folds. Setting a -* b
means the AES system is trained on essay set a
and tested on essay set b. For each set of six results
comparing the methods, the best score is bold-
faced and the second-best score is underlined.
prompts 5 to 8, but fall short by 10% to 20% for
prompts 2 to 4. We also see that BLRR is com-
parable to linear SVM regression, giving almost
the same performance for prompts 4 to 7; slightly
poorer performance for prompts 1 to 3; and much
better performance for prompt 8. The subsequent
discussion in this section will refer to the BLRR
scores in Table 3 for in-domain scores.
Importance of domain adaptation The results
of the domain adaptation experiments are tabu-
lated in Table 4, where the best scores are bold-
faced and the second-best scores are underlined.
As expected, for pairs 1 -* 2, 3 -* 4, and 5 -* 6,
all the scores are below their corresponding up-
per bounds from the in-domain setting in Table 3.
However, for pair 7 -* 8, the QWK score for
domain adaptation with 100 target essays outper-
forms that of the in-domain, albeit only by 0.4%.
This can be explained by the small number of es-
says in prompt 8 that can be used in both the in-
domain and domain adaptation settings, and that
domain adaptation additionally involves prompt 7
which has more than twice the number of essays;
see column two in Table 2. Hence, domain adap-
tation is effective in the context of small number
of target essays with large number of source es-
says. This can also be seen in Table 4 where we
have simulated small number of target essays with
sizes 10, 25, 50, and 100. When we compare the
scores of TargetOnly against the best scores and
second-best scores, we find that domain adapta-
tion is effective and important in improving the
QWK scores.
By the above argument alone, one might have
thought that an overwhelming large number of
source domain essays was sufficient for the tar-
get domain. However, this is not true. When we
compare the scores of SourceOnly against the best
scores and second-best scores, we find that do-
main adaptation again improves the QWK scores.
In fact, with just 10 additional target domain es-
says, effective domain adaptation can improve
over SourceOnly for all target domains 2, 4, 6, and
8 respectively.
This is the first time where the effects of domain
adaptation are shown in the AES task. In addi-
tion, the large improvement with a small number
of additional target domain essays in 5 -* 6 and
7 -* 8 suggests the high domain-dependence na-
ture of the task: learning on one essay prompt and
testing on another should be strongly discouraged.
Contributions by target-domain essays It is
instructive to understand why domain adaptation
is important for AES. To this end, we estimate the
contribution of bag-of-words features to the over-
all prediction by computing the ratio
</bodyText>
<page confidence="0.958515">
2
</page>
<bodyText confidence="0.827571">
Ei over bag-of-words features wi
</bodyText>
<page confidence="0.953269">
2
</page>
<bodyText confidence="0.602714">
Ei over all features wi
</bodyText>
<page confidence="0.996592">
437
</page>
<bodyText confidence="0.99839931">
using weights learned in the in-domain setting; see
Table 1 for the complete list of features. For do-
mains 2, 4, 6, and 8, which are the target domains
in the domain adaptation experiments, these ra-
tios are 0.37, 0.73, 0.69, and 0.93. The ratios for
the other four domains are similarly high. This
shows that bag-of-words features play a signifi-
cant role in the prediction of the essay scores. We
examine the number of bag-of-words features that
100 additional target domain essays would add to
SourceOnly; that is, we compare the bag-of-words
features for SourceOnly with those of SharedHy-
per, EasyAdapt, Concat, and ML-p for nt = 100.
The numbers of these additional features, aver-
aged over the five folds, are 269, 351, 377, and
291 for target domains 2, 4, 6, and 8 respectively.
In terms of percentages, these are 67%, 87%, 94%,
and 72% more features over SourceOnly. Such a
large number of additional bag-of-words features
contributed by target-domain essays, together with
the fact that these features are given high weights,
means that target-domain essays are important.
Comparing domain adaptation methods We
now compare the four domain adaptation meth-
ods: SharedHyper, EasyAdapt, Concat, and ML-p.
We recall that the first three are constrained cases
of the last by fixing p to 0, 0.5, and 1 respec-
tively. First, we see that SharedHyper is a rather
poor domain adaptation method for AES, because
it gives the lowest QWK score, except for the case
of using 25, 50, and 100 target essays in adapt-
ing from prompt 7 to prompt 8, where it is better
than Concat. In fact, its scores are generally close
to the TargetOnly scores. This is unsurprising,
since in SharedHyper the weights are effectively
not shared between the target and source training
examples: only the hyper-parameters α and A are
shared. This is a weak form of information sharing
between the target and source domains. Hence,
we expect this to perform suboptimally when the
target and source domains bear more than spuri-
ous relationship, which is indeed the case here be-
cause we have chosen the source and target do-
main pairs based on their similarities, as described
in Section 5.1.
We now focus on EasyAdapt, Concat, and
ML-p, which are the better domain adaptation
methods from our results. We see that ML-p ei-
ther gives the best or second-best scores, except
for the one case of 5 —* 6 with 10 target essays.
In comparison, although Concat performs consis-
tently well for 1 —* 2, 3 —* 4, and 5 —* 6, its QWK
scores for 7 —* 8 are quite poor and even lower
than those of TargetOnly for 25 or more target es-
says. In contrast to Concat, EasyAdapt performs
well for 7 —* 8 but not so well for the other three
domain pairs.
Let us examine the reason for contrasting re-
sults between EasyAdapt and Concat to appreci-
ate the flexibility afforded by ML-p. The p es-
timated by ML-p for the pairs 1 —* 2, 3 —* 4,
5 —* 6, and 7 —* 8 with 100 target essays are 0.81,
0.97, 0.76, and 0.63 averaged over five folds. The
lower estimated correlation p for 7 —* 8 means
that prompt 7 and prompt 8 are not as similar as
the other pairs are. In such a case as this, Concat,
which in effect considers the target domain to be
exactly the same as the source domain, can per-
form very poorly. For the other three pairs which
are more similar, the correlation of 0.5 assumed by
EasyAdapt is not strong enough to fully exploit the
similarities between the domains. Unlike Concat
and EasyAdapt, ML-p has the flexibility to allow
it to traverse effectively between the different de-
grees of domain similarity or relatedness based on
the source domain and target domain training data.
In view of this, we consider ML-p to be a compet-
itive default domain adaptation algorithm for the
AES task.
In retrospect of our present results, it can be
obvious why prompts 7 and 8 are not as simi-
lar as we would have hoped for more effective
domain adaptation. Both prompts ask for narra-
tive essays, and these by nature are very prompt-
specific and require words and phrases relating di-
rectly to the prompts. In fact, referring to a pre-
vious discussion on the contributions by target-
domain essays, we see that weights for the bag-
of-words features for prompt 8 contribute a high
of 93% of the total. When we examine the bag-
of-words features, we see that prompt 7 (which is
to write about patience) contributes only 19% to
the bag-of-words features of prompt 8 (which is to
write about laughter) in the in-domain experiment.
This means that 81% of the bag-of-words features,
which are important to narrative essays, must be
contributed by the target-domain essays relating to
prompt 8. Future work on domain adaptation for
AES can explore chosing the prior pρ on p to better
reflect the nature of the essays involved.
</bodyText>
<page confidence="0.998624">
438
</page>
<sectionHeader confidence="0.998709" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999553">
In this work, we investigate the effectiveness of us-
ing domain adaptation when we only have a small
number of target domain essays. We have shown
that domain adaptation can achieve better results
compared to using just the small number of target
domain data or just using a large amount of data
from a different domain. As such, our research
will help reduce the amount of annotation work
needed to be done by human graders to introduce
a new prompt.
</bodyText>
<sectionHeader confidence="0.998259" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.945168666666667">
This research is supported by Singapore Ministry
of Education Academic Research Fund Tier 2
grant MOE2013-T2-1-150.
</bodyText>
<sectionHeader confidence="0.994434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797825581395">
Yigal Attali and Jill Burstein. 2004. Automated es-
say scoring with e-rater ® v. 2.0. Technical report,
Educational Testing Service.
Daniel Beck, Trevor Cohn, and Lucia Specia. 2014.
Joint emotion analysis via multi-task Gaussian pro-
cesses. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing.
Hongbo Chen and Ben He. 2013. Automated essay
scoring by maximizing human-machine agreement.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford.
Ronald A Fisher. 1922. On the interpretation of χ2
from contingency tables, and the calculation of p.
Journal of the Royal Statistical Society.
Peter W Foltz, Darrell Laham, and Thomas K Lan-
dauer. 1999. The intelligent essay assessor: Appli-
cations to educational technology. Interactive Mul-
timedia Electronic Journal of Computer-Enhanced
Learning.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014. An empirical comparison of features
and tuning for phrase-based machine translation. In
Proceedings of the Ninth Workshop on Statistical
Machine Translation.
Michael Heilman and Nitin Madnani. 2013. Ets: do-
main adaptation and stacking for short answer scor-
ing. In Proceedings of the Seventh International
Workshop on Semantic Evaluation.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse Processes.
Leah S Larkey. 1998. Automatic essay grading using
text categorization techniques. In Proceedings of the
21st International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Will Monroe, Spence Green, and Christopher D Man-
ning. 2014. Word segmentation of informal Ara-
bic with domain adaptation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics.
Ellis Batten Page. 1994. Computer grading of student
prose, using modern concepts and software. The
Journal of Experimental Education.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in Python. Journal of Machine
Learning Research.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. MIT Press.
Lawrence M Rudner and Tahung Liang. 2002. Au-
tomated essay scoring using Bayes’ theorem. The
Journal of Technology, Learning and Assessment.
Mark D. Shermis and Jill Burstein, editors. 2013.
Handbook of Automated Essay Evaluation: Current
Applications and New Directions. Routledge.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing.
</reference>
<page confidence="0.999266">
439
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610094">
<title confidence="0.9969725">Flexible Domain Adaptation for Automated Essay Scoring Correlated Linear Regression</title>
<author confidence="0.992147">Kian Ming A Hwee Tou</author>
<affiliation confidence="0.946799">1Department of Computer Science, National University of</affiliation>
<address confidence="0.683622">2DSO National</address>
<email confidence="0.90451">ckianmin@dso.org.sg</email>
<abstract confidence="0.996268857142857">Most of the current automated essay scoring (AES) systems are trained using manually graded essays from a specific prompt. These systems experience a drop in accuracy when used to grade an essay from a different prompt. Obtaining a large number of manually graded essays each time a new prompt is introduced is costly and not viable. We propose domain adaptation as a solution to adapt an AES system from an initial prompt to a new prompt. We also propose a novel domain adaptation technique that uses Bayesian linear ridge regression. We evaluate our domain adaptation technique on the publicly available Automated Student Assessment Prize (ASAP) dataset and show that our proposed technique is a competitive default domain adaptation algorithm for the AES task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater ®</title>
<date>2004</date>
<tech>v. 2.0. Technical report, Educational Testing Service.</tech>
<contexts>
<context position="4449" citStr="Attali and Burstein, 2004" startWordPosition="721" endWordPosition="724">al Language Processing, pages 431–439, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Section 5 describes our data, experimental setup, and evaluation metric. Section 6 presents and discusses the results. We conclude in Section 7. 2 Related Work We first introduce related work on automated essay scoring, followed by domain adaptation in the context of natural language processing. 2.1 Automated Essay Scoring Since the first AES system, Project Essay Grade, was created in 1966, a number of commercial systems have been deployed. One such system, erater (Attali and Burstein, 2004), is even used as a replacement for the second human grader in the Test of English as a Foreign Language (TOEFL) and Graduate Record Examination (GRE). Other AES commercial systems also exist, such as IntelliMetric2 and Intelligent Essay Assessor (Foltz et al., 1999). AES is generally considered as a machine learning problem. Some work, such as PEG (Page, 1994) and e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features</context>
</contexts>
<marker>Attali, Burstein, 2004</marker>
<rawString>Yigal Attali and Jill Burstein. 2004. Automated essay scoring with e-rater ® v. 2.0. Technical report, Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Joint emotion analysis via multi-task Gaussian processes.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="14548" citStr="Beck et al., 2014" startWordPosition="2467" endWordPosition="2470">gly) ease of use via encoding the correlation within an expanded feature representation scheme. In the same way, the current setup can be achieved readily by the expanded feature representation Φt(x) = * 0p) , D E (2) Φs(x) = ρx, (1 − ρ2)1/2x in R2p for the target and the source tasks. Associated with this expanded feature representation is the weight vector w def = (wt, ws) also in R2p. As we shall see in Section 4.1, such a representation eases the estimation of the parameters. The above model is related to the multi-task Gaussian Process model that has been used for joint emotion analysis (Beck et al., 2014). There, the intrinsic coregionalisation model (ICM) has been used with squared-exponential covariance function. Here, we use the simpler linear covariance function (Rasmussen and Williams, 2006), and this leads to Bayesian linear ridge regression. There are two reasons for this choice. The first is that linear combination of carefully chosen features, especially lexical ones, usually gives good performance in NLP tasks. The second is in the preceding paragraph: an intuitive feature expansion representation of the domain adaptation process that allows ease of parameter estimation. The above mo</context>
</contexts>
<marker>Beck, Cohn, Specia, 2014</marker>
<rawString>Daniel Beck, Trevor Cohn, and Lucia Specia. 2014. Joint emotion analysis via multi-task Gaussian processes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="10540" citStr="Bird et al., 2009" startWordPosition="1744" endWordPosition="1747">reater than or equal to the average score, and the remainder are considered as bad scoring essays. The top 201 n-grams with the highest Fisher values are then chosen as the bag features. We perform the calculation of useful n-grams separately for source and target domain essays, and join them together using set union during the domain adaptation experiment. This is done to prevent the system from choosing only n-grams from the source domain as the useful n-grams, since the 3https://github.com/edx/ease number of source domain essays is much larger than the target domain essays. EASE uses NLTK (Bird et al., 2009) for POS tagging and stemming, aspell for spellchecking, and WordNet (Fellbaum, 1998) to get the synonyms. Correct POS tags are generated using a grammatically correct text (provided by EASE). The POS tag sequences not included in the correct POS tags are considered as bad POS. EASE uses scikit-learn (Pedregosa et al., 2011) for extracting unigram and bigram features. For linear regression, a constant feature of value one is appended for the bias. 4 Correlated Bayesian Linear Ridge Regression First, consider the single-task setting. Let x E RP be the feature vector of an essay. p represents th</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7265" citStr="Blitzer et al., 2006" startWordPosition="1174" endWordPosition="1177">ion system trained on labeled news data might not perform as well on biomedical texts (Jiang and Zhai, 2007). We can solve this problem either by getting labeled data from the other domain, which might not be available, or by performing domain adaptation. Domain adaptation is the task of adapting knowledge learned in a source domain to a target domain. Various approaches to this task have been proposed and used in the context of NLP. Some commonly used approaches include EasyAdapt (Daum´e III, 2007), instance weighting (IW) (Jiang and Zhai, 2007), and structural correspondence learning (SCL) (Blitzer et al., 2006). We can divide the approaches of domain adaptation into two categories based on the availability of labeled target data. The case where a small number of labeled target data is available is usually referred to as supervised domain adaptation (such as EasyAdapt and IW). The case where no labeled target domain data is available is usually referred to as unsupervised domain adaptation (such as SCL). In our work, we focus on supervised domain adaptation. Daum´e III (2007) described a domain adaptation scheme called EasyAdapt which makes use of feature augmentation. Suppose we have a feature vecto</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongbo Chen</author>
<author>Ben He</author>
</authors>
<title>Automated essay scoring by maximizing human-machine agreement.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5598" citStr="Chen and He, 2013" startWordPosition="905" endWordPosition="908">sing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay scores as continuous values and scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ among the systems, ranging from simple features (e.g., word length, 2http://www.vantagelearning.com/products/intellimetric/ essay length, etc) to more complex feat</context>
<context position="21768" citStr="Chen and He, 2013" startWordPosition="3798" endWordPosition="3801">r loop for estimating p. We use scikitlearn’s default value for the parameters α1, α2, A1, and A2 which is 10−6. QWK scores Set # BLRR SVM Human 1 0.761 0.781 0.721 2 0.606 0.621 0.814 3 0.621 0.630 0.769 4 0.742 0.749 0.851 5 0.784 0.782 0.753 6 0.775 0.771 0.776 7 0.730 0.727 0.721 8 0.617 0.534 0.629 Table 3: In-domain experimental results. 5.3 Evaluation Metric Quadratic weighted Kappa (QWK) is used to measure the agreement between the human rater and the system. We choose to use this evaluation metric since it is the official evaluation metric of the ASAP competition. Other work such as (Chen and He, 2013) that uses the ASAP dataset also uses this evaluation metric. QWK is calculated using E, i,j wi,jEi,j where matrices O, (wi,j), and E are the matrices of observed scores, weights, and expected scores respectively. Matrix Oi,j corresponds to the number of essays that receive a score i by the first rater and a score j by the second rater. The weight entries are wi,j = (i − j)2/(N −1)2, where N is the number of possible ratings. Matrix E is calculated by taking the outer product between the score vectors of the two raters, which are then normalized to have the same sum as O. 6 Results and Discuss</context>
</contexts>
<marker>Chen, He, 2013</marker>
<rawString>Hongbo Chen and Ben He. 2013. Automated essay scoring by maximizing human-machine agreement. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>Bradford.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. Bradford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald A Fisher</author>
</authors>
<title>On the interpretation of χ2 from contingency tables, and the calculation of p.</title>
<date>1922</date>
<journal>Journal of the Royal Statistical Society.</journal>
<contexts>
<context position="9875" citStr="Fisher, 1922" startWordPosition="1634" endWordPosition="1635">n problem and use Bayesian linear ridge regression (BLRR) as our learning algorithm. We choose BLRR as our learning algorithm so as to use the correlated BLRR approach which will be explained in Section 4. We use an open source essay scoring system, EASE (Enhanced AI Scoring Engine)3, to extract the features. EASE is created by one of the winners of the ASAP competition so the features they use have been proven to be robust. Table 1 gives the features used by EASE. Useful n-grams are defined as n-grams that separate good scoring essays and bad scoring essays, determined using the Fisher test (Fisher, 1922). Good scoring essays are essays with a score greater than or equal to the average score, and the remainder are considered as bad scoring essays. The top 201 n-grams with the highest Fisher values are then chosen as the bag features. We perform the calculation of useful n-grams separately for source and target domain essays, and join them together using set union during the domain adaptation experiment. This is done to prevent the system from choosing only n-grams from the source domain as the useful n-grams, since the 3https://github.com/edx/ease number of source domain essays is much larger </context>
</contexts>
<marker>Fisher, 1922</marker>
<rawString>Ronald A Fisher. 1922. On the interpretation of χ2 from contingency tables, and the calculation of p. Journal of the Royal Statistical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
<author>Thomas K Landauer</author>
</authors>
<title>The intelligent essay assessor: Applications to educational technology. Interactive Multimedia Electronic</title>
<date>1999</date>
<journal>Journal of Computer-Enhanced Learning.</journal>
<contexts>
<context position="4716" citStr="Foltz et al., 1999" startWordPosition="765" endWordPosition="768">2 Related Work We first introduce related work on automated essay scoring, followed by domain adaptation in the context of natural language processing. 2.1 Automated Essay Scoring Since the first AES system, Project Essay Grade, was created in 1966, a number of commercial systems have been deployed. One such system, erater (Attali and Burstein, 2004), is even used as a replacement for the second human grader in the Test of English as a Foreign Language (TOEFL) and Graduate Record Examination (GRE). Other AES commercial systems also exist, such as IntelliMetric2 and Intelligent Essay Assessor (Foltz et al., 1999). AES is generally considered as a machine learning problem. Some work, such as PEG (Page, 1994) and e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>Peter W Foltz, Darrell Laham, and Thomas K Landauer. 1999. The intelligent essay assessor: Applications to educational technology. Interactive Multimedia Electronic Journal of Computer-Enhanced Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>An empirical comparison of features and tuning for phrase-based machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="8411" citStr="Green et al., 2014" startWordPosition="1376" endWordPosition="1379"> which makes use of feature augmentation. Suppose we have a feature vector x in the original feature space. This scheme will map this instance using the mapping functions 4,s(x) and 4,t(x) for the source and target domain respectively, where 4,s(x) _ (x, x, 0) 4,t(x) _ (x, 0, x), and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 Automated Essay Scoring This section describes the Automated Essay Scoring (AES) task and the features we use for the task. 3.1 Task Description In AES, the input to the system is a student essay, and the output is the score assigned to the essay. The score assigned by the AES system will be compared against the human assigned score to measure their agreement. Common agreement measures used</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>Spence Green, Daniel Cer, and Christopher D. Manning. 2014. An empirical comparison of features and tuning for phrase-based machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>Ets: domain adaptation and stacking for short answer scoring.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="8513" citStr="Heilman and Madnani, 2013" startWordPosition="1392" endWordPosition="1395">ature space. This scheme will map this instance using the mapping functions 4,s(x) and 4,t(x) for the source and target domain respectively, where 4,s(x) _ (x, x, 0) 4,t(x) _ (x, 0, x), and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 Automated Essay Scoring This section describes the Automated Essay Scoring (AES) task and the features we use for the task. 3.1 Task Description In AES, the input to the system is a student essay, and the output is the score assigned to the essay. The score assigned by the AES system will be compared against the human assigned score to measure their agreement. Common agreement measures used include Pearson’s correlation, Spearman’s correlation, and quadratic weighted Kappa (QWK). We use QWK</context>
</contexts>
<marker>Heilman, Madnani, 2013</marker>
<rawString>Michael Heilman and Nitin Madnani. 2013. Ets: domain adaptation and stacking for short answer scoring. In Proceedings of the Seventh International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6752" citStr="Jiang and Zhai, 2007" startWordPosition="1092" endWordPosition="1095">com/products/intellimetric/ essay length, etc) to more complex features (e.g., grammatical errors). Some of these features are generic in the sense that they could apply to all kinds of prompts. Such features include the number of spelling errors, grammatical errors, lexical complexity, etc. Others are prompt-specific features such as bag of words features. 2.2 Domain Adaptation The knowledge learned from a single domain might not be directly applicable to another domain. For example, a named entity recognition system trained on labeled news data might not perform as well on biomedical texts (Jiang and Zhai, 2007). We can solve this problem either by getting labeled data from the other domain, which might not be available, or by performing domain adaptation. Domain adaptation is the task of adapting knowledge learned in a source domain to a target domain. Various approaches to this task have been proposed and used in the context of NLP. Some commonly used approaches include EasyAdapt (Daum´e III, 2007), instance weighting (IW) (Jiang and Zhai, 2007), and structural correspondence learning (SCL) (Blitzer et al., 2006). We can divide the approaches of domain adaptation into two categories based on the av</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis. Discourse Processes.</title>
<date>1998</date>
<contexts>
<context position="5437" citStr="Landauer et al., 1998" startWordPosition="878" endWordPosition="881">d e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay scores as continuous values and scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ am</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
</authors>
<title>Automatic essay grading using text categorization techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="5202" citStr="Larkey, 1998" startWordPosition="846" endWordPosition="847">tion (GRE). Other AES commercial systems also exist, such as IntelliMetric2 and Intelligent Essay Assessor (Foltz et al., 1999). AES is generally considered as a machine learning problem. Some work, such as PEG (Page, 1994) and e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does no</context>
</contexts>
<marker>Larkey, 1998</marker>
<rawString>Leah S Larkey. 1998. Automatic essay grading using text categorization techniques. In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Monroe</author>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Word segmentation of informal Arabic with domain adaptation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8369" citStr="Monroe et al., 2014" startWordPosition="1369" endWordPosition="1372">a domain adaptation scheme called EasyAdapt which makes use of feature augmentation. Suppose we have a feature vector x in the original feature space. This scheme will map this instance using the mapping functions 4,s(x) and 4,t(x) for the source and target domain respectively, where 4,s(x) _ (x, x, 0) 4,t(x) _ (x, 0, x), and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 Automated Essay Scoring This section describes the Automated Essay Scoring (AES) task and the features we use for the task. 3.1 Task Description In AES, the input to the system is a student essay, and the output is the score assigned to the essay. The score assigned by the AES system will be compared against the human assigned score to measure their</context>
</contexts>
<marker>Monroe, Green, Manning, 2014</marker>
<rawString>Will Monroe, Spence Green, and Christopher D Manning. 2014. Word segmentation of informal Arabic with domain adaptation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellis Batten Page</author>
</authors>
<title>Computer grading of student prose, using modern concepts and software.</title>
<date>1994</date>
<journal>The Journal of Experimental Education.</journal>
<contexts>
<context position="4812" citStr="Page, 1994" startWordPosition="784" endWordPosition="785"> in the context of natural language processing. 2.1 Automated Essay Scoring Since the first AES system, Project Essay Grade, was created in 1966, a number of commercial systems have been deployed. One such system, erater (Attali and Burstein, 2004), is even used as a replacement for the second human grader in the Test of English as a Foreign Language (TOEFL) and Graduate Record Examination (GRE). Other AES commercial systems also exist, such as IntelliMetric2 and Intelligent Essay Assessor (Foltz et al., 1999). AES is generally considered as a machine learning problem. Some work, such as PEG (Page, 1994) and e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA</context>
</contexts>
<marker>Page, 1994</marker>
<rawString>Ellis Batten Page. 1994. Computer grading of student prose, using modern concepts and software. The Journal of Experimental Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>Journal of Machine Learning Research.</journal>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian Processes for Machine Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14743" citStr="Rasmussen and Williams, 2006" startWordPosition="2493" endWordPosition="2496">sentation Φt(x) = * 0p) , D E (2) Φs(x) = ρx, (1 − ρ2)1/2x in R2p for the target and the source tasks. Associated with this expanded feature representation is the weight vector w def = (wt, ws) also in R2p. As we shall see in Section 4.1, such a representation eases the estimation of the parameters. The above model is related to the multi-task Gaussian Process model that has been used for joint emotion analysis (Beck et al., 2014). There, the intrinsic coregionalisation model (ICM) has been used with squared-exponential covariance function. Here, we use the simpler linear covariance function (Rasmussen and Williams, 2006), and this leads to Bayesian linear ridge regression. There are two reasons for this choice. The first is that linear combination of carefully chosen features, especially lexical ones, usually gives good performance in NLP tasks. The second is in the preceding paragraph: an intuitive feature expansion representation of the domain adaptation process that allows ease of parameter estimation. The above model is derived from the Cholesky decomposition (1 ρ) �1 � ~1 � 0 ρ = ρ 1 ρ (1 − ρ2)1/2 0 (1 − ρ2)1/2 of the desired correlation matrix that will eventually lead to equation (1). Other choices are</context>
<context position="16714" citStr="Rasmussen and Williams, 2006" startWordPosition="2846" endWordPosition="2850">ions towards zero with a = 1/10 in order to ameliorate spurious correlations. For the training data, let there be nt examples in the target domain and ns in the source domain. Let Xt (resp. Xs) be the nt-by-p (resp. ns-by-p) design matrix for the training data in the target (resp. source) domain. Let yt and ys be the corresponding observed essay scores. The expanded feature matrix due to equation (2) is 1/2Xs) . Similarly, let y be the stacking of yt and ys. Let K def = λ−1XXT + α−1I, which is also known as the Gramian for the observations. The log marginal likelihood of the training data is (Rasmussen and Williams, 2006) L = −2yTK−1y − 2 1 log |K |− nt + ns 1 2 log 2π. This is penalized to give Lp by adding (α1 − 1) log(α) − α2α + α1 log α2 − log F(α1) +(λ1 − 1) log(λ) − λ2λ + λ1 log λ2 − log F(λ1) + log(1 + a − 2aρ). The estimation of these parameters is then done by optimising Lp. In our implementation, we use scikit-learn for estimating α and λ in an inner loop, and we use gradient descent for estimating ρ in the outer loop using 1 ��γγT − K−1� ∂K � − 2a 2 tr 1 + a − 2aρ, ∂ρ def where γ = K−1y and � 0 � ∂K Xt(Xs)T = λ−1 ∂ρ Xs(Xt)T 0 4.2 Prediction We report the mean prediction as the score of an essay. Thi</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence M Rudner</author>
<author>Tahung Liang</author>
</authors>
<title>Automated essay scoring using Bayes’ theorem.</title>
<date>2002</date>
<journal>The Journal of Technology, Learning and Assessment.</journal>
<contexts>
<context position="5261" citStr="Rudner and Liang, 2002" startWordPosition="852" endWordPosition="855">st, such as IntelliMetric2 and Intelligent Essay Assessor (Foltz et al., 1999). AES is generally considered as a machine learning problem. Some work, such as PEG (Page, 1994) and e-rater, considers it as a regression problem. PEG uses a large number of features with regression to predict the human score. e-rater uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay </context>
</contexts>
<marker>Rudner, Liang, 2002</marker>
<rawString>Lawrence M Rudner and Tahung Liang. 2002. Automated essay scoring using Bayes’ theorem. The Journal of Technology, Learning and Assessment.</rawString>
</citation>
<citation valid="true">
<date>2013</date>
<booktitle>Handbook of Automated Essay Evaluation: Current Applications and New Directions.</booktitle>
<editor>Mark D. Shermis and Jill Burstein, editors.</editor>
<publisher>Routledge.</publisher>
<contexts>
<context position="2181" citStr="(2013)" startWordPosition="352" endWordPosition="352">omatically evaluate an essay written in an educational setting by giving it a score. Work related to essay scoring can be traced back to 1966 when Ellis Page created a computer grading software called Project Essay Grade (PEG). Research on AES has continued through the years. The recent Automated Student Assessment Prize (ASAP) Competition1 sponsored by the Hewlett Foundation in 2012 has renewed interest on this topic. The agreement between the scores assigned by state-of-the-art AES systems and the scores assigned by human raters has been shown to be relatively high. See Shermis and Burstein (2013) for a recent overview of AES. AES is usually treated as a supervised machine learning problem, either as a classification, regression, or rank preference task. Using this approach, a training set in the form of human graded essays is needed. However, human graded essays are not readily available. This is perhaps why research in this area was mostly done by commercial organizations. After the ASAP competition, research interest in this area has been rekindled because of the released dataset. Most of the recent AES related work is promptspecific. That is, an AES system is trained using essays f</context>
</contexts>
<marker>2013</marker>
<rawString>Mark D. Shermis and Jill Burstein, editors. 2013. Handbook of Automated Essay Evaluation: Current Applications and New Directions. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A new dataset and method for automatically grading ESOL texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5578" citStr="Yannakoudakis et al., 2011" startWordPosition="901" endWordPosition="904">uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay scores as continuous values and scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ among the systems, ranging from simple features (e.g., word length, 2http://www.vantagelearning.com/products/intellimetric/ essay length, etc) </context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
<author>Yee Seng Chan</author>
</authors>
<title>Word sense disambiguation using OntoNotes: An empirical study.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8459" citStr="Zhong et al., 2008" startWordPosition="1383" endWordPosition="1386">e we have a feature vector x in the original feature space. This scheme will map this instance using the mapping functions 4,s(x) and 4,t(x) for the source and target domain respectively, where 4,s(x) _ (x, x, 0) 4,t(x) _ (x, 0, x), and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 Automated Essay Scoring This section describes the Automated Essay Scoring (AES) task and the features we use for the task. 3.1 Task Description In AES, the input to the system is a student essay, and the output is the score assigned to the essay. The score assigned by the AES system will be compared against the human assigned score to measure their agreement. Common agreement measures used include Pearson’s correlation, Spearman’s corre</context>
</contexts>
<marker>Zhong, Ng, Chan, 2008</marker>
<rawString>Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008. Word sense disambiguation using OntoNotes: An empirical study. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000631">
<title confidence="0.997596">
Summarizing topical contents from PubMed documents using
a thematic analysis
</title>
<author confidence="0.996101">
Sun Kim, Lana Yeganova and W. John Wilbur
</author>
<affiliation confidence="0.840541333333333">
National Center for Biotechnology Information
National Library of Medicine, National Institutes of Health
Bethesda, MD 20894, USA
</affiliation>
<email confidence="0.996578">
{sun.kim,yeganova,wilbur}@ncbi.nlm.nih.gov
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930807692308">
Improving the search and browsing ex-
perience in PubMed® is a key compo-
nent in helping users detect information
of interest. In particular, when explor-
ing a novel field, it is important to pro-
vide a comprehensive view for a specific
subject. One solution for providing this
panoramic picture is to find sub-topics
from a set of documents. We propose a
method that finds sub-topics that we refer
to as themes and computes representative
titles based on a set of documents in each
theme. The method combines a thematic
clustering algorithm and the Pool Adja-
cent Violators algorithm to induce signifi-
cant themes. Then, for each theme, a title
is computed using PubMed document ti-
tles and theme-dependent term scores. We
tested our system on five disease sets from
OMIM® and evaluated the results based
on normalized point-wise mutual informa-
tion and MeSH® terms. For both perfor-
mance measures, the proposed approach
outperformed LDA. The quality of theme
titles were also evaluated by comparing
them with manually created titles.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999227">
PubMed1, currently a collection of about 25 mil-
lion bibliographic records, has grown exponen-
tially in size. With the abundance and diversity
of information in PubMed many queries retrieve
thousands of documents making it difficult for
users to browse the results and identify the infor-
mation most relevant to their topic of interest. The
query ‘cystic fibrosis’, for example, retrieves pa-
pers that discuss different aspects of the disease,
including its clinical features, treatment options,
</bodyText>
<footnote confidence="0.905071">
1http://pubmed.gov
</footnote>
<bodyText confidence="0.99945943902439">
diagnosis, etc. A possible solution to this problem
is to automatically group the retrieved documents
into meaningful thematic clusters or themes (these
terms are used interchangeably). However, clus-
tering alone does not solve the problem entirely,
as a significant amount of human post-processing
is required to infer the topic of the cluster.
There exists a vast collection of probabilistic
clustering methods. One common problem among
most of them is that different results are obtained
depending on the cluster initialization, suggesting
that some clusters are unstable or weak. How-
ever, there is no obvious way to effectively and
efficiently evaluate the quality of clusters. In this
paper, we combine EM-based thematic cluster-
ing (Kim and Wilbur, 2012) with the Pool Adja-
cent Violators (PAV) algorithm (Ayer et al., 1955;
Wilbur et al., 2005). PAV is an isotonic regression
algorithm which we use as a method for convert-
ing a score into a probability. Here, we show how
PAV can be applied to evaluate the quality of clus-
ters.
Another issue that motivated this research is that
most existing algorithms produce clusters that are
not self-descriptive. Presenting meaningful titles
can significantly improve the user perception of
clustering results. To that end, we utilize PubMed
document titles and cluster-related term scores to
automatically obtain a title for each theme. The
method results in thematic clusters of documents
with cluster titles.
Studies similar to our approach are ASI (Adap-
tive Subspace Iteration) (Li et al., 2004) and
SKWIC (Simultaneous Keyword Identification
and Clustering of text documents) (Frigui and
Nasraoui, 2004). Both perform document clus-
tering and cluster-dependent keyword identifica-
tion simultaneously. SKWIC can only produce
hard clustering, while ASI is computationally very
expensive as it heavily depends on matrix opera-
tions. A study by Hammouda et al. (2005) sug-
</bodyText>
<page confidence="0.980615">
805
</page>
<note confidence="0.658067">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999956055555555">
gests automatic keyphrase extraction from a clus-
ter of documents as a surrogate to providing a clus-
ter title, but they treat document clustering and
cluster-dependent keyword extraction as separate
problems.
Topic modeling (Hofmann, 1999; Blei et al.,
2003; Blei and Lafferty, 2005) is the most pop-
ular and an alternative approach that has a simi-
lar underlying goal of discovering hidden thematic
structure of a document collection and organizing
the collection according to the discovered topics.
Topic models are based upon the idea that docu-
ments are mixtures of topics, where a topic is a
probability distribution over words (Steyvers and
Griffiths, 2007). However, topic modeling is not a
document clustering scheme in nature. Although a
list of keywords that represent a topic is available,
the title of the cluster may not be evident.
</bodyText>
<sectionHeader confidence="0.996019" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.9999612">
We here describe the EM-based clustering algo-
rithm, and show how PAV is incorporated with
it to yield the PAV-EM thematic clustering tech-
nique. We further present a cluster summarization
method to induce theme titles.
</bodyText>
<subsectionHeader confidence="0.991784">
2.1 Theme definition
</subsectionHeader>
<bodyText confidence="0.998967818181818">
Let D be a document set and let T be the set of
terms in D. Let R denote the relation between el-
ements of T and D. tRd means t ∈ d. We define a
theme as a subject that is described by non-empty
sets U ⊆ T and V ⊆ D, where all the elements
of U have a high probability of occurring in all the
element of V . An EM framework is used to ex-
tract subject terms for a theme (Wilbur, 2002). In
addition to the observed data R, a theme is defined
by the latent indicator variables zd, {zd}d∈D. The
parameters are
</bodyText>
<equation confidence="0.997357">
O = U(kUk = nU), {pt, qt}t∈U, {rt}t∈T, (1)
</equation>
<bodyText confidence="0.900533588235294">
where nU is the size of the set U. For any t ∈ U,
pt is the probability that for any d ∈ V , tRd. qt
is the probability that for any d ∈ D − V , tRd.
For any t ∈ T, rt is the probability that for any
d ∈ D, tRd. Assuming all relations tRd are in-
dependent of each other, the goal is to obtain the
highest probabilities
p(R, {zd}|O) = p(R|{zd}, O)p({zd}|O). (2)
E-step (expectation step) evaluates the expectation
of the logarithm of Eqn. 2. M-step (maximization
Algorithm 1 PAV-EM algorithm
Let D be the dataset, where d ∈ D.
Give a value for the parameter q.
Set X = ∅.
for i ← 1, n do
Create q random clusters.
Run the theme clustering algorithm.
</bodyText>
<equation confidence="0.911849">
For each cluster C and d with pzCd ,
X ← X ∪ {&lt; pzCd ,1,1 &gt;}2 if d ∈ C,
X ← X ∪ {&lt; pzCd ,1,0 &gt;} if d ∈/C.
</equation>
<bodyText confidence="0.898263833333333">
Obtain the PAV function, PAV (pzCd ), over X.
Set S = ∅, where S is the output cluster set.
repeat
Create q random clusters for {d|d ∈/ ∪S}.
Run the theme clustering algorithm.
Select any cluster C, where
</bodyText>
<equation confidence="0.99467">
C0 = {d|d ∈ C,PAV (pzCd ) &gt; 0.9}
satisfies |C0 |&gt; 10.
S ← S ∪ {C0}.
</equation>
<bodyText confidence="0.9938677">
until no more changes in S.
step) maximizes this expectation over the parame-
ters O. For each term, t, we define a quantity αt
which is the difference between the contribution
coming from t depending on whether ut = 1 or
ut = 0. The maximization is completed by choos-
ing the nU largest αt’s and setting ut = 1 for each
of them and ut = 0 for all others. Details of this
theme extraction scheme can be found in Wilbur
(2002).
</bodyText>
<subsectionHeader confidence="0.998089">
2.2 PAV-EM thematic clustering
</subsectionHeader>
<bodyText confidence="0.999689928571429">
In thematic clustering, a document is assigned to
a theme that has the highest probability to the
document (Kim and Wilbur, 2012). Although
this approach shows a reasonable performance for
theme-based document clustering, the dynamic
nature of random initialization and multiple sub-
jects described in a document may create many
weak themes. Moreover, there is no clear guide-
line to distinguish strong and weak themes. Thus,
we here propose a method that extracts strong
themes more effectively. In the EM-based theme
extraction scheme, the log odds score pzCd indi-
cates the extent to which a document d is cou-
pled with a specific theme C. If a cluster in-
</bodyText>
<footnote confidence="0.5363955">
2The second and the third arguments in the bracket are the
weight and the probability estimate of the data, respectively.
</footnote>
<page confidence="0.994602">
806
</page>
<bodyText confidence="0.999958185185185">
cludes a reasonable number of documents that
have high pzCd s, it indicates that the cluster rep-
resents a strong theme. Therefore, we can obtain
strong themes by collecting these clusters.
Let the probability p(score) be a monoton-
ically non-decreasing function of score. The
PAV algorithm (Ayer et al., 1955; Wilbur et al.,
2005) is a regression method to derive from the
data that monotonically non-decreasing estimate
of p(score) which assigns maximal likelihood to
the data. For our approach, score = pzCd .
Algorithm 1 shows the theme clustering process
using the PAV algorithm. For the given dateset D
and the initial number of clusters q, theme cluster-
ing is performed n times, and an isotonic regres-
sion function is learned by applying the PAV algo-
rithm. Note that q is an initial guess for the number
of clusters and it is not guaranteed to remain the
same in the output set. For our experiments, we
set q = 50 and n = 100. After the PAV algorithm
is applied, theme clustering is performed. At each
iteration, we select any cluster in which there are
more than 10 documents with PAV scores higher
than 0.9. Unselected documents are re-used for
clustering in the next iteration. This procedure is
repeated until there are no more changes in the se-
lected cluster set S.
</bodyText>
<subsectionHeader confidence="0.997782">
2.3 Theme summarization
</subsectionHeader>
<bodyText confidence="0.998091">
After obtaining themes (document clusters and
their subject terms), we summarize each theme by
choosing a text segment from PubMed document
titles. A title should cover as many subject terms
as possible, but also it should be well-formed, i.e.
be descriptive enough and humanly understand-
able. To achieve this goal, we first extract all pos-
sible candidates from document titles as follows:
</bodyText>
<listItem confidence="0.904955071428572">
(i) Extract all possible candidates as n-grams,
where n = 1, ..., 20. Noun phrases are
treated as units and must be totally inside or
outside a candidate.
(ii) Check POS tags for starting and ending
words in a candidate. Starting with a con-
junction, verb, preposition and symbol is not
allowed. Ending with a conjunction, verb,
preposition, symbol, determiner, adjective or
certain pronouns is not allowed.
(iii) Discard any candidates that start or end with
‘-’ or ‘.’. The candidates including certain
characters such as ‘/’, ‘;’, ‘:’ are also re-
moved.
</listItem>
<bodyText confidence="0.913125">
(iv) Check grammatical dependency relations.
We discard candidates for which the head
word of a preposition does not appear in the
same candidate as the proposition. Also, we
validate the case, ‘between A and B’, so that
A and B are not separated.
Next, for each candidate, a score is calculated
</bodyText>
<equation confidence="0.850924">
by
score(candi) = log Ht∈U(tftαt) (3)
l lt/∈U tft ,
</equation>
<bodyText confidence="0.9999435">
where tft is the term frequency of the term t.
However, an ideal title should have enough words
to be descriptive, hence we subtract (len(candi)−
5)2 from score(candi), where len(candi) is the
number of words in candi, and choose the top
score as a title.
</bodyText>
<sectionHeader confidence="0.999021" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.99997408">
We applied our method to the five disease
sets, “cystic fibrosis”, “deafness”, “DiGeorge
syndrome”, “autism” and “hypertrophic car-
diomyopathy” from OMIM3. These sets con-
sist of 3000, 3000, 956, 2917 and 1997
PubMed documents, respectively, and are avail-
able at http://www.ncbi.nlm.nih.gov/
CBBresearch/Wilbur/IRET/PAVEM.
For evaluating PAV-EM and comparing with the
topic modeling method, latent Dirichlet allocation
(LDA) (Blei et al., 2003), both approaches were
performed 10 times for each disease set and scores
were averaged over all runs. Mallet4 was used to
run LDA. The same tokenization was applied to
LDA and PAV-EM. The number of topics given for
LDA was 50 and the recommended optimization
parameter was used for producing LDA topics.
Table 1 presents average runtimes5 for LDA
and PAV-EM. LDA and PAV-EM spent 15.2 and
13.3 seconds on average for processing the small-
est set, “DiGeorge syndrome”. However, in larger
sets, e.g. “autism”, it took 46.9 and 31.3 seconds
for LDA and PAV-EM, respectively. We also ran
another implementation6 of LDA, which was 30
times slower than Mallet. While PAV-EM and
</bodyText>
<footnote confidence="0.992141571428572">
3http://www.ncbi.nlm.nih.gov/omim
4http://mallet.cs.umass.edu
5Both methods were tested on a single linux server. The
processing times reported do not include the preprocessing
stages done by Mallet and our implementation.
6http://www.cs.princeton.edu/˜blei/
lda-c
</footnote>
<page confidence="0.9892">
807
</page>
<table confidence="0.999434166666667">
Dataset LDA PAV-EM
Set 1 25.7 18.4
Set 2 36.5 24.7
Set 3 15.2 13.3
Set 4 46.9 31.3
Set 5 30.3 19.2
</table>
<tableCaption confidence="0.9404205">
Table 1: Average runtimes for LDA and PAV-EM
in seconds. Sets 1, 2, 3, 4 and 5 are “cystic fibro-
sis”, “deafness”, “DiGeorge syndrome”, “autism”
and “hypertrophic cardiomyopathy”, respectively.
</tableCaption>
<table confidence="0.999161">
Method Topic terms
Top 5 Top 10
LDA 2.8906 10.9760
PAV-EM 4.0322 14.6213
</table>
<tableCaption confidence="0.999196">
Table 2: NMPI scores for LDA and PAV-EM.
</tableCaption>
<bodyText confidence="0.999555684210526">
LDA can be implemented in parallel computa-
tion7, this indicates that PAV-EM may be more ef-
ficient to obtain themes for a larger set of PubMed
documents.
The PAV-EM algorithm automatically learns
themes from unlabeled PubMed documents, hence
the performance measures that are used in super-
vised learning cannot be applied to our setup. Re-
cent studies have shown more interest in topic co-
herence measures (Chang et al., 2009; Newman et
al., 2010; Mimno et al., 2011), which capture the
semantic interpretability of topics based on subject
terms. Table 2 shows the topic coherence scores
measured by normalized point-wise mutual infor-
mation (NPMI). For both top 5 and top 10 sub-
ject terms, PAV-EM achieves better NMPI scores
than LDA. NPMI is known to be strongly corre-
lated with human ratings (Aletras and Stevenson,
2013; R¨oder et al., 2015) and is defined by
</bodyText>
<equation confidence="0.865731">
log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4)
</equation>
<bodyText confidence="0.999841">
where p(ti, tj) is the fraction of documents con-
taining both terms ti and tj, and N indicates the
number of top subject terms. E = 1D is the smooth-
ing factor, where D is the size of the dataset.
</bodyText>
<footnote confidence="0.700919">
MeSH (Medical Subject Headings) is a con-
trolled vocabulary for indexing and searching
biomedical literature (Lowe and Barnett, 1994).
7A parallel implementation of LDA appears in Wang et
al. (2009)
</footnote>
<table confidence="0.9739265">
MeSH Method Prec. Recall F1
LDA 0.4529 0.3827 0.4125
Top 1
PAV-EM 0.3842 0.5303 0.4427
Top 3 LDA 0.3935 0.3931 0.3925
PAV-EM 0.3388 0.5239 0.4086
</table>
<tableCaption confidence="0.897872">
Table 3: Classification performance based on top
significant MeSH terms appearing in themes.
</tableCaption>
<bodyText confidence="0.999572818181818">
MeSH terms assigned to an article are often used
to indicate the topics of the article, thus these
terms can be used to identify how well documents
are grouped by topics. In each cluster, p-values
of MeSH terms are calculated using the hypergeo-
metric distribution (Kim and Wilbur, 2001), and
the top N significant MeSH terms are used to
calculate precision, recall and F1. Table 3 com-
pares PAV-EM with LDA8 for the MeSH term-
based performance. In the table, PAV-EM pro-
vides higher recall and F1 for top 1 and top 3
MeSH terms. Higher recall has an advantage in
our task because the theme summarization pro-
cess uses a consensus among PubMed documents
to reach a theme title.
The next experiment was performed to compare
machine generated titles with manually labeled ti-
tles. Although human judgements are subjective,
it is not uncommon to collect human judgements
for evaluating topic modeling methods (Mei et al.,
2007; Chang et al., 2009; Xie and Xing, 2013). To
validate the performance of the theme summariza-
tion approach, we first chose 500 documents from
each disease set, and produced themes and titles.
For each topic, five strongest themes were chosen,
and they were shown to three human annotators
with extracted subject terms. Table 4 shows an
example of the proposed approach and the man-
ual annotation for the “hypertrophic cardiomyopa-
thy” set. Among 25 themes, our approach cor-
rectly identified 21 theme titles. We assumed that
a machine-generated title was correct if it included
any of manually annotated titles.
</bodyText>
<sectionHeader confidence="0.997175" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9997718">
This study was inspired by an EM-based thematic
clustering approach. In this probabilistic frame-
work, theme terms are iteratively selected and
documents are assigned to a most likely theme.
The number of themes is dynamically adjusted
</bodyText>
<footnote confidence="0.9790965">
8For LDA, each document was assigned to the highest
scoring topic.
</footnote>
<equation confidence="0.9907122">
N
i=2
NPMI =
�i− 1
j=1
</equation>
<page confidence="0.992169">
808
</page>
<table confidence="0.812171857142857">
Proposed approach Annotator 1 Annotator 2 Annotator 3
cytochrome c oxidase cytochrome c oxidase mitochondrial cytochrome- mitochondrial cytochrome
c-oxidase deficiency c oxidase deficiency
friedreich ataxia and dia- friedreich ataxia friedreich ataxia friedreich ataxia
betes mellitus
hepatitis c virus infection hepatitis c virus role of hepatitis c virus in hepatitis c virus infection
cardiomyopathies
</table>
<tableCaption confidence="0.983109">
Table 4: Comparison of the titles generated from the proposed approach and manual annotation for the
“hypertrophic cardiomyopathy” set.
</tableCaption>
<figure confidence="0.943003642857143">
cardiac myosin binding pro-
tein c
ptpn11 mutations in leopard
syndrome
ptpn11 mutations in leopard
syndrome
ptpn11 mutations in leopard
syndrome
cardiac myosin binding pro-
tein c
ptpn11 mutations in leopard
syndrome
myosin binding protein c cardiac myosin binding pro-
tein c
</figure>
<bodyText confidence="0.9985321">
by probabilistic evidence from documents. The
PAV algorithm is utilized to measure the quality of
themes. After themes are identified, subject term
weights and PubMed document titles are used to
form humanly understandable titles. The experi-
mental results show that our approach provides a
useful overview of a set of documents. In addition,
the method may allow for a new way of brows-
ing by semantically clustered documents as well as
searching with context-based query suggestions.
</bodyText>
<sectionHeader confidence="0.997476" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999488">
The authors would like to thank Donald C.
Comeau and Rezarta Islamaj Do˘gan for their con-
tribution to the manual evaluation. This research
was supported by the Intramural Research Pro-
gram of the NIH, National Library of Medicine.
</bodyText>
<sectionHeader confidence="0.998542" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995254">
N. Aletras and M. Stevenson. 2013. Evaluating topic
coherence using distributional semantics. In Proc.
International Conference on Computational Seman-
tics (IWCS 2013), pages 13–22, March.
M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and
E. Silverman. 1955. An empirical distribution func-
tion for sampling with incomplete information. The
Annals of Mathematical Statistics, 26(4):641–647.
D. Blei and J. Lafferty. 2005. Correlated topic models.
In Proc. Advances in Neural Information Processing
Systems (NIPS 2005), pages 147–154, December.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. M. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proc. Advances in Neural
Information Processing Systems (NIPS 2009), pages
288–296, December.
H. Frigui and O. Nasraoui. 2004. Simultaneous Clus-
tering and Dynamic Keyword Weighting for Text
Documents. Springer, New York, USA.
K. M. Hammouda, D. N. Matute, and M. S. Kamel.
2005. CorePhrase: keyphrase extraction for docu-
ment clustering. In Proc. International Conference
on Machine Learning and Data Mining, pages 265–
274, July.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proc. Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 50–57, August.
W. Kim and W. J. Wilbur. 2001. Corpus-based sta-
tistical screening for content-bearing terms. Journal
of the American Society for Information Science and
Technology, 52(3):247–259.
S. Kim and W. John Wilbur. 2012. Thematic cluster-
ing of text documents using an EM-based approach.
Journal of Biomedical Semantics, 3(Suppl 3):S6.
T. Li, S. Ma, and M. Ogihara. 2004. Document clus-
tering via adaptive subspace iteration. In Proc. An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 218–225, July.
H. J. Lowe and G. O. Barnett. 1994. Under-
standing and using the medical subject headings
(MeSH) vocabulary to perform literature searches.
The Journal of the American Medical Association,
271(14):1103–1108.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proc. ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2007), pages
490–499, August.
D. Mimno, H. M. Wallach, E. Talley, M. Leenders,
and A. McCallum. 2011. Optimizing semantic co-
herence in topic models. In Proc. Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011), pages 262–272, July.
D. Newman, J. H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
</reference>
<page confidence="0.987137">
809
</page>
<reference confidence="0.999635307692308">
In Proc. Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL 2010), pages 100–108, June.
M. R¨oder, A. Both, and A. Hinneburg. 2015. Explor-
ing the space of topic coherence measures. In Proc.
ACM International Conference on Web Search and
Data Mining (WSDM 2015), pages 399–408, Febru-
ary.
M. Steyvers and T. Griffiths. 2007. Probabilistic Topic
Models. Erlbaum, Hillsdale, NJ, USA.
Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, and E. Y.
Chang. 2009. PLDA: Parallel latent Dirichlet allo-
cation for large-scale applications. In Proc. Interna-
tional Conference on Algorithmic Aspects in Infor-
mation and Management (AAIM 2009), pages 301–
314, June.
W. John Wilbur, L. Yeganova, and W. Kim. 2005.
The synergy between PAV and AdaBoost. Machine
Learning, 61(1-3):71–103.
W. John Wilbur. 2002. A thematic analysis of the
AIDS literature. In Proc. Pacific Symposium on Bio-
computing, pages 386–397, January.
P. Xie and E. Xing. 2013. Integrating document clus-
tering and topic modeling. In Proc. Conference
on Uncertainty in Artificial Intelligence (UAI 2013),
pages 694–703, July.
</reference>
<page confidence="0.997609">
810
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927267">
<title confidence="0.9902485">Summarizing topical contents from PubMed documents a thematic analysis</title>
<author confidence="0.999478">Sun Kim</author>
<author confidence="0.999478">Lana Yeganova</author>
<author confidence="0.999478">W John</author>
<affiliation confidence="0.983514">National Center for Biotechnology National Library of Medicine, National Institutes of</affiliation>
<address confidence="0.989438">Bethesda, MD 20894,</address>
<abstract confidence="0.999507925925926">Improving the search and browsing experience in PubMed® is a key component in helping users detect information of interest. In particular, when exploring a novel field, it is important to provide a comprehensive view for a specific subject. One solution for providing this panoramic picture is to find sub-topics from a set of documents. We propose a method that finds sub-topics that we refer to as themes and computes representative titles based on a set of documents in each theme. The method combines a thematic clustering algorithm and the Pool Adjacent Violators algorithm to induce significant themes. Then, for each theme, a title is computed using PubMed document titles and theme-dependent term scores. We tested our system on five disease sets from OMIM® and evaluated the results based on normalized point-wise mutual informaand For both performance measures, the proposed approach outperformed LDA. The quality of theme titles were also evaluated by comparing them with manually created titles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Aletras</author>
<author>M Stevenson</author>
</authors>
<title>Evaluating topic coherence using distributional semantics.</title>
<date>2013</date>
<booktitle>In Proc. International Conference on Computational Semantics (IWCS</booktitle>
<pages>13--22</pages>
<contexts>
<context position="13343" citStr="Aletras and Stevenson, 2013" startWordPosition="2232" endWordPosition="2235">ns themes from unlabeled PubMed documents, hence the performance measures that are used in supervised learning cannot be applied to our setup. Recent studies have shown more interest in topic coherence measures (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2011), which capture the semantic interpretability of topics based on subject terms. Table 2 shows the topic coherence scores measured by normalized point-wise mutual information (NPMI). For both top 5 and top 10 subject terms, PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of top subject terms. E = 1D is the smoothing factor, where D is the size of the dataset. MeSH (Medical Subject Headings) is a controlled vocabulary for indexing and searching biomedical literature (Lowe and Barnett, 1994). 7A parallel implementation of LDA appears in Wang et al. (2009) MeSH Method Prec. Recall F1 LDA 0.4529 0.3827 0.4125 Top 1 PAV-EM 0.3842 0.5303 0.4427 Top 3 LDA 0.3935 0.3931 0.</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>N. Aletras and M. Stevenson. 2013. Evaluating topic coherence using distributional semantics. In Proc. International Conference on Computational Semantics (IWCS 2013), pages 13–22, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ayer</author>
<author>H D Brunk</author>
<author>G M Ewing</author>
<author>W T Reid</author>
<author>E Silverman</author>
</authors>
<title>An empirical distribution function for sampling with incomplete information.</title>
<date>1955</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="2679" citStr="Ayer et al., 1955" startWordPosition="408" endWordPosition="411">ne does not solve the problem entirely, as a significant amount of human post-processing is required to infer the topic of the cluster. There exists a vast collection of probabilistic clustering methods. One common problem among most of them is that different results are obtained depending on the cluster initialization, suggesting that some clusters are unstable or weak. However, there is no obvious way to effectively and efficiently evaluate the quality of clusters. In this paper, we combine EM-based thematic clustering (Kim and Wilbur, 2012) with the Pool Adjacent Violators (PAV) algorithm (Ayer et al., 1955; Wilbur et al., 2005). PAV is an isotonic regression algorithm which we use as a method for converting a score into a probability. Here, we show how PAV can be applied to evaluate the quality of clusters. Another issue that motivated this research is that most existing algorithms produce clusters that are not self-descriptive. Presenting meaningful titles can significantly improve the user perception of clustering results. To that end, we utilize PubMed document titles and cluster-related term scores to automatically obtain a title for each theme. The method results in thematic clusters of do</context>
<context position="8170" citStr="Ayer et al., 1955" startWordPosition="1381" endWordPosition="1384">themes more effectively. In the EM-based theme extraction scheme, the log odds score pzCd indicates the extent to which a document d is coupled with a specific theme C. If a cluster in2The second and the third arguments in the bracket are the weight and the probability estimate of the data, respectively. 806 cludes a reasonable number of documents that have high pzCd s, it indicates that the cluster represents a strong theme. Therefore, we can obtain strong themes by collecting these clusters. Let the probability p(score) be a monotonically non-decreasing function of score. The PAV algorithm (Ayer et al., 1955; Wilbur et al., 2005) is a regression method to derive from the data that monotonically non-decreasing estimate of p(score) which assigns maximal likelihood to the data. For our approach, score = pzCd . Algorithm 1 shows the theme clustering process using the PAV algorithm. For the given dateset D and the initial number of clusters q, theme clustering is performed n times, and an isotonic regression function is learned by applying the PAV algorithm. Note that q is an initial guess for the number of clusters and it is not guaranteed to remain the same in the output set. For our experiments, we</context>
</contexts>
<marker>Ayer, Brunk, Ewing, Reid, Silverman, 1955</marker>
<rawString>M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. 1955. An empirical distribution function for sampling with incomplete information. The Annals of Mathematical Statistics, 26(4):641–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2005</date>
<booktitle>In Proc. Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>147--154</pages>
<contexts>
<context position="4244" citStr="Blei and Lafferty, 2005" startWordPosition="643" endWordPosition="646">ce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extraction from a cluster of documents as a surrogate to providing a cluster title, but they treat document clustering and cluster-dependent keyword extraction as separate problems. Topic modeling (Hofmann, 1999; Blei et al., 2003; Blei and Lafferty, 2005) is the most popular and an alternative approach that has a similar underlying goal of discovering hidden thematic structure of a document collection and organizing the collection according to the discovered topics. Topic models are based upon the idea that documents are mixtures of topics, where a topic is a probability distribution over words (Steyvers and Griffiths, 2007). However, topic modeling is not a document clustering scheme in nature. Although a list of keywords that represent a topic is available, the title of the cluster may not be evident. 2 Methods We here describe the EM-based </context>
</contexts>
<marker>Blei, Lafferty, 2005</marker>
<rawString>D. Blei and J. Lafferty. 2005. Correlated topic models. In Proc. Advances in Neural Information Processing Systems (NIPS 2005), pages 147–154, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4218" citStr="Blei et al., 2003" startWordPosition="639" endWordPosition="642">KWIC can only produce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extraction from a cluster of documents as a surrogate to providing a cluster title, but they treat document clustering and cluster-dependent keyword extraction as separate problems. Topic modeling (Hofmann, 1999; Blei et al., 2003; Blei and Lafferty, 2005) is the most popular and an alternative approach that has a similar underlying goal of discovering hidden thematic structure of a document collection and organizing the collection according to the discovered topics. Topic models are based upon the idea that documents are mixtures of topics, where a topic is a probability distribution over words (Steyvers and Griffiths, 2007). However, topic modeling is not a document clustering scheme in nature. Although a list of keywords that represent a topic is available, the title of the cluster may not be evident. 2 Methods We h</context>
<context position="11177" citStr="Blei et al., 2003" startWordPosition="1880" endWordPosition="1883">escriptive, hence we subtract (len(candi)− 5)2 from score(candi), where len(candi) is the number of words in candi, and choose the top score as a title. 3 Experimental Results We applied our method to the five disease sets, “cystic fibrosis”, “deafness”, “DiGeorge syndrome”, “autism” and “hypertrophic cardiomyopathy” from OMIM3. These sets consist of 3000, 3000, 956, 2917 and 1997 PubMed documents, respectively, and are available at http://www.ncbi.nlm.nih.gov/ CBBresearch/Wilbur/IRET/PAVEM. For evaluating PAV-EM and comparing with the topic modeling method, latent Dirichlet allocation (LDA) (Blei et al., 2003), both approaches were performed 10 times for each disease set and scores were averaged over all runs. Mallet4 was used to run LDA. The same tokenization was applied to LDA and PAV-EM. The number of topics given for LDA was 50 and the recommended optimization parameter was used for producing LDA topics. Table 1 presents average runtimes5 for LDA and PAV-EM. LDA and PAV-EM spent 15.2 and 13.3 seconds on average for processing the smallest set, “DiGeorge syndrome”. However, in larger sets, e.g. “autism”, it took 46.9 and 31.3 seconds for LDA and PAV-EM, respectively. We also ran another implemen</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>S Gerrish</author>
<author>C Wang</author>
<author>D M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proc. Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>288--296</pages>
<contexts>
<context position="12946" citStr="Chang et al., 2009" startWordPosition="2166" endWordPosition="2169">”, “DiGeorge syndrome”, “autism” and “hypertrophic cardiomyopathy”, respectively. Method Topic terms Top 5 Top 10 LDA 2.8906 10.9760 PAV-EM 4.0322 14.6213 Table 2: NMPI scores for LDA and PAV-EM. LDA can be implemented in parallel computation7, this indicates that PAV-EM may be more efficient to obtain themes for a larger set of PubMed documents. The PAV-EM algorithm automatically learns themes from unlabeled PubMed documents, hence the performance measures that are used in supervised learning cannot be applied to our setup. Recent studies have shown more interest in topic coherence measures (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2011), which capture the semantic interpretability of topics based on subject terms. Table 2 shows the topic coherence scores measured by normalized point-wise mutual information (NPMI). For both top 5 and top 10 subject terms, PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of t</context>
<context position="15006" citStr="Chang et al., 2009" startWordPosition="2519" endWordPosition="2522">terms are used to calculate precision, recall and F1. Table 3 compares PAV-EM with LDA8 for the MeSH termbased performance. In the table, PAV-EM provides higher recall and F1 for top 1 and top 3 MeSH terms. Higher recall has an advantage in our task because the theme summarization process uses a consensus among PubMed documents to reach a theme title. The next experiment was performed to compare machine generated titles with manually labeled titles. Although human judgements are subjective, it is not uncommon to collect human judgements for evaluating topic modeling methods (Mei et al., 2007; Chang et al., 2009; Xie and Xing, 2013). To validate the performance of the theme summarization approach, we first chose 500 documents from each disease set, and produced themes and titles. For each topic, five strongest themes were chosen, and they were shown to three human annotators with extracted subject terms. Table 4 shows an example of the proposed approach and the manual annotation for the “hypertrophic cardiomyopathy” set. Among 25 themes, our approach correctly identified 21 theme titles. We assumed that a machine-generated title was correct if it included any of manually annotated titles. 4 Conclusio</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proc. Advances in Neural Information Processing Systems (NIPS 2009), pages 288–296, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Frigui</author>
<author>O Nasraoui</author>
</authors>
<title>Simultaneous Clustering and Dynamic Keyword Weighting for Text Documents.</title>
<date>2004</date>
<publisher>Springer,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="3504" citStr="Frigui and Nasraoui, 2004" startWordPosition="536" endWordPosition="539">sters. Another issue that motivated this research is that most existing algorithms produce clusters that are not self-descriptive. Presenting meaningful titles can significantly improve the user perception of clustering results. To that end, we utilize PubMed document titles and cluster-related term scores to automatically obtain a title for each theme. The method results in thematic clusters of documents with cluster titles. Studies similar to our approach are ASI (Adaptive Subspace Iteration) (Li et al., 2004) and SKWIC (Simultaneous Keyword Identification and Clustering of text documents) (Frigui and Nasraoui, 2004). Both perform document clustering and cluster-dependent keyword identification simultaneously. SKWIC can only produce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extraction from a cluster of documents as a surrogate to providing a cluster title, but they treat document clusteri</context>
</contexts>
<marker>Frigui, Nasraoui, 2004</marker>
<rawString>H. Frigui and O. Nasraoui. 2004. Simultaneous Clustering and Dynamic Keyword Weighting for Text Documents. Springer, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Hammouda</author>
<author>D N Matute</author>
<author>M S Kamel</author>
</authors>
<title>CorePhrase: keyphrase extraction for document clustering.</title>
<date>2005</date>
<booktitle>In Proc. International Conference on Machine Learning and Data Mining,</booktitle>
<pages>265--274</pages>
<contexts>
<context position="3761" citStr="Hammouda et al. (2005)" startWordPosition="574" endWordPosition="577">ocument titles and cluster-related term scores to automatically obtain a title for each theme. The method results in thematic clusters of documents with cluster titles. Studies similar to our approach are ASI (Adaptive Subspace Iteration) (Li et al., 2004) and SKWIC (Simultaneous Keyword Identification and Clustering of text documents) (Frigui and Nasraoui, 2004). Both perform document clustering and cluster-dependent keyword identification simultaneously. SKWIC can only produce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extraction from a cluster of documents as a surrogate to providing a cluster title, but they treat document clustering and cluster-dependent keyword extraction as separate problems. Topic modeling (Hofmann, 1999; Blei et al., 2003; Blei and Lafferty, 2005) is the most popular and an alternative approach that has a similar underlying goal of discovering hidden thematic st</context>
</contexts>
<marker>Hammouda, Matute, Kamel, 2005</marker>
<rawString>K. M. Hammouda, D. N. Matute, and M. S. Kamel. 2005. CorePhrase: keyphrase extraction for document clustering. In Proc. International Conference on Machine Learning and Data Mining, pages 265– 274, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing. In</title>
<date>1999</date>
<booktitle>Proc. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="4199" citStr="Hofmann, 1999" startWordPosition="637" endWordPosition="638">multaneously. SKWIC can only produce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extraction from a cluster of documents as a surrogate to providing a cluster title, but they treat document clustering and cluster-dependent keyword extraction as separate problems. Topic modeling (Hofmann, 1999; Blei et al., 2003; Blei and Lafferty, 2005) is the most popular and an alternative approach that has a similar underlying goal of discovering hidden thematic structure of a document collection and organizing the collection according to the discovered topics. Topic models are based upon the idea that documents are mixtures of topics, where a topic is a probability distribution over words (Steyvers and Griffiths, 2007). However, topic modeling is not a document clustering scheme in nature. Although a list of keywords that represent a topic is available, the title of the cluster may not be evid</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic latent semantic indexing. In Proc. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 50–57, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kim</author>
<author>W J Wilbur</author>
</authors>
<title>Corpus-based statistical screening for content-bearing terms.</title>
<date>2001</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="14355" citStr="Kim and Wilbur, 2001" startWordPosition="2407" endWordPosition="2410">ature (Lowe and Barnett, 1994). 7A parallel implementation of LDA appears in Wang et al. (2009) MeSH Method Prec. Recall F1 LDA 0.4529 0.3827 0.4125 Top 1 PAV-EM 0.3842 0.5303 0.4427 Top 3 LDA 0.3935 0.3931 0.3925 PAV-EM 0.3388 0.5239 0.4086 Table 3: Classification performance based on top significant MeSH terms appearing in themes. MeSH terms assigned to an article are often used to indicate the topics of the article, thus these terms can be used to identify how well documents are grouped by topics. In each cluster, p-values of MeSH terms are calculated using the hypergeometric distribution (Kim and Wilbur, 2001), and the top N significant MeSH terms are used to calculate precision, recall and F1. Table 3 compares PAV-EM with LDA8 for the MeSH termbased performance. In the table, PAV-EM provides higher recall and F1 for top 1 and top 3 MeSH terms. Higher recall has an advantage in our task because the theme summarization process uses a consensus among PubMed documents to reach a theme title. The next experiment was performed to compare machine generated titles with manually labeled titles. Although human judgements are subjective, it is not uncommon to collect human judgements for evaluating topic mod</context>
</contexts>
<marker>Kim, Wilbur, 2001</marker>
<rawString>W. Kim and W. J. Wilbur. 2001. Corpus-based statistical screening for content-bearing terms. Journal of the American Society for Information Science and Technology, 52(3):247–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>W John Wilbur</author>
</authors>
<title>Thematic clustering of text documents using an EM-based approach.</title>
<date>2012</date>
<journal>Journal of Biomedical Semantics,</journal>
<volume>3</volume>
<pages>3--6</pages>
<contexts>
<context position="2611" citStr="Kim and Wilbur, 2012" startWordPosition="396" endWordPosition="399">r themes (these terms are used interchangeably). However, clustering alone does not solve the problem entirely, as a significant amount of human post-processing is required to infer the topic of the cluster. There exists a vast collection of probabilistic clustering methods. One common problem among most of them is that different results are obtained depending on the cluster initialization, suggesting that some clusters are unstable or weak. However, there is no obvious way to effectively and efficiently evaluate the quality of clusters. In this paper, we combine EM-based thematic clustering (Kim and Wilbur, 2012) with the Pool Adjacent Violators (PAV) algorithm (Ayer et al., 1955; Wilbur et al., 2005). PAV is an isotonic regression algorithm which we use as a method for converting a score into a probability. Here, we show how PAV can be applied to evaluate the quality of clusters. Another issue that motivated this research is that most existing algorithms produce clusters that are not self-descriptive. Presenting meaningful titles can significantly improve the user perception of clustering results. To that end, we utilize PubMed document titles and cluster-related term scores to automatically obtain a</context>
<context position="7212" citStr="Kim and Wilbur, 2012" startWordPosition="1223" endWordPosition="1226">} satisfies |C0 |&gt; 10. S ← S ∪ {C0}. until no more changes in S. step) maximizes this expectation over the parameters O. For each term, t, we define a quantity αt which is the difference between the contribution coming from t depending on whether ut = 1 or ut = 0. The maximization is completed by choosing the nU largest αt’s and setting ut = 1 for each of them and ut = 0 for all others. Details of this theme extraction scheme can be found in Wilbur (2002). 2.2 PAV-EM thematic clustering In thematic clustering, a document is assigned to a theme that has the highest probability to the document (Kim and Wilbur, 2012). Although this approach shows a reasonable performance for theme-based document clustering, the dynamic nature of random initialization and multiple subjects described in a document may create many weak themes. Moreover, there is no clear guideline to distinguish strong and weak themes. Thus, we here propose a method that extracts strong themes more effectively. In the EM-based theme extraction scheme, the log odds score pzCd indicates the extent to which a document d is coupled with a specific theme C. If a cluster in2The second and the third arguments in the bracket are the weight and the p</context>
</contexts>
<marker>Kim, Wilbur, 2012</marker>
<rawString>S. Kim and W. John Wilbur. 2012. Thematic clustering of text documents using an EM-based approach. Journal of Biomedical Semantics, 3(Suppl 3):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Li</author>
<author>S Ma</author>
<author>M Ogihara</author>
</authors>
<title>Document clustering via adaptive subspace iteration. In</title>
<date>2004</date>
<booktitle>Proc. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="3395" citStr="Li et al., 2004" startWordPosition="522" endWordPosition="525">ing a score into a probability. Here, we show how PAV can be applied to evaluate the quality of clusters. Another issue that motivated this research is that most existing algorithms produce clusters that are not self-descriptive. Presenting meaningful titles can significantly improve the user perception of clustering results. To that end, we utilize PubMed document titles and cluster-related term scores to automatically obtain a title for each theme. The method results in thematic clusters of documents with cluster titles. Studies similar to our approach are ASI (Adaptive Subspace Iteration) (Li et al., 2004) and SKWIC (Simultaneous Keyword Identification and Clustering of text documents) (Frigui and Nasraoui, 2004). Both perform document clustering and cluster-dependent keyword identification simultaneously. SKWIC can only produce hard clustering, while ASI is computationally very expensive as it heavily depends on matrix operations. A study by Hammouda et al. (2005) sug805 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 805–810, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. gests automatic keyphrase extract</context>
</contexts>
<marker>Li, Ma, Ogihara, 2004</marker>
<rawString>T. Li, S. Ma, and M. Ogihara. 2004. Document clustering via adaptive subspace iteration. In Proc. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 218–225, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Lowe</author>
<author>G O Barnett</author>
</authors>
<title>Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.</title>
<date>1994</date>
<journal>The Journal of the American Medical Association,</journal>
<volume>271</volume>
<issue>14</issue>
<contexts>
<context position="13764" citStr="Lowe and Barnett, 1994" startWordPosition="2309" endWordPosition="2312">e mutual information (NPMI). For both top 5 and top 10 subject terms, PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of top subject terms. E = 1D is the smoothing factor, where D is the size of the dataset. MeSH (Medical Subject Headings) is a controlled vocabulary for indexing and searching biomedical literature (Lowe and Barnett, 1994). 7A parallel implementation of LDA appears in Wang et al. (2009) MeSH Method Prec. Recall F1 LDA 0.4529 0.3827 0.4125 Top 1 PAV-EM 0.3842 0.5303 0.4427 Top 3 LDA 0.3935 0.3931 0.3925 PAV-EM 0.3388 0.5239 0.4086 Table 3: Classification performance based on top significant MeSH terms appearing in themes. MeSH terms assigned to an article are often used to indicate the topics of the article, thus these terms can be used to identify how well documents are grouped by topics. In each cluster, p-values of MeSH terms are calculated using the hypergeometric distribution (Kim and Wilbur, 2001), and the</context>
</contexts>
<marker>Lowe, Barnett, 1994</marker>
<rawString>H. J. Lowe and G. O. Barnett. 1994. Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches. The Journal of the American Medical Association, 271(14):1103–1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Shen</author>
<author>C Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<pages>490--499</pages>
<contexts>
<context position="14986" citStr="Mei et al., 2007" startWordPosition="2515" endWordPosition="2518"> significant MeSH terms are used to calculate precision, recall and F1. Table 3 compares PAV-EM with LDA8 for the MeSH termbased performance. In the table, PAV-EM provides higher recall and F1 for top 1 and top 3 MeSH terms. Higher recall has an advantage in our task because the theme summarization process uses a consensus among PubMed documents to reach a theme title. The next experiment was performed to compare machine generated titles with manually labeled titles. Although human judgements are subjective, it is not uncommon to collect human judgements for evaluating topic modeling methods (Mei et al., 2007; Chang et al., 2009; Xie and Xing, 2013). To validate the performance of the theme summarization approach, we first chose 500 documents from each disease set, and produced themes and titles. For each topic, five strongest themes were chosen, and they were shown to three human annotators with extracted subject terms. Table 4 shows an example of the proposed approach and the manual annotation for the “hypertrophic cardiomyopathy” set. Among 25 themes, our approach correctly identified 21 theme titles. We assumed that a machine-generated title was correct if it included any of manually annotated</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Q. Mei, X. Shen, and C. Zhai. 2007. Automatic labeling of multinomial topic models. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2007), pages 490–499, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H M Wallach</author>
<author>E Talley</author>
<author>M Leenders</author>
<author>A McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP 2011),</booktitle>
<pages>262--272</pages>
<contexts>
<context position="12988" citStr="Mimno et al., 2011" startWordPosition="2174" endWordPosition="2177">ertrophic cardiomyopathy”, respectively. Method Topic terms Top 5 Top 10 LDA 2.8906 10.9760 PAV-EM 4.0322 14.6213 Table 2: NMPI scores for LDA and PAV-EM. LDA can be implemented in parallel computation7, this indicates that PAV-EM may be more efficient to obtain themes for a larger set of PubMed documents. The PAV-EM algorithm automatically learns themes from unlabeled PubMed documents, hence the performance measures that are used in supervised learning cannot be applied to our setup. Recent studies have shown more interest in topic coherence measures (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2011), which capture the semantic interpretability of topics based on subject terms. Table 2 shows the topic coherence scores measured by normalized point-wise mutual information (NPMI). For both top 5 and top 10 subject terms, PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of top subject terms. E = 1D is the smoothing </context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>D. Mimno, H. M. Wallach, E. Talley, M. Leenders, and A. McCallum. 2011. Optimizing semantic coherence in topic models. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 262–272, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>J H Lau</author>
<author>K Grieser</author>
<author>T Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<contexts>
<context position="12967" citStr="Newman et al., 2010" startWordPosition="2170" endWordPosition="2173">e”, “autism” and “hypertrophic cardiomyopathy”, respectively. Method Topic terms Top 5 Top 10 LDA 2.8906 10.9760 PAV-EM 4.0322 14.6213 Table 2: NMPI scores for LDA and PAV-EM. LDA can be implemented in parallel computation7, this indicates that PAV-EM may be more efficient to obtain themes for a larger set of PubMed documents. The PAV-EM algorithm automatically learns themes from unlabeled PubMed documents, hence the performance measures that are used in supervised learning cannot be applied to our setup. Recent studies have shown more interest in topic coherence measures (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2011), which capture the semantic interpretability of topics based on subject terms. Table 2 shows the topic coherence scores measured by normalized point-wise mutual information (NPMI). For both top 5 and top 10 subject terms, PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of top subject terms. E =</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. 2010. Automatic evaluation of topic coherence.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proc. Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2010),</booktitle>
<pages>100--108</pages>
<marker></marker>
<rawString>In Proc. Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2010), pages 100–108, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R¨oder</author>
<author>A Both</author>
<author>A Hinneburg</author>
</authors>
<title>Exploring the space of topic coherence measures. In</title>
<date>2015</date>
<booktitle>Proc. ACM International Conference on Web Search and Data Mining (WSDM 2015),</booktitle>
<pages>399--408</pages>
<marker>R¨oder, Both, Hinneburg, 2015</marker>
<rawString>M. R¨oder, A. Both, and A. Hinneburg. 2015. Exploring the space of topic coherence measures. In Proc. ACM International Conference on Web Search and Data Mining (WSDM 2015), pages 399–408, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>T Griffiths</author>
</authors>
<title>Probabilistic Topic Models. Erlbaum,</title>
<date>2007</date>
<location>Hillsdale, NJ, USA.</location>
<contexts>
<context position="4621" citStr="Steyvers and Griffiths, 2007" startWordPosition="704" endWordPosition="707">action from a cluster of documents as a surrogate to providing a cluster title, but they treat document clustering and cluster-dependent keyword extraction as separate problems. Topic modeling (Hofmann, 1999; Blei et al., 2003; Blei and Lafferty, 2005) is the most popular and an alternative approach that has a similar underlying goal of discovering hidden thematic structure of a document collection and organizing the collection according to the discovered topics. Topic models are based upon the idea that documents are mixtures of topics, where a topic is a probability distribution over words (Steyvers and Griffiths, 2007). However, topic modeling is not a document clustering scheme in nature. Although a list of keywords that represent a topic is available, the title of the cluster may not be evident. 2 Methods We here describe the EM-based clustering algorithm, and show how PAV is incorporated with it to yield the PAV-EM thematic clustering technique. We further present a cluster summarization method to induce theme titles. 2.1 Theme definition Let D be a document set and let T be the set of terms in D. Let R denote the relation between elements of T and D. tRd means t ∈ d. We define a theme as a subject that </context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>M. Steyvers and T. Griffiths. 2007. Probabilistic Topic Models. Erlbaum, Hillsdale, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>H Bai</author>
<author>M Stanton</author>
<author>W-Y Chen</author>
<author>E Y Chang</author>
</authors>
<title>PLDA: Parallel latent Dirichlet allocation for large-scale applications.</title>
<date>2009</date>
<booktitle>In Proc. International Conference on Algorithmic Aspects in Information and Management (AAIM</booktitle>
<pages>301--314</pages>
<contexts>
<context position="13829" citStr="Wang et al. (2009)" startWordPosition="2320" endWordPosition="2323">PAV-EM achieves better NMPI scores than LDA. NPMI is known to be strongly correlated with human ratings (Aletras and Stevenson, 2013; R¨oder et al., 2015) and is defined by log p(ti,tj)+� p(ti)p(tj) − log (p(ti, tj) + �), (4) where p(ti, tj) is the fraction of documents containing both terms ti and tj, and N indicates the number of top subject terms. E = 1D is the smoothing factor, where D is the size of the dataset. MeSH (Medical Subject Headings) is a controlled vocabulary for indexing and searching biomedical literature (Lowe and Barnett, 1994). 7A parallel implementation of LDA appears in Wang et al. (2009) MeSH Method Prec. Recall F1 LDA 0.4529 0.3827 0.4125 Top 1 PAV-EM 0.3842 0.5303 0.4427 Top 3 LDA 0.3935 0.3931 0.3925 PAV-EM 0.3388 0.5239 0.4086 Table 3: Classification performance based on top significant MeSH terms appearing in themes. MeSH terms assigned to an article are often used to indicate the topics of the article, thus these terms can be used to identify how well documents are grouped by topics. In each cluster, p-values of MeSH terms are calculated using the hypergeometric distribution (Kim and Wilbur, 2001), and the top N significant MeSH terms are used to calculate precision, re</context>
</contexts>
<marker>Wang, Bai, Stanton, Chen, Chang, 2009</marker>
<rawString>Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, and E. Y. Chang. 2009. PLDA: Parallel latent Dirichlet allocation for large-scale applications. In Proc. International Conference on Algorithmic Aspects in Information and Management (AAIM 2009), pages 301– 314, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
<author>L Yeganova</author>
<author>W Kim</author>
</authors>
<date>2005</date>
<booktitle>The synergy between PAV and AdaBoost. Machine Learning,</booktitle>
<pages>61--1</pages>
<contexts>
<context position="2701" citStr="Wilbur et al., 2005" startWordPosition="412" endWordPosition="415">he problem entirely, as a significant amount of human post-processing is required to infer the topic of the cluster. There exists a vast collection of probabilistic clustering methods. One common problem among most of them is that different results are obtained depending on the cluster initialization, suggesting that some clusters are unstable or weak. However, there is no obvious way to effectively and efficiently evaluate the quality of clusters. In this paper, we combine EM-based thematic clustering (Kim and Wilbur, 2012) with the Pool Adjacent Violators (PAV) algorithm (Ayer et al., 1955; Wilbur et al., 2005). PAV is an isotonic regression algorithm which we use as a method for converting a score into a probability. Here, we show how PAV can be applied to evaluate the quality of clusters. Another issue that motivated this research is that most existing algorithms produce clusters that are not self-descriptive. Presenting meaningful titles can significantly improve the user perception of clustering results. To that end, we utilize PubMed document titles and cluster-related term scores to automatically obtain a title for each theme. The method results in thematic clusters of documents with cluster t</context>
<context position="8192" citStr="Wilbur et al., 2005" startWordPosition="1385" endWordPosition="1388">vely. In the EM-based theme extraction scheme, the log odds score pzCd indicates the extent to which a document d is coupled with a specific theme C. If a cluster in2The second and the third arguments in the bracket are the weight and the probability estimate of the data, respectively. 806 cludes a reasonable number of documents that have high pzCd s, it indicates that the cluster represents a strong theme. Therefore, we can obtain strong themes by collecting these clusters. Let the probability p(score) be a monotonically non-decreasing function of score. The PAV algorithm (Ayer et al., 1955; Wilbur et al., 2005) is a regression method to derive from the data that monotonically non-decreasing estimate of p(score) which assigns maximal likelihood to the data. For our approach, score = pzCd . Algorithm 1 shows the theme clustering process using the PAV algorithm. For the given dateset D and the initial number of clusters q, theme clustering is performed n times, and an isotonic regression function is learned by applying the PAV algorithm. Note that q is an initial guess for the number of clusters and it is not guaranteed to remain the same in the output set. For our experiments, we set q = 50 and n = 10</context>
</contexts>
<marker>Wilbur, Yeganova, Kim, 2005</marker>
<rawString>W. John Wilbur, L. Yeganova, and W. Kim. 2005. The synergy between PAV and AdaBoost. Machine Learning, 61(1-3):71–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
</authors>
<title>A thematic analysis of the AIDS literature.</title>
<date>2002</date>
<booktitle>In Proc. Pacific Symposium on Biocomputing,</booktitle>
<pages>386--397</pages>
<contexts>
<context position="5435" citStr="Wilbur, 2002" startWordPosition="864" endWordPosition="865">describe the EM-based clustering algorithm, and show how PAV is incorporated with it to yield the PAV-EM thematic clustering technique. We further present a cluster summarization method to induce theme titles. 2.1 Theme definition Let D be a document set and let T be the set of terms in D. Let R denote the relation between elements of T and D. tRd means t ∈ d. We define a theme as a subject that is described by non-empty sets U ⊆ T and V ⊆ D, where all the elements of U have a high probability of occurring in all the element of V . An EM framework is used to extract subject terms for a theme (Wilbur, 2002). In addition to the observed data R, a theme is defined by the latent indicator variables zd, {zd}d∈D. The parameters are O = U(kUk = nU), {pt, qt}t∈U, {rt}t∈T, (1) where nU is the size of the set U. For any t ∈ U, pt is the probability that for any d ∈ V , tRd. qt is the probability that for any d ∈ D − V , tRd. For any t ∈ T, rt is the probability that for any d ∈ D, tRd. Assuming all relations tRd are independent of each other, the goal is to obtain the highest probabilities p(R, {zd}|O) = p(R|{zd}, O)p({zd}|O). (2) E-step (expectation step) evaluates the expectation of the logarithm of Eq</context>
<context position="7050" citStr="Wilbur (2002)" startWordPosition="1199" endWordPosition="1200">ster set. repeat Create q random clusters for {d|d ∈/ ∪S}. Run the theme clustering algorithm. Select any cluster C, where C0 = {d|d ∈ C,PAV (pzCd ) &gt; 0.9} satisfies |C0 |&gt; 10. S ← S ∪ {C0}. until no more changes in S. step) maximizes this expectation over the parameters O. For each term, t, we define a quantity αt which is the difference between the contribution coming from t depending on whether ut = 1 or ut = 0. The maximization is completed by choosing the nU largest αt’s and setting ut = 1 for each of them and ut = 0 for all others. Details of this theme extraction scheme can be found in Wilbur (2002). 2.2 PAV-EM thematic clustering In thematic clustering, a document is assigned to a theme that has the highest probability to the document (Kim and Wilbur, 2012). Although this approach shows a reasonable performance for theme-based document clustering, the dynamic nature of random initialization and multiple subjects described in a document may create many weak themes. Moreover, there is no clear guideline to distinguish strong and weak themes. Thus, we here propose a method that extracts strong themes more effectively. In the EM-based theme extraction scheme, the log odds score pzCd indicat</context>
</contexts>
<marker>Wilbur, 2002</marker>
<rawString>W. John Wilbur. 2002. A thematic analysis of the AIDS literature. In Proc. Pacific Symposium on Biocomputing, pages 386–397, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Xie</author>
<author>E Xing</author>
</authors>
<title>Integrating document clustering and topic modeling.</title>
<date>2013</date>
<booktitle>In Proc. Conference on Uncertainty in Artificial Intelligence (UAI</booktitle>
<pages>694--703</pages>
<contexts>
<context position="15027" citStr="Xie and Xing, 2013" startWordPosition="2523" endWordPosition="2526">lculate precision, recall and F1. Table 3 compares PAV-EM with LDA8 for the MeSH termbased performance. In the table, PAV-EM provides higher recall and F1 for top 1 and top 3 MeSH terms. Higher recall has an advantage in our task because the theme summarization process uses a consensus among PubMed documents to reach a theme title. The next experiment was performed to compare machine generated titles with manually labeled titles. Although human judgements are subjective, it is not uncommon to collect human judgements for evaluating topic modeling methods (Mei et al., 2007; Chang et al., 2009; Xie and Xing, 2013). To validate the performance of the theme summarization approach, we first chose 500 documents from each disease set, and produced themes and titles. For each topic, five strongest themes were chosen, and they were shown to three human annotators with extracted subject terms. Table 4 shows an example of the proposed approach and the manual annotation for the “hypertrophic cardiomyopathy” set. Among 25 themes, our approach correctly identified 21 theme titles. We assumed that a machine-generated title was correct if it included any of manually annotated titles. 4 Conclusion This study was insp</context>
</contexts>
<marker>Xie, Xing, 2013</marker>
<rawString>P. Xie and E. Xing. 2013. Integrating document clustering and topic modeling. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI 2013), pages 694–703, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997432">
Bilingual Correspondence Recursive Autoencoders
for Statistical Machine Translation
</title>
<author confidence="0.999714">
Jinsong Su1, Deyi Xiong?∗, Biao Zhang1, Yang Liu3, Junfeng Yao1, Min Zhang?
</author>
<affiliation confidence="0.946964">
Xiamen University, Xiamen, P.R. China1
Soochow University, Suzhou, P.R. China?
Tsinghua University, Beijing, P.R. China3
</affiliation>
<email confidence="0.959300666666667">
{jssu, biaozhang, yao0010}@xmu.edu.cn
{dyxiong, minzhang}@suda.edu.cn
liuyang2011@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999050964285714">
Learning semantic representations and
tree structures of bilingual phrases is ben-
eficial for statistical machine translation.
In this paper, we propose a new neu-
ral network model called Bilingual Corre-
spondence Recursive Autoencoder (BCor-
rRAE) to model bilingual phrases in trans-
lation. We incorporate word alignments
into BCorrRAE to allow it freely ac-
cess bilingual constraints at different lev-
els. BCorrRAE minimizes a joint objec-
tive on the combination of a recursive au-
toencoder reconstruction error, a structural
alignment consistency error and a cross-
lingual reconstruction error so as to not
only generate alignment-consistent phrase
structures, but also capture different lev-
els of semantic relations within bilingual
phrases. In order to examine the effective-
ness of BCorrRAE, we incorporate both
semantic and structural similarity features
built on bilingual phrase representations
and tree structures learned by BCorrRAE
into a state-of-the-art SMT system. Exper-
iments on NIST Chinese-English test sets
show that our model achieves a substantial
improvement of up to 1.55 BLEU points
over the baseline.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99168248">
Recently a variety of “deep architecture” ap-
proaches, including autoencoders, have been suc-
cessfully used in statistical machine translation
(SMT) (Yang et al., 2013; Liu et al., 2013; Zou
et al., 2013; Devlin et al., 2014; Tamura et al.,
2014; Sundermeyer et al., 2014; Wang et al., 2014;
Koˇcisk´y et al., 2014). Typically, these approaches
represent words as dense, low-dimensional and
∗Corresponding author.
real-valued vectors, i.e., word embeddings. How-
ever, translation units in machine translation have
long since shifted from words to phrases (se-
quence of words), of which syntactic and se-
mantic information cannot be adequately captured
and represented by word embeddings. There-
fore, learning compact vector representations for
phrases or even longer expressions is more crucial
for successful “deep” SMT.
To address this issue, many efforts have been
initiated on learning representations for bilingual
phrases in the context of SMT, inspired by the suc-
cess of work on monolingual phrase embeddings
(Socher et al., 2010; Socher et al., 2011a; Socher
et al., 2013b; Chen and Manning, 2014; Kalch-
brenner et al., 2014; Kim, 2014). The learning
process of bilingual phrase embeddings in these
efforts is normally interacted and mingled with
single or multiple essential components of SMT,
e.g., with reordering models (Li et al., 2013),
translation models (Cui et al., 2014; Zhang et al.,
2014; Gao et al., 2014), or both language and
translation models (Liu et al., 2014). In spite of
their success, these approaches center around cap-
turing relations between entire source and target
phrases. They do not take into account internal
phrase structures and bilingual correspondences of
sub-phrases within source and target phrases. The
neglect of these important clues may be due to the
big challenge imposed by the integration of them
into the learning process of bilingual phrase rep-
resentations. However, we believe such internal
structures and correspondences can help us learn
better phrase representations since they provide
multi-level syntactic and semantic constraints.
In this paper, we propose a Bilingual Corre-
spondence Recursive Autoencoder (BCorrRAE)
to learn bilingual phrase embeddings. BCorrRAE
substantially extends the Bilingually-constrained
Recursive Auto-encoder (BRAE) (Zhang et al.,
2014) to exploit both inner structures and corre-
</bodyText>
<page confidence="0.923934">
1248
</page>
<note confidence="0.9913405">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1248–1258,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.9693">
(b) BCorrRAE
</figure>
<figureCaption confidence="0.719587666666667">
Figure 1: BRAE vs BCorrRAE models for generating of a
bilingual phrase (“䘸坕 嚤 埏䍌 䛻懆”, “resolution adopted
today”) with word alignments (“0-2 2-1 3-0”). The subscript
</figureCaption>
<bodyText confidence="0.99899068">
number of each word indicates its position within phrase.
Solid lines depict the generation procedure of phrase struc-
tures, while dash lines illustrate the reconstruction procedure
from one language to the other. In this paper, the dimension-
ality of vector d in all figures is set to 3 for better illustration.
spondences within bilingual phrases. The intu-
itions behind BCorrRAE are twofold: 1) bilingual
phrase structure generation should satisfy word
alignment constraints as much as possible; and
2) corresponding sub-phrases on the source and
target side of bilingual phrases should be able
to reconstruct each other as they are semantic
equivalents. In order to model the first intuition,
BCorrRAE punishes bilingual structures that vio-
late word alignment constraints and rewards those
in consistent with word alignments. This en-
ables BCorrRAE to produce desirable bilingual
phrase structures from the perspective of word
alignments. With regard to the second intuition,
BCorrRAE reconstructs structures of sub-phrases
of one language according to aligned nodes in the
other language and minimizes the gap between
original and reconstructed structures. In doing so,
BCorrRAE is capable of capturing semantic rela-
tions at different levels.
To better illustrate our model, let us consider
the example in Figure 1. Similar to the conven-
tional recursive antoencoder (RAE), BRAE ne-
glects bilingual correspondences of sub-phrases.
Thus, it may combine “adopted” and “today” to-
gether to generate an undesirable target tree struc-
ture which violates word alignments. In contrast,
BCorrRAE aligns source-side nodes (e.g. (“埏
䍌”, “䛻懆”)) to their corresponding target-side
nodes (accordingly (“resolution”, “adopted”)) ac-
cording to word alignments. Furthermore, in
BCorrRAE, each subtree on the target side can be
reconstructed from the corresponding source node
that aligns to the target-side node dominating the
subtree and vice versa. These advantages allow us
to obtain improved bilingual phrase embeddings
with better inner correspondences of sub-phrases
and word alignment consistency.
We conduct experiments with a state-of-the-art
SMT system on large-scale data to evaluate the ef-
fectiveness of BCorrRAE model. Results on the
NIST 2006 and 2008 datasets show that our sys-
tem achieves significant improvements over base-
line methods. The main contributions of our work
lie in the following three aspects:
</bodyText>
<listItem confidence="0.9841404">
• We learn both embeddings and tree struc-
tures for bilingual phrases using cross-lingual
RAE reconstruction that minimizes semantic
distances between original and reconstructed
subtrees. To the best of our knowledge, this
has not been investigated before.
• We incorporate word alignment information
to guide phrase structure generation and es-
tablish internal semantic associations of sub-
phrases within bilingual phrases.
• We integrate two similarity features based on
BCorrRAE to enhance translation candidate
selection, and achieve an improvement of
1.55 BLEU points on Chinese-English trans-
lation.
</listItem>
<sectionHeader confidence="0.86035" genericHeader="introduction">
2 RAE and BRAE
</sectionHeader>
<bodyText confidence="0.99912525">
In this section, we briefly introduce the RAE and
its bilingual variation BRAE. This will provide
background knowledge on our proposed BCor-
rRAE.
</bodyText>
<subsectionHeader confidence="0.924303">
2.1 RAE
</subsectionHeader>
<bodyText confidence="0.998660428571429">
The component in the dash box of Figure 2 illus-
trates an instance of an RAE applied to a three-
word phrase. The input to the RAE is x =
(x1, x2, x3), which are the d-dimensional vector
representations of the ordered words in a phrase.
For two children c1 = x1 and c2 = x2, the parent
vector y1 can be computed in the following way:
</bodyText>
<equation confidence="0.998349">
p = f(W(1)[c1;c2] + b(1)) (1)
</equation>
<bodyText confidence="0.898281">
where [c1; c2] E R2d×1 is the concatenation of
c1 and c2, W(1) E Rd×2d is a parameter matrix,
b(1) E Rd×1 is a bias term, and f is an element-
</bodyText>
<figure confidence="0.973357615384615">
今天 所t 通过2 决议3
resolutiona adopted, today2
resolutiona adopted, today2
今天 所t 通过2 决议3
reconstructing source sub-trees according to corresponding target nodes
n¯f�
n¯e
n¯f2
(a) BRAE
n¯f�
n¯e
n¯f2
reconstructing target sub-trees according to corresponding source nodes
</figure>
<page confidence="0.830562">
1249
</page>
<figureCaption confidence="0.998634">
Figure 2: An illustration of the BRAE architecture.
</figureCaption>
<bodyText confidence="0.999461333333333">
wise activation function such as tanh(·), which is
used for all activation functions in BRAE and our
model. The learned parent vector p is also a d-
dimensional vector. In order to measure how well
p represents its children, we reconstruct the origi-
nal children nodes in a reconstruction layer:
</bodyText>
<equation confidence="0.95288">
[c01; c02] = f(W (2)p + b(2)) (2)
</equation>
<bodyText confidence="0.999926181818182">
where c01 and c02 are reconstructed children vectors,
W(2) E R2d×d and b(2) E R2d×1.
We can set y1 = p and then further use Eq. (1)
again to compute y2 by setting [c1; c2] = [y1; x3].
This combination and reconstruction process of
auto-encoder repeats at each node until the vec-
tor of the entire phrase is generated. To obtain the
optimal binary tree and phrase representation for
x, we employ a greedy algorithm (Socher et al.,
2011c) to minimize the sum of reconstruction er-
ror at each node in the binary tree T (x):
</bodyText>
<subsectionHeader confidence="0.992968">
2.2 BRAE
</subsectionHeader>
<bodyText confidence="0.999931714285714">
BRAE jointly learns two RAEs for source and tar-
get phrase embeddings as shown in Figure 1(a).
The core idea behind BRAE is that a source
phrase and its target correct translation should
share the same semantic representations, while
non-equivalent pairs should have different seman-
tic representations. Zhang et al. (2014) use this
intuition to constrain semantic pharse embedding
learning.
As shown in Figure 2, in addition to the above-
mentioned reconstruction error, BRAE introduces
a max-semantic-margin error to minimize the se-
mantic distance between translation equivalents
and maximize the semantic distance between non-
</bodyText>
<equation confidence="0.9860215">
(5)
− Esem(f|e0, 0) + 11
</equation>
<bodyText confidence="0.995659777777778">
where Esem(f|e, 0) is defined as the semantic dis-
tance between the learned vector representations
of f and e, denoted by pf and pe, respectively.
Since phrase embeddings for the source and target
language are learned separately in different vec-
tor spaces, a transformation matrix W(f3) E Rd×d
is introduced to capture this semantic transfor-
mation in the source-to-target direction. Thus,
Esem(f|e, 0) is calculated as
</bodyText>
<equation confidence="0.948602">
Esem(f|e, 0) = 2 11 pe − f(W (3)
1 f pf + b(3)
f ) 112 (6)
</equation>
<bodyText confidence="0.901477">
where b(3)
f E Rd×1 is a bias term. E∗sem(e|f, 0) and
Esem(e|f, 0) can be computed in a similar way.
The joint error of (f, e) is therefore defined as fol-
lows:
</bodyText>
<equation confidence="0.9489565">
E(f, e; 0) = α(Erec(f, 0) + Erec(e, 0)) 7
+(1 − α)(E∗sem(f  |e, 0) + E∗sem(e |f, 0)) ( )
</equation>
<bodyText confidence="0.9805505">
The final BRAE objective function over the train-
ing instance set D becomes:
</bodyText>
<equation confidence="0.92214">
λ
E(f, e; 0) + 2110112 (8)
</equation>
<bodyText confidence="0.992751">
Model parameters can be optimized over the total
errors on training bilingual phrases in a co-training
style algorithm (Zhang et al., 2014).
</bodyText>
<sectionHeader confidence="0.997041" genericHeader="method">
3 The BCorrRAE Model
</sectionHeader>
<bodyText confidence="0.999952727272727">
As depicted above, the learned embeddings us-
ing BRAE may be unreasonable due to the ne-
glect of bilingual constraints at different levels.
To address this drawback, we propose the BCor-
rRAE for bilingual phrase embeddings, which in-
corporates bilingual correspondence information
into the learning process of structures and embed-
dings via word alignments. In our model, we ex-
plore word alignments in two ways: (1) ensuring
that a learned bilingual phrase structure is con-
sistent with word alignments as much as possi-
</bodyText>
<equation confidence="0.779147">
Max-Semantic-
Reconstruction Error Margin Error
x1 x2 x3
Reconstruction Error
W(2)
W(1)
y1
W(2)
W(1)
y2
W(3)
</equation>
<bodyText confidence="0.999829">
where 0 denotes model parameters and n repre-
sents a node in T(x).
</bodyText>
<equation confidence="0.9644562">
�
Erec(x; 0) =
n∈T(x)
2 11 [c1;c2]n−[c01;c02]n 112 (3)
1
</equation>
<bodyText confidence="0.867815">
equivalent pairs simultaneously. Formally, the
max-semantic-margin error of a bilingual phrase
(f, e) is defined as
</bodyText>
<equation confidence="0.954523">
Esem(f, e; 0) = E∗sem(f|e, 0)+E∗sem(e|f, 0) (4)
</equation>
<bodyText confidence="0.99997675">
where E∗sem(f|e, 0) is used to ensure that the se-
mantic error for an equivalent pair is much smaller
than that for a non-equivalent pair (the source
phrase f and a bad translation e0):
</bodyText>
<equation confidence="0.945519333333333">
E∗sem(f|e, 0) = max{0, Esem(f|e, 0)
�JBRAE =
(f,e)∈D
</equation>
<page confidence="0.681577">
1250
</page>
<bodyText confidence="0.999782428571429">
ble; (2) identifying corresponding sub-phrases in
the source language for reconstructing sub-phrases
in the target language, and vice versa. More
specifically, the former is to encourage alignment-
consistent generation of sub-structures, while the
latter is to minimize semantic distances between
bilingual sub-phrases.
In this section, we first formally introduce a
concept of structural alignment consistency en-
coded in bilingual phrase structure learning, which
is the basis of our model. Then, we describe
the objective function which is composed of three
types of errors. Finally, we provide details on the
training of our model.
</bodyText>
<subsectionHeader confidence="0.997473">
3.1 Structural Alignment Consistency
</subsectionHeader>
<bodyText confidence="0.9999735">
We adapt word alignment to structural alignment
and introduce some related concepts. Given a
bilingual phrase (f, e) with its binary tree struc-
tures (Tf, Te), if the source node nf¯ E Tf cov-
ers a source-side sub-phrase ¯f, and there exists
a target-side sub-phrase e¯ such that (¯f, ¯e) are
consistent with word alignments (Och and Ney,
2003), we say nf¯ satisfies the structural alignment
consistency, and it is referred to as a structural-
alignment-consistent (SAC) node. Further, if e¯ is
covered by a target node n¯e E Te, we say n¯e is
the aligned node of n ¯f. In this way, several dif-
ferent target nodes may be all aligned to the same
source node because of null alignments. For this,
we choose the target node with the smallest span
as the aligned one for the considered source node.
This is because a smaller span reflects a stronger
semantic relevance in most situations.
Likewise, we have similar definitions for tar-
get nodes. Note that alignment relations between
source- and target-side nodes may not be symmet-
ric. For example, in Figure 1(b), node n¯e is the
aligned node of node n ¯f1, while node n¯f2 rather
than n¯f1 is the aligned node of n¯e.
</bodyText>
<subsectionHeader confidence="0.999779">
3.2 The Objective Function
</subsectionHeader>
<bodyText confidence="0.999379666666667">
We elaborate the three types of errors defined for
a bilingual phrase (f, e) with its binary tree struc-
tures (Tf, Te) on both sides below.
</bodyText>
<subsectionHeader confidence="0.947818">
3.2.1 Reconstruction Error
</subsectionHeader>
<bodyText confidence="0.99907175">
Similar to RAE, the first error function is used to
estimate how well learned phrase embeddings rep-
resent corresponding phrases. The reconstruction
error Erec(f, e; θ) of (f, e) is defined as follows:
</bodyText>
<equation confidence="0.986365">
Erec(f, e; θ) = Erec(f; θ)
</equation>
<bodyText confidence="0.9995655">
where both Erec(f; θ) and Erec(e; θ) can be calcu-
lated according to Eq. (3).
</bodyText>
<subsectionHeader confidence="0.808651">
3.2.2 Consistency Error
</subsectionHeader>
<bodyText confidence="0.981962833333333">
This metric corresponds to the first way in which
we exploit word alignments mentioned before,
which enables our model to generate as many SAC
nodes as possible to respect word alignments.
Formally, the consistency error Econ(f, e; θ) of
(f, e) is defined in the following way:
</bodyText>
<equation confidence="0.999506">
Econ(f, e; θ) = Econ(Tf; θ) + Econ(Te; θ) (10)
</equation>
<bodyText confidence="0.999955769230769">
where Econ(Tf; θ) and Econ(Te; θ) denote the con-
sistency error score for Tf and Te, given word
alignments. Here we only describe the calculation
of the former while the latter can be calculated in
exactly the same way.
To calculate Econ(Tf; θ), we first judge whether
a source node nf¯ is an SAC node according to
word alignments. Let pn f� be the vector repre-
sentation of n ¯f. Following Socher et al. (2010),
who use a simple inner product to measure how
well the two words are combined into a phrase,
we use inner product to calculate the consis-
tency/inconsistency score for n ¯f:
</bodyText>
<equation confidence="0.869615">
s(n ¯f) = Wscorepnf (11)
</equation>
<bodyText confidence="0.989828333333333">
where Wscore E R1×d is the score parameter. We
calculate Wscore by distinguishing SAC from non-
SAC nodes defined as follows:
</bodyText>
<equation confidence="0.741874">
�
Wscore
cns if nf¯ is an SAC node
Wscore =
</equation>
<bodyText confidence="0.976420769230769">
Wscore
inc otherwise
where the subscript cns and inc represent consis-
tency and inconsistency, respectively. For exam-
ple, in Figure 3, as n¯fs is a non-SAC node, we
calculate the inconsistency score using W score
inc for
it.
We expect Tf to satisfy structural alignment
consistency as much as possible. Therefore we en-
courage the consistency score for Tf to be larger
than its inconsistency score using a max-margin
consistency error function:
</bodyText>
<equation confidence="0.9471945">
Econ(Tf; θ) =max{0,1 − s(Tf)cns (12)
+ s(Tf)insl
</equation>
<bodyText confidence="0.999961166666667">
where s(Tf)cns denotes the sum of consistency
scores over all SAC nodes and s(Tf)ins the sum
of inconsistency scores over all non-SAC nodes in
Tf. Minimizing this error function will maximize
the sum of consistency scores of SAC nodes and
minimize (up to a margin) the sum of inconsis-
</bodyText>
<equation confidence="0.965735">
+ Erec(e; θ) (9)
</equation>
<page confidence="0.972943">
1251
</page>
<figureCaption confidence="0.9871078">
Figure 3: The structure generation procedure of the source
sub-phrase “嚤 埏䍌 䛻懆” and the structure reconstruction
procedure of the target sub-phrase “resolution adopted”. Ac-
cording to word alignments (“2-1 3-0”), the node n f1 and nf2
are SAC ones while the node nf3 is a non-SAC node.
</figureCaption>
<bodyText confidence="0.842166">
tency scores of non-SAC nodes.
</bodyText>
<subsectionHeader confidence="0.653912">
3.2.3 Cross-Lingual Reconstruction Error
</subsectionHeader>
<bodyText confidence="0.999609071428572">
This metric corresponds to the second way in
which we exploit word alignments. The assump-
tion behind this is that a source/target node should
be able to reconstruct the entire subtree rooted at
its target/source aligned node as they are seman-
tically equivalent. Based on this, for the consid-
ered node, we calculate the cross-lingual recon-
struction error along the entire subtree rooted at
its aligned node in the other language and use the
error to measure how well the learned vector rep-
resents this node.
Similarly, the cross-lingual reconstruction error
Eclrec(f, e; θ) of (f, e) can be decomposed into
two parts as follows:
</bodyText>
<equation confidence="0.998100666666667">
Eclrec(f, e; θ) = Ef2e·rec(Tf, Te; θ)
(13)
+ Ee2f·rec(Tf, Te; θ)
</equation>
<bodyText confidence="0.989269925925926">
where Ef2e·rec(Tf, Te; θ) denotes the error score
using Tf to reconstruct Te. Note that in this pro-
cess, the structure and the original node vector rep-
resentations of Te have been already generated.
Ee2f·rec(Tf, Te; θ) denotes the reconstruction er-
ror score using Te to reconstruct Tf. Here we still
only describe the method of computing the former,
which also applies to the latter.
To calculate Ef2e·rec(Tf, Te; θ), we first collect
all source nodes (n ¯f) in Tf and their aligned nodes
(n¯e) in Te to form a set of aligned node pairs
S = l(n ¯f, n¯e)} according to word alignments.
We then calculate Ef2e·rec(Tf, Te; θ) as the sum
of error scores over all node pairs in S. Given a
source node nf¯ with its aligned node n¯e on the
target side, we use nf¯ to reconstruct the sub-tree
structure T¯e rooted at n¯e and compute the error
score based on the semantic distance between the
original and reconstructed vector representations
of nodes in T¯e. As source and target phrase em-
beddings are separately learned, we first introduce
a transformation matrix W (3)
f and a bias term b(3)
f
to transform source phrase embeddings into the
target-side semantic space, following Zhang et al.
(2014) and Hermann and Blunsom (2014):
</bodyText>
<equation confidence="0.968925">
p�ne = f(W(3)
f pn f� + b(3)
</equation>
<bodyText confidence="0.976665125">
f )
here p&apos;ne denotes the reconstructed vector represen-
tation of n¯e, which is transformed from the vec-
tor representation pn f� of n ¯f. Then, we repeat the
reconstruction procedure in a top-down manner
along the corresponding target tree structure un-
til leaf nodes are reached, following Socher et al.
(2011a). Specifically, given the vector representa-
tion p&apos;ne, we reconstruct vector representations of
its two children nodes:
[cue1; cue2] = f(Weu p�ne + bue) (15)
where cue1 and cue2 are the reconstructed vector rep-
resentations of the children nodes, Weu E R2d×d,
and bueER2d×1. Eventually, given the original
and reconstructed target phrase representations,
we calculate Ef2e·rec(Tf, Te; θ) as follows:
</bodyText>
<equation confidence="0.76762175">
1 � �
Ef2e·rec(Tf, Te; θ) = 2
(n f�,ne)ES nETe
(16)
</equation>
<bodyText confidence="0.99975925">
where pn and p&apos;n are the original and reconstructed
vector representations of node n in the sub-tree
structure T¯e rooted at n¯e. This error function
will be minimized so that semantic differences
between original and reconstructed structures are
minimal.
Figure 3 demonstrates the structure reconstruc-
tion from a generated source sub-tree to its target
counterpart. In this way, BCorrRAE propagates
semantic information along dash lines sequentially
until leaf nodes in the generated structure of the
target phrase.
</bodyText>
<subsectionHeader confidence="0.772924">
3.2.4 The Final Objective
</subsectionHeader>
<bodyText confidence="0.999901333333333">
Similar to Eq. (8), we define the final objective
function of our model based on the three types of
errors described above
</bodyText>
<equation confidence="0.9709682">
�JBCorrRAE = lα (Erec(f; θ) + Erec(e; θ))
(f,e)ED
+ β (Econ(Tf; θ) + Econ(Te; θ))
+ γ (Ef2e·rec(Tf, Te; θ) + Ee2f·rec(Tf, Te; θ))}
+ R(θ) (17)
n¯f3
所1 通t2 ik议3
resolution0 adopted,
n ¯f�
W(3)
</equation>
<figure confidence="0.788777333333333">
f
n¯e
n¯f2
Weu
(14)
11 pn−p�n 112
</figure>
<page confidence="0.959749">
1252
</page>
<bodyText confidence="0.99932475">
where weights α, β, γ (s.t. α+β+γ = 1) are used
to balance the preference among the three errors,
and R(θ) is the regularization term. Parameters θ
are divided into four sets1:
</bodyText>
<listItem confidence="0.998614">
1. θL: the word embedding matrix;
2. θrec: the RAE parameter matrices W(1),
W(2) and bias terms b(1), b(2) (Section 2.1);
3. θcon: the consistency/inconsistency score pa-
rameter matrices Wscore cns , W score
inc (Section
3.2.2);
4. θclrec: the cross-lingual RAE semantic trans-
</listItem>
<bodyText confidence="0.567364">
formation parameter matrices W(3), Wu and
bias terms b(3), bu (Section 3.2.3).
For regularization, we assign each parameter set a
unique weight:
</bodyText>
<equation confidence="0.985529">
R(θ) = 2 IIθrecII2
λL 2 IIθLII2 + λrec (18)
λcon λ
+ 2 II θcon II2 + l 2 ec IIθlcrecII2
</equation>
<bodyText confidence="0.9856108">
Additionally, in order to prevent the hidden layer
from being very small, we normalize all output
vectors of the hidden layer to have length 1, p =
p following Socher et al. (2011c).
kpk ,
</bodyText>
<subsectionHeader confidence="0.999641">
3.3 Model Training
</subsectionHeader>
<bodyText confidence="0.999940095238095">
Similar to Zhang et al. (2014), we adopt a co-
training style algorithm to train model parameters
in the following two steps:
First, we use a normal distribution (µ = 0, σ =
0.01) to randomly initialize all model parameters,
and adopt the standard RAE to pre-train source-
and target-side phrase embeddings and tree struc-
tures (Section 2.1).
Second, for each bilingual phrase, we update
its source-side parameters to obtain the fine-tuned
vector representation and binary tree of the source
phrase, given the target-side phrase structure and
node representations, and vice versa. In this pro-
cess, we apply L-BFGS to tune parameters based
on gradients over the joint error, as implemented
in (Socher et al., 2011c).
We repeat the procedure of the second step until
either the joint error (shown in Eq. (17)) reaches a
local minima or the number of iterations is larger
than a pre-defined number (25 is used in experi-
ments).
</bodyText>
<footnote confidence="0.7464995">
1Note that the source and target languages have different
four sets of parameters.
</footnote>
<sectionHeader confidence="0.895336" genericHeader="method">
4 Decoding with BCorrRAE
</sectionHeader>
<bodyText confidence="0.999947071428572">
Once the model training is completed, we in-
corporate two different phrasal similarity features
built on the trained BCorrRAE into the standard
log-linear framework of SMT. Given a bilingual
phrase (f, e), we first obtain their semantic phrase
representations (pf, pe). Then we transform pf
into p0e in the target semantic space and pe into
p0f in the source semantic space via transforma-
tion matrixes. Finally, we reconstruct sub-trees of
p0f along the source structure Tf learned by BCor-
rRAE, sub-trees of p0e along the target structure Te.
We exploit two kinds of phrasal similarity fea-
tures based on the learned phrase representations
and their tree structures as follows:
</bodyText>
<listItem confidence="0.998284">
• Semantic Similarity measures the similarity
between original and transformed phrase rep-
resentations of (f, e):
</listItem>
<equation confidence="0.9196985">
SimSM(pe, p0e) = 2IIpe − p0
1 eII2
</equation>
<listItem confidence="0.89216725">
• Structural Similarity calculates the similarity
between original and reconstructed tree struc-
tures learned by BCorrRAE for (f, e):
1
</listItem>
<equation confidence="0.9415595">
SimST(pf,p0f) = 2Cf � IIp
n∈Tfn − p0 II2
n
1 �
SimST(pe,p0 e) = 2Ce
n∈Te
</equation>
<bodyText confidence="0.999929">
where pn and p0n represent vector representations
of original and reconstructed node n, and Cf and
Ce count the number of nodes in the source and
target tree structure respectively. Note that if we
only compute the similarity for root nodes in the
bilingual tree of (f, e), the structural similarity
equals to the semantic similarity in Eq. (19).
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999872666666667">
We conducted experiments on NIST Chinese-
English translation task to validate the effective-
ness of BCorrRAE.
</bodyText>
<subsectionHeader confidence="0.991868">
5.1 System Overview
</subsectionHeader>
<bodyText confidence="0.9998624">
Our baseline decoder is a state-of-the-art phrase-
based translation system equipped with a maxi-
mum entropy based reordering model (MEBTG).
It adopts three bracketing transduction grammar
rules (Wu, 1997; Xiong et al., 2006): merging
</bodyText>
<equation confidence="0.98483">
SimSM(pf,p0f) = 2IIpf − p0
1 fII2
IIpn − p0nII2
</equation>
<page confidence="0.847535">
1253
</page>
<bodyText confidence="0.996949741935484">
rules A -* [A1, A2]|(A1, A2) which are used to
merge two neighboring blocks2 A1 and A2 in a
straight|inverted order, and lexical rule A -* f/e
used to translate a source phrase f into a target
phrase e.
The MEBTG system features a maximal en-
tropy classifier based reordering model that pre-
dicts orientations of neighboring blocks. During
training, we extract bilingual phrases containing
up to 7 words on the source side from the training
corpus. With the collected reordering examples,
we adopt the maximal entropy toolkit3 developed
by Zhang to train the reordering model with the
following parameters: iteration number iter=200
and gaussian prior g=1.0. Following Xiong et al.
(2006), we use only boundary words of blocks to
trigger the reordering model.
The whole translation model is organized in a
log-linear framework (Och and Ney, 2002). The
adopted sub-models mainly include: (1) rule trans-
lation probabilities in two directions, (2) lexical
weights in two directions, (3) targets-side word
number, (4) phrase number, (5) language model
score, and (6) the score of maximal entropy based
reordering model. We perform minimum error
rate training (Och, 2003) to tune various fea-
ture weights. During decoding, we set ttable-
limit=20 for translation candidates kept for each
source phrase, stack-size=100 for hypotheses in
each span, and swap-span=15 for the length of
the maximal reordering span.
</bodyText>
<subsectionHeader confidence="0.992688">
5.2 Setup
</subsectionHeader>
<bodyText confidence="0.999905133333333">
Our bilingual data is the combination of the FBIS
corpus and Hansards part of LDC2004T07 corpus,
which contains 1.0M parallel sentences (25.2M
Chinese words and 29M English words). Follow-
ing Zhang et al. (2014), we collected 1.44M bilin-
gual phrases using forced decoding (Wuebker et
al., 2010) to train BCorrRAE from the training
data. We used a 5-gram language model trained
on the Xinhua portion of Gigaword corpus using
SRILM Toolkits4. Translation quality is evaluated
by case-insensitive BLEU-4 metric (Papineni et
al., 2002). We performed paired bootstrap sam-
pling (Koehn, 2004) to test the significance in
BLEU score differences. In our experiments, we
used NIST MT05 and MT06/MT08 data set as the
</bodyText>
<footnote confidence="0.99676475">
2A block is a bilingual phrase without maximum length
limitation.
3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4http://www.speech.sri.com/projects/srilm/download.html
</footnote>
<table confidence="0.999839875">
Parameter BRAE BCorrRAE
α 0.119 0.121
β - 0.6331
γ - 0.2459
AL 4.95 x10−5 3.13 x10−5
Arec 2.64 x10−7 2.05 x10−5
Acon - 7.32 x10−6
Alcrec 9.31 x10−5 5.25 x10−6
</table>
<tableCaption confidence="0.931008">
Table 1: Hyper-parameters for BCorrRAE and BRAE model.
</tableCaption>
<table confidence="0.999963111111111">
Method d MT06 MT08 AVG
BCorrRAESM 25 30.81 22.684 26.75
50 30.581 22.724 26.65
75 30.50 22.534 26.52
100 30.344 22.614 26.48
BCorrRAEST 25 30.56 23.28 26.92
50 30.94 23.33 27.14
75 30.73 23.40 27.07
100 30.90 23.50 27.20
</table>
<tableCaption confidence="0.958938">
Table 2: Experiment results for different dimensions (d).
BCorrRAESm and BCorrRAEST are our systems that are
enhanced with the semantic and structural similarity features
learned by BCorrRAE, respectively. 1/4: significantly worse
than the BCorrRAEST with the same dimensionality (p &lt;
0.05/p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.974371263157895">
development and test set, respectively.
In addition to the baseline described below,
we also compare our method against the BRAE
model, which focuses on modeling relations of
source and target phrases as a whole unit. Word
embeddings in BRAE are pre-trained with toolkit
Word2Vec5 (Mikolov et al., 2013) on large-scale
monolingual data that contains 0.83B words for
Chinese and 0.11B words for English.
Hyper-parameters in all neural models are op-
timized by random search (Bergstra and Bengio,
2012) based on related joint errors. We ran-
domly extracted 250, 000 bilingual phrases from
the above-mentioned training data as training set,
5, 000 as development set and another 5, 000 as
test set. We drew α, β, γ uniformly from 0.10 to
0.50, and λL, λTC,, λ,,n and λl,,, exponentially
from 10−8 to 10−2. Final parameters are shown in
Table 1 for both BRAE and BCorrRAE.
</bodyText>
<subsectionHeader confidence="0.999599">
5.3 Dimensionality of Embeddings
</subsectionHeader>
<bodyText confidence="0.999914142857143">
To investigate the impact of embedding dimen-
sionality on our BCorrRAE, we tried four differ-
ent dimensions from 25 to 100 with an increment
of 25 each time. The results are displayed in Ta-
ble 2. We can observe that the performance of our
model is not consistently improved with the incre-
ment of dimensionality. This may be because a
</bodyText>
<footnote confidence="0.982595">
5https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.994004">
1254
</page>
<bodyText confidence="0.999973333333333">
larger dimension brings in much more parameters,
and therefore makes parameter tuning more diffi-
cult. In practice, setting the dimension d to 50, we
can get satisfactory results without much compu-
tation effort, which has also been found by Zhang
et al. (2014).
</bodyText>
<subsectionHeader confidence="0.994102">
5.4 Structural Similarity vs. Semantic
Similarity
</subsectionHeader>
<bodyText confidence="0.999914307692308">
Table 2 also shows that the performance of
BCorrRAEST, the system with the structural sim-
ilarity feature in Eq. (20), is always superior to
that of BCorrRAESM with the semantic similar-
ity feature in Eq. (19). BCorrRAEST is bet-
ter than BCorrRAESM by 0.483 BLEU points
on average. In most cases, differences between
BCorrRAEST and BCorrRAESM with the same
dimensionality are statistically significant. This
suggests that digging into structures of bilingual
phrases (BCorrRAEST) can obtain further im-
provements over only modeling bilingual phrases
as whole units (BCorrRAESM).
</bodyText>
<subsectionHeader confidence="0.996599">
5.5 Overall Performance
</subsectionHeader>
<bodyText confidence="0.99997025">
Table 3 summarizes the comparison results of dif-
ferent models on the test sets. The BCorrRAESM
outperforms the baseline and BRAE by 1.06 and
0.25 BLEU points on average respectively, while
BCorrRAEST gains 1.55 and 0.74 BLEU points
on average over the baseline and BRAE. The im-
provements of BCorrRAEST over the baseline,
BRAE and BCorrRAESM are statistically signif-
icant at different levels. This demonstrates the
advantage of our BCorrRAE over BRAE in that
BCorrRAE is able to explore sub-structures of
bilingual phrases.
</bodyText>
<subsectionHeader confidence="0.997136">
5.6 Analysis
</subsectionHeader>
<bodyText confidence="0.99342">
We compute a ratio of aligned nodes (Section 3.1)
over all nodes to estimate how well tree struc-
tures of bilingual phrases generated by BRAE and
BCorrRAE are consistent with word alignments.
We consider two factors when computing the ra-
tio: the length of the source side of a bilingual
phrase ls and the length of a span covered by an
aligned node la. The result is illustrated in Table
4.6 We find that BCorrRAE significantly outper-
</bodyText>
<footnote confidence="0.9019578">
6We only give ratios for bilingual phrases with source-
side length from 3 to 4 words because 1) ratios of BRAE and
BCorrRAE in the case of la &lt; 3 are very close and 2) phrases
with length &gt; 4 are rarely used during decoding (accounting
for &lt; 0.5%).
</footnote>
<table confidence="0.9983146">
Method MT06 MT08 AVG
Baseline 29.664 21.524 25.59
BRAE 30.274 22.534 26.40
BCorrRAESM 30.581 22.724 26.65
BCorrRAEST 30.94 23.33 27.14
</table>
<tableCaption confidence="0.98725425">
Table 3: Experiment results on the test sets. AVG = average
BLEU scores for test sets. For both BRAE and BCorrRAE,
we set d=50. ↓/⇓: significantly worse than the BCorrRAEST
with d=50 (p &lt; 0.05/p &lt; 0.01, respectively).
</tableCaption>
<table confidence="0.999523">
[ls, la] [3,2] [4,2] [4,3]
BRAE 52.70% 39.88% 46.58%
BCorrRAE 60.08% 46.32% 54.43%
</table>
<tableCaption confidence="0.9909835">
Table 4: Aligned node ratio for source phrases of different
lengths.
</tableCaption>
<bodyText confidence="0.999790875">
forms BRAE model by 7.22% on average in terms
of the aligned node ratio. This strongly demon-
strates that the proposed BCorrRAE is able to gen-
erate tree structures that are more consistent with
word alignments than those generated by BRAE.
We further show example source phrases in Ta-
ble 5 with their most semantically similar trans-
lations learned by BRAE and BCorrRAE in the
training corpus. Both models can select correct
translations for content words. However, they are
different in dealing with function words. Com-
pared to our model, the BRAE model prefers
longer target phrases surrounded with function
words. Take the source phrase “惮䜃 坝揔” as an
example, the BRAE model learns both “a serious
challenge to” and “a serious challenge from” as
its semantically similar target phrases. Although
the content words “惮䜃” and “坝揔” are trans-
lated correctly into “serious” and “challenge”, the
function words “to” and “from” express exactly
the opposite meanings. In contrast, our model, es-
pecially the BCorrRAEST model, tends to choose
shorter translations that are consistent with word
alignments.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.990056727272727">
A variety of efforts have been devoted to learn-
ing vector representations for words/phrases with
deep neural networks. According to the differ-
ence of learning contexts, previous work mainly
include the following two strands.
(1) Monolingual Word/Phrase Embeddings.
The straightforward approach to represent
word/phrases is to learn their hidden represen-
tations with traditional feature vectors, which
requires manual and task-dependent feature
engineering (Cui et al., 2014; Wu et al., 2014;
</bodyText>
<page confidence="0.682559">
1255
</page>
<bodyText confidence="0.9955335">
Source Phrase BRAE BCorrRAESM BCorrRAEST
䌓㥎 to advocate the out to advocate encouraging
(advocate) in preaching the been encouraging claimed
the promotion of an advocate advocate
惮䜃坝揔 as well as severe challenges of rigorous challenges rigorous challenge
(serious challenge) a serious challenge to as well as severe challenges enormous challenge
a serious challenge from of severe challenges severe challenge
䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data
(data released) the statistics released by at the figures published released figures
data published by the the statistics released by the estimates announced
</bodyText>
<tableCaption confidence="0.98737">
Table 5: Semantically similar target phrases in the training set for example source phrases.
</tableCaption>
<bodyText confidence="0.999157265306122">
Chen and Manning, 2014). To avoid exploiting
manually input features, Bengio et al. (2003)
convert words to dense, real-valued vectors by
learning probability distributions of n-grams.
Mikolov et al. (2013) generate word vectors by
predicting their limited context words. Instead of
exploiting outside context information, recursive
auto-encoder is usually adopted to learn the com-
position of internal words (Socher et al., 2010;
Socher et al., 2011b; Socher et al., 2013b; Socher
et al., 2013a). Recently, convolution architecture
has drawn more and more attention due to its
ability to explicitly capture short and long-range
relations (Collobert et al., 2011; Kalchbrenner
and Blunsom, 2013; Kalchbrenner et al., 2014;
Kim, 2014).
(2) Bilingual Word/Phrase Embeddings. In the
field of machine translation and cross-lingual in-
formation processing, bilingual embedding learn-
ing has become an increasingly important study.
The bilingual embedding research origins in the
word embedding learning, upon which Zou et
al. (2013) utilize word alignments to constrain
translational equivalence. Koˇcisk´y et al. (2014)
propose a probability model to capture more se-
mantic information by marginalizing over word
alignments. More specifically to SMT, its main
components have been exploited to learn better
bilingual phrase embeddings in different aspects:
language models (Wang et al., 2014; Garmash and
Monz, 2014), reordering models (Li et al., 2013)
and translation models (Tran et al., 2014; Zhang
et al., 2014). Instead of exploiting a single model,
Liu et al. (2014) combine the recursive and recur-
rent neural network to incorporate the language
and translation model.
Different from the methods mentioned above,
our model considers both the cross-language con-
sistency of phrase structures and internal corre-
spondence relations inside bilingual phrases. The
most related works include Zhang et al. (2014)
and Socher et al. (2011a). Compared with these
works, our model exploits different levels of cor-
respondence relations inside bilingual phrases in-
stead of only the top level of entire phrases, and
reconstructs tree structures of sub-phrases in one
language according to aligned nodes in the other
language, which, to the best of our knowledge, has
never been investigated before.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999744375">
In this paper, we have presented the BCorrRAE
to learn phrase embeddings and tree structures of
bilingual phrases for SMT. Punishing structural-
alignment-inconsistent sub-structures and mini-
mizing the gap between original and reconstructed
structures, our approach is able to not only gen-
erate alignment-consistent phrase structures, but
also capture different levels of semantic rela-
tions within bilingual phrases. Experiment results
demonstrate the effectiveness of our model.
In the future, we would like to derive
more features from BCorrRAE, e.g., consis-
tency/inconsistency scores of bilingual phrases, to
further enhance SMT. Additionally, we also want
to apply our model to other bilingual tasks, e.g.,
learning bilingual terminology or paraphrases.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999853666666667">
The authors were supported by National Nat-
ural Science Foundation of China (Grant Nos
61303082 and 61403269), Natural Science
Foundation of Jiangsu Province (Grant No.
BK20140355), National 863 program of China
(No. 2015AA011808), the Special and Major
Subject Project of the Industrial Science and
Technology in Fujian Province 2013 (Grant
No. 2013HZ0004-1), and 2014 Key Project of
Anhui Science and Technology Bureau (Grant
No. 1301021018). We also thank the anonymous
reviewers for their insightful comments.
</bodyText>
<page confidence="0.989462">
1256
</page>
<sectionHeader confidence="0.996278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912605769231">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1139–1155.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13:22–29.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP 2014, pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.,
16:2493–2537.
Lei Cui, Dongdong Zhang, Shujie Liu, Qiming Chen,
Mu Li, Ming Zhou, and Muyun Yang. 2014. Learn-
ing topic representation for smt with neural net-
works. In Proc. of ACL 2014, pages 133–143.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proc. of ACL 2014,
pages 1370–1380.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proc. of ACL
2014, pages 699–709.
Ekaterina Garmash and Christof Monz. 2014.
Dependency-based bilingual language models for
reordering in statistical machine translation. In
Proc. of EMNLP 2014, pages 1689–1700.
Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proc. of ACL 2014, pages 58–68.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. of EMNLP
2013, pages 1700–1709.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proc. of ACL 2014, pages
655–665.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proc. of EMNLP 2014,
pages 1746–1751.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395.
Tom´aˇs Koˇcisk´y, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In Proc. of ACL 2014,
pages 224–229.
Peng Li, Yang Liu, and Maosong Sun. 2013. Re-
cursive autoencoders for ITG-based translation. In
Proc. of EMNLP 2013, pages 567–577.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In Proc. of ACL
2013, pages 791–801.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proc. of ACL 2014, pages
1491–1500.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proc. of NIPS 2013.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL 2002,
pages 295–302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. of
ACL 2003, pages 160–167. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. ofACL 2002,
pages 311–318.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proc. of NIPS 2010.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proc. of NIPS
2011.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011b. Parsing natu-
ral scenes and natural language with recursive neural
networks. In Proc. of ICML 2011.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP
2011, pages 151–161.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with compo-
sitional vector grammars. In Proc. of ACL 2013,
pages 455–465.
</reference>
<page confidence="0.804338">
1257
</page>
<reference confidence="0.999381215686274">
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. of EMNLP 2013, pages 1631–1642.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In
Proc. of EMNLP 2014, pages 14–25.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2014. Recurrent neural networks for word align-
ment model. In Proc. of ACL 2014, pages 1470–
1480.
Ke M. Tran, Arianna Bisazza, and Christof Monz.
2014. Word translation prediction for morphologi-
cally rich languages with bilingual neural networks.
In Proc. of EMNLP 2014, pages 1676–1688.
Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama,
and Eiichiro Sumita. 2014. Neural network based
bilingual language model growing for statistical ma-
chine translation. In Proc. of EMNLP 2014, pages
189–195.
Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dian-
hai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting
Liu. 2014. Improve statistical machine transla-
tion with context-sensitive bilingual semantic em-
bedding model. In Proc. of EMNLP 2014, pages
142–146.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proc. of ACL 2010, pages 475–
484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL 2006,
pages 521–528.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with context
dependent deep neural network. In Proc. of ACL
2013, pages 166–175.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Proc.
of ACL 2014, pages 111–121.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Proc.
of EMNLP 2013, pages 1393–1398.
</reference>
<page confidence="0.992436">
1258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255438">
<title confidence="0.993969">Bilingual Correspondence Recursive for Statistical Machine Translation</title>
<author confidence="0.934141">Deyi Biao Yang Junfeng Min</author>
<affiliation confidence="0.5585855">University, Xiamen, P.R. University, Suzhou, P.R.</affiliation>
<address confidence="0.929628">University, Beijing, P.R.</address>
<email confidence="0.8123385">biaozhang,liuyang2011@tsinghua.edu.cn</email>
<abstract confidence="0.999714034482758">Learning semantic representations and tree structures of bilingual phrases is beneficial for statistical machine translation. In this paper, we propose a new neural network model called Bilingual Correspondence Recursive Autoencoder (BCorrRAE) to model bilingual phrases in translation. We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels. BCorrRAE minimizes a joint objective on the combination of a recursive autoencoder reconstruction error, a structural alignment consistency error and a crosslingual reconstruction error so as to not only generate alignment-consistent phrase structures, but also capture different levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial of up to points over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1139</pages>
<contexts>
<context position="33220" citStr="Bengio et al. (2003)" startWordPosition="5376" endWordPosition="5379"> of rigorous challenges rigorous challenge (serious challenge) a serious challenge to as well as severe challenges enormous challenge a serious challenge from of severe challenges severe challenge 䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data (data released) the statistics released by at the figures published released figures data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1139–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Yoshua Bengio</author>
</authors>
<title>Random search for hyper-parameter optimization.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--22</pages>
<contexts>
<context position="27373" citStr="Bergstra and Bengio, 2012" startWordPosition="4437" endWordPosition="4440">rned by BCorrRAE, respectively. 1/4: significantly worse than the BCorrRAEST with the same dimensionality (p &lt; 0.05/p &lt; 0.01). development and test set, respectively. In addition to the baseline described below, we also compare our method against the BRAE model, which focuses on modeling relations of source and target phrases as a whole unit. Word embeddings in BRAE are pre-trained with toolkit Word2Vec5 (Mikolov et al., 2013) on large-scale monolingual data that contains 0.83B words for Chinese and 0.11B words for English. Hyper-parameters in all neural models are optimized by random search (Bergstra and Bengio, 2012) based on related joint errors. We randomly extracted 250, 000 bilingual phrases from the above-mentioned training data as training set, 5, 000 as development set and another 5, 000 as test set. We drew α, β, γ uniformly from 0.10 to 0.50, and λL, λTC,, λ,,n and λl,,, exponentially from 10−8 to 10−2. Final parameters are shown in Table 1 for both BRAE and BCorrRAE. 5.3 Dimensionality of Embeddings To investigate the impact of embedding dimensionality on our BCorrRAE, we tried four different dimensions from 25 to 100 with an increment of 25 each time. The results are displayed in Table 2. We ca</context>
</contexts>
<marker>Bergstra, Bengio, 2012</marker>
<rawString>James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>740--750</pages>
<contexts>
<context position="2616" citStr="Chen and Manning, 2014" startWordPosition="374" endWordPosition="377"> units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phr</context>
<context position="33153" citStr="Chen and Manning, 2014" startWordPosition="5366" endWordPosition="5369">he promotion of an advocate advocate 惮䜃坝揔 as well as severe challenges of rigorous challenges rigorous challenge (serious challenge) a serious challenge to as well as severe challenges enormous challenge a serious challenge from of severe challenges severe challenge 䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data (data released) the statistics released by at the figures published released figures data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-r</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. of EMNLP 2014, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research.,</journal>
<pages>16--2493</pages>
<contexts>
<context position="33791" citStr="Collobert et al., 2011" startWordPosition="5460" endWordPosition="5463">iting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main component</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research., 16:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Shujie Liu</author>
<author>Qiming Chen</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Muyun Yang</author>
</authors>
<title>Learning topic representation for smt with neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>133--143</pages>
<contexts>
<context position="2898" citStr="Cui et al., 2014" startWordPosition="418" endWordPosition="421">ressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us</context>
<context position="32364" citStr="Cui et al., 2014" startWordPosition="5246" endWordPosition="5249">ast, our model, especially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase BRAE BCorrRAESM BCorrRAEST 䌓㥎 to advocate the out to advocate encouraging (advocate) in preaching the been encouraging claimed the promotion of an advocate advocate 惮䜃坝揔 as well as severe challenges of rigorous challenges rigorous challenge (serious challenge) a serious challenge to as well as severe challenges enormous challenge a serious challenge from of severe challenges severe challenge 䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data (data released) the statistics released by at the figures published released figures </context>
</contexts>
<marker>Cui, Zhang, Liu, Chen, Li, Zhou, Yang, 2014</marker>
<rawString>Lei Cui, Dongdong Zhang, Shujie Liu, Qiming Chen, Mu Li, Ming Zhou, and Muyun Yang. 2014. Learning topic representation for smt with neural networks. In Proc. of ACL 2014, pages 133–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="1740" citStr="Devlin et al., 2014" startWordPosition="241" endWordPosition="244">s. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To add</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proc. of ACL 2014, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>699--709</pages>
<contexts>
<context position="2937" citStr="Gao et al., 2014" startWordPosition="426" endWordPosition="429">l “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations si</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proc. of ACL 2014, pages 699–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Garmash</author>
<author>Christof Monz</author>
</authors>
<title>Dependency-based bilingual language models for reordering in statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1689--1700</pages>
<contexts>
<context position="34538" citStr="Garmash and Monz, 2014" startWordPosition="5566" endWordPosition="5569">of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of corresponde</context>
</contexts>
<marker>Garmash, Monz, 2014</marker>
<rawString>Ekaterina Garmash and Christof Monz. 2014. Dependency-based bilingual language models for reordering in statistical machine translation. In Proc. of EMNLP 2014, pages 1689–1700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>58--68</pages>
<contexts>
<context position="18426" citStr="Hermann and Blunsom (2014)" startWordPosition="2985" endWordPosition="2988">ulate Ef2e·rec(Tf, Te; θ) as the sum of error scores over all node pairs in S. Given a source node nf¯ with its aligned node n¯e on the target side, we use nf¯ to reconstruct the sub-tree structure T¯e rooted at n¯e and compute the error score based on the semantic distance between the original and reconstructed vector representations of nodes in T¯e. As source and target phrase embeddings are separately learned, we first introduce a transformation matrix W (3) f and a bias term b(3) f to transform source phrase embeddings into the target-side semantic space, following Zhang et al. (2014) and Hermann and Blunsom (2014): p�ne = f(W(3) f pn f� + b(3) f ) here p&apos;ne denotes the reconstructed vector representation of n¯e, which is transformed from the vector representation pn f� of n ¯f. Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached, following Socher et al. (2011a). Specifically, given the vector representation p&apos;ne, we reconstruct vector representations of its two children nodes: [cue1; cue2] = f(Weu p�ne + bue) (15) where cue1 and cue2 are the reconstructed vector representations of the children nodes, Weu E R2d×d, a</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual models for compositional distributed semantics. In Proc. of ACL 2014, pages 58–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="33823" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="5464" endWordPosition="5467">tures, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn b</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. of EMNLP 2013, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>655--665</pages>
<contexts>
<context position="2643" citStr="Kalchbrenner et al., 2014" startWordPosition="378" endWordPosition="382">ation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and targ</context>
<context position="33850" citStr="Kalchbrenner et al., 2014" startWordPosition="5468" endWordPosition="5471">ert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embe</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proc. of ACL 2014, pages 655–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1746--1751</pages>
<contexts>
<context position="2655" citStr="Kim, 2014" startWordPosition="383" endWordPosition="384">ed from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. </context>
<context position="33862" citStr="Kim, 2014" startWordPosition="5472" endWordPosition="5473">lued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in di</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proc. of EMNLP 2014, pages 1746–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="25835" citStr="Koehn, 2004" startWordPosition="4206" endWordPosition="4207"> reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4. Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2A block is a bilingual phrase without maximum length limitation. 3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4http://www.speech.sri.com/projects/srilm/download.html Parameter BRAE BCorrRAE α 0.119 0.121 β - 0.6331 γ - 0.2459 AL 4.95 x10−5 3.13 x10−5 Arec 2.64 x10−7 2.05 x10−5 Acon - 7.32 x10−6 Alcrec 9.31 x10−5 5.25 x10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method d MT06 MT08 AVG BCorrRAESM 25 30.81 22.684 26.75 50 30.581 22.724 26.65 75 3</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Koˇcisk´y</author>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Learning bilingual word representations by marginalizing alignments.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>224--229</pages>
<marker>Koˇcisk´y, Hermann, Blunsom, 2014</marker>
<rawString>Tom´aˇs Koˇcisk´y, Karl Moritz Hermann, and Phil Blunsom. 2014. Learning bilingual word representations by marginalizing alignments. In Proc. of ACL 2014, pages 224–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for ITG-based translation.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>567--577</pages>
<contexts>
<context position="2860" citStr="Li et al., 2013" startWordPosition="412" endWordPosition="415">tations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal stru</context>
<context position="34575" citStr="Li et al., 2013" startWordPosition="5572" endWordPosition="5575">ormation processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrase</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proc. of EMNLP 2013, pages 567–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Tiejun Zhao</author>
</authors>
<title>Additive neural networks for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>791--801</pages>
<contexts>
<context position="1701" citStr="Liu et al., 2013" startWordPosition="233" endWordPosition="236">ic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more cr</context>
</contexts>
<marker>Liu, Watanabe, Sumita, Zhao, 2013</marker>
<rawString>Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013. Additive neural networks for statistical machine translation. In Proc. of ACL 2013, pages 791–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1491--1500</pages>
<contexts>
<context position="2997" citStr="Liu et al., 2014" startWordPosition="436" endWordPosition="439">initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations since they provide multi-level syntactic and semantic constrai</context>
<context position="34695" citStr="Liu et al. (2014)" startWordPosition="5593" endWordPosition="5596">esearch origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases, and reconstructs tree structures of sub-phrases in one language accor</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proc. of ACL 2014, pages 1491–1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS</booktitle>
<contexts>
<context position="27177" citStr="Mikolov et al., 2013" startWordPosition="4407" endWordPosition="4410">0.90 23.50 27.20 Table 2: Experiment results for different dimensions (d). BCorrRAESm and BCorrRAEST are our systems that are enhanced with the semantic and structural similarity features learned by BCorrRAE, respectively. 1/4: significantly worse than the BCorrRAEST with the same dimensionality (p &lt; 0.05/p &lt; 0.01). development and test set, respectively. In addition to the baseline described below, we also compare our method against the BRAE model, which focuses on modeling relations of source and target phrases as a whole unit. Word embeddings in BRAE are pre-trained with toolkit Word2Vec5 (Mikolov et al., 2013) on large-scale monolingual data that contains 0.83B words for Chinese and 0.11B words for English. Hyper-parameters in all neural models are optimized by random search (Bergstra and Bengio, 2012) based on related joint errors. We randomly extracted 250, 000 bilingual phrases from the above-mentioned training data as training set, 5, 000 as development set and another 5, 000 as test set. We drew α, β, γ uniformly from 0.10 to 0.50, and λL, λTC,, λ,,n and λl,,, exponentially from 10−8 to 10−2. Final parameters are shown in Table 1 for both BRAE and BCorrRAE. 5.3 Dimensionality of Embeddings To </context>
<context position="33336" citStr="Mikolov et al. (2013)" startWordPosition="5392" endWordPosition="5395">enormous challenge a serious challenge from of severe challenges severe challenge 䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data (data released) the statistics released by at the figures published released figures data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translatio</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>295--302</pages>
<contexts>
<context position="24684" citStr="Och and Ney, 2002" startWordPosition="4027" endWordPosition="4030">es a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combina</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12912" citStr="Och and Ney, 2003" startWordPosition="2027" endWordPosition="2030">nt consistency encoded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model. 3.1 Structural Alignment Consistency We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree structures (Tf, Te), if the source node nf¯ E Tf covers a source-side sub-phrase ¯f, and there exists a target-side sub-phrase e¯ such that (¯f, ¯e) are consistent with word alignments (Och and Ney, 2003), we say nf¯ satisfies the structural alignment consistency, and it is referred to as a structuralalignment-consistent (SAC) node. Further, if e¯ is covered by a target node n¯e E Te, we say n¯e is the aligned node of n ¯f. In this way, several different target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations. Likewise, we have similar definitions for target nodes. Note </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL 2003,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25004" citStr="Och, 2003" startWordPosition="4077" endWordPosition="4078">he reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25781" citStr="Papineni et al., 2002" startWordPosition="4196" endWordPosition="4199">ses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4. Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2A block is a bilingual phrase without maximum length limitation. 3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4http://www.speech.sri.com/projects/srilm/download.html Parameter BRAE BCorrRAE α 0.119 0.121 β - 0.6331 γ - 0.2459 AL 4.95 x10−5 3.13 x10−5 Arec 2.64 x10−7 2.05 x10−5 Acon - 7.32 x10−6 Alcrec 9.31 x10−5 5.25 x10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method d MT06 MT08 AVG BCorrR</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proc. of NIPS</booktitle>
<contexts>
<context position="2548" citStr="Socher et al., 2010" startWordPosition="362" endWordPosition="365"> real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account</context>
<context position="14987" citStr="Socher et al. (2010)" startWordPosition="2395" endWordPosition="2398">odel to generate as many SAC nodes as possible to respect word alignments. Formally, the consistency error Econ(f, e; θ) of (f, e) is defined in the following way: Econ(f, e; θ) = Econ(Tf; θ) + Econ(Te; θ) (10) where Econ(Tf; θ) and Econ(Te; θ) denote the consistency error score for Tf and Te, given word alignments. Here we only describe the calculation of the former while the latter can be calculated in exactly the same way. To calculate Econ(Tf; θ), we first judge whether a source node nf¯ is an SAC node according to word alignments. Let pn f� be the vector representation of n ¯f. Following Socher et al. (2010), who use a simple inner product to measure how well the two words are combined into a phrase, we use inner product to calculate the consistency/inconsistency score for n ¯f: s(n ¯f) = Wscorepnf (11) where Wscore E R1×d is the score parameter. We calculate Wscore by distinguishing SAC from nonSAC nodes defined as follows: � Wscore cns if nf¯ is an SAC node Wscore = Wscore inc otherwise where the subscript cns and inc represent consistency and inconsistency, respectively. For example, in Figure 3, as n¯fs is a non-SAC node, we calculate the inconsistency score using W score inc for it. We expec</context>
<context position="33558" citStr="Socher et al., 2010" startWordPosition="5424" endWordPosition="5427">blished released figures data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) uti</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proc. of NIPS 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proc. of NIPS</booktitle>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="366" endWordPosition="369"> i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase stru</context>
<context position="9015" citStr="Socher et al., 2011" startWordPosition="1376" endWordPosition="1379">so a ddimensional vector. In order to measure how well p represents its children, we reconstruct the original children nodes in a reconstruction layer: [c01; c02] = f(W (2)p + b(2)) (2) where c01 and c02 are reconstructed children vectors, W(2) E R2d×d and b(2) E R2d×1. We can set y1 = p and then further use Eq. (1) again to compute y2 by setting [c1; c2] = [y1; x3]. This combination and reconstruction process of auto-encoder repeats at each node until the vector of the entire phrase is generated. To obtain the optimal binary tree and phrase representation for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin</context>
<context position="18765" citStr="Socher et al. (2011" startWordPosition="3045" endWordPosition="3048">in T¯e. As source and target phrase embeddings are separately learned, we first introduce a transformation matrix W (3) f and a bias term b(3) f to transform source phrase embeddings into the target-side semantic space, following Zhang et al. (2014) and Hermann and Blunsom (2014): p�ne = f(W(3) f pn f� + b(3) f ) here p&apos;ne denotes the reconstructed vector representation of n¯e, which is transformed from the vector representation pn f� of n ¯f. Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached, following Socher et al. (2011a). Specifically, given the vector representation p&apos;ne, we reconstruct vector representations of its two children nodes: [cue1; cue2] = f(Weu p�ne + bue) (15) where cue1 and cue2 are the reconstructed vector representations of the children nodes, Weu E R2d×d, and bueER2d×1. Eventually, given the original and reconstructed target phrase representations, we calculate Ef2e·rec(Tf, Te; θ) as follows: 1 � � Ef2e·rec(Tf, Te; θ) = 2 (n f�,ne)ES nETe (16) where pn and p&apos;n are the original and reconstructed vector representations of node n in the sub-tree structure T¯e rooted at n¯e. This error functio</context>
<context position="20978" citStr="Socher et al. (2011" startWordPosition="3418" endWordPosition="3421">s W(1), W(2) and bias terms b(1), b(2) (Section 2.1); 3. θcon: the consistency/inconsistency score parameter matrices Wscore cns , W score inc (Section 3.2.2); 4. θclrec: the cross-lingual RAE semantic transformation parameter matrices W(3), Wu and bias terms b(3), bu (Section 3.2.3). For regularization, we assign each parameter set a unique weight: R(θ) = 2 IIθrecII2 λL 2 IIθLII2 + λrec (18) λcon λ + 2 II θcon II2 + l 2 ec IIθlcrecII2 Additionally, in order to prevent the hidden layer from being very small, we normalize all output vectors of the hidden layer to have length 1, p = p following Socher et al. (2011c). kpk , 3.3 Model Training Similar to Zhang et al. (2014), we adopt a cotraining style algorithm to train model parameters in the following two steps: First, we use a normal distribution (µ = 0, σ = 0.01) to randomly initialize all model parameters, and adopt the standard RAE to pre-train sourceand target-side phrase embeddings and tree structures (Section 2.1). Second, for each bilingual phrase, we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase, given the target-side phrase structure and node representations, and vice ve</context>
<context position="33579" citStr="Socher et al., 2011" startWordPosition="5428" endWordPosition="5431">res data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments </context>
<context position="35057" citStr="Socher et al. (2011" startWordPosition="5647" endWordPosition="5650"> phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases, and reconstructs tree structures of sub-phrases in one language according to aligned nodes in the other language, which, to the best of our knowledge, has never been investigated before. 7 Conclusions and Future Work In this paper, we have presented the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT. Punishing structuralalignment-inconsistent sub-structures and minimizing the gap between or</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proc. of NIPS 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proc. of ICML</booktitle>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="366" endWordPosition="369"> i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase stru</context>
<context position="9015" citStr="Socher et al., 2011" startWordPosition="1376" endWordPosition="1379">so a ddimensional vector. In order to measure how well p represents its children, we reconstruct the original children nodes in a reconstruction layer: [c01; c02] = f(W (2)p + b(2)) (2) where c01 and c02 are reconstructed children vectors, W(2) E R2d×d and b(2) E R2d×1. We can set y1 = p and then further use Eq. (1) again to compute y2 by setting [c1; c2] = [y1; x3]. This combination and reconstruction process of auto-encoder repeats at each node until the vector of the entire phrase is generated. To obtain the optimal binary tree and phrase representation for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin</context>
<context position="18765" citStr="Socher et al. (2011" startWordPosition="3045" endWordPosition="3048">in T¯e. As source and target phrase embeddings are separately learned, we first introduce a transformation matrix W (3) f and a bias term b(3) f to transform source phrase embeddings into the target-side semantic space, following Zhang et al. (2014) and Hermann and Blunsom (2014): p�ne = f(W(3) f pn f� + b(3) f ) here p&apos;ne denotes the reconstructed vector representation of n¯e, which is transformed from the vector representation pn f� of n ¯f. Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached, following Socher et al. (2011a). Specifically, given the vector representation p&apos;ne, we reconstruct vector representations of its two children nodes: [cue1; cue2] = f(Weu p�ne + bue) (15) where cue1 and cue2 are the reconstructed vector representations of the children nodes, Weu E R2d×d, and bueER2d×1. Eventually, given the original and reconstructed target phrase representations, we calculate Ef2e·rec(Tf, Te; θ) as follows: 1 � � Ef2e·rec(Tf, Te; θ) = 2 (n f�,ne)ES nETe (16) where pn and p&apos;n are the original and reconstructed vector representations of node n in the sub-tree structure T¯e rooted at n¯e. This error functio</context>
<context position="20978" citStr="Socher et al. (2011" startWordPosition="3418" endWordPosition="3421">s W(1), W(2) and bias terms b(1), b(2) (Section 2.1); 3. θcon: the consistency/inconsistency score parameter matrices Wscore cns , W score inc (Section 3.2.2); 4. θclrec: the cross-lingual RAE semantic transformation parameter matrices W(3), Wu and bias terms b(3), bu (Section 3.2.3). For regularization, we assign each parameter set a unique weight: R(θ) = 2 IIθrecII2 λL 2 IIθLII2 + λrec (18) λcon λ + 2 II θcon II2 + l 2 ec IIθlcrecII2 Additionally, in order to prevent the hidden layer from being very small, we normalize all output vectors of the hidden layer to have length 1, p = p following Socher et al. (2011c). kpk , 3.3 Model Training Similar to Zhang et al. (2014), we adopt a cotraining style algorithm to train model parameters in the following two steps: First, we use a normal distribution (µ = 0, σ = 0.01) to randomly initialize all model parameters, and adopt the standard RAE to pre-train sourceand target-side phrase embeddings and tree structures (Section 2.1). Second, for each bilingual phrase, we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase, given the target-side phrase structure and node representations, and vice ve</context>
<context position="33579" citStr="Socher et al., 2011" startWordPosition="5428" endWordPosition="5431">res data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments </context>
<context position="35057" citStr="Socher et al. (2011" startWordPosition="5647" endWordPosition="5650"> phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases, and reconstructs tree structures of sub-phrases in one language according to aligned nodes in the other language, which, to the best of our knowledge, has never been investigated before. 7 Conclusions and Future Work In this paper, we have presented the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT. Punishing structuralalignment-inconsistent sub-structures and minimizing the gap between or</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proc. of ICML 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>151--161</pages>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="366" endWordPosition="369"> i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase stru</context>
<context position="9015" citStr="Socher et al., 2011" startWordPosition="1376" endWordPosition="1379">so a ddimensional vector. In order to measure how well p represents its children, we reconstruct the original children nodes in a reconstruction layer: [c01; c02] = f(W (2)p + b(2)) (2) where c01 and c02 are reconstructed children vectors, W(2) E R2d×d and b(2) E R2d×1. We can set y1 = p and then further use Eq. (1) again to compute y2 by setting [c1; c2] = [y1; x3]. This combination and reconstruction process of auto-encoder repeats at each node until the vector of the entire phrase is generated. To obtain the optimal binary tree and phrase representation for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin</context>
<context position="18765" citStr="Socher et al. (2011" startWordPosition="3045" endWordPosition="3048">in T¯e. As source and target phrase embeddings are separately learned, we first introduce a transformation matrix W (3) f and a bias term b(3) f to transform source phrase embeddings into the target-side semantic space, following Zhang et al. (2014) and Hermann and Blunsom (2014): p�ne = f(W(3) f pn f� + b(3) f ) here p&apos;ne denotes the reconstructed vector representation of n¯e, which is transformed from the vector representation pn f� of n ¯f. Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached, following Socher et al. (2011a). Specifically, given the vector representation p&apos;ne, we reconstruct vector representations of its two children nodes: [cue1; cue2] = f(Weu p�ne + bue) (15) where cue1 and cue2 are the reconstructed vector representations of the children nodes, Weu E R2d×d, and bueER2d×1. Eventually, given the original and reconstructed target phrase representations, we calculate Ef2e·rec(Tf, Te; θ) as follows: 1 � � Ef2e·rec(Tf, Te; θ) = 2 (n f�,ne)ES nETe (16) where pn and p&apos;n are the original and reconstructed vector representations of node n in the sub-tree structure T¯e rooted at n¯e. This error functio</context>
<context position="20978" citStr="Socher et al. (2011" startWordPosition="3418" endWordPosition="3421">s W(1), W(2) and bias terms b(1), b(2) (Section 2.1); 3. θcon: the consistency/inconsistency score parameter matrices Wscore cns , W score inc (Section 3.2.2); 4. θclrec: the cross-lingual RAE semantic transformation parameter matrices W(3), Wu and bias terms b(3), bu (Section 3.2.3). For regularization, we assign each parameter set a unique weight: R(θ) = 2 IIθrecII2 λL 2 IIθLII2 + λrec (18) λcon λ + 2 II θcon II2 + l 2 ec IIθlcrecII2 Additionally, in order to prevent the hidden layer from being very small, we normalize all output vectors of the hidden layer to have length 1, p = p following Socher et al. (2011c). kpk , 3.3 Model Training Similar to Zhang et al. (2014), we adopt a cotraining style algorithm to train model parameters in the following two steps: First, we use a normal distribution (µ = 0, σ = 0.01) to randomly initialize all model parameters, and adopt the standard RAE to pre-train sourceand target-side phrase embeddings and tree structures (Section 2.1). Second, for each bilingual phrase, we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase, given the target-side phrase structure and node representations, and vice ve</context>
<context position="33579" citStr="Socher et al., 2011" startWordPosition="5428" endWordPosition="5431">res data published by the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments </context>
<context position="35057" citStr="Socher et al. (2011" startWordPosition="5647" endWordPosition="5650"> phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases, and reconstructs tree structures of sub-phrases in one language according to aligned nodes in the other language, which, to the best of our knowledge, has never been investigated before. 7 Conclusions and Future Work In this paper, we have presented the BCorrRAE to learn phrase embeddings and tree structures of bilingual phrases for SMT. Punishing structuralalignment-inconsistent sub-structures and minimizing the gap between or</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011c. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proc. of EMNLP 2011, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>455--465</pages>
<contexts>
<context position="2591" citStr="Socher et al., 2013" startWordPosition="370" endWordPosition="373">. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c</context>
<context position="33601" citStr="Socher et al., 2013" startWordPosition="5432" endWordPosition="5435">the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translati</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013a. Parsing with compositional vector grammars. In Proc. of ACL 2013, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="2591" citStr="Socher et al., 2013" startWordPosition="370" endWordPosition="373">. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c</context>
<context position="33601" citStr="Socher et al., 2013" startWordPosition="5432" endWordPosition="5435">the the statistics released by the estimates announced Table 5: Semantically similar target phrases in the training set for example source phrases. Chen and Manning, 2014). To avoid exploiting manually input features, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translati</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP 2013, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>14--25</pages>
<contexts>
<context position="1787" citStr="Sundermeyer et al., 2014" startWordPosition="249" endWordPosition="252">f BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiat</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proc. of EMNLP 2014, pages 14–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Recurrent neural networks for word alignment model.</title>
<date>2014</date>
<booktitle>In Proc. of ACL 2014,</booktitle>
<pages>1470--1480</pages>
<contexts>
<context position="1761" citStr="Tamura et al., 2014" startWordPosition="245" endWordPosition="248">e the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2014</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2014. Recurrent neural networks for word alignment model. In Proc. of ACL 2014, pages 1470– 1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke M Tran</author>
<author>Arianna Bisazza</author>
<author>Christof Monz</author>
</authors>
<title>Word translation prediction for morphologically rich languages with bilingual neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1676--1688</pages>
<contexts>
<context position="34617" citStr="Tran et al., 2014" startWordPosition="5579" endWordPosition="5582">learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire </context>
</contexts>
<marker>Tran, Bisazza, Monz, 2014</marker>
<rawString>Ke M. Tran, Arianna Bisazza, and Christof Monz. 2014. Word translation prediction for morphologically rich languages with bilingual neural networks. In Proc. of EMNLP 2014, pages 1676–1688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Hai Zhao</author>
<author>Bao-Liang Lu</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Neural network based bilingual language model growing for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>189--195</pages>
<contexts>
<context position="1806" citStr="Wang et al., 2014" startWordPosition="253" endWordPosition="256"> both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning repr</context>
<context position="34513" citStr="Wang et al., 2014" startWordPosition="5562" endWordPosition="5565">ings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits differ</context>
</contexts>
<marker>Wang, Zhao, Lu, Utiyama, Sumita, 2014</marker>
<rawString>Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama, and Eiichiro Sumita. 2014. Neural network based bilingual language model growing for statistical machine translation. In Proc. of EMNLP 2014, pages 189–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haiyang Wu</author>
<author>Daxiang Dong</author>
<author>Xiaoguang Hu</author>
<author>Dianhai Yu</author>
<author>Wei He</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Improve statistical machine translation with context-sensitive bilingual semantic embedding model.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>142--146</pages>
<contexts>
<context position="32381" citStr="Wu et al., 2014" startWordPosition="5250" endWordPosition="5253">pecially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase BRAE BCorrRAESM BCorrRAEST 䌓㥎 to advocate the out to advocate encouraging (advocate) in preaching the been encouraging claimed the promotion of an advocate advocate 惮䜃坝揔 as well as severe challenges of rigorous challenges rigorous challenge (serious challenge) a serious challenge to as well as severe challenges enormous challenge a serious challenge from of severe challenges severe challenge 䋺㟙 䀛 嗪䛢 by the figures published by the to the estimates announced published data (data released) the statistics released by at the figures published released figures data published by</context>
</contexts>
<marker>Wu, Dong, Hu, Yu, He, Wu, Wang, Liu, 2014</marker>
<rawString>Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dianhai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting Liu. 2014. Improve statistical machine translation with context-sensitive bilingual semantic embedding model. In Proc. of EMNLP 2014, pages 142–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="23756" citStr="Wu, 1997" startWordPosition="3875" endWordPosition="3876">de n, and Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging SimSM(pf,p0f) = 2IIpf − p0 1 fII2 IIpn − p0nII2 1253 rules A -* [A1, A2]|(A1, A2) which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A -* f/e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out.</title>
<date>2010</date>
<booktitle>In Proc. of ACL 2010,</booktitle>
<pages>475--484</pages>
<contexts>
<context position="25544" citStr="Wuebker et al., 2010" startWordPosition="4160" endWordPosition="4163">ntropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4. Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2A block is a bilingual phrase without maximum length limitation. 3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4http://www.speech.sri.com/projects/srilm/download.html Parame</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training phrase translation models with leaving-one-out. In Proc. of ACL 2010, pages 475– 484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>521--528</pages>
<contexts>
<context position="23777" citStr="Xiong et al., 2006" startWordPosition="3877" endWordPosition="3880">Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging SimSM(pf,p0f) = 2IIpf − p0 1 fII2 IIpn − p0nII2 1253 rules A -* [A1, A2]|(A1, A2) which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A -* f/e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proc. of ACL 2006, pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word alignment modeling with context dependent deep neural network.</title>
<date>2013</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>166--175</pages>
<contexts>
<context position="1683" citStr="Yang et al., 2013" startWordPosition="229" endWordPosition="232">nt levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expr</context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context dependent deep neural network. In Proc. of ACL 2013, pages 166–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>111--121</pages>
<contexts>
<context position="2918" citStr="Zhang et al., 2014" startWordPosition="422" endWordPosition="425">rucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase</context>
<context position="9431" citStr="Zhang et al. (2014)" startWordPosition="1446" endWordPosition="1449"> auto-encoder repeats at each node until the vector of the entire phrase is generated. To obtain the optimal binary tree and phrase representation for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) − Esem(f|e0, 0) + 11 where Esem(f|e, 0) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe, respectively. Since phrase embeddings for the source and target language are learned separately in different vector spaces, a transformation</context>
<context position="10726" citStr="Zhang et al., 2014" startWordPosition="1675" endWordPosition="1678">n the source-to-target direction. Thus, Esem(f|e, 0) is calculated as Esem(f|e, 0) = 2 11 pe − f(W (3) 1 f pf + b(3) f ) 112 (6) where b(3) f E Rd×1 is a bias term. E∗sem(e|f, 0) and Esem(e|f, 0) can be computed in a similar way. The joint error of (f, e) is therefore defined as follows: E(f, e; 0) = α(Erec(f, 0) + Erec(e, 0)) 7 +(1 − α)(E∗sem(f |e, 0) + E∗sem(e |f, 0)) ( ) The final BRAE objective function over the training instance set D becomes: λ E(f, e; 0) + 2110112 (8) Model parameters can be optimized over the total errors on training bilingual phrases in a co-training style algorithm (Zhang et al., 2014). 3 The BCorrRAE Model As depicted above, the learned embeddings using BRAE may be unreasonable due to the neglect of bilingual constraints at different levels. To address this drawback, we propose the BCorrRAE for bilingual phrase embeddings, which incorporates bilingual correspondence information into the learning process of structures and embeddings via word alignments. In our model, we explore word alignments in two ways: (1) ensuring that a learned bilingual phrase structure is consistent with word alignments as much as possiMax-SemanticReconstruction Error Margin Error x1 x2 x3 Reconstru</context>
<context position="18395" citStr="Zhang et al. (2014)" startWordPosition="2980" endWordPosition="2983">alignments. We then calculate Ef2e·rec(Tf, Te; θ) as the sum of error scores over all node pairs in S. Given a source node nf¯ with its aligned node n¯e on the target side, we use nf¯ to reconstruct the sub-tree structure T¯e rooted at n¯e and compute the error score based on the semantic distance between the original and reconstructed vector representations of nodes in T¯e. As source and target phrase embeddings are separately learned, we first introduce a transformation matrix W (3) f and a bias term b(3) f to transform source phrase embeddings into the target-side semantic space, following Zhang et al. (2014) and Hermann and Blunsom (2014): p�ne = f(W(3) f pn f� + b(3) f ) here p&apos;ne denotes the reconstructed vector representation of n¯e, which is transformed from the vector representation pn f� of n ¯f. Then, we repeat the reconstruction procedure in a top-down manner along the corresponding target tree structure until leaf nodes are reached, following Socher et al. (2011a). Specifically, given the vector representation p&apos;ne, we reconstruct vector representations of its two children nodes: [cue1; cue2] = f(Weu p�ne + bue) (15) where cue1 and cue2 are the reconstructed vector representations of the</context>
<context position="21037" citStr="Zhang et al. (2014)" startWordPosition="3429" endWordPosition="3432">on: the consistency/inconsistency score parameter matrices Wscore cns , W score inc (Section 3.2.2); 4. θclrec: the cross-lingual RAE semantic transformation parameter matrices W(3), Wu and bias terms b(3), bu (Section 3.2.3). For regularization, we assign each parameter set a unique weight: R(θ) = 2 IIθrecII2 λL 2 IIθLII2 + λrec (18) λcon λ + 2 II θcon II2 + l 2 ec IIθlcrecII2 Additionally, in order to prevent the hidden layer from being very small, we normalize all output vectors of the hidden layer to have length 1, p = p following Socher et al. (2011c). kpk , 3.3 Model Training Similar to Zhang et al. (2014), we adopt a cotraining style algorithm to train model parameters in the following two steps: First, we use a normal distribution (µ = 0, σ = 0.01) to randomly initialize all model parameters, and adopt the standard RAE to pre-train sourceand target-side phrase embeddings and tree structures (Section 2.1). Second, for each bilingual phrase, we update its source-side parameters to obtain the fine-tuned vector representation and binary tree of the source phrase, given the target-side phrase structure and node representations, and vice versa. In this process, we apply L-BFGS to tune parameters ba</context>
<context position="25461" citStr="Zhang et al. (2014)" startWordPosition="4147" endWordPosition="4150">mber, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4. Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2A block is a bilingual phrase without maximum length limitation. 3http://homepages.inf.ed.ac.uk/lzhang10</context>
<context position="28407" citStr="Zhang et al. (2014)" startWordPosition="4612" endWordPosition="4615"> the impact of embedding dimensionality on our BCorrRAE, we tried four different dimensions from 25 to 100 with an increment of 25 each time. The results are displayed in Table 2. We can observe that the performance of our model is not consistently improved with the increment of dimensionality. This may be because a 5https://code.google.com/p/word2vec/ 1254 larger dimension brings in much more parameters, and therefore makes parameter tuning more difficult. In practice, setting the dimension d to 50, we can get satisfactory results without much computation effort, which has also been found by Zhang et al. (2014). 5.4 Structural Similarity vs. Semantic Similarity Table 2 also shows that the performance of BCorrRAEST, the system with the structural similarity feature in Eq. (20), is always superior to that of BCorrRAESM with the semantic similarity feature in Eq. (19). BCorrRAEST is better than BCorrRAESM by 0.483 BLEU points on average. In most cases, differences between BCorrRAEST and BCorrRAESM with the same dimensionality are statistically significant. This suggests that digging into structures of bilingual phrases (BCorrRAEST) can obtain further improvements over only modeling bilingual phrases as</context>
<context position="34638" citStr="Zhang et al., 2014" startWordPosition="5583" endWordPosition="5586"> an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire phrases, and reconstr</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In Proc. of ACL 2014, pages 111–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="1719" citStr="Zou et al., 2013" startWordPosition="237" endWordPosition="240">n bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successf</context>
<context position="34154" citStr="Zou et al. (2013)" startWordPosition="5511" endWordPosition="5514"> (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to inco</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proc. of EMNLP 2013, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
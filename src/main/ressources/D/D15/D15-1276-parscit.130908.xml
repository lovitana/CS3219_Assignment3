<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000119">
<title confidence="0.9987695">
Morphological Analysis for Unsegmented Languages
using Recurrent Neural Network Language Model
</title>
<author confidence="0.956152">
Hajime Morita1,2 Daisuke Kawahara1 Sadao Kurohashi1,2
</author>
<affiliation confidence="0.86118">
1 Kyoto University 2 CREST, Japan Science and Technology Agency
</affiliation>
<email confidence="0.919354">
{hmorita, dk, kuro}@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.981729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945789473684">
We present a new morphological analy-
sis model that considers semantic plausi-
bility of word sequences by using a re-
current neural network language model
(RNNLM). In unsegmented languages,
since language models are learned from
automatically segmented texts and in-
evitably contain errors, it is not apparent
that conventional language models con-
tribute to morphological analysis. To solve
this problem, we do not use language mod-
els based on raw word sequences but use a
semantically generalized language model,
RNNLM, in morphological analysis. In
our experiments on two Japanese corpora,
our proposed model significantly outper-
formed baseline models. This result indi-
cates the effectiveness of RNNLM in mor-
phological analysis.
</bodyText>
<sectionHeader confidence="0.992541" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99828425">
In contrast to space-delimited languages like En-
glish, word segmentation is the first and most cru-
cial step for natural language processing (NLP)
in unsegmented languages like Japanese, Chinese,
and Thai (Kudo et al., 2004; Kaji and Kitsure-
gawa, 2014; Shen et al., 2014; Kruengkrai et al.,
2006). Word segmentation is usually performed
jointly with related analysis: POS tagging for Chi-
nese, and POS tagging and lemmatization (anal-
ysis of inflected words) for Japanese. Morpho-
logical analysis including word segmentation has
been widely and actively studied, and for exam-
ple, Japanese word segmentation accuracy is in the
high 90s. However, we often observe that strange
outputs of downstream NLP applications such as
machine translation and question answering come
from incorrect word segmentations.
For example, the state-of-the-art and popu-
lar Japanese morphological analyzers, JUMAN
(Kurohashi and Kawahara, 2009) and MeCab
(Kudo et al., 2004) both analyze “外国人参政権
(foreigner’s right to vote)” not into the correct seg-
mentation of (1a), but into the incorrect and awk-
ward segmentation of (1b).
</bodyText>
<equation confidence="0.850851">
(1) a. 外国 / 人
foreigner / 参政 / 権
right to vote
b. 外国 / 人参 / 政権
foreign carrot regime
</equation>
<bodyText confidence="0.987091242424242">
JUMAN is a rule-based morphological analyzer,
defining word-to-word (including inflection) con-
nectivities and their scores. MeCab is a supervised
morphological analyzer, learning the probabilities
of word/POS/inflection sequence from an anno-
tated corpus of tens of thousands of sentences.
Both systems, however, cannot realize semanti-
cally appropriate analysis, and often produce to-
tally strange outputs like the above.
This paper proposes a semantically appropriate
morphological analysis method for unsegmented
languages using a language model. For unseg-
mented languages, morphological analysis and
language modeling form a chicken-and-egg prob-
lem. That is, if high-quality morphological analy-
sis is available, we can learn a high-quality lan-
guage model from a morphologically analyzed
large corpus. On the other hand, if a high-quality
language model is available, we can achieve high-
quality morphological analysis by looking for a
segmented word sequence with a large language
model score. However, even if we learn a language
model from a corpus analyzed by a certain level
of morphological analyzer, the language model is
affected by the analysis errors of the morphologi-
cal analyzer and it is no practical use for the im-
provement of the morphological analyzer. A lan-
guage model trained by incorrectly segmented “外
国 (foreign)/人参 (carrot)/政権 (regime)” just sup-
ports that incorrect segmentation.
The point of the paper is that we have tackled
the chicken-and-egg problem, not by using a lan-
2292
</bodyText>
<note confidence="0.81128">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2292–2297,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<equation confidence="0.958043454545455">
EOS
(foreign)
(carrot)
(regime)
国人 参政
(a person name)�� (voting)
N(perso n)n(coun try)l( out)o(po li tics )0n o u
を (pa rticl e) r論( disc uss)enoun, nou ny verb
p ast formanou nenounlnoun l外国 人参 政 権 論じる じたを外
国 人 参 政 権 国 外 政 参 人 権 (right)n( three)
ninounv no un.n oun1no une nounM BO SrInput :n
</equation>
<bodyText confidence="0.977467910447761">
Fi gure1:An examp leofa wor d latti ce .gu agemode
lofrawword seque nces,but byusinga semantica llyg
eneralized lang ua ge m odelbasedon w ordembedd
in gs,RNNLM (R ecurrent Ne uralNetworkL anguag
eModel ) (Miko lo v e ta l.,2010;M ikolovet al.,2011
).Th eRNNLM istrained onan autom atica lly analyzedc
orpusof tenmillionsen tences,w hich poss ibl yinc
ludesinco rr ect s eg- menta tionssu chas“外国(f oreig
n)/人 参(car rot)/政権 (reg i me).”However ,onsema nti
callyge ner- alize dlevel ,itis an u nnatur alsema
nticseq uencelikena tion vegetable p olit i cs.S
incethe sta te-of-th e-art mor phol o gicalanalyz era
chievesthe highaccu racy, i td oes notoften produce
incorrectanal -yse sw h ichsupportsu ch asemant
ic allystrang ese-que nc e.This wouldpref
er analysistow ardsema n-tic allya pp rop
riatewo rdsequ en ces. When a mor-phol ogi cala
nalyzer utili zessuch ageneral izedandreason
ablelangu
inganan-nota ted cor pus ofma nu allys egment
agemodel,itc
anpen aliz estr angeseg mentati onsl ike“ 外国(fo
reign) /人参(carr ot)/政権 (reg ime),”leading tobet
terac cura cy .We furthe rmoreret rainRNN LMus
ed45ksen -tences ,whi ch furtherimprov esmorphol
ogicala nalysis. 2 Related WorkTherehaveb eens
eve ralstudies thathave inte-gratedl anguage mode
lsint omorpho log ica lanal- ysis.Wa nge tal.
(201 1)impro vedCh inesew ord segmentatio na
ndPOStaggingb yusing N-g ramf eatu reslear ne
dfro manautomati call ysegm entedc orpus.How ever,
since theau to-segmented cor- pus inevitablycont
ains segmen tationer rors,f re-quen t N-gra msareno
talwa ys cor rectandthu st hisprobl em might
corp
affe ctt heperforman ceofmo rpho l ogical anal
ysis. Theya lsodivi ded N-gramfr eque nciesinto
thre ebi nne dfeature s:hig h-frequen cy ,midd l
e-freque ncyan d l ow -frequenc y.Suchcoar sefeatu
resca nn ot express sl ightdiffer -ence sin thelik el
ihoodofla ngua gemo d
els. Kajiand Kits ureg awa(2014)used ab igra
mlan- guage m ode lfeatur e for Japaneseword seg
men ta-tion andPOS ta ggin g. The iro bj ecti ve ofu s
ingalang uagem odelistono rmal iz einformal ly
spelledw ordsinmic roblogs.Theref ore, thei robj
ecti veis diffe rentfromour s.So mestudiesh aveu
sedch aracter
-bas edlan-guag emod elsf or Ch ines ewo rdse
gmentation andPOS tag ging(Z heng eta l., 20 13;Liu
etal .,20 14).Alth oug htheir a ppro acheshave
no dr awb ack soflear ning in corr ectse gm enta
tions,they on ly cap-t ur emor e local infor matio
nth an word- base dlan-guage mode l s.Worde mbe
ddin gsh aveb ee nal souse df orm or-pho logicalan
aly sis.Neur al net workba sedm odelshave b een
propos edf orChi nesewordse gme nta- tio n andP
OSt aggi ng(Peiet al. ,2 014)orwordseg mentation
(Ma nsur e ta l.,2 013).These me th-odsacq uire
wor dembeddi ngsfrom a
corpus,and
se themas theinput thenu ofthe neuralnetworks.
proposed model learnsword embeddings via Our
andtheseembeddingsareusedforscor- RNNLM,
word transitions in morphologicalanalysis. ing
usageofwordembeddingsisdifferent from Our
reviou s studies. thep
</bodyText>
<sectionHeader confidence="0.577756" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.571528333333333">
We
us and a manually labeled corpus.
propose odel a nw morphoogical anlysis m
t
se- hat conidrs semantic plausibility of word
RNNLM using both n automtically analyzed th
</bodyText>
<subsectionHeader confidence="0.9964465">
3.1 Recurrent Neural Network Language
Model
</subsectionHeader>
<bodyText confidence="0.971041625">
M quences by using RNNLM. We integrae RNN L
into morphological analysis (Figure 2). We train
RNNLM is a recurrent neural network language
model (Mikolov et al., 2010), which outputs a
probability distribution of the next word, given the
embedding of the last word and its context. We
2293
Figure 2: Workflow for training RNNLM and base
model.
employ the RNNME language model1 proposed
by (Mikolov et al., 2011; Mikolov, 2012) as the
implementation of RNNLM. The RNNME lan-
guage model has direct connections from the input
layer of the recurrent neural network to the output
layer, which act as a maximum entropy model and
avoid to waste a lot of parameters to describe sim-
ple patterns. Hereafter, we refer to the RNNME
language model simply as RNNLM.
To train RNNLM, we use a raw corpus of 10
million sentences from the web corpus (Kawa-
hara and Kurohashi, 2006). These sentences are
automatically segmented by JUMAN (Kurohashi
and Kawahara, 2009). The training of RNNLM
is based on lemmatized word sequences without
POS tags.
The trained model contains errors caused by
an automatically analyzed corpus. We retrain
RNNLM using a manually labeled corpus after
training RNNLM using the automatically ana-
lyzed corpus as shown in Figure 2. The retraining
aims to cope with errors related to function word
sequences.
</bodyText>
<subsectionHeader confidence="0.996353">
3.2 Base Model
</subsectionHeader>
<bodyText confidence="0.999997666666667">
For our base model, we adopt a model for su-
pervised morphological analysis, which performs
segmentation, lemmatization and POS tagging
jointly. We train this model using a tagged cor-
pus of tens of thousands of sentences that contain
gold segmentations, lemmas, inflection forms and
POS tags. To predict the most probable sequence
of words with lemmas and POS tags given an input
sentence, we execute the following procedure:
</bodyText>
<listItem confidence="0.9986758">
1. Look up the string of the input sentence using
a dictionary.
2. Make a word lattice.
3. Search for the path with the highest score
from the lattice.
</listItem>
<bodyText confidence="0.882095545454545">
1RNNME is the abbreviation of Recurrent Neural Net-
work trained jointly with Maximum Entropy model.
Figure 1 illustrates the constructed lattice during
the procedure. At the dictionary lookup step, we
use the basic dictionary of JUMAN and an ad-
ditional dictionary comprising 0.8 million words,
both of which have lemma, POS and inflection in-
formation. The additional dictionary mainly con-
sists of itemizations in articles and article titles in
Japanese Wikipedia.
We define the scoring function as follows:
</bodyText>
<equation confidence="0.996433">
scoreB(y) _ &apos;b(y) · ⃗�, (1)
</equation>
<bodyText confidence="0.9998435">
where y is a tagged word sequence, 4b(y) is a
feature vector for y, and w is a weight vector.
Each element in w gives a weight to its corre-
sponding feature in 4b(y). We use the unigram
and the bigram features composed from word base
form, POS and inflection described in Kudo et al.
(2004). We also use additional lexical features
such as character type, and trigram features used
in Zhang and Clark (2008). To learn the weight
vector, we adopt exact soft confidence-weighted
learning (Wang et al., 2012).
To consider out-of-vocabulary (OOV) words
that are not found in the dictionary, we automat-
ically generate words at the lookup step by seg-
menting the input string by character types2. For
training, we regard words that are not found in the
dictionary but found in the training corpus as OOV
words to learn their weights.
</bodyText>
<subsectionHeader confidence="0.903645">
3.3 RNNLM Integrated Model
</subsectionHeader>
<bodyText confidence="0.999935666666667">
Based on retrained RNNLM, we calculate an
RNNLM score (scoreR(y)) to be integrated into
the base model. The RNNLM score is defined as
the log probability of the next word given its con-
text (path). Here, the score for an OOV word is
given by the following formula:
</bodyText>
<equation confidence="0.972533">
−CP − LP · length(n), (2)
</equation>
<bodyText confidence="0.99985675">
where CP is a constant penalty for OOV words,
LP is a factor for the character length penalty, and
length(n) returns the character length of the next
word n. This formula is defined to penalize longer
words, which are likely to produce segmentation
errors.
We then integrate the RNNLM score into the
base model using the following equation:
</bodyText>
<figure confidence="0.877291571428572">
scoreI(y) _ (1 − α)scoreB(y) + α scoreR(y),
(3)
2Japanese has three types of characters: Kanji, Hiragana
and Katakana.
Auto segmented
corpus
Training
RNNLM
Labeled corpus
Re-training
Training
Proposed model
RNNLM retrained
Base model
</figure>
<page confidence="0.508291">
2294
</page>
<bodyText confidence="0.99998712">
where α is an interpolation parameter that is tuned
on development data.
For decoding, we employ beam search as used
in Zhang and Clark (2008). Since the possi-
ble context (paths in the word lattice) consid-
ered in RNNLM falls into combinatorial explosion
in morphological analysis, we keep only prob-
able context candidates inside the beam. That
is, each node keeps candidates inside the beam
width. Each candidate has a vector represent-
ing context, and two words of history. The re-
current model makes decoding harder than non-
recurrent neural network language models. How-
ever, we use RNNLM because the model outper-
forms other NNLMs (Mikolov, 2012) and the re-
sult suggests that the model is more likely to cap-
ture semantic plausibility. Since a sentence rarely
contains ambiguous and semantically appropriate
word sequences, we think that beam search with
enough beam size is able to keep the ambiguous
candidates of word sequences. In the case of non-
recurrent NNLMs and the base model, which uses
trigram features, we can conduct exact decoding
using the second-order Viterbi algorithm (Thede
and Harper, 1999).
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998262">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999727775510204">
In our experiments, we used the Kyoto University
Text Corpus (Kawahara et al., 2002) and Kyoto
University Web Document Leads Corpus (Hangyo
et al., 2012) as manually tagged corpora. We ran-
domly chose 2,000 sentences from each corpus
for test data, and 500 sentences for development
data. We used the remaining part of the corpora
as training data to train our base model and retrain
RNNLM. In total, we used 45,000 sentences for
training.
For comparative purposes, we used the follow-
ing four baselines: the Japanese morphological an-
alyzer JUMAN, the supervised morphological an-
alyzer MeCab, the base model, and a model using
a conventional language model. For this language
model, we built a trigram language model with
Kneser-Ney smoothing using SRILM (Stolcke,
2002) from the same automatically segmented cor-
pus. The language model is modified to have an
interpolation parameter α and length penalty for
OOV, LP.
We set the beam width to 5 by preliminary ex-
periments. We also set a constant penalty for OOV
words (CP) as 5, which is the default value in
the implementation of Mikolov et al. (2011). We
tuned the parameters of our proposed model and
the baseline model (α and LP) and the parameters
of language models using grid search on the de-
velopment data. We set α = 0.3, LP =1.5 for the
proposed model (“ Base + RNNLMretrain”).3
We measured the performance of the baseline
models and the proposed model by F-value of
word segmentation and F-value of joint evaluation
of word segmentation and POS tagging. We calcu-
lated F-value for the two corpora (news and web)
and the merged corpus (all).
We used the bootstrapping method (Zhang et
al., 2004) to test statistical significance between
proposed models and other models. Suppose we
have a test set T that includes N sentences. The
method repeatedly creates M new test sets by re-
sampling N sentences with replacement from T.
We calculate the F-value of each model on M + 1
test sets including T, and then we have M + 1
score differences. From the scores, we calculate
the 95% confidence interval. If the interval does
not overlap with zero, the two models are consid-
ered as statistically significantly different. In our
evaluation, M is set to 2,000.
</bodyText>
<subsectionHeader confidence="0.863968">
4.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.999548095238095">
Table 1 lists the results of our proposed model and
the baseline models. Our proposed model (“Base
+ RNNLMretrain”) significantly outperforms all the
baseline models and “Base + RNNLM,” which
does not use retraining. In particular, we achieved
a large improvement for segmentation. This can be
attributed to the use of RNNLM that was learned
based on lemmatized word sequence without POS
tags.
“Base + SRILM” segmented the example de-
scribed in Section 1 (“外国人参政権”) into the
incorrect segmentation “外国/人参/政権” in the
same way as JUMAN. This segmentation error
was caused by errors in the automatically seg-
mented corpus that was used to train the language
model. Our proposed model can correctly seg-
ment this example if a proper context is available
by semantically capturing word transitions using
RNNLM.
The base model, JUMAN and “Base + SRILM”
incorrectly segmented “健康 (healthy)/など (etc.)/
</bodyText>
<table confidence="0.929276818181818">
3We set α = 0.1, LP = 2.0 for “Base + RNNLM”, and α =
0.3, LP = 0.5 for “Base + SRILM.”
2295
Segmentation Seg + POS Segmentation Seg + POS Segmentation Seg + POS
(news) (news) (web) (web) (all) (all)
JUMAN 98.92 98.47 98.20 97.64 98.64 98.14
MeCab 99.07 98.58 98.22 97.51 98.74 98.16
Base model 98.94 98.46 97.71 96.90 98.46 97.85
Base + SRILM 98.94 98.40 98.13 97.33 98.62 97.98
Base + RNNLM 99.06 98.59 98.17 97.45 98.71 98.14
Base + RNNLMretrain 99.15* 98.70* 98.37* 97.68* 98.84* 98.30*
</table>
<tableCaption confidence="0.9380845">
Table 1: Results for test datasets. * means the score of “Base + RNNLMretrain” is significantly improved
from that of all other models.
</tableCaption>
<listItem confidence="0.796976">
0) (of)/ A (point)/-C (in)/ ” (in terms of health
</listItem>
<bodyText confidence="0.966288">
and so on) into “Mfg fid:(healthy)/Z0)(any)/ A
(point)/-C (in)/ .” Although this segmentation
can be grammatically accepted, it is difficult to
semantically interpret this word sequence. Our
proposed model can correctly segment this exam-
ple because RNNLM learns semantically plausible
word sequences.
</bodyText>
<sectionHeader confidence="0.987717" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999947230769231">
In this paper, we proposed a new model for
morphological analysis that is integrated with
RNNLM. We trained RNNLM on an automati-
cally segmented corpus and tuned on a manually
tagged corpus. The proposed model was able to
significantly reduce errors in the base model by
capturing semantic plausibility of word sequences
using RNNLM. In the future, we will design fea-
tures derived from RNNLM models, and integrate
them into a unified learning framework. We also
intend to apply our method to unsegmented lan-
guages other than Japanese, such as Chinese and
Thai.
</bodyText>
<sectionHeader confidence="0.975097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987967837209303">
Masatsugu Hangyo, Daisuke Kawahara, and Sadao
Kurohashi. 2012. Building a diverse document
leads corpus annotated with semantic relations. In
Proceedings of the 26th Pacific Asia Conference
on Language, Information, and Computation, pages
535–544.
Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Ac-
curate word segmentation and POS tagging for
japanese microblogs: Corpus annotation and joint
modeling with lexical normalization. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
99–109, Doha, Qatar. Association for Computa-
tional Linguistics.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation, pages 1344–1347.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the
Third International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Palmas,
Canary Islands - Spain, May. European Language
Resources Association (ELRA). ACL Anthology
Identifier: L02-1302.
Canasai Kruengkrai, Virach Sornlertlamvanich, and
Hitoshi Isahara. 2006. A conditional random field
framework for Thai morphological analysis. In Pro-
ceedings of LREC, pages 2419–2424.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings
of the Conference on Emprical Methods in Natural
Language Processing (EMNLP 2004), volume 2004.
Sadao Kurohashi and Daisuke Kawahara, 2009.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Xiaodong Liu, Kevin Duh, Yuji Matsumoto, and To-
moya Iwakura. 2014. Learning character repre-
sentations for Chinese word segmentation. In NIPS
2014 Workshop on Modern Machine Learning and
Natural Language Processing.
Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
Chinese word segmentation. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1271–1277, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky. 2011. Strate-
gies for training large scale neural network language
2296
models. In Proceedings of ASRU 2011, pages 196–
201. IEEE Automatic Speech Recognition and Un-
derstanding Workshop.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Brno uni-
versity of technology.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for Chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 1: Long Papers, pages 293–303.
Mo Shen, Hongxiao Liu, Daisuke Kawahara, and
Sadao Kurohashi. 2014. Chinese Morphological
Analysis with Character-level POS Tagging. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 253–258, Baltimore, Maryland. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In John H L Hansen and
Bryan L Pellom, editors, 7th International Confer-
ence on Spoken Language Processing, ICSLP2002
- INTERSPEECH 2002, Denver, Colorado, USA,
September 16-20, 2002, pages 901–904. ISCA.
Scott M. Thede and Mary P. Harper. 1999. A second-
order Hidden Markov Model for part-of-speech tag-
ging. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 175–182, Morris-
town, NJ, USA, June. Association for Computa-
tional Linguistics.
Yiou Wang, Jun’ichi Kazama, Wenliang Chen, Yu-
jie Zhang, Kentaro Torisawa, and Yoshimasa Tsu-
ruoka. 2011. Improving chinese word segmenta-
tion and POS tagging with semi-supervised meth-
ods using large auto-analyzed data. In Proceedings
of the Fifth International Joint Conference on Nat-
ural Language Processing (IJCNLP-2011), pages
309–317, Chiang Mai, Thailand. Asian Federation
of Natural Language Processing.
Jialei Wang, Peilin Zhao, and Steven C.H. Hoi.
2012. Exact soft confidence-weighted learning. In
29th International Conference on Machine Learning
(ICML 2012), pages 121–128.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888–896,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores: How much improve-
ment do we need to have a better system? In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation (LREC-2004),
Lisbon, Portugal, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L04-1489.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmen-
tation and POS tagging. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2013, 18-21 Octo-
ber 2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL, pages 647–657.
</reference>
<page confidence="0.763977">
2297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.246626">
<title confidence="0.9610115">Morphological Analysis for Unsegmented using Recurrent Neural Network Language Model</title>
<affiliation confidence="0.47865">Daisuke Sadao</affiliation>
<address confidence="0.590694">University 2CREST, Japan Science and Technology</address>
<email confidence="0.70103">dk,</email>
<abstract confidence="0.9980098">We present a new morphological analysis model that considers semantic plausibility of word sequences by using a recurrent neural network language model (RNNLM). In unsegmented languages, since language models are learned from automatically segmented texts and inevitably contain errors, it is not apparent that conventional language models contribute to morphological analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masatsugu Hangyo</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Building a diverse document leads corpus annotated with semantic relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation,</booktitle>
<pages>535--544</pages>
<contexts>
<context position="12846" citStr="Hangyo et al., 2012" startWordPosition="2059" endWordPosition="2062">l is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 Experiments 4.1 Experimental Settings In our experiments, we used the Kyoto University Text Corpus (Kawahara et al., 2002) and Kyoto University Web Document Leads Corpus (Hangyo et al., 2012) as manually tagged corpora. We randomly chose 2,000 sentences from each corpus for test data, and 500 sentences for development data. We used the remaining part of the corpora as training data to train our base model and retrain RNNLM. In total, we used 45,000 sentences for training. For comparative purposes, we used the following four baselines: the Japanese morphological analyzer JUMAN, the supervised morphological analyzer MeCab, the base model, and a model using a conventional language model. For this language model, we built a trigram language model with Kneser-Ney smoothing using SRILM </context>
</contexts>
<marker>Hangyo, Kawahara, Kurohashi, 2012</marker>
<rawString>Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus annotated with semantic relations. In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation, pages 535–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Accurate word segmentation and POS tagging for japanese microblogs: Corpus annotation and joint modeling with lexical normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>99--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="1250" citStr="Kaji and Kitsuregawa, 2014" startWordPosition="178" endWordPosition="182">l analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Jap</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2014</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Accurate word segmentation and POS tagging for japanese microblogs: Corpus annotation and joint modeling with lexical normalization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99–109, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th</booktitle>
<contexts>
<context position="8243" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1297" endWordPosition="1301">n the embedding of the last word and its context. We 2293 Figure 2: Workflow for training RNNLM and base model. employ the RNNME language model1 proposed by (Mikolov et al., 2011; Mikolov, 2012) as the implementation of RNNLM. The RNNME language model has direct connections from the input layer of the recurrent neural network to the output layer, which act as a maximum entropy model and avoid to waste a lot of parameters to describe simple patterns. Hereafter, we refer to the RNNME language model simply as RNNLM. To train RNNLM, we use a raw corpus of 10 million sentences from the web corpus (Kawahara and Kurohashi, 2006). These sentences are automatically segmented by JUMAN (Kurohashi and Kawahara, 2009). The training of RNNLM is based on lemmatized word sequences without POS tags. The trained model contains errors caused by an automatically analyzed corpus. We retrain RNNLM using a manually labeled corpus after training RNNLM using the automatically analyzed corpus as shown in Figure 2. The retraining aims to cope with errors related to function word sequences. 3.2 Base Model For our base model, we adopt a model for supervised morphological analysis, which performs segmentation, lemmatization and POS tagging</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using highperformance computing. In Proceedings of the 5th</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Language Resources and Evaluation,</booktitle>
<pages>1344--1347</pages>
<marker></marker>
<rawString>International Conference on Language Resources and Evaluation, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands - Spain, May. European Language Resources Association (ELRA). ACL Anthology Identifier:</booktitle>
<pages>02--1302</pages>
<contexts>
<context position="12777" citStr="Kawahara et al., 2002" startWordPosition="2048" endWordPosition="2051">forms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 Experiments 4.1 Experimental Settings In our experiments, we used the Kyoto University Text Corpus (Kawahara et al., 2002) and Kyoto University Web Document Leads Corpus (Hangyo et al., 2012) as manually tagged corpora. We randomly chose 2,000 sentences from each corpus for test data, and 500 sentences for development data. We used the remaining part of the corpora as training data to train our base model and retrain RNNLM. In total, we used 45,000 sentences for training. For comparative purposes, we used the following four baselines: the Japanese morphological analyzer JUMAN, the supervised morphological analyzer MeCab, the base model, and a model using a conventional language model. For this language model, we </context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands - Spain, May. European Language Resources Association (ELRA). ACL Anthology Identifier: L02-1302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A conditional random field framework for Thai morphological analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2419--2424</pages>
<contexts>
<context position="1295" citStr="Kruengkrai et al., 2006" startWordPosition="187" endWordPosition="190">e language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological analyzers, JUMAN (Kurohas</context>
</contexts>
<marker>Kruengkrai, Sornlertlamvanich, Isahara, 2006</marker>
<rawString>Canasai Kruengkrai, Virach Sornlertlamvanich, and Hitoshi Isahara. 2006. A conditional random field framework for Thai morphological analysis. In Proceedings of LREC, pages 2419–2424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Emprical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<volume>volume</volume>
<contexts>
<context position="1222" citStr="Kudo et al., 2004" startWordPosition="174" endWordPosition="177">ute to morphological analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the stat</context>
<context position="10109" citStr="Kudo et al. (2004)" startWordPosition="1607" endWordPosition="1610">he basic dictionary of JUMAN and an additional dictionary comprising 0.8 million words, both of which have lemma, POS and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB(y) _ &apos;b(y) · ⃗�, (1) where y is a tagged word sequence, 4b(y) is a feature vector for y, and w is a weight vector. Each element in w gives a weight to its corresponding feature in 4b(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2. For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calcula</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Emprical Methods in Natural Language Processing (EMNLP 2004), volume 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<date>2009</date>
<booktitle>Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i. kyoto-u.ac.jp/EN/index.php?JUMAN.</booktitle>
<contexts>
<context position="1917" citStr="Kurohashi and Kawahara, 2009" startWordPosition="276" endWordPosition="279">, 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological analyzers, JUMAN (Kurohashi and Kawahara, 2009) and MeCab (Kudo et al., 2004) both analyze “外国人参政権 (foreigner’s right to vote)” not into the correct segmentation of (1a), but into the incorrect and awkward segmentation of (1b). (1) a. 外国 / 人 foreigner / 参政 / 権 right to vote b. 外国 / 人参 / 政権 foreign carrot regime JUMAN is a rule-based morphological analyzer, defining word-to-word (including inflection) connectivities and their scores. MeCab is a supervised morphological analyzer, learning the probabilities of word/POS/inflection sequence from an annotated corpus of tens of thousands of sentences. Both systems, however, cannot realize semanti</context>
<context position="8328" citStr="Kurohashi and Kawahara, 2009" startWordPosition="1309" endWordPosition="1312">ning RNNLM and base model. employ the RNNME language model1 proposed by (Mikolov et al., 2011; Mikolov, 2012) as the implementation of RNNLM. The RNNME language model has direct connections from the input layer of the recurrent neural network to the output layer, which act as a maximum entropy model and avoid to waste a lot of parameters to describe simple patterns. Hereafter, we refer to the RNNME language model simply as RNNLM. To train RNNLM, we use a raw corpus of 10 million sentences from the web corpus (Kawahara and Kurohashi, 2006). These sentences are automatically segmented by JUMAN (Kurohashi and Kawahara, 2009). The training of RNNLM is based on lemmatized word sequences without POS tags. The trained model contains errors caused by an automatically analyzed corpus. We retrain RNNLM using a manually labeled corpus after training RNNLM using the automatically analyzed corpus as shown in Figure 2. The retraining aims to cope with errors related to function word sequences. 3.2 Base Model For our base model, we adopt a model for supervised morphological analysis, which performs segmentation, lemmatization and POS tagging jointly. We train this model using a tagged corpus of tens of thousands of sentences</context>
</contexts>
<marker>Kurohashi, Kawahara, 2009</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara, 2009. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i. kyoto-u.ac.jp/EN/index.php?JUMAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
<author>Tomoya Iwakura</author>
</authors>
<title>Learning character representations for Chinese word segmentation.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Workshop on Modern Machine Learning and Natural Language Processing.</booktitle>
<marker>Liu, Duh, Matsumoto, Iwakura, 2014</marker>
<rawString>Xiaodong Liu, Kevin Duh, Yuji Matsumoto, and Tomoya Iwakura. 2014. Learning character representations for Chinese word segmentation. In NIPS 2014 Workshop on Modern Machine Learning and Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mairgup Mansur</author>
<author>Wenzhe Pei</author>
<author>Baobao Chang</author>
</authors>
<title>Feature-based neural language model and Chinese word segmentation.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1271--1277</pages>
<location>Nagoya, Japan,</location>
<marker>Mansur, Pei, Chang, 2013</marker>
<rawString>Mairgup Mansur, Wenzhe Pei, and Baobao Chang. 2013. Feature-based neural language model and Chinese word segmentation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1271–1277, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Dan Povey</author>
<author>Lukar Burget</author>
<author>Jan Honza Cernocky</author>
</authors>
<title>Strategies for training large scale neural network language</title>
<date>2011</date>
<pages>2296</pages>
<contexts>
<context position="7792" citStr="Mikolov et al., 2011" startWordPosition="1218" endWordPosition="1221">us and a manually labeled corpus. propose odel a nw morphoogical anlysis m t se- hat conidrs semantic plausibility of word RNNLM using both n automtically analyzed th 3.1 Recurrent Neural Network Language Model M quences by using RNNLM. We integrae RNN L into morphological analysis (Figure 2). We train RNNLM is a recurrent neural network language model (Mikolov et al., 2010), which outputs a probability distribution of the next word, given the embedding of the last word and its context. We 2293 Figure 2: Workflow for training RNNLM and base model. employ the RNNME language model1 proposed by (Mikolov et al., 2011; Mikolov, 2012) as the implementation of RNNLM. The RNNME language model has direct connections from the input layer of the recurrent neural network to the output layer, which act as a maximum entropy model and avoid to waste a lot of parameters to describe simple patterns. Hereafter, we refer to the RNNME language model simply as RNNLM. To train RNNLM, we use a raw corpus of 10 million sentences from the web corpus (Kawahara and Kurohashi, 2006). These sentences are automatically segmented by JUMAN (Kurohashi and Kawahara, 2009). The training of RNNLM is based on lemmatized word sequences wi</context>
<context position="13792" citStr="Mikolov et al. (2011)" startWordPosition="2218" endWordPosition="2221">ollowing four baselines: the Japanese morphological analyzer JUMAN, the supervised morphological analyzer MeCab, the base model, and a model using a conventional language model. For this language model, we built a trigram language model with Kneser-Ney smoothing using SRILM (Stolcke, 2002) from the same automatically segmented corpus. The language model is modified to have an interpolation parameter α and length penalty for OOV, LP. We set the beam width to 5 by preliminary experiments. We also set a constant penalty for OOV words (CP) as 5, which is the default value in the implementation of Mikolov et al. (2011). We tuned the parameters of our proposed model and the baseline model (α and LP) and the parameters of language models using grid search on the development data. We set α = 0.3, LP =1.5 for the proposed model (“ Base + RNNLMretrain”).3 We measured the performance of the baseline models and the proposed model by F-value of word segmentation and F-value of joint evaluation of word segmentation and POS tagging. We calculated F-value for the two corpora (news and web) and the merged corpus (all). We used the bootstrapping method (Zhang et al., 2004) to test statistical significance between propos</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar Burget, and Jan Honza Cernocky. 2011. Strategies for training large scale neural network language 2296</rawString>
</citation>
<citation valid="false">
<authors>
<author>models</author>
</authors>
<journal>IEEE Automatic Speech Recognition and Understanding Workshop.</journal>
<booktitle>In Proceedings of ASRU 2011,</booktitle>
<pages>196--201</pages>
<marker>models, </marker>
<rawString>models. In Proceedings of ASRU 2011, pages 196– 201. IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno university of technology.</institution>
<contexts>
<context position="7808" citStr="Mikolov, 2012" startWordPosition="1222" endWordPosition="1223">led corpus. propose odel a nw morphoogical anlysis m t se- hat conidrs semantic plausibility of word RNNLM using both n automtically analyzed th 3.1 Recurrent Neural Network Language Model M quences by using RNNLM. We integrae RNN L into morphological analysis (Figure 2). We train RNNLM is a recurrent neural network language model (Mikolov et al., 2010), which outputs a probability distribution of the next word, given the embedding of the last word and its context. We 2293 Figure 2: Workflow for training RNNLM and base model. employ the RNNME language model1 proposed by (Mikolov et al., 2011; Mikolov, 2012) as the implementation of RNNLM. The RNNME language model has direct connections from the input layer of the recurrent neural network to the output layer, which act as a maximum entropy model and avoid to waste a lot of parameters to describe simple patterns. Hereafter, we refer to the RNNME language model simply as RNNLM. To train RNNLM, we use a raw corpus of 10 million sentences from the web corpus (Kawahara and Kurohashi, 2006). These sentences are automatically segmented by JUMAN (Kurohashi and Kawahara, 2009). The training of RNNLM is based on lemmatized word sequences without POS tags. </context>
<context position="12188" citStr="Mikolov, 2012" startWordPosition="1956" endWordPosition="1957">ameter that is tuned on development data. For decoding, we employ beam search as used in Zhang and Clark (2008). Since the possible context (paths in the word lattice) considered in RNNLM falls into combinatorial explosion in morphological analysis, we keep only probable context candidates inside the beam. That is, each node keeps candidates inside the beam width. Each candidate has a vector representing context, and two words of history. The recurrent model makes decoding harder than nonrecurrent neural network language models. However, we use RNNLM because the model outperforms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 Experiments 4.1 Experimental Settings In our experiments, we used the Kyoto University Text Corpus (Kawahara et al., 2002) and Kyoto </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Brno university of technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Baobao Chang</author>
</authors>
<title>Maxmargin tensor neural network for Chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>293--303</pages>
<location>Baltimore, MD, USA,</location>
<marker>Pei, Ge, Chang, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Maxmargin tensor neural network for Chinese word segmentation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Shen</author>
<author>Hongxiao Liu</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Chinese Morphological Analysis with Character-level POS Tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>253--258</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="1269" citStr="Shen et al., 2014" startWordPosition="183" endWordPosition="186">oblem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological</context>
</contexts>
<marker>Shen, Liu, Kawahara, Kurohashi, 2014</marker>
<rawString>Mo Shen, Hongxiao Liu, Daisuke Kawahara, and Sadao Kurohashi. 2014. Chinese Morphological Analysis with Character-level POS Tagging. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 253–258, Baltimore, Maryland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>7th International Conference on Spoken Language Processing, ICSLP2002 - INTERSPEECH 2002,</booktitle>
<pages>901--904</pages>
<editor>John H L Hansen and Bryan L Pellom, editors,</editor>
<publisher>ISCA.</publisher>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="13461" citStr="Stolcke, 2002" startWordPosition="2160" endWordPosition="2161">as manually tagged corpora. We randomly chose 2,000 sentences from each corpus for test data, and 500 sentences for development data. We used the remaining part of the corpora as training data to train our base model and retrain RNNLM. In total, we used 45,000 sentences for training. For comparative purposes, we used the following four baselines: the Japanese morphological analyzer JUMAN, the supervised morphological analyzer MeCab, the base model, and a model using a conventional language model. For this language model, we built a trigram language model with Kneser-Ney smoothing using SRILM (Stolcke, 2002) from the same automatically segmented corpus. The language model is modified to have an interpolation parameter α and length penalty for OOV, LP. We set the beam width to 5 by preliminary experiments. We also set a constant penalty for OOV words (CP) as 5, which is the default value in the implementation of Mikolov et al. (2011). We tuned the parameters of our proposed model and the baseline model (α and LP) and the parameters of language models using grid search on the development data. We set α = 0.3, LP =1.5 for the proposed model (“ Base + RNNLMretrain”).3 We measured the performance of t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In John H L Hansen and Bryan L Pellom, editors, 7th International Conference on Spoken Language Processing, ICSLP2002 - INTERSPEECH 2002, Denver, Colorado, USA, September 16-20, 2002, pages 901–904. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A secondorder Hidden Markov Model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>175--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="12651" citStr="Thede and Harper, 1999" startWordPosition="2029" endWordPosition="2032">nt model makes decoding harder than nonrecurrent neural network language models. However, we use RNNLM because the model outperforms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 Experiments 4.1 Experimental Settings In our experiments, we used the Kyoto University Text Corpus (Kawahara et al., 2002) and Kyoto University Web Document Leads Corpus (Hangyo et al., 2012) as manually tagged corpora. We randomly chose 2,000 sentences from each corpus for test data, and 500 sentences for development data. We used the remaining part of the corpora as training data to train our base model and retrain RNNLM. In total, we used 45,000 sentences for training. For comparative purposes, we used the following four baselines: the Japanese morphological analyzer JUMAN, the supervis</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A secondorder Hidden Markov Model for part-of-speech tagging. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 175–182, Morristown, NJ, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011),</booktitle>
<pages>309--317</pages>
<location>Chiang Mai,</location>
<marker>Wang, Kazama, Chen, Zhang, Torisawa, Tsuruoka, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Wenliang Chen, Yujie Zhang, Kentaro Torisawa, and Yoshimasa Tsuruoka. 2011. Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011), pages 309–317, Chiang Mai, Thailand. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jialei Wang</author>
<author>Peilin Zhao</author>
<author>Steven C H Hoi</author>
</authors>
<title>Exact soft confidence-weighted learning.</title>
<date>2012</date>
<booktitle>In 29th International Conference on Machine Learning (ICML</booktitle>
<pages>121--128</pages>
<contexts>
<context position="10324" citStr="Wang et al., 2012" startWordPosition="1641" endWordPosition="1644"> and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB(y) _ &apos;b(y) · ⃗�, (1) where y is a tagged word sequence, 4b(y) is a feature vector for y, and w is a weight vector. Each element in w gives a weight to its corresponding feature in 4b(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2. For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calculate an RNNLM score (scoreR(y)) to be integrated into the base model. The RNNLM score is defined as the log probability of the next word given its context (path). Here, the score for an OOV word is given by the follow</context>
</contexts>
<marker>Wang, Zhao, Hoi, 2012</marker>
<rawString>Jialei Wang, Peilin Zhao, and Steven C.H. Hoi. 2012. Exact soft confidence-weighted learning. In 29th International Conference on Machine Learning (ICML 2012), pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>888--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="10226" citStr="Zhang and Clark (2008)" startWordPosition="1626" endWordPosition="1629"> POS and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB(y) _ &apos;b(y) · ⃗�, (1) where y is a tagged word sequence, 4b(y) is a feature vector for y, and w is a weight vector. Each element in w gives a weight to its corresponding feature in 4b(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2. For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calculate an RNNLM score (scoreR(y)) to be integrated into the base model. The RNNLM score is defined as the log probability</context>
<context position="11685" citStr="Zhang and Clark (2008)" startWordPosition="1872" endWordPosition="1875">ength(n) returns the character length of the next word n. This formula is defined to penalize longer words, which are likely to produce segmentation errors. We then integrate the RNNLM score into the base model using the following equation: scoreI(y) _ (1 − α)scoreB(y) + α scoreR(y), (3) 2Japanese has three types of characters: Kanji, Hiragana and Katakana. Auto segmented corpus Training RNNLM Labeled corpus Re-training Training Proposed model RNNLM retrained Base model 2294 where α is an interpolation parameter that is tuned on development data. For decoding, we employ beam search as used in Zhang and Clark (2008). Since the possible context (paths in the word lattice) considered in RNNLM falls into combinatorial explosion in morphological analysis, we keep only probable context candidates inside the beam. That is, each node keeps candidates inside the beam width. Each candidate has a vector representing context, and two words of history. The recurrent model makes decoding harder than nonrecurrent neural network language models. However, we use RNNLM because the model outperforms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of ACL-08: HLT, pages 888–896, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting bleu/nist scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<contexts>
<context position="14344" citStr="Zhang et al., 2004" startWordPosition="2315" endWordPosition="2318">is the default value in the implementation of Mikolov et al. (2011). We tuned the parameters of our proposed model and the baseline model (α and LP) and the parameters of language models using grid search on the development data. We set α = 0.3, LP =1.5 for the proposed model (“ Base + RNNLMretrain”).3 We measured the performance of the baseline models and the proposed model by F-value of word segmentation and F-value of joint evaluation of word segmentation and POS tagging. We calculated F-value for the two corpora (news and web) and the merged corpus (all). We used the bootstrapping method (Zhang et al., 2004) to test statistical significance between proposed models and other models. Suppose we have a test set T that includes N sentences. The method repeatedly creates M new test sets by resampling N sentences with replacement from T. We calculate the F-value of each model on M + 1 test sets including T, and then we have M + 1 score differences. From the scores, we calculate the 95% confidence interval. If the interval does not overlap with zero, the two models are considered as statistically significantly different. In our evaluation, M is set to 2,000. 4.2 Results and Discussions Table 1 lists the</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting bleu/nist scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC-2004),</rawString>
</citation>
<citation valid="true">
<authors>
<author>Portugal Lisbon</author>
</authors>
<date></date>
<journal>European Language Resources Association (ELRA). ACL Anthology Identifier:</journal>
<pages>04--1489</pages>
<marker>Lisbon, </marker>
<rawString>Lisbon, Portugal, May. European Language Resources Association (ELRA). ACL Anthology Identifier: L04-1489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for Chinese word segmentation and POS tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>18--21</pages>
<location>Grand Hyatt Seattle, Seattle, Washington, USA,</location>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 647–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
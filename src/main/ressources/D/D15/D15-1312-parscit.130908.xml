<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.9985335">
Identification and Verification of Simple Claims
about Statistical Properties
</title>
<author confidence="0.99505">
Andreas Vlachos and Sebastian Riedel
</author>
<affiliation confidence="0.992043">
Department of Computer Science
University College London
</affiliation>
<email confidence="0.971939">
{a.vlachos,s.riedel}@cs.ucl.ac.uk
</email>
<bodyText confidence="0.974530888888889">
Text: Lesotho, a landlocked enclave of
South Africa, has a population of nearly
2 million and covers an area slightly
smaller than the U.S. state of Maryland.
Entity: Lesotho
Property: population
Value claimed in text: 2,000,000
Value in knowledge base: 2,193,843
Absolute percentage error: 0.09
</bodyText>
<sectionHeader confidence="0.951439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99938380952381">
In this paper we study the identification
and verification of simple claims about
statistical properties, e.g. claims about the
population or the inflation rate of a coun-
try. We show that this problem is similar
to extracting numerical information from
text and following recent work, instead of
annotating data for each property of inter-
est in order to learn supervised models,
we develop a distantly supervised base-
line approach using a knowledge base and
raw text. In experiments on 16 statistical
properties about countries from Freebase
we show that our approach identifies sim-
ple statistical claims about properties with
60% precision, while it is able to verify
these claims without requiring any explicit
supervision for either tasks. Furthermore,
we evaluate our approach as a statistical
property extractor and we show it achieves
0.11 mean absolute percentage error.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999562411764706">
Statistical properties are commonly used to de-
scribe entities, e.g. population for countries,
net value for companies, points scored for ath-
letes, etc. Claims about such properties are very
common in news articles and social media, how-
ever they can be erroneous, either due to au-
thor error or negligence at the time of writing
or because they eventually become out of date.
While manual verification (also referred to as fact-
checking) is conducted by journalists in news or-
ganizations and dedicated websites such as www.
emergent.info, the volume of the claims calls
for automated approaches, which is one of the
main objectives of computational journalism (Co-
hen et al., 2011; Flew et al., 2012).
In this paper we develop a baseline approach to
identify and verify simple claims about statistical
</bodyText>
<figureCaption confidence="0.998661">
Figure 1: Claim identification and verification.
</figureCaption>
<bodyText confidence="0.999952892857143">
properties against a database. The task is illus-
trated in Figure 1. Given a sentence, we first iden-
tify whether it contains a claim about a property
we are interested in (population in the exam-
ple), which entity it is about and the value claimed
(Lesotho and 2,000,000 respectively). We then
proceed to verify the value claimed in text for the
property of this entity against the value known in
a knowledge base such as Freebase and return a
score reflecting the accuracy of the claim (abso-
lute percentage error in the example).
Claim identification is essentially an instance of
information extraction. While it would be possi-
ble to develop supervised models, this would re-
quire expensive manual data annotation for each
property of interest. Instead, we follow the dis-
tant supervision paradigm (Craven and Kumlien,
1999; Mintz et al., 2009) using supervision ob-
tained by combining triples from a knowledge
base and raw text. However, statistical properties
are more challenging in applying the distant super-
vision assumption than relations between named
entities due to the fact that the numerical values
are often approximated in text, as in the example
of Figure 1. Consequently, linking the values men-
tioned in text with those in the knowledge base is
not trivial and thus it is not straightforward to gen-
erate training instances for the property of interest.
</bodyText>
<page confidence="0.859049">
2596
</page>
<note confidence="0.63596">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2596–2601,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999649666666667">
To address this issue, we propose a distantly
supervised claim identification approach that re-
lies on approximate instead of exact matching
between values in text and the knowledge base.
In experiments on 16 statistical properties about
countries from Freebase we show that our ap-
proach identifies simple statistical claims with
60% precision, while it is able to verify these
claims without requiring any explicit supervision
for this task. In developing our approach, we
also evaluate it as a statistical property extrac-
tor achieving 0.11 mean absolute percentage er-
ror. The code and the datasets developed are pub-
licly available from https://github.com/
uclmr/simpleNumericalFactChecker.
</bodyText>
<sectionHeader confidence="0.939783" genericHeader="method">
2 Claim identification algorithm
</sectionHeader>
<bodyText confidence="0.999988171052632">
Our approach to claim identification relies on dis-
covering textual patterns between an entity and a
numerical value used to express the property of in-
terest. For example, the first, second and fourth
patterns in Table 1 express the population prop-
erty, and we would like our approach to select
them to identify claims about this property.
During training, we assume as input a set of
entity-value pairs from the knowledge base for
the property of interest (top-right part of Table 1)
and a set of textual patterns (bottom-left part),
each associated with entity-value pairs (bottom-
right part). The patterns and the entity-value pairs
associated with them are obtained by processing
raw text, which we discuss in Section 3.
The key difficulty compared to other applica-
tions of distant supervision is that numerical val-
ues are often approximated in text. Thus, instead
of looking for patterns that report the exact val-
ues for each entity, we develop an approach for
finding the patterns that predict the values well.
Intuitively, the algorithm ranks all text patterns ac-
cording to how well they predict the values for the
property at question and then greedily selects them
till the accuracy of the aggregate predictions by the
selected paterns stop improving. To compare the
predicted entity-value pairs ˆEV against the prop-
erty entity-value pairs EVprop we use the mean ab-
solute percentage error (MAPE):
Note that only the values predicted for entities in
both EVprop and ˆEV (denoted by E&apos;) are taken
into account in this equation, thus in calculat-
ing MAPE for the pattern “X has inhabitants”
from Table 1 against the entity-values for popu-
lation only the two values present are considered.
MAPE is commonly used in measuring forecast-
ing accuracy of algorithms in finance (Hyndman
and Koehler, 2006). Unlike mean absolute error
or mean squared error it adjusts the errors accord-
ing to the magnitude of the correct value.
Initially (line 1) the algorithm decides on a de-
fault value (vdef) to return for the property at ques-
tion among three options: the mean of the training
values, their median or zero. The criterion for the
choice is which one results in a better MAPE score
on the training data. We refer to this prediction as
the InformedGuess. This default value is used
when predicting (lines 16-29) in case there are no
values for an entity in the patterns selected, e.g. if
only the pattern “X has inhabitants” is selected
and the prediction for Iceland is requested.
Following this, the patterns are ranked accord-
ing to the MAPE score of their entity values with
respect to the entity values of the property at ques-
tion (lines 2-4). We then iterate over the patterns in
the order they were ranked. For every pattern, we
add it to the set of patterns used in predicting (lines
8-9), and evaluate the resulting predictions using
MAPE with respect to the training values (line 10).
If MAPE is increased (predictions become worse),
then we remove the newly added pattern from the
set and stop. Otherwise, we continue with the next
pattern in the queue.
In experiments with this algorithm we found
that while it often identified useful patterns, some-
times it was misled by patterns that had very
few entities in common with the property and the
values of those entities happen to be similar to
those of the property. For example, the pattern “
tourists visited X” in Figure 1 has only one entity-
value pair (“France:68,000,000”) and the value is
very close to the population value for “France”.
To ameliorate this issue, we adjusted the MAPE
scores used in the ranking step (line 4) according
to the number of values used in the calculation us-
ing the following formula:
</bodyText>
<equation confidence="0.976699">
c
adjustedMAPE = c + N MAPE (2)
</equation>
<bodyText confidence="0.9999305">
where N is the number of values used in calcu-
lating MAPE in Equation 1 and c is a parame-
ter that regulates the adjustment. Lower c puts
more importance on the number of values used,
</bodyText>
<equation confidence="0.586415333333333">
MAPE(EVPTOP, � |ve − ˆve |(1)
1 |ve|
ˆEV ) = |E |e∈E&apos;
</equation>
<page confidence="0.734025">
2597
</page>
<bodyText confidence="0.915048">
population
the population of X is
X’s population is estimated at
X’s inflation rate is
X has inhabitants
</bodyText>
<equation confidence="0.795585857142857">
France:66,028,467, Russia:143,700,000, Iceland:325,600
France:66,000,000, Russia:140,000,000, Iceland:325,000
France:66,030,000, Russia:145,000,000
France:0.9, Iceland:4.0
Russia:140,000,000, Iceland:300,000
tourists visited X
France:68,000,000
</equation>
<tableCaption confidence="0.7380985">
Table 1: Property and text patterns associated with entity-value pairs.
Algorithm 1: Claim identification algorithm
</tableCaption>
<bodyText confidence="0.781214">
thus leading the algorithm to choosing patterns as-
sessed with more values, and thus more reliably.
</bodyText>
<figure confidence="0.995755">
Input: Entity-values for property 3 Data collection
EVprop = {(e1, v1), (e2, v2), ...},
patterns P = {p1, p2, ...},
entity-values for pattern p: EVp
Output: Selected patterns Psel
1 vdef = InformedGuess (EVprop)
2 priorityQueue Q = 0
3 foreach pattern p E P do
4 push (Q, (p, MAPE (EVprop, EVp)))
5 Psel = 0
6 mp = MAPE (EVprop, predict(E, Psel))
7 while Q =� 0 do
pattern p = pop (Q)
P0sel = Psel U {p}
mp0 = MAPE (EVprop, predict
(E, P0sel))
if mp0 &gt; mp then
break
else
mp = mp0
Psel = P0sel
8
9
10
11
12
13
14
15
16 function predict(entities E, patterns
</figure>
<equation confidence="0.917919428571428">
Psel)
ˆEV = 0
foreach e E E do
sum = 0
count = 0
foreach p E Psel do
if (e, v) E EVp then
sum+ = p{e}
count+ = 1
if count &gt; 0 then
ˆEV = ˆEV U (e, sum/count)
else
ˆEV = ˆEV U (e, vdef)
return ˆEV
</equation>
<page confidence="0.538114">
17
</page>
<figure confidence="0.903326272727273">
18
19
20
21
22
23
24
25
26
27
28
</figure>
<page confidence="0.996888">
29
</page>
<bodyText confidence="0.99957672972973">
To evaluate the claim identification approach de-
veloped we compiled a dataset of statistical prop-
erties from Freebase. We downloaded a snapshot1
of all instances of the statistical region entity type
with all their properties with their most recent val-
ues, keeping only those were from 2010 onwards.
From those we selected the 16 properties listed in
Table 2, each property having values for 150-175
regions (mostly countries).
To collect texts from which the text patterns be-
tween entities and numerical values will be ex-
tracted we downloaded documents from the web.
In particular, for each region combined with each
property we formed a query consisting of the two
and submitted it to Bing via its Search API. Fol-
lowing this we obtained the top 50 results for each
query, downloaded the HTML pages correspond-
ing to each result and extracted their textual con-
tent with BoilerPipe (Kohlsch¨utter et al., 2010).
We then processed the texts using the Stanford
CoreNLP toolkit (Manning et al., 2014) and from
each sentence we extracted textual patterns be-
tween all the named entities recognized as loca-
tions and all the numerical values. Two kinds of
patterns were extracted for each location and nu-
merical value: surface patterns (as the ones shown
in Table 1) and lexicalized dependency paths.
This pattern extraction process resulted in a
large set of triples consisting of a region, a pattern
and a value. Different sentences might result in
triples containing the same region and textual pat-
tern but different value. Such variation can arise
due to either the approximations of values in text
or due to the pattern being highly ambiguous, e.g.
“X is ”. We distinguish between the two by re-
quiring each region-pattern combination to have
appeared at least twice and its values to have stan-
</bodyText>
<footnote confidence="0.975895">
1Data was collected in May 2014.
</footnote>
<page confidence="0.994334">
2598
</page>
<bodyText confidence="0.992639">
dard deviation less than 0.1. In this case, then the
region-pattern value is set to the mean of the val-
ues it is encountered with, otherwise is removed.
</bodyText>
<sectionHeader confidence="0.971188" genericHeader="method">
4 Information extraction experiments
</sectionHeader>
<bodyText confidence="0.999980375">
We first evaluate our approach as a statistical prop-
erty extractor for two reasons. First, while our
main goal is to develop a claim identification ap-
proach, there is no data for this task to evaluate,
thus making development difficult. On the other
hand, we can evaluate statistical property extrac-
tion in a straightforward way, thus facilitating de-
velopment and parameter tuning. Second, the al-
gorithm described learns such an extractor, thus it
is of interest to know its performance.
We split the values collected from Freebase into
2/3 for training and 1/3 for testing, ensuring that
all regions are present in both datasets. The ac-
curacy is evaluated using MAPE. When using ad-
justed MAPE we set the parameter c for each prop-
erty using 4-fold cross-validation.
The performance of Algorithm 1 using the
unadjusted MAPE was 0.72 averaged over all
properties. Using the adjusted version this was
greatly improved to 0.49. We also evaluated the
InformedGuess prediction which returns the
same value for all regions (it chooses the value
that performs best among the mean, the median
and 0), and its overall MAPE was 0.79. Recalling
that Algorithm 1 returns the InformedGuess
in case no pattern is found for an entity, we also
evaluate the performance without returning a value
for such entities, thus ignoring them in the evalua-
tion. In that case the performance with unadjusted
MAPE improves to 0.17 but 10% coverage, while
with adjusted MAPE it improves to 0.11 with 43%
coverage. Best performances were achieved for
relations such as population which have a wide
range of values that is well separated from the rest,
while percentage rates were usually harder for the
opposite reason. Thus we conclude that the algo-
rithm with adjusted MAPE selects better patterns
for each property that are encountered more fre-
quently, which is important for the main goal of
this paper, claim identification.
</bodyText>
<sectionHeader confidence="0.944184" genericHeader="method">
5 Claim identification and verification
</sectionHeader>
<bodyText confidence="0.594063523809524">
We now evaluate our approach on claim identifica-
tion. For each property, we run Algorithm 1 using
adjusted MAPE and the parameter c as chosen in
the experiments of the previous section to select
Freebase property
consumer price index
cpi inflation rate
diesel price liter
fertility rate
gdp growth rate
gdp nominal
gdp nominal per capita
gni
gni per capita
health expenditure
internet users %
life expectancy
population
population growth rate
renewable freshwater
undernourishment
</bodyText>
<sectionHeader confidence="0.273594" genericHeader="method">
OVERALL
</sectionHeader>
<tableCaption confidence="0.984426">
Table 2: Claim identification results.
</tableCaption>
<bodyText confidence="0.999481620689655">
patterns expressing it. We then process all texts
and if a sentence contains one of the selected pat-
terns between an entity and a value, it is returned
for manual inspection as shown in Figure 1.
The claims returned were labeled by the authors
of the paper as correctly or incorrectly identified
according to the following guidelines. A claim is
extracted correctly only if both the entity and the
value are extracted correctly and the sentence ex-
presses the property at question. E.g. a claim iden-
tified in a sentence containing a country and its
correct GDP growth rate without stating it as such
(the same percentage rate can be true for multiple
statistical properties) is considered incorrect. Fur-
thermore, we considered claims referring to past
measurements (e.g. results of a past census) to be
correctly identified.
Results for each property are shown in Table. 2.
Overall precision was 60% over 7,092 statements,
and it varied substantially across properties. Per-
fect precision was found for claims of renew-
able freshwater for which one textual pattern was
responsible for all the claims identified and it was
correct. On the other hand, the zero precision for
claims of internet user % was due to identifying
correctly sentences listing countries and their re-
spective values for this property but not identify-
ing the country-value pairs correctly. More repre-
sentative of properties with precise claim identifi-
</bodyText>
<table confidence="0.620137277777778">
claims precision
116 0.93
464 0.92
212 1.00
307 0.99
39 0.31
308 0.98
415 0.20
413 0.62
795 0.49
197 0.99
93 0.00
581 0.45
1583 0.9
1377 0.11
105 1.00
87 0.13
7092 0.60
</table>
<page confidence="0.995428">
2599
</page>
<bodyText confidence="0.999943333333333">
cation was population, for which the relatively few
errors were due to the patterns learned not being
able to distinguish between different types of pop-
ulation e.g. general vs working population. On the
other hand, the claims for gni per capita had low
accuracy because they were confused with those
of gdp nominal per capita, as their values tend to
be relatively close. The claims identified and an-
notated manually are attached to our submission.
Finally, some errors are due to the algorithm being
constrained to extract a claim considering only the
text pattern between the entity and the value, thus
ignoring parts of the sentence that might be rele-
vant. For example, the pattern “the population of
X is ” is generally reliable, but in the sentence
“The population of Tajikistan is 90 % Muslim” it
extracts a claim incorrectly.
The verification stage of the simple claims we
extract is rather simple; we just score the claims
according to the absolute percentage error of the
value claimed in text with respect to the value
in known in Freebase. In the process of la-
beling the claims identified we did a qualita-
tive analysis of the claims with high error. We
found cases where our algorithm flagged cases
of out of date estimates of populations used, e.g.
the webpage http://www.economywatch.
com/world_economy/bolivia2 states that
the population of Bolivia is 9 million, while it is
estimated to be above 10 million.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="method">
6 Discussion - Related work
</sectionHeader>
<bodyText confidence="0.998988444444444">
As explained, we tackle claim identification as an
instance of information extraction, and propose a
baseline able to perform both tasks. However, it is
important to distinguish between them. In claim
identification we are interested in all claims about
a property, even inaccurate ones; in information
extraction on the other hand, and especially its for-
mulation as knowledge base population, we are in-
terested in the accurate claims only, since extract-
ing inaccurate ones will lead to erroneous informa-
tion added to the knowledge base. The difference
between the two tasks is captured by the verifica-
tion task. In this paper our main goals are identifi-
cation and verification, but we train our approach
on information extraction, relying on the assump-
tion that most claims made in the texts retrieved
via the web search engine are accurate.
In related work, Nakashole and Mitchell (2014)
</bodyText>
<footnote confidence="0.65617">
2Accessed in August 2015.
</footnote>
<bodyText confidence="0.99991115">
developed an approach to verify subject-verb-
object triples against a knowledge base, taking into
account the objectivity of the language used in the
sources stating the triple. Our approach is ag-
nostic to the syntactic form of the claims, thus it
can identify claims expressed in greater linguis-
tic variety. Ciampaglia et al. (2015) fact-checked
subject-predicate-object triples against a knowl-
edge graph constructed from DBpedia, but they
considered only the paths between the subject and
the predicate in their algorithm thus ignoring the
predicate itself. Dong et al. (2015) established
the trustworthiness of a web source by comparing
the subject-predicate-object triples extracted from
it to the Knowledge Vault built by Google, but did
not focus on claim identification and verification.
Adar et al. (2009) developed an approach to detect
inconsistencies between versions of Wikipedia in
different languages, but they focused on manually
extracted infoboxes. Finally, Vlachos and Riedel
(2014) compiled a dataset of claims fact-checked
by journalists, but the claims are much more com-
plex than the ones we considered in this paper.
Other work that discussed the extraction of
statistical properties includes the approaches of
Hoffmann et al. (2010) and Intxaurrondo et al.
(2015), both employing approximate matching to
deal with the approximation of numerical values
in text. In order to learn their model, Hoffmann
et al. (2010) take advantage of the structure of
the articles in Wikipedia developing a classifier
that identifies the schema followed by each arti-
cle, which is not straightforward to extend to texts
beyond this source. Intxaurrondo et al. (2015) on
the other hand focus on tweets and make the as-
sumption that the entity discussed in each tweet
is determined in advance, thus the extractor needs
only to associate a numerical value with the prop-
erty of interest, i.e. the task is reduced from triple
extraction to labeling values.
</bodyText>
<sectionHeader confidence="0.99865" genericHeader="conclusions">
7 Conclusions - Future work
</sectionHeader>
<bodyText confidence="0.998777111111111">
In this paper we developed a distantly supervised
approach for identification and verification of sim-
ple statistical claims. We evaluated both as statis-
tical property extractor and as a claim identifier on
16 relations from Freebase. In future work we aim
to improve our approach by taking into account
continuous representations of the words in the pat-
terns and to extend it to more complex claims, e.g.
claims about change in financial indicators.
</bodyText>
<page confidence="0.972935">
2600
</page>
<sectionHeader confidence="0.995822" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897972222222">
Eytan Adar, Michael Skinner, and Daniel S. Weld.
2009. Information arbitrage across multi-lingual
wikipedia. In Proceedings of the Second ACM Inter-
national Conference on Web Search and Data Min-
ing, pages 94–103.
Giovanni Luca Ciampaglia, Prashant Shiralkar,
Luis M. Rocha, Johan Bollen, Filippo Menczer, and
Alessandro Flammini. 2015. Computational fact
checking from knowledge networks. PLoS ONE,
10(6):e0128193, 06.
Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu.
2011. Computational journalism: A call to arms to
database researchers. In Proceedings of the Confer-
ence on Innovative Data Systems Research, volume
2011, pages 148–151.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge-bases by extracting informa-
tion from text sources. In Proceedings of the 7th
International Conference on Intelligent Systems for
Molecular Biology, pages 77–86.
Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy,
Van Dang, Wilko Horn, Camillo Lugaresi, Shao-
hua Sun, and Wei Zhang. 2015. Knowledge-based
trust: Estimating the trustworthiness of web sources.
pages 938–949.
Terry Flew, Anna Daniel, and Christina L. Spurgeon.
2012. The promise of computational journalism.
In Proceedings of the Australian and New Zealand
Communication Association Conference, pages 1–
19.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, page 286295.
Rob J. Hyndman and Anne B. Koehler. 2006. Another
look at measures of forecast accuracy. International
Journal of Forecasting, 22(4):679 – 688.
Ander Intxaurrondo, Eneko Agirre, Oier Lopez de La-
calle, and Mihai Surdeanu. 2015. Diamonds in the
rough: Event extraction from imperfect microblog
data. In Proceedings of the 2015 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics.
Christian Kohlsch¨utter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441–450.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011.
Ndapandula Nakashole and Tom M Mitchell. 2014.
Languageaware truth assessment of fact candidates.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (ACL).
Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construction.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technology and Computational Social Sci-
ence, July.
</reference>
<page confidence="0.988069">
2601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.389776">
<title confidence="0.9913285">Identification and Verification of Simple about Statistical Properties</title>
<author confidence="0.849939">Vlachos</author>
<affiliation confidence="0.873933">Department of Computer University College Lesotho, a landlocked enclave of</affiliation>
<abstract confidence="0.986118821428571">South Africa, has a population of nearly 2 million and covers an area slightly smaller than the U.S. state of Maryland. claimed in text: in knowledge base: percentage error: Abstract In this paper we study the identification and verification of simple claims about statistical properties, e.g. claims about the the rate a country. We show that this problem is similar to extracting numerical information from text and following recent work, instead of annotating data for each property of interest in order to learn supervised models, we develop a distantly supervised baseline approach using a knowledge base and raw text. In experiments on 16 statistical properties about countries from Freebase we show that our approach identifies simple statistical claims about properties with 60% precision, while it is able to verify these claims without requiring any explicit supervision for either tasks. Furthermore, we evaluate our approach as a statistical property extractor and we show it achieves 0.11 mean absolute percentage error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Michael Skinner</author>
<author>Daniel S Weld</author>
</authors>
<title>Information arbitrage across multi-lingual wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>94--103</pages>
<contexts>
<context position="19099" citStr="Adar et al. (2009)" startWordPosition="3145" endWordPosition="3148">gnostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguistic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more complex than the ones we considered in this paper. Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model</context>
</contexts>
<marker>Adar, Skinner, Weld, 2009</marker>
<rawString>Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009. Information arbitrage across multi-lingual wikipedia. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 94–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Luca Ciampaglia</author>
<author>Prashant Shiralkar</author>
<author>Luis M Rocha</author>
<author>Johan Bollen</author>
<author>Filippo Menczer</author>
<author>Alessandro Flammini</author>
</authors>
<title>Computational fact checking from knowledge networks.</title>
<date>2015</date>
<journal>PLoS ONE,</journal>
<volume>10</volume>
<issue>6</issue>
<pages>06</pages>
<contexts>
<context position="18619" citStr="Ciampaglia et al. (2015)" startWordPosition="3075" endWordPosition="3078">er our main goals are identification and verification, but we train our approach on information extraction, relying on the assumption that most claims made in the texts retrieved via the web search engine are accurate. In related work, Nakashole and Mitchell (2014) 2Accessed in August 2015. developed an approach to verify subject-verbobject triples against a knowledge base, taking into account the objectivity of the language used in the sources stating the triple. Our approach is agnostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguistic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused </context>
</contexts>
<marker>Ciampaglia, Shiralkar, Rocha, Bollen, Menczer, Flammini, 2015</marker>
<rawString>Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M. Rocha, Johan Bollen, Filippo Menczer, and Alessandro Flammini. 2015. Computational fact checking from knowledge networks. PLoS ONE, 10(6):e0128193, 06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Cohen</author>
<author>Chengkai Li</author>
<author>Jun Yang</author>
<author>Cong Yu</author>
</authors>
<title>Computational journalism: A call to arms to database researchers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Innovative Data Systems Research,</booktitle>
<volume>volume</volume>
<pages>148--151</pages>
<contexts>
<context position="2080" citStr="Cohen et al., 2011" startWordPosition="313" endWordPosition="317">ies, e.g. population for countries, net value for companies, points scored for athletes, etc. Claims about such properties are very common in news articles and social media, however they can be erroneous, either due to author error or negligence at the time of writing or because they eventually become out of date. While manual verification (also referred to as factchecking) is conducted by journalists in news organizations and dedicated websites such as www. emergent.info, the volume of the claims calls for automated approaches, which is one of the main objectives of computational journalism (Cohen et al., 2011; Flew et al., 2012). In this paper we develop a baseline approach to identify and verify simple claims about statistical Figure 1: Claim identification and verification. properties against a database. The task is illustrated in Figure 1. Given a sentence, we first identify whether it contains a claim about a property we are interested in (population in the example), which entity it is about and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase </context>
</contexts>
<marker>Cohen, Li, Yang, Yu, 2011</marker>
<rawString>Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu. 2011. Computational journalism: A call to arms to database researchers. In Proceedings of the Conference on Innovative Data Systems Research, volume 2011, pages 148–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge-bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="3072" citStr="Craven and Kumlien, 1999" startWordPosition="477" endWordPosition="480">, which entity it is about and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score reflecting the accuracy of the claim (absolute percentage error in the example). Claim identification is essentially an instance of information extraction. While it would be possible to develop supervised models, this would require expensive manual data annotation for each property of interest. Instead, we follow the distant supervision paradigm (Craven and Kumlien, 1999; Mintz et al., 2009) using supervision obtained by combining triples from a knowledge base and raw text. However, statistical properties are more challenging in applying the distant supervision assumption than relations between named entities due to the fact that the numerical values are often approximated in text, as in the example of Figure 1. Consequently, linking the values mentioned in text with those in the knowledge base is not trivial and thus it is not straightforward to generate training instances for the property of interest. 2596 Proceedings of the 2015 Conference on Empirical Met</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge-bases by extracting information from text sources. In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology, pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Luna Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Kevin Murphy</author>
<author>Wilko Horn Van Dang</author>
<author>Camillo Lugaresi</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge-based trust: Estimating the trustworthiness of web sources.</title>
<date>2015</date>
<pages>938--949</pages>
<marker>Dong, Gabrilovich, Murphy, Van Dang, Lugaresi, Sun, Zhang, 2015</marker>
<rawString>Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua Sun, and Wei Zhang. 2015. Knowledge-based trust: Estimating the trustworthiness of web sources. pages 938–949.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Flew</author>
<author>Anna Daniel</author>
<author>Christina L Spurgeon</author>
</authors>
<title>The promise of computational journalism.</title>
<date>2012</date>
<booktitle>In Proceedings of the Australian and New Zealand Communication Association Conference,</booktitle>
<pages>1--19</pages>
<contexts>
<context position="2100" citStr="Flew et al., 2012" startWordPosition="318" endWordPosition="321"> for countries, net value for companies, points scored for athletes, etc. Claims about such properties are very common in news articles and social media, however they can be erroneous, either due to author error or negligence at the time of writing or because they eventually become out of date. While manual verification (also referred to as factchecking) is conducted by journalists in news organizations and dedicated websites such as www. emergent.info, the volume of the claims calls for automated approaches, which is one of the main objectives of computational journalism (Cohen et al., 2011; Flew et al., 2012). In this paper we develop a baseline approach to identify and verify simple claims about statistical Figure 1: Claim identification and verification. properties against a database. The task is illustrated in Figure 1. Given a sentence, we first identify whether it contains a claim about a property we are interested in (population in the example), which entity it is about and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score r</context>
</contexts>
<marker>Flew, Daniel, Spurgeon, 2012</marker>
<rawString>Terry Flew, Anna Daniel, and Christina L. Spurgeon. 2012. The promise of computational journalism. In Proceedings of the Australian and New Zealand Communication Association Conference, pages 1– 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286295</pages>
<contexts>
<context position="19541" citStr="Hoffmann et al. (2010)" startWordPosition="3211" endWordPosition="3214">paring the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more complex than the ones we considered in this paper. Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model, Hoffmann et al. (2010) take advantage of the structure of the articles in Wikipedia developing a classifier that identifies the schema followed by each article, which is not straightforward to extend to texts beyond this source. Intxaurrondo et al. (2015) on the other hand focus on tweets and make the assumption that the entity discussed in each tweet is determined in advance, thus the extractor needs only to associate a numerical value</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page 286295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob J Hyndman</author>
<author>Anne B Koehler</author>
</authors>
<title>Another look at measures of forecast accuracy.</title>
<date>2006</date>
<journal>International Journal of Forecasting,</journal>
<volume>22</volume>
<issue>4</issue>
<pages>688</pages>
<contexts>
<context position="6340" citStr="Hyndman and Koehler, 2006" startWordPosition="994" endWordPosition="997">ll the accuracy of the aggregate predictions by the selected paterns stop improving. To compare the predicted entity-value pairs ˆEV against the property entity-value pairs EVprop we use the mean absolute percentage error (MAPE): Note that only the values predicted for entities in both EVprop and ˆEV (denoted by E&apos;) are taken into account in this equation, thus in calculating MAPE for the pattern “X has inhabitants” from Table 1 against the entity-values for population only the two values present are considered. MAPE is commonly used in measuring forecasting accuracy of algorithms in finance (Hyndman and Koehler, 2006). Unlike mean absolute error or mean squared error it adjusts the errors according to the magnitude of the correct value. Initially (line 1) the algorithm decides on a default value (vdef) to return for the property at question among three options: the mean of the training values, their median or zero. The criterion for the choice is which one results in a better MAPE score on the training data. We refer to this prediction as the InformedGuess. This default value is used when predicting (lines 16-29) in case there are no values for an entity in the patterns selected, e.g. if only the pattern “</context>
</contexts>
<marker>Hyndman, Koehler, 2006</marker>
<rawString>Rob J. Hyndman and Anne B. Koehler. 2006. Another look at measures of forecast accuracy. International Journal of Forecasting, 22(4):679 – 688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ander Intxaurrondo</author>
</authors>
<title>Eneko Agirre, Oier Lopez de Lacalle, and Mihai Surdeanu.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<marker>Intxaurrondo, 2015</marker>
<rawString>Ander Intxaurrondo, Eneko Agirre, Oier Lopez de Lacalle, and Mihai Surdeanu. 2015. Diamonds in the rough: Event extraction from imperfect microblog data. In Proceedings of the 2015 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlsch¨utter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>441--450</pages>
<marker>Kohlsch¨utter, Fankhauser, Nejdl, 2010</marker>
<rawString>Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, pages 441–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="10839" citStr="Manning et al., 2014" startWordPosition="1788" endWordPosition="1791">aving values for 150-175 regions (mostly countries). To collect texts from which the text patterns between entities and numerical values will be extracted we downloaded documents from the web. In particular, for each region combined with each property we formed a query consisting of the two and submitted it to Bing via its Search API. Following this we obtained the top 50 results for each query, downloaded the HTML pages corresponding to each result and extracted their textual content with BoilerPipe (Kohlsch¨utter et al., 2010). We then processed the texts using the Stanford CoreNLP toolkit (Manning et al., 2014) and from each sentence we extracted textual patterns between all the named entities recognized as locations and all the numerical values. Two kinds of patterns were extracted for each location and numerical value: surface patterns (as the ones shown in Table 1) and lexicalized dependency paths. This pattern extraction process resulted in a large set of triples consisting of a region, a pattern and a value. Different sentences might result in triples containing the same region and textual pattern but different value. Such variation can arise due to either the approximations of values in text o</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3093" citStr="Mintz et al., 2009" startWordPosition="481" endWordPosition="484"> and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score reflecting the accuracy of the claim (absolute percentage error in the example). Claim identification is essentially an instance of information extraction. While it would be possible to develop supervised models, this would require expensive manual data annotation for each property of interest. Instead, we follow the distant supervision paradigm (Craven and Kumlien, 1999; Mintz et al., 2009) using supervision obtained by combining triples from a knowledge base and raw text. However, statistical properties are more challenging in applying the distant supervision assumption than relations between named entities due to the fact that the numerical values are often approximated in text, as in the example of Figure 1. Consequently, linking the values mentioned in text with those in the knowledge base is not trivial and thus it is not straightforward to generate training instances for the property of interest. 2596 Proceedings of the 2015 Conference on Empirical Methods in Natural Langu</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Tom M Mitchell</author>
</authors>
<title>Languageaware truth assessment of fact candidates.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="18260" citStr="Nakashole and Mitchell (2014)" startWordPosition="3017" endWordPosition="3020">erty, even inaccurate ones; in information extraction on the other hand, and especially its formulation as knowledge base population, we are interested in the accurate claims only, since extracting inaccurate ones will lead to erroneous information added to the knowledge base. The difference between the two tasks is captured by the verification task. In this paper our main goals are identification and verification, but we train our approach on information extraction, relying on the assumption that most claims made in the texts retrieved via the web search engine are accurate. In related work, Nakashole and Mitchell (2014) 2Accessed in August 2015. developed an approach to verify subject-verbobject triples against a knowledge base, taking into account the objectivity of the language used in the sources stating the triple. Our approach is agnostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguistic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2</context>
</contexts>
<marker>Nakashole, Mitchell, 2014</marker>
<rawString>Ndapandula Nakashole and Tom M Mitchell. 2014. Languageaware truth assessment of fact candidates. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Sebastian Riedel</author>
</authors>
<title>Fact checking: Task definition and dataset construction.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Language Technology and Computational Social Science,</booktitle>
<contexts>
<context position="19286" citStr="Vlachos and Riedel (2014)" startWordPosition="3170" endWordPosition="3173">ples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more complex than the ones we considered in this paper. Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model, Hoffmann et al. (2010) take advantage of the structure of the articles in Wikipedia developing a classifier that identifies the schema followed by each article, which is not straightfor</context>
</contexts>
<marker>Vlachos, Riedel, 2014</marker>
<rawString>Andreas Vlachos and Sebastian Riedel. 2014. Fact checking: Task definition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technology and Computational Social Science, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
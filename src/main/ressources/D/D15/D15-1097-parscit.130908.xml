<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.9996795">
An Unsupervised Method for Discovering Lexical Variations
in Roman Urdu Informal Text
</title>
<author confidence="0.990239">
Abdul Rafae, $Abdul Qayyum, $Muhammad Moeenuddin,
Asim Karim, †Hassan Sajjad and $Faisal Kamiran,
</author>
<affiliation confidence="0.9967355">
†Qatar Computing Research Institute, Hamad Bin Khalifa University
Lahore University of Management Sciences, $Information Technology University
</affiliation>
<sectionHeader confidence="0.978802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966263157895">
We present an unsupervised method to
find lexical variations in Roman Urdu
informal text. Our method includes a
phonetic algorithm UrduPhone, a feature-
based similarity function, and a clustering
algorithm Lex-C. UrduPhone encodes ro-
man Urdu strings to their phonetic equiv-
alent representations. This produces an
initial grouping of different spelling vari-
ations of a word. The similarity function
incorporates word features and their con-
text. Lex-C is a variant of k-medoids clus-
tering algorithm that group lexical varia-
tions. It incorporates a similarity thresh-
old to balance the number of clusters and
their maximum similarity. We test our sys-
tem on two datasets of SMS and blogs and
show an f-measure gain of up to 12% from
baseline systems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995432">
Urdu is the national language of Pakistan and one
of the official languages of India. It is written in
Perso-Arabic script. However in social media and
short text messages (SMS), a large proportion of
Urdu speakers use roman script (i.e., the English
alphabet) for writing, called Roman Urdu.
Roman Urdu lacks standard lexicon and usu-
ally many spelling variations exist for a given
word, e.g., the word zindagi [life] is also written as
zindagee, zindagy, zaindagee and zndagi. Specifi-
cally, the following normalization issues arise: (1)
differently spelled words (see example above), (2)
identically spelled words that are lexically differ-
ent (e.g., bahar can be used for both [outside]
and [spring], and (3) spellings that match words
in English (e.g, had [limit] for the English word
‘had’). These inconsistencies cause a problem of
data sparsity in basic natural language processing
tasks such as Urdu word segmentation (Durrani
and Hussain, 2010), part of speech tagging (Saj-
jad and Schmid, 2009), spell checking (Naseem
and Hussain, 2007), machine translation (Durrani
et al., 2010), etc.
In this paper, we propose an unsupervised
feature-based method that tackles above men-
tioned challenges in discovering lexical variations
in Roman Urdu. We exploit phonetic and string
similarity based features and incorporate contex-
tual features via top-k previous and next words’
features. For phonetic information, we develop an
encoding scheme for Roman Urdu, UrduPhone,
motivated from Soundex. Compared to other
available phonetic-based schemes that are mostly
limited to English sounds only, UrduPhone maps
Roman Urdu homophones effectively. Unlike pre-
vious work on short text normalization (see Sec-
tion 2), we do not have information about stan-
dard word forms in the dataset. The problem be-
comes more challenging as every word in the cor-
pus is a candidate of every other word. We present
a variant of the k-medoids clustering algorithm
that forms clusters in which every word has at
least a specified minimum similarity with the clus-
ter’s centroidal word. We conduct experiments on
two Roman Urdu datasets: an SMS dataset and
a blog dataset and evaluate performance using a
gold standard. Our method shows an f-measure
gain of up to 12% compared to baseline methods.
The dataset and code are made available to the re-
search community.
</bodyText>
<sectionHeader confidence="0.99553" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.994693428571429">
Normalization of short text messages and tweets
has been in focus (Sproat et al., 2001; Wei et al.,
2011; Clark and Araki, 2011; Roy et al., 2013;
Chrupala, 2014; Kaufmann and Kalita, 2010;
Sidarenka et al., 2013; Ling et al., 2013; Desai
and Narvekar, 2015; Pinto et al., 2012). However,
most of the work is limited to English or to other
</bodyText>
<page confidence="0.985586">
823
</page>
<note confidence="0.658273">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997794516129032">
resource-rich languages. In this paper, we focus on
Roman Urdu, an under-resourced language, that
does not have any gold standard corpus with stan-
dard word forms. Therefore, we are restricted to
the task of finding lexical variations in informal
text. This is a rather more challenging problem
since in this case every word is a possible varia-
tion of every other word in the corpus.
Researchers have used phonetic, string, and
contextual knowledge to find lexical variations in
informal text.1 Pinto et al. (2012; Han et al.
(2012; Zhang et al. (2015) used phonetic-based
methods to find lexical variations. Han et al.
(2012) also used word similarity and word con-
text to enhance performance. Wang and Ng (2013)
used normalization operations e.g., missing word
recovery and punctuation correction to improve
normalization process. Irvine et al. (2012) used
manually prepared training data to build an au-
tomatic normalization system. Contractor et al.
(2010) used string edit distance to find candidate
lexical variations. Yang and Eisenstein (2013)
used an unsupervised approach with log linear
model and sequential Monte Carlo approximation.
We propose an unsupervised method to find lex-
ical variations. It uses string edit distance like
Contractor et al. (2010), Sound-based encoding
like Pinto et al. (2012) and context like Han et al.
(2012) combined in a discriminative framework.
However, in contrast, it does not use any corpus of
standard word forms to find lexical variations.
</bodyText>
<sectionHeader confidence="0.994266" genericHeader="method">
3 Our Method
</sectionHeader>
<bodyText confidence="0.9999484">
The lexical variations of a lexical entry usually
have high phonetic, string-based, and contextual
similarity. We integrate a phonetic-based encod-
ing scheme, UrduPhone, a feature-based similarity
function, and a clustering algorithm, Lex-C.
</bodyText>
<subsectionHeader confidence="0.997007">
3.1 UrduPhone
</subsectionHeader>
<bodyText confidence="0.998530333333333">
Several sound-based encoding schemes for words
have been proposed in literature such as Soundex
(Knuth, 1973; Hall and Dowling, 1980), NYSIIS
(Taft, 1970), Metaphone (Philips, 1990), Caver-
phone (Wang, 2009) and Double Metaphone.2
These schemes encode words based on their sound
</bodyText>
<footnote confidence="0.936968166666667">
1Spell correction is also considered as a variant of text
normalization (Damerau, 1964; Tahira, 2004; Fossati and
Di Eugenio, 2007). Here, we limit ourselves to the previous
work on short text normalization.
2http://en.wikipedia.org/wiki/
Metaphone
</footnote>
<bodyText confidence="0.9989085">
which in turn serves as grouping words of similar
sounds (lexical variations) to one code. However,
most of the schemes are designed for English and
European languages and are limited when apply to
other family of languages like Urdu.
In this work, we propose a phonetic encoding
scheme, UrduPhone, tailored for Roman Urdu.
The scheme is derived from the Soundex algo-
rithm. It groups consonants on the basis of com-
mon homophones in Urdu and English. It is differ-
ent from Soundex in two particular ways:3 Firstly,
UrduPhone generates encoding of length six com-
pared to length four in Soundex. This enables
UrduPhone to avoid mapping different forms of
a root word to same code. For example, musku-
rana [smiling] and mshuraht [smile] encode to
one form MSKR in Soundex but in UrduPhone,
they have different encoding which are MSKRN,
MSKRHT respectively. Secondly, we introduce
consonant groups which are mapped differently in
Soundex. We do this by analyzing Urdu alpha-
bets that map to a single roman form e.g. words
samar [reward], sabar [patience] and saib [apple],
all start with different Urdu alphabets that have
identical roman representation: s. In UrduPhone,
we map all such cases to a single form.4
</bodyText>
<subsectionHeader confidence="0.999823">
3.2 Similarity Function
</subsectionHeader>
<bodyText confidence="0.999975">
The similarity between two words wi and wj is
computed by the following similarity function:
</bodyText>
<equation confidence="0.939115666666667">
F f=1 α(f) × σ(f)
ij
S(wi, wj) =
Here, α(f) &gt; 0 is the weight given to feature f,
σij ∈ [0, 1] is the similarity contribution made by
(f)
</equation>
<bodyText confidence="0.999910833333333">
feature f, and F is the total number of features.
In the absence of additional information, and as
used in the experiments in this work, all weights
can be taken equal to one. The similarity function
returns a value in the interval [0, 1] with larger
values signifying higher similarity.
We use two types of features in our method:
word features and contextual features. Word fea-
tures can be based on phonetics and/or string sim-
ilarity. The phonetic similarity between words wi
and wj is 1 (i.e., σij = 1) if both words have the
same UrduPhone ID or encoding; otherwise, their
</bodyText>
<footnote confidence="0.99838175">
3Due to limited space, we limit the description of Urdu-
Phone to its comparison with Soundex.
4A complete table of UrduPhone mappings is provided in
the supplementary material.
</footnote>
<equation confidence="0.707243">
EF
f=1 α(f)
</equation>
<page confidence="0.981599">
824
</page>
<bodyText confidence="0.999546846153846">
similarity is zero. The string similarity between
words wi and wj is defined as follows:
Here, lcs(wi, wj) is the length of the longest com-
mon subsequence in words wi and wj and len(wi)
is the length of word wi. edist(wi, wj) returns the
edit distance between words except when the edit
distance is 0, in which case it returns 1.
Contextual features include top-k frequently oc-
curring previous and next words’ features. Let
ai1, ai2, ... , ai5 and aj1, aj2, ..., aj5 be the word IDs
for the top-5 frequently occurring words preceding
word wi and wj, respectively. Then, the similarity
between words is given by (Hassan et al., 2009)
</bodyText>
<equation confidence="0.997412666666667">
E5k=1 ρk
σij =5
�k=1 k
</equation>
<bodyText confidence="0.971391714285714">
It finds the centroidal word,
, for cluster k as
the word with which the sum of similarities of all
other words in the cluster is a maximum. Then,
each non-centroidal word is assigned to the clus-
ter k if
is a maximum among all clusters
</bodyText>
<equation confidence="0.853113714285714">
Wck
wkc
S(wi,wkc)
S(wi,
) ≥ t. If the latter condition is not
S(wi,
s a new cluster.
</equation>
<bodyText confidence="0.739187">
These two steps are repeated until convergence.
</bodyText>
<sectionHeader confidence="0.954484" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.924605">
call
with low precision (all words belong to one
cluster).
</bodyText>
<subsectionHeader confidence="0.970119">
4.1 Dataset and Gold Standard
</subsectionHeader>
<equation confidence="0.528484">
satisfied (i.e.,
wck) &lt; t) then instead of as-
</equation>
<bodyText confidence="0.970105636363636">
signing word wi to cluster k, it start
man Urdu websites on
poetry6,
and
The second dataset, SMS dataset, is ob-
tained from chopaal, an
based group SMS
service9. For evaluation, we use a manually an-
notated database of Roman Urdu variations (Khan
and Karim, 2012). Table 1 shows statistics of the
datasets in comparison with the gold stan
</bodyText>
<figure confidence="0.562173916666667">
news5,
SMS7
blog8.
internet
dard.
et Web SMS
Unique words 22,044 28,908
dard 12,600 13,087
UrduPhone IDs 3,952 3,599
pearing in gold stan
dard; UrduPhone IDs = num-
ber of distinct UrduPhone encodings.
</figure>
<subsectionHeader confidence="0.986927">
4.2 UrduPhone Evaluation
</subsectionHeader>
<bodyText confidence="0.99865775">
Here, ρk is zero if aik does not have a match in
aj∗ (i.e., in the context of word wj); otherwise,
ρk = 5 − max[k, l] − 1 where aik = ajl and l is
the highest rank (smallest integer) at which a pre-
vious match had not occurred. Instead of word IDs
in ai’s, UrduPhone IDs or string similarity based
cluster IDs can be used to reduce sparsity and im-
prove matches among similar words.
</bodyText>
<subsectionHeader confidence="0.99754">
3.3 Lex-C: Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.992222">
We develop a new clustering algorithm, called
Lex-C, for discovering lexical variations in infor-
mal text. This algorithm is a modified version of
the k-medoids algorithm (Han, 2005). It incorpo-
rates an assignment similarity threshold, t &gt; 0, for
controlling the number of clusters and their simi-
larity. In particular, it ensures that all words in
a cluster have a similarity greater than or equal
to this threshold. It is important to note that the
poular k-means algorithm is known to be effective
for numeric datasets only which is not true in our
case, and it cannot utilize our specialized similar-
ity function for lexical variation discovery.
Specifically, Lex-C starts from an initial clus-
tering based on UrduPhone or string similarity.
and
We empirically evaluate UrduPhone and our com-
plete method involving Lex-C separately on two
real-world datasets. Performance is reported with
B-Cubed precision, recall, and f-measure (Bagga
and Baldwin, 1998; Hassan et al., 2015) on a gold
standard dataset. These performance measures are
based on element-wise comparisons between pre-
dicted and actual clusters that are then aggregated
over all elements in the clustering. This avoided
the issue of 100% precision with low recall (all
words belong to separate clusters) and 100% re-
The first dataset, Web dataset, is scraped from Ro-
</bodyText>
<table confidence="0.7422105">
Datas
Overlap with Gold Stan
</table>
<tableCaption confidence="0.785765">
Table 1: Datasets and gold standard statistics.
Overlap with gold standard =number of words ap-
</tableCaption>
<bodyText confidence="0.8016846">
We compare UrduPhone with Soundex and its
variants.10 These algorithms are used to group
words based on their encoding and then evalu-
ated against the gold stan
dard. Table 2 shows
</bodyText>
<footnote confidence="0.998439857142857">
6https://hadi763.wordpress.com/
7http://www.replysms.com/
9http://chopaal.org
10We use
phonetic library
5http://www.shashca.com,http://stepforwardpak.com/
8http://roman.urdu.co/
</footnote>
<equation confidence="0.8861816">
NLTK-Trainer’s
http://bit.ly/1OJGL9Q
lcs(wi, wj)
min[len(wi), len(wj)] x edist(wi, wj)
σij =
</equation>
<page confidence="0.99455">
825
</page>
<figure confidence="0.998281461538461">
Precision, Recall &amp; F−Measure
5000
0
−5000
Predicted − Actual Clusters
0.5
0
1
1 2 3 4 5 6 7 8 9
Precision
Recall
FMeasure
Cluster Difference
</figure>
<bodyText confidence="0.996501333333333">
the results on Web dataset. UrduPhone out-
performs Soundex, Caverphone, and Metaphone
while Nysiis’s f-measure is comparable to that of
UrduPhone. We observe that Nysiis produces a
large number of single word clusters (out of 6,943
clusters produced 5,159 have only one word). This
gives high precision but recall is low. UrduPhone
produces fewer clusters (and fewer one word clus-
ters) with high recall.
</bodyText>
<table confidence="0.923551">
Experiments
Algorithm Pre Rec Fme
Soundex 0.30 0.97 0.46
Metaphone 0.49 0.80 0.64
Caverphone 0.31 0.92 0.46
Nysiis 0.63 0.69 0.66
UrduPhone 0.51 0.94 0.67
</table>
<tableCaption confidence="0.9936485">
Table 2: Comparison of UrduPhone with other al-
gorithms on Web dataset
</tableCaption>
<subsectionHeader confidence="0.983521">
4.3 Lex-C Evaluation
</subsectionHeader>
<bodyText confidence="0.999958111111111">
We compared Lex-C with k-means and EM clus-
tering algorithms. With both of these algorithms
we used the same feature set (i.e., word features,
phonetic features, and contextual features), How-
ever, their performance lagged the performance of
our approach. The primary reason for this is that
our feature space is not continuous while k-means
and EM algorithms work best for continuous fea-
ture spaces.
</bodyText>
<subsectionHeader confidence="0.381955">
Experiments
</subsectionHeader>
<figureCaption confidence="0.989996">
Figure 1: Performance results for Web dataset
</figureCaption>
<subsectionHeader confidence="0.991521">
4.4 Performance of our Method
</subsectionHeader>
<bodyText confidence="0.999876166666667">
We conduct extensive experiments to evaluate the
performance of our method. We vary initial clus-
terings (UrduPhone encoding or string similarity
based clustering); evaluate various combinations
of phonetic, string, and contextual features; and
consider different previous/next top-5 words’ fea-
</bodyText>
<figureCaption confidence="0.931972">
Figure 2: Performance results for SMS dataset
</figureCaption>
<table confidence="0.9649747">
ID Initial Word Context-1 Context-2
1 – UPhone – –
2 – String – –
3 String String Word ID –
4 String String Cluster ID –
5 String UPhone Cluster ID –
6 UPhone UPhone Cluster ID –
7 UPhone UPhone Word ID –
8 UPhone UPhone UPhone ID Word ID
9 UPhone UPhone UPhone ID Cluster ID
</table>
<tableCaption confidence="0.9428594">
Table 3: Details of experiments’ settings where
Initial is initial clustering based on String or Urdu-
Phone (UPhone)
tures (word ID, cluster ID, and/or UrduPhone ID).
Table 3 gives details of each experiment setting.
</tableCaption>
<bodyText confidence="0.999784347826087">
Figures 1 and 2 show results of selected ex-
periments for Web and SMS datasets respectively.
The x-axis shows the experiment (Exp.) IDs while
the left y-axis gives the precision, recall, and f-
measure and the right y-axis shows the difference
between the number of predicted and actual clus-
ters. Exp. 1 and 2 are baselines corresponding
to UrduPhone encoding (UPhone ID) and string
similarity based word clustering (Cluster ID) re-
spectively. The remaining experiments have dif-
ferent initial clustering, word features, and up to
two contextual features. In these results, the simi-
larity threshold t is selected such that the number
of discovered clusters is as close as possible to the
number of actual clusters in the gold standard for
each dataset. This is done to make the results com-
parable across different settings.
Compared to baselines, our method shows a
gain of up to 12% and 8% in Web and SMS
datasets respectively. The best performances are
obtained when UrduPhone is used as a feature
and UrduPhone IDs are used to define the context
(Exp. 8 and 9). In particular, when both Urdu-
</bodyText>
<figure confidence="0.997174470588235">
1 2 3 4 5 6 7 8 9
Precision, Recall &amp; F−Measure
0.8
0.6
0.4
0.2
0
1
Precision
Recall
FMeasure
Cluster Difference
1000 Predicted − Actual Clusters
0
−1000
−2000
−3000
</figure>
<page confidence="0.997361">
826
</page>
<bodyText confidence="0.999064727272727">
Phone IDs and word IDs/cluster IDs are used for
contextual information (i.e, with two sets of top-5
previous and next words’ features) the f-measure
is consistently high.
We analyzed the performance of Exp. 8 (best
settings for Web dataset) with varying t and
showed it in Figure 3. It is observed that the value
of t controls the number of clusters smoothly, and
precision increases with the number of clusters
and f-measure reaches a peak when number of
clusters is close to that in the gold standard.
</bodyText>
<figureCaption confidence="0.97036">
Figure 3: Effect of varying threshold t on Web
dataset (experiment 8)
</figureCaption>
<sectionHeader confidence="0.989283" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995">
We proposed an unsupervised method for finding
lexical variations in Roman Urdu. We presented a
phonetic encoding scheme UrduPhone for Roman
Urdu, and developed a feature-based clustering al-
gorithm Lex-C. Our experiments are evaluated on
a manually developed gold standard. The results
confirmed that our method outperforms baseline
methods. We made the datasets and algorithm
code available to the research community.
</bodyText>
<sectionHeader confidence="0.989012" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997732">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.
Grzegorz Chrupala. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2014,
June 22-27, 2014, Baltimore, MD, USA, Volume 2:
Short Papers, pages 680–686.
Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: Progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia-Social and Behavioral Sciences,
27:2–11.
Danish Contractor, Tanveer A Faruquie, and L Venkata
Subramaniam. 2010. Unsupervised cleansing of
noisy text. In Proceedings of the 23rd International
Conference on ComputationalLinguistics: Posters,
pages 189–196. Association for Computational Lin-
guistics.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7(3), March.
Neelmay Desai and Meera Narvekar. 2015. Normal-
ization of noisy text data. Procedia Computer Sci-
ence, 45:127–132.
Nadir Durrani and Sarmad Hussain. 2010. Urdu word
segmentation. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 528–536, Los Angeles, California,
June. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Alexander Fraser, and
Helmut Schmid. 2010. Hindi-to-urdu machine
translation through transliteration. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 465–474, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Davide Fossati and Barbara Di Eugenio. 2007. A
mixed trigrams approach for context sensitive spell
checking. In Computational Linguistics and Intelli-
gent Text Processing, pages 623–633. Springer.
Patrick A. V. Hall and Geoff R. Dowling. 1980. Ap-
proximate string matching. ACM Comput. Surv.,
12(4):381–402.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421–432. Association
for Computational Linguistics.
Jiawei Han. 2005. Data Mining: Concepts and Tech-
niques. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Malik Tahir Hassan, Khurum Nazir Junejo, and Asim
Karim. 2009. Learning and predicting key web nav-
igation patterns using bayesian models. In Compu-
tational Science and Its Applications–ICCSA, pages
877–887. Springer.
Malik Tahir Hassan, Asim Karim, Jeong-Bae Kim, and
Moongu Jeon. 2015. Cdim: Document clustering
by discrimination information maximization. Infor-
mation Sciences.
</reference>
<figure confidence="0.9933108">
1.1
6000
1
3000
0.9
0.8
0
0.7
0.6
−3000
0.5
0.4 −6000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Threshold
Precision, Recall &amp; F−Measure
Predicted − Actual Clusters
Precision
Recall
FMeasure
Cluster Difference
</figure>
<page confidence="0.956758">
827
</page>
<reference confidence="0.999641487804878">
Ann Irvine, Jonathan Weese, and Chris Callison-Burch.
2012. Processing informal, romanized pakistani text
messages. In Proceedings of the Second Workshop
on Language in Social Media, LSM ’12, pages 75–
78. Association for Computational Linguistics.
Max Kaufmann and Jugal Kalita. 2010. Syntac-
tic normalization of twitter messages. In Interna-
tional Conference on Natural Language Processing,
Kharagpur, India.
Osama Khan and Asim Karim. 2012. A rule-based
model for normalization of sms text. In IEEE 24th
International Conference on Tools with Artificial In-
telligence, ICTAI 2012, Athens, Greece, November
7-9, 2012, pages 634–641.
Donald E Knuth. 1973. The Art of Computer Program-
ming: Volume 3, Sorting and Searching. Addison-
Wesley.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In EMNLP, pages 73–84.
Tahira Naseem and Sarmad Hussain. 2007. A
novel approach for ranking spelling error correc-
tions for urdu. Language Resources and Evaluation,
41(2):117–128.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language Magazine, 7(12):39–44, De-
cember.
David Pinto, Darnes Vilari˜no Ayala, Yuridiana Alem´an,
Helena G´omez-Adorno, Nahun Loya, and H´ector
Jim´enez-Salazar. 2012. The soundex phonetic algo-
rithm revisited for SMS text representation. In Text,
Speech and Dialogue - 15th International Confer-
ence, TSD 2012, Brno, Czech Republic, September
3-7, 2012. Proceedings, pages 47–55.
Sudipta Roy, Sourish Dhar, Saprativa Bhattacharjee,
and Anirban Das. 2013. A lexicon based algo-
rithm for noisy text normalization as pre processing
for sentiment analysis. International Journal of Re-
search in Engineering and Technology, 02.
Hassan Sajjad and Helmut Schmid. 2009. Tagging
urdu text with parts of speech: A tagger compar-
ison. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL-09), pages 692–700,
Athens, Greece.
Uladzimir Sidarenka, Tatjana Scheffler, and Manfred
Stede. 2013. Rule-based normalization of german
twitter messages. In Proc. of the GSCL Workshop
Verarbeitung und Annotation von Sprachdaten aus
Genres internetbasierter Kommunikation.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech &amp; Language, 15(3):287–
333.
R. Taft. 1970. Name search techniques. Special
report. Bureau of Systems Development,New York
State Identification and Intelligence System.
Naseem Tahira. 2004. A hybrid approach for Urdu
spell checking.
Pidong Wang and Hwee Tou Ng. 2013. A beam-search
decoder for normalization of social media text with
application to machine translation. In HLT-NAACL,
pages 471–481.
John Wang, editor. 2009. Encyclopedia of Data Ware-
housing and Mining, Second Edition (4 Volumes).
IGI Global.
Zhongyu Wei, Lanjun Zhou, Binyang Li, Kam-Fai
Wong, Wei Gao, and Kam-Fai Wong. 2011. Explor-
ing tweets normalization and query time sensitivity
for twitter search. In TREC.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2013,
18-21 October 2013, Grand Hyatt Seattle, Seattle,
Washington, USA, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 61–72.
Xin Zhang, Jiaying Song, Yu He, and Guohong Fu.
2015. Normalization of homophonic words in chi-
nese microblogs. In Intelligent Computation in Big
Data Era, pages 177–187. Springer.
</reference>
<page confidence="0.997625">
828
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534215">
<title confidence="0.831966">An Unsupervised Method for Discovering Lexical in Roman Urdu Informal Text Rafae, Qayyum,</title>
<author confidence="0.727057">Sajjad Karim</author>
<affiliation confidence="0.9934105">Computing Research Institute, Hamad Bin Khalifa University of Management Sciences, Technology University</affiliation>
<abstract confidence="0.9995675">We present an unsupervised method to find lexical variations in Roman Urdu informal text. Our method includes a algorithm a featurebased similarity function, and a clustering UrduPhone encodes roman Urdu strings to their phonetic equivalent representations. This produces an initial grouping of different spelling variations of a word. The similarity function incorporates word features and their context. Lex-C is a variant of k-medoids clustering algorithm that group lexical variations. It incorporates a similarity threshold to balance the number of clusters and their maximum similarity. We test our system on two datasets of SMS and blogs and show an f-measure gain of up to 12% from baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="11606" citStr="Bagga and Baldwin, 1998" startWordPosition="1892" endWordPosition="1895">hat all words in a cluster have a similarity greater than or equal to this threshold. It is important to note that the poular k-means algorithm is known to be effective for numeric datasets only which is not true in our case, and it cannot utilize our specialized similarity function for lexical variation discovery. Specifically, Lex-C starts from an initial clustering based on UrduPhone or string similarity. and We empirically evaluate UrduPhone and our complete method involving Lex-C separately on two real-world datasets. Performance is reported with B-Cubed precision, recall, and f-measure (Bagga and Baldwin, 1998; Hassan et al., 2015) on a gold standard dataset. These performance measures are based on element-wise comparisons between predicted and actual clusters that are then aggregated over all elements in the clustering. This avoided the issue of 100% precision with low recall (all words belong to separate clusters) and 100% reThe first dataset, Web dataset, is scraped from RoDatas Overlap with Gold Stan Table 1: Datasets and gold standard statistics. Overlap with gold standard =number of words apWe compare UrduPhone with Soundex and its variants.10 These algorithms are used to group words based on</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupala</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>680--686</pages>
<location>Baltimore, MD, USA, Volume</location>
<contexts>
<context position="3607" citStr="Chrupala, 2014" startWordPosition="565" endWordPosition="566"> of the k-medoids clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical v</context>
</contexts>
<marker>Chrupala, 2014</marker>
<rawString>Grzegorz Chrupala. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 680–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Clark</author>
<author>Kenji Araki</author>
</authors>
<title>Text normalization in social media: Progress, problems and applications for a pre-processing system of casual english.</title>
<date>2011</date>
<booktitle>Procedia-Social and Behavioral Sciences,</booktitle>
<pages>27--2</pages>
<contexts>
<context position="3573" citStr="Clark and Araki, 2011" startWordPosition="557" endWordPosition="560">of every other word. We present a variant of the k-medoids clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricte</context>
</contexts>
<marker>Clark, Araki, 2011</marker>
<rawString>Eleanor Clark and Kenji Araki. 2011. Text normalization in social media: Progress, problems and applications for a pre-processing system of casual english. Procedia-Social and Behavioral Sciences, 27:2–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
</authors>
<title>Tanveer A Faruquie, and L Venkata Subramaniam.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on ComputationalLinguistics: Posters,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Contractor, 2010</marker>
<rawString>Danish Contractor, Tanveer A Faruquie, and L Venkata Subramaniam. 2010. Unsupervised cleansing of noisy text. In Proceedings of the 23rd International Conference on ComputationalLinguistics: Posters, pages 189–196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Commun. ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="6101" citStr="Damerau, 1964" startWordPosition="947" endWordPosition="948">xical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu. In this work, we propose a phonetic encoding scheme, UrduPhone, tailored for Roman Urdu. The scheme is derived from the Soundex algorithm. It groups consonants on the basis of common homophones in Urdu </context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Commun. ACM, 7(3), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neelmay Desai</author>
<author>Meera Narvekar</author>
</authors>
<title>Normalization of noisy text data.</title>
<date>2015</date>
<booktitle>Procedia Computer Science,</booktitle>
<pages>45--127</pages>
<contexts>
<context position="3703" citStr="Desai and Narvekar, 2015" startWordPosition="579" endWordPosition="582">least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem since in this case every w</context>
</contexts>
<marker>Desai, Narvekar, 2015</marker>
<rawString>Neelmay Desai and Meera Narvekar. 2015. Normalization of noisy text data. Procedia Computer Science, 45:127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Sarmad Hussain</author>
</authors>
<title>Urdu word segmentation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>528--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="2047" citStr="Durrani and Hussain, 2010" startWordPosition="310" endWordPosition="313">n and usually many spelling variations exist for a given word, e.g., the word zindagi [life] is also written as zindagee, zindagy, zaindagee and zndagi. Specifically, the following normalization issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing tasks such as Urdu word segmentation (Durrani and Hussain, 2010), part of speech tagging (Sajjad and Schmid, 2009), spell checking (Naseem and Hussain, 2007), machine translation (Durrani et al., 2010), etc. In this paper, we propose an unsupervised feature-based method that tackles above mentioned challenges in discovering lexical variations in Roman Urdu. We exploit phonetic and string similarity based features and incorporate contextual features via top-k previous and next words’ features. For phonetic information, we develop an encoding scheme for Roman Urdu, UrduPhone, motivated from Soundex. Compared to other available phonetic-based schemes that are</context>
</contexts>
<marker>Durrani, Hussain, 2010</marker>
<rawString>Nadir Durrani and Sarmad Hussain. 2010. Urdu word segmentation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 528–536, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Hindi-to-urdu machine translation through transliteration.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>465--474</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2184" citStr="Durrani et al., 2010" startWordPosition="331" endWordPosition="334">nd zndagi. Specifically, the following normalization issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing tasks such as Urdu word segmentation (Durrani and Hussain, 2010), part of speech tagging (Sajjad and Schmid, 2009), spell checking (Naseem and Hussain, 2007), machine translation (Durrani et al., 2010), etc. In this paper, we propose an unsupervised feature-based method that tackles above mentioned challenges in discovering lexical variations in Roman Urdu. We exploit phonetic and string similarity based features and incorporate contextual features via top-k previous and next words’ features. For phonetic information, we develop an encoding scheme for Roman Urdu, UrduPhone, motivated from Soundex. Compared to other available phonetic-based schemes that are mostly limited to English sounds only, UrduPhone maps Roman Urdu homophones effectively. Unlike previous work on short text normalizatio</context>
</contexts>
<marker>Durrani, Sajjad, Fraser, Schmid, 2010</marker>
<rawString>Nadir Durrani, Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2010. Hindi-to-urdu machine translation through transliteration. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 465–474, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Fossati</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>A mixed trigrams approach for context sensitive spell checking.</title>
<date>2007</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>623--633</pages>
<publisher>Springer.</publisher>
<marker>Fossati, Di Eugenio, 2007</marker>
<rawString>Davide Fossati and Barbara Di Eugenio. 2007. A mixed trigrams approach for context sensitive spell checking. In Computational Linguistics and Intelligent Text Processing, pages 623–633. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick A V Hall</author>
<author>Geoff R Dowling</author>
</authors>
<title>Approximate string matching.</title>
<date>1980</date>
<journal>ACM Comput. Surv.,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="5870" citStr="Hall and Dowling, 1980" startWordPosition="911" endWordPosition="914"> Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 Our Method The lexical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other fa</context>
</contexts>
<marker>Hall, Dowling, 1980</marker>
<rawString>Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate string matching. ACM Comput. Surv., 12(4):381–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4513" citStr="Han et al. (2012" startWordPosition="708" endWordPosition="711">on, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo ap</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiawei Han</author>
</authors>
<title>Data Mining: Concepts and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10834" citStr="Han, 2005" startWordPosition="1771" endWordPosition="1772">hone Evaluation Here, ρk is zero if aik does not have a match in aj∗ (i.e., in the context of word wj); otherwise, ρk = 5 − max[k, l] − 1 where aik = ajl and l is the highest rank (smallest integer) at which a previous match had not occurred. Instead of word IDs in ai’s, UrduPhone IDs or string similarity based cluster IDs can be used to reduce sparsity and improve matches among similar words. 3.3 Lex-C: Clustering Algorithm We develop a new clustering algorithm, called Lex-C, for discovering lexical variations in informal text. This algorithm is a modified version of the k-medoids algorithm (Han, 2005). It incorporates an assignment similarity threshold, t &gt; 0, for controlling the number of clusters and their similarity. In particular, it ensures that all words in a cluster have a similarity greater than or equal to this threshold. It is important to note that the poular k-means algorithm is known to be effective for numeric datasets only which is not true in our case, and it cannot utilize our specialized similarity function for lexical variation discovery. Specifically, Lex-C starts from an initial clustering based on UrduPhone or string similarity. and We empirically evaluate UrduPhone a</context>
</contexts>
<marker>Han, 2005</marker>
<rawString>Jiawei Han. 2005. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malik Tahir Hassan</author>
<author>Khurum Nazir Junejo</author>
<author>Asim Karim</author>
</authors>
<title>Learning and predicting key web navigation patterns using bayesian models.</title>
<date>2009</date>
<booktitle>In Computational Science and Its Applications–ICCSA,</booktitle>
<pages>877--887</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9125" citStr="Hassan et al., 2009" startWordPosition="1461" endWordPosition="1464">s zero. The string similarity between words wi and wj is defined as follows: Here, lcs(wi, wj) is the length of the longest common subsequence in words wi and wj and len(wi) is the length of word wi. edist(wi, wj) returns the edit distance between words except when the edit distance is 0, in which case it returns 1. Contextual features include top-k frequently occurring previous and next words’ features. Let ai1, ai2, ... , ai5 and aj1, aj2, ..., aj5 be the word IDs for the top-5 frequently occurring words preceding word wi and wj, respectively. Then, the similarity between words is given by (Hassan et al., 2009) E5k=1 ρk σij =5 �k=1 k It finds the centroidal word, , for cluster k as the word with which the sum of similarities of all other words in the cluster is a maximum. Then, each non-centroidal word is assigned to the cluster k if is a maximum among all clusters Wck wkc S(wi,wkc) S(wi, ) ≥ t. If the latter condition is not S(wi, s a new cluster. These two steps are repeated until convergence. 4 Experimental Evaluation call with low precision (all words belong to one cluster). 4.1 Dataset and Gold Standard satisfied (i.e., wck) &lt; t) then instead of assigning word wi to cluster k, it start man Urdu</context>
</contexts>
<marker>Hassan, Junejo, Karim, 2009</marker>
<rawString>Malik Tahir Hassan, Khurum Nazir Junejo, and Asim Karim. 2009. Learning and predicting key web navigation patterns using bayesian models. In Computational Science and Its Applications–ICCSA, pages 877–887. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malik Tahir Hassan</author>
<author>Asim Karim</author>
<author>Jeong-Bae Kim</author>
<author>Moongu Jeon</author>
</authors>
<title>Cdim: Document clustering by discrimination information maximization. Information Sciences.</title>
<date>2015</date>
<contexts>
<context position="11628" citStr="Hassan et al., 2015" startWordPosition="1896" endWordPosition="1899">r have a similarity greater than or equal to this threshold. It is important to note that the poular k-means algorithm is known to be effective for numeric datasets only which is not true in our case, and it cannot utilize our specialized similarity function for lexical variation discovery. Specifically, Lex-C starts from an initial clustering based on UrduPhone or string similarity. and We empirically evaluate UrduPhone and our complete method involving Lex-C separately on two real-world datasets. Performance is reported with B-Cubed precision, recall, and f-measure (Bagga and Baldwin, 1998; Hassan et al., 2015) on a gold standard dataset. These performance measures are based on element-wise comparisons between predicted and actual clusters that are then aggregated over all elements in the clustering. This avoided the issue of 100% precision with low recall (all words belong to separate clusters) and 100% reThe first dataset, Web dataset, is scraped from RoDatas Overlap with Gold Stan Table 1: Datasets and gold standard statistics. Overlap with gold standard =number of words apWe compare UrduPhone with Soundex and its variants.10 These algorithms are used to group words based on their encoding and th</context>
</contexts>
<marker>Hassan, Karim, Kim, Jeon, 2015</marker>
<rawString>Malik Tahir Hassan, Asim Karim, Jeong-Bae Kim, and Moongu Jeon. 2015. Cdim: Document clustering by discrimination information maximization. Information Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Jonathan Weese</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Processing informal, romanized pakistani text messages.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM ’12,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4834" citStr="Irvine et al. (2012)" startWordPosition="756" endWordPosition="759">ns in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word</context>
</contexts>
<marker>Irvine, Weese, Callison-Burch, 2012</marker>
<rawString>Ann Irvine, Jonathan Weese, and Chris Callison-Burch. 2012. Processing informal, romanized pakistani text messages. In Proceedings of the Second Workshop on Language in Social Media, LSM ’12, pages 75– 78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of twitter messages.</title>
<date>2010</date>
<booktitle>In International Conference on Natural Language Processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="3634" citStr="Kaufmann and Kalita, 2010" startWordPosition="567" endWordPosition="570">s clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text.</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Max Kaufmann and Jugal Kalita. 2010. Syntactic normalization of twitter messages. In International Conference on Natural Language Processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osama Khan</author>
<author>Asim Karim</author>
</authors>
<title>A rule-based model for normalization of sms text.</title>
<date>2012</date>
<booktitle>In IEEE 24th International Conference on Tools with Artificial Intelligence, ICTAI 2012,</booktitle>
<pages>634--641</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="9939" citStr="Khan and Karim, 2012" startWordPosition="1611" endWordPosition="1614">rd is assigned to the cluster k if is a maximum among all clusters Wck wkc S(wi,wkc) S(wi, ) ≥ t. If the latter condition is not S(wi, s a new cluster. These two steps are repeated until convergence. 4 Experimental Evaluation call with low precision (all words belong to one cluster). 4.1 Dataset and Gold Standard satisfied (i.e., wck) &lt; t) then instead of assigning word wi to cluster k, it start man Urdu websites on poetry6, and The second dataset, SMS dataset, is obtained from chopaal, an based group SMS service9. For evaluation, we use a manually annotated database of Roman Urdu variations (Khan and Karim, 2012). Table 1 shows statistics of the datasets in comparison with the gold stan news5, SMS7 blog8. internet dard. et Web SMS Unique words 22,044 28,908 dard 12,600 13,087 UrduPhone IDs 3,952 3,599 pearing in gold stan dard; UrduPhone IDs = number of distinct UrduPhone encodings. 4.2 UrduPhone Evaluation Here, ρk is zero if aik does not have a match in aj∗ (i.e., in the context of word wj); otherwise, ρk = 5 − max[k, l] − 1 where aik = ajl and l is the highest rank (smallest integer) at which a previous match had not occurred. Instead of word IDs in ai’s, UrduPhone IDs or string similarity based cl</context>
</contexts>
<marker>Khan, Karim, 2012</marker>
<rawString>Osama Khan and Asim Karim. 2012. A rule-based model for normalization of sms text. In IEEE 24th International Conference on Tools with Artificial Intelligence, ICTAI 2012, Athens, Greece, November 7-9, 2012, pages 634–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>The Art of Computer Programming: Volume 3, Sorting and Searching.</title>
<date>1973</date>
<publisher>AddisonWesley.</publisher>
<contexts>
<context position="5845" citStr="Knuth, 1973" startWordPosition="909" endWordPosition="910">t al. (2010), Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 Our Method The lexical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limit</context>
</contexts>
<marker>Knuth, 1973</marker>
<rawString>Donald E Knuth. 1973. The Art of Computer Programming: Volume 3, Sorting and Searching. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>73--84</pages>
<contexts>
<context position="3677" citStr="Ling et al., 2013" startWordPosition="575" endWordPosition="578"> every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem </context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization. In EMNLP, pages 73–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Sarmad Hussain</author>
</authors>
<title>A novel approach for ranking spelling error corrections for urdu. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--2</pages>
<contexts>
<context position="2140" citStr="Naseem and Hussain, 2007" startWordPosition="325" endWordPosition="328">s also written as zindagee, zindagy, zaindagee and zndagi. Specifically, the following normalization issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing tasks such as Urdu word segmentation (Durrani and Hussain, 2010), part of speech tagging (Sajjad and Schmid, 2009), spell checking (Naseem and Hussain, 2007), machine translation (Durrani et al., 2010), etc. In this paper, we propose an unsupervised feature-based method that tackles above mentioned challenges in discovering lexical variations in Roman Urdu. We exploit phonetic and string similarity based features and incorporate contextual features via top-k previous and next words’ features. For phonetic information, we develop an encoding scheme for Roman Urdu, UrduPhone, motivated from Soundex. Compared to other available phonetic-based schemes that are mostly limited to English sounds only, UrduPhone maps Roman Urdu homophones effectively. Unl</context>
</contexts>
<marker>Naseem, Hussain, 2007</marker>
<rawString>Tahira Naseem and Sarmad Hussain. 2007. A novel approach for ranking spelling error corrections for urdu. Language Resources and Evaluation, 41(2):117–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<title>Hanging on the metaphone.</title>
<date>1990</date>
<journal>Computer Language Magazine,</journal>
<volume>7</volume>
<issue>12</issue>
<contexts>
<context position="5918" citStr="Philips, 1990" startWordPosition="919" endWordPosition="920">t like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 Our Method The lexical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu. In this work, we pr</context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Lawrence Philips. 1990. Hanging on the metaphone. Computer Language Magazine, 7(12):39–44, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Darnes Vilari˜no Ayala</author>
<author>Yuridiana Alem´an</author>
<author>Helena G´omez-Adorno</author>
<author>Nahun Loya</author>
<author>H´ector Jim´enez-Salazar</author>
</authors>
<title>The soundex phonetic algorithm revisited for SMS text representation.</title>
<date>2012</date>
<booktitle>In Text, Speech and Dialogue - 15th International Conference, TSD 2012,</booktitle>
<pages>47--55</pages>
<location>Brno, Czech Republic,</location>
<marker>Pinto, Ayala, Alem´an, G´omez-Adorno, Loya, Jim´enez-Salazar, 2012</marker>
<rawString>David Pinto, Darnes Vilari˜no Ayala, Yuridiana Alem´an, Helena G´omez-Adorno, Nahun Loya, and H´ector Jim´enez-Salazar. 2012. The soundex phonetic algorithm revisited for SMS text representation. In Text, Speech and Dialogue - 15th International Conference, TSD 2012, Brno, Czech Republic, September 3-7, 2012. Proceedings, pages 47–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudipta Roy</author>
<author>Sourish Dhar</author>
<author>Saprativa Bhattacharjee</author>
<author>Anirban Das</author>
</authors>
<title>A lexicon based algorithm for noisy text normalization as pre processing for sentiment analysis.</title>
<date>2013</date>
<journal>International Journal of Research in Engineering and Technology,</journal>
<volume>02</volume>
<contexts>
<context position="3591" citStr="Roy et al., 2013" startWordPosition="561" endWordPosition="564"> present a variant of the k-medoids clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of f</context>
</contexts>
<marker>Roy, Dhar, Bhattacharjee, Das, 2013</marker>
<rawString>Sudipta Roy, Sourish Dhar, Saprativa Bhattacharjee, and Anirban Das. 2013. A lexicon based algorithm for noisy text normalization as pre processing for sentiment analysis. International Journal of Research in Engineering and Technology, 02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Helmut Schmid</author>
</authors>
<title>Tagging urdu text with parts of speech: A tagger comparison.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-09),</booktitle>
<pages>692--700</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2097" citStr="Sajjad and Schmid, 2009" startWordPosition="318" endWordPosition="322">iven word, e.g., the word zindagi [life] is also written as zindagee, zindagy, zaindagee and zndagi. Specifically, the following normalization issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing tasks such as Urdu word segmentation (Durrani and Hussain, 2010), part of speech tagging (Sajjad and Schmid, 2009), spell checking (Naseem and Hussain, 2007), machine translation (Durrani et al., 2010), etc. In this paper, we propose an unsupervised feature-based method that tackles above mentioned challenges in discovering lexical variations in Roman Urdu. We exploit phonetic and string similarity based features and incorporate contextual features via top-k previous and next words’ features. For phonetic information, we develop an encoding scheme for Roman Urdu, UrduPhone, motivated from Soundex. Compared to other available phonetic-based schemes that are mostly limited to English sounds only, UrduPhone </context>
</contexts>
<marker>Sajjad, Schmid, 2009</marker>
<rawString>Hassan Sajjad and Helmut Schmid. 2009. Tagging urdu text with parts of speech: A tagger comparison. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-09), pages 692–700, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uladzimir Sidarenka</author>
<author>Tatjana Scheffler</author>
<author>Manfred Stede</author>
</authors>
<title>Rule-based normalization of german twitter messages.</title>
<date>2013</date>
<booktitle>In Proc. of the GSCL Workshop Verarbeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation.</booktitle>
<contexts>
<context position="3658" citStr="Sidarenka et al., 2013" startWordPosition="571" endWordPosition="574"> forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more c</context>
</contexts>
<marker>Sidarenka, Scheffler, Stede, 2013</marker>
<rawString>Uladzimir Sidarenka, Tatjana Scheffler, and Manfred Stede. 2013. Rule-based normalization of german twitter messages. In Proc. of the GSCL Workshop Verarbeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley F Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>333</pages>
<contexts>
<context position="3532" citStr="Sproat et al., 2001" startWordPosition="549" endWordPosition="552">very word in the corpus is a candidate of every other word. We present a variant of the k-medoids clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standar</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Taft</author>
</authors>
<title>Name search techniques. Special report. Bureau of Systems Development,New York State Identification and Intelligence System.</title>
<date>1970</date>
<contexts>
<context position="5891" citStr="Taft, 1970" startWordPosition="916" endWordPosition="917">et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 Our Method The lexical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages lik</context>
</contexts>
<marker>Taft, 1970</marker>
<rawString>R. Taft. 1970. Name search techniques. Special report. Bureau of Systems Development,New York State Identification and Intelligence System.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naseem Tahira</author>
</authors>
<title>A hybrid approach for Urdu spell checking.</title>
<date>2004</date>
<contexts>
<context position="6115" citStr="Tahira, 2004" startWordPosition="949" endWordPosition="950">s of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding scheme, UrduPhone, a feature-based similarity function, and a clustering algorithm, Lex-C. 3.1 UrduPhone Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caverphone (Wang, 2009) and Double Metaphone.2 These schemes encode words based on their sound 1Spell correction is also considered as a variant of text normalization (Damerau, 1964; Tahira, 2004; Fossati and Di Eugenio, 2007). Here, we limit ourselves to the previous work on short text normalization. 2http://en.wikipedia.org/wiki/ Metaphone which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu. In this work, we propose a phonetic encoding scheme, UrduPhone, tailored for Roman Urdu. The scheme is derived from the Soundex algorithm. It groups consonants on the basis of common homophones in Urdu and English. I</context>
</contexts>
<marker>Tahira, 2004</marker>
<rawString>Naseem Tahira. 2004. A hybrid approach for Urdu spell checking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pidong Wang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beam-search decoder for normalization of social media text with application to machine translation.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>471--481</pages>
<contexts>
<context position="4694" citStr="Wang and Ng (2013)" startWordPosition="738" endWordPosition="741">that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012) </context>
</contexts>
<marker>Wang, Ng, 2013</marker>
<rawString>Pidong Wang and Hwee Tou Ng. 2013. A beam-search decoder for normalization of social media text with application to machine translation. In HLT-NAACL, pages 471–481.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Encyclopedia of Data Warehousing and Mining, Second Edition (4 Volumes). IGI Global.</booktitle>
<editor>John Wang, editor.</editor>
<marker>2009</marker>
<rawString>John Wang, editor. 2009. Encyclopedia of Data Warehousing and Mining, Second Edition (4 Volumes). IGI Global.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongyu Wei</author>
<author>Lanjun Zhou</author>
<author>Binyang Li</author>
<author>Kam-Fai Wong</author>
<author>Wei Gao</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Exploring tweets normalization and query time sensitivity for twitter search.</title>
<date>2011</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="3550" citStr="Wei et al., 2011" startWordPosition="553" endWordPosition="556">us is a candidate of every other word. We present a variant of the k-medoids clustering algorithm that forms clusters in which every word has at least a specified minimum similarity with the cluster’s centroidal word. We conduct experiments on two Roman Urdu datasets: an SMS dataset and a blog dataset and evaluate performance using a gold standard. Our method shows an f-measure gain of up to 12% compared to baseline methods. The dataset and code are made available to the research community. 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Ther</context>
</contexts>
<marker>Wei, Zhou, Li, Wong, Gao, Wong, 2011</marker>
<rawString>Zhongyu Wei, Lanjun Zhou, Binyang Li, Kam-Fai Wong, Wei Gao, and Kam-Fai Wong. 2011. Exploring tweets normalization and query time sensitivity for twitter search. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<journal>A meeting of SIGDAT, a Special Interest Group of the ACL,</journal>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>18--21</pages>
<location>Grand Hyatt Seattle, Seattle, Washington, USA,</location>
<contexts>
<context position="5031" citStr="Yang and Eisenstein (2013)" startWordPosition="785" endWordPosition="788"> and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 Our Method The lexical variations of a lexical entry usually have high phonetic, string-based, and contextual similarity. We integrate a phonetic-based encoding</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Zhang</author>
<author>Jiaying Song</author>
<author>Yu He</author>
<author>Guohong Fu</author>
</authors>
<title>Normalization of homophonic words in chinese microblogs.</title>
<date>2015</date>
<booktitle>In Intelligent Computation in Big Data Era,</booktitle>
<pages>177--187</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4534" citStr="Zhang et al. (2015)" startWordPosition="712" endWordPosition="715">1 September 2015. c�2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propo</context>
</contexts>
<marker>Zhang, Song, He, Fu, 2015</marker>
<rawString>Xin Zhang, Jiaying Song, Yu He, and Guohong Fu. 2015. Normalization of homophonic words in chinese microblogs. In Intelligent Computation in Big Data Era, pages 177–187. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
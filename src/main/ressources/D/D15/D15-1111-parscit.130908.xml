<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.947443">
Feature-Rich Two-Stage Logistic Regression for Monolingual Alignment
</title>
<author confidence="0.939898">
Md Arafat Sultan†, Steven Bethard‡ and Tamara Sumner††Institute of Cognitive Science and Department of Computer Science
</author>
<affiliation confidence="0.998279333333333">
University of Colorado Boulder
‡Department of Computer and Information Sciences
University of Alabama at Birmingham
</affiliation>
<email confidence="0.983002">
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972588235294">
Monolingual alignment is the task of pair-
ing semantically similar units from two
pieces of text. We report a top-performing
supervised aligner that operates on short
text snippets. We employ a large feature set
to (1) encode similarities among semantic
units (words and named entities) in context,
and (2) address cooperation and competi-
tion for alignment among units in the same
snippet. These features are deployed in a
two-stage logistic regression framework for
alignment. On two benchmark data sets,
our aligner achieves F1 scores of 92.1%
and 88.5%, with statistically significant er-
ror reductions of 4.8% and 7.3% over the
previous best aligner. It produces top re-
sults in extrinsic evaluation as well.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959693548387">
Computer applications frequently require seman-
tic comparison between short snippets of natural
language text. Such comparisons are key to para-
phrase detection (Das and Smith, 2009; Madnani et
al., 2012), textual similarity identification (Agirre
et al., 2015; Sultan et al., 2015) and recognition
of textual entailment (Dagan and Glickman, 2004;
Pad´o et al., 2015). And they underpin applications
such as short answer grading (Mohler et al., 2011),
question answering (Hixon et al., 2015), machine
translation evaluation (Pad´o et al., 2009), and ma-
chine reading (de Marneffe et al., 2007).
A central problem underlying all text compari-
son tasks is that of alignment: pairing related se-
mantic units (i.e. words and phrases) across the
two snippets (MacCartney et al., 2008; Thadani
and McKeown, 2011; Thadani et al., 2012; Yao et
al., 2013a; Yao et al., 2013b; Sultan et al., 2014a).
Studies have shown that such tasks can benefit from
an explicit alignment component (Hickl and Bens-
ley, 2007; Sultan et al., 2014b; Sultan et al., 2015).
However, alignment is still an open research prob-
lem. We present a supervised monolingual aligner
that produces top results in several intrinsic and
extrinsic evaluation experiments. We pinpoint a set
of key challenges for alignment and design a model
with components targeted at each.
Lexical and phrasal alignments can both be rep-
resented as pairs of words – in the form of many-
to-many mappings among the two phrases’ com-
ponent words in the latter case. Thus without loss
of generality, we formulate alignment as a binary
classification task where given all word pairs across
two sentences, the goal is to assign each a class la-
bel in {aligned, not aligned}. However, this is not a
straightforward classification scenario where each
word pair can be treated independently – words in
the same snippet can play both mutually cooper-
ating and competing roles in complex ways. For
example, semantically similar words in a snippet
can be in competition for alignment with a word
in the other snippet, whereas words that constitute
a phrase can provide supporting evidence for one
another (e.g. in named entity alignments such as
Watson ↔ John Hamish Watson). To han-
dle such interdependencies, we employ a two-stage
logistic regression model – stage 1 computes an
alignment probability for each word pair based
solely on its own feature values, and stage 2 assigns
the eventual alignment labels to all pairs following
a comparative assessment of stage 1 probabilities
of cooperating and competing pairs.
On two alignment data sets reported in (Brock-
ett, 2007) and (Thadani et al., 2012), our aligner
demonstrates respective F1 scores of 92.1% and
88.5%, with statistically significant error reduc-
tions of 4.8% and 7.3% over the previous best
aligner (Sultan et al., 2014a). We also present ex-
trinsic evaluation of the aligner within two text
comparison tasks, namely sentence similarity iden-
tification and paraphrase detection, where it demon-
strates state-of-the-art results.
</bodyText>
<page confidence="0.981047">
949
</page>
<note confidence="0.9959285">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 949–959,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999928">
Figure 1: A human-aligned sentence pair from
</figureCaption>
<bodyText confidence="0.99850975">
the MSR alignment corpus. The shaded
cells depict the alignment, which can also be
represented as the set of word index pairs
{(1,1), (2, 2), (3, 3), (4, 3), (5, 4), ... , (15, 9)}.
</bodyText>
<sectionHeader confidence="0.954226" genericHeader="method">
2 Alignment: Key Pieces of the Puzzle
</sectionHeader>
<bodyText confidence="0.9994465">
We illustrate with examples key pieces of the align-
ment puzzle and discuss techniques used by exist-
ing aligners to solve them. We use the term ‘unit’
to refer to both words and phrases in a snippet.
Figure 1 shows a shortened version of sentence
pair 712 in the MSR alignment corpus dev set
(Brockett, 2007) with related units aligned by hu-
man annotators. Evident from these alignments is
the fact that aligned units are typically semantically
similar or related. Existing aligners utilize a variety
of resources and techniques for computing simi-
larity between units: WordNet (MacCartney et al.,
2008; Thadani and McKeown, 2011), PPDB (Yao
et al., 2013b; Sultan et al., 2014a), distributional
similarity measures (MacCartney et al., 2008; Yao
et al., 2013b) and string similarity measures (Mac-
Cartney et al., 2008; Yao et al., 2013a). Recent
work on neural word embeddings (Mikolov et al.,
2013; Baroni et al., 2014) have advanced the state
of distributional similarity, but remain largely un-
explored in the context of alignment.
Lexical or phrasal similarity does not entail align-
ment, however. Consider function words: the align-
ment (5,4) in Figure 1 exists not just because
both units are the word a, but also because they
modify semantically equivalent units: jail and
police station. The influence of context on
content word alignment becomes salient particu-
larly in the presence of competing words. In Fig-
ure 1, (soldiers(10), troops(2)) arenot
aligned despite the two words’ semantic equiva-
lence in isolation, due to the presence of a compet-
ing pair, (armor(2), troops(2)), which is
a better fit in context.
The above examples reveal a second aligner re-
quirement: the ability to incorporate context into
similarity calculations. Existing supervised align-
ers use various contextual features within a learning
algorithm for this purpose. Such features include
both shallow surface measures (e.g., the relative
positions of the tokens being aligned in the respec-
tive sentences, similarities in the immediate left
or right words) (MacCartney et al., 2008; Thadani
and McKeown, 2011; Yao et al., 2013a) and syn-
tactic measures like typed dependencies (Thadani
and McKeown, 2011; Thadani et al., 2012). Sul-
tan et al. (2014a) design an unsupervised model
that more directly encodes context, via surface and
dependency-based neighbors which allow contex-
tual similarity to be represented as a weighted sum
of lexical similarity. But their model lacks a key
structural advantage of supervised models: to be
able to use an arbitrarily large feature set to robustly
encode lexical and/or contextual similarity.
The third and final key component of an aligner
is a mechanism to combine lexical/phrasal and con-
textual similarities to produce alignments. This
task is non-trivial due to the presence of cooperat-
ing and competing units. We first discuss compet-
ing units: semantically similar units in one snippet,
each of which is a potential candidate for alignment
with one or more units in the other snippet. At least
three different possible scenarios of varying diffi-
culty exist concerning such units:
</bodyText>
<listItem confidence="0.99847375">
• Scenario 1: No competing units. In Figure 1,
the aligned pair (British(1), UK(1))
represents this scenario.
• Scenario 2: Many-to-one competition: when
multiple units in one snippet are similar to
a single unit in the other snippet. In Fig-
ure 1, pairs (armors(2), troops(2))
and (soldiers(10), troops(2)) are
in such competition.
• Scenario 3: Many-to-many competition:
when similar units in one snippet have multi-
ple potential alignments in the other snippet.
</listItem>
<bodyText confidence="0.979621">
Groups of mutually cooperating units can also
exist where one unit provides supporting evidence
</bodyText>
<equation confidence="0.500864">
1 2 3 4 5 6 7 8 9
</equation>
<footnote confidence="0.672326466666667">
British 1
armor 2
crashed 3
into 4
a 5
jail 6
to 7
free 8
two 9
soldiers 10
arrested 11
by 12
Iraqi 13
police 14
. 15
</footnote>
<page confidence="0.99262">
950
</page>
<bodyText confidence="0.999706058823529">
for the alignment of other units in the group. Ex-
amples (besides named entities) include individual
words in one snippet that are grouped together in
the other snippet (e.g., state of the art H
state-of-the-art or headquarters in
Paris H Paris-based).
We briefly discuss the working principles of ex-
isting aligners to show how they respsond to these
challenges. MacCartney et al. (2008), Thadani
and McKeown (2011) and Thadani et al. (2012)
frame alignment as a set of phrase edit (insertion,
deletion and substitution) operations that transform
one snippet into the other. Each edit operation is
scored as a weighted sum of feature values (in-
cluding lexical and contextual similarity features),
and an optimal set of edits is computed. Yao et
al. (2013a; 2013b) take a sequence labeling ap-
proach: input snippets are considered sequences
of units and for each unit in one snippet, units in
the other snippet are considered potential labels.
A first order conditional random field is used for
prediction. Sultan et al. (2014a) treat alignment as
a bipartite matching problem and use a greedy al-
gorithm to perform one-to-one word alignment. A
weighted sum of two words’ lexical and contextual
similarities serves as the pair’s edge weight.
Noticeable in the designs of the supervised align-
ers is a lack of attention to the scenarios competing
units can pose – alignment of a unit depends only
on its own feature values. While the unsupervised
aligner by Sultan et al. (2014a) employs techniques
to deal with such scenarios, it allows only one-to-
one alignment, which fundamentally limits the set
of reachable alignments.
</bodyText>
<sectionHeader confidence="0.992007" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999221">
We primarily focus on word alignment, which Yao
et al. (2013b) report to cover more than 95% of
all alignments in multiple human-annotated cor-
pora. Named entities are the only phrasal units we
consider for alignment; in a later section we dis-
cuss how our techniques can be extended to general
phrasal alignment.
Figure 2 shows our two-stage logistic regres-
sion model. We address the first two challenges,
namely identifying lexical and contextual simi-
larities, in stage 1 of the model. Given input
</bodyText>
<equation confidence="0.92593">
text snippets T(1) = (T (1)
1 , ...,T(1)
n ) and T(2) =
(T(2)
1 , ..., T(2)
</equation>
<footnote confidence="0.66727475">
m ) where T(t)
k is the k-th word of snip-
pet T(t), the goal of this stage is to assign each
word pair of the form (T (1)
</footnote>
<equation confidence="0.630248">
i ,T (2)
</equation>
<bodyText confidence="0.538156">
j ) an alignment
</bodyText>
<figureCaption confidence="0.9965495">
Figure 2: Two-stage logistic regression for align-
ment. Stage 1 computes an alignment probability
</figureCaption>
<bodyText confidence="0.9307177">
0ij for each word pair based on local features f(1)
ij
and learned weights θ(1)
tij (see Section 4.1). Stage
2 assigns each pair a label Aij E {aligned, not
aligned} based on its own 0, the 0 of its cooperat-
ing and competing pairs, a max-weighted bipartite
matching Mφ with all 0 values as edge weights,
the semantic similarities Sw of the pair’s words and
words in all cooperating pairs, and learned weights
</bodyText>
<equation confidence="0.947922">
θ(2)
</equation>
<bodyText confidence="0.994709423076923">
tij for these global features.
probability 0ij, based on the pair’s lexical and con-
textual similarity features. We discuss our stage 1
features in Section 4.1.
We categorize each word along two different di-
mensions: (1) whether or not it is part of a named
entity, and (2) which of the following groups it
belongs to: content words, function words, and
punctuation marks. This distinction is important
because, (1) certain features apply only to certain
types of words (e.g., acronymy applies only to
named entities; punctuation marks do not partici-
pate in dependency relationships), and (2) certain
features can be more important for certain types of
words (e.g., the role of a function word depends
heavily on its surrounding words and therefore con-
textual features can be more important for function
words). Combined, the two above dimensions form
a domain of six possible values which can be repre-
sented as the Cartesian product {non-named entity,
named entity} x {content word, function word,
punctuation mark}. Each member of this set is a
word type in our model; for instance, named entity
function word is a word type.
This notion of types is then extended to word
pairs in T(1) x T(2): the type of pair (T (1)
</bodyText>
<equation confidence="0.815165875">
i ,T (2)
j )
is the union of the types of T (1)
i and T (2)
j . Given
𝜽(1)
𝑡
𝜽(1)
</equation>
<figure confidence="0.882230055555555">
𝑡
𝑖 𝑗
(1)
𝜽𝑡11
𝑛𝑚
Stage 1
1
𝒇11
(1)
𝒇𝑖𝑗 (1)
... ... 𝒇𝑛𝑚
𝜙11 ... 𝜙𝑖𝑘 ... 𝜙𝑖𝑗 ... 𝜙𝑙𝑗 ... 𝜙𝑛𝑚
Stage 2
𝑴𝜙
𝑺𝑤
(2)
𝜽𝑡𝑖𝑗
𝐴𝑖𝑗
</figure>
<page confidence="0.812476">
951
</page>
<bodyText confidence="0.8431005">
the pair’s stage 1 feature vector f(1) ijand the stage
1 weight vector θ(1)
tij for its type tij, we compute its
stage 1 alignment probability 0ij as:
The weight vector θ(1)
t for word pair type t is
</bodyText>
<equation confidence="0.970620357142857">
derived by minimizing the L1-regularized loss:
Nt
J(θ(1)) = −Nt t [yt
p
l
0
p
t
og(
(
(l
) +
(1 − y(p)t) log(1 − 0tp))] + λIlθ(1)
t Il1
</equation>
<bodyText confidence="0.990066428571429">
where Nt is the number of word pairs of type t
over all sentence pairs in the training data, y(p)
t is
the gold label for pair p of type t (1 = aligned,
0 = not aligned), and 0(p) tis its stage 1 alignment
probability.
Stage 2 of the model assigns the final alignment
</bodyText>
<equation confidence="0.783909">
label Aij E 10,1} to (T (1)
i ,T(2)
</equation>
<bodyText confidence="0.99980984">
j ). Like stage 1, it
uses L1-regularized logistic regression to compute
an alignment probability for each word pair, but
additionally assigns a final 0/1 label using a 0.5
threshold. Stage 2 factors in the stage 1 probabil-
ities of cooperating and competing pairs as well
as a maximum-weighted matching Mφ between
T(1) and T(2), where word pairs in T(1) x T(2)
are weighted by their stage 1 0 values. Such global
knowledge is useful in addressing cooperation and
competition among words. We describe our stage
2 features in Section 4.2.
The two stages are trained separately, each as
n standard logistic regression models where n is
the number of word pair types for which at least
one instance per class is observed in the training
data. The stage 1 models are first trained and used
to make predictions for each training sentence pair
(for each training pair, all other training pairs are
used to train the model). Given all the stage 1 align-
ment probabilities and the other stage 2 features,
the stage 2 models are then trained. At test time,
the two sets of trained models (i.e. stage 1 and
2 models) are successively applied to each input
sentence pair.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.886181166666666">
As mentioned above, we train a separate model
for each individual word pair type. Our feature
set is largely the same across word pair types, with
some differences. In the two following sections, we
discuss these features and indicate the associated
word pair types. We assume alignment of the two
</bodyText>
<equation confidence="0.7381435">
words T(1) iE T(1) and T (2)
j E T(2).
</equation>
<subsectionHeader confidence="0.9943875">
4.1 Stage 1: Assessing Pairs Individually
4.1.1 Word Similarity Features
</subsectionHeader>
<bodyText confidence="0.999393375">
Our first feature combines neural word embed-
dings, used previously for word similarity predic-
tion (Mikolov et al., 2013; Baroni et al., 2014),
with a paraphrase database (Ganitkevitch et al.,
2013). Our feature is the output of a ridge regres-
sion model trained on human annotations of word
similarity (Radinsky et al., 2011; Halawi et al.,
2012; Bruni et al., 2014) with two features: the
cosine similarity between the neural embedding
vectors of the two words (using a publicly avail-
able set of 400-dimensional word vectors (Baroni
et al., 2014)), and the presence/absence of the word
pair in the PPDB XXXL database. This regression
model produces similarities (sim henceforth) in
[0, 1], though we only consider similarities above
0.5 as lower scores are often noisy. To deal with
single-letter spelling errors, we consider T (1)
i and
T(2) jto be an exact match if exactly one of the two
is correctly spelled and their Levenshtein distance
is 1 (words of length &gt; 3 only).
We also use the following semantic and string
similarity features: a boolean feature that is 1 iff
one of T (1)
</bodyText>
<equation confidence="0.647191">
i and T (2)
</equation>
<bodyText confidence="0.999661416666667">
j is hyphenated and the other
is identical to a hyphen-delimited part of the first,
the same feature for highly similar (sim &gt; 0.9)
words, two features that show what proportion of
the characters of one word is covered by the other
if the latter is a prefix or a suffix of the former and
zero otherwise (length &lt; 3 words are discarded).
For named entities, we (1) consider acronymy
as exact match, (2) use membership in two lists of
alternative country names and country-nationality
pairs (from Wikipedia) as features, and (3) include
a feature that encodes whether T (1)
</bodyText>
<equation confidence="0.660932">
i and T (2)
</equation>
<bodyText confidence="0.9502498">
j be-
long to the same named entity (determined by one
mention containing all words of the other, e.g.,
Einstein and Albert Einstein).
)
</bodyText>
<subsubsectionHeader confidence="0.582666">
4.1.2 Contextual Features
</subsubsectionHeader>
<bodyText confidence="0.99991525">
Effective identification of contextual similarity
calls for a robust representation of word context in
a sentence. Our contextual features are based on
two different sentence representations. The word-
</bodyText>
<equation confidence="0.84581075">
0ij = −θt1)·f(1)
1 + e ijij
1
=
</equation>
<page confidence="0.864728">
952
</page>
<figure confidence="0.949819">
G¨unter Grass won the Nobel Prize.
G¨unter Grass won the Nobel Prize.
</figure>
<figureCaption confidence="0.979332">
Figure 3: Word and entity-based representations
of a sentence. Words in the same named entity are
grouped together in the latter representation.
</figureCaption>
<bodyText confidence="0.9995655625">
based representation treats each individual word
as a semantic unit whereas the entity-based rep-
resentation (1) groups together words in a multi-
word named entity, and (2) treats non-name words
as individual entities. Figure 3 shows an exam-
ple. The two representations are complementary –
the entity-based representation can capture equiva-
lences between mentions of different lengths of a
named entity, while the word-based representation
allows the use of similarity resources for named
entity words. Non-name words are treated iden-
tically. For simplicity we only discuss our word-
based features below, but each feature also has an
entity-based variant.
Dependency-based context. These features ap-
ply only if neither of T (1)
</bodyText>
<equation confidence="0.713102">
i and T (2)
</equation>
<bodyText confidence="0.956249">
j is a punctuation
mark. We compute the proportion of identical and
highly similar (sim &gt; 0.9) parents and children of
</bodyText>
<equation confidence="0.997796">
T (1)
i and T (2)
</equation>
<bodyText confidence="0.974552666666667">
j in the dependency trees of T(1) and
T(2) (Stanford collapsed dependencies (de Marn-
effe et al., 2006)). Equivalent dependency types
(Sultan et al., 2014a) are included in the above
computation, which encode semantic equivalences
between typed dependencies (e.g., nsubjpass and
dobj). We employ separate features for identicality
and similarity. Similar features are also computed
for a dependency neighborhood of size 2 (parents,
grandparents, children and grandchildren), where
we consider only content word neighbors.
Dependency neighbors of T (1)
</bodyText>
<equation confidence="0.733052">
i and T (2)
</equation>
<bodyText confidence="0.985016523809524">
j that
are less similar (0.9 &gt; sim &gt; 0.5; e.g., (gas,
energy) or (award, winner)) can also con-
tain useful semantic information for an aligner. To
accommodate this relatively large range of word
similarities, rather than counting such pairs, we find
a maximum-weighted bipartite matching of T (1)
i
and T(2) jneighbors in a neighborhood of size 2 us-
ing the primal-dual algorithm (content words only),
where word similarities across the two neighbor-
hoods serve as edge weights. We use as a feature
the sum of similarities between the matched neigh-
bors, normalized by the total number of content
words in the two neighborhoods.
Surface-form context. We draw several contex-
tual features from nearby words of T (1)
i and T (2)
j
in the surface forms of T(1) and T (2): (1) whether
the left and/or the right word/lemma is identical ,
</bodyText>
<listItem confidence="0.991205">
(2) whether the two are highly similar (sim &gt; 0.9),
(3) the longest common word/lemma sequence con-
</listItem>
<equation confidence="0.903156">
taining T (1)
i and T (2)
</equation>
<bodyText confidence="0.999905071428571">
j such that at least one word in
the sequence is a content word, (4) proportion of
identical and highly similar (sim &gt; 0.9) words in
a neighborhood of 3 content words to the left and
3 content words to the right; we use two versions
of this feature, one compares neighbors only in the
same direction (i.e. left with left, right with right)
and the other compares neighbors across the two di-
rections, (5) similarly to dependency-based context,
similarity in a max-weighted matching of all neigh-
bors with sim E [0.5, 0.9) in the above [−3,3]
window. For punctuation mark pairs, we use an
additional feature indicating whether or not they
both mark the end of their respective sentences.
</bodyText>
<subsectionHeader confidence="0.863503">
4.2 Stage 2: Cooperation and Competition
</subsectionHeader>
<bodyText confidence="0.999138285714286">
We consider two groups of mutually cooperating
words in a sentence: (1) words that belong to the
same named entity, and (2) words in a sentence
that are joined together to form a larger word in
the other sentence (e.g., state-of-the-art).
Speaking in terms of T (1)
i , the goal is to be able to
</bodyText>
<equation confidence="0.9655733">
use any evidence present for a (T(1)
k , T (2)
j ) align-
ment also as evidence for a (T (1)
i , T (2)
j ) alignment
if T (1)
i and T(1)
k both belong to such a group. We
call T (1)
i and T(1)
k mutually cooperating words
with respect to T (2)
j in such cases. Any word
T (1)
l E T(1) which is not a cooperating word for
T (1)
i is a competing word: a word that can poten-
tially make (T (1)
i , T (2)
</equation>
<bodyText confidence="0.985991">
j ) a less viable alignment by
having a larger stage 1 alignment probability in
</bodyText>
<equation confidence="0.726662">
(Tl(1),T(2)j). We call a pair (T(1)
k , T (2)
</equation>
<bodyText confidence="0.557063">
j ) a cooper-
</bodyText>
<equation confidence="0.930756833333333">
ating (competing) pair for (T (1)
i , T (2)
j ) if T(k1) is a
cooperating (competing) word for T (1)
i with respect
to T (2)
</equation>
<bodyText confidence="0.9701905">
j . With a reversal of word order and appro-
priate substitution of indexes, the above discussion
</bodyText>
<figure confidence="0.982533">
nn nsubj
dobj
det
nn
nsubj det
dobj
</figure>
<page confidence="0.94073">
953
</page>
<bodyText confidence="0.871332428571429">
equally holds for T (2)
j .
Given sets of stage 1 probabilities Φcop
ij and
Φcmp
ij of cooperating and competing pairs for
the pair (T (1)
</bodyText>
<equation confidence="0.930085">
i ,T (2)
</equation>
<bodyText confidence="0.9964655">
j ), we employ three features
to deal with scenario 2 of Section 2: (1)
max(φij, max(Φcop
ij )): the greater of the pair’s
own stage 1 alignment probability and the high-
est among all cooperating pair probabilities, (2)
max(Φcmp
ij ): the highest of all competing pair
probabilities, and (3) a binary feature indicating
which of the two above is larger.
To address scenario 3, we construct a weighted
bipartite graph: nodes represent words in T(1) and
T(2) and the weight of each edge represents the
stage 1 alignment probability of a word pair in
T(1) × T(2). We find a max-weighted bipartite
matching Mφ of word pairs in this graph. For each
word pair, we employ a feature indicating whether
or not it is in Mφ. The presence of (Ti(1),T(2)j)
and (T(1)k,Tl(2)) in Mφ, where all four words are
similar, is a potential indicator that (T (1)
</bodyText>
<equation confidence="0.99186375">
i ,T (2)
l ) and
(T(1)
k , T (2)
</equation>
<bodyText confidence="0.997577555555556">
j ) are no longer viable alignments.
Low recall has traditionally been the primary
weakness of supervised aligners (as we later show
in Table 1). Our observation of the aligner’s be-
havior on the dev set of the MSR alignment cor-
pus (Brockett, 2007) suggests that this happens
primarily due to highly similar word pairs being
left unaligned even in the absence of competing
pairs because of relatively low contextual evidence.
Consequently, aligner performance suffers in sen-
tences with few common or similar words. To
promote high recall, we employ the higher of a
word pair’s own lexical similarity and the lexical
similarity of the cooperating pair with the highest
stage 1 probability as a stage 2 feature.
The stage 2 feature set is identical across word
pair types, but as in stage 1, we train individual
models for different pair types.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999212">
5.1 System Evaluation
</subsectionHeader>
<bodyText confidence="0.999927">
We report evaluation on two alignment data sets
and extrinsic evaluation on two tasks: sentence
similarity identification and paraphrase detection.
</bodyText>
<subsectionHeader confidence="0.508883">
5.1.1 Alignment
</subsectionHeader>
<bodyText confidence="0.996106666666667">
We adopt the evaluation procedure for aligners re-
ported in prior work (MacCartney et al., 2008;
Thadani and McKeown, 2011; Yao et al., 2013a).
</bodyText>
<table confidence="0.5370485">
Aligner P % R % Fi % E %
MacCartney et al. (2008) 85.4 85.3 85.3 21.3
Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0
R Yao et al. (2013a) 93.7 84.0 88.6 35.3
Yao et al. (2013b) 92.1 82.8 86.8 29.1
Sultan et al. (2014a) 93.7 89.8 91.7 43.8
Our Aligner 95.4 89.0 92.1 47.3
Thadani et al. (2012) 76.6 83.8 79.2 12.2
++ Yao et al. (2013a) 91.3 82.0 86.4 15.0
B Yao et al. (2013b) 90.4 81.9 85.9 13.7
E Sultan et al. (2014a) 93.5 82.5 87.6 18.3
Our Aligner 92.1 85.2 88.5 18.3
</table>
<tableCaption confidence="0.976719">
Table 1: Performance on two alignment data sets.
Improvements in F1 are statistically significant.
</tableCaption>
<bodyText confidence="0.998234666666667">
Data. The MSR alignment corpus (Brockett,
2007) contains 800 dev and 800 test sentence pairs
from the PASCAL RTE 2006 challenge. Each pair
is aligned by three human annotators; Fleiss Kappa
agreement of about 0.73 (“substantial agreement”)
is reported on both sets. Following prior work, we
only consider the sure alignments, take the majority
opinion on each word pair, and leave out three-way
disagreements.
The Edinburgh++ corpus (Thadani et al., 2012)
contains 714 training and 306 test sentence pairs.
Each test pair is aligned by two annotators and the
final gold alignments consist of a random but even
selection of the two sets of annotations.
Evaluation metrics. Our primary evaluation
metrics are macro-averaged precision (P), recall
(R) and F1 score. A fourth metric E measures the
proportion of sentence pairs for which the system
alignments are identical to the gold alignments.
Model setup. For each corpus, we train our
model using the dev set and evaluate on the test set.
We use the logistic regression implementation of
Scikit-learn (Pedregosa et al., 2011) and use leave-
one-out cross-validation on the dev pairs to set the
regularization parameter C.
Results. Table 1 shows the performance of dif-
ferent aligners on the two test sets. Our aligner
demonstrates the best overall performance in terms
of both F1 and E. Wilcoxon signed-rank tests
(with Pratt’s treatment for zero-difference pairs)
show that the improvements in F1 over the previous
best aligner (Sultan et al., 2014a) are statistically
significant at p &lt; 0.01 for both test sets.
</bodyText>
<subsectionHeader confidence="0.729223">
5.1.2 Identification of Sentence Similarity
</subsectionHeader>
<bodyText confidence="0.998423">
Given two input sentences, the goal in this task,
known also as Semantic Textual Similarity (STS),
is to output a real-valued semantic similarity score.
</bodyText>
<page confidence="0.997213">
954
</page>
<table confidence="0.9887958">
System Pearson’s r Rank
Han et al. (2013) 73.7 1
Yao et al. (2013a) 46.2 66
Sultan et al. (2014a) 67.2 7
Our Aligner 67.8 4
</table>
<tableCaption confidence="0.745373">
Table 2: STS results. Performances of past systems
are reported by Sultan et al. (2014a).
</tableCaption>
<bodyText confidence="0.999646194444444">
Data. To be able to directly compare with past
aligners, we select three data sets (headlines: pairs
of news headlines; OnWN, FNWN: gloss pairs)
from the 2013 *SEM STS corpus (Agirre et al.,
2013), containing 1500 sentence pairs in total. Sul-
tan et al. (2014a) reports the performance of two
state-of-the-art aligners on these pairs.
Evaluation metric. At SemEval, STS systems
output a similarity score in [0, 5]. For each individ-
ual test set, the Pearson product-moment correla-
tion coefficient (Pearson’s r) is computed between
system scores and human annotations. The final
evaluation metric is a weighted sum of r’s over
all test sets, where the weight assigned to a set is
proportional to its number of pairs.
Method. Being a logistic regression model,
stage 2 of our aligner assigns each word pair an
alignment probability. For STS, we compute a
length-normalized sum of alignment probabilities
of content word pairs across the two sentences. We
include all pairs with probability &gt; 0.5; the re-
maining pairs are included in decreasing order of
their probabilities and already included words are
ignored. Following (Sultan et al., 2014a), we nor-
malize by dividing with the harmonic mean of the
numbers of content words in the two sentences.
Results. Table 2 shows the performance of dif-
ferent aligners on the three STS 2013 test sets. We
also show the performance of the contest-winning
system (Han et al., 2013). Our STS system demon-
strates a weighted correlation of 67.8%, which is
better than similar STS systems based on the two
previous best aligners. The difference with the next
best aligner is statistically significant at p &lt; 0.05
(two-sample one-tailed z-test). Overall, our system
outperforms 86 of the 89 participating systems.
</bodyText>
<subsubsectionHeader confidence="0.648016">
5.1.3 Paraphrase Detection
</subsubsectionHeader>
<bodyText confidence="0.9974706">
Given two input sentences, the goal in this task is
to determine if their meanings are the same.
Data. The MSR paraphrase corpus (Dolan et al.,
2004) contains 4076 dev and 1725 test sentence
pairs; a paraphrase label (true/false) for each pair
</bodyText>
<table confidence="0.989056666666667">
System A % P % R % F, %
Madnani et al. (2012) 77.4 79.0 89.9 84.1
Yao et al. (2013a) 70.0 72.6 88.1 79.6
Yao et al. (2013b) 68.1 68.6 95.8 79.9
Sultan et al. (2014a) 73.4 76.6 86.4 81.2
Our Aligner 73.2 75.3 88.8 81.5
</table>
<tableCaption confidence="0.9557085">
Table 3: Paraphrase results. Performances of past
systems are taken from (Sultan et al., 2014a).
</tableCaption>
<bodyText confidence="0.9976885">
is provided by human annotators.
Evaluation Metrics. We report performance in
terms of: (1) accuracy in classifying the sentences
into true and false classes (A), and (2) true class
precision (P), recall (R) and F1 score.
Method. Following prior aligners (MacCartney
et al., 2008; Yao et al., 2013b; Sultan et al., 2014a),
we output a true decision for a test sentence pair iff
the length-normalized alignment score for the pair
exceeds a threshold derived from the dev set.
Results. The top row of Table 3 shows the best
result by any system on the MSR test set. Among
all aligners (all other rows), ours achieves the best
F1 score and the second best accuracy.
We report paraphrase detection results primarily
to allow comparison with past aligners. However,
this simplistic application to a complex task only
gives a ballpark estimate of an aligner’s quality.
</bodyText>
<table confidence="0.9964632">
EDB++ MSR Model P % R % F1 % E %
Two-Stage Model 95.4 89.0 92.1 47.3
Stage 1 Only 92.9 85.6 89.1 28.0
Two-Stage Model 92.1 85.2 88.5 18.3
Stage 1 Only 93.0 79.0 85.4 13.7
</table>
<tableCaption confidence="0.999922">
Table 4: Performance with and without stage 2.
</tableCaption>
<subsectionHeader confidence="0.999375">
5.2 Ablation
</subsectionHeader>
<bodyText confidence="0.999219">
We perform ablation tests to find out how important
(1) the two-stage framework, and (2) the different
features are for our aligner.
</bodyText>
<subsectionHeader confidence="0.648813">
5.2.1 Results without Stage 2
</subsectionHeader>
<bodyText confidence="0.9998803">
Stage 1 of our aligner can operate as an aligner by
itself by mapping each alignment probability to a
0/1 alignment decision based on a threshold of 0.5.
From a design perspective, this is an aligner that
does not address scenarios 2 and 3 of Section 2.
The performance of the aligner with and without
stage 2 is shown in Table 4. On each test set, the F1
and E scores increase with the addition of stage 2.
On the MSR test set, performance improves along
all dimensions. On the Edinburgh++ test set, the
</bodyText>
<page confidence="0.996016">
955
</page>
<table confidence="0.9700691">
MSR EDB++
Features P % R % F1 % P % R % F1 %
All Features 95.4 89.0 92.1 92.1 85.2 88.5
- Lexical 95.1 82.8 88.5 90.9 84.0 87.3
- Resources 96.0 87.0 91.3 92.2 84.3 88.1
- Contextual 89.0 79.2 83.9 89.9 66.3 76.3
- Dependency 95.3 88.2 91.6 91.9 84.9 88.3
- Surface 94.4 85.6 89.8 90.6 76.9 83.2
- Word-Based 94.6 87.7 91.0 92.0 85.1 88.4
- Entity-Based 95.5 89.0 92.1 92.1 85.1 88.5
</table>
<tableCaption confidence="0.999574">
Table 5: Results without different stage 1 features.
</tableCaption>
<table confidence="0.981751333333333">
MSR EDB++
Features P % R % F1 % P % R % F1 %
All Features 95.4 89.0 92.1 92.1 85.2 88.5
- φ values 87.3 90.7 88.9 86.4 86.9 86.7
- Matching 95.3 87.9 91.5 92.3 84.6 88.3
- Word Sim 95.5 88.4 91.8 92.7 84.7 88.5
</table>
<tableCaption confidence="0.999915">
Table 6: Results without different stage 2 features.
</tableCaption>
<bodyText confidence="0.915493666666667">
precision drops a little, but this effect is offset by a
larger improvement in recall. These results show
that stage 2 is central to the aligner’s success.
</bodyText>
<subsectionHeader confidence="0.737215">
5.2.2 Without Different Stage 1 Features
</subsectionHeader>
<bodyText confidence="0.99982832">
We exclude different stage 1 features (which fall
into one of two groups: lexical and contextual) and
examine the resulting model’s performance. Table
5 shows the results. The subtraction sign represents
the exclusion of the corresponding feature.
Without any lexical feature (i.e., if the model re-
lies only on contextual features), both precision and
recall decrease, resulting in a considerable overall
performance drop. Exclusion of word similarity
resources (i.e. embeddings and PPDB) improves
precision, but again harms overall performance.
Without any contextual features, the model suf-
fers badly in both precision and recall. The extreme
overall performance degradation indicates that con-
textual features are more important for the aligner
than lexical features. Leaving out surface-form
neighbors results in a larger performance drop than
when dependency-based neighbors are excluded,
pointing to a more robust role of the former group
in representing context. Finally, our entity-based
representation of context neither helps nor harms
system performance, but relying only on entity-
based neighbors has detrimental effects. Factoring
in semantic similarities of named entities should
improve the utility of these features.
</bodyText>
<subsectionHeader confidence="0.49382">
5.2.3 Without Different Stage 2 Features
</subsectionHeader>
<bodyText confidence="0.9337235">
Table 6 shows the aligner’s performance after the
exclusion of different stage 2 features. Leaving out
</bodyText>
<figureCaption confidence="0.882993333333333">
Figure 4: % distribution of aligned word pair types;
nne: non-named entity, ne: named entity, c: content
word, f : function word, p: punctuation mark.
</figureCaption>
<table confidence="0.9796725">
MSR EDB++
Pair Type P % R % F1 % P % R % F1 %
{nne-c, nne-c} 95.7 84.3 89.7 92.2 89.2 90.7
{nne-c, nne-f} 100.0 2.7 5.3 61.4 7.7 13.6
{nne-c, ne-c} 89.2 66.7 76.3 71.9 43.4 54.1
{nne-f, nne-f } 90.7 86.0 88.3 93.4 86.5 89.8
{nne-p, nne-p} 99.4 99.2 99.3 93.0 91.5 92.2
{ne-c, ne-c} 96.2 97.8 97.0 90.9 94.2 92.6
</table>
<tableCaption confidence="0.999671">
Table 7: Performance on different word pair types.
</tableCaption>
<bodyText confidence="0.999873285714286">
the stage 1 alignment probabilities harms overall
performance the most by causing a large drop in
precision. Exclusion of the maximum-weighted
bipartite matching feature results in worse recall
and overall performance. The lexical similarity
feature improves overall results only on the MSR
test set but increases recall on both test sets.
</bodyText>
<subsectionHeader confidence="0.98454">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999969272727273">
We examine the aligner’s performance on different
word pair types. Figure 4 shows the % distribution
of word pair types with at least 20 aligned instances
in at least one test set. These six types account for
more than 99% of all alignments in both test sets.
Table 7 shows the results. We ignore punctua-
tion mark pairs in the following discussion. Per-
formance is worst on the two rarest types: {nne-c,
nne-f } and {nne-c, ne-c}, due primarily to very
low recall. A relatively low availability of posi-
tive examples in the training sets (in the hundreds,
in contrast to thousands of examples for each of
the other three types) is a primary factor affect-
ing classifier performance on these two pair types.
The {nne-c, nne-f } pairs, nonetheless, are intrin-
sically the most difficult type for a word aligner
because they occur frequently as part of phrasal
alignments. On {nne-c, ne-c} pairs, errors also oc-
cur due to failure in recognition of certain named
entity types (e.g., acronyms and multiword named
entities) and the aligner’s lack of world knowledge
(e.g., in daughter H Chelsea).
</bodyText>
<figure confidence="0.955885142857143">
50
45
40
35
30
25
20
15
10
5
0
MSR EDB++
{nne-c, {nne-c, {nne-c, {nne-f, {nne-p, {ne-c,
nne-c} nne-f} ne-c} nne-f} nne-p} ne-c}
</figure>
<page confidence="0.996051">
956
</page>
<bodyText confidence="0.999984">
Low recall remains the primary issue, albeit to a
much lesser extent, for {nne-c, nne-c} and {nne-f,
nne-f } pairs. For the former, two major sources
of error are: (1) inability to utilize contextual ev-
idence outside the local neighborhood examined
by the aligner, and (2) failure to address one-to-
many alignments. Low recall for the latter follows
naturally, as function word alignment is heavily
dependent on related content word alignment. On
{ne-c, ne-c} pairs the aligner performs the best, but
still suffers from the two above issues.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999888833333333">
We mentioned major standalone monolingual align-
ers and briefly discussed their working principles
in Section 2. There are, however, at least two ad-
ditional groups of related work which can inform
future research on monolingual alignment. First,
alignment is often performed in the context of ex-
trinsic tasks, e.g., textual entailment recognition
(Wang and Manning, 2010), question answering
(Heilman and Smith, 2010), discourse generation
(Roth and Frank, 2012) and redundancy detection
(Thadani and McKeown, 2008). Such systems may
contain useful design elements yet to be utilized by
standalone aligners. Second, a large body of work
exists in the bilingual alignment literature (Och and
Ney, 2003; Blunsom and Cohn, 2006; Chang et al.,
2014), elements of which (such as the machine
learning models) can be useful for monolingual
aligners (see (Yao et al., 2013a) for an example).
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99870847368421">
We present a two-stage classification framework
for monolingual alignment that demonstrates top
results in intrinsic and extrinsic evaluation experi-
ments. While our work focuses primarily on word
alignment, given a mechanism to compute phrasal
similarity, the notion of cooperating words can be
exploited to extend our model for phrasal align-
ment. Another important future direction is the con-
struction of a robust representation of context, as
our model currently utilizes contextual information
only within a local neighborhood of a predefined
size and therefore fails to utilize long-distance se-
mantic relationships between words. Incorporating
a single background model which is trained on all
word pair types might also improve performance,
especially on types that are rare in the training data.
Finally, studying the explicit requirements of dif-
ferent extrinsic tasks can shed light on the design
of a robust aligner.
</bodyText>
<sectionHeader confidence="0.996056" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998088666666667">
This material is based in part upon work supported
by the National Science Foundation under Grant
Numbers EHR/0835393 and EHR/0835381.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998526555555556">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, *SEM ’13, pages 32-43, At-
lanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. SemEval-2015 Task 2: Semantic Tex-
tual Similarity, English, Spanish and Pilot on Inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation, pages 252-263,
Denver, Colorado, USA.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t Count, Predict! A
Systematic Comparison of Context-Counting vs.
Context-Predicting Semantic Vectors. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ’14, pages
238-247, Baltimore, Maryland, USA.
Phil Blunsom and Trevor Cohn. 2006. Discrimi-
native Word Alignment with Conditional Random
Fields. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the ACL, pages 65-72, Sydney, Aus-
tralia.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal Distributional Semantics. Journal ofAr-
tificial Intelligence Research, vol. 49, pages 1-47.
Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A Constrained Viterbi
Relaxation for Bidirectional Word Alignment. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics, pages
14811490, Baltimore, Maryland, USA.
Ido Dagan and Oren Glickman. 2004. Probabilistic
Textual Entailment: Generic Applied Modeling of
Language Variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
</reference>
<page confidence="0.989284">
957
</page>
<reference confidence="0.990996821428571">
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
Identification as Probabilistic Quasi-Synchronous
Recognition. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 468-476,
Singapore.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation, pages 449-454,
Genoa, Italy.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chlo Kid-
don, and Christopher D. Manning. 2007. Aligning
Semantic Graphs for Textual Inference and Machine
Reading. In Proceedings of the AAAI Spring Sympo-
sium, pages 468-476, Stanford, California, USA.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase Cor-
pora: Exploiting Massively Parallel News Sources.
In Proceedings of the International Conference on
Computational Linguistics, pages 350-356, Geneva,
Switzerland.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 758-764, Atlanta,
Georgia, USA.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-Scale Learning of
Word Relatedness with Constraints. In Proceedings
of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1406-1414, Beijing, China.
Lushan Han, Abhay Kashyap, Tim Finin, James
Mayfield, and Jonathan Weese. 2013. UMBC
EBIQUITY-CORE: Semantic Textual Similarity
Systems. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics,
*SEM ’13, pages 44-52, Atlanta, Georgia, USA.
Michael Heilman and Noah A. Smith. 2010. Tree
Edit Models for Recognizing Textual Entailments,
Paraphrases, and Answers to Questions. In Pro-
ceedings of the 2010 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
10111019, Los Angeles, California, USA.
Andrew Hickl and Jeremy Bensley. 2007. A Dis-
course Commitment-Based Framework for Recog-
nizing Textual Entailment. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 171-176, Prague, Czech Re-
public.
Ben Hixon, Peter Clark, and Hannaneh Hajishirzi.
2015. Learning Knowledge Graphs for Question
Answering through Conversational Dialog. In Pro-
ceedings of the the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Denver, Colorado, USA.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A Phrase-Based Alignment Model
for Natural Language Inference. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 802-811, Honolulu,
Hawaii, USA.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics
for Paraphrase Identification. In Proceedings of
2012 Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 182-190, Montreal, Canada.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of the
International Conference on Learning Representa-
tions Workshop, Scottsdale, Arizona, USA.
Michael Mohler, Razvan Bunescu, and Rada Mihal-
cea. 2011. Learning to Grade Short Answer Ques-
tions Using Semantic Similarity Measures and De-
pendency Graph Alignments. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics, pages 752-762, Portland, Ore-
gon, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):1951,
MIT Press.
Sebastian Pad´o, Michel Galley, Dan Jurafsky, and
Chris Manning. 2009. Robust Machine Transla-
tion Evaluation with Entailment Features. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 297-305, Singapore.
Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2015. Design and Realization
of a Modular Architecture for Textual Entailment.
Natural Language Engineering, 21 (2), pages 167-
200, Cambridge University Press.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, vol. 12, pages 2825-
2830.
</reference>
<page confidence="0.979261">
958
</page>
<reference confidence="0.999745822580645">
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
Word at a Time: Computing Word Relatedness
using Temporal Semantic Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computa-
tional Natural Language Learning, pages 337-346,
Hyderabad, India.
Michael Roth and Anette Frank. 2012. Aligning Pred-
icates across Monolingual Comparable Texts using
Graph-based Clustering. In Proceedings of the 20th
International Conference on World Wide Web, pages
pages 171182, Jeju Island, Korea.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to Basics for Monolingual Align-
ment: Exploiting Word Similarity and Contextual
Evidence. Transactions of the Association for Com-
putational Linguistics, 2 (May), pages 219-230.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014b. DLS@CU: Sentence Similarity from Word
Alignment. In Proceedings of the 8th International
Workshop on Semantic Evaluation, pages 241-246,
Dublin, Ireland.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2015. DLS@CU: Sentence Similarity from Word
Alignment and Semantic Vector Composition. In
Proceedings of the 9th International Workshop on
Semantic Evaluation, pages 148-153, Denver, Col-
orado, USA.
Kapil Thadani and Kathleen McKeown. 2008. A
Framework for Identifying Textual Redundancy.
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 873880,
Manchester, UK.
Kapil Thadani and Kathleen McKeown. 2011. Opti-
mal and Syntactically-Informed Decoding for Mono-
lingual Phrase-Based Alignment. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 254-259, Portland,
Oregon, USA.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A Joint Phrasal and Dependency Model for Para-
phrase Alignment. In Proceedings of COLING
2012, pages 1229-1238, Mumbai, India.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic Tree-Edit Models with Structured La-
tent Variables for Textual Entailment and Ques-
tion Answering. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 11641172, Beijing, China.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013a. A Lightweight and
High Performance Monolingual Word Aligner. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 702-
707, Sofia, Bulgaria.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013b. Semi-Markov
Phrase-based Monolingual Alignment. In Proceed-
ings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 590-600,
Seattle, Washington, USA.
</reference>
<page confidence="0.998771">
959
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696401">
<title confidence="0.992245">Feature-Rich Two-Stage Logistic Regression for Monolingual Alignment</title>
<author confidence="0.741241">Arafat Steven Tamara of Cognitive Science</author>
<author confidence="0.741241">Department of Computer</author>
<affiliation confidence="0.992824666666667">University of Colorado of Computer and Information University of Alabama at</affiliation>
<email confidence="0.997444">arafat.sultan@colorado.edu,bethard@cis.uab.edu,sumner@colorado.edu</email>
<abstract confidence="0.997841944444445">Monolingual alignment is the task of pairing semantically similar units from two pieces of text. We report a top-performing supervised aligner that operates on short text snippets. We employ a large feature set to (1) encode similarities among semantic units (words and named entities) in context, and (2) address cooperation and competition for alignment among units in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, aligner achieves of statistically significant erreductions of the previous best aligner. It produces top results in extrinsic evaluation as well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>Shared Task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="26331" citStr="Agirre et al., 2013" startWordPosition="4490" endWordPosition="4493">st sets. 5.1.2 Identification of Sentence Similarity Given two input sentences, the goal in this task, known also as Semantic Textual Similarity (STS), is to output a real-valued semantic similarity score. 954 System Pearson’s r Rank Han et al. (2013) 73.7 1 Yao et al. (2013a) 46.2 66 Sultan et al. (2014a) 67.2 7 Our Aligner 67.8 4 Table 2: STS results. Performances of past systems are reported by Sultan et al. (2014a). Data. To be able to directly compare with past aligners, we select three data sets (headlines: pairs of news headlines; OnWN, FNWN: gloss pairs) from the 2013 *SEM STS corpus (Agirre et al., 2013), containing 1500 sentence pairs in total. Sultan et al. (2014a) reports the performance of two state-of-the-art aligners on these pairs. Evaluation metric. At SemEval, STS systems output a similarity score in [0, 5]. For each individual test set, the Pearson product-moment correlation coefficient (Pearson’s r) is computed between system scores and human annotations. The final evaluation metric is a weighted sum of r’s over all test sets, where the weight assigned to a set is proportional to its number of pairs. Method. Being a logistic regression model, stage 2 of our aligner assigns each wor</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task: Semantic Textual Similarity. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 32-43, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation,</booktitle>
<pages>252--263</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="1366" citStr="Agirre et al., 2015" startWordPosition="190" endWordPosition="193">its in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 201</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation, pages 252-263, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>238--247</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="5496" citStr="Baroni et al., 2014" startWordPosition="860" endWordPosition="863">kett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word a, but also because they modify semantically equivalent units: jail and police station. The influence of context on content word alignment becomes salient particularly in the presence of competing words. In Figure 1, (soldiers(10), troops(2)) arenot aligned despite the two words’ semantic equivalence in isolation,</context>
<context position="15022" citStr="Baroni et al., 2014" startWordPosition="2516" endWordPosition="2519">ls) are successively applied to each input sentence pair. 4 Features As mentioned above, we train a separate model for each individual word pair type. Our feature set is largely the same across word pair types, with some differences. In the two following sections, we discuss these features and indicate the associated word pair types. We assume alignment of the two words T(1) iE T(1) and T (2) j E T(2). 4.1 Stage 1: Assessing Pairs Individually 4.1.1 Word Similarity Features Our first feature combines neural word embeddings, used previously for word similarity prediction (Mikolov et al., 2013; Baroni et al., 2014), with a paraphrase database (Ganitkevitch et al., 2013). Our feature is the output of a ridge regression model trained on human annotations of word similarity (Radinsky et al., 2011; Halawi et al., 2012; Bruni et al., 2014) with two features: the cosine similarity between the neural embedding vectors of the two words (using a publicly available set of 400-dimensional word vectors (Baroni et al., 2014)), and the presence/absence of the word pair in the PPDB XXXL database. This regression model produces similarities (sim henceforth) in [0, 1], though we only consider similarities above 0.5 as l</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t Count, Predict! A Systematic Comparison of Context-Counting vs. Context-Predicting Semantic Vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 238-247, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative Word Alignment with Conditional Random Fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>65--72</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="35875" citStr="Blunsom and Cohn, 2006" startWordPosition="6100" endWordPosition="6103">Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is the construction of a ro</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative Word Alignment with Conditional Random Fields. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65-72, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<contexts>
<context position="3701" citStr="Brockett, 2007" startWordPosition="574" endWordPosition="576">mpetition for alignment with a word in the other snippet, whereas words that constitute a phrase can provide supporting evidence for one another (e.g. in named entity alignments such as Watson ↔ John Hamish Watson). To handle such interdependencies, we employ a two-stage logistic regression model – stage 1 computes an alignment probability for each word pair based solely on its own feature values, and stage 2 assigns the eventual alignment labels to all pairs following a comparative assessment of stage 1 probabilities of cooperating and competing pairs. On two alignment data sets reported in (Brockett, 2007) and (Thadani et al., 2012), our aligner demonstrates respective F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner (Sultan et al., 2014a). We also present extrinsic evaluation of the aligner within two text comparison tasks, namely sentence similarity identification and paraphrase detection, where it demonstrates state-of-the-art results. 949 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 949–959, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational</context>
<context position="22652" citStr="Brockett, 2007" startWordPosition="3876" endWordPosition="3877"> 1 alignment probability of a word pair in T(1) × T(2). We find a max-weighted bipartite matching Mφ of word pairs in this graph. For each word pair, we employ a feature indicating whether or not it is in Mφ. The presence of (Ti(1),T(2)j) and (T(1)k,Tl(2)) in Mφ, where all four words are similar, is a potential indicator that (T (1) i ,T (2) l ) and (T(1) k , T (2) j ) are no longer viable alignments. Low recall has traditionally been the primary weakness of supervised aligners (as we later show in Table 1). Our observation of the aligner’s behavior on the dev set of the MSR alignment corpus (Brockett, 2007) suggests that this happens primarily due to highly similar word pairs being left unaligned even in the absence of competing pairs because of relatively low contextual evidence. Consequently, aligner performance suffers in sentences with few common or similar words. To promote high recall, we employ the higher of a word pair’s own lexical similarity and the lexical similarity of the cooperating pair with the highest stage 1 probability as a stage 2 feature. The stage 2 feature set is identical across word pair types, but as in stage 1, we train individual models for different pair types. 5 Exp</context>
<context position="24206" citStr="Brockett, 2007" startWordPosition="4143" endWordPosition="4144">ligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The MSR alignment corpus (Brockett, 2007) contains 800 dev and 800 test sentence pairs from the PASCAL RTE 2006 challenge. Each pair is aligned by three human annotators; Fleiss Kappa agreement of about 0.73 (“substantial agreement”) is reported on both sets. Following prior work, we only consider the sure alignments, take the majority opinion on each word pair, and leave out three-way disagreements. The Edinburgh++ corpus (Thadani et al., 2012) contains 714 training and 306 test sentence pairs. Each test pair is aligned by two annotators and the final gold alignments consist of a random but even selection of the two sets of annotati</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 Corpus. Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal Distributional Semantics.</title>
<date>2014</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>49</volume>
<pages>1--47</pages>
<contexts>
<context position="15246" citStr="Bruni et al., 2014" startWordPosition="2554" endWordPosition="2557">erences. In the two following sections, we discuss these features and indicate the associated word pair types. We assume alignment of the two words T(1) iE T(1) and T (2) j E T(2). 4.1 Stage 1: Assessing Pairs Individually 4.1.1 Word Similarity Features Our first feature combines neural word embeddings, used previously for word similarity prediction (Mikolov et al., 2013; Baroni et al., 2014), with a paraphrase database (Ganitkevitch et al., 2013). Our feature is the output of a ridge regression model trained on human annotations of word similarity (Radinsky et al., 2011; Halawi et al., 2012; Bruni et al., 2014) with two features: the cosine similarity between the neural embedding vectors of the two words (using a publicly available set of 400-dimensional word vectors (Baroni et al., 2014)), and the presence/absence of the word pair in the PPDB XXXL database. This regression model produces similarities (sim henceforth) in [0, 1], though we only consider similarities above 0.5 as lower scores are often noisy. To deal with single-letter spelling errors, we consider T (1) i and T(2) jto be an exact match if exactly one of the two is correctly spelled and their Levenshtein distance is 1 (words of length </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal Distributional Semantics. Journal ofArtificial Intelligence Research, vol. 49, pages 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Alexander M Rush</author>
<author>John DeNero</author>
<author>Michael Collins</author>
</authors>
<title>A Constrained Viterbi Relaxation for Bidirectional Word Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>14811490</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="35896" citStr="Chang et al., 2014" startWordPosition="6104" endWordPosition="6107">wever, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is the construction of a robust representation o</context>
</contexts>
<marker>Chang, Rush, DeNero, Collins, 2014</marker>
<rawString>Yin-Wen Chang, Alexander M. Rush, John DeNero, and Michael Collins. 2014. A Constrained Viterbi Relaxation for Bidirectional Word Alignment. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 14811490, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic Textual Entailment: Generic Applied Modeling of Language Variability.</title>
<date>2004</date>
<booktitle>In Proceedings of the PASCAL Workshop on Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble, France.</location>
<contexts>
<context position="1452" citStr="Dagan and Glickman, 2004" startWordPosition="203" endWordPosition="206">ession framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an expl</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic Textual Entailment: Generic Applied Modeling of Language Variability. In Proceedings of the PASCAL Workshop on Learning Methods for Text Understanding and Mining, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="1287" citStr="Das and Smith, 2009" startWordPosition="179" endWordPosition="182"> in context, and (2) address cooperation and competition for alignment among units in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thad</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 468-476, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the International Conference on Language Resources and Evaluation, pages 449-454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Trond Grenager</author>
<author>Bill MacCartney</author>
<author>Daniel Cer</author>
<author>Daniel Ramage</author>
<author>Chlo Kiddon</author>
<author>Christopher D Manning</author>
</authors>
<title>Aligning Semantic Graphs for Textual Inference and Machine Reading.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium,</booktitle>
<pages>468--476</pages>
<location>Stanford, California, USA.</location>
<marker>de Marneffe, Grenager, MacCartney, Cer, Ramage, Kiddon, Manning, 2007</marker>
<rawString>Marie-Catherine de Marneffe, Trond Grenager, Bill MacCartney, Daniel Cer, Daniel Ramage, Chlo Kiddon, and Christopher D. Manning. 2007. Aligning Semantic Graphs for Textual Inference and Machine Reading. In Proceedings of the AAAI Spring Symposium, pages 468-476, Stanford, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="28055" citStr="Dolan et al., 2004" startWordPosition="4771" endWordPosition="4774">ners on the three STS 2013 test sets. We also show the performance of the contest-winning system (Han et al., 2013). Our STS system demonstrates a weighted correlation of 67.8%, which is better than similar STS systems based on the two previous best aligners. The difference with the next best aligner is statistically significant at p &lt; 0.05 (two-sample one-tailed z-test). Overall, our system outperforms 86 of the 89 participating systems. 5.1.3 Paraphrase Detection Given two input sentences, the goal in this task is to determine if their meanings are the same. Data. The MSR paraphrase corpus (Dolan et al., 2004) contains 4076 dev and 1725 test sentence pairs; a paraphrase label (true/false) for each pair System A % P % R % F, % Madnani et al. (2012) 77.4 79.0 89.9 84.1 Yao et al. (2013a) 70.0 72.6 88.1 79.6 Yao et al. (2013b) 68.1 68.6 95.8 79.9 Sultan et al. (2014a) 73.4 76.6 86.4 81.2 Our Aligner 73.2 75.3 88.8 81.5 Table 3: Paraphrase results. Performances of past systems are taken from (Sultan et al., 2014a). is provided by human annotators. Evaluation Metrics. We report performance in terms of: (1) accuracy in classifying the sentences into true and false classes (A), and (2) true class precisio</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of the International Conference on Computational Linguistics, pages 350-356, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The Paraphrase Database.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>758--764</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics, pages 758-764, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Halawi</author>
<author>Gideon Dror</author>
<author>Evgeniy Gabrilovich</author>
<author>Yehuda Koren</author>
</authors>
<title>Large-Scale Learning of Word Relatedness with Constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1406--1414</pages>
<location>Beijing, China.</location>
<contexts>
<context position="15225" citStr="Halawi et al., 2012" startWordPosition="2550" endWordPosition="2553">types, with some differences. In the two following sections, we discuss these features and indicate the associated word pair types. We assume alignment of the two words T(1) iE T(1) and T (2) j E T(2). 4.1 Stage 1: Assessing Pairs Individually 4.1.1 Word Similarity Features Our first feature combines neural word embeddings, used previously for word similarity prediction (Mikolov et al., 2013; Baroni et al., 2014), with a paraphrase database (Ganitkevitch et al., 2013). Our feature is the output of a ridge regression model trained on human annotations of word similarity (Radinsky et al., 2011; Halawi et al., 2012; Bruni et al., 2014) with two features: the cosine similarity between the neural embedding vectors of the two words (using a publicly available set of 400-dimensional word vectors (Baroni et al., 2014)), and the presence/absence of the word pair in the PPDB XXXL database. This regression model produces similarities (sim henceforth) in [0, 1], though we only consider similarities above 0.5 as lower scores are often noisy. To deal with single-letter spelling errors, we consider T (1) i and T(2) jto be an exact match if exactly one of the two is correctly spelled and their Levenshtein distance i</context>
</contexts>
<marker>Halawi, Dror, Gabrilovich, Koren, 2012</marker>
<rawString>Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-Scale Learning of Word Relatedness with Constraints. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1406-1414, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>44--52</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="25962" citStr="Han et al. (2013)" startWordPosition="4423" endWordPosition="4426"> shows the performance of different aligners on the two test sets. Our aligner demonstrates the best overall performance in terms of both F1 and E. Wilcoxon signed-rank tests (with Pratt’s treatment for zero-difference pairs) show that the improvements in F1 over the previous best aligner (Sultan et al., 2014a) are statistically significant at p &lt; 0.01 for both test sets. 5.1.2 Identification of Sentence Similarity Given two input sentences, the goal in this task, known also as Semantic Textual Similarity (STS), is to output a real-valued semantic similarity score. 954 System Pearson’s r Rank Han et al. (2013) 73.7 1 Yao et al. (2013a) 46.2 66 Sultan et al. (2014a) 67.2 7 Our Aligner 67.8 4 Table 2: STS results. Performances of past systems are reported by Sultan et al. (2014a). Data. To be able to directly compare with past aligners, we select three data sets (headlines: pairs of news headlines; OnWN, FNWN: gloss pairs) from the 2013 *SEM STS corpus (Agirre et al., 2013), containing 1500 sentence pairs in total. Sultan et al. (2014a) reports the performance of two state-of-the-art aligners on these pairs. Evaluation metric. At SemEval, STS systems output a similarity score in [0, 5]. For each indi</context>
<context position="27551" citStr="Han et al., 2013" startWordPosition="4690" endWordPosition="4693">air an alignment probability. For STS, we compute a length-normalized sum of alignment probabilities of content word pairs across the two sentences. We include all pairs with probability &gt; 0.5; the remaining pairs are included in decreasing order of their probabilities and already included words are ignored. Following (Sultan et al., 2014a), we normalize by dividing with the harmonic mean of the numbers of content words in the two sentences. Results. Table 2 shows the performance of different aligners on the three STS 2013 test sets. We also show the performance of the contest-winning system (Han et al., 2013). Our STS system demonstrates a weighted correlation of 67.8%, which is better than similar STS systems based on the two previous best aligners. The difference with the next best aligner is statistically significant at p &lt; 0.05 (two-sample one-tailed z-test). Overall, our system outperforms 86 of the 89 participating systems. 5.1.3 Paraphrase Detection Given two input sentences, the goal in this task is to determine if their meanings are the same. Data. The MSR paraphrase corpus (Dolan et al., 2004) contains 4076 dev and 1725 test sentence pairs; a paraphrase label (true/false) for each pair S</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 44-52, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>10111019</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="35568" citStr="Heilman and Smith, 2010" startWordPosition="6053" endWordPosition="6056"> naturally, as function word alignment is heavily dependent on related content word alignment. On {ne-c, ne-c} pairs the aligner performs the best, but still suffers from the two above issues. 6 Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 10111019, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
</authors>
<title>A Discourse Commitment-Based Framework for Recognizing Textual Entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>171--176</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2101" citStr="Hickl and Bensley, 2007" startWordPosition="309" endWordPosition="313"> they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-many mappings among the two phrases’ component words in the latter case. Thus without loss of generality, we formulate alignment as a binary classification task where give</context>
</contexts>
<marker>Hickl, Bensley, 2007</marker>
<rawString>Andrew Hickl and Jeremy Bensley. 2007. A Discourse Commitment-Based Framework for Recognizing Textual Entailment. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 171-176, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hixon</author>
<author>Peter Clark</author>
<author>Hannaneh Hajishirzi</author>
</authors>
<title>Learning Knowledge Graphs for Question Answering through Conversational Dialog.</title>
<date>2015</date>
<booktitle>In Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="1597" citStr="Hixon et al., 2015" startWordPosition="226" endWordPosition="229">ductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research probl</context>
</contexts>
<marker>Hixon, Clark, Hajishirzi, 2015</marker>
<rawString>Ben Hixon, Peter Clark, and Hannaneh Hajishirzi. 2015. Learning Knowledge Graphs for Question Answering through Conversational Dialog. In Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Phrase-Based Alignment Model for Natural Language Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>802--811</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="1881" citStr="MacCartney et al., 2008" startWordPosition="272" endWordPosition="275">etection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be repre</context>
<context position="5180" citStr="MacCartney et al., 2008" startWordPosition="809" endWordPosition="812">y Pieces of the Puzzle We illustrate with examples key pieces of the alignment puzzle and discuss techniques used by existing aligners to solve them. We use the term ‘unit’ to refer to both words and phrases in a snippet. Figure 1 shows a shortened version of sentence pair 712 in the MSR alignment corpus dev set (Brockett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word</context>
<context position="6637" citStr="MacCartney et al., 2008" startWordPosition="1037" endWordPosition="1040">roops(2)) arenot aligned despite the two words’ semantic equivalence in isolation, due to the presence of a competing pair, (armor(2), troops(2)), which is a better fit in context. The above examples reveal a second aligner requirement: the ability to incorporate context into similarity calculations. Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011; Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. The third and final key component of an aligner is a</context>
<context position="8762" citStr="MacCartney et al. (2008)" startWordPosition="1391" endWordPosition="1394">. Groups of mutually cooperating units can also exist where one unit provides supporting evidence 1 2 3 4 5 6 7 8 9 British 1 armor 2 crashed 3 into 4 a 5 jail 6 to 7 free 8 two 9 soldiers 10 arrested 11 by 12 Iraqi 13 police 14 . 15 950 for the alignment of other units in the group. Examples (besides named entities) include individual words in one snippet that are grouped together in the other snippet (e.g., state of the art H state-of-the-art or headquarters in Paris H Paris-based). We briefly discuss the working principles of existing aligners to show how they respsond to these challenges. MacCartney et al. (2008), Thadani and McKeown (2011) and Thadani et al. (2012) frame alignment as a set of phrase edit (insertion, deletion and substitution) operations that transform one snippet into the other. Each edit operation is scored as a weighted sum of feature values (including lexical and contextual similarity features), and an optimal set of edits is computed. Yao et al. (2013a; 2013b) take a sequence labeling approach: input snippets are considered sequences of units and for each unit in one snippet, units in the other snippet are considered potential labels. A first order conditional random field is use</context>
<context position="23541" citStr="MacCartney et al., 2008" startWordPosition="4015" endWordPosition="4018"> To promote high recall, we employ the higher of a word pair’s own lexical similarity and the lexical similarity of the cooperating pair with the highest stage 1 probability as a stage 2 feature. The stage 2 feature set is identical across word pair types, but as in stage 1, we train individual models for different pair types. 5 Experiments 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistic</context>
<context position="28744" citStr="MacCartney et al., 2008" startWordPosition="4892" endWordPosition="4895">l (true/false) for each pair System A % P % R % F, % Madnani et al. (2012) 77.4 79.0 89.9 84.1 Yao et al. (2013a) 70.0 72.6 88.1 79.6 Yao et al. (2013b) 68.1 68.6 95.8 79.9 Sultan et al. (2014a) 73.4 76.6 86.4 81.2 Our Aligner 73.2 75.3 88.8 81.5 Table 3: Paraphrase results. Performances of past systems are taken from (Sultan et al., 2014a). is provided by human annotators. Evaluation Metrics. We report performance in terms of: (1) accuracy in classifying the sentences into true and false classes (A), and (2) true class precision (P), recall (R) and F1 score. Method. Following prior aligners (MacCartney et al., 2008; Yao et al., 2013b; Sultan et al., 2014a), we output a true decision for a test sentence pair iff the length-normalized alignment score for the pair exceeds a threshold derived from the dev set. Results. The top row of Table 3 shows the best result by any system on the MSR test set. Among all aligners (all other rows), ours achieves the best F1 score and the second best accuracy. We report paraphrase detection results primarily to allow comparison with past aligners. However, this simplistic application to a complex task only gives a ballpark estimate of an aligner’s quality. EDB++ MSR Model </context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A Phrase-Based Alignment Model for Natural Language Inference. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802-811, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining Machine Translation Metrics for Paraphrase Identification.</title>
<date>2012</date>
<booktitle>In Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>182--190</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1310" citStr="Madnani et al., 2012" startWordPosition="183" endWordPosition="186">address cooperation and competition for alignment among units in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; </context>
<context position="28195" citStr="Madnani et al. (2012)" startWordPosition="4799" endWordPosition="4802">trates a weighted correlation of 67.8%, which is better than similar STS systems based on the two previous best aligners. The difference with the next best aligner is statistically significant at p &lt; 0.05 (two-sample one-tailed z-test). Overall, our system outperforms 86 of the 89 participating systems. 5.1.3 Paraphrase Detection Given two input sentences, the goal in this task is to determine if their meanings are the same. Data. The MSR paraphrase corpus (Dolan et al., 2004) contains 4076 dev and 1725 test sentence pairs; a paraphrase label (true/false) for each pair System A % P % R % F, % Madnani et al. (2012) 77.4 79.0 89.9 84.1 Yao et al. (2013a) 70.0 72.6 88.1 79.6 Yao et al. (2013b) 68.1 68.6 95.8 79.9 Sultan et al. (2014a) 73.4 76.6 86.4 81.2 Our Aligner 73.2 75.3 88.8 81.5 Table 3: Paraphrase results. Performances of past systems are taken from (Sultan et al., 2014a). is provided by human annotators. Evaluation Metrics. We report performance in terms of: (1) accuracy in classifying the sentences into true and false classes (A), and (2) true class precision (P), recall (R) and F1 score. Method. Following prior aligners (MacCartney et al., 2008; Yao et al., 2013b; Sultan et al., 2014a), we outp</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining Machine Translation Metrics for Paraphrase Identification. In Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics, pages 182-190, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Learning Representations Workshop,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="5474" citStr="Mikolov et al., 2013" startWordPosition="856" endWordPosition="859">t corpus dev set (Brockett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word a, but also because they modify semantically equivalent units: jail and police station. The influence of context on content word alignment becomes salient particularly in the presence of competing words. In Figure 1, (soldiers(10), troops(2)) arenot aligned despite the two words’ semantic equ</context>
<context position="15000" citStr="Mikolov et al., 2013" startWordPosition="2512" endWordPosition="2515">.e. stage 1 and 2 models) are successively applied to each input sentence pair. 4 Features As mentioned above, we train a separate model for each individual word pair type. Our feature set is largely the same across word pair types, with some differences. In the two following sections, we discuss these features and indicate the associated word pair types. We assume alignment of the two words T(1) iE T(1) and T (2) j E T(2). 4.1 Stage 1: Assessing Pairs Individually 4.1.1 Word Similarity Features Our first feature combines neural word embeddings, used previously for word similarity prediction (Mikolov et al., 2013; Baroni et al., 2014), with a paraphrase database (Ganitkevitch et al., 2013). Our feature is the output of a ridge regression model trained on human annotations of word similarity (Radinsky et al., 2011; Halawi et al., 2012; Bruni et al., 2014) with two features: the cosine similarity between the neural embedding vectors of the two words (using a publicly available set of 400-dimensional word vectors (Baroni et al., 2014)), and the presence/absence of the word pair in the PPDB XXXL database. This regression model produces similarities (sim henceforth) in [0, 1], though we only consider simil</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of the International Conference on Learning Representations Workshop, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to Grade Short Answer Questions Using Semantic Similarity Measures and Dependency Graph Alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>752--762</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1556" citStr="Mohler et al., 2011" startWordPosition="220" endWordPosition="223">%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, </context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011. Learning to Grade Short Answer Questions Using Semantic Similarity Measures and Dependency Graph Alignments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752-762, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<publisher>MIT Press.</publisher>
<contexts>
<context position="35851" citStr="Och and Ney, 2003" startWordPosition="6096" endWordPosition="6099">king principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):1951, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Chris Manning</author>
</authors>
<title>Robust Machine Translation Evaluation with Entailment Features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>297--305</pages>
<marker>Pad´o, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Michel Galley, Dan Jurafsky, and Chris Manning. 2009. Robust Machine Translation Evaluation with Entailment Features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 297-305, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Tae-Gil Noh</author>
<author>Asher Stern</author>
<author>Rui Wang</author>
<author>Roberto Zanoli</author>
</authors>
<title>Design and Realization of a Modular Architecture for Textual Entailment.</title>
<date>2015</date>
<journal>Natural Language Engineering,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>167--200</pages>
<publisher>Cambridge University Press.</publisher>
<marker>Pad´o, Noh, Stern, Wang, Zanoli, 2015</marker>
<rawString>Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang, and Roberto Zanoli. 2015. Design and Realization of a Modular Architecture for Textual Entailment. Natural Language Engineering, 21 (2), pages 167-200, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, vol. 12, pages 2825-2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A Word at a Time: Computing Word Relatedness using Temporal Semantic Analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>337--346</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="15204" citStr="Radinsky et al., 2011" startWordPosition="2546" endWordPosition="2549"> same across word pair types, with some differences. In the two following sections, we discuss these features and indicate the associated word pair types. We assume alignment of the two words T(1) iE T(1) and T (2) j E T(2). 4.1 Stage 1: Assessing Pairs Individually 4.1.1 Word Similarity Features Our first feature combines neural word embeddings, used previously for word similarity prediction (Mikolov et al., 2013; Baroni et al., 2014), with a paraphrase database (Ganitkevitch et al., 2013). Our feature is the output of a ridge regression model trained on human annotations of word similarity (Radinsky et al., 2011; Halawi et al., 2012; Bruni et al., 2014) with two features: the cosine similarity between the neural embedding vectors of the two words (using a publicly available set of 400-dimensional word vectors (Baroni et al., 2014)), and the presence/absence of the word pair in the PPDB XXXL database. This regression model produces similarities (sim henceforth) in [0, 1], though we only consider similarities above 0.5 as lower scores are often noisy. To deal with single-letter spelling errors, we consider T (1) i and T(2) jto be an exact match if exactly one of the two is correctly spelled and their L</context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A Word at a Time: Computing Word Relatedness using Temporal Semantic Analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 337-346, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web,</booktitle>
<pages>171182</pages>
<location>Jeju Island,</location>
<contexts>
<context position="35613" citStr="Roth and Frank, 2012" startWordPosition="6059" endWordPosition="6062">y dependent on related content word alignment. On {ne-c, ne-c} pairs the aligner performs the best, but still suffers from the two above issues. 6 Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experi</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012. Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering. In Proceedings of the 20th International Conference on World Wide Web, pages pages 171182, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>219--230</pages>
<contexts>
<context position="1989" citStr="Sultan et al., 2014" startWordPosition="292" endWordPosition="295">ltan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-many mappings among the two phrases’ component words in the</context>
<context position="3912" citStr="Sultan et al., 2014" startWordPosition="607" endWordPosition="610">Watson). To handle such interdependencies, we employ a two-stage logistic regression model – stage 1 computes an alignment probability for each word pair based solely on its own feature values, and stage 2 assigns the eventual alignment labels to all pairs following a comparative assessment of stage 1 probabilities of cooperating and competing pairs. On two alignment data sets reported in (Brockett, 2007) and (Thadani et al., 2012), our aligner demonstrates respective F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner (Sultan et al., 2014a). We also present extrinsic evaluation of the aligner within two text comparison tasks, namely sentence similarity identification and paraphrase detection, where it demonstrates state-of-the-art results. 949 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 949–959, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: A human-aligned sentence pair from the MSR alignment corpus. The shaded cells depict the alignment, which can also be represented as the set of word index pairs {(1,1), (2, 2), (3, 3), (4</context>
<context position="5254" citStr="Sultan et al., 2014" startWordPosition="822" endWordPosition="825"> puzzle and discuss techniques used by existing aligners to solve them. We use the term ‘unit’ to refer to both words and phrases in a snippet. Figure 1 shows a shortened version of sentence pair 712 in the MSR alignment corpus dev set (Brockett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word a, but also because they modify semantically equivalent units: jail and p</context>
<context position="6802" citStr="Sultan et al. (2014" startWordPosition="1064" endWordPosition="1068">in context. The above examples reveal a second aligner requirement: the ability to incorporate context into similarity calculations. Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011; Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. The third and final key component of an aligner is a mechanism to combine lexical/phrasal and contextual similarities to produce alignments. This task is non-trivial due to the presence of cooperating and competing un</context>
<context position="9399" citStr="Sultan et al. (2014" startWordPosition="1495" endWordPosition="1498">own (2011) and Thadani et al. (2012) frame alignment as a set of phrase edit (insertion, deletion and substitution) operations that transform one snippet into the other. Each edit operation is scored as a weighted sum of feature values (including lexical and contextual similarity features), and an optimal set of edits is computed. Yao et al. (2013a; 2013b) take a sequence labeling approach: input snippets are considered sequences of units and for each unit in one snippet, units in the other snippet are considered potential labels. A first order conditional random field is used for prediction. Sultan et al. (2014a) treat alignment as a bipartite matching problem and use a greedy algorithm to perform one-to-one word alignment. A weighted sum of two words’ lexical and contextual similarities serves as the pair’s edge weight. Noticeable in the designs of the supervised aligners is a lack of attention to the scenarios competing units can pose – alignment of a unit depends only on its own feature values. While the unsupervised aligner by Sultan et al. (2014a) employs techniques to deal with such scenarios, it allows only one-toone alignment, which fundamentally limits the set of reachable alignments. 3 App</context>
<context position="18204" citStr="Sultan et al., 2014" startWordPosition="3052" endWordPosition="3055">the word-based representation allows the use of similarity resources for named entity words. Non-name words are treated identically. For simplicity we only discuss our wordbased features below, but each feature also has an entity-based variant. Dependency-based context. These features apply only if neither of T (1) i and T (2) j is a punctuation mark. We compute the proportion of identical and highly similar (sim &gt; 0.9) parents and children of T (1) i and T (2) j in the dependency trees of T(1) and T(2) (Stanford collapsed dependencies (de Marneffe et al., 2006)). Equivalent dependency types (Sultan et al., 2014a) are included in the above computation, which encode semantic equivalences between typed dependencies (e.g., nsubjpass and dobj). We employ separate features for identicality and similarity. Similar features are also computed for a dependency neighborhood of size 2 (parents, grandparents, children and grandchildren), where we consider only content word neighbors. Dependency neighbors of T (1) i and T (2) j that are less similar (0.9 &gt; sim &gt; 0.5; e.g., (gas, energy) or (award, winner)) can also contain useful semantic information for an aligner. To accommodate this relatively large range of w</context>
<context position="23804" citStr="Sultan et al. (2014" startWordPosition="4069" endWordPosition="4072">stage 1, we train individual models for different pair types. 5 Experiments 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The MSR alignment corpus (Brockett, 2007) contains 800 dev and 800 test sentence pairs from the PASCAL RTE 2006 challenge. Each pair is aligned by three human annotators; Fleiss Kappa agreement of about 0.73 (“substantial agreement”) is re</context>
<context position="25655" citStr="Sultan et al., 2014" startWordPosition="4374" endWordPosition="4377">ld alignments. Model setup. For each corpus, we train our model using the dev set and evaluate on the test set. We use the logistic regression implementation of Scikit-learn (Pedregosa et al., 2011) and use leaveone-out cross-validation on the dev pairs to set the regularization parameter C. Results. Table 1 shows the performance of different aligners on the two test sets. Our aligner demonstrates the best overall performance in terms of both F1 and E. Wilcoxon signed-rank tests (with Pratt’s treatment for zero-difference pairs) show that the improvements in F1 over the previous best aligner (Sultan et al., 2014a) are statistically significant at p &lt; 0.01 for both test sets. 5.1.2 Identification of Sentence Similarity Given two input sentences, the goal in this task, known also as Semantic Textual Similarity (STS), is to output a real-valued semantic similarity score. 954 System Pearson’s r Rank Han et al. (2013) 73.7 1 Yao et al. (2013a) 46.2 66 Sultan et al. (2014a) 67.2 7 Our Aligner 67.8 4 Table 2: STS results. Performances of past systems are reported by Sultan et al. (2014a). Data. To be able to directly compare with past aligners, we select three data sets (headlines: pairs of news headlines; </context>
<context position="27274" citStr="Sultan et al., 2014" startWordPosition="4641" endWordPosition="4644"> between system scores and human annotations. The final evaluation metric is a weighted sum of r’s over all test sets, where the weight assigned to a set is proportional to its number of pairs. Method. Being a logistic regression model, stage 2 of our aligner assigns each word pair an alignment probability. For STS, we compute a length-normalized sum of alignment probabilities of content word pairs across the two sentences. We include all pairs with probability &gt; 0.5; the remaining pairs are included in decreasing order of their probabilities and already included words are ignored. Following (Sultan et al., 2014a), we normalize by dividing with the harmonic mean of the numbers of content words in the two sentences. Results. Table 2 shows the performance of different aligners on the three STS 2013 test sets. We also show the performance of the contest-winning system (Han et al., 2013). Our STS system demonstrates a weighted correlation of 67.8%, which is better than similar STS systems based on the two previous best aligners. The difference with the next best aligner is statistically significant at p &lt; 0.05 (two-sample one-tailed z-test). Overall, our system outperforms 86 of the 89 participating syst</context>
<context position="28784" citStr="Sultan et al., 2014" startWordPosition="4900" endWordPosition="4903">R % F, % Madnani et al. (2012) 77.4 79.0 89.9 84.1 Yao et al. (2013a) 70.0 72.6 88.1 79.6 Yao et al. (2013b) 68.1 68.6 95.8 79.9 Sultan et al. (2014a) 73.4 76.6 86.4 81.2 Our Aligner 73.2 75.3 88.8 81.5 Table 3: Paraphrase results. Performances of past systems are taken from (Sultan et al., 2014a). is provided by human annotators. Evaluation Metrics. We report performance in terms of: (1) accuracy in classifying the sentences into true and false classes (A), and (2) true class precision (P), recall (R) and F1 score. Method. Following prior aligners (MacCartney et al., 2008; Yao et al., 2013b; Sultan et al., 2014a), we output a true decision for a test sentence pair iff the length-normalized alignment score for the pair exceeds a threshold derived from the dev set. Results. The top row of Table 3 shows the best result by any system on the MSR test set. Among all aligners (all other rows), ours achieves the best F1 score and the second best accuracy. We report paraphrase detection results primarily to allow comparison with past aligners. However, this simplistic application to a complex task only gives a ballpark estimate of an aligner’s quality. EDB++ MSR Model P % R % F1 % E % Two-Stage Model 95.4 89</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014a. Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence. Transactions of the Association for Computational Linguistics, 2 (May), pages 219-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>DLS@CU: Sentence Similarity from Word Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>241--246</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1989" citStr="Sultan et al., 2014" startWordPosition="292" endWordPosition="295">ltan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-many mappings among the two phrases’ component words in the</context>
<context position="3912" citStr="Sultan et al., 2014" startWordPosition="607" endWordPosition="610">Watson). To handle such interdependencies, we employ a two-stage logistic regression model – stage 1 computes an alignment probability for each word pair based solely on its own feature values, and stage 2 assigns the eventual alignment labels to all pairs following a comparative assessment of stage 1 probabilities of cooperating and competing pairs. On two alignment data sets reported in (Brockett, 2007) and (Thadani et al., 2012), our aligner demonstrates respective F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner (Sultan et al., 2014a). We also present extrinsic evaluation of the aligner within two text comparison tasks, namely sentence similarity identification and paraphrase detection, where it demonstrates state-of-the-art results. 949 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 949–959, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: A human-aligned sentence pair from the MSR alignment corpus. The shaded cells depict the alignment, which can also be represented as the set of word index pairs {(1,1), (2, 2), (3, 3), (4</context>
<context position="5254" citStr="Sultan et al., 2014" startWordPosition="822" endWordPosition="825"> puzzle and discuss techniques used by existing aligners to solve them. We use the term ‘unit’ to refer to both words and phrases in a snippet. Figure 1 shows a shortened version of sentence pair 712 in the MSR alignment corpus dev set (Brockett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word a, but also because they modify semantically equivalent units: jail and p</context>
<context position="6802" citStr="Sultan et al. (2014" startWordPosition="1064" endWordPosition="1068">in context. The above examples reveal a second aligner requirement: the ability to incorporate context into similarity calculations. Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011; Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. The third and final key component of an aligner is a mechanism to combine lexical/phrasal and contextual similarities to produce alignments. This task is non-trivial due to the presence of cooperating and competing un</context>
<context position="9399" citStr="Sultan et al. (2014" startWordPosition="1495" endWordPosition="1498">own (2011) and Thadani et al. (2012) frame alignment as a set of phrase edit (insertion, deletion and substitution) operations that transform one snippet into the other. Each edit operation is scored as a weighted sum of feature values (including lexical and contextual similarity features), and an optimal set of edits is computed. Yao et al. (2013a; 2013b) take a sequence labeling approach: input snippets are considered sequences of units and for each unit in one snippet, units in the other snippet are considered potential labels. A first order conditional random field is used for prediction. Sultan et al. (2014a) treat alignment as a bipartite matching problem and use a greedy algorithm to perform one-to-one word alignment. A weighted sum of two words’ lexical and contextual similarities serves as the pair’s edge weight. Noticeable in the designs of the supervised aligners is a lack of attention to the scenarios competing units can pose – alignment of a unit depends only on its own feature values. While the unsupervised aligner by Sultan et al. (2014a) employs techniques to deal with such scenarios, it allows only one-toone alignment, which fundamentally limits the set of reachable alignments. 3 App</context>
<context position="18204" citStr="Sultan et al., 2014" startWordPosition="3052" endWordPosition="3055">the word-based representation allows the use of similarity resources for named entity words. Non-name words are treated identically. For simplicity we only discuss our wordbased features below, but each feature also has an entity-based variant. Dependency-based context. These features apply only if neither of T (1) i and T (2) j is a punctuation mark. We compute the proportion of identical and highly similar (sim &gt; 0.9) parents and children of T (1) i and T (2) j in the dependency trees of T(1) and T(2) (Stanford collapsed dependencies (de Marneffe et al., 2006)). Equivalent dependency types (Sultan et al., 2014a) are included in the above computation, which encode semantic equivalences between typed dependencies (e.g., nsubjpass and dobj). We employ separate features for identicality and similarity. Similar features are also computed for a dependency neighborhood of size 2 (parents, grandparents, children and grandchildren), where we consider only content word neighbors. Dependency neighbors of T (1) i and T (2) j that are less similar (0.9 &gt; sim &gt; 0.5; e.g., (gas, energy) or (award, winner)) can also contain useful semantic information for an aligner. To accommodate this relatively large range of w</context>
<context position="23804" citStr="Sultan et al. (2014" startWordPosition="4069" endWordPosition="4072">stage 1, we train individual models for different pair types. 5 Experiments 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The MSR alignment corpus (Brockett, 2007) contains 800 dev and 800 test sentence pairs from the PASCAL RTE 2006 challenge. Each pair is aligned by three human annotators; Fleiss Kappa agreement of about 0.73 (“substantial agreement”) is re</context>
<context position="25655" citStr="Sultan et al., 2014" startWordPosition="4374" endWordPosition="4377">ld alignments. Model setup. For each corpus, we train our model using the dev set and evaluate on the test set. We use the logistic regression implementation of Scikit-learn (Pedregosa et al., 2011) and use leaveone-out cross-validation on the dev pairs to set the regularization parameter C. Results. Table 1 shows the performance of different aligners on the two test sets. Our aligner demonstrates the best overall performance in terms of both F1 and E. Wilcoxon signed-rank tests (with Pratt’s treatment for zero-difference pairs) show that the improvements in F1 over the previous best aligner (Sultan et al., 2014a) are statistically significant at p &lt; 0.01 for both test sets. 5.1.2 Identification of Sentence Similarity Given two input sentences, the goal in this task, known also as Semantic Textual Similarity (STS), is to output a real-valued semantic similarity score. 954 System Pearson’s r Rank Han et al. (2013) 73.7 1 Yao et al. (2013a) 46.2 66 Sultan et al. (2014a) 67.2 7 Our Aligner 67.8 4 Table 2: STS results. Performances of past systems are reported by Sultan et al. (2014a). Data. To be able to directly compare with past aligners, we select three data sets (headlines: pairs of news headlines; </context>
<context position="27274" citStr="Sultan et al., 2014" startWordPosition="4641" endWordPosition="4644"> between system scores and human annotations. The final evaluation metric is a weighted sum of r’s over all test sets, where the weight assigned to a set is proportional to its number of pairs. Method. Being a logistic regression model, stage 2 of our aligner assigns each word pair an alignment probability. For STS, we compute a length-normalized sum of alignment probabilities of content word pairs across the two sentences. We include all pairs with probability &gt; 0.5; the remaining pairs are included in decreasing order of their probabilities and already included words are ignored. Following (Sultan et al., 2014a), we normalize by dividing with the harmonic mean of the numbers of content words in the two sentences. Results. Table 2 shows the performance of different aligners on the three STS 2013 test sets. We also show the performance of the contest-winning system (Han et al., 2013). Our STS system demonstrates a weighted correlation of 67.8%, which is better than similar STS systems based on the two previous best aligners. The difference with the next best aligner is statistically significant at p &lt; 0.05 (two-sample one-tailed z-test). Overall, our system outperforms 86 of the 89 participating syst</context>
<context position="28784" citStr="Sultan et al., 2014" startWordPosition="4900" endWordPosition="4903">R % F, % Madnani et al. (2012) 77.4 79.0 89.9 84.1 Yao et al. (2013a) 70.0 72.6 88.1 79.6 Yao et al. (2013b) 68.1 68.6 95.8 79.9 Sultan et al. (2014a) 73.4 76.6 86.4 81.2 Our Aligner 73.2 75.3 88.8 81.5 Table 3: Paraphrase results. Performances of past systems are taken from (Sultan et al., 2014a). is provided by human annotators. Evaluation Metrics. We report performance in terms of: (1) accuracy in classifying the sentences into true and false classes (A), and (2) true class precision (P), recall (R) and F1 score. Method. Following prior aligners (MacCartney et al., 2008; Yao et al., 2013b; Sultan et al., 2014a), we output a true decision for a test sentence pair iff the length-normalized alignment score for the pair exceeds a threshold derived from the dev set. Results. The top row of Table 3 shows the best result by any system on the MSR test set. Among all aligners (all other rows), ours achieves the best F1 score and the second best accuracy. We report paraphrase detection results primarily to allow comparison with past aligners. However, this simplistic application to a complex task only gives a ballpark estimate of an aligner’s quality. EDB++ MSR Model P % R % F1 % E % Two-Stage Model 95.4 89</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014b. DLS@CU: Sentence Similarity from Word Alignment. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 241-246, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>DLS@CU: Sentence Similarity from Word Alignment and Semantic Vector Composition.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation,</booktitle>
<pages>148--153</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="1388" citStr="Sultan et al., 2015" startWordPosition="194" endWordPosition="197">et. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 1 Introduction Computer applications frequently require semantic comparison between short snippets of natural language text. Such comparisons are key to paraphrase detection (Das and Smith, 2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 201</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2015</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2015. DLS@CU: Sentence Similarity from Word Alignment and Semantic Vector Composition. In Proceedings of the 9th International Workshop on Semantic Evaluation, pages 148-153, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>A Framework for Identifying Textual Redundancy.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>873880</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="35666" citStr="Thadani and McKeown, 2008" startWordPosition="6066" endWordPosition="6069">n {ne-c, ne-c} pairs the aligner performs the best, but still suffers from the two above issues. 6 Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word align</context>
</contexts>
<marker>Thadani, McKeown, 2008</marker>
<rawString>Kapil Thadani and Kathleen McKeown. 2008. A Framework for Identifying Textual Redundancy. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 873880, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>254--259</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1908" citStr="Thadani and McKeown, 2011" startWordPosition="276" endWordPosition="279">2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – </context>
<context position="5208" citStr="Thadani and McKeown, 2011" startWordPosition="813" endWordPosition="816"> illustrate with examples key pieces of the alignment puzzle and discuss techniques used by existing aligners to solve them. We use the term ‘unit’ to refer to both words and phrases in a snippet. Figure 1 shows a shortened version of sentence pair 712 in the MSR alignment corpus dev set (Brockett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5,4) in Figure 1 exists not just because both units are the word a, but also because they mo</context>
<context position="6664" citStr="Thadani and McKeown, 2011" startWordPosition="1041" endWordPosition="1044">despite the two words’ semantic equivalence in isolation, due to the presence of a competing pair, (armor(2), troops(2)), which is a better fit in context. The above examples reveal a second aligner requirement: the ability to incorporate context into similarity calculations. Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011; Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. The third and final key component of an aligner is a mechanism to combine lexic</context>
<context position="8790" citStr="Thadani and McKeown (2011)" startWordPosition="1395" endWordPosition="1398">rating units can also exist where one unit provides supporting evidence 1 2 3 4 5 6 7 8 9 British 1 armor 2 crashed 3 into 4 a 5 jail 6 to 7 free 8 two 9 soldiers 10 arrested 11 by 12 Iraqi 13 police 14 . 15 950 for the alignment of other units in the group. Examples (besides named entities) include individual words in one snippet that are grouped together in the other snippet (e.g., state of the art H state-of-the-art or headquarters in Paris H Paris-based). We briefly discuss the working principles of existing aligners to show how they respsond to these challenges. MacCartney et al. (2008), Thadani and McKeown (2011) and Thadani et al. (2012) frame alignment as a set of phrase edit (insertion, deletion and substitution) operations that transform one snippet into the other. Each edit operation is scored as a weighted sum of feature values (including lexical and contextual similarity features), and an optimal set of edits is computed. Yao et al. (2013a; 2013b) take a sequence labeling approach: input snippets are considered sequences of units and for each unit in one snippet, units in the other snippet are considered potential labels. A first order conditional random field is used for prediction. Sultan et </context>
<context position="23568" citStr="Thadani and McKeown, 2011" startWordPosition="4019" endWordPosition="4022">we employ the higher of a word pair’s own lexical similarity and the lexical similarity of the cooperating pair with the highest stage 1 probability as a stage 2 feature. The stage 2 feature set is identical across word pair types, but as in stage 1, we train individual models for different pair types. 5 Experiments 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The</context>
<context position="23684" citStr="Thadani &amp; McKeown (2011)" startWordPosition="4044" endWordPosition="4047">the highest stage 1 probability as a stage 2 feature. The stage 2 feature set is identical across word pair types, but as in stage 1, we train individual models for different pair types. 5 Experiments 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The MSR alignment corpus (Brockett, 2007) contains 800 dev and 800 test sentence pairs from the PASCAL RTE 2006 challen</context>
</contexts>
<marker>Thadani, McKeown, 2011</marker>
<rawString>Kapil Thadani and Kathleen McKeown. 2011. Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 254-259, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Scott Martin</author>
<author>Michael White</author>
</authors>
<title>A Joint Phrasal and Dependency Model for Paraphrase Alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1229--1238</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="1930" citStr="Thadani et al., 2012" startWordPosition="280" endWordPosition="283">, textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-</context>
<context position="3728" citStr="Thadani et al., 2012" startWordPosition="578" endWordPosition="581">nt with a word in the other snippet, whereas words that constitute a phrase can provide supporting evidence for one another (e.g. in named entity alignments such as Watson ↔ John Hamish Watson). To handle such interdependencies, we employ a two-stage logistic regression model – stage 1 computes an alignment probability for each word pair based solely on its own feature values, and stage 2 assigns the eventual alignment labels to all pairs following a comparative assessment of stage 1 probabilities of cooperating and competing pairs. On two alignment data sets reported in (Brockett, 2007) and (Thadani et al., 2012), our aligner demonstrates respective F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner (Sultan et al., 2014a). We also present extrinsic evaluation of the aligner within two text comparison tasks, namely sentence similarity identification and paraphrase detection, where it demonstrates state-of-the-art results. 949 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 949–959, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: A h</context>
<context position="6781" citStr="Thadani et al., 2012" startWordPosition="1060" endWordPosition="1063"> which is a better fit in context. The above examples reveal a second aligner requirement: the ability to incorporate context into similarity calculations. Existing supervised aligners use various contextual features within a learning algorithm for this purpose. Such features include both shallow surface measures (e.g., the relative positions of the tokens being aligned in the respective sentences, similarities in the immediate left or right words) (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) and syntactic measures like typed dependencies (Thadani and McKeown, 2011; Thadani et al., 2012). Sultan et al. (2014a) design an unsupervised model that more directly encodes context, via surface and dependency-based neighbors which allow contextual similarity to be represented as a weighted sum of lexical similarity. But their model lacks a key structural advantage of supervised models: to be able to use an arbitrarily large feature set to robustly encode lexical and/or contextual similarity. The third and final key component of an aligner is a mechanism to combine lexical/phrasal and contextual similarities to produce alignments. This task is non-trivial due to the presence of coopera</context>
<context position="8816" citStr="Thadani et al. (2012)" startWordPosition="1400" endWordPosition="1403">re one unit provides supporting evidence 1 2 3 4 5 6 7 8 9 British 1 armor 2 crashed 3 into 4 a 5 jail 6 to 7 free 8 two 9 soldiers 10 arrested 11 by 12 Iraqi 13 police 14 . 15 950 for the alignment of other units in the group. Examples (besides named entities) include individual words in one snippet that are grouped together in the other snippet (e.g., state of the art H state-of-the-art or headquarters in Paris H Paris-based). We briefly discuss the working principles of existing aligners to show how they respsond to these challenges. MacCartney et al. (2008), Thadani and McKeown (2011) and Thadani et al. (2012) frame alignment as a set of phrase edit (insertion, deletion and substitution) operations that transform one snippet into the other. Each edit operation is scored as a weighted sum of feature values (including lexical and contextual similarity features), and an optimal set of edits is computed. Yao et al. (2013a; 2013b) take a sequence labeling approach: input snippets are considered sequences of units and for each unit in one snippet, units in the other snippet are considered potential labels. A first order conditional random field is used for prediction. Sultan et al. (2014a) treat alignmen</context>
<context position="23880" citStr="Thadani et al. (2012)" startWordPosition="4083" endWordPosition="4086">s 5.1 System Evaluation We report evaluation on two alignment data sets and extrinsic evaluation on two tasks: sentence similarity identification and paraphrase detection. 5.1.1 Alignment We adopt the evaluation procedure for aligners reported in prior work (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a). Aligner P % R % Fi % E % MacCartney et al. (2008) 85.4 85.3 85.3 21.3 Thadani &amp; McKeown (2011) 89.5 86.2 87.8 33.0 R Yao et al. (2013a) 93.7 84.0 88.6 35.3 Yao et al. (2013b) 92.1 82.8 86.8 29.1 Sultan et al. (2014a) 93.7 89.8 91.7 43.8 Our Aligner 95.4 89.0 92.1 47.3 Thadani et al. (2012) 76.6 83.8 79.2 12.2 ++ Yao et al. (2013a) 91.3 82.0 86.4 15.0 B Yao et al. (2013b) 90.4 81.9 85.9 13.7 E Sultan et al. (2014a) 93.5 82.5 87.6 18.3 Our Aligner 92.1 85.2 88.5 18.3 Table 1: Performance on two alignment data sets. Improvements in F1 are statistically significant. Data. The MSR alignment corpus (Brockett, 2007) contains 800 dev and 800 test sentence pairs from the PASCAL RTE 2006 challenge. Each pair is aligned by three human annotators; Fleiss Kappa agreement of about 0.73 (“substantial agreement”) is reported on both sets. Following prior work, we only consider the sure alignme</context>
</contexts>
<marker>Thadani, Martin, White, 2012</marker>
<rawString>Kapil Thadani, Scott Martin, and Michael White. 2012. A Joint Phrasal and Dependency Model for Paraphrase Alignment. In Proceedings of COLING 2012, pages 1229-1238, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>11641172</pages>
<location>Beijing, China.</location>
<contexts>
<context position="35522" citStr="Wang and Manning, 2010" startWordPosition="6047" endWordPosition="6050">alignments. Low recall for the latter follows naturally, as function word alignment is heavily dependent on related content word alignment. On {ne-c, ne-c} pairs the aligner performs the best, but still suffers from the two above issues. 6 Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monol</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 11641172, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>A Lightweight and High Performance Monolingual Word Aligner.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>702--707</pages>
<location>Sofia, Bulgaria.</location>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013a. A Lightweight and High Performance Monolingual Word Aligner. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702-707, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>Semi-Markov Phrase-based Monolingual Alignment.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>590--600</pages>
<location>Seattle, Washington, USA.</location>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013b. Semi-Markov Phrase-based Monolingual Alignment. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590-600, Seattle, Washington, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
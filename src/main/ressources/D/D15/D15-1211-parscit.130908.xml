<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000089">
<title confidence="0.972797">
A Transition-based Model for Joint Segmentation, POS-tagging and
Normalization
</title>
<author confidence="0.989481">
Tao Qian&apos;,3, Yue Zhang2, Meishan Zhang2∗, Yafeng Ren&apos; and Donghong Ji&apos;
</author>
<affiliation confidence="0.976511">
&apos;Computer School, Wuhan University, Wuhan, China
2Singapore University of Technology and Design
3College of Computer Science and Technology, Hubei University of
Science and Technology, XianNing, China
</affiliation>
<email confidence="0.924179">
{taoqian, renyafeng, dhji}@whu.edu.cn
{yue zhang, meishan zhang}@sutd.edu.sg
</email>
<sectionHeader confidence="0.993101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861785714286">
We propose a transition-based model for
joint word segmentation, POS tagging and
text normalization. Different from pre-
vious methods, the model can be trained
on standard text corpora, overcoming the
lack of annotated microblog corpora. To
evaluate our model, we develop an anno-
tated corpus based on microblogs. Exper-
imental results show that our joint model
can help improve the performance of word
segmentation on microblogs, giving an er-
ror reduction in segmentation accuracy of
12.02%, compared to the traditional ap-
proach.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995485033333333">
Microblogs, such as Twitter, SMS and Weibo, has
become an important research topic in NLP. Pre-
vious work has shown that off-the-shelf NLP tools
can perform poorly on microblogs (Foster et al.,
2011; Gimpel et al., 2011; Han and Baldwin,
2011). One of the major challenges for microblog
processing is the issue of informal words. For ex-
ample, “tmrw” has been frequently used in tweets
for “tomorrow”, causing OOV problems.
Text normalization has been introduced as
a pre-processing step for microblog processing,
which transforms informal words into their stan-
dard forms. Most work in the literature focuses
on English microblog normalization, treating it as
a noisy channel problem (Pennell and Liu, 2014;
Cook and Stevenson, 2009; Yang and Eisenstein,
2013) or a translation problem (Aw et al., 2006;
Contractor et al., 2010; Li and Liu, 2012; Zhang
et al., 2014c), and training models based on words.
Lack of annotated corpora, text normalization
is more challenging for Chinese. Unlike En-
glish, Chinese informal words are more difficult
∗corresponding author
to mechanically normalize for two main reasons.
First, Chinese does not have word delimiters.
Second, Chinese informal words manifest diver-
sity, such as abbreviations, neologisms, unconven-
tional spellings and phonetic substitutions. Intu-
itively, there is mutual dependency between Chi-
nese word segmentation and normalization, and
therefore two tasks should be solved jointly.
Wang and Kan (2013) proposed a joint model
to process word segmentation and informal word
detection. However, text normalization was not
included in the joint model. Kaji et al (2014)
proposed a joint model for word segmentation,
POS tagging and normalization for Japanese Mi-
croblogs, which was trained on a partially anno-
tated microblog corpus. Their method requires
special annotation for text normalization, which
can be expensive.
In this paper, we propose a joint model for Chi-
nese text normalization, word-segmentation and
POS tagging, which can be trained using standard
segmentation and POS tagging annotation, over-
coming the lack of an annotated corpus on Chi-
nese microblogs. Our model is based on Zhang
and Clark (2010), with an extended set of transi-
tion actions to handle joint normalization. In our
model, word segmentation and POS tagging are
based on normalized text transformed from infor-
mal text. Assuming that the majority of informal
words can be normalized into formal equivalents
(Han et al., 2012; Li and Yarowsky, 2008), we
seek standard forms of informal words from an au-
tomatically constructed normalization dictionary.
To evaluate our model, we developed an anno-
tated corpus of microblog texts. Results show that
our model achieves the best performances on three
tasks compared with several baseline systems.
</bodyText>
<sectionHeader confidence="0.992609" genericHeader="introduction">
2 Text Normalization
</sectionHeader>
<bodyText confidence="0.994284">
Text normalization is a relatively new research
topic. There are no precise definitions of a text
</bodyText>
<page confidence="0.949893">
1837
</page>
<note confidence="0.9848975">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999868757575758">
normalization task that are widely accepted by
researchers. The task is generally divided into
three categories: lexical-level, sentence-level and
discourse-level normalization. In this paper we
focus on lexical-level normalization, which aims
to transform informal words into their standard
forms.
Lexical normalization can be regarded as a
spelling correction problem. However, researches
on spelling correction focus on typographic
and cognitive/orthographic errors (Kukich, 1992),
while text normalization focuses on lexical vari-
ants, such as phonetic substitutions, abbreviation
and paraphrases.
Unlike English, for which informal words are
detected according to whether they are out of vo-
cabulary, Chinese informal words manifest diver-
sity. Wang et al. (2013) divided informal words
into three types: phonetic substitutions, abbrevi-
ations and neologisms. Li and Yarowsky (2008)
classified them into four types: homophone, ab-
breviation, transliteration and others. Due to vari-
ant characteristics, they normalise informal words
by training a model per type, leading to increased
system complexity.
Research reveals that most lexical variants have
an unambiguous standard form (Han et al., 2012;
Li and Yarowsky, 2008). The validity of this as-
sumption is also empirically assessed on our cor-
pus annotation in Section 6.1. Based on this as-
sumption, we seek standard forms of informal
words from a constructed normalization dictio-
nary, avoiding diversity on informal words.
</bodyText>
<sectionHeader confidence="0.730746" genericHeader="method">
3 Joint Segmentation and Normalization
</sectionHeader>
<subsectionHeader confidence="0.994222">
3.1 Transition-based Segmentation
</subsectionHeader>
<bodyText confidence="0.996205">
We adapt the segmenter of Zhang and Clark
(2007) as our baseline segmenter. Given an input
sentence x, the baseline segmenter finds a segmen-
tation by maximizing:
</bodyText>
<equation confidence="0.9972835">
F(x) = argmax Score(y) (1)
yeGen(x)
</equation>
<bodyText confidence="0.995104625">
where Gen(x) denotes the set of all possible seg-
mentations for an input sentence.
Zhang and Clark (2007) proposed a graph-
based scoring model, with features based on com-
plete words and word sequences. We adapt their
method slightly, under a transition-based frame-
work (Zhang and Clark, 2011), which gives us a
consistent way of defining all models in this paper.
</bodyText>
<figureCaption confidence="0.998602">
Figure 1: A state of transition-based model.
</figureCaption>
<bodyText confidence="0.999848615384615">
Here a transition model is defined as a quadruple
M = (C, T, W, Ct), where C is a state space, T
is a set of transitions, each of which is a function:
C → C, W is an input sentence c1... cn, Ct is a
set of terminal states. A model scores the output
by scoring the corresponding transition sequence.
As shown in Figure 1, a state is a tuple ST =
(S, Q), where S contains partially segmented se-
quences, and Q = (ci, ci+1, ..., cn) is the sequence
of input characters that have not been processed.
When the character ci is processing, the transition
system would operate one of two actions that are
defined as follows:
</bodyText>
<listItem confidence="0.9956796">
(1) APP(ci), removing ci from Q, and append-
ing it to the last (partial) word in S.
(2) SEP(ci), removing ci from Q, making the
last word in S as completed, and adding ci as a
new partial word.
</listItem>
<bodyText confidence="0.9971492">
Given the sentence “Z ( jJ &apos; UK !(How
great work pressure is!)”, the sequences of ac-
tion “SEP(Z), APP(J( ), SEP(E), APP(j7),
SEP(&apos;), SEP(UJ), SEP(!)” can be used to ana-
lyze its structure.
</bodyText>
<subsectionHeader confidence="0.999973">
3.2 Joint Segmentation and Normalization
</subsectionHeader>
<bodyText confidence="0.999085">
Our SN model extends the transition-based seg-
mentation model. In addition to the actions APP
and SEP, the transition system also contains a
SEPS action, which substitutes an in formal word
on the top of S if it exists in the normalization dic-
tionary. Figure 2 gives a normalization transition
process for the sentence “�J( ��&apos;J!(How
great work pressure is!)”. During processing the
character “&apos;(big)”, the following actions can be
applied.
</bodyText>
<listItem confidence="0.999523666666667">
(1) APP(“&apos;(big)”), appending “&apos;(big)” to the
last word “YjV(y¯ali, pear)” in the informal la-
beled sequence.
(2) SEP(“&apos;(big)”), making the last word “Yj
�(y¯ali, pear)” in the informal labeled sequence as
a completed word, and adding “&apos;(big)” as a new
partial word.
(3) SEPS(“&apos;(big)”, “,s )J(y¯ali, pressure)”),
operating the action SEP(“&apos;(big)”), and using
</listItem>
<figure confidence="0.806873333333333">
Stack
Queue
... S2
S1
S0
C1 ... Cn
</figure>
<page confidence="0.977092">
1838
</page>
<table confidence="0.983356458333333">
Sentence: å\-¨&apos;J! (How great work pressure is!)
State Action Stack Queue Dictionary
&amp;&amp; Org: å\ -¨ &apos;J! -¨- 
work pear big ah! pear - pressure
Nor: å\ Am-
work child paper -child
N#- VZ
neckerchief - microblog
baË- MË
basin friend - friend
&amp;&amp;&amp;&amp;
3;+1 APP(“&apos;”) 09!
Org: å\ -¨&apos; (ah!)
work pear big
Nor: å\
work
SEP(“&apos;”) Org: å\ -¨ &apos;
work pear big
Nor: å\
work
SEPS(“&apos;”, Org: å\ -¨ &apos;
“”) work pear big
Nor: å\ 
work pressure
</table>
<figureCaption confidence="0.9911185">
Figure 2: Transition actions for joint segmentation
and normalization.
</figureCaption>
<bodyText confidence="0.99410252631579">
the standard form “(y¯ali, pressure)” for the
informal word “ -¨(y¯ali, pear)”.
Given the sentence “å \ -¨ &apos; UK !(How
great work pressure is!)”, the sequences of ac-
tion “SEP(å), APP(\), SEP(-), APP(¨),
SEPS(&apos;, ), SEP(UJ), SEP(!)” can be used
to analyze its structure.
Lexical substitution is based on a normalization
dictionary whose entries consist of &lt;lexical vari-
ant, standard form&gt; pairs. The output is a pair
of labeled sequences, containing the informal la-
beled sequence and the corresponding formal la-
beled sequence. To rank the candidates, both la-
beled sequences can be scored. However, lacking
annotated corpora on informal texts, we only use
the score of formal labeled sequence in our model.
The advantage is that we can train our model by
using standard corpus only, overcoming the lack
of annotated corpora on informal texts.
</bodyText>
<subsectionHeader confidence="0.999291">
3.3 Training and Decoding
</subsectionHeader>
<bodyText confidence="0.999962733333333">
We apply the global training and beam-search de-
coding framework of Zhang and Clark (2011). An
agenda is used by the decoder to keep the N-best
states during the incremental process. Before de-
coding starts, the agenda is initialized with the ini-
tial state. When a character is processed, existing
states are removed from the agenda and extended
with all possible actions, and the N-best newly
generated states are put back onto the agenda. Af-
ter all states have been terminal, the highest-scored
state from the agenda is taken as the output.
Algorithm 1 shows pseudocode for the decoder.
ADDITEM adds a new item into the agenda, N-
BEST returns the N highest-scored items from the
agenda, and BEST returns the highest-scored item
</bodyText>
<construct confidence="0.862946666666667">
Algorithm 1: Decoder
Input: sent, Dictionary // sent: informal sentence
Output:Best normalization sentence
</construct>
<listItem confidence="0.777328230769231">
1. agenda NULL
2. for idx in [0..LEN(sent)]:
3. for state in agenda:
4. new APP(state, sent[idx])
5. ADDITEM(agenda, new)
6. new SEP(state, sent[idx])
7. ADDITEM(agenda, new)
8. norWordsGETNWORD(state.lastWord)
9. for word in norWords
10. new SEPS(state,sent[idx],word)
12. ADDITEM(agenda, new)
13. agenda N-BEST(agenda)
14. return BEST(agenda)
</listItem>
<bodyText confidence="0.947165428571429">
from the agenda. GETNWORD returns a possible
standard form set of last word, seeking from nor-
malization dictionary. APP appends a character
to the last word in a state, SEP joins a character
as the start of a new word in a state, SEPS oper-
ates SEP and replaces the last word by a possible
standard form.
</bodyText>
<subsectionHeader confidence="0.746112">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.99998376">
In the experiments, we use the segmentation fea-
ture templates of Zhang and Clark (2011). These
features are effective for segmentation on formal
text. However, for text normalization, these fea-
tures contain insufficient information. Our exper-
iments show that by using Zhang and Clark’s fea-
tures, the F-Score on normalization is only 0.4207.
Prior work has shown that the language statis-
tic information is important for text normalization
(Wang et al., 2013; Li and Yarowsky, 2008; Kaji
and Kitsuregawa, 2014). As a result, we extract
language model features by using word-based lan-
guage model learned from a large quantity of stan-
dard texts. In particular, 1-gram, 2-gram, 3-gram
features are extracted. Every type of n-gram is di-
vided into ten probability ranges. For example, if
the probability of the word bigram: “ - &apos; ”
(high pressure) is in the 2nd range, the feature is
represented as “word-2-gram=2”.
In our experiments, language models are trained
on the Gigaword corpus1 with SRILM tools2. To
train a word-based language model, we segmented
the corpus using our re-implementation of Zhang
and Clark (2010). Results show that language
model information not only improves the perfor-
</bodyText>
<footnote confidence="0.999873">
1https://catalog.ldc.upenn.edu/LDC2003T05
2http://www.speech.sri.com/projects/srilm/
</footnote>
<page confidence="0.995871">
1839
</page>
<bodyText confidence="0.986809">
mance of text normalization, but also increases the
performance of word-segmentation.
</bodyText>
<sectionHeader confidence="0.992436" genericHeader="method">
4 Extension for Joint Segmentation,
Normalization and POS tagging
</sectionHeader>
<subsectionHeader confidence="0.999892">
4.1 Joint Segmentation and POS Tagging
</subsectionHeader>
<bodyText confidence="0.999521461538462">
In order to reduce the error propagation of word
segmentation, joint models have been applied to
some NLP tasks, such as POS tagging (Zhang and
Clark, 2010; Kruengkrai et al., 2009) and Parsing
(Zhang et al., 2014a; Qian and Liu, 2012; Zhang
et al., 2014b).
We take the joint word segmentation and POS
tagging model of Zhang and Clark (2010) as the
joint baseline. It extends from transition-based
segmenter, adding POS arguments to the original
actions. In Figure 1, when the current character ci
is processing, the transition system for ST would
operate as follows :
</bodyText>
<listItem confidence="0.815391166666667">
(1) APP(ci), removing ci from Q, and append-
ing it to the last (partial) word in 5 with the same
POS tag, .
(2) SEP(ci, pos), removing ci from Q, making
the last word in 5 as completed, and adding ci as
a new partial word with a POS tag “pos”.
</listItem>
<bodyText confidence="0.9999186">
Given the sentence “Z f�  )J &apos; UK !(How
great work pressure is!)”, the sequences of ac-
tion “SEP(Z, NN), APP(J( ), SEP(, NN),
APP()J), SEP(&apos;, VA), SEP(UJ, SP), SEP(!,
PU)” can be used to analyze its structure.
</bodyText>
<subsectionHeader confidence="0.985227">
4.2 Joint Segmentation, Normalization and
POS Tagging
</subsectionHeader>
<bodyText confidence="0.9990338">
Our joint model extends the model of Zhang and
Clark (2010) by adding a SEPS action, which sub-
stitutes formal word for last word in 5 if exists in
the dictionary. On the other hand, it can also be
regarded as an extension of the joint segmentation
and normalization model, adding POS arguments
to the original actions.
Using the same example shown in Figure 2, the
following three actions can be applied for the char-
acter “ &apos;(big)”:
</bodyText>
<listItem confidence="0.957751375">
(1) APP(“&apos;(big)”), appending “&apos;(big)” to the
last word “YI¨(y¯al´ı, pear)” in the informal la-
beled sequence, which remain with the same POS
tag “NN”.
(2) SEP(“&apos;(big)”, VA), making the last word
“YI ¨(y¯ali, pear)” in the informal labeled se-
quence as a completed word and adding “&apos;(big)”
as a new partial word with a POS tag “VA”.
</listItem>
<equation confidence="0.856241363636364">
Text Relation
*I0A**fft- (WiI0, Wit)
(Overseas returnees is also (overseas ret-
referred to as turtles.) urnee, turtle)
—õ*1;(fAA1I-Mi4±7
}��-���õ�ëAIC (A�ICI, AIC)
�A�IC��-
(A tree, seemingly a little high,
fails a lot of people. Well, (advanced mathem-
this tree is called high number atics, high number)
(advanced mathematics))
</equation>
<tableCaption confidence="0.990526">
Table 1: Relation patterns in microblogs.
</tableCaption>
<listItem confidence="0.647847">
(3) SEPS(“&apos;(big)”, VA, “ )J(y¯ali, pres-
</listItem>
<bodyText confidence="0.984848272727273">
sure)”), operating the action SEP(“&apos;(big)”, VA),
and using the standard form “)J(y¯al`ı, pressure)”
for the informal word “YI¨(y¯ali, pear)”.
Given the sentence “Z f� YI ¨ &apos; UK !(How
great work pressure is!)”, the sequences of ac-
tion “SEP(Z, NN), APP(J( ), SEP(YI, NN),
APP(¨), SEPS(&apos;, VA,  )J), SEP(UJ, SP),
SEP(!, PU)” can be used to analyze its structure.
We use the same training and decoding frame-
work as our joint segmentation, normalization and
POS tagging model, as described in section 3.3.
</bodyText>
<sectionHeader confidence="0.9964255" genericHeader="method">
5 Construction of Normalization
Dictionary
</sectionHeader>
<bodyText confidence="0.999460260869565">
Although large-scale normalization dictionaries
are difficult to obtain, informal/formal relations
could be extracted from large-scale web corpora
(Li and Yarowsky, 2008), and informal words are
mainly derived using fixed word-formation pat-
terns. In this paper, we adopt two methods to con-
struct a normalization dictionary.
The first method is to extract informal/formal
pairs from large-scale text. In general, many infor-
mal and formal words co-occur in the same texts
or similar contexts. We can find their relations
with text patterns. As shown in Table 1, the first
example follows the “formalA/Finformal” (“A
F” means “is also referred to as”) definition pat-
tern, while the second example follows the pattern
“informal(formal)”. This gives us a reliable way to
seed and bootstrap a list of informal/formal pairs.
We use a bootstrapping algorithm to extract in-
formal/formal pairs from large-scale microblogs.
First, a small set of example relations are collected
manually. Second, using these relations as a seed
set, we extract the text patterns, with which we
identify more new relations from the data and aug-
</bodyText>
<page confidence="0.980092">
1840
</page>
<construct confidence="0.958521363636363">
informaltb AformalS,01, formaltb Oinformal,
informal(formal), )jW40formalO)jinformal,
formalADginformal,informal:...ùformalOIN,
informalA &amp;quot;Hformal, “formal”*frTl“informal”,
M“informal”W,ft“formal”, informal4tAformalSA,
informall7VS, (formal, formal---1—gAinformal,
formallMinformal, RÜ...formalDginformal,
O...formal)jinformal, informalAformalA H,
“formal”uMAT“informal”, nformallMinformal,
informal74T1---AformalMO, :formal—q�informal,
RÜ...infromalSAAformal.
</construct>
<tableCaption confidence="0.994545">
Table 2: Examples of text patterns.
</tableCaption>
<bodyText confidence="0.999940441176471">
ment them into the seed set. Table 2 shows the
initial text patterns extracted form the examples.
The procedure iterates until it cannot identify new
relations. There is much noise in the extracted
informal/formal pairs. We re-rank them using a
similarity-based classifier with weak supervision,
with the positive pairs being inserted into dictio-
nary.
The second method is to generate new infor-
mal/formal pairs using word-formation patterns
extracted from informal/formal pairs. Although
Chinese informal words manifest diversity, infor-
mal words are mainly derived using fixing word-
formation methods, such as compounds, phonetic
substitutions, abbreviations, acronym, reduplica-
tion. We can learn the pattern of informal word-
formation from informal/formal pairs. For exam-
ple, in informal/formal pair “ 4¸(m`eizhˇı, sis-
ter paper)/4P(m`eizˇı, sister)”, informal word “4
¸(m`eizhˇı, sister paper)” is builded from formal
word “4P(m`eizˇı, sister)” by the pattern “P→
¸”. Using this pattern, we can generate many new
informal/formal pairs, such as “&amp;X¸(h`anzhˇı, man
paper)/&amp;XP(h`anzˇı, man)”, “-%¸(n´anzhˇı, man pa-
per)/-%P(n´anzˇı, man)”, “Y¸(s¯unzhˇı, grandson
paper)/ YP(s¯unzˇı, grandson)”, in which the for-
mal words contain character “P”.
In the experiments, we constructed the nor-
malization dictionary consisting of 32,787 infor-
mal/formal word pairs in total. The dictionary
is used to tamper the formal training data for
the joint segmentation and normalization systems
with 25% of the formal words in the dictionary be-
ing replaced with their informal equivalents.
</bodyText>
<table confidence="0.999697">
Num Ratio Agree
Phonetic 572 0.870 0.95
Substitutions
Abbreviation 69 0.105 0.97
Paraphrases 17 0.025 0.90
Total 658 1 0.95
</table>
<tableCaption confidence="0.988038">
Table 3: Frequency distribution and annotation
agreement on various types of informal words.
</tableCaption>
<sectionHeader confidence="0.998983" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999335">
6.1 Microblog Corpus Annotation
</subsectionHeader>
<bodyText confidence="0.999992071428572">
To evaluate our model, we develop a microblog
corpus. Our annotated corpus is collected from
Sina Weibo3, which is the largest microblogging
platform in China. More than 1,000,000 Chinese
posts are crawled using Sina Weibo API. Among
these, 4,000 posts were randomly selected. We
follow Wang et al. (2012) and apply rules to
preprocess the corpus’URLs, emoticons, “@user-
names&amp;quot;and Hashtags as pre-segmented words.
As a result, we obtain 2,000 sentences as a source
of the corpus.
Two human participants annotated the 2,000
sentences by using the tools we developed. The
tools can simultaneously annotate word bound-
aries, POS and text normalization. We used the
CTB scheme for word segmentation and POS
tagging. We divided informal words into three
types: Phonetic Substitutions, Abbreviation, Para-
phrases. In total, we annotated 1,129 informal
word-pairs in the 2,000 sentences, which con-
tained 658 different informal words.
Table 3 shows the frequency distribution and
annotation agreement over three types of informal
words in corpus. The Cohen’s kappa is 0.95 for
informal words annotation, which shows that it
is easy for humans to distinguish informal words,
and validates our assumption that informal word
generally has one formal word equivalent.
</bodyText>
<subsectionHeader confidence="0.999729">
6.2 Settings and Measures
</subsectionHeader>
<bodyText confidence="0.999813428571429">
Our model is trained on the Chinese Treebank
(CTB) 74, which is a large, word segmented, POS
tagged and fully bracketed Chinese news corpus.
The annotated microblog corpus is randomly di-
vided into two parts: 1,000 sentences for develop-
ment and 1,000 sentences for test.
The standard F-measure is used to measure the
</bodyText>
<footnote confidence="0.995888">
3http://www.weibo.com/
4https://catalog.ldc.upenn.edu/LDC2010T07
</footnote>
<page confidence="0.949794">
1841
</page>
<table confidence="0.999472666666667">
Development Test
Seg-F Nor-F Seg-F Nor-F
S;N 0.8859 0.3956 0.8885 0.4058
SN 0.8946 0.4053 0.8945 0.4207
S;N+lm 0.9101 0.5897 0.9132 0.6276
SN+lm 0.9202 0.6009 0.9240 0.6392
</table>
<tableCaption confidence="0.687371">
Table 4: Segmentation and normalization results.
S;N denotes the pipeline model. SN denotes the
joint model. lm denotes language model features.
</tableCaption>
<bodyText confidence="0.998736538461538">
accuracies of word segmentation, POS tagging
and text normalization, where the accuracy is F
= 2PR/(P+R). In addition, we use recall rates to
evaluate the identification accuracies of formal, in-
formal and all words. The recall rate of formal
words N-R is defined as the percentage of gold
standard output formal words that are correctly
segmented, the recall rate of informal words I-R
is defined as the percentage of gold-standard out-
put informal words that are correctly segmented
and the recall rate of all words ALL-R is defined as
the percentage of gold standard output words that
are correctly segmented.
</bodyText>
<subsectionHeader confidence="0.999733">
6.3 Joint Segmentation and Normalization
</subsectionHeader>
<bodyText confidence="0.999982423076923">
Our development set is used to decide the beam
size and the number of training iterations. The best
performances on the development set are obtained
when the beam size is set to 16 and the number of
iterations is set to 32.
Comparison with pipeline We investigate the
influence of the language model and analyze the
result compared to the baseline. Table 4 shows the
results on the development and test sets, where SN
model is joint model and S;N is pipeline model.
Our SN model performs better on segmentation
than pipeline S;N model, demonstrating the effec-
tiveness of normalization.
Table 5 shows the accuracies (i.e., recall rate)
of formal and informal word identification on the
development set. After normalization, the accu-
racy of informal word identification has a large
improvement, and the accuracy of formal word
identification also increases. This shows that for-
mal words can be better recognized when infor-
mal words are identified correctly. It demonstrates
that text normalization is effective for both infor-
mal words and formal words.
The effect of language model From Table 4,
we observe that the performances increase when
using language model features. Particularly, the
</bodyText>
<table confidence="0.779527166666667">
Segmentation
models N-R I-R ALL-R
S;N 0.8711 0.5100 0.8624
SN 0.8716 0.6653 0.8652
S;N+lm 0.9143 0.4229 0.9025
SN+lm 0.9149 0.7752 0.9109
</table>
<tableCaption confidence="0.712053">
Table 5: Formal and informal word accuracies on
</tableCaption>
<bodyText confidence="0.993997142857143">
the development test. N-R denotes the recall rate
of formal words, I-R denotes the recall rate of in-
formal words, ALL-R denotes the recall rate of all
words.
normalization accuracy improves more signifi-
cantly. It indicates that statistical language model
knowledge play an important role on text normal-
ization. Using language model features, our SN
model improves more in the segmentation F-Score
compared with the baseline system.
Furthermore, we also find that the language
model features are helpful to identifying the for-
mal words, as shown in Table 5. The identification
accuracy of informal words increases on the SN
model, while the accuracy decreases on the S;N
model. Due to the relatively low frequency of in-
formal words, they score lower on informal text by
using the language model information, resulting in
incorrect word segmentations. This illustrates that
our joint model is more suitable for microblogs
than the pipeline method.
</bodyText>
<subsectionHeader confidence="0.964687">
6.4 Joint Segmentation, Normalization and
POS tagging
</subsectionHeader>
<bodyText confidence="0.99920447368421">
We compare the following models on word seg-
mentation, text normalization and POS tagging.
ST Our re-implementation of Zhang and
Clark(2010). We investigate how the joint model
contributes to improving accuracy of word seg-
mentation and POS tagging in microblog domain.
S;N;T It is a pipe-line method for segmentation,
normalization and POS tagging. The segmentation
model does not use the features of POS. The nor-
malization model uses segmentation information,
but not features of POS. The POS tagging model
does not need to segmentation.
SN;T It is another pipe-line method that first
performs segmentation and normalization, then
performs POS tagging. The SN model does not
use the features of POS, and the POS tagging
model does not need to segmentation.
SNT Our joint segmentation, normalization,
and POS tagging model.
</bodyText>
<page confidence="0.996343">
1842
</page>
<sectionHeader confidence="0.969922" genericHeader="evaluation">
6.4.1 Results
</sectionHeader>
<bodyText confidence="0.99981487804878">
Table 6 shows the final results on the test set. Pre-
vious work has shown that the systems trained on
news data give poor accuracies of word segmen-
tation and POS tagging in the microblog domain.
As shown in Table 6,the F-Score of segmentation
and POS tagging is 0.902 and 0.8163 respectively
by using the Stanford segmenter and POS tagger.
Comparing ST and SNT, we find that text nor-
malization can enhance word segmentation and
POS tagging in the microblog. SNT achieved
larger improvements over the baseline with lan-
guage features, reducing segmentation errors by
12.02% and POS errors by 3.63%.
Another goal of the experiment is to illustrate
whether the three tasks benefit from each other.
Comparing SN;T to S;N;T shows that the perfor-
mance increases by join segmentation and normal-
ization. It indicates that segmentation and text
normalization benefit from each other. On other
hand, our SNT model yields better performance
than SN;T. It indicates that POS features are effec-
tive for segmentation and text normalization, and
hence three tasks benefit from each other.
The effect of the normalization dictionary
The dictionary plays an important role in our
model, which reduces the number of OOV words.
Intuitively, the performance is higher when the
coverage of dictionary is larger. In the experi-
ments, the coverage of our dictionary on the devel-
opment and tests are 45.8%,48.2% respectively.
To investigate the effect of the dictionary on
our model, we manually construct ten dictionar-
ies from our development data, with coverage be-
tween 10% and 100%. Figure 3 shows the F-
score curves of test set on segmentation and POS-
tagging for both SNT+lm and ST+lm model by
different dictionaries. With the coverage of the
dictionaries increasing from 10% to 100%, the
F-score generally increases. When the coverage
is greater than about 20%, the F-score for joint
model is higher than for the baseline model.
</bodyText>
<subsectionHeader confidence="0.649855">
6.4.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999959375">
We found two major categories of errors. Abbre-
viation is sometimes incorrectly normalised, es-
pecially an informal word mapping to more than
one formal word. For example, informal word “
Xv” mapped to “XP9vÏ” (American idol),
which consists of two words: “XQ” (American)
and “ vÏ” (idol). However, our model cannot
normalise the word “ Xv” in the experiment.
</bodyText>
<figure confidence="0.991288888888889">
0.94
0.92
0.9
0.88
0.86
0.84
0.82
10 20 30 40 50 60 70 80 90 100
Dictionary coverage(%)
</figure>
<figureCaption confidence="0.9767745">
Figure 3: Results of SNT+lm and ST+lm based on
different dictionaries for test set.
</figureCaption>
<table confidence="0.9993786">
Seg-F POS-F Nor-F
Stanford 0.9058 0.8163
ST 0.8934 0.8263
S;N;T 0.8885 0.8197 0.4058
SN;T 0.8945 0.8287 0.4207
SNT 0.8995 0.8296 0.4391
ST+lm 0.9162 0.8401
S;N;T+lm 0.9132 0.8341 0.6276
SN;T+lm 0.9240 0.8439 0.6392
SNT+lm 0.9261 0.8459 0.6413
</table>
<tableCaption confidence="0.984091">
Table 6: Results on the test set. ST denotes the
</tableCaption>
<bodyText confidence="0.988219142857143">
joint segmentation and POS tagging model. S;N;T
denotes the pipeline model. SN denotes the joint
segmentation and normalization model. SNT de-
notes the joint segmentatin. normalization and
POS tagging model. lm denotes language model
features. Seg-F denotes the F-Score of segmenta-
tion. POS-F denotes the F-Score of POS tagging.
Nor-F denotes the F-Score of normalization.
Another type of error is phonetic substitutions
of numbers, which are sometimes identified incor-
rectly. For example. “7456” is identified as a num-
ber in the experiments, but it means “����”
(I’m so angry). To settle this problem, it needs
more context information.
</bodyText>
<subsectionHeader confidence="0.991926">
6.5 Results of Lexical Normalization
</subsectionHeader>
<bodyText confidence="0.9690455">
It is interesting to explore how well the joint model
can normalize informal words. We compare our
results with two existing systems on text normal-
ization based on our annotated microblog corpus.
(1) WangDT We re-implement Wang et al.
(2013), which formalized the task as a classifica-
tion problem and proposed rule-based and statisti-
cal features to model three plausible channels that
explain the connection between formal and infor-
mal pairs. We use a single decision tree classifier
</bodyText>
<equation confidence="0.7013055">
POS(ST+lm) Seg(ST+lm) POS(SNT+lm) Seg(SNT+lm)
F-Score
</equation>
<page confidence="0.790955">
1843
</page>
<table confidence="0.99970725">
P R F
SNT+lm 0.9027 0.4920 0.6413
WangDT 0.6214 0.5543 0.5859
LYTop1 0.6338 0.4920 0.5540
</table>
<tableCaption confidence="0.999778">
Table 7: Results of lexical normalization.
</tableCaption>
<bodyText confidence="0.99955816">
in the experiment.
(2) LYTop1 Li and Yarowsky (2008) formal-
ized the task as a ranking problem and proposed
a conditional log-linear model to normalization.
In the experiment, we select top 1 as the standard
form of informal word.
We use the same division with 1000 sentences
for training and 1000 for test. The training data is
used for both the WangDT and LY. We re-segment
the corpus using Stanford tools for the two base-
lines. WangDT uses CRF to detection informal
words and LYTop1 uses the informal words de-
tected using our joint model.
Although it is a little unfair for the two baselines
compared with our joint model, which uses the ex-
ternal knowledge - normalization dictionary. The
experiments can partly reflect some conclusions.
Table 7 shows the results of normalization by dif-
ferent systems. The performance of our model is
the best among the three systems. In particular,
the precision in our SNT model improves upon
the baselines significantly. The main reason is that
our model is based on global features over whole
sentences, while the two baselines based on local
windows features.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999982768115942">
There has much work on text normalization. The
task is generally treated as a noisy channel prob-
lem (Pennell and Liu, 2014; Cook and Steven-
son, 2009; Yang and Eisenstein, 2013; Sonmez
and Ozgur, 2014) or a translation problem (Aw
et al., 2006; Contractor et al., 2010; Li and Liu,
2012; Zhang et al., 2014c). For English, most
recent work (Han and Baldwin, 2011; Gouws et
al., 2011; Han et al., 2012) uses two-step unsuper-
vised approaches to first detect and then normalize
informal words. They aim to produce and use in-
formal/formal word lexicons and mappings.
In processing Chinese informal text, Wong and
Xia (2008) address the problem of informal words
in bulletin board system (BBS) chats by employ-
ing pattern matching. Xia et al. (2005) also use
SVM-based classification to recognize Chinese in-
formal sentences chats. Both methods have their
advantages: the learning-based method does bet-
ter on recall, while the pattern matching performs
better on precision.
Li and Yarowsky (2008) tackle the problem of
identifying informal/formal Chinese word pairs by
generating candidates from Baidu search engine
and ranking using a conditional log-linear model.
Zhang et al. (2014c) analyze the phenomena of
mixed text in Chinese microblogs, proposing a
two-stage method to normalise mixed texts. How-
ever, their models employ pipelined words seg-
mentation, resulting in reduced performance.
Wang and Kan (2013) propose a joint model to
process word segmentation and informal word de-
tection. However, text normalization is split to an-
other task (Wang et al., 2013). Our joint model
process word segmentation, POS tagging and nor-
malization simultaneously. Kaji et al. (2014)
propose a joint model for word segmentation,
POS tagging and normalization for Japanese Mi-
croblogs. Their model is trained on a partially an-
notated microblog corpus. In contrast, our model
can be trained on existing annotated corpora in
standard text.
Researchers have recently developed various
microblog corpora annotated with rich linguistic
information. Gimpel et al. (2011) and Foster et
al. (2011) annotate English microblog posts with
POS tags. Han and Baldwin (2011) release a mi-
croblog corpus annotated with normalized words.
Duan et al. (2012) develop a Chinese microblog
corpus annotated with segmentation for SIGHAN
bakeoff. Wang et al. (2013) release a Chinese mi-
croblog corpus for word segmentation and infor-
mal word detection. However, there are no mi-
croblog corpora annotated Chinese word segmen-
tation, POS tags, and normalized sentences.
Our work is alse related to the work of word
segmentation (Zhang and Clark, 2007; Zhang et
al., 2013; Chen et al., 2015) and joint word seg-
mentation and POS-tagging (Jiang et al., 2008;
Zhang and Clark, 2010). A comprehensive sur-
vey is out of the scope of this paper, but interested
readers can refer to Pei et al. (Pei et al., 2014) for
a recent literature review of the fields.
To evaluate our model, we develop an annotated
microblog corpus with word segmentation, POS
tags, and normalization. Furthermore, we train
our model by using a standard segmented and POS
tagged corpus. We also present a comprehensive
evaluation in terms of precision and recall on our
</bodyText>
<page confidence="0.983622">
1844
</page>
<bodyText confidence="0.999805666666667">
microblog test corpus. Such an evaluation has not
been conducted in previous work due to the lack
of annotated corpora for Chinese microblogs.
</bodyText>
<sectionHeader confidence="0.994654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999970368421053">
We proposed a joint model of word segmentation,
POS tagging and normalization, in which the three
tasks benefit from each other. The model is trained
on standard corpora, hence there is no need to re-
train it for new microblog corpora. The results
demonstrated that the model can improve the per-
formance of word segmentation and POS tagging
with text normalization on microblogs, and our
model can benefit from the language statistical in-
formation, which is not suitable to segment word
and tag POS directly for microblogs because of the
relatively low frequency of informal words.
In our model, lexical substitution is based
on a normalization dictionary, which avoids
the diversity of informal words, simplifying
this problem for real world applications. The
codes of the joint model and data set are pub-
lished at the website: https://github.com/
qtxcm/JointModelNSP.
</bodyText>
<sectionHeader confidence="0.997248" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999450666666667">
We thank all reviewers for the insightful com-
ments. This work is supported by the
State Key Program of National Natural Sci-
ence Foundation of China (No.61133012), the
National Natural Science Foundation of China
(No.61373108, 61373056, 61202193), the Key
Program of Natural Science Foundation of Hubei,
China(No.2012FFA088), the National Philoso-
phy Social Science Major Bidding Project of
China (No.11&amp;ZD189) and the Singapore Min-
istration and Education (MOE) AcRF project
T2MOE201301.
</bodyText>
<sectionHeader confidence="0.997883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99923984375">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In Proceedings of the COLING/ACL, pages
33–40.
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015. Gated recursive neural network for
chinese word segmentation. In Proceedings of the
53rd ACL, pages 1744–1753, July.
Danish Contractor, Tanveer A Faruquie, and L Venkata
Subramaniam. 2010. Unsupervised cleansing of
noisy text. In Proceedings of the 23rd COLING,
pages 189–196.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of the workshop on computational ap-
proaches to linguistic creativity, pages 71–78.
Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.
2012. The cips-sighan clp 2012 chinese word seg-
mentation on microblog corpora bakeoff. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Con-
ference on Chinese Language Processing, Tianjin,
China, pages 35–40.
Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011. #
hardtoparse: Pos tagging and parsing the twitter-
verse. In AAAI 2011 Workshop on Analyzing Mi-
crotext, pages 20–25.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th ACL, pages 42–47.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the
Workshop on Languages in Social Media, pages 20–
29.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th ACL, pages 368–378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 EMNLP,
pages 421–432.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings ofACL-08: HLT, pages 897–904.
Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Accu-
rate word segmentation and pos tagging for japanese
microblogs: Corpus annotation and joint modeling
with lexical normalization. In Proceedings of the
2014 EMNLP), pages 99–109, October.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th ACL and the 4th AFNLP, pages 513–521.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys
(CSUR), 24(4):377–439.
</reference>
<page confidence="0.825806">
1845
</page>
<reference confidence="0.999439222222222">
Chen Li and Yang Liu. 2012. Normalization of text
messages using character-and phone-based machine
translation approaches. In Thirteenth Annual Con-
ference of the International Speech Communication
Association.
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal chi-
nese phrases from web corpora. In Proceedings of
the 2008 EMNLP, pages 1031–1040.
Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd ACL, pages
293–303, June.
Deana L Pennell and Yang Liu. 2014. Normalization
of informal text. Computer Speech &amp; Language,
28(1):256–277.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Proceed-
ings of the 2012 EMNLP, pages 501–511, July.
Cagil Sonmez and Arzucan Ozgur. 2014. A graph-
based approach for contextual text normalization. In
Proceedings of the 2014EMNLP, pages 313–324.
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from chinese microtext: Joint word recog-
nition and segmentation. In ACL (1), pages 731–
741.
Aobo Wang, Tao Chen, and Min-Yen Kan. 2012. Re-
tweeting from a linguistic perspective. In Proceed-
ings of the second workshop on language in social
media, pages 46–55.
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of IJCNLP, pages 127–135.
Kam-Fai Wong and Yunqing Xia. 2008. Normaliza-
tion of chinese chat language. Language Resources
and Evaluation, 42(2):219–242.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. Nil
is not nothing: Recognition of chinese network in-
formal language expressions. In 4th SIGHAN Work-
shop on Chinese Language Processing at IJCNLP,
volume 5.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In
EMNLP, pages 61–72.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th ACL, pages 840–847, June.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using a
single discriminative model. In Proceedings of the
2010 EMNLP, pages 843–852.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 EMNLP,
pages 311–321, October.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014a. Character-level chinese dependency
parsing. In Proceedings of the 52nd ACL, pages
1326–1336, June.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014b. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In Proceedings
of the 14th EACL, pages 588–597.
Qi Zhang, Huan Chen, and Xuanjing Huang. 2014c.
Chinese-english mixed text normalization. In Pro-
ceedings of the 7th ACM international conference on
Web search and data mining, pages 433–442. ACM.
</reference>
<page confidence="0.993229">
1846
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239376">
<title confidence="0.9980745">A Transition-based Model for Joint Segmentation, POS-tagging Normalization</title>
<author confidence="0.959558">Yue Meishan Yafeng Donghong</author>
<affiliation confidence="0.998674666666667">School, Wuhan University, Wuhan, University of Technology and of Computer Science and Technology, Hubei University</affiliation>
<address confidence="0.518369">Science and Technology, XianNing,</address>
<email confidence="0.8340675">renyafeng,zhang,meishan</email>
<abstract confidence="0.975583">We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for sms text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1780" citStr="Aw et al., 2006" startWordPosition="262" endWordPosition="265">et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefor</context>
<context position="30000" citStr="Aw et al., 2006" startWordPosition="4710" endWordPosition="4713">le 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have </context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization. In Proceedings of the COLING/ACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Gated recursive neural network for chinese word segmentation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd ACL,</booktitle>
<pages>1744--1753</pages>
<contexts>
<context position="32401" citStr="Chen et al., 2015" startWordPosition="5087" endWordPosition="5090"> Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not </context>
</contexts>
<marker>Chen, Qiu, Zhu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015. Gated recursive neural network for chinese word segmentation. In Proceedings of the 53rd ACL, pages 1744–1753, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danish Contractor</author>
</authors>
<title>Tanveer A Faruquie, and L Venkata Subramaniam.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd COLING,</booktitle>
<pages>189--196</pages>
<marker>Contractor, 2010</marker>
<rawString>Danish Contractor, Tanveer A Faruquie, and L Venkata Subramaniam. 2010. Unsupervised cleansing of noisy text. In Proceedings of the 23rd COLING, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Proceedings of the workshop on computational approaches to linguistic creativity,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="1710" citStr="Cook and Stevenson, 2009" startWordPosition="250" endWordPosition="253">as shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual depend</context>
<context position="29906" citStr="Cook and Stevenson, 2009" startWordPosition="4693" endWordPosition="4697">external knowledge - normalization dictionary. The experiments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also </context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proceedings of the workshop on computational approaches to linguistic creativity, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiming Duan</author>
<author>Zhifang Sui</author>
<author>Ye Tian</author>
<author>Wenjie Li</author>
</authors>
<title>The cips-sighan clp 2012 chinese word segmentation on microblog corpora bakeoff.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>35--40</pages>
<location>Tianjin, China,</location>
<contexts>
<context position="31979" citStr="Duan et al. (2012)" startWordPosition="5018" endWordPosition="5021">POS tagging and normalization simultaneously. Kaji et al. (2014) propose a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs. Their model is trained on a partially annotated microblog corpus. In contrast, our model can be trained on existing annotated corpora in standard text. Researchers have recently developed various microblog corpora annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can ref</context>
</contexts>
<marker>Duan, Sui, Tian, Li, 2012</marker>
<rawString>Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li. 2012. The cips-sighan clp 2012 chinese word segmentation on microblog corpora bakeoff. In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, Tianjin, China, pages 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>¨Ozlem C¸etinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef Van Genabith</author>
</authors>
<title>hardtoparse: Pos tagging and parsing the twitterverse.</title>
<date>2011</date>
<booktitle>In AAAI 2011 Workshop on Analyzing Microtext,</booktitle>
<pages>20--25</pages>
<marker>Foster, C¸etinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, Van Genabith, 2011</marker>
<rawString>Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef Van Genabith. 2011. # hardtoparse: Pos tagging and parsing the twitterverse. In AAAI 2011 Workshop on Analyzing Microtext, pages 20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th ACL, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Donald Metzler</author>
<author>Congxing Cai</author>
<author>Eduard Hovy</author>
</authors>
<title>Contextual bearing on linguistic variation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="30139" citStr="Gouws et al., 2011" startWordPosition="4735" endWordPosition="4738">cular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky</context>
</contexts>
<marker>Gouws, Metzler, Cai, Hovy, 2011</marker>
<rawString>Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. 2011. Contextual bearing on linguistic variation in social media. In Proceedings of the Workshop on Languages in Social Media, pages 20– 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a# twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="1222" citStr="Han and Baldwin, 2011" startWordPosition="175" endWordPosition="178">tandard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach. 1 Introduction Microblogs, such as Twitter, SMS and Weibo, has become an important research topic in NLP. Previous work has shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 201</context>
<context position="30119" citStr="Han and Baldwin, 2011" startWordPosition="4731" endWordPosition="4734">three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precis</context>
<context position="31900" citStr="Han and Baldwin (2011)" startWordPosition="5005" endWordPosition="5008">it to another task (Wang et al., 2013). Our joint model process word segmentation, POS tagging and normalization simultaneously. Kaji et al. (2014) propose a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs. Their model is trained on a partially annotated microblog corpus. In contrast, our model can be trained on existing annotated corpora in standard text. Researchers have recently developed various microblog corpora annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A compreh</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. In Proceedings of the 49th ACL, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 EMNLP,</booktitle>
<pages>421--432</pages>
<contexts>
<context position="3436" citStr="Han et al., 2012" startWordPosition="517" endWordPosition="520">ization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, Lisbon, Portugal, 17-21 September 2015. c�2015 Associa</context>
<context position="5269" citStr="Han et al., 2012" startWordPosition="775" endWordPosition="778">on and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 Joint Segmentation and Normalization 3.1 Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F(x) = argmax Score(y) (1) yeGen(x) where Gen(x) denotes the set of all possible segmen</context>
<context position="30158" citStr="Han et al., 2012" startWordPosition="4739" endWordPosition="4742"> in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the </context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 EMNLP, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>897--904</pages>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings ofACL-08: HLT, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Accurate word segmentation and pos tagging for japanese microblogs: Corpus annotation and joint modeling with lexical normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 EMNLP),</booktitle>
<pages>99--109</pages>
<contexts>
<context position="11473" citStr="Kaji and Kitsuregawa, 2014" startWordPosition="1815" endWordPosition="1818">start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ - &apos; ” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2. To train a word-based language model, we segmented the corpus using our re-implementation of Zhang and Cla</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2014</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Accurate word segmentation and pos tagging for japanese microblogs: Corpus annotation and joint modeling with lexical normalization. In Proceedings of the 2014 EMNLP), pages 99–109, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th ACL and the 4th AFNLP,</booktitle>
<pages>513--521</pages>
<contexts>
<context position="12619" citStr="Kruengkrai et al., 2009" startWordPosition="1986" endWordPosition="1989">ge model, we segmented the corpus using our re-implementation of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new part</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings of the Joint Conference of the 47th ACL and the 4th AFNLP, pages 513–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="4555" citStr="Kukich, 1992" startWordPosition="673" endWordPosition="674">l Language Processing, pages 1837–1846, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. normalization task that are widely accepted by researchers. The task is generally divided into three categories: lexical-level, sentence-level and discourse-level normalization. In this paper we focus on lexical-level normalization, which aims to transform informal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys (CSUR), 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of text messages using character-and phone-based machine translation approaches.</title>
<date>2012</date>
<booktitle>In Thirteenth Annual Conference of the International Speech Communication Association.</booktitle>
<contexts>
<context position="1823" citStr="Li and Liu, 2012" startWordPosition="270" endWordPosition="273"> Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang </context>
<context position="30043" citStr="Li and Liu, 2012" startWordPosition="4718" endWordPosition="4721"> different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method</context>
</contexts>
<marker>Li, Liu, 2012</marker>
<rawString>Chen Li and Yang Liu. 2012. Normalization of text messages using character-and phone-based machine translation approaches. In Thirteenth Annual Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Mining and modeling relations between formal and informal chinese phrases from web corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 EMNLP,</booktitle>
<pages>1031--1040</pages>
<contexts>
<context position="3460" citStr="Li and Yarowsky, 2008" startWordPosition="521" endWordPosition="524"> be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational L</context>
<context position="4955" citStr="Li and Yarowsky (2008)" startWordPosition="729" endWordPosition="732">rmal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 Joint </context>
<context position="11444" citStr="Li and Yarowsky, 2008" startWordPosition="1811" endWordPosition="1814">ins a character as the start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ - &apos; ” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2. To train a word-based language model, we segmented the corpus using our re-im</context>
<context position="15402" citStr="Li and Yarowsky, 2008" startWordPosition="2451" endWordPosition="2454">ssure)” for the informal word “YI¨(y¯ali, pear)”. Given the sentence “Z f� YI ¨ &apos; UK !(How great work pressure is!)”, the sequences of action “SEP(Z, NN), APP(J( ), SEP(YI, NN), APP(¨), SEPS(&apos;, VA,  )J), SEP(UJ, SP), SEP(!, PU)” can be used to analyze its structure. We use the same training and decoding framework as our joint segmentation, normalization and POS tagging model, as described in section 3.3. 5 Construction of Normalization Dictionary Although large-scale normalization dictionaries are difficult to obtain, informal/formal relations could be extracted from large-scale web corpora (Li and Yarowsky, 2008), and informal words are mainly derived using fixed word-formation patterns. In this paper, we adopt two methods to construct a normalization dictionary. The first method is to extract informal/formal pairs from large-scale text. In general, many informal and formal words co-occur in the same texts or similar contexts. We can find their relations with text patterns. As shown in Table 1, the first example follows the “formalA/Finformal” (“A F” means “is also referred to as”) definition pattern, while the second example follows the pattern “informal(formal)”. This gives us a reliable way to seed</context>
<context position="28694" citStr="Li and Yarowsky (2008)" startWordPosition="4487" endWordPosition="4490"> with two existing systems on text normalization based on our annotated microblog corpus. (1) WangDT We re-implement Wang et al. (2013), which formalized the task as a classification problem and proposed rule-based and statistical features to model three plausible channels that explain the connection between formal and informal pairs. We use a single decision tree classifier POS(ST+lm) Seg(ST+lm) POS(SNT+lm) Seg(SNT+lm) F-Score 1843 P R F SNT+lm 0.9027 0.4920 0.6413 WangDT 0.6214 0.5543 0.5859 LYTop1 0.6338 0.4920 0.5540 Table 7: Results of lexical normalization. in the experiment. (2) LYTop1 Li and Yarowsky (2008) formalized the task as a ranking problem and proposed a conditional log-linear model to normalization. In the experiment, we select top 1 as the standard form of informal word. We use the same division with 1000 sentences for training and 1000 for test. The training data is used for both the WangDT and LY. We re-segment the corpus using Stanford tools for the two baselines. WangDT uses CRF to detection informal words and LYTop1 uses the informal words detected using our joint model. Although it is a little unfair for the two baselines compared with our joint model, which uses the external kno</context>
<context position="30746" citStr="Li and Yarowsky (2008)" startWordPosition="4830" endWordPosition="4833">ws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segmentation, resulting in reduced performance. Wang and Kan (2013) propose a joint model to process word segmentation and informal word detection. However, text normalization is split to another task (Wang et al., 2013). Our joint model process word</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal chinese phrases from web corpora. In Proceedings of the 2008 EMNLP, pages 1031–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Baobao Chang</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd ACL,</booktitle>
<pages>293--303</pages>
<contexts>
<context position="32614" citStr="Pei et al., 2014" startWordPosition="5127" endWordPosition="5130"> microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora for Chinese microblogs. 8 Conclusion We proposed a joint model of word segmentation, POS tagging and normalization, in which the three tasks ben</context>
</contexts>
<marker>Pei, Ge, Chang, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings of the 52nd ACL, pages 293–303, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana L Pennell</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of informal text.</title>
<date>2014</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="1684" citStr="Pennell and Liu, 2014" startWordPosition="246" endWordPosition="249">in NLP. Previous work has shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitive</context>
<context position="29880" citStr="Pennell and Liu, 2014" startWordPosition="4689" endWordPosition="4692"> model, which uses the external knowledge - normalization dictionary. The experiments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matchin</context>
</contexts>
<marker>Pennell, Liu, 2014</marker>
<rawString>Deana L Pennell and Yang Liu. 2014. Normalization of informal text. Computer Speech &amp; Language, 28(1):256–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Joint chinese word segmentation, pos tagging and parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 EMNLP,</booktitle>
<pages>501--511</pages>
<contexts>
<context position="12672" citStr="Qian and Liu, 2012" startWordPosition="1996" endWordPosition="1999">on of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new partial word with a POS tag “pos”. Given the sentence “Z </context>
</contexts>
<marker>Qian, Liu, 2012</marker>
<rawString>Xian Qian and Yang Liu. 2012. Joint chinese word segmentation, pos tagging and parsing. In Proceedings of the 2012 EMNLP, pages 501–511, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cagil Sonmez</author>
<author>Arzucan Ozgur</author>
</authors>
<title>A graphbased approach for contextual text normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014EMNLP,</booktitle>
<pages>313--324</pages>
<contexts>
<context position="29958" citStr="Sonmez and Ozgur, 2014" startWordPosition="4702" endWordPosition="4705">periments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese in</context>
</contexts>
<marker>Sonmez, Ozgur, 2014</marker>
<rawString>Cagil Sonmez and Arzucan Ozgur. 2014. A graphbased approach for contextual text normalization. In Proceedings of the 2014EMNLP, pages 313–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Mining informal language from chinese microtext: Joint word recognition and segmentation.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>731--741</pages>
<contexts>
<context position="2437" citStr="Wang and Kan (2013)" startWordPosition="358" endWordPosition="361"> 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) proposed a joint model to process word segmentation and informal word detection. However, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, </context>
<context position="31163" citStr="Wang and Kan (2013)" startWordPosition="4891" endWordPosition="4894">ize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segmentation, resulting in reduced performance. Wang and Kan (2013) propose a joint model to process word segmentation and informal word detection. However, text normalization is split to another task (Wang et al., 2013). Our joint model process word segmentation, POS tagging and normalization simultaneously. Kaji et al. (2014) propose a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs. Their model is trained on a partially annotated microblog corpus. In contrast, our model can be trained on existing annotated corpora in standard text. Researchers have recently developed various microblog corpora annotated with rich lin</context>
</contexts>
<marker>Wang, Kan, 2013</marker>
<rawString>Aobo Wang and Min-Yen Kan. 2013. Mining informal language from chinese microtext: Joint word recognition and segmentation. In ACL (1), pages 731– 741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Tao Chen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Retweeting from a linguistic perspective.</title>
<date>2012</date>
<booktitle>In Proceedings of the second workshop on language in social media,</booktitle>
<pages>46--55</pages>
<contexts>
<context position="18995" citStr="Wang et al. (2012)" startWordPosition="2948" endWordPosition="2951">y being replaced with their informal equivalents. Num Ratio Agree Phonetic 572 0.870 0.95 Substitutions Abbreviation 69 0.105 0.97 Paraphrases 17 0.025 0.90 Total 658 1 0.95 Table 3: Frequency distribution and annotation agreement on various types of informal words. 6 Experiments 6.1 Microblog Corpus Annotation To evaluate our model, we develop a microblog corpus. Our annotated corpus is collected from Sina Weibo3, which is the largest microblogging platform in China. More than 1,000,000 Chinese posts are crawled using Sina Weibo API. Among these, 4,000 posts were randomly selected. We follow Wang et al. (2012) and apply rules to preprocess the corpus’URLs, emoticons, “@usernames&amp;quot;and Hashtags as pre-segmented words. As a result, we obtain 2,000 sentences as a source of the corpus. Two human participants annotated the 2,000 sentences by using the tools we developed. The tools can simultaneously annotate word boundaries, POS and text normalization. We used the CTB scheme for word segmentation and POS tagging. We divided informal words into three types: Phonetic Substitutions, Abbreviation, Paraphrases. In total, we annotated 1,129 informal word-pairs in the 2,000 sentences, which contained 658 differe</context>
</contexts>
<marker>Wang, Chen, Kan, 2012</marker>
<rawString>Aobo Wang, Tao Chen, and Min-Yen Kan. 2012. Retweeting from a linguistic perspective. In Proceedings of the second workshop on language in social media, pages 46–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Min-Yen Kan</author>
<author>Daniel Andrade</author>
<author>Takashi Onishi</author>
<author>Kai Ishikawa</author>
</authors>
<title>Chinese informal word normalization: an experimental study.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="4837" citStr="Wang et al. (2013)" startWordPosition="713" endWordPosition="716">discourse-level normalization. In this paper we focus on lexical-level normalization, which aims to transform informal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standa</context>
<context position="11421" citStr="Wang et al., 2013" startWordPosition="1807" endWordPosition="1810"> in a state, SEP joins a character as the start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ - &apos; ” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2. To train a word-based language model, we segmented the</context>
<context position="28207" citStr="Wang et al. (2013)" startWordPosition="4413" endWordPosition="4416">es the F-Score of POS tagging. Nor-F denotes the F-Score of normalization. Another type of error is phonetic substitutions of numbers, which are sometimes identified incorrectly. For example. “7456” is identified as a number in the experiments, but it means “����” (I’m so angry). To settle this problem, it needs more context information. 6.5 Results of Lexical Normalization It is interesting to explore how well the joint model can normalize informal words. We compare our results with two existing systems on text normalization based on our annotated microblog corpus. (1) WangDT We re-implement Wang et al. (2013), which formalized the task as a classification problem and proposed rule-based and statistical features to model three plausible channels that explain the connection between formal and informal pairs. We use a single decision tree classifier POS(ST+lm) Seg(ST+lm) POS(SNT+lm) Seg(SNT+lm) F-Score 1843 P R F SNT+lm 0.9027 0.4920 0.6413 WangDT 0.6214 0.5543 0.5859 LYTop1 0.6338 0.4920 0.5540 Table 7: Results of lexical normalization. in the experiment. (2) LYTop1 Li and Yarowsky (2008) formalized the task as a ranking problem and proposed a conditional log-linear model to normalization. In the ex</context>
<context position="31316" citStr="Wang et al., 2013" startWordPosition="4917" endWordPosition="4920">orms better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segmentation, resulting in reduced performance. Wang and Kan (2013) propose a joint model to process word segmentation and informal word detection. However, text normalization is split to another task (Wang et al., 2013). Our joint model process word segmentation, POS tagging and normalization simultaneously. Kaji et al. (2014) propose a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs. Their model is trained on a partially annotated microblog corpus. In contrast, our model can be trained on existing annotated corpora in standard text. Researchers have recently developed various microblog corpora annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a micro</context>
</contexts>
<marker>Wang, Kan, Andrade, Onishi, Ishikawa, 2013</marker>
<rawString>Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi Onishi, and Kai Ishikawa. 2013. Chinese informal word normalization: an experimental study. In Proceedings of IJCNLP, pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kam-Fai Wong</author>
<author>Yunqing Xia</author>
</authors>
<title>Normalization of chinese chat language.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>2</issue>
<contexts>
<context position="30376" citStr="Wong and Xia (2008)" startWordPosition="4773" endWordPosition="4776"> There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in </context>
</contexts>
<marker>Wong, Xia, 2008</marker>
<rawString>Kam-Fai Wong and Yunqing Xia. 2008. Normalization of chinese chat language. Language Resources and Evaluation, 42(2):219–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
<author>Wei Gao</author>
</authors>
<title>Nil is not nothing: Recognition of chinese network informal language expressions.</title>
<date>2005</date>
<booktitle>In 4th SIGHAN Workshop on Chinese Language Processing at IJCNLP,</booktitle>
<volume>5</volume>
<contexts>
<context position="30500" citStr="Xia et al. (2005)" startWordPosition="4794" endWordPosition="4797">ook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segm</context>
</contexts>
<marker>Xia, Wong, Gao, 2005</marker>
<rawString>Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. Nil is not nothing: Recognition of chinese network informal language expressions. In 4th SIGHAN Workshop on Chinese Language Processing at IJCNLP, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1738" citStr="Yang and Eisenstein, 2013" startWordPosition="254" endWordPosition="257">f NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word se</context>
<context position="29933" citStr="Yang and Eisenstein, 2013" startWordPosition="4698" endWordPosition="4701">lization dictionary. The experiments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classificatio</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In EMNLP, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL,</booktitle>
<pages>840--847</pages>
<contexts>
<context position="5668" citStr="Zhang and Clark (2007)" startWordPosition="837" endWordPosition="840">to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 Joint Segmentation and Normalization 3.1 Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F(x) = argmax Score(y) (1) yeGen(x) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Figure 1: A state of transition-based model. Here a transition model is defined as a qua</context>
<context position="32361" citStr="Zhang and Clark, 2007" startWordPosition="5079" endWordPosition="5082">annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog </context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th ACL, pages 840–847, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and pos-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 EMNLP,</booktitle>
<pages>843--852</pages>
<contexts>
<context position="3147" citStr="Zhang and Clark (2010)" startWordPosition="470" endWordPosition="473">r, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline sys</context>
<context position="12082" citStr="Zhang and Clark (2010)" startWordPosition="1915" endWordPosition="1918">regawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ - &apos; ” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2. To train a word-based language model, we segmented the corpus using our re-implementation of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et</context>
<context position="13574" citStr="Zhang and Clark (2010)" startWordPosition="2159" endWordPosition="2162">he transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new partial word with a POS tag “pos”. Given the sentence “Z f�  )J &apos; UK !(How great work pressure is!)”, the sequences of action “SEP(Z, NN), APP(J( ), SEP(, NN), APP()J), SEP(&apos;, VA), SEP(UJ, SP), SEP(!, PU)” can be used to analyze its structure. 4.2 Joint Segmentation, Normalization and POS Tagging Our joint model extends the model of Zhang and Clark (2010) by adding a SEPS action, which substitutes formal word for last word in 5 if exists in the dictionary. On the other hand, it can also be regarded as an extension of the joint segmentation and normalization model, adding POS arguments to the original actions. Using the same example shown in Figure 2, the following three actions can be applied for the character “ &apos;(big)”: (1) APP(“&apos;(big)”), appending “&apos;(big)” to the last word “YI¨(y¯al´ı, pear)” in the informal labeled sequence, which remain with the same POS tag “NN”. (2) SEP(“&apos;(big)”, VA), making the last word “YI ¨(y¯ali, pear)” in the infor</context>
<context position="32489" citStr="Zhang and Clark, 2010" startWordPosition="5102" endWordPosition="5105"> POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora for Chinese microbl</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and pos-tagging using a single discriminative model. In Proceedings of the 2010 EMNLP, pages 843–852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="6108" citStr="Zhang and Clark, 2011" startWordPosition="909" endWordPosition="912">malization dictionary, avoiding diversity on informal words. 3 Joint Segmentation and Normalization 3.1 Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F(x) = argmax Score(y) (1) yeGen(x) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Figure 1: A state of transition-based model. Here a transition model is defined as a quadruple M = (C, T, W, Ct), where C is a state space, T is a set of transitions, each of which is a function: C → C, W is an input sentence c1... cn, Ct is a set of terminal states. A model scores the output by scoring the corresponding transition sequence. As shown in Figure 1, a state is a tuple ST = (S, Q), where S contains partially segmented sequences, and Q = (ci, ci+1, ..., cn) is the sequence of input characters that have not been</context>
<context position="9552" citStr="Zhang and Clark (2011)" startWordPosition="1509" endWordPosition="1512">e entries consist of &lt;lexical variant, standard form&gt; pairs. The output is a pair of labeled sequences, containing the informal labeled sequence and the corresponding formal labeled sequence. To rank the candidates, both labeled sequences can be scored. However, lacking annotated corpora on informal texts, we only use the score of formal labeled sequence in our model. The advantage is that we can train our model by using standard corpus only, overcoming the lack of annotated corpora on informal texts. 3.3 Training and Decoding We apply the global training and beam-search decoding framework of Zhang and Clark (2011). An agenda is used by the decoder to keep the N-best states during the incremental process. Before decoding starts, the agenda is initialized with the initial state. When a character is processed, existing states are removed from the agenda and extended with all possible actions, and the N-best newly generated states are put back onto the agenda. After all states have been terminal, the highest-scored state from the agenda is taken as the output. Algorithm 1 shows pseudocode for the decoder. ADDITEM adds a new item into the agenda, NBEST returns the N highest-scored items from the agenda, and</context>
<context position="11052" citStr="Zhang and Clark (2011)" startWordPosition="1750" endWordPosition="1753">EP(state, sent[idx]) 7. ADDITEM(agenda, new) 8. norWordsGETNWORD(state.lastWord) 9. for word in norWords 10. new SEPS(state,sent[idx],word) 12. ADDITEM(agenda, new) 13. agenda N-BEST(agenda) 14. return BEST(agenda) from the agenda. GETNWORD returns a possible standard form set of last word, seeking from normalization dictionary. APP appends a character to the last word in a state, SEP joins a character as the start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram feature</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
<author>Xu Sun</author>
<author>Mairgup Mansur</author>
</authors>
<title>Exploring representations from unlabeled data with co-training for Chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 EMNLP,</booktitle>
<pages>311--321</pages>
<contexts>
<context position="32381" citStr="Zhang et al., 2013" startWordPosition="5083" endWordPosition="5086">guistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an</context>
</contexts>
<marker>Zhang, Wang, Sun, Mansur, 2013</marker>
<rawString>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for Chinese word segmentation. In Proceedings of the 2013 EMNLP, pages 311–321, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Character-level chinese dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd ACL,</booktitle>
<pages>1326--1336</pages>
<contexts>
<context position="1843" citStr="Zhang et al., 2014" startWordPosition="274" endWordPosition="277">ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) propo</context>
<context position="12651" citStr="Zhang et al., 2014" startWordPosition="1992" endWordPosition="1995">g our re-implementation of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new partial word with a POS tag “pos”. G</context>
<context position="30063" citStr="Zhang et al., 2014" startWordPosition="4722" endWordPosition="4725">. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on reca</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2014</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014a. Character-level chinese dependency parsing. In Proceedings of the 52nd ACL, pages 1326–1336, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Type-supervised domain adaptation for joint segmentation and pos-tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th EACL,</booktitle>
<pages>588--597</pages>
<contexts>
<context position="1843" citStr="Zhang et al., 2014" startWordPosition="274" endWordPosition="277">ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) propo</context>
<context position="12651" citStr="Zhang et al., 2014" startWordPosition="1992" endWordPosition="1995">g our re-implementation of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new partial word with a POS tag “pos”. G</context>
<context position="30063" citStr="Zhang et al., 2014" startWordPosition="4722" endWordPosition="4725">. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on reca</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2014</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014b. Type-supervised domain adaptation for joint segmentation and pos-tagging. In Proceedings of the 14th EACL, pages 588–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Huan Chen</author>
<author>Xuanjing Huang</author>
</authors>
<title>Chinese-english mixed text normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th ACM international conference on Web search and data mining,</booktitle>
<pages>433--442</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1843" citStr="Zhang et al., 2014" startWordPosition="274" endWordPosition="277">ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) propo</context>
<context position="12651" citStr="Zhang et al., 2014" startWordPosition="1992" endWordPosition="1995">g our re-implementation of Zhang and Clark (2010). Results show that language model information not only improves the perfor1https://catalog.ldc.upenn.edu/LDC2003T05 2http://www.speech.sri.com/projects/srilm/ 1839 mance of text normalization, but also increases the performance of word-segmentation. 4 Extension for Joint Segmentation, Normalization and POS tagging 4.1 Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci), removing ci from Q, and appending it to the last (partial) word in 5 with the same POS tag, . (2) SEP(ci, pos), removing ci from Q, making the last word in 5 as completed, and adding ci as a new partial word with a POS tag “pos”. G</context>
<context position="30063" citStr="Zhang et al., 2014" startWordPosition="4722" endWordPosition="4725">. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on reca</context>
</contexts>
<marker>Zhang, Chen, Huang, 2014</marker>
<rawString>Qi Zhang, Huan Chen, and Xuanjing Huang. 2014c. Chinese-english mixed text normalization. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 433–442. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
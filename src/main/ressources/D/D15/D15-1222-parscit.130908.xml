<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004397">
<title confidence="0.953615">
Better Summarization Evaluation with Word Embeddings for ROUGE
</title>
<author confidence="0.735232">
Jun-Ping Ng Viktoria Abrecht
</author>
<affiliation confidence="0.218034">
Bloomberg L.P. Bloomberg L.P.
</affiliation>
<address confidence="0.834381">
New York, USA New York, USA
</address>
<email confidence="0.971985">
jng324@bloomberg.net vkanchakousk@bloomberg.net
</email>
<sectionHeader confidence="0.992952" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99980352631579">
ROUGE is a widely adopted, automatic
evaluation measure for text summariza-
tion. While it has been shown to corre-
late well with human judgements, it is bi-
ased towards surface lexical similarities.
This makes it unsuitable for the evalua-
tion of abstractive summarization, or sum-
maries with substantial paraphrasing. We
study the effectiveness of word embed-
dings to overcome this disadvantage of
ROUGE. Specifically, instead of measur-
ing lexical overlaps, word embeddings are
used to compute the semantic similarity of
the words used in summaries instead. Our
experimental results show that our pro-
posal is able to achieve better correlations
with human judgements when measured
with the Spearman and Kendall rank co-
efficients.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922470588235">
Automatic text summarization is a rich field of re-
search. For example, shared task evaluation work-
shops for summarization were held for more than
a decade in the Document Understanding Con-
ference (DUC), and subsequently the Text Anal-
ysis Conference (TAC). An important element of
these shared tasks is the evaluation of participating
systems. Initially, manual evaluation was carried
out, where human judges were tasked to assess
the quality of automatically generated summaries.
However in an effort to make evaluation more
scaleable, the automatic ROUGE1 measure (Lin,
2004b) was introduced in DUC-2004. ROUGE
determines the quality of an automatic summary
through comparing overlapping units such as n-
grams, word sequences, and word pairs with hu-
man written summaries.
</bodyText>
<footnote confidence="0.61364">
1Recall-Oriented Understudy of Gisting Evaluation
</footnote>
<bodyText confidence="0.999383658536585">
ROUGE is not perfect however. Two problems
with ROUGE are that 1) it favors lexical simi-
larities between generated summaries and model
summaries, which makes it unsuitable to evaluate
abstractive summarization, or summaries with a
significant amount of paraphrasing, and 2) it does
not make any provision to cater for the readability
or fluency of the generated summaries.
There has been on-going efforts to improve
on automatic summarization evaluation measures,
such as the Automatically Evaluating Summaries
of Peers (AESOP) task in TAC (Dang and
Owczarzak, 2009; Owczarzak, 2010; Owczarzak
and Dang, 2011). However, ROUGE remains as
one of the most popular metric of choice, as it has
repeatedly been shown to correlate very well with
human judgements (Lin, 2004a; Over and Yen,
2004; Owczarzak and Dang, 2011).
In this work, we describe our efforts to tackle
the first problem of ROUGE that we have iden-
tified above — its bias towards lexical similari-
ties. We propose to do this by making use of word
embeddings (Bengio et al., 2003). Word embed-
dings refer to the mapping of words into a multi-
dimensional vector space. We can construct the
mapping, such that the distance between two word
projections in the vector space corresponds to the
semantic similarity between the two words. By in-
corporating these word embeddings into ROUGE,
we can overcome its bias towards lexical similar-
ities and instead make comparisons based on the
semantics of words sequences. We believe that
this will result in better correlations with human
assessments, and avoid situations where two word
sequences share similar meanings, but get unfairly
penalized by ROUGE due to differences in lexico-
graphic representations.
As an example, consider these two phrases: 1)
It is raining heavily, and 2) It is pouring. If we
are performing a lexical string match, as ROUGE
does, there is nothing in common between the
</bodyText>
<page confidence="0.938306">
1925
</page>
<note confidence="0.651673">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925–1930,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999916071428572">
terms “raining”, “heavily”, and “pouring”. How-
ever, these two phrases mean the same thing. If
one of the phrases was part of a human written
summary, while the other was output by an auto-
matic summarization system, we want to be able
to reward the automatic system accordingly.
In our experiments, we show that word embed-
dings indeed give us better correlations with hu-
man judgements when measured with the Spear-
man and Kendall rank coefficient. This is a signif-
icant and exciting result. Beyond just improving
the evaluation prowess of ROUGE, it has the po-
tential to expand the applicability of ROUGE to
abstractive summmarization as well.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999896567567568">
While ROUGE is widely-used, as we have noted
earlier, there is a significant body of work study-
ing the evaluation of automatic text summarization
systems. A good survey of many of these mea-
sures has been written by Steinberger and Jeˇzek
(2012). We will thus not attempt to go through
every measure here, but rather highlight the more
significant efforts in this area.
Besides ROUGE, Basic Elements (BE) (Hovy
et al., 2005) has also been used in the DUC/TAC
shared task evaluations. It is an automatic method
which evaluates the content completeness of a
generated summary by breaking up sentences into
smaller, more granular units of information (re-
ferred to as “Basic Elements”).
The pyramid method originally proposed by
Passonneau et al. (2005) is another staple in
DUC/TAC. However it is a semi-automated
method, where significant human intervention is
required to identify units of information, called
Summary Content Units (SCUs), and then to
map content within generated summaries to these
SCUs. Recently however, an automated variant of
this method has been proposed (Passonneau et al.,
2013). In this variant, word embeddings are used,
as we are proposing in this paper, to map text con-
tent within generated summaries to SCUs. How-
ever the SCUs still need to be manually identified,
limiting this variant’s scalability and applicability.
Many systems have also been proposed in the
AESOP task in TAC from 2009 to 2011. For ex-
ample, the top system reported in Owczarzak and
Dang (2011), AutoSummENG (Giannakopoulos
and Karkaletsis, 2009), is a graph-based system
which scores summaries based on the similarity
between the graph structures of the generated sum-
maries and model summaries.
</bodyText>
<sectionHeader confidence="0.997117" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9999854">
Let us now describe our proposal to integrate word
embeddings into ROUGE in greater detail.
To start off, we will first describe the word em-
beddings that we intend to adopt. A word embed-
ding is really a function W, where W : w → Rn,
and w is a word or word sequence. For our pur-
pose, we want W to map two words w1 and w2
such that their respective projections are closer
to each other if the words are semantically sim-
ilar, and further apart if they are not. Mikolov
et al. (2013b) describe one such variant, called
word2vec, which gives us this desired property2.
We will thus be making use of word2vec.
We will now explain how word embeddings
can be incorporated into ROUGE. There are sev-
eral variants of ROUGE, of which ROUGE-1,
ROUGE-2, and ROUGE-SU4 have often been
used. This is because they have been found to cor-
relate well with human judgements (Lin, 2004a;
Over and Yen, 2004; Owczarzak and Dang, 2011).
ROUGE-1 measures the amount of unigram over-
lap between model summaries and automatic sum-
maries, and ROUGE-2 measures the amount of bi-
gram overlap. ROUGE-SU4 measures the amount
of overlap of skip-bigrams, which are pairs of
words in the same order as they appear in a sen-
tence. In each of these variants, overlap is com-
puted by matching the lexical form of the words
within the target pieces of text. Formally, we can
define this as a similarity function fR such that:
</bodyText>
<equation confidence="0.980301">
�
fR(w
1 if w1 = w2
1
1, w 2) = ,
0, otherwise ( )
</equation>
<bodyText confidence="0.9996028">
where w1 and w2 are the words (could be unigrams
or n-grams) being compared.
In our proposal3, which we will refer to as
ROUGE-WE, we define a new similarity function
fWE such that:
</bodyText>
<equation confidence="0.955532666666667">
10, if v1 or v2 are OOV
fWE (w1, w2 ) =SI (2)
v1 · v2, otherwise
</equation>
<bodyText confidence="0.996099">
where w1 and w2 are the words being compared,
and vx = W(wx). OOV here means a situation
</bodyText>
<footnote confidence="0.9896755">
2The effectiveness of the learnt mapping is such that we
can now compute analogies such as king − man + woman =
queen.
3https://github.com/ng-j-p/rouge-we
</footnote>
<page confidence="0.993565">
1926
</page>
<bodyText confidence="0.99293380952381">
where we encounter a word w that our word em-
bedding function W returns no vector for. For
the purpose of this work, we make use of a set
of 3 million pre-trained vector mappings4 trained
from part of Google’s news dataset (Mikolov et al.,
2013a) for W.
Reducing OOV terms for n-grams. With our
formulation for fWE, we are able to compute
variants of ROUGE-WE that correspond to those
of ROUGE, including ROUGE-WE-1, ROUGE-
WE-2, and ROUGE-WE-SU4. However, despite
the large number of vector mappings that we have,
there will still be a large number of OOV terms in
the case of ROUGE-WE-2 and ROUGE-WE-SU4,
where the basic units of comparison are bigrams.
To solve this problem, we can compose individ-
ual word embeddings together. We follow the sim-
ple multiplicative approach described by Mitchell
and Lapata (2008), where individual vectors of
constituent tokens are multiplied together to pro-
duce the vector for a n-gram, i.e.,
</bodyText>
<equation confidence="0.9966">
W (w) = W (w1) x ... x W (wn) (3)
</equation>
<bodyText confidence="0.7629026">
where w is a n-gram composed of individual word
tokens, i.e., w = w1w2 ... wn. Multiplication be-
tween two vectors W (wi) = {vi1, ... , vik} and
W(wj) = {vj1, ... , vjk} in this case is defined
as:
</bodyText>
<equation confidence="0.975853">
{vi1 x vj1,...,vik x vjk} (4)
</equation>
<sectionHeader confidence="0.999777" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998569">
4.1 Dataset and Metrics
</subsectionHeader>
<bodyText confidence="0.9998208125">
For our experiments, we make use of the dataset
used in AESOP (Owczarzak and Dang, 2011), and
the corresponding correlation measures.
For clarity, let us first describe the dataset used
in the main TAC summarization task. The main
summarization dataset consists of 44 topics, each
of which is associated with a set of 10 docu-
ments. There are also four human-curated model
summaries for each of these topics. Each of the
51 participating systems generated a summary for
each of these topics. These automatically gener-
ated summaries, together with the human-curated
model summaries, then form the basis of the
dataset for AESOP.
To assess how effective an automatic evaluation
system is, the system is first tasked to assign a
</bodyText>
<footnote confidence="0.966034">
4https://drive.google.com/file/d/
0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=
sharing
</footnote>
<bodyText confidence="0.999404368421053">
score for each of the summaries generated by all of
the 51 participating systems. Each of these sum-
maries would also have been assessed by human
judges using these three key metrics:
Pyramid. As reviewed in Section 2, this is a semi-
automated measure described in Passonneau et al.
(2005).
Responsiveness. Human judges are tasked to
evaluate how well a summary adheres to the infor-
mation requested, as well as the linguistic quality
of the generated summary.
Readability. Human judges give their judgement
on how fluent and readable a summary is.
The evaluation system’s scores are then tested to
see how well they correlate with the human assess-
ments. The correlation is evaluated with a set of
three metrics, including 1) Pearson correlation (P),
2) Spearman rank coefficient (S), and 3) Kendall
rank coefficient (K).
</bodyText>
<sectionHeader confidence="0.803771" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9996805625">
We evaluate three different variants of our
proposal, ROUGE-WE-1, ROUGE-WE-2, and
ROUGE-WE-SU4, against their corresponding
variants of ROUGE (i.e., ROUGE-1, ROUGE-2,
ROUGE-SU4). It is worth noting here that in AE-
SOP in 2011, ROUGE-SU4 was shown to corre-
late very well with human judgements, especially
for pyramid and responsiveness, and out-performs
most of the participating systems.
Tables 1, 2, and 3 show the correlation of the
scores produced by each variant of ROUGE-WE
with human assessed scores for pyramid, respon-
siveness, and readability respectively. The tables
also show the correlations achieved by ROUGE-1,
ROUGE-2, and ROUGE-SU4. The best result for
each column has been bolded for readability.
</bodyText>
<table confidence="0.999762857142857">
Measure P S K
ROUGE-WE-1 0.9492 0.9138 0.7534
ROUGE-WE-2 0.9765 0.8984 0.7439
ROUGE-WE-SU4 0.9783 0.8808 0.7198
ROUGE-1 0.9661 0.9085 0.7466
ROUGE-2 0.9606 0.8943 0.7450
ROUGE-SU4 0.9806 0.8935 0.7371
</table>
<tableCaption confidence="0.965393333333333">
Table 1: Correlation with pyramid scores, mea-
sured with Pearson r (P), Spearman p (S), and
Kendall τ (K) coefficients.
</tableCaption>
<footnote confidence="0.8857875">
ROUGE-WE-1 is observed to correlate very
well with the pyramid, responsiveness, and read-
</footnote>
<page confidence="0.974725">
1927
</page>
<table confidence="0.999837285714286">
Measure P S K
ROUGE-WE-1 0.9155 0.8192 0.6308
ROUGE-WE-2 0.9534 0.7974 0.6149
ROUGE-WE-SU4 0.9538 0.7872 0.5969
ROUGE-1 0.9349 0.8182 0.6334
ROUGE-2 0.9416 0.7897 0.6096
ROUGE-SU4 0.9545 0.7902 0.6017
</table>
<tableCaption confidence="0.974401666666667">
Table 2: Correlation with responsiveness scores,
measured with Pearson r (P), Spearman p (S), and
Kendall ,r (K) coefficients.
</tableCaption>
<table confidence="0.999930857142857">
Measure P S K
ROUGE-WE-1 0.7846 0.4312 0.3216
ROUGE-WE-2 0.7819 0.4141 0.3042
ROUGE-WE-SU4 0.7931 0.4068 0.3020
ROUGE-1 0.7900 0.3914 0.2846
ROUGE-2 0.7524 0.3975 0.2925
ROUGE-SU4 0.7840 0.3953 0.2925
</table>
<tableCaption confidence="0.832515">
Table 3: Correlation with readability scores, mea-
sured with Pearson r (P), Spearman p (S), and
Kendall ,r (K) coefficients.
</tableCaption>
<bodyText confidence="0.999693714285714">
ability scores when measured with the Spear-
man and Kendall rank correlation. However,
ROUGE-SU4 correlates better with human assess-
ments for the Pearson correlation. The key differ-
ence between the Pearson correlation and Spear-
man/Kendall rank correlation, is that the former
assumes that the variables being tested are nor-
mally distributed. It also further assumes that the
variables are linearly related to each other. The lat-
ter two measures are however non-parametric and
make no assumptions about the distribution of the
variables being tested. We argue that the assump-
tions made by the Pearson correlation may be too
constraining, given that any two independent eval-
uation systems may not exhibit linearity.
Looking at the two bigram based variants,
ROUGE-WE-2 and ROUGE-WE-SU4, we ob-
serve that ROUGE-WE-2 improves on ROUGE-2
most of the time, regardless of the correlation met-
ric used. This lends further support to our proposal
to use word embeddings with ROUGE.
However ROUGE-WE-SU4 is only better than
ROUGE-SU4 when evaluating readability. It does
consistently worse than ROUGE-SU4 for pyramid
and responsiveness. The reason for this is likely
due to how we have chosen to compose unigram
word vectors into bigram equivalents. The mul-
tiplicative approach that we have taken worked
better for ROUGE-WE-2 which looks at contigu-
ous bigrams. These are easier to interpret seman-
tically than skip-bigrams (the target of ROUGE-
WE-SU4). The latter, by nature of their construc-
tion, loses some of the semantic meaning attached
to each word, and thus may not be as amenable to
the linear composition of word vectors.
Owczarzak and Dang (2011) reports only the
results of the top systems in AESOP in terms of
Pearson’s correlation. To get a more complete
picture of the usefulness of our proposal, it will
be instructive to also compare it against the other
top systems in AESOP, when measured with the
Spearman/Kendall correlations. We show in Ta-
ble 4 the top three systems which correlate best
with the pyramid score when measured with the
Spearman rank coefficient. C S IIITH3 (Ku-
mar et al., 2011) is a graph-based system which
assess summaries based on differences in word
co-locations between generated summaries and
model summaries. BE-HM (baseline by the orga-
nizers of the AESOP task) is the BE system (Hovy
et al., 2005), where basic elements are identi-
fied using a head-modifier criterion on parse re-
sults from Minipar. Lastly, catolicasc1 (de
Oliveira, 2011) is also a graph-based system which
frames the summary evaluation problem as a max-
imum bipartite graph matching problem.
</bodyText>
<table confidence="0.9994998">
Measure S K
ROUGE-WE-1 0.9138 0.7534
C S IIITH3 0.9033 0.7582
BE-HM 0.9030 0.7456
catolicasc1 0.9017 0.7351
</table>
<tableCaption confidence="0.770574666666667">
Table 4: Correlation with pyramid scores of
top systems in AESOP 2011, measured with the
Spearman p (S), and Kendall ,r (K) coefficients.
</tableCaption>
<bodyText confidence="0.999631428571429">
We see that ROUGE-WE-1 displays better cor-
relations with pyramid scores than the top system
in AESOP 2011 (i.e., C S IIITH3) when mea-
sured with the Spearman coefficient. The latter
does slightly better however for the Kendall coef-
ficient. This observation further validates that our
proposal is an effective enhancement to ROUGE.
</bodyText>
<sectionHeader confidence="0.99584" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9984695">
We proposed an enhancement to the popu-
lar ROUGE metric in this work, ROUGE-WE.
</bodyText>
<page confidence="0.989153">
1928
</page>
<bodyText confidence="0.999885139534884">
ROUGE is biased towards identifying lexical sim-
ilarity when assessing the quality of a generated
summary. We improve on this by incorporat-
ing the use of word embeddings. This enhance-
ment allows us to go beyond surface lexicographic
matches, and capture instead the semantic similar-
ities between words used in a generated summary
and a human-written model summary. Experi-
menting on the TAC AESOP dataset, we show that
this proposal exhibits very good correlations with
human assessments, measured with the Spear-
man and Kendall rank coefficients. In particular,
ROUGE-WE-1 outperforms leading state-of-the-
art systems consistently.
Looking ahead, we want to continue building
on this work. One area to improve on is the use
of a more inclusive evaluation dataset. The AE-
SOP summaries that we have used in our experi-
ments are drawn from systems participating in the
TAC summarization task, where there is a strong
exhibited bias towards extractive summarizers. It
will be helpful to enlarge this set of summaries to
include output from summarizers which carry out
substantial paraphrasing (Li et al., 2013; Ng et al.,
2014; Liu et al., 2015).
Another immediate goal is to study the use of
better compositional embedding models. The gen-
eralization of unigram word embeddings into bi-
grams (or phrases), is still an open problem (Yin
and Sch¨utze, 2014; Yu et al., 2014). A better com-
positional embedding model than the one that we
adopted in this work should help us improve the
results achieved by bigram variants of ROUGE-
WE, especially ROUGE-WE-SU4. This is im-
portant because earlier works have demonstrated
the value of using skip-bigrams for summarization
evaluation.
An effective and accurate automatic evaluation
measure will be a big boon to our quest for bet-
ter text summarization systems. Word embeddings
add a promising dimension to summarization eval-
uation, and we hope to expand on the work we
have shared to further realize its potential.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999393915254238">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Hoa Trang Dang and Karolina Owczarzak. 2009.
Overview of the TAC 2009 Summarization Track.
In Proceedings of the Text Analysis Conference
(TAC).
Paulo C. F. de Oliveira. 2011. CatolicaSC at TAC
2011. In Proceedings of the Text Analysis Confer-
ence (TAC).
George Giannakopoulos and Vangelis Karkaletsis.
2009. AutoSummENG and MeMoG in Evaluat-
ing Guided Summaries. In Proceedings of the Text
Analysis Conference (TAC).
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. In
Proceedings of the Document Understanding Con-
ference (DUC).
Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.
2011. Using Unsupervised System with Least Lin-
guistic Features for TAC-AESOP Task. In Proceed-
ings of the Text Analysis Conference (TAC).
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013. Document Summarization via Guided Sen-
tence Compression. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 490–500.
Chin-Yew Lin. 2004a. Looking for a Few Good Met-
rics: ROUGE and its Evaluation. In Working Notes
of the 4th NTCIR Workshop Meeting.
Chin-Yew Lin. 2004b. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop.
Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman
Sadeh, and Noah A. Smith. 2015. Toward Ab-
stractive Summarization Using Semantic Represen-
tations. In Proceedings of the Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), pages 1077–1086.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Proceedings of the 27th Annual Conference
on Neural Information Processing Systems (NIPS),
pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 746–751.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL), pages 236–244.
</reference>
<page confidence="0.922737">
1929
</page>
<reference confidence="0.99952445945946">
Jun-Ping Ng, Yan Chen, Min-Yen Kan, and Zhoujun
Li. 2014. Exploiting Timelines to Enhance Multi-
document Summarization. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 923–933.
Paul Over and James Yen. 2004. An Introduction to
DUC 2004 Intrinsic Evaluation of Generic New Text
Summarization Systems. In Proceedings of the Doc-
ument Understanding Conference (DUC).
Karolina Owczarzak and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Summarization Track:
Guided Task and AESOP Task. In Proceedings of
the Text Analysis Conference (TAC).
Karolina Owczarzak. 2010. Overview of the TAC
2010 Summarization Track. In Proceedings of the
Text Analysis Conference (TAC).
Rebecca J Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying the
Pyramid Method in DUC 2005. In Proceedings of
the Document Understanding Conference (DUC).
Rebecca J Passonneau, Emily Chen, Weiwei Guo, and
Dolores Perin. 2013. Automated Pyramid Scoring
of Summaries using Distributional Semantics. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
143–147.
Josef Steinberger and Karel Jeˇzek. 2012. Evaluation
Measures for Text Summarization. Computing and
Informatics, 28(2):251–275.
Wenpeng Yin and Hinrich Sch¨utze. 2014. An Ex-
ploration of Embeddings for Generalized Phrases.
In Proceedings of the ACL 2014 Student Research
Workshop, pages 41–47.
Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based Compositional Embedding Models. In
Proceedings of the NIPS 2014 Deep Learning and
Representation Learning Workshop.
</reference>
<page confidence="0.99222">
1930
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285016">
<title confidence="0.992952">Better Summarization Evaluation with Word Embeddings for ROUGE</title>
<author confidence="0.7353775">Jun-Ping Ng Viktoria Abrecht Bloomberg L P Bloomberg L P</author>
<address confidence="0.89604">New York, USA New York, USA</address>
<email confidence="0.72698">jng324@bloomberg.netvkanchakousk@bloomberg.net</email>
<abstract confidence="0.991832">ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2807" citStr="Bengio et al., 2003" startWordPosition="428" endWordPosition="431"> automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that the distance between two word projections in the vector space corresponds to the semantic similarity between the two words. By incorporating these word embeddings into ROUGE, we can overcome its bias towards lexical similarities and instead make comparisons based on the semantics of words sequences. We believe that this will result in better correlations with human assessments, and avoid situations where two word sequences share similar meanings, but get unfairly penali</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Karolina Owczarzak</author>
</authors>
<title>Summarization Track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="2334" citStr="Dang and Owczarzak, 2009" startWordPosition="345" endWordPosition="348">mmaries. 1Recall-Oriented Understudy of Gisting Evaluation ROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries. There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that t</context>
</contexts>
<marker>Dang, Owczarzak, 2009</marker>
<rawString>Hoa Trang Dang and Karolina Owczarzak. 2009. Overview of the TAC 2009 Summarization Track. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paulo C F de Oliveira</author>
</authors>
<title>CatolicaSC at TAC</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<marker>de Oliveira, 2011</marker>
<rawString>Paulo C. F. de Oliveira. 2011. CatolicaSC at TAC 2011. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Giannakopoulos</author>
<author>Vangelis Karkaletsis</author>
</authors>
<title>AutoSummENG and MeMoG in Evaluating Guided Summaries.</title>
<date>2009</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="6064" citStr="Giannakopoulos and Karkaletsis, 2009" startWordPosition="952" endWordPosition="955">Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, an automated variant of this method has been proposed (Passonneau et al., 2013). In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs. However the SCUs still need to be manually identified, limiting this variant’s scalability and applicability. Many systems have also been proposed in the AESOP task in TAC from 2009 to 2011. For example, the top system reported in Owczarzak and Dang (2011), AutoSummENG (Giannakopoulos and Karkaletsis, 2009), is a graph-based system which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries. 3 Methodology Let us now describe our proposal to integrate word embeddings into ROUGE in greater detail. To start off, we will first describe the word embeddings that we intend to adopt. A word embedding is really a function W, where W : w → Rn, and w is a word or word sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further </context>
</contexts>
<marker>Giannakopoulos, Karkaletsis, 2009</marker>
<rawString>George Giannakopoulos and Vangelis Karkaletsis. 2009. AutoSummENG and MeMoG in Evaluating Guided Summaries. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhou</author>
</authors>
<title>Evaluating DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC).</booktitle>
<contexts>
<context position="4942" citStr="Hovy et al., 2005" startWordPosition="775" endWordPosition="778">ficant and exciting result. Beyond just improving the evaluation prowess of ROUGE, it has the potential to expand the applicability of ROUGE to abstractive summmarization as well. 2 Related Work While ROUGE is widely-used, as we have noted earlier, there is a significant body of work studying the evaluation of automatic text summarization systems. A good survey of many of these measures has been written by Steinberger and Jeˇzek (2012). We will thus not attempt to go through every measure here, but rather highlight the more significant efforts in this area. Besides ROUGE, Basic Elements (BE) (Hovy et al., 2005) has also been used in the DUC/TAC shared task evaluations. It is an automatic method which evaluates the content completeness of a generated summary by breaking up sentences into smaller, more granular units of information (referred to as “Basic Elements”). The pyramid method originally proposed by Passonneau et al. (2005) is another staple in DUC/TAC. However it is a semi-automated method, where significant human intervention is required to identify units of information, called Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, a</context>
<context position="15125" citStr="Hovy et al., 2005" startWordPosition="2461" endWordPosition="2464">earson’s correlation. To get a more complete picture of the usefulness of our proposal, it will be instructive to also compare it against the other top systems in AESOP, when measured with the Spearman/Kendall correlations. We show in Table 4 the top three systems which correlate best with the pyramid score when measured with the Spearman rank coefficient. C S IIITH3 (Kumar et al., 2011) is a graph-based system which assess summaries based on differences in word co-locations between generated summaries and model summaries. BE-HM (baseline by the organizers of the AESOP task) is the BE system (Hovy et al., 2005), where basic elements are identified using a head-modifier criterion on parse results from Minipar. Lastly, catolicasc1 (de Oliveira, 2011) is also a graph-based system which frames the summary evaluation problem as a maximum bipartite graph matching problem. Measure S K ROUGE-WE-1 0.9138 0.7534 C S IIITH3 0.9033 0.7582 BE-HM 0.9030 0.7456 catolicasc1 0.9017 0.7351 Table 4: Correlation with pyramid scores of top systems in AESOP 2011, measured with the Spearman p (S), and Kendall ,r (K) coefficients. We see that ROUGE-WE-1 displays better correlations with pyramid scores than the top system i</context>
</contexts>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005. Evaluating DUC 2005 using Basic Elements. In Proceedings of the Document Understanding Conference (DUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niraj Kumar</author>
<author>Kannan Srinathan</author>
<author>Vasudeva Varma</author>
</authors>
<title>Using Unsupervised System with Least Linguistic Features for TAC-AESOP Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="14897" citStr="Kumar et al., 2011" startWordPosition="2423" endWordPosition="2427">on, loses some of the semantic meaning attached to each word, and thus may not be as amenable to the linear composition of word vectors. Owczarzak and Dang (2011) reports only the results of the top systems in AESOP in terms of Pearson’s correlation. To get a more complete picture of the usefulness of our proposal, it will be instructive to also compare it against the other top systems in AESOP, when measured with the Spearman/Kendall correlations. We show in Table 4 the top three systems which correlate best with the pyramid score when measured with the Spearman rank coefficient. C S IIITH3 (Kumar et al., 2011) is a graph-based system which assess summaries based on differences in word co-locations between generated summaries and model summaries. BE-HM (baseline by the organizers of the AESOP task) is the BE system (Hovy et al., 2005), where basic elements are identified using a head-modifier criterion on parse results from Minipar. Lastly, catolicasc1 (de Oliveira, 2011) is also a graph-based system which frames the summary evaluation problem as a maximum bipartite graph matching problem. Measure S K ROUGE-WE-1 0.9138 0.7534 C S IIITH3 0.9033 0.7582 BE-HM 0.9030 0.7456 catolicasc1 0.9017 0.7351 Tab</context>
</contexts>
<marker>Kumar, Srinathan, Varma, 2011</marker>
<rawString>Niraj Kumar, Kannan Srinathan, and Vasudeva Varma. 2011. Using Unsupervised System with Least Linguistic Features for TAC-AESOP Task. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Yang Liu</author>
</authors>
<title>Document Summarization via Guided Sentence Compression.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>490--500</pages>
<contexts>
<context position="17160" citStr="Li et al., 2013" startWordPosition="2786" endWordPosition="2789"> measured with the Spearman and Kendall rank coefficients. In particular, ROUGE-WE-1 outperforms leading state-of-theart systems consistently. Looking ahead, we want to continue building on this work. One area to improve on is the use of a more inclusive evaluation dataset. The AESOP summaries that we have used in our experiments are drawn from systems participating in the TAC summarization task, where there is a strong exhibited bias towards extractive summarizers. It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015). Another immediate goal is to study the use of better compositional embedding models. The generalization of unigram word embeddings into bigrams (or phrases), is still an open problem (Yin and Sch¨utze, 2014; Yu et al., 2014). A better compositional embedding model than the one that we adopted in this work should help us improve the results achieved by bigram variants of ROUGEWE, especially ROUGE-WE-SU4. This is important because earlier works have demonstrated the value of using skip-bigrams for summarization evaluation. An effective and accurate automatic</context>
</contexts>
<marker>Li, Liu, Weng, Liu, 2013</marker>
<rawString>Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document Summarization via Guided Sentence Compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 490–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Looking for a Few Good Metrics: ROUGE and its Evaluation.</title>
<date>2004</date>
<booktitle>In Working Notes of the 4th NTCIR Workshop Meeting.</booktitle>
<contexts>
<context position="1522" citStr="Lin, 2004" startWordPosition="226" endWordPosition="227">rank coefficients. 1 Introduction Automatic text summarization is a rich field of research. For example, shared task evaluation workshops for summarization were held for more than a decade in the Document Understanding Conference (DUC), and subsequently the Text Analysis Conference (TAC). An important element of these shared tasks is the evaluation of participating systems. Initially, manual evaluation was carried out, where human judges were tasked to assess the quality of automatically generated summaries. However in an effort to make evaluation more scaleable, the automatic ROUGE1 measure (Lin, 2004b) was introduced in DUC-2004. ROUGE determines the quality of an automatic summary through comparing overlapping units such as ngrams, word sequences, and word pairs with human written summaries. 1Recall-Oriented Understudy of Gisting Evaluation ROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the g</context>
<context position="7092" citStr="Lin, 2004" startWordPosition="1140" endWordPosition="1141">d sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further apart if they are not. Mikolov et al. (2013b) describe one such variant, called word2vec, which gives us this desired property2. We will thus be making use of word2vec. We will now explain how word embeddings can be incorporated into ROUGE. There are several variants of ROUGE, of which ROUGE-1, ROUGE-2, and ROUGE-SU4 have often been used. This is because they have been found to correlate well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). ROUGE-1 measures the amount of unigram overlap between model summaries and automatic summaries, and ROUGE-2 measures the amount of bigram overlap. ROUGE-SU4 measures the amount of overlap of skip-bigrams, which are pairs of words in the same order as they appear in a sentence. In each of these variants, overlap is computed by matching the lexical form of the words within the target pieces of text. Formally, we can define this as a similarity function fR such that: � fR(w 1 if w1 = w2 1 1, w 2) = , 0, otherwise ( ) where w1 and w2 are the words </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004a. Looking for a Few Good Metrics: ROUGE and its Evaluation. In Working Notes of the 4th NTCIR Workshop Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</booktitle>
<contexts>
<context position="1522" citStr="Lin, 2004" startWordPosition="226" endWordPosition="227">rank coefficients. 1 Introduction Automatic text summarization is a rich field of research. For example, shared task evaluation workshops for summarization were held for more than a decade in the Document Understanding Conference (DUC), and subsequently the Text Analysis Conference (TAC). An important element of these shared tasks is the evaluation of participating systems. Initially, manual evaluation was carried out, where human judges were tasked to assess the quality of automatically generated summaries. However in an effort to make evaluation more scaleable, the automatic ROUGE1 measure (Lin, 2004b) was introduced in DUC-2004. ROUGE determines the quality of an automatic summary through comparing overlapping units such as ngrams, word sequences, and word pairs with human written summaries. 1Recall-Oriented Understudy of Gisting Evaluation ROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the g</context>
<context position="7092" citStr="Lin, 2004" startWordPosition="1140" endWordPosition="1141">d sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further apart if they are not. Mikolov et al. (2013b) describe one such variant, called word2vec, which gives us this desired property2. We will thus be making use of word2vec. We will now explain how word embeddings can be incorporated into ROUGE. There are several variants of ROUGE, of which ROUGE-1, ROUGE-2, and ROUGE-SU4 have often been used. This is because they have been found to correlate well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). ROUGE-1 measures the amount of unigram overlap between model summaries and automatic summaries, and ROUGE-2 measures the amount of bigram overlap. ROUGE-SU4 measures the amount of overlap of skip-bigrams, which are pairs of words in the same order as they appear in a sentence. In each of these variants, overlap is computed by matching the lexical form of the words within the target pieces of text. Formally, we can define this as a similarity function fR such that: � fR(w 1 if w1 = w2 1 1, w 2) = , 0, otherwise ( ) where w1 and w2 are the words </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004b. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Norman Sadeh</author>
<author>Noah A Smith</author>
</authors>
<title>Toward Abstractive Summarization Using Semantic Representations.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="17196" citStr="Liu et al., 2015" startWordPosition="2794" endWordPosition="2797">ndall rank coefficients. In particular, ROUGE-WE-1 outperforms leading state-of-theart systems consistently. Looking ahead, we want to continue building on this work. One area to improve on is the use of a more inclusive evaluation dataset. The AESOP summaries that we have used in our experiments are drawn from systems participating in the TAC summarization task, where there is a strong exhibited bias towards extractive summarizers. It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015). Another immediate goal is to study the use of better compositional embedding models. The generalization of unigram word embeddings into bigrams (or phrases), is still an open problem (Yin and Sch¨utze, 2014; Yu et al., 2014). A better compositional embedding model than the one that we adopted in this work should help us improve the results achieved by bigram variants of ROUGEWE, especially ROUGE-WE-SU4. This is important because earlier works have demonstrated the value of using skip-bigrams for summarization evaluation. An effective and accurate automatic evaluation measure will be a big bo</context>
</contexts>
<marker>Liu, Flanigan, Thomson, Sadeh, Smith, 2015</marker>
<rawString>Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward Abstractive Summarization Using Semantic Representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1077–1086.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<booktitle>2013a. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS),</booktitle>
<pages>3111--3119</pages>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013a. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS), pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>746--751</pages>
<contexts>
<context position="6707" citStr="Mikolov et al. (2013" startWordPosition="1073" endWordPosition="1076">em which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries. 3 Methodology Let us now describe our proposal to integrate word embeddings into ROUGE in greater detail. To start off, we will first describe the word embeddings that we intend to adopt. A word embedding is really a function W, where W : w → Rn, and w is a word or word sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further apart if they are not. Mikolov et al. (2013b) describe one such variant, called word2vec, which gives us this desired property2. We will thus be making use of word2vec. We will now explain how word embeddings can be incorporated into ROUGE. There are several variants of ROUGE, of which ROUGE-1, ROUGE-2, and ROUGE-SU4 have often been used. This is because they have been found to correlate well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). ROUGE-1 measures the amount of unigram overlap between model summaries and automatic summaries, and ROUGE-2 measures the amount of bigram overlap. ROUGE-SU4 measures</context>
<context position="8401" citStr="Mikolov et al., 2013" startWordPosition="1385" endWordPosition="1388">s ROUGE-WE, we define a new similarity function fWE such that: 10, if v1 or v2 are OOV fWE (w1, w2 ) =SI (2) v1 · v2, otherwise where w1 and w2 are the words being compared, and vx = W(wx). OOV here means a situation 2The effectiveness of the learnt mapping is such that we can now compute analogies such as king − man + woman = queen. 3https://github.com/ng-j-p/rouge-we 1926 where we encounter a word w that our word embedding function W returns no vector for. For the purpose of this work, we make use of a set of 3 million pre-trained vector mappings4 trained from part of Google’s news dataset (Mikolov et al., 2013a) for W. Reducing OOV terms for n-grams. With our formulation for fWE, we are able to compute variants of ROUGE-WE that correspond to those of ROUGE, including ROUGE-WE-1, ROUGEWE-2, and ROUGE-WE-SU4. However, despite the large number of vector mappings that we have, there will still be a large number of OOV terms in the case of ROUGE-WE-2 and ROUGE-WE-SU4, where the basic units of comparison are bigrams. To solve this problem, we can compose individual word embeddings together. We follow the simple multiplicative approach described by Mitchell and Lapata (2008), where individual vectors of c</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based Models of Semantic Composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL),</booktitle>
<pages>236--244</pages>
<contexts>
<context position="8970" citStr="Mitchell and Lapata (2008)" startWordPosition="1478" endWordPosition="1481"> from part of Google’s news dataset (Mikolov et al., 2013a) for W. Reducing OOV terms for n-grams. With our formulation for fWE, we are able to compute variants of ROUGE-WE that correspond to those of ROUGE, including ROUGE-WE-1, ROUGEWE-2, and ROUGE-WE-SU4. However, despite the large number of vector mappings that we have, there will still be a large number of OOV terms in the case of ROUGE-WE-2 and ROUGE-WE-SU4, where the basic units of comparison are bigrams. To solve this problem, we can compose individual word embeddings together. We follow the simple multiplicative approach described by Mitchell and Lapata (2008), where individual vectors of constituent tokens are multiplied together to produce the vector for a n-gram, i.e., W (w) = W (w1) x ... x W (wn) (3) where w is a n-gram composed of individual word tokens, i.e., w = w1w2 ... wn. Multiplication between two vectors W (wi) = {vi1, ... , vik} and W(wj) = {vj1, ... , vjk} in this case is defined as: {vi1 x vj1,...,vik x vjk} (4) 4 Experiments 4.1 Dataset and Metrics For our experiments, we make use of the dataset used in AESOP (Owczarzak and Dang, 2011), and the corresponding correlation measures. For clarity, let us first describe the dataset used </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based Models of Semantic Composition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL), pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Yan Chen</author>
<author>Min-Yen Kan</author>
<author>Zhoujun Li</author>
</authors>
<title>Exploiting Timelines to Enhance Multidocument Summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>923--933</pages>
<contexts>
<context position="17177" citStr="Ng et al., 2014" startWordPosition="2790" endWordPosition="2793">e Spearman and Kendall rank coefficients. In particular, ROUGE-WE-1 outperforms leading state-of-theart systems consistently. Looking ahead, we want to continue building on this work. One area to improve on is the use of a more inclusive evaluation dataset. The AESOP summaries that we have used in our experiments are drawn from systems participating in the TAC summarization task, where there is a strong exhibited bias towards extractive summarizers. It will be helpful to enlarge this set of summaries to include output from summarizers which carry out substantial paraphrasing (Li et al., 2013; Ng et al., 2014; Liu et al., 2015). Another immediate goal is to study the use of better compositional embedding models. The generalization of unigram word embeddings into bigrams (or phrases), is still an open problem (Yin and Sch¨utze, 2014; Yu et al., 2014). A better compositional embedding model than the one that we adopted in this work should help us improve the results achieved by bigram variants of ROUGEWE, especially ROUGE-WE-SU4. This is important because earlier works have demonstrated the value of using skip-bigrams for summarization evaluation. An effective and accurate automatic evaluation measu</context>
</contexts>
<marker>Ng, Chen, Kan, Li, 2014</marker>
<rawString>Jun-Ping Ng, Yan Chen, Min-Yen Kan, and Zhoujun Li. 2014. Exploiting Timelines to Enhance Multidocument Summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 923–933.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>James Yen</author>
</authors>
<title>An Introduction to DUC</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC).</booktitle>
<contexts>
<context position="2556" citStr="Over and Yen, 2004" startWordPosition="382" endWordPosition="385">le to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries. There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that the distance between two word projections in the vector space corresponds to the semantic similarity between the two words. By incorporating these word embeddings into ROUGE, we can overcome its bias towards lexical similar</context>
<context position="7113" citStr="Over and Yen, 2004" startWordPosition="1142" endWordPosition="1145">For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other if the words are semantically similar, and further apart if they are not. Mikolov et al. (2013b) describe one such variant, called word2vec, which gives us this desired property2. We will thus be making use of word2vec. We will now explain how word embeddings can be incorporated into ROUGE. There are several variants of ROUGE, of which ROUGE-1, ROUGE-2, and ROUGE-SU4 have often been used. This is because they have been found to correlate well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). ROUGE-1 measures the amount of unigram overlap between model summaries and automatic summaries, and ROUGE-2 measures the amount of bigram overlap. ROUGE-SU4 measures the amount of overlap of skip-bigrams, which are pairs of words in the same order as they appear in a sentence. In each of these variants, overlap is computed by matching the lexical form of the words within the target pieces of text. Formally, we can define this as a similarity function fR such that: � fR(w 1 if w1 = w2 1 1, w 2) = , 0, otherwise ( ) where w1 and w2 are the words (could be unigrams or</context>
</contexts>
<marker>Over, Yen, 2004</marker>
<rawString>Paul Over and James Yen. 2004. An Introduction to DUC 2004 Intrinsic Evaluation of Generic New Text Summarization Systems. In Proceedings of the Document Understanding Conference (DUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Summarization Track: Guided Task and AESOP Task.</title>
<date>2011</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="2378" citStr="Owczarzak and Dang, 2011" startWordPosition="351" endWordPosition="354">ting Evaluation ROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries. There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that the distance between two word projections in </context>
<context position="6012" citStr="Owczarzak and Dang (2011)" startWordPosition="947" endWordPosition="950">o identify units of information, called Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, an automated variant of this method has been proposed (Passonneau et al., 2013). In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs. However the SCUs still need to be manually identified, limiting this variant’s scalability and applicability. Many systems have also been proposed in the AESOP task in TAC from 2009 to 2011. For example, the top system reported in Owczarzak and Dang (2011), AutoSummENG (Giannakopoulos and Karkaletsis, 2009), is a graph-based system which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries. 3 Methodology Let us now describe our proposal to integrate word embeddings into ROUGE in greater detail. To start off, we will first describe the word embeddings that we intend to adopt. A word embedding is really a function W, where W : w → Rn, and w is a word or word sequence. For our purpose, we want W to map two words w1 and w2 such that their respective projections are closer to each other</context>
<context position="9472" citStr="Owczarzak and Dang, 2011" startWordPosition="1576" endWordPosition="1579">e individual word embeddings together. We follow the simple multiplicative approach described by Mitchell and Lapata (2008), where individual vectors of constituent tokens are multiplied together to produce the vector for a n-gram, i.e., W (w) = W (w1) x ... x W (wn) (3) where w is a n-gram composed of individual word tokens, i.e., w = w1w2 ... wn. Multiplication between two vectors W (wi) = {vi1, ... , vik} and W(wj) = {vj1, ... , vjk} in this case is defined as: {vi1 x vj1,...,vik x vjk} (4) 4 Experiments 4.1 Dataset and Metrics For our experiments, we make use of the dataset used in AESOP (Owczarzak and Dang, 2011), and the corresponding correlation measures. For clarity, let us first describe the dataset used in the main TAC summarization task. The main summarization dataset consists of 44 topics, each of which is associated with a set of 10 documents. There are also four human-curated model summaries for each of these topics. Each of the 51 participating systems generated a summary for each of these topics. These automatically generated summaries, together with the human-curated model summaries, then form the basis of the dataset for AESOP. To assess how effective an automatic evaluation system is, th</context>
<context position="14440" citStr="Owczarzak and Dang (2011)" startWordPosition="2344" endWordPosition="2347">-SU4 when evaluating readability. It does consistently worse than ROUGE-SU4 for pyramid and responsiveness. The reason for this is likely due to how we have chosen to compose unigram word vectors into bigram equivalents. The multiplicative approach that we have taken worked better for ROUGE-WE-2 which looks at contiguous bigrams. These are easier to interpret semantically than skip-bigrams (the target of ROUGEWE-SU4). The latter, by nature of their construction, loses some of the semantic meaning attached to each word, and thus may not be as amenable to the linear composition of word vectors. Owczarzak and Dang (2011) reports only the results of the top systems in AESOP in terms of Pearson’s correlation. To get a more complete picture of the usefulness of our proposal, it will be instructive to also compare it against the other top systems in AESOP, when measured with the Spearman/Kendall correlations. We show in Table 4 the top three systems which correlate best with the pyramid score when measured with the Spearman rank coefficient. C S IIITH3 (Kumar et al., 2011) is a graph-based system which assess summaries based on differences in word co-locations between generated summaries and model summaries. BE-H</context>
</contexts>
<marker>Owczarzak, Dang, 2011</marker>
<rawString>Karolina Owczarzak and Hoa Trang Dang. 2011. Overview of the TAC 2011 Summarization Track: Guided Task and AESOP Task. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
</authors>
<title>Overview of the TAC 2010 Summarization Track.</title>
<date>2010</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="2351" citStr="Owczarzak, 2010" startWordPosition="349" endWordPosition="350">Understudy of Gisting Evaluation ROUGE is not perfect however. Two problems with ROUGE are that 1) it favors lexical similarities between generated summaries and model summaries, which makes it unsuitable to evaluate abstractive summarization, or summaries with a significant amount of paraphrasing, and 2) it does not make any provision to cater for the readability or fluency of the generated summaries. There has been on-going efforts to improve on automatic summarization evaluation measures, such as the Automatically Evaluating Summaries of Peers (AESOP) task in TAC (Dang and Owczarzak, 2009; Owczarzak, 2010; Owczarzak and Dang, 2011). However, ROUGE remains as one of the most popular metric of choice, as it has repeatedly been shown to correlate very well with human judgements (Lin, 2004a; Over and Yen, 2004; Owczarzak and Dang, 2011). In this work, we describe our efforts to tackle the first problem of ROUGE that we have identified above — its bias towards lexical similarities. We propose to do this by making use of word embeddings (Bengio et al., 2003). Word embeddings refer to the mapping of words into a multidimensional vector space. We can construct the mapping, such that the distance betwe</context>
</contexts>
<marker>Owczarzak, 2010</marker>
<rawString>Karolina Owczarzak. 2010. Overview of the TAC 2010 Summarization Track. In Proceedings of the Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
<author>Sergey Sigelman</author>
</authors>
<title>Applying the Pyramid Method in DUC</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC).</booktitle>
<contexts>
<context position="5267" citStr="Passonneau et al. (2005)" startWordPosition="826" endWordPosition="829">ic text summarization systems. A good survey of many of these measures has been written by Steinberger and Jeˇzek (2012). We will thus not attempt to go through every measure here, but rather highlight the more significant efforts in this area. Besides ROUGE, Basic Elements (BE) (Hovy et al., 2005) has also been used in the DUC/TAC shared task evaluations. It is an automatic method which evaluates the content completeness of a generated summary by breaking up sentences into smaller, more granular units of information (referred to as “Basic Elements”). The pyramid method originally proposed by Passonneau et al. (2005) is another staple in DUC/TAC. However it is a semi-automated method, where significant human intervention is required to identify units of information, called Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, an automated variant of this method has been proposed (Passonneau et al., 2013). In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs. However the SCUs still need to be manually identified, limiting this variant’s scalability and applicability. M</context>
<context position="10477" citStr="Passonneau et al. (2005)" startWordPosition="1732" endWordPosition="1735">each of these topics. These automatically generated summaries, together with the human-curated model summaries, then form the basis of the dataset for AESOP. To assess how effective an automatic evaluation system is, the system is first tasked to assign a 4https://drive.google.com/file/d/ 0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp= sharing score for each of the summaries generated by all of the 51 participating systems. Each of these summaries would also have been assessed by human judges using these three key metrics: Pyramid. As reviewed in Section 2, this is a semiautomated measure described in Passonneau et al. (2005). Responsiveness. Human judges are tasked to evaluate how well a summary adheres to the information requested, as well as the linguistic quality of the generated summary. Readability. Human judges give their judgement on how fluent and readable a summary is. The evaluation system’s scores are then tested to see how well they correlate with the human assessments. The correlation is evaluated with a set of three metrics, including 1) Pearson correlation (P), 2) Spearman rank coefficient (S), and 3) Kendall rank coefficient (K). 4.2 Results We evaluate three different variants of our proposal, RO</context>
</contexts>
<marker>Passonneau, Nenkova, McKeown, Sigelman, 2005</marker>
<rawString>Rebecca J Passonneau, Ani Nenkova, Kathleen McKeown, and Sergey Sigelman. 2005. Applying the Pyramid Method in DUC 2005. In Proceedings of the Document Understanding Conference (DUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Emily Chen</author>
<author>Weiwei Guo</author>
<author>Dolores Perin</author>
</authors>
<title>Automated Pyramid Scoring of Summaries using Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>143--147</pages>
<contexts>
<context position="5620" citStr="Passonneau et al., 2013" startWordPosition="879" endWordPosition="882">ions. It is an automatic method which evaluates the content completeness of a generated summary by breaking up sentences into smaller, more granular units of information (referred to as “Basic Elements”). The pyramid method originally proposed by Passonneau et al. (2005) is another staple in DUC/TAC. However it is a semi-automated method, where significant human intervention is required to identify units of information, called Summary Content Units (SCUs), and then to map content within generated summaries to these SCUs. Recently however, an automated variant of this method has been proposed (Passonneau et al., 2013). In this variant, word embeddings are used, as we are proposing in this paper, to map text content within generated summaries to SCUs. However the SCUs still need to be manually identified, limiting this variant’s scalability and applicability. Many systems have also been proposed in the AESOP task in TAC from 2009 to 2011. For example, the top system reported in Owczarzak and Dang (2011), AutoSummENG (Giannakopoulos and Karkaletsis, 2009), is a graph-based system which scores summaries based on the similarity between the graph structures of the generated summaries and model summaries. 3 Meth</context>
</contexts>
<marker>Passonneau, Chen, Guo, Perin, 2013</marker>
<rawString>Rebecca J Passonneau, Emily Chen, Weiwei Guo, and Dolores Perin. 2013. Automated Pyramid Scoring of Summaries using Distributional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 143–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Karel Jeˇzek</author>
</authors>
<title>Evaluation Measures for Text Summarization.</title>
<date>2012</date>
<journal>Computing and Informatics,</journal>
<volume>28</volume>
<issue>2</issue>
<marker>Steinberger, Jeˇzek, 2012</marker>
<rawString>Josef Steinberger and Karel Jeˇzek. 2012. Evaluation Measures for Text Summarization. Computing and Informatics, 28(2):251–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpeng Yin</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>An Exploration of Embeddings for Generalized Phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Student Research Workshop,</booktitle>
<pages>41--47</pages>
<marker>Yin, Sch¨utze, 2014</marker>
<rawString>Wenpeng Yin and Hinrich Sch¨utze. 2014. An Exploration of Embeddings for Generalized Phrases. In Proceedings of the ACL 2014 Student Research Workshop, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based Compositional Embedding Models.</title>
<date>2014</date>
<booktitle>In Proceedings of the NIPS 2014 Deep Learning and Representation Learning Workshop.</booktitle>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew Gormley, and Mark Dredze. 2014. Factor-based Compositional Embedding Models. In Proceedings of the NIPS 2014 Deep Learning and Representation Learning Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
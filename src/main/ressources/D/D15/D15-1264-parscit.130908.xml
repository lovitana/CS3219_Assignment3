<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.147088">
<title confidence="0.995359">
Closing the Gap:
Domain Adaptation from Explicit to Implicit Discourse Relations
</title>
<author confidence="0.999038">
Yangfeng Ji Gongbo Zhang Jacob Eisenstein
</author>
<affiliation confidence="0.9991035">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.99922">
{jiyfeng,gzhang64,jacobe}@gatech.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997024">
Many discourse relations are explicitly
marked with discourse connectives, and
these examples could potentially serve as a
plentiful source of training data for recog-
nizing implicit discourse relations. How-
ever, there are important linguistic differ-
ences between explicit and implicit dis-
course relations, which limit the accuracy
of such an approach. We account for
these differences by applying techniques
from domain adaptation, treating implic-
itly and explicitly-marked discourse rela-
tions as separate domains. The distribu-
tion of surface features varies across these
two domains, so we apply a marginalized
denoising autoencoder to induce a dense,
domain-general representation. The label
distribution is also domain-specific, so we
apply a resampling technique that is simi-
lar to instance weighting. In combination
with a set of automatically-labeled data,
these improvements eliminate more than
80% of the transfer loss incurred by train-
ing an implicit discourse relation classifier
on explicitly-marked discourse relations.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967409090909">
Discourse relations reveal the structural orga-
nization of text, potentially supporting applica-
tions such as summarization (Louis et al., 2010;
Yoshida et al., 2014), sentiment analysis (Soma-
sundaran et al., 2009), and coherence evaluation
(Lin et al., 2011). While some relations are sig-
naled explicitly with connectives such as how-
ever (Pitler et al., 2008), many more are im-
plicit. Expert-annotated datasets of implicit dis-
course relations are expensive to produce, so it
would be preferable to use weak supervision, by
automatically labeling instances with explicit con-
nectives (Marcu and Echihabi, 2003).
However, Sporleder and Lascarides (2008)
show that models trained on explicitly marked ex-
amples generalize poorly to implicit relation iden-
tification. They argued that explicit and implicit
examples may be linguistically dissimilar, as writ-
ers tend to avoid discourse connectives if the dis-
course relation could be inferred from context
(Grice, 1975). Similar observations are made by
Rutherford and Xue (2015), who attempt to add
automatically-labeled instances to improve super-
vised classification of implicit discourse relations.
In this paper, we approach this problem from
the perspective of domain adaptation. Specifically,
we argue that the reason that automatically-labeled
examples generalize poorly is due to domain mis-
match from the explicit relations (source domain)
to the implicit relations (target domain). We pro-
pose to close the gap by using two simple meth-
ods from domain adaptation: (1) feature represen-
tation learning: mapping the source domain and
target domain to a shared latent feature space; (2)
resampling: modifying the relation distribution in
the explicit relations to match the distribution over
implicit relations. Our results on the Penn Dis-
course Treebank (Prasad et al., 2008) show that
these two methods improve the performance on
unsupervised discourse relation identification by
more than 8.4% on average F1 score across all
relation types, an 82% reduction on the transfer
loss incurred by training on explicitly-marked dis-
course relations.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9935865">
Marcu and Echihabi (2003) train a classifier for
implicit intra-sentence discourse relations from
</bodyText>
<page confidence="0.938261">
2219
</page>
<note confidence="0.6486035">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2219–2224,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999966522727273">
explicitly-marked examples in the rhetorical struc-
ture theory (RST) treebank, where the relations
are automatically labeled by their discourse con-
nectives: for example, labeling the relation as
CONTRAST if the connective is but. However,
Sporleder and Lascarides (2008) argue that explic-
itly marked relations are too different from im-
plicit relations to serve as an adequate supervision
signal, obtaining negative results in segmented
discourse representation theory (SDRT) relations.
More recent work has focused on the Penn Dis-
course Treebank (PDTB), using explicitly-marked
relations to supplement, rather than replace, a la-
beled corpus of implicit relations. For example,
Biran and McKeown (2013) collect word pairs
from arguments of explicit examples to help the
supervised learning on implicit relation identifica-
tion. Lan et al. (2013) present a multi-task learning
framework, using explicit relation identification as
auxiliary tasks to help main task on implicit re-
lation identification. Rutherford and Xue (2015)
explore several selection heuristics for adding
automatically-labeled examples from Gigaword to
their system for implicit relation detection, obtain-
ing a 2% improvement in Macro-F1. Our work
differs from these previous efforts in that we fo-
cus exclusively on training from automatically-
labeled explicit instances, rather than supplement-
ing a training set of manually-labeled implicit ex-
amples.
Learning good feature representations (Ben-
David et al., 2007) and reducing mismatched la-
bel distributions (Joshi et al., 2012) are two main
ways to make a domain adaptation task successful.
Structural correspondence learning is an early ex-
ample of representation learning for domain adap-
tation (Blitzer et al., 2006); we build on the more
computationally tractable approach of marginal-
ized denoising autoencoders (Chen et al., 2012).
Instance weighting is an approach for correct-
ing label distribution mismatch (Jiang and Zhai,
2007); we apply a simpler approach of resampling
the source domain according to an estimate of the
target domain label distribution.
</bodyText>
<sectionHeader confidence="0.984426" genericHeader="method">
3 Domain Adaptation for Implicit
Relation Identification
</sectionHeader>
<bodyText confidence="0.99971">
We employ two domain adaptation techniques:
learning feature representations, and resampling to
match the target label distribution.
</bodyText>
<subsectionHeader confidence="0.9992225">
3.1 Learning feature representation:
Marginalized denoising autoencoders
</subsectionHeader>
<bodyText confidence="0.99998625">
The goal of feature representation learning is to
obtain dense features that capture feature correla-
tions between the source and target domains. De-
noising autoencoders (Glorot et al., 2011) do this
by first “corrupting” the original data, x1, ... , xn
into ˜x1, ... , ˜xn, either by adding Gaussian noise
(in the case of real-valued data) or by randomly ze-
roing out features (in the case of binary data). We
can then learn a function to reconstruct the origi-
nal data, thereby capturing feature correlations and
improving resilience to domain shift.
Chen et al. (2012) propose a particularly sim-
ple and elegant form of denoising autencoder, by
marginalizing over the noising process. Their
single-layer marginalized denoising autoencoder
(mDA) solves the following problem:
</bodyText>
<equation confidence="0.829792">
min E˜.i|.i[llxi − W˜xill2] (1)
W
</equation>
<bodyText confidence="0.9999490625">
where the parameter W E Rd×d is a projection
matrix. After learning the projection matrix, we
use tanh(Wx) as the representation for our rela-
tion identification task.
Usually, xi E Rd is a sparse vector with more
than 105 dimensions. Solving the optimization
problem defined in equation 1 will produce a
d x d dense matrix W, and is prohibitively ex-
pensive. We employ the trick proposed by Blitzer
et al. (2006) to select κ pivot features to be re-
constructed. We then split all features into non-
overlapping subsets of size &lt; K. Then, a set of
projection matrices are learned, so as to transform
each feature subset to the pivot feature set. The
final projection matrix W is the stack of all pro-
jection matrices learned from the feature subsets.
</bodyText>
<subsectionHeader confidence="0.9994125">
3.2 Handling mismatched label distributions:
Resampling with minimal supervision
</subsectionHeader>
<bodyText confidence="0.999990636363636">
There is a notable mismatch between the relation
distributions for implicit and explicitly-marked
discourse relations in the Penn Discourse Tree-
bank: as shown in Figure 1, the EXPANSION and
CONTINGENCY relations comprise a greater share
of the implicit relations, while the TEMPORAL
and COMPARISON relations comprise a greater
share of the explicitly-marked discourse relations.
Such label distribution mismatches can be a ma-
jor source of transfer loss across domains, and
therefore, reducing this mismatch can be an easy
</bodyText>
<page confidence="0.985947">
2220
</page>
<figureCaption confidence="0.8908515">
Figure 1: The relation distributions of training ex-
amples from the source domain (explicitly-marked
relations) and target domain (implicit relations) in
the PDTB.
</figureCaption>
<bodyText confidence="0.999219318181818">
way to obtain performance gains in domain adap-
tation (Joshi et al., 2012). Specifically, our goal
is to modify the relation distribution in the source
domain (explicitly-marked relations) and make it
as similar as possible to the target domain (im-
plicit relations). Given the label distribution from
the target domain, we resample training examples
from the source domain with replacement, in or-
der to match the label distribution in the target do-
main. As this requires the label distribution from
the target domain, it is no longer purely unsuper-
vised domain adaptation; instead, we call it resam-
pling with minimal supervision.
It may also be desirable to ensure that the source
and target training instances are similar in terms
of their observed features; this is the idea behind
the instance weighting approach to domain adap-
tation (Jiang and Zhai, 2007). Motivated by this
idea, we require that sampled instances from the
source domain have a cosine similarity of at least
T with at least one target domain instance (Ruther-
ford and Xue, 2015).
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999787">
Our experiments test the utility of the two do-
main adaptation methods, using the Penn Dis-
course Treebank (Prasad et al., 2008) and some
extra-training data collected from a external re-
source.
</bodyText>
<subsectionHeader confidence="0.962695">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999971293103448">
Datasets The test examples are implicit rela-
tion instances from section 21-22 in the PDTB.
For the domain adaptation setting, the training
set consists of the explicitly-marked examples ex-
tracted from sections 02-20 and 23-24, and the
development set consists of the explicit relations
from sections 21-22. All relations in the explicit
examples are automatically labeled by using the
connective-to-relation mapping from Table 2 in
(Prasad et al., 2007), where we only keep the
majority relation type for every connective. For
each identified connective, we use its annotated
arguments in the PDTB. As an upper bound, we
also train a supervised discourse relation classi-
fier, using the implicit examples in sections 02-20
and 23-24 as the training set, and using sections
00-01 as the development set. Following prior
work (Pitler et al., 2009; Park and Cardie, 2012;
Biran and McKeown, 2013), we consider the first-
level discourse relations in the PDTB — Temporal
(TEMP.), Comparison (COMP.), Expansion (EXP.)
and Contingency (CONT.). We train binary classi-
fiers and report F1 score on each binary classifica-
tion task. Extension of this approach to multi-class
classification is important, but since this is not the
setting considered in most of the prior research,
we leave it for future work.
The true power of learning from automatically
labeled examples is that we could leverage much
larger datasets than hand-annotated corpora such
as the Penn Discourse Treebank. To test this idea,
we collected 1,000 news articles from CNN.com
as extra training data. Explicitly-marked dis-
course relations from this data are automatically
extracted by matching the PDTB discourse con-
nectives (Prasad et al., 2007). For this data, we
also need to extract the arguments of the iden-
tified connectives: for every identified connec-
tive, the sentence following this connective is la-
beled as Arg2 and the preceding sentence is la-
beled as Arg1, as suggested by Biran and McKe-
own (2013). In a pilot study we found that larger
amounts of additional training data yielded no fur-
ther improvements, which is consistent with the
recent results of Rutherford and Xue (2015).
Model selection We use a linear support vec-
tor machine (Fan et al., 2008) as the classifica-
tion model. Our model includes five tunable pa-
rameters: the number of pivot features n, the
size of the feature subset K, the noise level for
the denoising autoencoder q, the cosine similar-
ity threshold for resampling T, and the penalty pa-
rameter C for the SVM classifier. We consider
n E 11000, 2000, 3000} for pivot features and
C E 10.001, 0.01, 0.1,1.0,10.0} for penalty pa-
rameters, q E 10.90, 0.95, 0.99} for noise levels.
To reduce the free parameters, we set K = 5n
and simply fix the cosine similarity threshold T =
</bodyText>
<figure confidence="0.9926005">
0.6
Explicit
Implicit
Temporal Comparison Expansion Contingency
Relation distribution
0.4
0.2
0
</figure>
<page confidence="0.97455">
2221
</page>
<table confidence="0.9797052">
Surface Features +Rep. Learning +Resampling Relations Average F1
TEMP. COMP. EXP. CONT.
Implicit → Implicit
1. FULL 24.15 28.87 68.84 43.45 41.32
Explicit [PDTB] → Implicit
2. FULL No No 17.13 20.54 50.55 36.14 31.04
3. FULL No Yes 15.38 23.88 62.04 35.29 34.14
4. FULL Yes No 17.53 22.77 50.85 36.43 31.90
5. FULL Yes Yes 17.05 22.00 63.51 38.23 35.20
6. PIVOT No No 17.33 23.89 53.53 36.22 32.74
7. PIVOT No Yes 17.73 25.39 62.65 36.02 35.44
8. PIVOT Yes No 18.66 25.86 63.37 38.87 36.69
9. PIVOT Yes Yes 19.26 25.74 68.08 41.39 38.62
Explicit [PDTB + CNN] → Implicit
10. PIVOT Yes Yes 20.35 26.32 68.92 42.25 39.46
</table>
<tableCaption confidence="0.999911">
Table 1: Performance of cross-domain learning for implicit discourse relation identification.
</tableCaption>
<bodyText confidence="0.9997774">
0.85; pilot studies found that results are not sensi-
tive to the value of T across a range of values.
Features All features are motivated by prior
work on implicit discourse relation classification:
from each training example with two arguments,
we extract (1) Lexical features, including word
pairs, the first and last words from both argu-
ments (Pitler et al., 2009); (2) Syntactic features,
including production rules from each argument,
and the shared production rules between two argu-
ments (Lin et al., 2009); (3) Other features, includ-
ing modality, Inquirer tags, Levin verb classes, and
argument polarity (Park and Cardie, 2012). We
re-implement these features as closely as possible
to the cited works, using the Stanford CoreNLP
Toolkit to obtain syntactic annotations (Manning
et al., 2014).
The FULL feature set for domain adaptation
is constructed by collecting all features from the
training set, and then removing features that oc-
cur fewer than ten times. The PIVOT feature
set includes κ high-frequency features from the
FULL feature set. To focus on testing the domain
adaptation techniques, we use the same FULL and
PIVOT set for all four relations, and leave fea-
ture set optimization for each relation as a future
work (Park and Cardie, 2012). To obtain the up-
per bound, we employ the same feature categories
and frequency threshold to extract features from
the in-domain data, hand-annotated implicit dis-
course relations. To include the representations
from the marginalized denoising autoencoder for
relation identification, we concatenate them with
the original surface feature representations of the
same examples.
</bodyText>
<subsectionHeader confidence="0.989961">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.99999216">
In experiments, we start with surface feature rep-
resentations as baselines, then incorporate the two
domain adaptation techniques incrementally. As
shown in line 2 of Table 1, the performance is
poor if directly applying a model trained on the ex-
plicit examples with the FULL feature set, which
is consistent with the observations of Sporleder
and Lascarides (2008): there is a 10.28% abso-
lute reduction on average F1 score from the up-
per bound obtained with in-domain supervision
(line 1). With mDA, the overall performance in-
creases by 0.86% (line 4); resampling gives a fur-
ther 4.16% improvement mainly because of the
performance gain on the EXP. relation (line 5).
The resampling method itself (line 3) also gives
a better overall performance then mDA (line 4).
However, the F1 scores on the TEMP. and CONT.
are even worse than the baseline (line 2).
Surface representations with the FULL feature
set were found to cause serious overfitting in the
experiments. To deal with this problem, we pro-
pose to use only κ pivot features, which gives
a stronger baseline of the cross-domain relation
identification, as shown in line 6. Then, by in-
corporating resampling and feature representation
</bodyText>
<page confidence="0.980666">
2222
</page>
<bodyText confidence="0.999973147058823">
learning individually, the average F1 increases
from 32.74% to 35.44% (line 7) and 36.69% (line
8) respectively. The combination of these two
domain adaptation techniques boosts the average
F1 further to 38.62% (line 9). The additional
CNN training data further improves performance
to 39.46% (line 10). This represents an 8.42% im-
provement of average F1 from the original result
(line 2), for more than 80% reduction on the trans-
fer loss incurred by training on explicit discourse
relations.
An additional experiment is to use automatic ar-
gument extraction in both the PDTB and the CNN
data, which would correspond to more truly un-
supervised domain adaptation. (Recall that in the
CNN data, we used adjacent sentences as argu-
ment spans, while in the PDTB data, we use ex-
pert annotations.) When using adjacent sentences
as argument spans in both datasets, the average
F1 is 38.52% for the combination of representa-
tion learning and resampling. Compared to line
10, this is a 0.94% performance drop, indicating
the importance of argument identification in the
PDTB data. In future work we may consider bet-
ter heuristics for argument extraction, such as ob-
taining automatically-labeled examples only from
those connectors for whom the arguments usu-
ally are the adjacent sentences; for example, the
connector nonetheless usually connects adjacent
spans (e.g., Bob was hungry. Nonetheless he gave
Tina the burger.), while the connector even though
may connect two spans that follow the connector
in the same sentence (e.g., Even though Bob was
hungry, he gave Tina the burger.).
</bodyText>
<sectionHeader confidence="0.99519" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999935148148148">
We have presented two methods — feature rep-
resentation learning and resampling — from do-
main adaptation to close the gap of using explicit
examples for unsupervised implicit discourse re-
lation identification. Experiments on the PDTB
show the combination of these two methods elimi-
nates more than 80% of the transfer loss caused by
training on explicit examples, increasing average
F1 from 31% to 39.5%, against a supervised up-
per bound of 41.3%. Future work will explore the
combination of this approach with more sophis-
ticated techniques for instance selection (Ruther-
ford and Xue, 2015) and feature selection (Park
and Cardie, 2012; Biran and McKeown, 2013),
while also tackling the more difficult problems of
multi-class relation classification and fine-grained
level-2 discourse relations.
Acknowledgments This research was supported
by a Google Faculty Research Award to the third
author. The following members of the Georgia
Tech Computational Linguistics Laboratory of-
fered feedback throughout the research process:
Parminder Bhatia, Naman Goyal, Vinodh Kris-
han, Umashanthi Pavalanathan, Ana Smith, Yijie
Wang, and especially Yi Yang. Thanks to the re-
viewers for their constructive and helpful sugges-
tions.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999302078947368">
Shai Ben-David, John Blitzer, Koby Crammer, Fer-
nando Pereira, et al. 2007. Analysis of representa-
tions for domain adaptation. In Neural Information
Processing Systems (NIPS), Vancouver.
Or Biran and Kathleen McKeown. 2013. Aggre-
gated word pair features for implicit discourse re-
lation disambiguation. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
69–73, Sophia, Bulgaria.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of Empirical Meth-
ods for Natural Language Processing (EMNLP),
pages 120–128.
Minmin Chen, Z. Xu, Killian Weinberger, and Fei Sha.
2012. Marginalized denoising autoencoders for do-
main adaptation. In Proceedings of the International
Conference on Machine Learning (ICML).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. The Journal
of Machine Learning Research, 9:1871–1874.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the International Conference on Ma-
chine Learning (ICML), Seattle, WA.
H Paul Grice. 1975. Logic and Conversation. In
P. Cole and J. L. Morgan, editors, Syntax and Seman-
tics Volume 3: Speech Acts, pages 41–58. Academic
Press.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), Prague.
Mahesh Joshi, William W Cohen, Mark Dredze, and
Carolyn P Ros´e. 2012. Multi-domain learning:
when do domains matter? In Proceedings of the
</reference>
<page confidence="0.614184">
2223
</page>
<reference confidence="0.997335096385542">
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1302–1312. As-
sociation for Computational Linguistics.
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging
Synthetic Discourse Data via Multi-task Learning
for Implicit Discourse Relation Recognition. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL), pages 476–485, Sophia, Bulgaria.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of Empir-
ical Methods for Natural Language Processing
(EMNLP), pages 343–351, Singapore.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically Evaluating Text Coherence Using
Discourse Relations. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
997–1006, Portland, OR.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147–156. Association for Computa-
tional Linguistics.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Daniel Marcu and Abdessamad Echihabi. 2003. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the Association for Com-
putational Linguistics (ACL), pages 368–375.
Joonsuk Park and Claire Cardie. 2012. Improving
Implicit Discourse Relation Recognition Through
Feature Set Optimization. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 108–112, Seoul,
South Korea, July. Association for Computational
Linguistics.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings
of the International Conference on Computational
Linguistics (COLING), pages 87–90, Manchester,
UK.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In Proceedings of the Association
for Computational Linguistics (ACL), Suntec, Sin-
gapore.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L
Webber. 2007. The Penn Discourse Treebank 2.0
annotation manual. The PDTB Research Group.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of LREC.
Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the Inference of Implicit Discourse Relations via
Classifying Explicit Discourse Connectives. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL),
pages 799–808, Denver, CO, May–June.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of Empirical Methods for Natural
Language Processing (EMNLP), Singapore.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: An assessment. Natural Language En-
gineering, 14(3):369–416.
Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and
Masaaki Nagata. 2014. Dependency-based Dis-
course Parser for Single-Document Summarization.
In Proceedings of Empirical Methods for Natural
Language Processing (EMNLP).
</reference>
<page confidence="0.994153">
2224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989799">
<title confidence="0.998301">Closing the Gap: Domain Adaptation from Explicit to Implicit Discourse Relations</title>
<author confidence="0.999987">Yangfeng Ji Gongbo Zhang Jacob Eisenstein</author>
<affiliation confidence="0.999866">School of Interactive Computing Georgia Institute of Technology</affiliation>
<abstract confidence="0.999742653846154">Many discourse relations are explicitly marked with discourse connectives, and these examples could potentially serve as a plentiful source of training data for recognizing implicit discourse relations. However, there are important linguistic differences between explicit and implicit discourse relations, which limit the accuracy of such an approach. We account for these differences by applying techniques from domain adaptation, treating implicitly and explicitly-marked discourse relations as separate domains. The distribution of surface features varies across these two domains, so we apply a marginalized denoising autoencoder to induce a dense, domain-general representation. The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>In Neural Information Processing Systems (NIPS),</booktitle>
<location>Vancouver.</location>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. 2007. Analysis of representations for domain adaptation. In Neural Information Processing Systems (NIPS), Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>69--73</pages>
<location>Sophia, Bulgaria.</location>
<contexts>
<context position="4381" citStr="Biran and McKeown (2013)" startWordPosition="627" endWordPosition="630">, where the relations are automatically labeled by their discourse connectives: for example, labeling the relation as CONTRAST if the connective is but. However, Sporleder and Lascarides (2008) argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory (SDRT) relations. More recent work has focused on the Penn Discourse Treebank (PDTB), using explicitly-marked relations to supplement, rather than replace, a labeled corpus of implicit relations. For example, Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification. Lan et al. (2013) present a multi-task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification. Rutherford and Xue (2015) explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtaining a 2% improvement in Macro-F1. Our work differs from these previous efforts in that we focus exclusively on training from auto</context>
<context position="10515" citStr="Biran and McKeown, 2013" startWordPosition="1585" endWordPosition="1588">xplicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treebank. To test this idea, we </context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the Association for Computational Linguistics (ACL), pages 69–73, Sophia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>120--128</pages>
<contexts>
<context position="5414" citStr="Blitzer et al., 2006" startWordPosition="779" endWordPosition="782">ord to their system for implicit relation detection, obtaining a 2% improvement in Macro-F1. Our work differs from these previous efforts in that we focus exclusively on training from automaticallylabeled explicit instances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful. Structural correspondence learning is an early example of representation learning for domain adaptation (Blitzer et al., 2006); we build on the more computationally tractable approach of marginalized denoising autoencoders (Chen et al., 2012). Instance weighting is an approach for correcting label distribution mismatch (Jiang and Zhai, 2007); we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution. 3 Domain Adaptation for Implicit Relation Identification We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution. 3.1 Learning feature representation: Marginalized denoising autoe</context>
<context position="7240" citStr="Blitzer et al. (2006)" startWordPosition="1063" endWordPosition="1066"> and elegant form of denoising autencoder, by marginalizing over the noising process. Their single-layer marginalized denoising autoencoder (mDA) solves the following problem: min E˜.i|.i[llxi − W˜xill2] (1) W where the parameter W E Rd×d is a projection matrix. After learning the projection matrix, we use tanh(Wx) as the representation for our relation identification task. Usually, xi E Rd is a sparse vector with more than 105 dimensions. Solving the optimization problem defined in equation 1 will produce a d x d dense matrix W, and is prohibitively expensive. We employ the trick proposed by Blitzer et al. (2006) to select κ pivot features to be reconstructed. We then split all features into nonoverlapping subsets of size &lt; K. Then, a set of projection matrices are learned, so as to transform each feature subset to the pivot feature set. The final projection matrix W is the stack of all projection matrices learned from the feature subsets. 3.2 Handling mismatched label distributions: Resampling with minimal supervision There is a notable mismatch between the relation distributions for implicit and explicitly-marked discourse relations in the Penn Discourse Treebank: as shown in Figure 1, the EXPANSION</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Z Xu</author>
<author>Killian Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="5530" citStr="Chen et al., 2012" startWordPosition="796" endWordPosition="799">e previous efforts in that we focus exclusively on training from automaticallylabeled explicit instances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful. Structural correspondence learning is an early example of representation learning for domain adaptation (Blitzer et al., 2006); we build on the more computationally tractable approach of marginalized denoising autoencoders (Chen et al., 2012). Instance weighting is an approach for correcting label distribution mismatch (Jiang and Zhai, 2007); we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution. 3 Domain Adaptation for Implicit Relation Identification We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution. 3.1 Learning feature representation: Marginalized denoising autoencoders The goal of feature representation learning is to obtain dense features that capture feature correlations be</context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Z. Xu, Killian Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11847" citStr="Fan et al., 2008" startWordPosition="1802" endWordPosition="1805">data are automatically extracted by matching the PDTB discourse connectives (Prasad et al., 2007). For this data, we also need to extract the arguments of the identified connectives: for every identified connective, the sentence following this connective is labeled as Arg2 and the preceding sentence is labeled as Arg1, as suggested by Biran and McKeown (2013). In a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of Rutherford and Xue (2015). Model selection We use a linear support vector machine (Fan et al., 2008) as the classification model. Our model includes five tunable parameters: the number of pivot features n, the size of the feature subset K, the noise level for the denoising autoencoder q, the cosine similarity threshold for resampling T, and the penalty parameter C for the SVM classifier. We consider n E 11000, 2000, 3000} for pivot features and C E 10.001, 0.01, 0.1,1.0,10.0} for penalty parameters, q E 10.90, 0.95, 0.99} for noise levels. To reduce the free parameters, we set K = 5n and simply fix the cosine similarity threshold T = 0.6 Explicit Implicit Temporal Comparison Expansion Contin</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="6211" citStr="Glorot et al., 2011" startWordPosition="891" endWordPosition="894">ribution mismatch (Jiang and Zhai, 2007); we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution. 3 Domain Adaptation for Implicit Relation Identification We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution. 3.1 Learning feature representation: Marginalized denoising autoencoders The goal of feature representation learning is to obtain dense features that capture feature correlations between the source and target domains. Denoising autoencoders (Glorot et al., 2011) do this by first “corrupting” the original data, x1, ... , xn into ˜x1, ... , ˜xn, either by adding Gaussian noise (in the case of real-valued data) or by randomly zeroing out features (in the case of binary data). We can then learn a function to reconstruct the original data, thereby capturing feature correlations and improving resilience to domain shift. Chen et al. (2012) propose a particularly simple and elegant form of denoising autencoder, by marginalizing over the noising process. Their single-layer marginalized denoising autoencoder (mDA) solves the following problem: min E˜.i|.i[llxi</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the International Conference on Machine Learning (ICML), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics Volume 3: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. L. Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2239" citStr="Grice, 1975" startWordPosition="318" endWordPosition="319"> al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975). Similar observations are made by Rutherford and Xue (2015), who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of domain adaptation. Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mismatch from the explicit relations (source domain) to the implicit relations (target domain). We propose to close the gap by using two simple methods from domain adaptation: (1) feature representation learning: mapping the</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H Paul Grice. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics Volume 3: Speech Acts, pages 41–58. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<location>Prague.</location>
<contexts>
<context position="5631" citStr="Jiang and Zhai, 2007" startWordPosition="811" endWordPosition="814">tances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful. Structural correspondence learning is an early example of representation learning for domain adaptation (Blitzer et al., 2006); we build on the more computationally tractable approach of marginalized denoising autoencoders (Chen et al., 2012). Instance weighting is an approach for correcting label distribution mismatch (Jiang and Zhai, 2007); we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution. 3 Domain Adaptation for Implicit Relation Identification We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution. 3.1 Learning feature representation: Marginalized denoising autoencoders The goal of feature representation learning is to obtain dense features that capture feature correlations between the source and target domains. Denoising autoencoders (Glorot et al., 2011) do this by first “c</context>
<context position="9203" citStr="Jiang and Zhai, 2007" startWordPosition="1374" endWordPosition="1377">domain (implicit relations). Given the label distribution from the target domain, we resample training examples from the source domain with replacement, in order to match the label distribution in the target domain. As this requires the label distribution from the target domain, it is no longer purely unsupervised domain adaptation; instead, we call it resampling with minimal supervision. It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adaptation (Jiang and Zhai, 2007). Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least T with at least one target domain instance (Rutherford and Xue, 2015). 4 Experiments Our experiments test the utility of the two domain adaptation methods, using the Penn Discourse Treebank (Prasad et al., 2008) and some extra-training data collected from a external resource. 4.1 Experimental setup Datasets The test examples are implicit relation instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked exa</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the Association for Computational Linguistics (ACL), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>William W Cohen</author>
<author>Mark Dredze</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Multi-domain learning: when do domains matter?</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1302--1312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Joshi, Cohen, Dredze, Ros´e, 2012</marker>
<rawString>Mahesh Joshi, William W Cohen, Mark Dredze, and Carolyn P Ros´e. 2012. Multi-domain learning: when do domains matter? In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1302–1312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Man Lan</author>
<author>Yu Xu</author>
<author>Zhengyu Niu</author>
</authors>
<title>Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>476--485</pages>
<location>Sophia, Bulgaria.</location>
<contexts>
<context position="4523" citStr="Lan et al. (2013)" startWordPosition="649" endWordPosition="652">ut. However, Sporleder and Lascarides (2008) argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory (SDRT) relations. More recent work has focused on the Penn Discourse Treebank (PDTB), using explicitly-marked relations to supplement, rather than replace, a labeled corpus of implicit relations. For example, Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification. Lan et al. (2013) present a multi-task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification. Rutherford and Xue (2015) explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtaining a 2% improvement in Macro-F1. Our work differs from these previous efforts in that we focus exclusively on training from automaticallylabeled explicit instances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature rep</context>
</contexts>
<marker>Lan, Xu, Niu, 2013</marker>
<rawString>Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition. In Proceedings of the Association for Computational Linguistics (ACL), pages 476–485, Sophia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>343--351</pages>
<contexts>
<context position="13713" citStr="Lin et al., 2009" startWordPosition="2120" endWordPosition="2123">42.25 39.46 Table 1: Performance of cross-domain learning for implicit discourse relation identification. 0.85; pilot studies found that results are not sensitive to the value of T across a range of values. Features All features are motivated by prior work on implicit discourse relation classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The FULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times. The PIVOT feature set includes κ high-frequency features from the FULL feature set. To focus on testing the domain adaptation techniques, we us</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 343–351, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically Evaluating Text Coherence Using Discourse Relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>997--1006</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1537" citStr="Lin et al., 2011" startWordPosition="210" endWordPosition="213"> label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse Relations. In Proceedings of the Association for Computational Linguistics (ACL), pages 997–1006, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Ani Nenkova</author>
</authors>
<title>Discourse indicators for content selection in summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>147--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1421" citStr="Louis et al., 2010" startWordPosition="192" endWordPosition="195">e two domains, so we apply a marginalized denoising autoencoder to induce a dense, domain-general representation. The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relati</context>
</contexts>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, and Ani Nenkova. 2010. Discourse indicators for content selection in summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="14000" citStr="Manning et al., 2014" startWordPosition="2163" endWordPosition="2166">tion classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The FULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times. The PIVOT feature set includes κ high-frequency features from the FULL feature set. To focus on testing the domain adaptation techniques, we use the same FULL and PIVOT set for all four relations, and leave feature set optimization for each relation as a future work (Park and Cardie, 2012). To obtain the upper bound, we employ the same feature categories and frequency threshold to extract features from the in-domain data, hand</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>368--375</pages>
<contexts>
<context position="1887" citStr="Marcu and Echihabi, 2003" startWordPosition="264" endWordPosition="267">s. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975). Similar observations are made by Rutherford and Xue (2015), who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of d</context>
<context position="3408" citStr="Marcu and Echihabi (2003)" startWordPosition="492" endWordPosition="495">tation: (1) feature representation learning: mapping the source domain and target domain to a shared latent feature space; (2) resampling: modifying the relation distribution in the explicit relations to match the distribution over implicit relations. Our results on the Penn Discourse Treebank (Prasad et al., 2008) show that these two methods improve the performance on unsupervised discourse relation identification by more than 8.4% on average F1 score across all relation types, an 82% reduction on the transfer loss incurred by training on explicitly-marked discourse relations. 2 Related Work Marcu and Echihabi (2003) train a classifier for implicit intra-sentence discourse relations from 2219 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2219–2224, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explicitly-marked examples in the rhetorical structure theory (RST) treebank, where the relations are automatically labeled by their discourse connectives: for example, labeling the relation as CONTRAST if the connective is but. However, Sporleder and Lascarides (2008) argue that explicitly marked relations are too different </context>
</contexts>
<marker>Marcu, Echihabi, 2003</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2003. An unsupervised approach to recognizing discourse relations. In Proceedings of the Association for Computational Linguistics (ACL), pages 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving Implicit Discourse Relation Recognition Through Feature Set Optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>108--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seoul, South</location>
<contexts>
<context position="10489" citStr="Park and Cardie, 2012" startWordPosition="1581" endWordPosition="1584">t set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treeba</context>
<context position="13835" citStr="Park and Cardie, 2012" startWordPosition="2138" endWordPosition="2141"> studies found that results are not sensitive to the value of T across a range of values. Features All features are motivated by prior work on implicit discourse relation classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The FULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times. The PIVOT feature set includes κ high-frequency features from the FULL feature set. To focus on testing the domain adaptation techniques, we use the same FULL and PIVOT set for all four relations, and leave feature set optimization for each relation as a future wor</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving Implicit Discourse Relation Recognition Through Feature Set Optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 108–112, Seoul, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Mridhula Raghupathy</author>
<author>Hena Mehta</author>
<author>Ani Nenkova</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>87--90</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="1638" citStr="Pitler et al., 2008" startWordPosition="227" endWordPosition="230"> instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily identifiable discourse relations. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 87–90, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic Sense Prediction for Implicit Discourse Relations in Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL), Suntec,</booktitle>
<contexts>
<context position="10466" citStr="Pitler et al., 2009" startWordPosition="1577" endWordPosition="1580">4, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as th</context>
<context position="13568" citStr="Pitler et al., 2009" startWordPosition="2098" endWordPosition="2101">o 18.66 25.86 63.37 38.87 36.69 9. PIVOT Yes Yes 19.26 25.74 68.08 41.39 38.62 Explicit [PDTB + CNN] → Implicit 10. PIVOT Yes Yes 20.35 26.32 68.92 42.25 39.46 Table 1: Performance of cross-domain learning for implicit discourse relation identification. 0.85; pilot studies found that results are not sensitive to the value of T across a range of values. Features All features are motivated by prior work on implicit discourse relation classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The FULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In Proceedings of the Association for Computational Linguistics (ACL), Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Eleni Miltsakaki</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Livio Robaldo</author>
<author>Bonnie L Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0 annotation manual.</title>
<date>2007</date>
<journal>The PDTB Research Group.</journal>
<contexts>
<context position="10076" citStr="Prasad et al., 2007" startWordPosition="1513" endWordPosition="1516">in adaptation methods, using the Penn Discourse Treebank (Prasad et al., 2008) and some extra-training data collected from a external resource. 4.1 Experimental setup Datasets The test examples are implicit relation instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked examples extracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (TEMP.), Comparison (COMP.), Expansion (EXP.) and Contingency (CONT.). We train binary cl</context>
<context position="11327" citStr="Prasad et al., 2007" startWordPosition="1711" endWordPosition="1714">n each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treebank. To test this idea, we collected 1,000 news articles from CNN.com as extra training data. Explicitly-marked discourse relations from this data are automatically extracted by matching the PDTB discourse connectives (Prasad et al., 2007). For this data, we also need to extract the arguments of the identified connectives: for every identified connective, the sentence following this connective is labeled as Arg2 and the preceding sentence is labeled as Arg1, as suggested by Biran and McKeown (2013). In a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of Rutherford and Xue (2015). Model selection We use a linear support vector machine (Fan et al., 2008) as the classification model. Our model includes five tunable parameters: the nu</context>
</contexts>
<marker>Prasad, Miltsakaki, Dinesh, Lee, Joshi, Robaldo, Webber, 2007</marker>
<rawString>Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie L Webber. 2007. The Penn Discourse Treebank 2.0 annotation manual. The PDTB Research Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="3099" citStr="Prasad et al., 2008" startWordPosition="445" endWordPosition="448">ive of domain adaptation. Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mismatch from the explicit relations (source domain) to the implicit relations (target domain). We propose to close the gap by using two simple methods from domain adaptation: (1) feature representation learning: mapping the source domain and target domain to a shared latent feature space; (2) resampling: modifying the relation distribution in the explicit relations to match the distribution over implicit relations. Our results on the Penn Discourse Treebank (Prasad et al., 2008) show that these two methods improve the performance on unsupervised discourse relation identification by more than 8.4% on average F1 score across all relation types, an 82% reduction on the transfer loss incurred by training on explicitly-marked discourse relations. 2 Related Work Marcu and Echihabi (2003) train a classifier for implicit intra-sentence discourse relations from 2219 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2219–2224, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explicitly-marked </context>
<context position="9534" citStr="Prasad et al., 2008" startWordPosition="1431" endWordPosition="1434">n; instead, we call it resampling with minimal supervision. It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adaptation (Jiang and Zhai, 2007). Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least T with at least one target domain instance (Rutherford and Xue, 2015). 4 Experiments Our experiments test the utility of the two domain adaptation methods, using the Penn Discourse Treebank (Prasad et al., 2008) and some extra-training data collected from a external resource. 4.1 Experimental setup Datasets The test examples are implicit relation instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked examples extracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every </context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives.</title>
<date>2015</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>799--808</pages>
<location>Denver, CO, May–June.</location>
<contexts>
<context position="2299" citStr="Rutherford and Xue (2015)" startWordPosition="325" endWordPosition="328">otated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975). Similar observations are made by Rutherford and Xue (2015), who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of domain adaptation. Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mismatch from the explicit relations (source domain) to the implicit relations (target domain). We propose to close the gap by using two simple methods from domain adaptation: (1) feature representation learning: mapping the source domain and target domain to a shared latent feature </context>
<context position="4703" citStr="Rutherford and Xue (2015)" startWordPosition="674" endWordPosition="677">taining negative results in segmented discourse representation theory (SDRT) relations. More recent work has focused on the Penn Discourse Treebank (PDTB), using explicitly-marked relations to supplement, rather than replace, a labeled corpus of implicit relations. For example, Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification. Lan et al. (2013) present a multi-task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification. Rutherford and Xue (2015) explore several selection heuristics for adding automatically-labeled examples from Gigaword to their system for implicit relation detection, obtaining a 2% improvement in Macro-F1. Our work differs from these previous efforts in that we focus exclusively on training from automaticallylabeled explicit instances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful. Structural corr</context>
<context position="9392" citStr="Rutherford and Xue, 2015" startWordPosition="1406" endWordPosition="1410">ribution in the target domain. As this requires the label distribution from the target domain, it is no longer purely unsupervised domain adaptation; instead, we call it resampling with minimal supervision. It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adaptation (Jiang and Zhai, 2007). Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least T with at least one target domain instance (Rutherford and Xue, 2015). 4 Experiments Our experiments test the utility of the two domain adaptation methods, using the Penn Discourse Treebank (Prasad et al., 2008) and some extra-training data collected from a external resource. 4.1 Experimental setup Datasets The test examples are implicit relation instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked examples extracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically label</context>
<context position="11772" citStr="Rutherford and Xue (2015)" startWordPosition="1788" endWordPosition="1791">om CNN.com as extra training data. Explicitly-marked discourse relations from this data are automatically extracted by matching the PDTB discourse connectives (Prasad et al., 2007). For this data, we also need to extract the arguments of the identified connectives: for every identified connective, the sentence following this connective is labeled as Arg2 and the preceding sentence is labeled as Arg1, as suggested by Biran and McKeown (2013). In a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of Rutherford and Xue (2015). Model selection We use a linear support vector machine (Fan et al., 2008) as the classification model. Our model includes five tunable parameters: the number of pivot features n, the size of the feature subset K, the noise level for the denoising autoencoder q, the cosine similarity threshold for resampling T, and the penalty parameter C for the SVM classifier. We consider n E 11000, 2000, 3000} for pivot features and C E 10.001, 0.01, 0.1,1.0,10.0} for penalty parameters, q E 10.90, 0.95, 0.99} for noise levels. To reduce the free parameters, we set K = 5n and simply fix the cosine similari</context>
<context position="18234" citStr="Rutherford and Xue, 2015" startWordPosition="2842" endWordPosition="2846">gave Tina the burger.). 5 Conclusion We have presented two methods — feature representation learning and resampling — from domain adaptation to close the gap of using explicit examples for unsupervised implicit discourse relation identification. Experiments on the PDTB show the combination of these two methods eliminates more than 80% of the transfer loss caused by training on explicit examples, increasing average F1 from 31% to 39.5%, against a supervised upper bound of 41.3%. Future work will explore the combination of this approach with more sophisticated techniques for instance selection (Rutherford and Xue, 2015) and feature selection (Park and Cardie, 2012; Biran and McKeown, 2013), while also tackling the more difficult problems of multi-class relation classification and fine-grained level-2 discourse relations. Acknowledgments This research was supported by a Google Faculty Research Award to the third author. The following members of the Georgia Tech Computational Linguistics Laboratory offered feedback throughout the research process: Parminder Bhatia, Naman Goyal, Vinodh Krishan, Umashanthi Pavalanathan, Ana Smith, Yijie Wang, and especially Yi Yang. Thanks to the reviewers for their constructive</context>
</contexts>
<marker>Rutherford, Xue, 2015</marker>
<rawString>Attapol Rutherford and Nianwen Xue. 2015. Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 799–808, Denver, CO, May–June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="1492" citStr="Somasundaran et al., 2009" startWordPosition="202" endWordPosition="206"> to induce a dense, domain-general representation. The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may </context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="1929" citStr="Sporleder and Lascarides (2008)" startWordPosition="269" endWordPosition="272">s reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975). Similar observations are made by Rutherford and Xue (2015), who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of domain adaptation. Specifically, we argue t</context>
<context position="3950" citStr="Sporleder and Lascarides (2008)" startWordPosition="564" endWordPosition="567">aining on explicitly-marked discourse relations. 2 Related Work Marcu and Echihabi (2003) train a classifier for implicit intra-sentence discourse relations from 2219 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2219–2224, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. explicitly-marked examples in the rhetorical structure theory (RST) treebank, where the relations are automatically labeled by their discourse connectives: for example, labeling the relation as CONTRAST if the connective is but. However, Sporleder and Lascarides (2008) argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory (SDRT) relations. More recent work has focused on the Penn Discourse Treebank (PDTB), using explicitly-marked relations to supplement, rather than replace, a labeled corpus of implicit relations. For example, Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification. Lan et al. (2013) present a multi-task learn</context>
<context position="15227" citStr="Sporleder and Lascarides (2008)" startWordPosition="2354" endWordPosition="2357">hand-annotated implicit discourse relations. To include the representations from the marginalized denoising autoencoder for relation identification, we concatenate them with the original surface feature representations of the same examples. 4.2 Experimental results In experiments, we start with surface feature representations as baselines, then incorporate the two domain adaptation techniques incrementally. As shown in line 2 of Table 1, the performance is poor if directly applying a model trained on the explicit examples with the FULL feature set, which is consistent with the observations of Sporleder and Lascarides (2008): there is a 10.28% absolute reduction on average F1 score from the upper bound obtained with in-domain supervision (line 1). With mDA, the overall performance increases by 0.86% (line 4); resampling gives a further 4.16% improvement mainly because of the performance gain on the EXP. relation (line 5). The resampling method itself (line 3) also gives a better overall performance then mDA (line 4). However, the F1 scores on the TEMP. and CONT. are even worse than the baseline (line 2). Surface representations with the FULL feature set were found to cause serious overfitting in the experiments. </context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14(3):369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhisa Yoshida</author>
<author>Jun Suzuki</author>
<author>Tsutomu Hirao</author>
<author>Masaaki Nagata</author>
</authors>
<title>Dependency-based Discourse Parser for Single-Document Summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1444" citStr="Yoshida et al., 2014" startWordPosition="196" endWordPosition="199"> apply a marginalized denoising autoencoder to induce a dense, domain-general representation. The label distribution is also domain-specific, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classifier on explicitly-marked discourse relations. 1 Introduction Discourse relations reveal the structural organization of text, potentially supporting applications such as summarization (Louis et al., 2010; Yoshida et al., 2014), sentiment analysis (Somasundaran et al., 2009), and coherence evaluation (Lin et al., 2011). While some relations are signaled explicitly with connectives such as however (Pitler et al., 2008), many more are implicit. Expert-annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They</context>
</contexts>
<marker>Yoshida, Suzuki, Hirao, Nagata, 2014</marker>
<rawString>Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and Masaaki Nagata. 2014. Dependency-based Discourse Parser for Single-Document Summarization. In Proceedings of Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
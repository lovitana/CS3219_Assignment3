<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7659435">
Multi- and Cross-Modal Semantics Beyond Vision:
Grounding in Auditory Perception
</title>
<author confidence="0.985795">
Douwe Kiela Stephen Clark
</author>
<affiliation confidence="0.9992245">
Computer Laboratory Computer Laboratory
University of Cambridge University of Cambridge
</affiliation>
<email confidence="0.996335">
douwe.kiela@cl.cam.ac.uk stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996171875">
Multi-modal semantics has relied on fea-
ture norms or raw image data for per-
ceptual input. In this paper we examine
grounding semantic representations in raw
auditory data, using standard evaluations
for multi-modal semantics, including mea-
suring conceptual similarity and related-
ness. We also evaluate cross-modal map-
pings, through a zero-shot learning task
mapping between linguistic and auditory
modalities. In addition, we evaluate multi-
modal representations on an unsupervised
musical instrument clustering task. To our
knowledge, this is the first work to com-
bine linguistic and auditory information
into multi-modal representations.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829113207547">
Although distributional models (Turney and Pan-
tel, 2010; Clark, 2015) have proved useful for a
variety of NLP tasks, the fact that the meaning of
a word is represented as a distribution over other
words implies that they suffer from the ground-
ing problem (Harnad, 1990); i.e. they do not ac-
count for the fact that human semantic knowledge
is grounded in the perceptual system (Louwerse,
2008). Motivated by human concept acquisition,
multi-modal semantics enhances linguistic repre-
sentations with extra-linguistic perceptual input.
These models outperform language-only models
on a range of tasks, including modelling semantic
similarity and relatedness, and predicting compo-
sitionality (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013; Bruni et al., 2014). Al-
though feature norms have also been used, raw
image data has become the de-facto perceptual
modality in multi-modal models.
However, if the objective is to ground seman-
tic representations in perceptual information, why
stop at image data? The meaning of violin is
surely not only grounded in its visual properties,
such as shape, color and texture, but also in its
sound, pitch and timbre. To understand how per-
ceptual input leads to conceptual representation,
we should use as many perceptual modalities as
possible. A recent preliminary study by Lopopolo
and van Miltenburg (2015) found that it is possible
to derive uni-modal semantic representations from
sound data. Here, we explore taking multi-modal
semantics beyond its current reliance on image
data and experiment with grounding semantic rep-
resentations in the auditory perceptual modality.
Multi-modal models that rely on raw image
data have typically used “bag of visual words”
(BoVW) representations (Sivic and Zisserman,
2003). We follow a similar approach for the
auditory modality and construct bag of audio
words (BoAW) representations. Following pre-
vious work in multi-modal semantics, we evalu-
ate these models on measuring conceptual simi-
larity and relatedness, and inducing cross-modal
mappings between modalities to perform zero-
shot learning. In addition, we evaluate on an
unsupervised musical instrument clustering task.
Our findings indicate that multi-modal representa-
tions enriched with auditory information perform
well on relatedness and similarity tasks, particu-
larly on words that have auditory assocations. To
our knowledge, this is the first work to combine
linguistic and auditory representations in multi-
modal semantics.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999928428571429">
Information processing in the brain can be roughly
described to occur on three levels: perceptual in-
put, conceptual representation and symbolic rea-
soning (Gazzaniga, 1995). While research in AI
has made great progress in understanding the first
and last of these, understanding the middle level is
still more of an open problem: how is it that per-
</bodyText>
<page confidence="0.910224">
2461
</page>
<note confidence="0.984354">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2461–2470,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99992737254902">
ceptual input leads to conceptual representations
that can be processed and reasoned with?
A key observation is that concepts are, through
perception, grounded in physical reality and sen-
sorimotor experience (Harnad, 1990; Louwerse,
2008), and there has been a surge of recent work
on perceptually grounded semantic models that try
to account for this fact. These models learn se-
mantic representations from both textual and per-
ceptual input, using either feature norms (Silberer
and Lapata, 2012; Roller and Schulte im Walde,
2013; Hill and Korhonen, 2014) or raw image
data (Feng and Lapata, 2010; Leong and Mihal-
cea, 2011; Bruni et al., 2014) as the source of per-
ceptual information. A popular approach in the
latter case is to collect images associated with a
concept, and then lay out each image as a set of
keypoints on a dense grid, where each keypoint
is represented by a robust local feature descriptor
such as SIFT (Lowe, 2004). These local descrip-
tors are subsequently clustered into a set of “vi-
sual words” using a standard clustering algorithm
such as k-means and then quantized into vector
representations by comparing the descriptors with
the centroids. An alternative to this bag of vi-
sual words (BoVW) approach is transferring fea-
tures from convolutional neural networks (Kiela
and Bottou, 2014).
Various ways of aggregating images into visual
representations have been proposed, such as tak-
ing the mean or the elementwise maximum. Ide-
ally, one would jointly learn multi-modal repre-
sentations from parallel multi-modal data, such as
text containing images (Silberer and Lapata, 2014)
or images described with speech (Synnaeve et al.,
2014), but such data is hard to obtain, has limited
coverage and can be noisy. Hence, image repre-
sentations are often learned independently. Ag-
gregated visual representations are subsequently
combined with a traditional linguistic space to
form a multi-modal model. This mixing can be
done in a variety of ways, ranging from simple
concatenation to more sophisticated fusion meth-
ods (Bruni et al., 2014).
Cross-modal semantics, instead of being con-
cerned with improving semantic representations
through grounding, focuses on the problem of ref-
erence. Using, for instance, mappings between
visual and textual space, the objective is to learn
which words refer to which objects (Lazaridou
et al., 2014). This problem is very much re-
</bodyText>
<equation confidence="0.615224">
SimLex-999 score
taxi-cab 0.92
plane-jet 0.81
horse-mare 0.83
sheep-lamb 0.84
bird-hawk 0.79
band-orchestra 0.71
music-melody 0.70
</equation>
<tableCaption confidence="0.9467325">
Table 1: Examples of pairs in the datasets where
auditory is relevant, with the similarity score.
</tableCaption>
<bodyText confidence="0.998618846153846">
lated to the object recognition task in computer
vision, but instead of using just visual data and
labels, these cross-modal models also utilize tex-
tual information (Socher et al., 2014; Frome et al.,
2013). This allows for zero-shot learning, where
the model can predict how an object relates to
other concepts just from seeing an image of the
object, but without ever having previously encoun-
tered an image of that particular object (Lazaridou
et al., 2014). Multi-modal and cross-modal ap-
proaches have outperformed state-of-the-art text-
based methods on a variety of tasks (Bruni et al.,
2014; Silberer and Lapata, 2014).
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="method">
3 Evaluations
</sectionHeader>
<bodyText confidence="0.999979571428572">
Following previous work in multi-modal seman-
tics, we evaluate on two standard similarity and re-
latedness datasets: SimLex-999 (Hill et al., 2014)
and the MEN test collection (Bruni et al., 2014).
These datasets consist of concept pairs together
with a human-annotated similarity or relatedness
score, where the former dataset focuses on gen-
uine similarity (e.g., teacher-instructor) and the
latter focuses more on relatedness (e.g., river-
water). In addition, following previous work in
cross-modal semantics, we evaluate on the zero-
shot learning task of inducing a cross-modal map-
ping to the correct label in the auditory modality
from the linguistic one and vice-versa.
</bodyText>
<subsectionHeader confidence="0.997847">
3.1 Multi-modal Semantics
</subsectionHeader>
<bodyText confidence="0.999852">
Evidence suggests that the inclusion of visual rep-
resentations only improves performance for cer-
tain concepts, and that in some cases the introduc-
tion of visual information is detrimental to perfor-
mance on similarity and relatedness tasks (Kiela
et al., 2014). The same is likely to be true for
other perceptual modalities: in the case of com-
parisons such as guitar-piano, the auditory modal-
</bodyText>
<figure confidence="0.9956914375">
MEN
automobile-car
rain-storm
cat-feline
jazz-musician
bird-eagle
highway-traffic
guitar-piano
score
1.00
0.98
0.96
0.88
0.88
0.88
0.86
</figure>
<page confidence="0.987857">
2462
</page>
<table confidence="0.995634333333333">
Dataset MEN AMEN SLex ASLex
Linguistic 3000 258 999 296
Auditory 2590 233 534 216
</table>
<tableCaption confidence="0.997007">
Table 2: Number of concept pairs for which repre-
sentations are available in each modality.
</tableCaption>
<bodyText confidence="0.997347956521739">
ity is certainly meaningful, whereas in the case
of democracy-anarchism it is probably less so.
Therefore, we had two graduate students annotate
the datasets according to whether auditory percep-
tion is relevant to the pairwise comparison. The
annotation criterion was as follows: if both con-
cepts in a pairwise comparison have a distinctive
associated sound, the modality is deemed rele-
vant. Inter-annotator agreement was high: n =
0.93 for MEN and n = 0.92 for SimLex-999.
Some examples of relevant pairs can be found in
Table 1. Hence, we now have four evaluation
datasets: the MEN test collection MEN and its
auditory-relevant subset AMEN; and the SimLex-
999 dataset SLex and its auditory-relevant sub-
set ASLex. Due to the nature of the auditory
data sources, it is not possible to build auditory
representations for all concepts in the test sets.
Hence, unless stated otherwise, we report results
for the covered subsets (using the same subsets
when comparing across modalities, to ensure a fair
comparison). Table 2 shows how much of the test
sets are covered for each modality.1
</bodyText>
<subsectionHeader confidence="0.997936">
3.2 Cross-modal Semantics
</subsectionHeader>
<bodyText confidence="0.999946142857143">
In addition to evaluating our models on the MEN
and SimLex tasks, we evaluate on the cross-
modal task of zero-shot learning. In the case
of vision, Lazaridou et al. (2014) studied the
possibility of predicting from “we found a cute,
hairy wampimuk sleeping behind the tree” that a
“wampimuk” will probably look like a small furry
animal, even though a wampimuk has never been
seen before. We evaluate zero-shot learning, using
partial least squares regression (PLSR) to obtain
cross-modal mappings from the linguistic to audi-
tory space and vice versa.2 Thus, given a linguistic
representation for e.g. guitar, the task is to map it
to the appropriate place in auditory space without
</bodyText>
<footnote confidence="0.95178625">
1To facilitate further work in multi-modal semantics be-
yond vision, our code and data have been made publicly
available at http://www.cl.cam.ac.uk/˜dk427/audio.html.
2To avoid introducing another parameter, we set the num-
</footnote>
<bodyText confidence="0.7854758">
ber of latent variables in the cross-modal PLSR map to a third
of the number of dimensions of the perceptual representation.
ever having heard a guitar; or map it to the appro-
priate place in linguistic space without ever having
read about a guitar (having only heard it).
</bodyText>
<sectionHeader confidence="0.994656" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999985875">
One reason for using raw image data in multi-
modal models is that there is a wide variety of re-
sources that contain tagged images, such as Im-
ageNet (Deng et al., 2009) and the ESP Game
dataset (Von Ahn and Dabbish, 2004). However,
such resources do not exist for audio files, and
so we follow a similar approach to Fergus et al.
(2005) and Bergsma and Goebel (2011), who use
Google Images to obtain images. We use the
online search engine Freesound3 to obtain audio
files. Freesound is a collaborative database re-
leased under Creative Commons licenses, in the
form of snippets, samples and recordings, that is
aimed at sound artists. The Freesound API allows
users to easily search for audio files that have been
tagged using certain keywords.
For each of the concepts in the evaluation
datasets, we used the Freesound API to obtain
samples encoded in the standard open source OGG
format4. Because the database contains variable
numbers of files, with varying duration per indi-
vidual file, we restrict the search to a maximum of
50 files and a maximum of 1 minute duration per
file. The Freesound API allows for various degrees
of keyword matching: we opted for the strictest
keyword matching, in that the audio file needs to
have been purposely tagged with the given word
(the alternative includes searching the text descrip-
tion for matching keywords). For example, if we
are searching for audio files of cars, we retrieve up
to 50 files with a maximum duration of 1 minute
per file that have been tagged with the label “car”.
</bodyText>
<subsectionHeader confidence="0.998222">
4.1 Linguistic Representations
</subsectionHeader>
<bodyText confidence="0.999976888888889">
For the linguistic representations we use the con-
tinuous vector representations from the log-linear
skip-gram model of Mikolov et al. (2013). Specifi-
cally, we trained 300-dimensional vector represen-
tations trained on a dump of the English Wikipedia
plus newswire (8 billion words in total).5 These
types of representations have been found to yield
the highest performance on a variety of semantic
similarity tasks (Baroni et al., 2014).
</bodyText>
<footnote confidence="0.99995175">
3http://www.freesound.org.
4http://www.vorbis.com.
5We used the demo-train-big-model-v1.sh script from
http://word2vec.googlecode.com to obtain this corpus.
</footnote>
<page confidence="0.916722">
2463
</page>
<subsectionHeader confidence="0.994348">
4.2 Auditory Representations
</subsectionHeader>
<bodyText confidence="0.999996277777778">
A common approach to obtaining acoustic fea-
tures of audio files is the Mel-scale Frequency
Cepstral Coefficient (MFCC) (O’Shaughnessy,
1987). MFCC features are abundant in a wide
variety of applications in audio signal process-
ing, ranging from audio information retrieval, to
speech and speaker recognition, and music analy-
sis (Eronen, 2003). Such features are derived from
the mel-frequency cepstrum representation of an
audio fragment (Stevens et al., 1937). In MFCC,
frequency bands are spaced along the mel scale,
which has the advantage that it approximates hu-
man auditory perception more closely than e.g.
linearly-spaced frequency bands. Hence, MFCC
takes human perceptual sensitivity to audio fre-
quencies into consideration, which makes it suit-
able for e.g. compression and recognition tasks,
but also for our current objective of modelling au-
ditory perception. We obtain MFCC descriptors
for frames of audio files using librosa, a popu-
lar library for audio and music analysis written
in Python.6 After having obtained the descrip-
tors, we cluster them using mini-batch k-means
(Sculley, 2010) and quantize the descriptors into a
“bag of audio words” (BoAW) (Foote, 1997) rep-
resentation by comparing the MFCC descriptors
to the cluster centroids. This gives us BoAW rep-
resentations for each of the audio files. Auditory
representations are obtained by taking the mean
of the BoAW representations of the relevant au-
dio files, and finally weighting them using positive
point-wise mutual information (PPMI), a standard
weighting scheme for improving vector represen-
tation quality (Bullinaria and Levy, 2007). We set
k = 300, which equals the number of dimensions
for the linguistic representations.
</bodyText>
<subsectionHeader confidence="0.99949">
4.3 Multi-modal Fusion Strategies
</subsectionHeader>
<bodyText confidence="0.999970636363636">
Since multi-modal semantics relies on two or more
modalities, there are several ways of combining
or fusing linguistic and perceptual cues (Bruni et
al., 2014). When computing similarity scores, for
instance, we can either jointly learn the represen-
tations; learn them independently, combine (e.g.
concatenate) them and compute similarity scores;
or learn them independently, compute similarity
scores independently and combine the scores. We
call these possibilities early, middle and late fu-
sion, respectively, and evaluate multi-modal mod-
</bodyText>
<footnote confidence="0.873848">
6http://bmcfee.github.io/librosa.
</footnote>
<bodyText confidence="0.599514">
els in each category.
</bodyText>
<subsubsectionHeader confidence="0.52811">
4.3.1 Early Fusion
</subsubsectionHeader>
<bodyText confidence="0.993548166666666">
A good example of early fusion is the recently
introduced multi-modal skip-gram model (Lazari-
dou et al., 2015). This model behaves like a nor-
mal skip-gram, but instead of only having a train-
ing objective for the linguistic representation, it in-
cludes an additional training objective for the vi-
sual context, which consists of an aggregated rep-
resentation of images associated with the given
target word. The skip-gram training objective for
a sequence of training words w1, w2,..., wT and a
context size c is:
where Jθ is the log-likelihood
</bodyText>
<equation confidence="0.41473725">
E
−c≤j≤c log p(wt+jJwt) and p(wt+jJwt) is
obtained via the softmax:
Ewl=1 expuTwlvwt
</equation>
<bodyText confidence="0.9999828">
where uw and vw are the context and target vec-
tor representations for the word w respectively,
and W is the vocabulary size. The objective for
the multi-modal skip-gram has an additional vi-
sual objective Jvis (in this case a margin criterion):
</bodyText>
<equation confidence="0.899849">
Jθ(wt) + Jvis(wt)
</equation>
<bodyText confidence="0.999964636363636">
Here, we take a similar but more straightfor-
ward approach by making the auditory context a
part of the initial training objective, which is pos-
sible because linguistic and auditory representa-
tions have the same dimensionality. That is, we
modified word2vec to predict additional auditory
contexts that have been set to the corresponding
BoAW representation. We jointly learn linguistic
and audio representations by taking the aggregated
mean µaw of the auditory vectors for a given word
w, and adding this mean vector to the context:
</bodyText>
<equation confidence="0.962977">
Jθ(wt) + log p(µawtJwt)
</equation>
<bodyText confidence="0.990672">
The intuition is that the induced vector for the
target word now has to predict an auditory vec-
tor as part of its context, as well as the linguis-
tic ones. As an alternative, we also investigate re-
</bodyText>
<equation confidence="0.992505785714286">
1
T
Jθ(wt)
T
t=1
exp
p(wt+jJwt) =
T
uwt+j vw
1 T
T t=1
1 T
T
t=1
</equation>
<page confidence="0.853199">
2464
</page>
<bodyText confidence="0.9887716">
placing the mean µa
wt with an auditory vector ob-
tained by uniformly sampling from the set of au-
ditory representations for the target word. We re-
fer to these two alternatives as MMSG-MEAN and
MMSG-SAMPLED, respectively. For this model,
auditory BoAW representations are built for the
ten thousand most frequent words in our corpus,
based on 10 audio files retrieved from FreeSound
for each word (or fewer when 10 are not available).
</bodyText>
<subsectionHeader confidence="0.827702">
4.3.2 Middle and Late Fusion
</subsectionHeader>
<bodyText confidence="0.999842333333333">
Whereas early fusion requires a joint training ob-
jective that takes into account both modalities,
middle fusion allows for individual training ob-
jectives and independent training data. Similarity
between two multi-modal representations is calcu-
lated as follows:
</bodyText>
<equation confidence="0.647381">
sim(u, v) = g(f(ul,ua), f(vl,va))
</equation>
<bodyText confidence="0.990321333333333">
where g is some similarity function, ul and vl are
linguistic representations, and ua and va are the
auditory representations. A typical formulation in
multi-modal semantics for f(x, y) is α x1l(1−α)y,
where 11 is concatenation (see e.g. Bruni et al.
(2014) and Kiela and Bottou (2014)).
Late fusion can be seen as the converse of mid-
dle fusion, in that the similarity function is com-
puted first before the similarity scores are com-
bined:
sim(u, v) = h(g(ul, vl), g(ua, va))
where g is some similarity function and h is a way
of combining similarities, in our case a weighted
average: h(x, y) = 12(α x+(1−α)y); and we use
g = x·y (cosine similarity). Since cosine simi-
larity is the normalized dot-product, and the uni-
modal representations are themselves normalized,
middle and late fusion are equivalent if α = 0.5,
which we call MM. However, when α =� 0.5, we
distinguish between the two models, calling them
MM-MIDDLE and MM-LATE respectively.
</bodyText>
<sectionHeader confidence="0.999993" genericHeader="method">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999862">
5.1 Conceptual Similarity and Relatedness
</subsectionHeader>
<bodyText confidence="0.999870333333333">
We evaluate performance by calculating the Spear-
man ρs correlation between the ranking of the
concept pairs produced by the automatic similar-
ity metric (cosine between the derived vectors)
and that produced by the gold-standard similarity
scores. To ensure a fair comparison, we evaluate
</bodyText>
<table confidence="0.999727666666667">
Modality MEN AMEN SLex ASLex
Linguistic 0.687 0.603 0.320 0.314
Auditory 0.325 0.510 0.161 0.201
MMSG-MEAN 0.612 0.537 0.274 0.266
MMSG-SAMPLED 0.690 0.602 0.321 0.314
MM 0.680 0.662 0.314 0.345
</table>
<tableCaption confidence="0.999063">
Table 3: Spearman ρs correlation comparison of
</tableCaption>
<bodyText confidence="0.990757527777778">
uni-modal and multi-modal representations. The
MMSG models perform early fusion, MM repre-
sents middle and late fusion with α = 0.5 (see
Section 4.3.2).
on the common subsets for which there are repre-
sentations in both modalities (see Table 2).
The results are reported in Table 3. We find
that, while performance decreases for linguistic
representations on the auditory-relevant subsets of
the two datasets, performance increases for the
uni-modal auditory representations on those sub-
sets. This indicates that our auditory representa-
tions are better at judging auditory-relevant com-
parisons than they are at non-auditory ones, as we
might expect.
For all datasets, the accuracy scores for multi-
modal models are at least as high as those for the
purely linguistic representations. In the case of
the full datasets this difference is only marginal,
which is to be expected given how few of the
words in the datasets are auditory-relevant. How-
ever, the results indicate that adding auditory in-
put even for words that are not directly auditory-
relevant is not detrimental to overall performance.
In the case of the auditory-relevant subsets, we
see a large increase in performance when using
multi-modal representations. It is also interesting
that this performance increase is found in the sim-
ple MM model, compared to the more complicated
MMSG models, which seems to indicate that the
latter models are still too reliant on linguistic in-
formation, which harms their performance when
performing auditory-specific comparisons. The
model which performs consistently well across the
four datasets is MM, the middle-late fusion model
with α = 0.5.
</bodyText>
<subsectionHeader confidence="0.995368">
5.2 Cross-modal Zero-shot Learning
</subsectionHeader>
<bodyText confidence="0.99997075">
We learn a cross-modal mapping between the
linguistic and auditory spaces using partial least
squares regression, taking out each concept, train-
ing on the others, and then projecting from one
</bodyText>
<page confidence="0.977957">
2465
</page>
<table confidence="0.9997825">
Mapping P@1 P@5 P@20 P@50
Chance 0.00 0.93 4.01 8.49
Auditory ⇒ Ling. 0.77 6.48 17.54 31.33
Ling. ⇒ Auditory 0.73 6.71 22.16 37.32
</table>
<tableCaption confidence="0.998719">
Table 4: Cross-modal zero-shot learning accuracy.
</tableCaption>
<bodyText confidence="0.999749384615385">
space into the other. Zero-shot performance is
evaluated using the average percentage correct at
N (P@N), which measures how many of the test
instances were ranked within the top N highest
ranked nearest neighbors. Results are shown in
Table 4, with the chance baseline obtained by ran-
domly ranking a concept’s nearest neighbors. In-
sofar as it is possible to make a direct comparison
with linguistic-visual zero-shot learning (which
uses entirely different data), it appears that the cur-
rent task may be more difficult: Lazaridou et al.
(2014) report a P@1 of 2.4 and P@20 of 33.0 for
their linguistic-visual model.
</bodyText>
<subsectionHeader confidence="0.998235">
5.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.9999736">
We also performed a small qualitative analysis of
the BoAW representations for the words in MEN
and SLex. As Table 5 shows, the nearest neighbors
are remarkably semantically coherent. For exam-
ple, the model groups together sounds produced
by machines, or by water. It even finds that dinner,
meal, lunch and breakfast are closely related. In
contrast, nearest neighbors for the linguistic model
tend to be of a more abstract nature: where we find
mouth and throat as auditory neighbors for lan-
guage, the linguistic model gives us concepts like
word and dictionary; while auditory gossip sounds
like maids and is something you might do in the
corridor, it is linguistically associated with more
abstract concepts like news and newspaper.
</bodyText>
<sectionHeader confidence="0.993253" genericHeader="method">
6 Parameter Tuning
</sectionHeader>
<bodyText confidence="0.9988898">
There are many parameters that were left fixed in
the main results that could have been adjusted to
improve performance, particularly in the middle
and late fusion models. It is useful to investigate
some of the parameters that are likely to have an
impact on performance: what the effect of the α
mixing parameter is, whether a different k would
have yielded better auditory representations, and
whether the number and duration of the audio files
from FreeSound has any effect.
</bodyText>
<figureCaption confidence="0.6462645">
Figure 1: Performance of middle and late multi-
modal fusion models compared to linguistic rep-
resentations on the four datasets when varying the
α mixing parameter on the x-axis.
</figureCaption>
<subsectionHeader confidence="0.999884">
6.1 Mixing with α
</subsectionHeader>
<bodyText confidence="0.99995475">
The mixing parameter α plays an important role
in the middle and late fusion models. We kept
it fixed at 0.5 for the MM model above, but here
we experiment with varying the parameter, yield-
ing results for two different models, MM-MIDDLE
and MM-LATE. The results are shown in Figure 1,
where moving to the right on the x-axis uses more
linguistic input and moving to the left uses more
auditory input. The late model consistently out-
performs the middle fusion model, which is prob-
ably because it is less susceptible to any noise in
the auditory representation. Optimal performance
seems to be around α = 0.6 for both fusion strate-
gies on all four datasets, indicating that it is bet-
ter to include a little more linguistic than auditory
input. It appears that any 0.5 ≤ α &lt; 1 (i.e.,
where we have more linguistic input but still some
auditory signal), outperforms the purely linguis-
tic representation, substantially in the case of the
auditory-relevant subsets.
</bodyText>
<subsectionHeader confidence="0.999531">
6.2 Number of Auditory Dimensions
</subsectionHeader>
<bodyText confidence="0.9999225">
We experimented with different values for the
number of audio words k (i.e. the number of clus-
ters in the k-means clustering that determines the
number of “audio words”). As Figure 2 shows, the
quality of the uni-modal auditory representations
is highly robust to the number of dimensions. In
fact, any choice of k in the range shown provides
similar results across the datasets.
</bodyText>
<page confidence="0.976256">
2466
</page>
<subsectionHeader confidence="0.494993">
Auditory Linguistic
</subsectionHeader>
<bodyText confidence="0.804004666666667">
navy language gossip dinner navy language gossip dinner
army mouth maid meal army word news lunch
aviation man guest lunch military words newspaper wedding
plane father elevator writer vessel literature cute meal
jet adult danger breakfast sunk dictionary sexy breakfast
cannon throat corridor couch ship tongue mirror cocktail
monster motor water dawn monster motor water dawn
orchestra engine stream summer zombie vehicle droplets dusk
demon rain bath child demon automobile salt sunrise
guitar beach river victor dragon car cold moon
beast boat bathroom morning beast motorcycle sunlight night
pilot car rain garden creatures truck milk misty
</bodyText>
<tableCaption confidence="0.994854">
Table 5: Example nearest neighbors for auditory (BoAW) representations and linguistic representations.
</tableCaption>
<figureCaption confidence="0.994523666666667">
Figure 2: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
k parameter.
</figureCaption>
<subsectionHeader confidence="0.794286">
6.3 Number and Duration of Audio Files
</subsectionHeader>
<bodyText confidence="0.9999918">
We experimented with the number of audio files
by querying FreeSound for up to 100 audio files
per search word, while keeping k = 300. The
results are shown in Figure 3. It appears that “the
more the better”, although performance does not
increase significantly after around 40 audio files.
In order to examine the effect of audio file du-
ration, we experimented with specifying the du-
ration of audio files when querying the database,
either taking very short (up to 5 seconds), medium
length (up to 1 minute) or files of any duration.
The results can be found in Figure 4, showing that
performance generally increases as the files get
longer (except on AMEN where a duration of 1
minute provides optimal performance).
</bodyText>
<sectionHeader confidence="0.893172" genericHeader="method">
7 Case Study: Musical Instruments
</sectionHeader>
<bodyText confidence="0.961677">
To strengthen the finding that multi-modal repre-
sentations perform well on the auditory-relevant
subsets of the datasets, we evaluate on an alto-
gether different task, namely that of musical in-
Figure 3: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
number of audio files per target word.
strument classification. We used Wikipedia to col-
lect a total of 52 instruments and divided them into
5 classes: brass, percussion, piano-based, string
and woodwind instruments. For each of the in-
struments, we collected as many audio files from
FreeSound as possible, and used the MM-MIDDLE
model with parameter settings that yielded good
results in the previous experiments (k = 300 and
α = 0.6). We then performed k-means cluster-
ing with five cluster centroids and compared re-
sults between auditory, linguistic and multi-modal,
evaluating the clustering quality using the standard
V-measure clustering evaluation metric (Rosen-
berg and Hirschberg, 2007).
This is an interesting problem because instru-
ment classes are determined somewhat by conven-
</bodyText>
<page confidence="0.978235">
2467
</page>
<table confidence="0.709072272727273">
Model Auditory Linguistic MM-MIDDLE
V-measure 0.39 0.47 0.54
Linguistic
1 baritone
2 lute, zither, xylophone, lyre, cymbals
3 piano, trombone, clarinet, cello, violin
4 castanets, tambourine, claves, maracas
5 trumpet, horn, bugle, cowbell, carillon
Multi-modal
drum, claves, bongo, bass, conga
xylophone, glockenspiel, tambourine, cymbals
</table>
<figure confidence="0.8068126">
1
2
cello, piano, clarinet, trombone, violin,
3
4
</figure>
<figureCaption confidence="0.930970666666667">
Figure 4: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
maximum duration.
</figureCaption>
<bodyText confidence="0.999947368421053">
tion (is a saxophone a brass or a woodwind in-
strument?). What is more, how instruments ac-
tually sound is rarely described in detail in text,
so corpus-based linguistic representations cannot
take this information into account. The results are
in Table 6, clearly showing that the multi-modal
representation which utilizes both linguistic infor-
mation and auditory input performs much better
on this task than the uni-modal representations. It
is interesting to observe that the linguistic repre-
sentations perform better than the auditory ones: a
possible explanation for this is that audio files in
FreeSound are rarely samples of a single individ-
ual instrument, so if a bass is often accompanied
by a drum this will affect the overall representa-
tion. The table also shows, for the 5 clusters under
both models, the nearest instruments to the cluster
centroids, qualitatively demonstrating the greater
cluster coherence for the multi-modal model.
</bodyText>
<sectionHeader confidence="0.999034" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999947384615385">
We have studied grounding semantic representa-
tions in raw auditory perceptual information, us-
ing a bag of audio words model to obtain au-
ditory representations, and combining them into
multi-modal representations using a variety of fu-
sion strategies. Following previous work in multi-
modal semantics, we evaluated on conceptual sim-
ilarity and relatedness datasets, and on the cross-
modal task of zero-shot learning. We presented
a short case study showing that multi-modal rep-
resentations perform much better than auditory or
linguistic representations on a musical instrument
clustering task. It may well be the case that the
</bodyText>
<figure confidence="0.18008">
chimes, bell
5 mandolin, banjo, harmonica, guitar, sitar
</figure>
<tableCaption confidence="0.824745">
Table 6: V-measure performance for clustering
</tableCaption>
<bodyText confidence="0.980339230769231">
musical instruments, together with instruments
closest to cluster centroid for linguistic and multi-
modal.
auditory modality is better suited for other evalua-
tions, but we have chosen to follow standard eval-
uations in multi-modal semantics to allow for a di-
rect comparison.
In future work, it would be interesting to inves-
tigate different sampling strategies for the early
fusion joint-learning approach and to investigate
more sophisticated mixing strategies for the mid-
dle and late fusion models, e.g. using the “audio
dispersion” of a word to determine how much au-
ditory input should be included in the multi-modal
representation (Kiela et al., 2014). Another in-
teresting possibility is to improve auditory repre-
sentations by training a neural network classifier
on the audio files and subsequently transferring
the hidden representations to tasks in semantics.
Lastly, now that the perceptual modalities of vi-
sion, audio and even olfaction (Kiela et al., 2015)
have been investigated in the context of distribu-
tional semantics, the logical next step for future
work is to explore different fusion strategies for
multi-modal models that combine various sources
of perceptual input into a single grounded model.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9632224">
DK is supported by EPSRC grant EP/I037512/1.
SC is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. We are
grateful to Xavier Serra, Frederic Font Corbera,
Alessandro Lopopolo and Emiel van Miltenburg
</bodyText>
<page confidence="0.960134">
2468
</page>
<bodyText confidence="0.995044">
for useful suggestions and thank the anonymous
reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.996041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP, pages 399–405.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: A computational study. Be-
havior Research Methods, 39:510–526.
Stephen Clark. 2015. Vector Space Models of Lexical
Meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, chapter 16.
Wiley-Blackwell, Oxford.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Proceedings of Com-
puter Vision and Pattern Recognition (CVPR), pages
248–255.
A. Eronen. 2003. Musical instrument recognition us-
ing ICA-based transform of features and discrimina-
tively trained HMMs. In Proceedings of the Seventh
International Symposium on Signal Processing and
Its Applications, volume 2, pages 133–136.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of NAACL, pages 91–99.
Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew
Zisserman. 2005. Learning object categories from
Google’s image search. In Proceedings of ICCV,
pages 1816–1823.
Jonathan T Foote. 1997. Content-based retrieval of
music and audio. In Voice, Video, and Data Com-
munications, pages 138–147.
Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. DeViSE: A Deep
Visual-Semantic Embedding Model. In Proceedings
of NIPS, pages 2121–2129.
Michael S. Gazzaniga, editor. 1995. The Cognitive
Neurosciences. MIT Press, Cambridge, MA.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335–346.
Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can’t see what I mean. In Pro-
ceedings of EMNLP, pages 255–265.
Felix Hill, Roi Reichart, and Anna Korhonen.
2014. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.
Douwe Kiela and L´eon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings of
EMNLP, pages 36–45.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL, pages 835–841.
Douwe Kiela, Luana Bulat, and Stephen Clark. 2015.
Grounding semantics in olfactory perception. In
Proceedings ofACL, pages 231–236, Beijing, China,
July.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL, pages 1403–1414.
Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skipgram model. In Proceedings of
NAACL.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.
A. Lopopolo and E. van Miltenburg. 2015. Sound-
based distributional models. In Proceedings of the
11th International Conference on Computational
Semantics (IWCS 2015).
Max M. Louwerse. 2008. Symbol interdependency in
symbolic and embodied cognition. Topics in Cogni-
tive Science, 59(1):617–645.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
D. O’Shaughnessy. 1987. Speech communication: hu-
man and machine. Addison-Wesley series in electri-
cal engineering: digital signal processing. Universi-
ties Press (India) Pvt. Limited.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of
EMNLP, pages 1146–1157.
</reference>
<page confidence="0.828038">
2469
</page>
<reference confidence="0.9998173">
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420.
David Sculley. 2010. Web-scale k-means clustering.
In Proceedings of WWW, pages 1177–1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423–1433.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL, pages 721–732.
Josef Sivic and Andrew Zisserman. 2003. Video
google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470–
1477.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
ACL, 2:207–218.
Stanley Smith Stevens, John Volkmann, and Edwin B.
Newman. 1937. A scale for the measurement of
the psychological magnitude pitch. Journal of the
Acoustical Society ofAmerica, 8(3):185—190.
Gabriel Synnaeve, Maarten Versteegh, and Emmanuel
Dupoux. 2014. Learning words from images and
speech. In NIPS Workshop on Learning Semantics,
Montreal, Canada.
Peter D. Turney and Patrick Pantel. 2010. From
Frequency to Meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188, January.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319–326. ACM.
</reference>
<page confidence="0.9862">
2470
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972064">
<title confidence="0.99816">Multiand Cross-Modal Semantics Beyond Grounding in Auditory Perception</title>
<author confidence="0.998608">Douwe Kiela Stephen Clark</author>
<affiliation confidence="0.999931">Computer Laboratory Computer Laboratory University of Cambridge University of Cambridge</affiliation>
<email confidence="0.98671">douwe.kiela@cl.cam.ac.ukstephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.999404823529412">Multi-modal semantics has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and relatedness. We also evaluate cross-modal mappings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="12888" citStr="Baroni et al., 2014" startWordPosition="2011" endWordPosition="2014">re searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label “car”. 4.1 Linguistic Representations For the linguistic representations we use the continuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013). Specifically, we trained 300-dimensional vector representations trained on a dump of the English Wikipedia plus newswire (8 billion words in total).5 These types of representations have been found to yield the highest performance on a variety of semantic similarity tasks (Baroni et al., 2014). 3http://www.freesound.org. 4http://www.vorbis.com. 5We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4.2 Auditory Representations A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O’Shaughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal processing, ranging from audio information retrieval, to speech and speaker recognition, and music analysis (Eronen, 2003). Such features are derived from the mel-frequency cepstrum repres</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>399--405</pages>
<contexts>
<context position="11259" citStr="Bergsma and Goebel (2011)" startWordPosition="1744" endWordPosition="1747">s in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. ever having heard a guitar; or map it to the appropriate place in linguistic space without ever having read about a guitar (having only heard it). 4 Approach One reason for using raw image data in multimodal models is that there is a wide variety of resources that contain tagged images, such as ImageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the online search engine Freesound3 to obtain audio files. Freesound is a collaborative database released under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords. For each of the concepts in the evaluation datasets, we used the Freesound API to obtain samples encoded in the standard open source OGG format4. Because the database contains variable numbers of files, with varying durat</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In Proceedings of RANLP, pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="1679" citStr="Bruni et al., 2014" startWordPosition="232" endWordPosition="235">rd is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models. However, if the objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary study by Lopopolo and van Miltenburg (2015) found that it</context>
<context position="4594" citStr="Bruni et al., 2014" startWordPosition="675" endWordPosition="678">nceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring f</context>
<context position="6003" citStr="Bruni et al., 2014" startWordPosition="901" endWordPosition="904">wise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much reSimLex-999 score taxi-cab 0.92 plane-jet 0.81 horse-mare 0.83 sheep-lamb 0.84 bird-hawk 0.79 band-orchestra 0.71 music-melody 0.70 Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in comp</context>
<context position="7391" citStr="Bruni et al., 2014" startWordPosition="1116" endWordPosition="1119">allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual representations only improves performance for cert</context>
<context position="14977" citStr="Bruni et al., 2014" startWordPosition="2317" endWordPosition="2320">esentations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant audio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector representation quality (Bullinaria and Levy, 2007). We set k = 300, which equals the number of dimensions for the linguistic representations. 4.3 Multi-modal Fusion Strategies Since multi-modal semantics relies on two or more modalities, there are several ways of combining or fusing linguistic and perceptual cues (Bruni et al., 2014). When computing similarity scores, for instance, we can either jointly learn the representations; learn them independently, combine (e.g. concatenate) them and compute similarity scores; or learn them independently, compute similarity scores independently and combine the scores. We call these possibilities early, middle and late fusion, respectively, and evaluate multi-modal mod6http://bmcfee.github.io/librosa. els in each category. 4.3.1 Early Fusion A good example of early fusion is the recently introduced multi-modal skip-gram model (Lazaridou et al., 2015). This model behaves like a norma</context>
<context position="18161" citStr="Bruni et al. (2014)" startWordPosition="2834" endWordPosition="2837">or fewer when 10 are not available). 4.3.2 Middle and Late Fusion Whereas early fusion requires a joint training objective that takes into account both modalities, middle fusion allows for individual training objectives and independent training data. Similarity between two multi-modal representations is calculated as follows: sim(u, v) = g(f(ul,ua), f(vl,va)) where g is some similarity function, ul and vl are linguistic representations, and ua and va are the auditory representations. A typical formulation in multi-modal semantics for f(x, y) is α x1l(1−α)y, where 11 is concatenation (see e.g. Bruni et al. (2014) and Kiela and Bottou (2014)). Late fusion can be seen as the converse of middle fusion, in that the similarity function is computed first before the similarity scores are combined: sim(u, v) = h(g(ul, vl), g(ua, va)) where g is some similarity function and h is a way of combining similarities, in our case a weighted average: h(x, y) = 12(α x+(1−α)y); and we use g = x·y (cosine similarity). Since cosine similarity is the normalized dot-product, and the unimodal representations are themselves normalized, middle and late fusion are equivalent if α = 0.5, which we call MM. However, when α =� 0.5,</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artifical Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting Semantic Representations from Word Cooccurrence Statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="14692" citStr="Bullinaria and Levy, 2007" startWordPosition="2273" endWordPosition="2276">written in Python.6 After having obtained the descriptors, we cluster them using mini-batch k-means (Sculley, 2010) and quantize the descriptors into a “bag of audio words” (BoAW) (Foote, 1997) representation by comparing the MFCC descriptors to the cluster centroids. This gives us BoAW representations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant audio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector representation quality (Bullinaria and Levy, 2007). We set k = 300, which equals the number of dimensions for the linguistic representations. 4.3 Multi-modal Fusion Strategies Since multi-modal semantics relies on two or more modalities, there are several ways of combining or fusing linguistic and perceptual cues (Bruni et al., 2014). When computing similarity scores, for instance, we can either jointly learn the representations; learn them independently, combine (e.g. concatenate) them and compute similarity scores; or learn them independently, compute similarity scores independently and combine the scores. We call these possibilities early,</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting Semantic Representations from Word Cooccurrence Statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector Space Models of Lexical Meaning.</title>
<date>2015</date>
<booktitle>Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell,</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<location>Oxford.</location>
<contexts>
<context position="979" citStr="Clark, 2015" startWordPosition="128" endWordPosition="129">ine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and relatedness. We also evaluate cross-modal mappings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations. 1 Introduction Although distributional models (Turney and Pantel, 2010; Clark, 2015) have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting</context>
</contexts>
<marker>Clark, 2015</marker>
<rawString>Stephen Clark. 2015. Vector Space Models of Lexical Meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>248--255</pages>
<contexts>
<context position="11061" citStr="Deng et al., 2009" startWordPosition="1708" endWordPosition="1711"> beyond vision, our code and data have been made publicly available at http://www.cl.cam.ac.uk/˜dk427/audio.html. 2To avoid introducing another parameter, we set the number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. ever having heard a guitar; or map it to the appropriate place in linguistic space without ever having read about a guitar (having only heard it). 4 Approach One reason for using raw image data in multimodal models is that there is a wide variety of resources that contain tagged images, such as ImageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the online search engine Freesound3 to obtain audio files. Freesound is a collaborative database released under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords. For each of the concept</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of Computer Vision and Pattern Recognition (CVPR), pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eronen</author>
</authors>
<title>Musical instrument recognition using ICA-based transform of features and discriminatively trained HMMs.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh International Symposium on Signal Processing and Its Applications,</booktitle>
<volume>2</volume>
<pages>133--136</pages>
<contexts>
<context position="13422" citStr="Eronen, 2003" startWordPosition="2081" endWordPosition="2082">est performance on a variety of semantic similarity tasks (Baroni et al., 2014). 3http://www.freesound.org. 4http://www.vorbis.com. 5We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4.2 Auditory Representations A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O’Shaughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal processing, ranging from audio information retrieval, to speech and speaker recognition, and music analysis (Eronen, 2003). Such features are derived from the mel-frequency cepstrum representation of an audio fragment (Stevens et al., 1937). In MFCC, frequency bands are spaced along the mel scale, which has the advantage that it approximates human auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio frequencies into consideration, which makes it suitable for e.g. compression and recognition tasks, but also for our current objective of modelling auditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a p</context>
</contexts>
<marker>Eronen, 2003</marker>
<rawString>A. Eronen. 2003. Musical instrument recognition using ICA-based transform of features and discriminatively trained HMMs. In Proceedings of the Seventh International Symposium on Signal Processing and Its Applications, volume 2, pages 133–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="4547" citStr="Feng and Lapata, 2010" startWordPosition="666" endWordPosition="669">putational Linguistics. ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proceedings of NAACL, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fergus</author>
<author>Fei-Fei Li</author>
<author>Pietro Perona</author>
<author>Andrew Zisserman</author>
</authors>
<title>Learning object categories from Google’s image search.</title>
<date>2005</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1816--1823</pages>
<contexts>
<context position="11229" citStr="Fergus et al. (2005)" startWordPosition="1739" endWordPosition="1742">number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. ever having heard a guitar; or map it to the appropriate place in linguistic space without ever having read about a guitar (having only heard it). 4 Approach One reason for using raw image data in multimodal models is that there is a wide variety of resources that contain tagged images, such as ImageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the online search engine Freesound3 to obtain audio files. Freesound is a collaborative database released under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords. For each of the concepts in the evaluation datasets, we used the Freesound API to obtain samples encoded in the standard open source OGG format4. Because the database contains variable number</context>
</contexts>
<marker>Fergus, Li, Perona, Zisserman, 2005</marker>
<rawString>Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew Zisserman. 2005. Learning object categories from Google’s image search. In Proceedings of ICCV, pages 1816–1823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan T Foote</author>
</authors>
<title>Content-based retrieval of music and audio.</title>
<date>1997</date>
<booktitle>In Voice, Video, and Data Communications,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="14259" citStr="Foote, 1997" startWordPosition="2210" endWordPosition="2211">an auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio frequencies into consideration, which makes it suitable for e.g. compression and recognition tasks, but also for our current objective of modelling auditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a popular library for audio and music analysis written in Python.6 After having obtained the descriptors, we cluster them using mini-batch k-means (Sculley, 2010) and quantize the descriptors into a “bag of audio words” (BoAW) (Foote, 1997) representation by comparing the MFCC descriptors to the cluster centroids. This gives us BoAW representations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant audio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector representation quality (Bullinaria and Levy, 2007). We set k = 300, which equals the number of dimensions for the linguistic representations. 4.3 Multi-modal Fusion Strategies Since multi-modal semantics relies on two</context>
</contexts>
<marker>Foote, 1997</marker>
<rawString>Jonathan T Foote. 1997. Content-based retrieval of music and audio. In Voice, Video, and Data Communications, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Gregory S Corrado</author>
<author>Jonathon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A Deep Visual-Semantic Embedding Model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2121--2129</pages>
<contexts>
<context position="6765" citStr="Frome et al., 2013" startWordPosition="1017" endWordPosition="1020">nce. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much reSimLex-999 score taxi-cab 0.92 plane-jet 0.81 horse-mare 0.83 sheep-lamb 0.84 bird-hawk 0.79 band-orchestra 0.71 music-melody 0.70 Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) and the MEN test colle</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A Deep Visual-Semantic Embedding Model. In Proceedings of NIPS, pages 2121–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Gazzaniga</author>
<author>editor</author>
</authors>
<title>The Cognitive Neurosciences.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Gazzaniga, editor, 1995</marker>
<rawString>Michael S. Gazzaniga, editor. 1995. The Cognitive Neurosciences. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem.</title>
<date>1990</date>
<journal>Physica D,</journal>
<pages>42--335</pages>
<contexts>
<context position="1179" citStr="Harnad, 1990" startWordPosition="164" endWordPosition="165">dal mappings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations. 1 Introduction Although distributional models (Turney and Pantel, 2010; Clark, 2015) have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual moda</context>
<context position="4171" citStr="Harnad, 1990" startWordPosition="606" endWordPosition="607">azzaniga, 1995). While research in AI has made great progress in understanding the first and last of these, understanding the middle level is still more of an open problem: how is it that per2461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2461–2470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D, 42:335–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
</authors>
<title>Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>255--265</pages>
<contexts>
<context position="4506" citStr="Hill and Korhonen, 2014" startWordPosition="658" endWordPosition="661">1 September 2015. c�2015 Association for Computational Linguistics. ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the</context>
</contexts>
<marker>Hill, Korhonen, 2014</marker>
<rawString>Felix Hill and Anna Korhonen. 2014. Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean. In Proceedings of EMNLP, pages 255–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>SimLex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="7342" citStr="Hill et al., 2014" startWordPosition="1107" endWordPosition="1110">(Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual r</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>L´eon Bottou</author>
</authors>
<title>Learning image embeddings using convolutional neural networks for improved multi-modal semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>36--45</pages>
<contexts>
<context position="5261" citStr="Kiela and Bottou, 2014" startWordPosition="786" endWordPosition="789">popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal mo</context>
<context position="18189" citStr="Kiela and Bottou (2014)" startWordPosition="2839" endWordPosition="2842"> available). 4.3.2 Middle and Late Fusion Whereas early fusion requires a joint training objective that takes into account both modalities, middle fusion allows for individual training objectives and independent training data. Similarity between two multi-modal representations is calculated as follows: sim(u, v) = g(f(ul,ua), f(vl,va)) where g is some similarity function, ul and vl are linguistic representations, and ua and va are the auditory representations. A typical formulation in multi-modal semantics for f(x, y) is α x1l(1−α)y, where 11 is concatenation (see e.g. Bruni et al. (2014) and Kiela and Bottou (2014)). Late fusion can be seen as the converse of middle fusion, in that the similarity function is computed first before the similarity scores are combined: sim(u, v) = h(g(ul, vl), g(ua, va)) where g is some similarity function and h is a way of combining similarities, in our case a weighted average: h(x, y) = 12(α x+(1−α)y); and we use g = x·y (cosine similarity). Since cosine similarity is the normalized dot-product, and the unimodal representations are themselves normalized, middle and late fusion are equivalent if α = 0.5, which we call MM. However, when α =� 0.5, we distinguish between the </context>
</contexts>
<marker>Kiela, Bottou, 2014</marker>
<rawString>Douwe Kiela and L´eon Bottou. 2014. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In Proceedings of EMNLP, pages 36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="8153" citStr="Kiela et al., 2014" startWordPosition="1231" endWordPosition="1234">genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of comparisons such as guitar-piano, the auditory modalMEN automobile-car rain-storm cat-feline jazz-musician bird-eagle highway-traffic guitar-piano score 1.00 0.98 0.96 0.88 0.88 0.88 0.86 2462 Dataset MEN AMEN SLex ASLex Linguistic 3000 258 999 296 Auditory 2590 233 534 216 Table 2: Number of concept pairs for which representations are available in each modality. ity is certainly meaningful, whereas in the case of democracy-anarchism it is probably less so. Therefore, we had two graduate students annotate the datas</context>
<context position="30533" citStr="Kiela et al., 2014" startWordPosition="4804" endWordPosition="4807">h instruments closest to cluster centroid for linguistic and multimodal. auditory modality is better suited for other evaluations, but we have chosen to follow standard evaluations in multi-modal semantics to allow for a direct comparison. In future work, it would be interesting to investigate different sampling strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the middle and late fusion models, e.g. using the “audio dispersion” of a word to determine how much auditory input should be included in the multi-modal representation (Kiela et al., 2014). Another interesting possibility is to improve auditory representations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics. Lastly, now that the perceptual modalities of vision, audio and even olfaction (Kiela et al., 2015) have been investigated in the context of distributional semantics, the logical next step for future work is to explore different fusion strategies for multi-modal models that combine various sources of perceptual input into a single grounded model. Acknowledgments DK is supported by EPSR</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings of ACL, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Luana Bulat</author>
<author>Stephen Clark</author>
</authors>
<title>Grounding semantics in olfactory perception.</title>
<date>2015</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>231--236</pages>
<location>Beijing, China,</location>
<marker>Kiela, Bulat, Clark, 2015</marker>
<rawString>Douwe Kiela, Luana Bulat, and Stephen Clark. 2015. Grounding semantics in olfactory perception. In Proceedings ofACL, pages 231–236, Beijing, China, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1403--1414</pages>
<contexts>
<context position="6300" citStr="Lazaridou et al., 2014" startWordPosition="945" endWordPosition="948">. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much reSimLex-999 score taxi-cab 0.92 plane-jet 0.81 horse-mare 0.83 sheep-lamb 0.84 bird-hawk 0.79 band-orchestra 0.71 music-melody 0.70 Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of </context>
<context position="9879" citStr="Lazaridou et al. (2014)" startWordPosition="1513" endWordPosition="1516">999 dataset SLex and its auditory-relevant subset ASLex. Due to the nature of the auditory data sources, it is not possible to build auditory representations for all concepts in the test sets. Hence, unless stated otherwise, we report results for the covered subsets (using the same subsets when comparing across modalities, to ensure a fair comparison). Table 2 shows how much of the test sets are covered for each modality.1 3.2 Cross-modal Semantics In addition to evaluating our models on the MEN and SimLex tasks, we evaluate on the crossmodal task of zero-shot learning. In the case of vision, Lazaridou et al. (2014) studied the possibility of predicting from “we found a cute, hairy wampimuk sleeping behind the tree” that a “wampimuk” will probably look like a small furry animal, even though a wampimuk has never been seen before. We evaluate zero-shot learning, using partial least squares regression (PLSR) to obtain cross-modal mappings from the linguistic to auditory space and vice versa.2 Thus, given a linguistic representation for e.g. guitar, the task is to map it to the appropriate place in auditory space without 1To facilitate further work in multi-modal semantics beyond vision, our code and data ha</context>
<context position="22029" citStr="Lazaridou et al. (2014)" startWordPosition="3454" endWordPosition="3457">uditory 0.73 6.71 22.16 37.32 Table 4: Cross-modal zero-shot learning accuracy. space into the other. Zero-shot performance is evaluated using the average percentage correct at N (P@N), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. Results are shown in Table 4, with the chance baseline obtained by randomly ranking a concept’s nearest neighbors. Insofar as it is possible to make a direct comparison with linguistic-visual zero-shot learning (which uses entirely different data), it appears that the current task may be more difficult: Lazaridou et al. (2014) report a P@1 of 2.4 and P@20 of 33.0 for their linguistic-visual model. 5.3 Qualitative Analysis We also performed a small qualitative analysis of the BoAW representations for the words in MEN and SLex. As Table 5 shows, the nearest neighbors are remarkably semantically coherent. For example, the model groups together sounds produced by machines, or by water. It even finds that dinner, meal, lunch and breakfast are closely related. In contrast, nearest neighbors for the linguistic model tend to be of a more abstract nature: where we find mouth and throat as auditory neighbors for language, th</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL, pages 1403–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
</authors>
<title>Nghia The Pham, and</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Lazaridou, 2015</marker>
<rawString>Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. 2015. Combining language and vision with a multimodal skipgram model. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="4573" citStr="Leong and Mihalcea, 2011" startWordPosition="670" endWordPosition="674"> ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) appro</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In Proceedings of IJCNLP, pages 1403–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopopolo</author>
<author>E van Miltenburg</author>
</authors>
<title>Soundbased distributional models.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics (IWCS</booktitle>
<marker>Lopopolo, van Miltenburg, 2015</marker>
<rawString>A. Lopopolo and E. van Miltenburg. 2015. Soundbased distributional models. In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max M Louwerse</author>
</authors>
<title>Symbol interdependency in symbolic and embodied cognition.</title>
<date>2008</date>
<journal>Topics in Cognitive Science,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="1302" citStr="Louwerse, 2008" startWordPosition="185" endWordPosition="186">ate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations. 1 Introduction Although distributional models (Turney and Pantel, 2010; Clark, 2015) have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models. However, if the objective is to ground semantic representations in perceptual information, why </context>
<context position="4188" citStr="Louwerse, 2008" startWordPosition="608" endWordPosition="609">). While research in AI has made great progress in understanding the first and last of these, understanding the middle level is still more of an open problem: how is it that per2461 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2461–2470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid,</context>
</contexts>
<marker>Louwerse, 2008</marker>
<rawString>Max M. Louwerse. 2008. Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science, 59(1):617–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="4886" citStr="Lowe, 2004" startWordPosition="730" endWordPosition="731">at try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal d</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="12593" citStr="Mikolov et al. (2013)" startWordPosition="1965" endWordPosition="1968">ile. The Freesound API allows for various degrees of keyword matching: we opted for the strictest keyword matching, in that the audio file needs to have been purposely tagged with the given word (the alternative includes searching the text description for matching keywords). For example, if we are searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label “car”. 4.1 Linguistic Representations For the linguistic representations we use the continuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013). Specifically, we trained 300-dimensional vector representations trained on a dump of the English Wikipedia plus newswire (8 billion words in total).5 These types of representations have been found to yield the highest performance on a variety of semantic similarity tasks (Baroni et al., 2014). 3http://www.freesound.org. 4http://www.vorbis.com. 5We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4.2 Auditory Representations A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of ICLR, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D O’Shaughnessy</author>
</authors>
<title>Speech communication: human and machine. Addison-Wesley series in electrical engineering: digital signal processing.</title>
<date>1987</date>
<publisher>Universities Press (India) Pvt. Limited.</publisher>
<marker>O’Shaughnessy, 1987</marker>
<rawString>D. O’Shaughnessy. 1987. Speech communication: human and machine. Addison-Wesley series in electrical engineering: digital signal processing. Universities Press (India) Pvt. Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1146--1157</pages>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of EMNLP, pages 1146–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<contexts>
<context position="27590" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="4356" endWordPosition="4360">. We used Wikipedia to collect a total of 52 instruments and divided them into 5 classes: brass, percussion, piano-based, string and woodwind instruments. For each of the instruments, we collected as many audio files from FreeSound as possible, and used the MM-MIDDLE model with parameter settings that yielded good results in the previous experiments (k = 300 and α = 0.6). We then performed k-means clustering with five cluster centroids and compared results between auditory, linguistic and multi-modal, evaluating the clustering quality using the standard V-measure clustering evaluation metric (Rosenberg and Hirschberg, 2007). This is an interesting problem because instrument classes are determined somewhat by conven2467 Model Auditory Linguistic MM-MIDDLE V-measure 0.39 0.47 0.54 Linguistic 1 baritone 2 lute, zither, xylophone, lyre, cymbals 3 piano, trombone, clarinet, cello, violin 4 castanets, tambourine, claves, maracas 5 trumpet, horn, bugle, cowbell, carillon Multi-modal drum, claves, bongo, bass, conga xylophone, glockenspiel, tambourine, cymbals 1 2 cello, piano, clarinet, trombone, violin, 3 4 Figure 4: Performance of uni-modal auditory representations on the four datasets when varying the maximum durati</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sculley</author>
</authors>
<title>Web-scale k-means clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>1177--1178</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14181" citStr="Sculley, 2010" startWordPosition="2197" endWordPosition="2198">are spaced along the mel scale, which has the advantage that it approximates human auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio frequencies into consideration, which makes it suitable for e.g. compression and recognition tasks, but also for our current objective of modelling auditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a popular library for audio and music analysis written in Python.6 After having obtained the descriptors, we cluster them using mini-batch k-means (Sculley, 2010) and quantize the descriptors into a “bag of audio words” (BoAW) (Foote, 1997) representation by comparing the MFCC descriptors to the cluster centroids. This gives us BoAW representations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant audio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector representation quality (Bullinaria and Levy, 2007). We set k = 300, which equals the number of dimensions for the linguistic representation</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>David Sculley. 2010. Web-scale k-means clustering. In Proceedings of WWW, pages 1177–1178. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1423--1433</pages>
<contexts>
<context position="1623" citStr="Silberer and Lapata, 2012" startWordPosition="222" endWordPosition="225"> for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models. However, if the objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary s</context>
<context position="4445" citStr="Silberer and Lapata, 2012" startWordPosition="648" endWordPosition="651">l Language Processing, pages 2461–2470, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ceptual input leads to conceptual representations that can be processed and reasoned with? A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of EMNLP, pages 1423–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>721--732</pages>
<contexts>
<context position="5549" citStr="Silberer and Lapata, 2014" startWordPosition="829" endWordPosition="832">ently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference</context>
<context position="7181" citStr="Silberer and Lapata, 2014" startWordPosition="1083" endWordPosition="1086">re. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of ACL, pages 721–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1470--1477</pages>
<contexts>
<context position="2673" citStr="Sivic and Zisserman, 2003" startWordPosition="385" endWordPosition="388"> sound, pitch and timbre. To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary study by Lopopolo and van Miltenburg (2015) found that it is possible to derive uni-modal semantic representations from sound data. Here, we explore taking multi-modal semantics beyond its current reliance on image data and experiment with grounding semantic representations in the auditory perceptual modality. Multi-modal models that rely on raw image data have typically used “bag of visual words” (BoVW) representations (Sivic and Zisserman, 2003). We follow a similar approach for the auditory modality and construct bag of audio words (BoAW) representations. Following previous work in multi-modal semantics, we evaluate these models on measuring conceptual similarity and relatedness, and inducing cross-modal mappings between modalities to perform zeroshot learning. In addition, we evaluate on an unsupervised musical instrument clustering task. Our findings indicate that multi-modal representations enriched with auditory information perform well on relatedness and similarity tasks, particularly on words that have auditory assocations. To</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pages 1470– 1477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of ACL,</journal>
<pages>2--207</pages>
<contexts>
<context position="6744" citStr="Socher et al., 2014" startWordPosition="1013" endWordPosition="1016">the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much reSimLex-999 score taxi-cab 0.92 plane-jet 0.81 horse-mare 0.83 sheep-lamb 0.84 bird-hawk 0.79 band-orchestra 0.71 music-melody 0.70 Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) a</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of ACL, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Smith Stevens</author>
<author>John Volkmann</author>
<author>Edwin B Newman</author>
</authors>
<title>A scale for the measurement of the psychological magnitude pitch.</title>
<date>1937</date>
<journal>Journal of the Acoustical Society ofAmerica,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="13540" citStr="Stevens et al., 1937" startWordPosition="2096" endWordPosition="2099">ttp://www.vorbis.com. 5We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4.2 Auditory Representations A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O’Shaughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal processing, ranging from audio information retrieval, to speech and speaker recognition, and music analysis (Eronen, 2003). Such features are derived from the mel-frequency cepstrum representation of an audio fragment (Stevens et al., 1937). In MFCC, frequency bands are spaced along the mel scale, which has the advantage that it approximates human auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio frequencies into consideration, which makes it suitable for e.g. compression and recognition tasks, but also for our current objective of modelling auditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a popular library for audio and music analysis written in Python.6 After having obtained the descriptors, we cluster them</context>
</contexts>
<marker>Stevens, Volkmann, Newman, 1937</marker>
<rawString>Stanley Smith Stevens, John Volkmann, and Edwin B. Newman. 1937. A scale for the measurement of the psychological magnitude pitch. Journal of the Acoustical Society ofAmerica, 8(3):185—190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Synnaeve</author>
<author>Maarten Versteegh</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Learning words from images and speech.</title>
<date>2014</date>
<booktitle>In NIPS Workshop on Learning Semantics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="5605" citStr="Synnaeve et al., 2014" startWordPosition="838" endWordPosition="841">d clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textu</context>
</contexts>
<marker>Synnaeve, Versteegh, Dupoux, 2014</marker>
<rawString>Gabriel Synnaeve, Maarten Versteegh, and Emmanuel Dupoux. 2014. Learning words from images and speech. In NIPS Workshop on Learning Semantics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="965" citStr="Turney and Pantel, 2010" startWordPosition="123" endWordPosition="127">ut. In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and relatedness. We also evaluate cross-modal mappings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations. 1 Introduction Although distributional models (Turney and Pantel, 2010; Clark, 2015) have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: vector space models of semantics. Journal of Artifical Intelligence Research, 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Luis Von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 319–326. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
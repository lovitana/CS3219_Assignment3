<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018033">
<title confidence="0.990305">
Learning to Identify the Best Contexts for Knowledge-based WSD
</title>
<author confidence="0.990257">
Evgenia Wasserman-Pritsker William W. Cohen Einat Minkov
</author>
<affiliation confidence="0.993948">
UniversityofHaifa Carnegie Mellon University University of Haifa
</affiliation>
<address confidence="0.95165">
Haifa, Israel, 31905 Pittsburgh, PA 15213 Haifa, Israel, 31905
</address>
<email confidence="0.999305">
evgeniaw@is.haifa.ac.il wcohen@cs.cmu.edu einatm@is.haifa.ac.il
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998745">
We outline a learning framework that
aims at identifying useful contextual cues
for knowledge-based word sense disam-
biguation. The usefulness of individual
context words is evaluated based on di-
verse lexico- statistical and syntactic in-
formation, as well as simple word dis-
tance. Experiments using two different
knowledge-based methods and benchmark
datasets show significant improvements
due to context modeling, beating the con-
ventional window-based approach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999854534482759">
Word sense disambiguation (WSD) is a key task
of natural language processing. Unsupervised
knowledge-based approaches to WSD (Navigli,
2009) make use of available lexical resources
rather than rely on costly annotated data. Sense
inference in this setting involves finding the word
sense that agrees most with the specified con-
text according to the information encoded in the
knowledge base (KB). The popular Lesk (1986)
method, for example, seeks to maximize word
overlap between the dictionary glosses associated
with the context words, and the glosses of can-
didate word senses. Similar methods are used in
named entity disambiguation and linking to a KB
(Hoffart et al., 2011).
Despite the sophistication of inference models
developed, little attention has been given so far
to context modeling for knowledge-based WSD.
Context is represented by bag-of-words, where
typically all context words are assigned equal im-
portance (Navigli, 2009; Ling et al., 2014). How-
ever, every simple definition of context will in-
clude some unrelated or uninformative context
words. Consider this usage of the word church:
&amp;quot;An ancient stone church stands amid the fields
, the sound of bells cascading from its tower&amp;quot;.
Known senses for &apos;church&apos; according to Word-
net 3.0 (Fellbaum, 1998) correspond to a group of
people, service, or a building. The latter sense is
intended in this case, as one may conclude from
the context words &apos;stone&apos;, &apos;stands&apos; or &apos;tower&apos;. We
wish to focus on such meaningful cues and avoid
the modeling of uninformative words (&apos;ancient&apos;,
`sound&apos;).
In this work, a learning framework is proposed
that is aimed at identifying contextual cues that are
predictive of the target word&apos;s sense. The use-
fulness of a candidate context word for the dis-
ambiguation of the target word is evaluated based
on syntactic and lexico-statistical information, as
well as simple word distance. Indirect supervi-
sion is provided using noisy example labels in-
duced automatically. Importantly, explicit lexical
information is not encoded—the prediction model
can thus be applied in settings where no sense-
tagged examples are available of the target word
type (see also (Szarvas et al., 2013)). Having as-
sessed the usefulness of available context words
given the learned model, we consider only the top
scoring context words in performing WSD.
We believe this work to be the first to perform
learning-based context selection for knowledge-
based sense disambiguation. Empirical evaluation
using two representative knowledge-based WSD
methods and different benchmark datasets indi-
cates on consistent improvements in performance
due to context selection using the proposed ap-
proach.
</bodyText>
<page confidence="0.996175">
1662
</page>
<sectionHeader confidence="0.8445875" genericHeader="method">
2 Learned context selection models
(LCS)
</sectionHeader>
<bodyText confidence="0.999956727272727">
We first define the WSD task. Given a word men-
tion w and available context Ctx, it is required to
infer the intended sense s* E S(w), where S(w) is
the set of known senses of w. Ctx may be a sen-
tence, a paragraph, or a window over words that
contain w.
Knowledge-based methods seek to maximize
some measure of agreement, or similarity, be-
tween candidate word senses and a given context.
We denote this as Sim(), where the sense infer-
ence procedure is defined as follows:
</bodyText>
<equation confidence="0.999503">
.§(w) = arg max Sim(s, Ctx) (1)
sEs(w)
</equation>
<bodyText confidence="0.9961275">
Typically, Ctx is represented as a bag-of-words,
and the similarity score Sim(s, Ctx) is additive,
i.e., it may be computed as a summation over the
similarity scores between sense s and the individ-
ual context words c3 E Ctx, using the general for-
mula:
</bodyText>
<equation confidence="0.98539">
Ctx) = E weight(c3)Sim(s, c3) (2)
c, Ectx
</equation>
<bodyText confidence="0.999038625">
According to this view, each context word serves
as a sense disambiguation &apos;expert&apos;. Context
words are usually assigned uniform weight, i.e.,
weight(c3) = 1. Alternatively, varying weights
may reflect the reliability, or relevancy, of con-
text word c3 in disambiguating target word w; ide-
ally, unuseful context information would be down-
weighted or discarded.
</bodyText>
<subsectionHeader confidence="0.849037">
2.1 Learning
</subsectionHeader>
<bodyText confidence="0.999930066666667">
Our goal is to learn models that assess whether
a candidate context word c3 serves as a reliable
&apos;expert&apos; in predicting the sense of target word
w. We propose a distantly supervised learning
scheme. Given sense-tagged instances of the form
(Ctx(wz)), we derive a dataset of context-target
word pairs (w,, ci3), cj E Ctx(wi). Defining
whether context work ci3 is useful, or reliable,
with respect to the disambiguation of wi is not
trivial, however. In particular, words that are per-
ceived as relevant according to human judgment
may not necessarily yield the correct prediction
using the inference algorithm. We consider a con-
text word to be reliable if it yields a correct sense
prediction of the target word, as follows:
</bodyText>
<equation confidence="0.9857845">
Y(
1, if arg maxs Sim(s, cij) = s*(wi)
wi, cij) =
(3)
</equation>
<bodyText confidence="0.999946318181818">
As the similarity measures Sim(), as well as the
reference knowledge base, are imperfect, the la-
bels assigned in this fashion are expected to be
noisy.
A context and target word pair is represented as
a feature vector, as described below. Importantly,
we avoid the representation of explicit lexical in-
formation, so that the learned models are applica-
ble to word pairs of arbitrary word types.
Given a new instance at test time, the learned
model is used to score the individual context
words. One can then assign respective non-
uniform weights to the context words (Eq. 2).
Here, we take a context selection approach—a rank-
ing is induced over the context words based on the
predicted scores, and only the top ranked context
words are modelled in the disambiguation process;
that is, the selected context words are assigned
weight 1.0, and the weight of the remaining con-
text words is set to zero. As discussed in Sec.3,
this design choice was found to give preferable re-
sults in preliminary experiments.
</bodyText>
<subsectionHeader confidence="0.995783">
2.2 Feature Types
</subsectionHeader>
<bodyText confidence="0.976822145833334">
Various aspects may be modeled as features in
this framework, describing properties of the con-
text word c3, as well as the relationship between
the target-context word pair (w, c3). In addition
to simple word distance, we encode the following
syntactic and lexico-statistical information.
Syntactic features. Word distance is further
assessed in syntactic terms, denoting the length
of the shortest dependency path linking the word
pair, as well as the length of the shortest connect-
ing path in a constituency parse tree (Swanson
and Gordon, 2006; Huang and Lu, 2011). It may
be useful to further encode information about the
edge types that comprise the connecting path, as
some dependency relations indicate more salient
semantic relatedness than others (Pado and Lap-
ata, 2007; Minkov and Cohen, 2013). In this work,
if the target and context words are directly con-
nected in the dependency graph, we include a fea-
ture indicating the label of the edge. The part-of-
speech tag of the context word may provide an-
other contextual cue (Yarowsky, 1993); dedicated
features indicate whether c3 is tagged as noun,
verb, adjective or adverb. We used the Stanford
parser (de Marneffe et al., 2006) in our experi-
ments.
otherwise
1663
Lexico-statistical information. We use the
pointwise mutual information (PMI) measure
(Turney, 2001) to assess the semantic relatedness
between the context—target word pair. In general,
we expect context words that are topically related
to the target word to be useful for its disambigua-
tion. To compute PMI, we obtained word frequen-
cies from the large ukWaC corpus (Ferraresi et
al., 2008), considering word co-occurrences over
a window of five words. It has been indicated
that highly frequent words are generally less topi-
cal, where this aspect is not fully captured by PMI
(Han et al., 2013). We therefore model as com-
plimentary information the inverse document fre-
quency (Salton and McGill, 1983) of c3, also com-
puted using ukWaC Finally, a context word is of-
ten ambiguous by itself, where low polysemy is
correlated with topic-specificity (Han et al., 2013).
We represent the number of known senses of the
context word c3 based on WordNet.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999114655172414">
We consider two WSD methods representative of
prevalent knowledge-based approaches, compar-
ing against previously published results. The pop-
ular Lesk approach (1986) mentioned before com-
putes Sim(s, c3) in terms of word overlap be-
tween the glosses of the senses of c3 and the
gloss of s. There exist multiple variants of the
Lesk algorithm (Kilgarriff and Rosenzweig, 2000;
Banerjee and Pedersen, 2003; Ponzetto and Nav-
igli, 2010). We experiment with Gloss vectors
(GV) (Patwardhan and Pedersen, 2006). This
method enriches WordNet glosses with glosses
of hypernyms and other related senses, as well
as with co-occurring words derived from raw
text. GV scores were obtained using the Word-
Net: :Similarity package (Pedersen et al., 2004).
Graph-based methods are also commonly
used for sense disambiguation (Mihalcea, 2005;
Hughes and Ramage, 2007). If the KB is rep-
resented as a graph, various metrics can be ap-
plied that reflect structural similarity between
word senses represented as graph nodes. We con-
sider the Personalized PageRank (PPR) algorithm,
which has been shown to yield state-of-the-art
WSD performance (Agirre and Soroa, 2009). Ac-
cording to the linearity theorem (Jeh and Widom,
2003), PPR scores can be computed for each of the
context words separately, and then aggregated (Eq.
2). In this case, Sim(s, c3) equals the PPR score
</bodyText>
<note confidence="0.5234475">
Koeling etal.
SemevaP07
</note>
<tableCaption confidence="0.999235">
Table 1: The experimental datasets: statistics
</tableCaption>
<bodyText confidence="0.9992578">
attributed to the node denoting sense s, having the
graph walk initiated at a uniform distribution over
the various senses of c3. PPR scores were gener-
ated using the UKB software (Agirre and Soroa,
2009).1-
</bodyText>
<subsectionHeader confidence="0.940189">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999962913043478">
We experiment with two benchmark datasets. The
lexical sample due to Koeling et al. (2005) in-
cludes annotated instances of 41 selected nouns.
About 300 example sentences are available per
noun, retrieved evenly from three sources: the
domain-specific sports and finance sections of
Reuters corpus, and the general British National
Corpus (BNC). The second dataset consists of all
noun examples from the SemEval-2007 English
lexical sample task (Pradhan et al., 2007), created
from another corpus—the WSJ Treebank.
The two datasets were transformed into target-
context word pairs. For every word pair (w, c3),
the scores Sim(s, s E S(w), were generated
using GV and PPR and WordNet 3.0 as the refer-
ence knowledge base. A context-target word pair
was labeled as a positive example if it yielded a
correct sense prediction, or as negative otherwise
(Eq. 3). Table 1 details statistics of the original
and respective word pair datasets, including the ra-
tio of context words labeled as positive examples—
as shown, this `pairwise accuracy&apos; is low, reaching
up to 0.35.
</bodyText>
<subsectionHeader confidence="0.99819">
3.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999958833333333">
We experimented with several classification
paradigms using the Weka learning suite (Hall et
al., 2009). Learning had to be robust to label noise.
We report results using Naive Bayes, due to both
its good performance and efficiency. Following
preliminary experiments, we adopted a context se-
lection approach—the learned model is used to rank
the available context words, where the top ranked
words, obtained by applying ratio r, are selected
as context. We tune r using training examples.
The reported performance uses rough values of
r = 0.5 for the Koeling et al. examples, which
</bodyText>
<footnote confidence="0.9244565">
1 http://ixa2. si.ehu.es/ukbi; we used the bin file
wn30+gloss, and the PPR_w2w graph walk variant.
</footnote>
<table confidence="0.9886356">
Word Target Context Pairwise acc.
types words words (PPR/ GV)
41 9.6K 121K 0.32/ 0.31
35 16K 390K 0.35/ 0.33
1664
Koeling el al. SemEval&apos;07
GV PPR GV PPR
Uniform .389 .494 .370 .432
LCS:CW .410+6% .511+3% .469+27% .494+14%
LCS:CD .411+6% .507+17%
</table>
<tableCaption confidence="0.999228">
Table 2: Main results: recall performance
</tableCaption>
<bodyText confidence="0.992702666666667">
include individual sentences, and r = 0.2 for Se-
mEval`07, where parahraphs of a few sentences
are provided as context.
</bodyText>
<subsectionHeader confidence="0.843876">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999946783783784">
Table 2 shows the results of applying context se-
lection for each of the dataset and methods. As in
previous works, performance is reported in terms
of recall, defined as the ratio of correct sense pre-
dictions out of total number of target word men-
tions.2 To avoid over fitting, we performed cross
word evaluation, predicting contexts for all in-
stances of each word type with a model trained us-
ing the other word types (LCS:CW). Concretely,
the Koeling et al. dataset was split into 41 bins,
according to the target word type. For each word
type, we generated a model using the examples
of the remaining (in this case, 40) word types.
This cross word evaluation procedure was applied
to both datasets. We further report the results
of cross dataset experiments (LCS:CD), in which
one dataset is used for training and the other for
evaluation. As baseline, we use all of the available
context words, weighting them uniformly (&amp;quot;uni-
form&amp;quot; in the table).
As shown, LCS yields substantial improve-
ments over the &amp;quot;uniform&amp;quot; baseline. The improve-
ment rate for each experiment is displayed in su-
perscript. Recall increased at high rates on the
SemEval dataset. This dataset is skewed, and
much of these gains are attributed to large in-
crease in recall for two word types, covering 27%
of the examples. Improvements on the balanced
dataset due to Koeling et al. were more mod-
est, yet significant. Interestingly, improvement
rates are higher using GV than PPR; we conjecture
that PPR predictions are biased towards highly-
connected graph nodes, being less sensitive to the
local context defined. Remarkably, the results us-
ing cross-dataset training are comparable to or ex-
ceed the within-dataset CW results, showing gen-
erality and robustness of the proposed approach.
</bodyText>
<footnote confidence="0.9211795">
2Since predictions are generated for all examples, recall
equals in this case to precision, and accuracy.
</footnote>
<table confidence="0.9996609">
GV PPR
Uniform .389 .494
Lexico-stat. features:
PMI only •397+21°&apos;o .502+1.6%
+IDF .403+36 .503+1.8%
+No. of senses .406+4 4%
+Syntactic features:
.411+57% .510+32%
+Word distance
.410+54% .511+34%
</table>
<tableCaption confidence="0.9906365">
Table 3: Feature ablation results using LCS:CW
and the Koeling et al. dataset
</tableCaption>
<table confidence="0.999932">
BNC Sports Finance
Uniform PPR .491 .437 .554
LCS:CW .502+2% .464+6% .565+2%
LCS:CD .501+2% .459+5% .570+3%
Uniform GV .382 .361 .423
LCS:CW .401+5% .377+4% .450+6%
LCS:CD .400+5% .386+7% .448+6%
AL&amp;S&apos;09 .438 .356 .469
H&amp;L&apos;ll .397
P&amp;N&apos; 10 .420 .478
R&amp;M&apos;12 .465 .493
</table>
<tableCaption confidence="0.977053">
Table 4: Detailed results on the Koeling et al.
</tableCaption>
<bodyText confidence="0.976645142857143">
dataset
Table 3 further shows the results of an abla-
tion study, assessing the contribution of the var-
ious feature types by adding them incrementally.
We found the contribution of the lexico-statistical
features to be the largest. In particular, model-
ing PMI yielded the best performance when used
as a standalone feature. This means that context
words that are topically related to the target word
are especially useful for knowledge-based WSD.
Modeling IDF information led to further gains in
performance. As discussed before, the two mea-
sures are complimentary, as common words are
generally less topical. Representing the number
of senses of the context words yielded further im-
provements. Overall, this combination of word
features accounted for the majority of the total
gains achieved. The syntactic features had a lesser
impact, yet improved results further, mainly using
the GV method Finally, simple word distance was
found to have little impact; similar behavior was
observed elsewhere (Hoffart et al., 2011).
In another set of experiments, we evaluated and
found LCS to be robust with respect to the ratio r—
while performance using LCS varied, it improved
over the baseline across the range 0 &lt; r &lt; 1.
In contrast, selecting equal-sized sets of context
words using the window approach was found to
</bodyText>
<page confidence="0.978991">
1665
</page>
<bodyText confidence="0.999698214285714">
hurt performance.
Finally, we compare our results against previous
works. Our approach outperforms the results ob-
tained by unsupervised systems on the noun por-
tion of the SemEval`07 dataset (Patwardhan et al.,
2007; Mohammad et al., 2007), achieving recall
of .507 vs. .497 (a higher result obtained by Mo-
hammad et al). Table 4 presents LCS results sepa-
rately for each of the source domains of the Koel-
ing dataset for comparison purposes. Previous re-
sults using PPR and uniform context weighting
reported by Agirre et al. (2009) (AL&amp;S &apos;09) are
substantially lower than our baseline; we mainly
attribute this to the different version of WordNet
used.3 Huang and Lu (2011) proposed a manually-
tuned syntax-based context selection and weight-
ing formula. They applied it in combination with
the GV method, reporting improvement on BNC
sentences only. Our baseline result using GV
was lower (.382 vs. .390), however LCS yielded
better final performance (.401 vs. .397). Com-
pared with their work, we use learning and model
richer types of evidence; with PPR and LCS, we
report best results on the BNC sentences. Ta-
ble 4 details also recent results obtained for the
BNC and Sports portions of the dataset. Ponzetto
and Navigli (2010) (P&amp;N`10) enriched the Word-
Net graph with additional relations projected onto
the graph from Wikipedia; the table reports their
best results using a graph centrality measure (Nav-
igli and Lapata, 2010). Raviv and Markovitch
(2012) (R&amp;M`12) reported state-of-the-art perfor-
mance in the specialized domains using Wikipedia
as the reference knowledge base. Each individ-
ual context word is represented in their work as
a weighted vector of Wikipedia concepts, where
sense inference is performed by maximizing co-
sine similarity between the centroid of the context
vectors and a vector representation of each word
sense. Our results using PPR and LCS exceed
or roughly match their results without using the
Wikipedia resource.
</bodyText>
<sectionHeader confidence="0.997175" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999485">
We presented a learning framework that identifies
useful contextual cues for knowledge-based sense
disambiguation. The generated models are non-
lexicalized, and are therefore applicable to new
</bodyText>
<footnote confidence="0.966053666666667">
3They used WordNet 1.7, while we use version 3.0. Large
performance gaps due to different versions of WordNet were
reported elsewhere (Agirre and Soroa, 2009).
</footnote>
<bodyText confidence="0.998999785714286">
word types. Existing approaches pay little at-
tention to context selection, or perform simplistic
context modeling, whereas the proposed approach
effectively consolidates diverse types of evidence.
In the future, we are interested in representing ad-
ditional word relatedness measures in this frame-
work, such as embedding-based word similarity
(Wang et al., 2015). We are further interested in
creating specialized models that fit different word
classes, e.g., of particular part-of-speech. In gen-
eral, the proposed approach may prove beneficial
for additional tasks that model word meaning in
context, such as lexical substitution and sense in-
duction.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999731">
We wish to thank Ido Dagan, Shuly Wintner and
the anonymous reviewers for their useful com-
ments. This work was supported by BSF under
grant 2010090.
</bodyText>
<sectionHeader confidence="0.997351" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984422344827586">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of EACL.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of HCAL
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of HCAL
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the WAC4 Workshop at LREC.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploratory Newsletter, 11(1):10-18.
Lushan Han, Tim Finin, Paul McNamee, Anupam
Joshi, and Yelena Yesha. 2013. Improving word
similarity by augmenting PMI with estimates of
word polysemy. IEEE Transactions on Knowledge
and Data Engineering (TKDE), 25(6).
</reference>
<page confidence="0.927333">
1666
</page>
<reference confidence="0.999187408163266">
Johannes Hoffart, Mohamed A. Yosef, Ilaria Bor-
dino, Hagen Furstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of EMNLP.
Heyan Huang and Wenpeng Lu. 2011. Knowledge-
based word sense disambiguation with feature words
based on dependency relation and syntax tree. In-
ternational Journal of Advancements in Computing
Technology, 3(8).
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
Proceedings of EMNLP.
Glen Jeh and Jennifer Widom. 2003. Scaling person-
alized web search. In Proceedings of the 12th inter-
national conference on World Wide Web (WWW).
Adam Kilgarriff and Joseph Rosenzweig. 2000. En-
glish senseval:report and results. In 2nd Interna-
tional Conference on Language Resourcesand Eval-
uation (LREC).
Rob Koeling, Diana McCarthy, and John Carrol. 2005.
Domain-specific sense distributions and predomi-
nant sense acquisition. In Proceedings of HLT-
EMNLP.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the international conference on Systems documenta-
tion.
Xiao Ling, Sameer Singh, and Daniel S. Weld. 2014.
Context representation for named entity linking In
Pacific Northwest Regional NLP Workshop.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT-EMNLP.
Einat Minkov and William W. Cohen. 2013. Adap-
tive graph walk-based similarity measures for parsed
text. Natural Language Engineering.
Sail Mohammad, Graeme Hirst, and Philip Resnik.
2007. Tor, TorMd: Distributional profiles of con-
cepts for unsupervised word sense disambiguation.
In Proceedings of SemEval-2007.
Roberto Navigli and Mirella Lapata. 2010. An experi-
mental study on graph connectivity for unsupervised
word sense disambiguation. IEEE Transactions on
Pattern Anaylsis and Machine Intelligence, 32(4).
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet-based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop on Making Sense of Sense.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2007. UMND1: unsupervised word
sense disambiguation using contextual semantic re-
latedness. In Proceedings of SemEval-2007.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the
relatedness of concepts. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI).
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, SRL and all words. In Pro-
ceedings of SemEval-2007.
Ariel Raviv and Shaul Markovitch. 2012. Concept-
based approach to word-sense disambiguation. In
Proceedings of the AAAI Conference on Artificial In-
telligence (AAAI).
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw-
Hill.
Reid Swanson and Andrew S. Gordon. 2006. A com-
parison of alternative parse tree paths for labeling
semantic roles. In the joint conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING-ACL).
Gyorgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised all-wwords lexical substitution
using delexicalized features. In Proceedings of HLT-
NAACL.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML.
Jing Wang, Mohit Bansal, Kevin Gimpel, Brian D.
Ziebart, and Clement T. Yu. 2015. A sense-topic
model for word sense induction with unsupervised
data enrichment. Transactions of Computational
Linguistics, 3.
David Yarowsky. 1993. One sense per collocation. In
ARPA Human Language Technology Workshop.
</reference>
<page confidence="0.993631">
1667
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744046">
<title confidence="0.99869">Learning to Identify the Best Contexts for Knowledge-based WSD</title>
<author confidence="0.890482">Evgenia Wasserman-Pritsker William W Cohen Einat Minkov</author>
<affiliation confidence="0.997916">UniversityofHaifa Carnegie Mellon University University of Haifa</affiliation>
<address confidence="0.999935">Haifa, Israel, 31905 Pittsburgh, PA 15213 Haifa, Israel, 31905</address>
<email confidence="0.997733">evgeniaw@is.haifa.ac.ilwcohen@cs.cmu.edueinatm@is.haifa.ac.il</email>
<abstract confidence="0.984020153846154">We outline a learning framework that aims at identifying useful contextual cues for knowledge-based word sense disambiguation. The usefulness of individual context words is evaluated based on diverse lexicostatistical and syntactic information, as well as simple word distance. Experiments using two different knowledge-based methods and benchmark datasets show significant improvements due to context modeling, beating the conventional window-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="9802" citStr="Agirre and Soroa, 2009" startWordPosition="1563" endWordPosition="1566">ches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art WSD performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the PPR score Koeling etal. SemevaP07 Table 1: The experimental datasets: statistics attributed to the node denoting sense s, having the graph walk initiated at a uniform distribution over the various senses of c3. PPR scores were generated using the UKB software (Agirre and Soroa, 2009).1- 3.1 Datasets We experiment with two benchmark datasets. The lexical sample due to Koeling et al. (2005) includes annot</context>
<context position="18557" citStr="Agirre and Soroa, 2009" startWordPosition="2976" endWordPosition="2979"> sense inference is performed by maximizing cosine similarity between the centroid of the context vectors and a vector representation of each word sense. Our results using PPR and LCS exceed or roughly match their results without using the Wikipedia resource. 4 Conclusion We presented a learning framework that identifies useful contextual cues for knowledge-based sense disambiguation. The generated models are nonlexicalized, and are therefore applicable to new 3They used WordNet 1.7, while we use version 3.0. Large performance gaps due to different versions of WordNet were reported elsewhere (Agirre and Soroa, 2009). word types. Existing approaches pay little attention to context selection, or perform simplistic context modeling, whereas the proposed approach effectively consolidates diverse types of evidence. In the future, we are interested in representing additional word relatedness measures in this framework, such as embedding-based word similarity (Wang et al., 2015). We are further interested in creating specialized models that fit different word classes, e.g., of particular part-of-speech. In general, the proposed approach may prove beneficial for additional tasks that model word meaning in contex</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Knowledge-based WSD on specific domains: performing better than generic supervised WSD.</title>
<date>2009</date>
<booktitle>In Proceedings of HCAL</booktitle>
<marker>Agirre, de Lacalle, Soroa, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009. Knowledge-based WSD on specific domains: performing better than generic supervised WSD. In Proceedings of HCAL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of HCAL</booktitle>
<contexts>
<context position="9061" citStr="Banerjee and Pedersen, 2003" startWordPosition="1449" endWordPosition="1452">C Finally, a context word is often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. W</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of HCAL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2068" citStr="Fellbaum, 1998" startWordPosition="299" endWordPosition="300">B (Hoffart et al., 2011). Despite the sophistication of inference models developed, little attention has been given so far to context modeling for knowledge-based WSD. Context is represented by bag-of-words, where typically all context words are assigned equal importance (Navigli, 2009; Ling et al., 2014). However, every simple definition of context will include some unrelated or uninformative context words. Consider this usage of the word church: &amp;quot;An ancient stone church stands amid the fields , the sound of bells cascading from its tower&amp;quot;. Known senses for &apos;church&apos; according to Wordnet 3.0 (Fellbaum, 1998) correspond to a group of people, service, or a building. The latter sense is intended in this case, as one may conclude from the context words &apos;stone&apos;, &apos;stands&apos; or &apos;tower&apos;. We wish to focus on such meaningful cues and avoid the modeling of uninformative words (&apos;ancient&apos;, `sound&apos;). In this work, a learning framework is proposed that is aimed at identifying contextual cues that are predictive of the target word&apos;s sense. The usefulness of a candidate context word for the disambiguation of the target word is evaluated based on syntactic and lexico-statistical information, as well as simple word d</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the WAC4 Workshop at LREC.</booktitle>
<contexts>
<context position="8091" citStr="Ferraresi et al., 2008" startWordPosition="1290" endWordPosition="1293">word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de Marneffe et al., 2006) in our experiments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In general, we expect context words that are topically related to the target word to be useful for its disambiguation. To compute PMI, we obtained word frequencies from the large ukWaC corpus (Ferraresi et al., 2008), considering word co-occurrences over a window of five words. It has been indicated that highly frequent words are generally less topical, where this aspect is not fully captured by PMI (Han et al., 2013). We therefore model as complimentary information the inverse document frequency (Salton and McGill, 1983) of c3, also computed using ukWaC Finally, a context word is often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods re</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the WAC4 Workshop at LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Exploratory Newsletter,</journal>
<pages>11--1</pages>
<contexts>
<context position="11486" citStr="Hall et al., 2009" startWordPosition="1834" endWordPosition="1837"> pairs. For every word pair (w, c3), the scores Sim(s, s E S(w), were generated using GV and PPR and WordNet 3.0 as the reference knowledge base. A context-target word pair was labeled as a positive example if it yielded a correct sense prediction, or as negative otherwise (Eq. 3). Table 1 details statistics of the original and respective word pair datasets, including the ratio of context words labeled as positive examples— as shown, this `pairwise accuracy&apos; is low, reaching up to 0.35. 3.2 Experimental setup We experimented with several classification paradigms using the Weka learning suite (Hall et al., 2009). Learning had to be robust to label noise. We report results using Naive Bayes, due to both its good performance and efficiency. Following preliminary experiments, we adopted a context selection approach—the learned model is used to rank the available context words, where the top ranked words, obtained by applying ratio r, are selected as context. We tune r using training examples. The reported performance uses rough values of r = 0.5 for the Koeling et al. examples, which 1 http://ixa2. si.ehu.es/ukbi; we used the bin file wn30+gloss, and the PPR_w2w graph walk variant. Word Target Context P</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Exploratory Newsletter, 11(1):10-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Tim Finin</author>
<author>Paul McNamee</author>
<author>Anupam Joshi</author>
<author>Yelena Yesha</author>
</authors>
<title>Improving word similarity by augmenting PMI with estimates of word polysemy.</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering (TKDE),</journal>
<volume>25</volume>
<issue>6</issue>
<contexts>
<context position="8296" citStr="Han et al., 2013" startWordPosition="1325" endWordPosition="1328">ments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In general, we expect context words that are topically related to the target word to be useful for its disambiguation. To compute PMI, we obtained word frequencies from the large ukWaC corpus (Ferraresi et al., 2008), considering word co-occurrences over a window of five words. It has been indicated that highly frequent words are generally less topical, where this aspect is not fully captured by PMI (Han et al., 2013). We therefore model as complimentary information the inverse document frequency (Salton and McGill, 1983) of c3, also computed using ukWaC Finally, a context word is often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between </context>
</contexts>
<marker>Han, Finin, McNamee, Joshi, Yesha, 2013</marker>
<rawString>Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi, and Yelena Yesha. 2013. Improving word similarity by augmenting PMI with estimates of word polysemy. IEEE Transactions on Knowledge and Data Engineering (TKDE), 25(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed A Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen Furstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1477" citStr="Hoffart et al., 2011" startWordPosition="205" endWordPosition="208">anguage processing. Unsupervised knowledge-based approaches to WSD (Navigli, 2009) make use of available lexical resources rather than rely on costly annotated data. Sense inference in this setting involves finding the word sense that agrees most with the specified context according to the information encoded in the knowledge base (KB). The popular Lesk (1986) method, for example, seeks to maximize word overlap between the dictionary glosses associated with the context words, and the glosses of candidate word senses. Similar methods are used in named entity disambiguation and linking to a KB (Hoffart et al., 2011). Despite the sophistication of inference models developed, little attention has been given so far to context modeling for knowledge-based WSD. Context is represented by bag-of-words, where typically all context words are assigned equal importance (Navigli, 2009; Ling et al., 2014). However, every simple definition of context will include some unrelated or uninformative context words. Consider this usage of the word church: &amp;quot;An ancient stone church stands amid the fields , the sound of bells cascading from its tower&amp;quot;. Known senses for &apos;church&apos; according to Wordnet 3.0 (Fellbaum, 1998) correspo</context>
<context position="15952" citStr="Hoffart et al., 2011" startWordPosition="2557" endWordPosition="2560"> are especially useful for knowledge-based WSD. Modeling IDF information led to further gains in performance. As discussed before, the two measures are complimentary, as common words are generally less topical. Representing the number of senses of the context words yielded further improvements. Overall, this combination of word features accounted for the majority of the total gains achieved. The syntactic features had a lesser impact, yet improved results further, mainly using the GV method Finally, simple word distance was found to have little impact; similar behavior was observed elsewhere (Hoffart et al., 2011). In another set of experiments, we evaluated and found LCS to be robust with respect to the ratio r— while performance using LCS varied, it improved over the baseline across the range 0 &lt; r &lt; 1. In contrast, selecting equal-sized sets of context words using the window approach was found to 1665 hurt performance. Finally, we compare our results against previous works. Our approach outperforms the results obtained by unsupervised systems on the noun portion of the SemEval`07 dataset (Patwardhan et al., 2007; Mohammad et al., 2007), achieving recall of .507 vs. .497 (a higher result obtained by </context>
</contexts>
<marker>Hoffart, Yosef, Bordino, Furstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed A. Yosef, Ilaria Bordino, Hagen Furstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heyan Huang</author>
<author>Wenpeng Lu</author>
</authors>
<title>Knowledgebased word sense disambiguation with feature words based on dependency relation and syntax tree.</title>
<date>2011</date>
<journal>International Journal of Advancements in Computing Technology,</journal>
<volume>3</volume>
<issue>8</issue>
<contexts>
<context position="7044" citStr="Huang and Lu, 2011" startWordPosition="1120" endWordPosition="1123">sults in preliminary experiments. 2.2 Feature Types Various aspects may be modeled as features in this framework, describing properties of the context word c3, as well as the relationship between the target-context word pair (w, c3). In addition to simple word distance, we encode the following syntactic and lexico-statistical information. Syntactic features. Word distance is further assessed in syntactic terms, denoting the length of the shortest dependency path linking the word pair, as well as the length of the shortest connecting path in a constituency parse tree (Swanson and Gordon, 2006; Huang and Lu, 2011). It may be useful to further encode information about the edge types that comprise the connecting path, as some dependency relations indicate more salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de M</context>
<context position="16919" citStr="Huang and Lu (2011)" startWordPosition="2721" endWordPosition="2724">gainst previous works. Our approach outperforms the results obtained by unsupervised systems on the noun portion of the SemEval`07 dataset (Patwardhan et al., 2007; Mohammad et al., 2007), achieving recall of .507 vs. .497 (a higher result obtained by Mohammad et al). Table 4 presents LCS results separately for each of the source domains of the Koeling dataset for comparison purposes. Previous results using PPR and uniform context weighting reported by Agirre et al. (2009) (AL&amp;S &apos;09) are substantially lower than our baseline; we mainly attribute this to the different version of WordNet used.3 Huang and Lu (2011) proposed a manuallytuned syntax-based context selection and weighting formula. They applied it in combination with the GV method, reporting improvement on BNC sentences only. Our baseline result using GV was lower (.382 vs. .390), however LCS yielded better final performance (.401 vs. .397). Compared with their work, we use learning and model richer types of evidence; with PPR and LCS, we report best results on the BNC sentences. Table 4 details also recent results obtained for the BNC and Sports portions of the dataset. Ponzetto and Navigli (2010) (P&amp;N`10) enriched the WordNet graph with add</context>
</contexts>
<marker>Huang, Lu, 2011</marker>
<rawString>Heyan Huang and Wenpeng Lu. 2011. Knowledgebased word sense disambiguation with feature words based on dependency relation and syntax tree. International Journal of Advancements in Computing Technology, 3(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="9507" citStr="Hughes and Ramage, 2007" startWordPosition="1516" endWordPosition="1519">rlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art WSD performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the PPR score Koeling etal. SemevaP07 Table 1: The experimental datasets: statistics attributed to the node denotin</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Jeh</author>
<author>Jennifer Widom</author>
</authors>
<title>Scaling personalized web search.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web (WWW).</booktitle>
<contexts>
<context position="9860" citStr="Jeh and Widom, 2003" startWordPosition="1573" endWordPosition="1576">ted senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art WSD performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the PPR score Koeling etal. SemevaP07 Table 1: The experimental datasets: statistics attributed to the node denoting sense s, having the graph walk initiated at a uniform distribution over the various senses of c3. PPR scores were generated using the UKB software (Agirre and Soroa, 2009).1- 3.1 Datasets We experiment with two benchmark datasets. The lexical sample due to Koeling et al. (2005) includes annotated instances of 41 selected nouns. About 300 example sen</context>
</contexts>
<marker>Jeh, Widom, 2003</marker>
<rawString>Glen Jeh and Jennifer Widom. 2003. Scaling personalized web search. In Proceedings of the 12th international conference on World Wide Web (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>English senseval:report and results.</title>
<date>2000</date>
<booktitle>In 2nd International Conference on Language Resourcesand Evaluation (LREC).</booktitle>
<contexts>
<context position="9032" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="1445" endWordPosition="1448">) of c3, also computed using ukWaC Finally, a context word is often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses </context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. English senseval:report and results. In 2nd International Conference on Language Resourcesand Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carrol</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<contexts>
<context position="10387" citStr="Koeling et al. (2005)" startWordPosition="1660" endWordPosition="1663">performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the PPR score Koeling etal. SemevaP07 Table 1: The experimental datasets: statistics attributed to the node denoting sense s, having the graph walk initiated at a uniform distribution over the various senses of c3. PPR scores were generated using the UKB software (Agirre and Soroa, 2009).1- 3.1 Datasets We experiment with two benchmark datasets. The lexical sample due to Koeling et al. (2005) includes annotated instances of 41 selected nouns. About 300 example sentences are available per noun, retrieved evenly from three sources: the domain-specific sports and finance sections of Reuters corpus, and the general British National Corpus (BNC). The second dataset consists of all noun examples from the SemEval-2007 English lexical sample task (Pradhan et al., 2007), created from another corpus—the WSJ Treebank. The two datasets were transformed into targetcontext word pairs. For every word pair (w, c3), the scores Sim(s, s E S(w), were generated using GV and PPR and WordNet 3.0 as the</context>
</contexts>
<marker>Koeling, McCarthy, Carrol, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carrol. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the international conference on Systems documentation.</booktitle>
<contexts>
<context position="1218" citStr="Lesk (1986)" startWordPosition="165" endWordPosition="166">ments using two different knowledge-based methods and benchmark datasets show significant improvements due to context modeling, beating the conventional window-based approach. 1 Introduction Word sense disambiguation (WSD) is a key task of natural language processing. Unsupervised knowledge-based approaches to WSD (Navigli, 2009) make use of available lexical resources rather than rely on costly annotated data. Sense inference in this setting involves finding the word sense that agrees most with the specified context according to the information encoded in the knowledge base (KB). The popular Lesk (1986) method, for example, seeks to maximize word overlap between the dictionary glosses associated with the context words, and the glosses of candidate word senses. Similar methods are used in named entity disambiguation and linking to a KB (Hoffart et al., 2011). Despite the sophistication of inference models developed, little attention has been given so far to context modeling for knowledge-based WSD. Context is represented by bag-of-words, where typically all context words are assigned equal importance (Navigli, 2009; Ling et al., 2014). However, every simple definition of context will include </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the international conference on Systems documentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Sameer Singh</author>
<author>Daniel S Weld</author>
</authors>
<title>Context representation for named entity linking In Pacific Northwest Regional NLP Workshop.</title>
<date>2014</date>
<contexts>
<context position="1759" citStr="Ling et al., 2014" startWordPosition="246" endWordPosition="249">o the information encoded in the knowledge base (KB). The popular Lesk (1986) method, for example, seeks to maximize word overlap between the dictionary glosses associated with the context words, and the glosses of candidate word senses. Similar methods are used in named entity disambiguation and linking to a KB (Hoffart et al., 2011). Despite the sophistication of inference models developed, little attention has been given so far to context modeling for knowledge-based WSD. Context is represented by bag-of-words, where typically all context words are assigned equal importance (Navigli, 2009; Ling et al., 2014). However, every simple definition of context will include some unrelated or uninformative context words. Consider this usage of the word church: &amp;quot;An ancient stone church stands amid the fields , the sound of bells cascading from its tower&amp;quot;. Known senses for &apos;church&apos; according to Wordnet 3.0 (Fellbaum, 1998) correspond to a group of people, service, or a building. The latter sense is intended in this case, as one may conclude from the context words &apos;stone&apos;, &apos;stands&apos; or &apos;tower&apos;. We wish to focus on such meaningful cues and avoid the modeling of uninformative words (&apos;ancient&apos;, `sound&apos;). In this </context>
</contexts>
<marker>Ling, Singh, Weld, 2014</marker>
<rawString>Xiao Ling, Sameer Singh, and Daniel S. Weld. 2014. Context representation for named entity linking In Pacific Northwest Regional NLP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="9481" citStr="Mihalcea, 2005" startWordPosition="1514" endWordPosition="1515">erms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art WSD performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the PPR score Koeling etal. SemevaP07 Table 1: The experimental datasets: statistics attr</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Adaptive graph walk-based similarity measures for parsed text. Natural Language Engineering.</title>
<date>2013</date>
<contexts>
<context position="7280" citStr="Minkov and Cohen, 2013" startWordPosition="1157" endWordPosition="1160">. In addition to simple word distance, we encode the following syntactic and lexico-statistical information. Syntactic features. Word distance is further assessed in syntactic terms, denoting the length of the shortest dependency path linking the word pair, as well as the length of the shortest connecting path in a constituency parse tree (Swanson and Gordon, 2006; Huang and Lu, 2011). It may be useful to further encode information about the edge types that comprise the connecting path, as some dependency relations indicate more salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de Marneffe et al., 2006) in our experiments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In ge</context>
</contexts>
<marker>Minkov, Cohen, 2013</marker>
<rawString>Einat Minkov and William W. Cohen. 2013. Adaptive graph walk-based similarity measures for parsed text. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sail Mohammad</author>
<author>Graeme Hirst</author>
<author>Philip Resnik</author>
</authors>
<title>Tor, TorMd: Distributional profiles of concepts for unsupervised word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="16487" citStr="Mohammad et al., 2007" startWordPosition="2647" endWordPosition="2650">d to have little impact; similar behavior was observed elsewhere (Hoffart et al., 2011). In another set of experiments, we evaluated and found LCS to be robust with respect to the ratio r— while performance using LCS varied, it improved over the baseline across the range 0 &lt; r &lt; 1. In contrast, selecting equal-sized sets of context words using the window approach was found to 1665 hurt performance. Finally, we compare our results against previous works. Our approach outperforms the results obtained by unsupervised systems on the noun portion of the SemEval`07 dataset (Patwardhan et al., 2007; Mohammad et al., 2007), achieving recall of .507 vs. .497 (a higher result obtained by Mohammad et al). Table 4 presents LCS results separately for each of the source domains of the Koeling dataset for comparison purposes. Previous results using PPR and uniform context weighting reported by Agirre et al. (2009) (AL&amp;S &apos;09) are substantially lower than our baseline; we mainly attribute this to the different version of WordNet used.3 Huang and Lu (2011) proposed a manuallytuned syntax-based context selection and weighting formula. They applied it in combination with the GV method, reporting improvement on BNC sentence</context>
</contexts>
<marker>Mohammad, Hirst, Resnik, 2007</marker>
<rawString>Sail Mohammad, Graeme Hirst, and Philip Resnik. 2007. Tor, TorMd: Distributional profiles of concepts for unsupervised word sense disambiguation. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study on graph connectivity for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Anaylsis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="17674" citStr="Navigli and Lapata, 2010" startWordPosition="2843" endWordPosition="2847"> reporting improvement on BNC sentences only. Our baseline result using GV was lower (.382 vs. .390), however LCS yielded better final performance (.401 vs. .397). Compared with their work, we use learning and model richer types of evidence; with PPR and LCS, we report best results on the BNC sentences. Table 4 details also recent results obtained for the BNC and Sports portions of the dataset. Ponzetto and Navigli (2010) (P&amp;N`10) enriched the WordNet graph with additional relations projected onto the graph from Wikipedia; the table reports their best results using a graph centrality measure (Navigli and Lapata, 2010). Raviv and Markovitch (2012) (R&amp;M`12) reported state-of-the-art performance in the specialized domains using Wikipedia as the reference knowledge base. Each individual context word is represented in their work as a weighted vector of Wikipedia concepts, where sense inference is performed by maximizing cosine similarity between the centroid of the context vectors and a vector representation of each word sense. Our results using PPR and LCS exceed or roughly match their results without using the Wikipedia resource. 4 Conclusion We presented a learning framework that identifies useful contextual</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study on graph connectivity for unsupervised word sense disambiguation. IEEE Transactions on Pattern Anaylsis and Machine Intelligence, 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="938" citStr="Navigli, 2009" startWordPosition="120" endWordPosition="121">outline a learning framework that aims at identifying useful contextual cues for knowledge-based word sense disambiguation. The usefulness of individual context words is evaluated based on diverse lexico- statistical and syntactic information, as well as simple word distance. Experiments using two different knowledge-based methods and benchmark datasets show significant improvements due to context modeling, beating the conventional window-based approach. 1 Introduction Word sense disambiguation (WSD) is a key task of natural language processing. Unsupervised knowledge-based approaches to WSD (Navigli, 2009) make use of available lexical resources rather than rely on costly annotated data. Sense inference in this setting involves finding the word sense that agrees most with the specified context according to the information encoded in the knowledge base (KB). The popular Lesk (1986) method, for example, seeks to maximize word overlap between the dictionary glosses associated with the context words, and the glosses of candidate word senses. Similar methods are used in named entity disambiguation and linking to a KB (Hoffart et al., 2011). Despite the sophistication of inference models developed, l</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7255" citStr="Pado and Lapata, 2007" startWordPosition="1152" endWordPosition="1156">ntext word pair (w, c3). In addition to simple word distance, we encode the following syntactic and lexico-statistical information. Syntactic features. Word distance is further assessed in syntactic terms, denoting the length of the shortest dependency path linking the word pair, as well as the length of the shortest connecting path in a constituency parse tree (Swanson and Gordon, 2006; Huang and Lu, 2011). It may be useful to further encode information about the edge types that comprise the connecting path, as some dependency relations indicate more salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de Marneffe et al., 2006) in our experiments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the contex</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using wordnet-based context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Making Sense of Sense.</booktitle>
<contexts>
<context position="9161" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1464" endWordPosition="1467">ic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using wordnet-based context vectors to estimate the semantic relatedness of concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>UMND1: unsupervised word sense disambiguation using contextual semantic relatedness.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="16463" citStr="Patwardhan et al., 2007" startWordPosition="2643" endWordPosition="2646">le word distance was found to have little impact; similar behavior was observed elsewhere (Hoffart et al., 2011). In another set of experiments, we evaluated and found LCS to be robust with respect to the ratio r— while performance using LCS varied, it improved over the baseline across the range 0 &lt; r &lt; 1. In contrast, selecting equal-sized sets of context words using the window approach was found to 1665 hurt performance. Finally, we compare our results against previous works. Our approach outperforms the results obtained by unsupervised systems on the noun portion of the SemEval`07 dataset (Patwardhan et al., 2007; Mohammad et al., 2007), achieving recall of .507 vs. .497 (a higher result obtained by Mohammad et al). Table 4 presents LCS results separately for each of the source domains of the Koeling dataset for comparison purposes. Previous results using PPR and uniform context weighting reported by Agirre et al. (2009) (AL&amp;S &apos;09) are substantially lower than our baseline; we mainly attribute this to the different version of WordNet used.3 Huang and Lu (2011) proposed a manuallytuned syntax-based context selection and weighting formula. They applied it in combination with the GV method, reporting imp</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2007</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2007. UMND1: unsupervised word sense disambiguation using contextual semantic relatedness. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="9396" citStr="Pedersen et al., 2004" startWordPosition="1501" endWordPosition="1504">published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized PageRank (PPR) algorithm, which has been shown to yield state-of-the-art WSD performance (Agirre and Soroa, 2009). According to the linearity theorem (Jeh and Widom, 2003), PPR scores can be computed for each of the context words separately, and then aggregated (Eq. 2). In this case, Sim(s, c3) equals the </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - measuring the relatedness of concepts. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9090" citStr="Ponzetto and Navigli, 2010" startWordPosition="1453" endWordPosition="1457">often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000; Banerjee and Pedersen, 2003; Ponzetto and Navigli, 2010). We experiment with Gloss vectors (GV) (Patwardhan and Pedersen, 2006). This method enriches WordNet glosses with glosses of hypernyms and other related senses, as well as with co-occurring words derived from raw text. GV scores were obtained using the WordNet: :Similarity package (Pedersen et al., 2004). Graph-based methods are also commonly used for sense disambiguation (Mihalcea, 2005; Hughes and Ramage, 2007). If the KB is represented as a graph, various metrics can be applied that reflect structural similarity between word senses represented as graph nodes. We consider the Personalized P</context>
<context position="17474" citStr="Ponzetto and Navigli (2010)" startWordPosition="2813" endWordPosition="2816">bute this to the different version of WordNet used.3 Huang and Lu (2011) proposed a manuallytuned syntax-based context selection and weighting formula. They applied it in combination with the GV method, reporting improvement on BNC sentences only. Our baseline result using GV was lower (.382 vs. .390), however LCS yielded better final performance (.401 vs. .397). Compared with their work, we use learning and model richer types of evidence; with PPR and LCS, we report best results on the BNC sentences. Table 4 details also recent results obtained for the BNC and Sports portions of the dataset. Ponzetto and Navigli (2010) (P&amp;N`10) enriched the WordNet graph with additional relations projected onto the graph from Wikipedia; the table reports their best results using a graph centrality measure (Navigli and Lapata, 2010). Raviv and Markovitch (2012) (R&amp;M`12) reported state-of-the-art performance in the specialized domains using Wikipedia as the reference knowledge base. Each individual context word is represented in their work as a weighted vector of Wikipedia concepts, where sense inference is performed by maximizing cosine similarity between the centroid of the context vectors and a vector representation of eac</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task 17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="10763" citStr="Pradhan et al., 2007" startWordPosition="1716" endWordPosition="1719">itiated at a uniform distribution over the various senses of c3. PPR scores were generated using the UKB software (Agirre and Soroa, 2009).1- 3.1 Datasets We experiment with two benchmark datasets. The lexical sample due to Koeling et al. (2005) includes annotated instances of 41 selected nouns. About 300 example sentences are available per noun, retrieved evenly from three sources: the domain-specific sports and finance sections of Reuters corpus, and the general British National Corpus (BNC). The second dataset consists of all noun examples from the SemEval-2007 English lexical sample task (Pradhan et al., 2007), created from another corpus—the WSJ Treebank. The two datasets were transformed into targetcontext word pairs. For every word pair (w, c3), the scores Sim(s, s E S(w), were generated using GV and PPR and WordNet 3.0 as the reference knowledge base. A context-target word pair was labeled as a positive example if it yielded a correct sense prediction, or as negative otherwise (Eq. 3). Table 1 details statistics of the original and respective word pair datasets, including the ratio of context words labeled as positive examples— as shown, this `pairwise accuracy&apos; is low, reaching up to 0.35. 3.2</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task 17: English lexical sample, SRL and all words. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Raviv</author>
<author>Shaul Markovitch</author>
</authors>
<title>Conceptbased approach to word-sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="17703" citStr="Raviv and Markovitch (2012)" startWordPosition="2848" endWordPosition="2851">NC sentences only. Our baseline result using GV was lower (.382 vs. .390), however LCS yielded better final performance (.401 vs. .397). Compared with their work, we use learning and model richer types of evidence; with PPR and LCS, we report best results on the BNC sentences. Table 4 details also recent results obtained for the BNC and Sports portions of the dataset. Ponzetto and Navigli (2010) (P&amp;N`10) enriched the WordNet graph with additional relations projected onto the graph from Wikipedia; the table reports their best results using a graph centrality measure (Navigli and Lapata, 2010). Raviv and Markovitch (2012) (R&amp;M`12) reported state-of-the-art performance in the specialized domains using Wikipedia as the reference knowledge base. Each individual context word is represented in their work as a weighted vector of Wikipedia concepts, where sense inference is performed by maximizing cosine similarity between the centroid of the context vectors and a vector representation of each word sense. Our results using PPR and LCS exceed or roughly match their results without using the Wikipedia resource. 4 Conclusion We presented a learning framework that identifies useful contextual cues for knowledge-based sen</context>
</contexts>
<marker>Raviv, Markovitch, 2012</marker>
<rawString>Ariel Raviv and Shaul Markovitch. 2012. Conceptbased approach to word-sense disambiguation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGrawHill.</publisher>
<contexts>
<context position="8402" citStr="Salton and McGill, 1983" startWordPosition="1341" endWordPosition="1344"> measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In general, we expect context words that are topically related to the target word to be useful for its disambiguation. To compute PMI, we obtained word frequencies from the large ukWaC corpus (Ferraresi et al., 2008), considering word co-occurrences over a window of five words. It has been indicated that highly frequent words are generally less topical, where this aspect is not fully captured by PMI (Han et al., 2013). We therefore model as complimentary information the inverse document frequency (Salton and McGill, 1983) of c3, also computed using ukWaC Finally, a context word is often ambiguous by itself, where low polysemy is correlated with topic-specificity (Han et al., 2013). We represent the number of known senses of the context word c3 based on WordNet. 3 Experiments We consider two WSD methods representative of prevalent knowledge-based approaches, comparing against previously published results. The popular Lesk approach (1986) mentioned before computes Sim(s, c3) in terms of word overlap between the glosses of the senses of c3 and the gloss of s. There exist multiple variants of the Lesk algorithm (K</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>A comparison of alternative parse tree paths for labeling semantic roles.</title>
<date>2006</date>
<booktitle>In the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL).</booktitle>
<contexts>
<context position="7023" citStr="Swanson and Gordon, 2006" startWordPosition="1116" endWordPosition="1119">ound to give preferable results in preliminary experiments. 2.2 Feature Types Various aspects may be modeled as features in this framework, describing properties of the context word c3, as well as the relationship between the target-context word pair (w, c3). In addition to simple word distance, we encode the following syntactic and lexico-statistical information. Syntactic features. Word distance is further assessed in syntactic terms, denoting the length of the shortest dependency path linking the word pair, as well as the length of the shortest connecting path in a constituency parse tree (Swanson and Gordon, 2006; Huang and Lu, 2011). It may be useful to further encode information about the edge types that comprise the connecting path, as some dependency relations indicate more salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the </context>
</contexts>
<marker>Swanson, Gordon, 2006</marker>
<rawString>Reid Swanson and Andrew S. Gordon. 2006. A comparison of alternative parse tree paths for labeling semantic roles. In the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gyorgy Szarvas</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>Supervised all-wwords lexical substitution using delexicalized features.</title>
<date>2013</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="2970" citStr="Szarvas et al., 2013" startWordPosition="442" endWordPosition="445">. In this work, a learning framework is proposed that is aimed at identifying contextual cues that are predictive of the target word&apos;s sense. The usefulness of a candidate context word for the disambiguation of the target word is evaluated based on syntactic and lexico-statistical information, as well as simple word distance. Indirect supervision is provided using noisy example labels induced automatically. Importantly, explicit lexical information is not encoded—the prediction model can thus be applied in settings where no sensetagged examples are available of the target word type (see also (Szarvas et al., 2013)). Having assessed the usefulness of available context words given the learned model, we consider only the top scoring context words in performing WSD. We believe this work to be the first to perform learning-based context selection for knowledgebased sense disambiguation. Empirical evaluation using two representative knowledge-based WSD methods and different benchmark datasets indicates on consistent improvements in performance due to context selection using the proposed approach. 1662 2 Learned context selection models (LCS) We first define the WSD task. Given a word mention w and available </context>
</contexts>
<marker>Szarvas, Biemann, Gurevych, 2013</marker>
<rawString>Gyorgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-wwords lexical substitution using delexicalized features. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="7801" citStr="Turney, 2001" startWordPosition="1244" endWordPosition="1245">ore salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de Marneffe et al., 2006) in our experiments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In general, we expect context words that are topically related to the target word to be useful for its disambiguation. To compute PMI, we obtained word frequencies from the large ukWaC corpus (Ferraresi et al., 2008), considering word co-occurrences over a window of five words. It has been indicated that highly frequent words are generally less topical, where this aspect is not fully captured by PMI (Han et al., 2013). We therefore model as complimentary information the inverse document frequency (Salton and McGill, 1983</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Wang</author>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Brian D Ziebart</author>
<author>Clement T Yu</author>
</authors>
<title>A sense-topic model for word sense induction with unsupervised data enrichment.</title>
<date>2015</date>
<journal>Transactions of Computational Linguistics,</journal>
<volume>3</volume>
<marker>Wang, Bansal, Gimpel, Ziebart, Yu, 2015</marker>
<rawString>Jing Wang, Mohit Bansal, Kevin Gimpel, Brian D. Ziebart, and Clement T. Yu. 2015. A sense-topic model for word sense induction with unsupervised data enrichment. Transactions of Computational Linguistics, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="7524" citStr="Yarowsky, 1993" startWordPosition="1203" endWordPosition="1204">pair, as well as the length of the shortest connecting path in a constituency parse tree (Swanson and Gordon, 2006; Huang and Lu, 2011). It may be useful to further encode information about the edge types that comprise the connecting path, as some dependency relations indicate more salient semantic relatedness than others (Pado and Lapata, 2007; Minkov and Cohen, 2013). In this work, if the target and context words are directly connected in the dependency graph, we include a feature indicating the label of the edge. The part-ofspeech tag of the context word may provide another contextual cue (Yarowsky, 1993); dedicated features indicate whether c3 is tagged as noun, verb, adjective or adverb. We used the Stanford parser (de Marneffe et al., 2006) in our experiments. otherwise 1663 Lexico-statistical information. We use the pointwise mutual information (PMI) measure (Turney, 2001) to assess the semantic relatedness between the context—target word pair. In general, we expect context words that are topically related to the target word to be useful for its disambiguation. To compute PMI, we obtained word frequencies from the large ukWaC corpus (Ferraresi et al., 2008), considering word co-occurrences</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In ARPA Human Language Technology Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
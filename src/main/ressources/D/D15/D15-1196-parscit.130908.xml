<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001969">
<title confidence="0.980462">
Online Learning of Interpretable Word Embeddings
</title>
<author confidence="0.99921">
Hongyin Luo1, Zhiyuan Liu1,2 ∗, Huanbo Luan1, Maosong Sun1,2
</author>
<affiliation confidence="0.970888">
1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China
</affiliation>
<sectionHeader confidence="0.978439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997197">
Word embeddings encode semantic mean-
ings of words into low-dimension word
vectors. In most word embeddings, one
cannot interpret the meanings of specific
dimensions of those word vectors. Non-
negative matrix factorization (NMF) has
been proposed to learn interpretable word
embeddings via non-negative constraints.
However, NMF methods suffer from scale
and memory issue because they have to
maintain a global matrix for learning. To
alleviate this challenge, we propose on-
line learning of interpretable word embed-
dings from streaming text data. Exper-
iments show that our model consistently
outperforms the state-of-the-art word em-
bedding methods in both representation a-
bility and interpretability. The source code
of this paper can be obtained from http:
//github.com/skTim/OIWE.
</bodyText>
<sectionHeader confidence="0.998768" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998625">
Word embeddings (Turian et al., 2010) aim to
encode semantic meanings of words into low-
dimensional dense vectors. As compared with tra-
ditional one-hot representation and distributional
representation, word embeddings can better ad-
dress the sparsity issue and have achieved success
in many NLP applications recent years.
There are two typical approaches for word em-
beddings. The neural-network (NN) approach
(Bengio et al., 2006) employs neural-based tech-
niques to learn word embeddings. The matrix fac-
torization (MF) approach (Pennington et al., 2014)
builds word embeddings by factorizing word-
context co-occurrence matrices. The MF approach
requires a global statistical matrix, while the N-
N approach can flexibly perform learning from
</bodyText>
<note confidence="0.516863">
∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)
</note>
<bodyText confidence="0.998986292682927">
streaming text data, which is efficient in both com-
putation and memory. For example, two recen-
t NN methods, Skip-Gram and Continuous Bag-
of-Word Model (CBOW) (Mikolov et al., 2013a;
Mikolov et al., 2013b), have achieved impressive
impact due to their simplicity and efficiency.
For most word embedding methods, a critical
issue is that, we are unaware of what each dimen-
sion represent in word embeddings. Hence, the
latent dimension for which a word has its largest
value is difficult to interpret. This makes word em-
beddings like a black-box, and prevents them from
being human-readable and further manipulation.
People have proposed non-negative matrix fac-
torization (NMF) for word representation, denoted
as non-negative sparse embedding (NNSE) (Mur-
phy et al., 2012). NNSE realizes interpretable
word embeddings by applying non-negative con-
straints for word embeddings. Although NNSE
learns word embeddings with good interpret-
abilities, like other MF methods, it also requires a
global matrix for learning, thus suffers from heavy
memory usage and cannot well deal with stream-
ing text data.
Inspired by the characteristics of NMF meth-
ods (Lee and Seung, 1999), we note that, non-
negative constraints only allow additive combi-
nations instead of subtractive combinations, and
lead to a parts-based representation. Hence, the
non-negative constraints derive interpretabilities
of word embeddings. In this paper, we aim to de-
sign an online NN method to efficiently learn in-
terpretable word embeddings. In order to achieve
the goal of interpretable embeddings, we design
projected gradient descent (Lin, 2007) for opti-
mization so as to apply non-negative constraints
on NN methods such as Skip-Gram. We also em-
ploy adaptive gradient descent (Sun et al., 2012)
to speedup learning convergence. We name the
proposed models as online interpretable word em-
beddings (OIWE).
</bodyText>
<page confidence="0.932256">
1687
</page>
<note confidence="0.65174">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9998402">
For experiments, we implement OIWE based
on Skip-Gram. We evaluate the representation
performance of word embedding methods on the
word similarity computation task. Experiment re-
sults show that, our OIWE models are signifi-
cantly superior to other baselines including Skip-
Gram, RNN and NNSE. We also evaluate the in-
terpretability performance on the word intrusion
detection task. The results demonstrate the effec-
tiveness of OIWE as compared to NNSE.
</bodyText>
<sectionHeader confidence="0.996205" genericHeader="method">
2 Our Model
</sectionHeader>
<bodyText confidence="0.999010666666667">
In this section, we first introduce Skip-Gram and
then introduce the proposed online interpretable
word embeddings based on Skip-Gram.
</bodyText>
<subsectionHeader confidence="0.994708">
2.1 Skip-Gram
</subsectionHeader>
<bodyText confidence="0.998967142857143">
Skip-Gram (Mikolov et al., 2013b) is simple and
effective to learn word embeddings. The objec-
tive of Skip-Gram is to make word vectors good
at predicting its context words. More specifically,
given a word sequence {w1, w2, ... , wT}, Skip-
Gram aims to maximize the average log probabil-
ity
</bodyText>
<equation confidence="0.999113">
1 X log Pr(wt+j|wt)), (1)
1 (
−k&lt;j&lt;k,j70
</equation>
<bodyText confidence="0.999744">
where k is the context window size, and
Pr(wt+j|wt) indicates the probability of seeing
wt+j in the context of wt, which are measured
with softmax function
</bodyText>
<equation confidence="0.938082666666667">
exp �wt+j - wt �
Pr(wt+j|wt) = PwEW exp �w - wt~,
(2)
</equation>
<bodyText confidence="0.999755">
where wt+j and wt are word embeddings of wt+j
and wt, and W is the vocabulary size. Since the
computation of full softmax is time consuming,
the techniques of hierarchical softmax and nega-
tive sampling (Mikolov et al., 2013b) are proposed
for approximation.
Take negative sampling for example. The log
probability Pr(wt+j|wt) can be approximate by
</bodyText>
<equation confidence="0.8558795">
Xlog σ(wt+j - wt) + log σ(w - wt), (3)
wENt
</equation>
<bodyText confidence="0.958035777777778">
where σ(x) = 1/(1 + exp(−x)), and Nt is the
set of negative samples as compared to the cor-
responding context word wt+j. The task can be
regarded as to distinguish the context word wt+j
from negative samples.
For Skip-Gram with negative sampling, we can
perform stochastic gradient descent for learning.
The update rule for the positive/negative context
words u E {wt+j} U Nt is
</bodyText>
<equation confidence="0.999166">
ui+1 = ui + γ[Iwt(u) − σ(u - wt)]wit, (4)
</equation>
<bodyText confidence="0.9999312">
where Iwt(u) = 1 when w is the positive contex-
t word of wt and Iwt(u) = 0 when w is nega-
tive, i is the iteration number, and γ is the learning
rate. Correspondingly, the update rule for the in-
put word wt is
</bodyText>
<equation confidence="0.9875996">
X
wi+1
t = wi t+γ
uE{wt}UNt
(5)
</equation>
<bodyText confidence="0.998249">
We note that, the learning rate γ in Skip-Gram is
shared by all word embeddings.
</bodyText>
<subsectionHeader confidence="0.955631">
2.2 OIWE
</subsectionHeader>
<bodyText confidence="0.9991376">
In order to learn interpretable word embeddings,
we have to make the word embeddings learned in
Skip-Gram keep non-negative. In order to achieve
this goal, we have to constrain the update rules in
Equation (4) and (5) as follows:
</bodyText>
<equation confidence="0.946188">
xi+1
k = P [xik + γVf(xk)], (6)
</equation>
<bodyText confidence="0.99993975">
where x may be u or wt, k is the corresponding di-
mension in word embedding x, Vf(xk) indicates
the gradient corresponding to xk, and P[-] is de-
fined as
</bodyText>
<equation confidence="0.9828635">
(
x if x &gt; 0,
P[x] = (7)
0 if x ≤ 0.
</equation>
<bodyText confidence="0.999894285714286">
Motivated by the projected gradient descent meth-
ods for NMF (Lin, 2007), in this paper we pro-
pose two methods for Skip-Gram to realize the
constraint in Equation (6).
Naive Projected Gradient (NPG). In NPG, we
consider the most straightforward update strategy
by simply setting
</bodyText>
<equation confidence="0.891498">
xi+1
k = max (0, xik + γVf(xk)). (8)
</equation>
<bodyText confidence="0.99993075">
The method has been used for NMF (Lin, 2007)
although the details are not discussed.
The NPG method only constrains the violated
dimensions without taking the update consisten-
cy among dimensions of a word embedding into
account. For example, if many dimensions en-
counter xik + γVf(xk) &lt; 0 at the same time,
which are set to 0 with Equation (8) with other
</bodyText>
<equation confidence="0.396173">
[Iwt(u)−σ(u-wt)]uit.
</equation>
<page confidence="0.824356">
1688
</page>
<bodyText confidence="0.999282466666667">
dimensions unchanged, the updated word embed-
ding may heavily deviate from its semantic mean-
ing. Hence, NPG may suffer from instable updat-
ing results. To address this issue, we propose to
employ the following improved projected gradient
method.
Improved Projected Gradient (IPG). In order
to make the non-negative update more consistent
among dimensions, we design an improved pro-
jected gradient by iteratively finding the most ap-
propriate learning rate γ. The basic idea is that,
we will find a good learning rate γ to make less
dimensions violate the non-negative constraint.
More specifically, in Equation (6), for a learning
rate γ, we define the violation ratio as
</bodyText>
<equation confidence="0.9737675">
���
�{k|xi k &gt; 0,xi k + γVf(xk) &lt; 01
, (9)
K
</equation>
<bodyText confidence="0.999968909090909">
where K is the dimension size of word embed-
dings. The violation ratio indicates how many di-
mensions violate the non-negative constraint and
require to be set to 0. When the learning rate γ de-
creases, the violation ratio will also decrease, and
the zero-setting in Equation (8) will bring less de-
viation to word embeddings.
We set a threshold δ for the violation ratio R(γ)
and a lower bound γL for the learning rate γ. S-
tarting from an initial learning rate γ0, we will re-
peatedly decrease the learning rate by
</bodyText>
<equation confidence="0.993686333333333">
γm+1 = γm · β (10)
with 0 &lt; β &lt; 1 until
R(γm+1) &lt; δ or γm+1 &lt;_ γL, (11)
</equation>
<bodyText confidence="0.9985775">
and then update with Equation (8) using γm+1.
In nature, the updating constraint of learning rate
in Equation (11) play a similar role to Equation
(13) in (Lin, 2007), which aims to prevent the pro-
jection operation from heavily deviating the word
embeddings.
</bodyText>
<subsectionHeader confidence="0.995387">
2.3 More Optimization Details
</subsectionHeader>
<bodyText confidence="0.9944566875">
In experiments, we explore many optimization
methods and find the following two strategies
are important: (1) Adaptive Gradient Descen-
t. Following the idea from (Sun et al., 2012),
we maintain different learning rates γw for each
word w, and the learning rates for those high-
frequency words may decrease faster than those
low-frequency words. This will speedup the con-
vergence of word embedding learning. (2) Unified
Word Embedding Space. Different from original
Skip-Gram (Mikolov et al., 2013b) which learn
embeddings of wt and its context words wt+j in
two separate spaces, in this paper both wt and it-
s context words wt+j share the same embedding
space. Hence, a word embedding may get more
opportunities for learning.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.986658153846154">
In this section, we investigate the representation
performance and interpretability of our OIWE
models with other baselines including typical N-
N and MF methods.
The representation performance is evaluated
with the word similarity computation task, and the
interpretability is evaluated with the word intru-
sion detection task. For the both tasks, we train our
OIWE models using the text8 corpus obtained
from word2vec website1, and the OIWE models
achieve the best performance by setting the dimen-
sion number K = 300, β = 0.6, δ = 1/60, and
γL = 2.5 x 10−6.
</bodyText>
<subsectionHeader confidence="0.998976">
3.1 Word Similarity Computation
</subsectionHeader>
<bodyText confidence="0.99831564">
Following the settings in (Murphy et al., 2012),
we also select the following three sets for word
similarity computation: (1) WS-203, the strict-
similarity subset of 203 pairs (Agirre et al., 2009)
selected from the wordsim-353 (Finkelstein et al.,
2001), (2) RG-65, 65 concrete word pairs built
by (Rubenstein and Goodenough, 1965) and (3)
MEN, 3, 000 word pairs built by (Bruni et al.,
2014). The performance is evaluated with the S-
pearman coefficient between human judgements
and similarities calculated using word embed-
dings.
We select three baselines including Skip-Gram
(Mikolov et al., 2013b), recurrent neural networks
(RNN) (Mikolov et al., 2011) and NNSE (Mur-
phy et al., 2012). For Skip-Gram, we report the
result we learned using word2vec on text8 cor-
pus. The result of RNN is from (Faruqui and Dyer,
2014) and the one of NNSE is from (Murphy et al.,
2012).
The evaluation results of word similarity com-
putation are shown in Table 1. We can ob-
serve that: (1) The OIWE models consistently
outperform other baselines. (2) IPG generally
achieves better representation performance than
</bodyText>
<footnote confidence="0.592221">
1https://code.google.com/p/word2vec/
</footnote>
<equation confidence="0.983831">
R(γ) =
</equation>
<page confidence="0.984993">
1689
</page>
<table confidence="0.9993605">
Model WS-203 RG-65 MEN
Skip-Gram 67.35 50.49 52.56
RNN 49.28 50.19 43.44
NNSE 51.06 56.48 -
OIWE-NPG 63.71 56.85 57.60
OIWE-IPG 71.74 57.16 56.68
</table>
<tableCaption confidence="0.934728">
Table 1: Spearman coefficient results (%) on word
similarity computation.
</tableCaption>
<bodyText confidence="0.8915298">
NPG. This indicates consistent updates are im-
portant for learning of word embeddings. One
can refer to http://github.com/skTim/
OIWE for the evaluation results on more evalua-
tion datasets.
</bodyText>
<subsectionHeader confidence="0.996542">
3.2 Word Intrusion Detection
</subsectionHeader>
<bodyText confidence="0.993204233333333">
We evaluate interpretability of word embeddings
with the task of word intrusion detection proposed
by (Murphy et al., 2012). In this task, for each
dimension we create a word set containing top-5
words in this dimension, and intruce a noisy word
from the bottom half of this dimension which
ranks high in other dimensions. Human editors
are asked to check each word set and try to pick
out the intrusion words, and the detection preci-
sion indicates the interpretability of word embed-
ding models. Note that, for this task we do not
perform normalization for word vectors.
Table 2: Experiment results (%) on word intrusion
detection.
The evaluation results are shown in Table 2. We
can observe that: (1) Skip-Gram performs poor
in word intrusion detection without doubt since it
is uninterpretable in nature. (2) The OIWE-NPG
model achieves better interpretability as compared
to Skip-Gram, but performs much worse than
the OIWE-IPG model. The OIWE-IPG model
achieves competitive interpretability with NNSE.
This indicates that reducing violation rations in
word embedding learning is crucial for preserving
interpretability.
In Table 3, we show top-5 words for some
dimensions, which clearly demonstrate semantic
meanings of these dimensions. One can also
refer to http://github.com/skTim/OIWE
to find top-5 words for all dimensions.
</bodyText>
<table confidence="0.929616625">
No. Top Words
1 type, form, way, kind, manner
2 translates, describes, combines, includ-
ed, includes
3 gospel, baptism, jesus, faith, judaism
4 Franz, Johann, Wilhelm, Friedrich, von
25 prominent, famous, important, influen-
tial, popular
</table>
<tableCaption confidence="0.9885375">
Table 3: Top words of some dimensions in word
embeddings.
</tableCaption>
<subsectionHeader confidence="0.9995">
3.3 Influence of Dimension Numbers
</subsectionHeader>
<bodyText confidence="0.9999736">
The dimension number is an important configura-
tion in word embeddings. In Fig. 1 we show the
performance of OIWE and Skip-Gram on word
similarity computation with varying dimension
numbers.
From the figure, we can observe that: (1) The
both models achieve their best performance un-
der the same dimension number. This indicates
that OIWE, to some extent, inherits the represen-
tation power of Skip-Gram. (2) The performance
of OIWE seems to be more sensitive to dimension
numbers. When the dimension number changes
from 300 to 200 or 400, the performance drops
much quickly than Skip-Gram. The reason may
be as follows. OIWE has to concern about both
representation ability of word embeddings and in-
terpretability of each dimension. An appropri-
ate dimension number is critical to make each di-
mension interpretable, just like the cluster num-
ber is important for clustering. On the contrary,
Skip-Gram is much free to learn word embeddings
only concerning about representation ability. (3)
The performance of OIWE with various dimen-
sions also varies on different evaluation dataset-
s. For example, OIWE-IPG with K = 400 get-
s 68.74 on MEN, which is much better than that
with K = 300. In future work, we will exten-
sively investigate the characteristics of OIWE with
respect to dimension numbers and other hyperpa-
rameters.
</bodyText>
<sectionHeader confidence="0.985994" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.985654">
In this paper, we present online interpretable word
embeddings. The OIWE models perform project-
</bodyText>
<figure confidence="0.960666909090909">
Model
Precision
Skip-Gram
32.62
NNSE
92.00
61.40
OIWE-NPG
94.80
OIWE-IPG
1690
</figure>
<figureCaption confidence="0.976611">
Figure 1: Influence of Dimension Number on
Words Similarity
</figureCaption>
<bodyText confidence="0.999963523809524">
ed gradient descent to apply non-negative con-
straints on NN methods such as Skip-Gram. Ex-
periment results on word similarity computation
and word intrusion detection demonstrate the ef-
fectiveness and efficiency of our models in both
representation ability and interpretability. We al-
so note that, our models can be easily extended to
other NN methods.
In future, we will explore the following re-
search issues: (1) We will extensively investigate
the characteristics of OIWE with respect to var-
ious hyperparameters including dimension num-
bers. (2) We will evaluate the performance of
our OIWE models in various NLP applications.
(3) We will also investigate possible extensions of
our OIWE models, including multiple-prototype
models for word sense embeddings (Huang et al.,
2012; Chen et al., 2014), semantic composition-
s for phrase embeddings (Zhao et al., 2015) and
knowledge representation (Bordes et al., 2013; Lin
et al., 2015).
</bodyText>
<sectionHeader confidence="0.997016" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996922">
Zhiyuan Liu and Maosong Sun are supported by
National Key Basic Research Program of Chi-
na (973 Program 2014CB340500) and Nation-
al Natural Science Foundation of China (NSFC
No. 62102140). Huanbo Luan is supported by
the National Natural Science Foundation of Chi-
na (NSFC No. 61303075). This research is al-
so supported by the Singapore National Research
Foundation under its International Research Cen-
tre@Singapore Funding Initiative and adminis-
tered by the IDM Programme.
</bodyText>
<sectionHeader confidence="0.981735" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999286346153846">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19–27.
Yoshua Bengio, Holger Schwenk, Jean-Sebastien
Senecal, Frederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137–186.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS, pages
2787–2795.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. JAIR,
49:1–47.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of EMNLP, pages
1025–1035.
Manaal Faruqui and Chris Dyer. 2014. Community
evaluation and exchange of word vectors at word-
vectors.org. In Proceedings of ACL System Demon-
strations.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406–
414. ACM.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873–882.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788–791.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of AAAI.
Chuan-bi Lin. 2007. Projected gradient methods for
nonnegative matrix factorization. Neural computa-
tion, 19(10):2756–2779.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
Jan Honza Cernocky, and Sanjeev Khudanpur.
2011. Extensions of recurrent neural network lan-
guage model. In Proceedings of ICASSP, pages
5528–5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
</reference>
<page confidence="0.803402">
1691
</page>
<reference confidence="0.998364740740741">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Brian Murphy, Partha Pratim Talukdar, and Tom M
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In Proceedings of COLING, pages 1933–
1950.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of EMNLP, 12:1532–
1543.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of ACL, pages 253–262.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384–394.
Yu Zhao, Zhiyuan Liu, and Maosong Sun. 2015.
Phrase type sensitive tensor indexing model for se-
mantic composition. In Proceedings of AAAI.
</reference>
<page confidence="0.994172">
1692
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864307">
<title confidence="0.999962">Online Learning of Interpretable Word Embeddings</title>
<author confidence="0.995549">Zhiyuan Huanbo Maosong</author>
<affiliation confidence="0.990638">of Computer Science and Technology, State Key Lab on Intelligent Technology and National Lab for Information Science and Technology, Tsinghua University, Beijing,</affiliation>
<address confidence="0.985063">Collaborative Innovation Center for Language Competence, Jiangsu, China</address>
<abstract confidence="0.99478955">Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code this paper can be obtained from</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="10590" citStr="Agirre et al., 2009" startWordPosition="1733" endWordPosition="1736">representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1, and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 x 10−6. 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Far</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of HLT-NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-Sebastien Senecal</author>
<author>Frederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<contexts>
<context position="1616" citStr="Bengio et al., 2006" startWordPosition="228" endWordPosition="231">e state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact </context>
</contexts>
<marker>Bengio, Schwenk, Senecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-Sebastien Senecal, Frederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2787--2795</pages>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Proceedings of NIPS, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>JAIR,</journal>
<pages>49--1</pages>
<contexts>
<context position="10786" citStr="Bruni et al., 2014" startWordPosition="1766" endWordPosition="1769">E models using the text8 corpus obtained from word2vec website1, and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 x 10−6. 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consist</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. JAIR, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>A unified model for word sense representation and disambiguation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1025--1035</pages>
<contexts>
<context position="16011" citStr="Chen et al., 2014" startWordPosition="2586" endWordPosition="2589">e effectiveness and efficiency of our models in both representation ability and interpretability. We also note that, our models can be easily extended to other NN methods. In future, we will explore the following research issues: (1) We will extensively investigate the characteristics of OIWE with respect to various hyperparameters including dimension numbers. (2) We will evaluate the performance of our OIWE models in various NLP applications. (3) We will also investigate possible extensions of our OIWE models, including multiple-prototype models for word sense embeddings (Huang et al., 2012; Chen et al., 2014), semantic compositions for phrase embeddings (Zhao et al., 2015) and knowledge representation (Bordes et al., 2013; Lin et al., 2015). Acknowledgments Zhiyuan Liu and Maosong Sun are supported by National Key Basic Research Program of China (973 Program 2014CB340500) and National Natural Science Foundation of China (NSFC No. 62102140). Huanbo Luan is supported by the National Natural Science Foundation of China (NSFC No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered</context>
</contexts>
<marker>Chen, Liu, Sun, 2014</marker>
<rawString>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of EMNLP, pages 1025–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Community evaluation and exchange of word vectors at wordvectors.org.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="11210" citStr="Faruqui and Dyer, 2014" startWordPosition="1834" endWordPosition="1837">09) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1https://code.google.com/p/word2vec/ R(γ) = 1689 Model WS-203 RG-65 MEN Skip-Gram 67.35 50.49 52.56 RNN 49.28 50.19 43.44 NNSE 51.06 56.48 - OIWE-NPG 63.71 56.85 57.60 OIWE-IPG 71.74 57.16 56.68 Table 1: Spearman coefficient results (%) on word similarity computation. NPG. This indicates consistent updates are important fo</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Community evaluation and exchange of word vectors at wordvectors.org. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10647" citStr="Finkelstein et al., 2001" startWordPosition="1741" endWordPosition="1744">d similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1, and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 x 10−6. 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of WWW, pages 406– 414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="15991" citStr="Huang et al., 2012" startWordPosition="2582" endWordPosition="2585">ction demonstrate the effectiveness and efficiency of our models in both representation ability and interpretability. We also note that, our models can be easily extended to other NN methods. In future, we will explore the following research issues: (1) We will extensively investigate the characteristics of OIWE with respect to various hyperparameters including dimension numbers. (2) We will evaluate the performance of our OIWE models in various NLP applications. (3) We will also investigate possible extensions of our OIWE models, including multiple-prototype models for word sense embeddings (Huang et al., 2012; Chen et al., 2014), semantic compositions for phrase embeddings (Zhao et al., 2015) and knowledge representation (Bordes et al., 2013; Lin et al., 2015). Acknowledgments Zhiyuan Liu and Maosong Sun are supported by National Key Basic Research Program of China (973 Program 2014CB340500) and National Natural Science Foundation of China (NSFC No. 62102140). Huanbo Luan is supported by the National Natural Science Foundation of China (NSFC No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiat</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="3140" citStr="Lee and Seung, 1999" startWordPosition="460" endWordPosition="463">, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE realizes interpretable word embeddings by applying non-negative constraints for word embeddings. Although NNSE learns word embeddings with good interpretabilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data. Inspired by the characteristics of NMF methods (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent (Lin, 2007) for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also employ adaptive gradient descent (Sun et al., 2012) to sp</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D Lee and H Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yankai Lin</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Yang Liu</author>
<author>Xuan Zhu</author>
</authors>
<title>Learning entity and relation embeddings for knowledge graph completion.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>Lin, Liu, Sun, Liu, Zhu, 2015</marker>
<rawString>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan-bi Lin</author>
</authors>
<title>Projected gradient methods for nonnegative matrix factorization.</title>
<date>2007</date>
<booktitle>Neural computation,</booktitle>
<pages>19--10</pages>
<contexts>
<context position="3584" citStr="Lin, 2007" startWordPosition="527" endWordPosition="528">for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data. Inspired by the characteristics of NMF methods (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent (Lin, 2007) for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also employ adaptive gradient descent (Sun et al., 2012) to speedup learning convergence. We name the proposed models as online interpretable word embeddings (OIWE). 1687 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. For experiments, we implement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word simi</context>
<context position="6857" citStr="Lin, 2007" startWordPosition="1099" endWordPosition="1100"> note that, the learning rate γ in Skip-Gram is shared by all word embeddings. 2.2 OIWE In order to learn interpretable word embeddings, we have to make the word embeddings learned in Skip-Gram keep non-negative. In order to achieve this goal, we have to constrain the update rules in Equation (4) and (5) as follows: xi+1 k = P [xik + γVf(xk)], (6) where x may be u or wt, k is the corresponding dimension in word embedding x, Vf(xk) indicates the gradient corresponding to xk, and P[-] is defined as ( x if x &gt; 0, P[x] = (7) 0 if x ≤ 0. Motivated by the projected gradient descent methods for NMF (Lin, 2007), in this paper we propose two methods for Skip-Gram to realize the constraint in Equation (6). Naive Projected Gradient (NPG). In NPG, we consider the most straightforward update strategy by simply setting xi+1 k = max (0, xik + γVf(xk)). (8) The method has been used for NMF (Lin, 2007) although the details are not discussed. The NPG method only constrains the violated dimensions without taking the update consistency among dimensions of a word embedding into account. For example, if many dimensions encounter xik + γVf(xk) &lt; 0 at the same time, which are set to 0 with Equation (8) with other [</context>
<context position="8945" citStr="Lin, 2007" startWordPosition="1469" endWordPosition="1470"> require to be set to 0. When the learning rate γ decreases, the violation ratio will also decrease, and the zero-setting in Equation (8) will bring less deviation to word embeddings. We set a threshold δ for the violation ratio R(γ) and a lower bound γL for the learning rate γ. Starting from an initial learning rate γ0, we will repeatedly decrease the learning rate by γm+1 = γm · β (10) with 0 &lt; β &lt; 1 until R(γm+1) &lt; δ or γm+1 &lt;_ γL, (11) and then update with Equation (8) using γm+1. In nature, the updating constraint of learning rate in Equation (11) play a similar role to Equation (13) in (Lin, 2007), which aims to prevent the projection operation from heavily deviating the word embeddings. 2.3 More Optimization Details In experiments, we explore many optimization methods and find the following two strategies are important: (1) Adaptive Gradient Descent. Following the idea from (Sun et al., 2012), we maintain different learning rates γw for each word w, and the learning rates for those highfrequency words may decrease faster than those low-frequency words. This will speedup the convergence of word embedding learning. (2) Unified Word Embedding Space. Different from original Skip-Gram (Mik</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Chuan-bi Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural computation, 19(10):2756–2779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Honza Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11048" citStr="Mikolov et al., 2011" startWordPosition="1803" endWordPosition="1806"> et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1https://code.google.com/p/word2vec/ R(γ) = 1689 Model WS-203 RG-65 MEN Skip-Gram 67.35 50.49 52.56 RNN 49.28 50.19 43.44 NNSE 51.06 56.48 - OIWE-NPG 63.71 56.85 </context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Honza Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Proceedings of ICASSP, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<contexts>
<context position="2157" citStr="Mikolov et al., 2013" startWordPosition="308" endWordPosition="311">hes for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE rea</context>
<context position="4686" citStr="Mikolov et al., 2013" startWordPosition="690" endWordPosition="693">plement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE. We also evaluate the interpretability performance on the word intrusion detection task. The results demonstrate the effectiveness of OIWE as compared to NNSE. 2 Our Model In this section, we first introduce Skip-Gram and then introduce the proposed online interpretable word embeddings based on Skip-Gram. 2.1 Skip-Gram Skip-Gram (Mikolov et al., 2013b) is simple and effective to learn word embeddings. The objective of Skip-Gram is to make word vectors good at predicting its context words. More specifically, given a word sequence {w1, w2, ... , wT}, SkipGram aims to maximize the average log probability 1 X log Pr(wt+j|wt)), (1) 1 ( −k&lt;j&lt;k,j70 where k is the context window size, and Pr(wt+j|wt) indicates the probability of seeing wt+j in the context of wt, which are measured with softmax function exp �wt+j - wt � Pr(wt+j|wt) = PwEW exp �w - wt~, (2) where wt+j and wt are word embeddings of wt+j and wt, and W is the vocabulary size. Since th</context>
<context position="9562" citStr="Mikolov et al., 2013" startWordPosition="1562" endWordPosition="1565">07), which aims to prevent the projection operation from heavily deviating the word embeddings. 2.3 More Optimization Details In experiments, we explore many optimization methods and find the following two strategies are important: (1) Adaptive Gradient Descent. Following the idea from (Sun et al., 2012), we maintain different learning rates γw for each word w, and the learning rates for those highfrequency words may decrease faster than those low-frequency words. This will speedup the convergence of word embedding learning. (2) Unified Word Embedding Space. Different from original Skip-Gram (Mikolov et al., 2013b) which learn embeddings of wt and its context words wt+j in two separate spaces, in this paper both wt and its context words wt+j share the same embedding space. Hence, a word embedding may get more opportunities for learning. 3 Experiments In this section, we investigate the representation performance and interpretability of our OIWE models with other baselines including typical NN and MF methods. The representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train ou</context>
<context position="10990" citStr="Mikolov et al., 2013" startWordPosition="1795" endWordPosition="1798"> Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1https://code.google.com/p/word2vec/ R(γ) = 1689 Model WS-203 RG-65 MEN Skip-Gram 67.35 50.49 52.56 RNN </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2157" citStr="Mikolov et al., 2013" startWordPosition="308" endWordPosition="311">hes for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE rea</context>
<context position="4686" citStr="Mikolov et al., 2013" startWordPosition="690" endWordPosition="693">plement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE. We also evaluate the interpretability performance on the word intrusion detection task. The results demonstrate the effectiveness of OIWE as compared to NNSE. 2 Our Model In this section, we first introduce Skip-Gram and then introduce the proposed online interpretable word embeddings based on Skip-Gram. 2.1 Skip-Gram Skip-Gram (Mikolov et al., 2013b) is simple and effective to learn word embeddings. The objective of Skip-Gram is to make word vectors good at predicting its context words. More specifically, given a word sequence {w1, w2, ... , wT}, SkipGram aims to maximize the average log probability 1 X log Pr(wt+j|wt)), (1) 1 ( −k&lt;j&lt;k,j70 where k is the context window size, and Pr(wt+j|wt) indicates the probability of seeing wt+j in the context of wt, which are measured with softmax function exp �wt+j - wt � Pr(wt+j|wt) = PwEW exp �w - wt~, (2) where wt+j and wt are word embeddings of wt+j and wt, and W is the vocabulary size. Since th</context>
<context position="9562" citStr="Mikolov et al., 2013" startWordPosition="1562" endWordPosition="1565">07), which aims to prevent the projection operation from heavily deviating the word embeddings. 2.3 More Optimization Details In experiments, we explore many optimization methods and find the following two strategies are important: (1) Adaptive Gradient Descent. Following the idea from (Sun et al., 2012), we maintain different learning rates γw for each word w, and the learning rates for those highfrequency words may decrease faster than those low-frequency words. This will speedup the convergence of word embedding learning. (2) Unified Word Embedding Space. Different from original Skip-Gram (Mikolov et al., 2013b) which learn embeddings of wt and its context words wt+j in two separate spaces, in this paper both wt and its context words wt+j share the same embedding space. Hence, a word embedding may get more opportunities for learning. 3 Experiments In this section, we investigate the representation performance and interpretability of our OIWE models with other baselines including typical NN and MF methods. The representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train ou</context>
<context position="10990" citStr="Mikolov et al., 2013" startWordPosition="1795" endWordPosition="1798"> Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1https://code.google.com/p/word2vec/ R(γ) = 1689 Model WS-203 RG-65 MEN Skip-Gram 67.35 50.49 52.56 RNN </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Pratim Talukdar</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning effective and interpretable semantic models using non-negative sparse embedding.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2747" citStr="Murphy et al., 2012" startWordPosition="398" endWordPosition="402">l (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE realizes interpretable word embeddings by applying non-negative constraints for word embeddings. Although NNSE learns word embeddings with good interpretabilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data. Inspired by the characteristics of NMF methods (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabi</context>
<context position="10441" citStr="Murphy et al., 2012" startWordPosition="1709" endWordPosition="1712"> we investigate the representation performance and interpretability of our OIWE models with other baselines including typical NN and MF methods. The representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1, and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 x 10−6. 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al.</context>
<context position="12096" citStr="Murphy et al., 2012" startWordPosition="1968" endWordPosition="1971">ance than 1https://code.google.com/p/word2vec/ R(γ) = 1689 Model WS-203 RG-65 MEN Skip-Gram 67.35 50.49 52.56 RNN 49.28 50.19 43.44 NNSE 51.06 56.48 - OIWE-NPG 63.71 56.85 57.60 OIWE-IPG 71.74 57.16 56.68 Table 1: Spearman coefficient results (%) on word similarity computation. NPG. This indicates consistent updates are important for learning of word embeddings. One can refer to http://github.com/skTim/ OIWE for the evaluation results on more evaluation datasets. 3.2 Word Intrusion Detection We evaluate interpretability of word embeddings with the task of word intrusion detection proposed by (Murphy et al., 2012). In this task, for each dimension we create a word set containing top-5 words in this dimension, and intruce a noisy word from the bottom half of this dimension which ranks high in other dimensions. Human editors are asked to check each word set and try to pick out the intrusion words, and the detection precision indicates the interpretability of word embedding models. Note that, for this task we do not perform normalization for word vectors. Table 2: Experiment results (%) on word intrusion detection. The evaluation results are shown in Table 2. We can observe that: (1) Skip-Gram performs po</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Pratim Talukdar, and Tom M Mitchell. 2012. Learning effective and interpretable semantic models using non-negative sparse embedding. In Proceedings of COLING, pages 1933– 1950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>12--1532</pages>
<contexts>
<context position="1739" citStr="Pennington et al., 2014" startWordPosition="246" endWordPosition="249">paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what e</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of EMNLP, 12:1532– 1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="10725" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1753" endWordPosition="1756">th the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1, and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 x 10−6. 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Houfeng Wang</author>
<author>Wenjie Li</author>
</authors>
<title>Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>253--262</pages>
<contexts>
<context position="3734" citStr="Sun et al., 2012" startWordPosition="551" endWordPosition="554">s (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent (Lin, 2007) for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also employ adaptive gradient descent (Sun et al., 2012) to speedup learning convergence. We name the proposed models as online interpretable word embeddings (OIWE). 1687 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. For experiments, we implement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE.</context>
<context position="9247" citStr="Sun et al., 2012" startWordPosition="1513" endWordPosition="1516">n initial learning rate γ0, we will repeatedly decrease the learning rate by γm+1 = γm · β (10) with 0 &lt; β &lt; 1 until R(γm+1) &lt; δ or γm+1 &lt;_ γL, (11) and then update with Equation (8) using γm+1. In nature, the updating constraint of learning rate in Equation (11) play a similar role to Equation (13) in (Lin, 2007), which aims to prevent the projection operation from heavily deviating the word embeddings. 2.3 More Optimization Details In experiments, we explore many optimization methods and find the following two strategies are important: (1) Adaptive Gradient Descent. Following the idea from (Sun et al., 2012), we maintain different learning rates γw for each word w, and the learning rates for those highfrequency words may decrease faster than those low-frequency words. This will speedup the convergence of word embedding learning. (2) Unified Word Embedding Space. Different from original Skip-Gram (Mikolov et al., 2013b) which learn embeddings of wt and its context words wt+j in two separate spaces, in this paper both wt and its context words wt+j share the same embedding space. Hence, a word embedding may get more opportunities for learning. 3 Experiments In this section, we investigate the repres</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection. In Proceedings of ACL, pages 253–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1225" citStr="Turian et al., 2010" startWordPosition="170" endWordPosition="173">ization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF appr</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Zhao</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Phrase type sensitive tensor indexing model for semantic composition.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>Zhao, Liu, Sun, 2015</marker>
<rawString>Yu Zhao, Zhiyuan Liu, and Maosong Sun. 2015. Phrase type sensitive tensor indexing model for semantic composition. In Proceedings of AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
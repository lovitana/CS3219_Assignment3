<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000368">
<title confidence="0.996593">
A Graph-based Readability Assessment Method using Word Coupling
</title>
<author confidence="0.921035">
Zhiwei Jiang, Gang Sun, Qing Gu∗, Tao Bai, Daoxu Chen
</author>
<affiliation confidence="0.846391">
State Key Laboratory for Novel Software Technology
Nanjing University, Nanjing 210023, China
</affiliation>
<email confidence="0.909119">
jiangzhiwei@outlook.com, sungangnju@163.com,
guq@nju.edu.cn, bt@xjau.edu.cn, cdx@nju.edu.cn
</email>
<sectionHeader confidence="0.994744" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973857142857">
This paper proposes a graph-based read-
ability assessment method using word
coupling. Compared to the state-of-the-
art methods such as the readability for-
mulae, the word-based and feature-based
methods, our method develops a coupled
bag-of-words model which combines the
merits of word frequencies and text fea-
tures. Unlike the general bag-of-words
model which assumes words are indepen-
dent, our model correlates the words based
on their similarities on readability. By
applying TF-IDF (Term Frequency and
Inverse Document Frequency), the cou-
pled TF-IDF matrix is built, and used in
the graph-based classification framework,
which involves graph building, merging
and label propagation. Experiments are
conducted on both English and Chinese
datasets. The results demonstrate both ef-
fectiveness and potential of the method.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991079568181818">
Readability assessment is a task that aims to eval-
uate the reading difficulty or comprehending easi-
ness of text documents. It is helpful for education-
ists to select texts appropriate to the reading/grade
levels of the students, and for web designers to or-
ganize texts on web pages for the users doing per-
sonalized searches for information retrieval.
Research on readability assessment starts from
the early 20th century (Dale and Chall, 1948).
Many useful readability formulae have been devel-
oped since then (Dale and Chall, 1948; McLaugh-
lin, 1969; Kincaid et al., 1975). Currently, due to
the development of natural language processing,
the methods on readability assessment have made
a great progress (Zakaluk and Samuels, 1988;
∗Corresponding author.
Benjamin, 2012; Gonzalez-Dios et al., 2014). The
word-based methods compute word frequencies in
documents to estimate their readability (Collins-
Thompson and Callan, 2004; Kidwell et al., 2009).
The feature-based methods extract text features
from documents and train classification models
to classify the readability (Schwarm and Osten-
dorf, 2005; Feng et al., 2010; Franc¸ois and Fairon,
2012; Hancke et al., 2012).
In this paper, we propose a graph-based method
using word coupling, which combines the mer-
its of both word frequencies and text features
for readability assessment. We design a cou-
pled bag-of-words model, which correlates words
based on their similarities on sentence-level read-
ability computed using text features. The model
is used in a graph-based classification frame-
work, which involves graph building, graph merg-
ing/combination, and label propagation. We per-
form experiments on datasets of both English and
Chinese. The results demonstrate both effective-
ness and potential of our method.
The rest of this paper is organized as follows:
Section 2 introduces backgrounds of our work.
Section 3 presents the details of the method. Sec-
tion 4 designs the experiments and explains the re-
sults. Finally, Section 5 concludes the paper with
planned future work.
</bodyText>
<sectionHeader confidence="0.990572" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9998675">
In this section, we introduce briefly three research
topics relevant to our work: readability assess-
ment, the bag-of-words model and the graph-
based label propagation method.
</bodyText>
<subsectionHeader confidence="0.918348">
2.1 Readability Assessment
</subsectionHeader>
<bodyText confidence="0.9986612">
Research on readability assessment has devel-
oped three types of methods: the readability for-
mula, the word-based methods and the feature-
based methods (Kincaid et al., 1975; Collins-
Thompson and Callan, 2004; Schwarm and Os-
</bodyText>
<page confidence="0.976572">
411
</page>
<note confidence="0.985141">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 411–420,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999452681818182">
tendorf, 2005). During the early time, many
well-known readability formulae have been devel-
oped to assess the readability of text documents
(Dale and Chall, 1948; McLaughlin, 1969; Kin-
caid et al., 1975). Surface text features are de-
fined in these formulae to measure both lexical
and grammatical complexities of a document. The
word-based methods focus on words and their fre-
quencies in a document to assess its readability,
which mainly include the unigram/bigram/n-gram
models (Collins-Thompson and Callan, 2004;
Schwarm and Ostendorf, 2005) and the word
acquisition model (Kidwell et al., 2009). The
feature-based methods focus on extracting text
features from a document and training a classifi-
cation model to classify its readability (Feng et
al., 2010; Franc¸ois and Fairon, 2012; Hancke et
al., 2012). Suitable text features are usually essen-
tial to the success of these methods. The Support
vector machine and logistic regression model are
two classification models commonly used in these
methods.
</bodyText>
<subsectionHeader confidence="0.991877">
2.2 The Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.999981357142857">
The bag-of-words model is mostly used for doc-
ument classification. It constructs a feature space
that contains all the distinct words in a language
(or the document set). A document is repre-
sented by a vector, whose components reflect the
weight of every distinct word contained in the doc-
ument. Normally, it assumes the words are inde-
pendent. Now the capturing of the relationship
among words has attracted considerable attention
(Wong et al., 1985; Cheng et al., 2013). Inspired
by these works, this paper adopts the bag-of-words
model in readability assessment, and refines the
model by computing similarity among words on
reading difficulty.
</bodyText>
<subsectionHeader confidence="0.9939515">
2.3 The Graph-based Label Propagation
Method
</subsectionHeader>
<bodyText confidence="0.999270888888889">
Graph-based label propagation is applied on a
graph to propagate class labels from labeled nodes
to unlabeled ones (Kim et al., 2013). It has been
successfully applied in various applications, such
as dictionary construction (Kim et al., 2013), word
segmentation and tagging (Zeng et al., 2013), and
sentiment classification (Ponomareva and Thel-
wall, 2012). Typically, a graph-based label propa-
gation method consists of two main steps: graph
construction and label propagation (Zeng et al.,
2013). During the first step, a similarity function
is required to build edges and compute weights
between pairs of the nodes (Daitch et al., 2009).
Some form of edge pruning is required to refine
the graph (Jebara et al., 2009). After that, effective
algorithms have been developed to propagate the
label distributions to all the nodes (Subramanya et
al., 2010; Kim et al., 2013).
</bodyText>
<sectionHeader confidence="0.987376" genericHeader="method">
3 The Proposed Method
</sectionHeader>
<bodyText confidence="0.999745230769231">
In this section, we present GRAW (Graph-based
Readability Assessment method using Word cou-
pling), which constructs a coupled bag-of-words
model by exploiting the correlation of readabil-
ity among the words. Unlike the general bag-of-
words model which models document relationship
on topic, the coupled bag-of-words model extends
it to model the relationship among documents on
readability. In the following sections, we describe
in detail how to build the coupled bag-of-words
model. The model is then used in the graph-
based classification framework for readability as-
sessment.
</bodyText>
<subsectionHeader confidence="0.999775">
3.1 The General Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.993958333333333">
TF-IDF (Term Frequency and Inverse Document
Frequency) is the most popular scheme of the bag-
of-words model. Given the set of documents D,
the TF-IDF matrix M can be calculated based on
the logarithmically scaled term (i.e. word) fre-
quency (Salton and Buckley, 1988) as follows.
</bodyText>
<equation confidence="0.9996255">
Mt,d = tft,d · idft,d
= (1 + log f(t, d)) · log j{djt E d}j
</equation>
<bodyText confidence="0.9997645">
where f(t, d) is the number of times that a term
(word) t occurs in a document d E D.
</bodyText>
<subsectionHeader confidence="0.999884">
3.2 The Coupled Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.9999045">
As shown in Figure 1, three main stages are
required to construct the coupled bag-of-words
model: per-sentence readability estimation, word
coupling matrix construction and coupled TF-IDF
matrix calculation. The following sections de-
scribe the details of these stages.
</bodyText>
<subsectionHeader confidence="0.880729">
3.2.1 Per-Sentence Readability Estimation
</subsectionHeader>
<bodyText confidence="0.999408">
Two steps are required for the per-sentence read-
ability estimation. The first is to compute a read-
ing score of a sentence by heuristic functions. The
second is to determine the difficulty level of the
sentence by discretizing the score.
</bodyText>
<equation confidence="0.875814">
jDj (1)
</equation>
<page confidence="0.995504">
412
</page>
<figureCaption confidence="0.999945">
Figure 1: The Framework of GRAW
</figureCaption>
<figure confidence="0.99934596">
Documents Sentences Levels
Documents
Documents
Documents Levels
Terms
Estimate the
readability of
sentences
Construct word
coupling matrix
Terms
Label the nodes
Calculate coupled
TF-IDF matrix
Label
propagation
Build graph
Construct
TF-IDF matrix
Terms
Terms
Extract
documents
Corpus
S
</figure>
<bodyText confidence="0.993849571428572">
Step 1. Given a sentence s, its reading diffi-
culty can be quantified as a reading score which is
a continuous variable denoted by r(s). The more
difficult s is, the greater r(s) will be. Based on
text features of s, r(s) can be computed by one of
the eight heuristic functions listed in Table 1 which
are grouped into three aspects.
</bodyText>
<subsectionHeader confidence="0.855017">
Aspect Function Description
</subsectionHeader>
<bodyText confidence="0.998000833333333">
Surface len(s) the length of the sentence s.
ans(s) the average number of syllables (or strokes for
Chinese) per word (or character for Chinese) in
s.
anc(s) the average number of characters per word in s.
Lexical lv(s) the number of distinct types of POS, i.e. part of
speech, in s.
atr(s) the ratio of adjectives in s.
ntr(s) the ratio of nouns in s.
Syntatic pth(s) the height of the syntax parser tree of s.
anp(s) the average number of (noun, verb, and preposi-
tion) phrases in s.
</bodyText>
<subsectionHeader confidence="0.946576">
3.2.2 Word Coupling Matrix Construction
</subsectionHeader>
<bodyText confidence="0.999647928571429">
Let V denote the set of all the words, a word cou-
pling matrix is defined as C* E R|V|x|V|, the ele-
ment of which reflects the correlation between two
words (i.e. terms). Two steps are required to con-
struct this matrix. The first is to count the difficulty
distributions of words, and the second is to com-
pute the correlation between each pair of words
according to the similarity of their difficulty dis-
tributions.
Step 1. Let S denote the set of all the sen-
tences, pt denote the difficulty distribution of a
word (term) t. pt is a vector containing η (i.e. the
number of difficulty levels) values, the i-th part of
which can be calculated by the following formula.
</bodyText>
<tableCaption confidence="0.962599">
Table 1: Three aspects of estimating reading diffi- 1 pt(i) = · δ(t E s) · δ(l*(s) = i) (3)
culty of sentences using heuristic functions nt sES
</tableCaption>
<bodyText confidence="0.999249588235294">
Step 2. Let η denote the pre-determined number
of difficulty levels, rmax and rmin denote the max-
imum and minimum reading score respectively of
all the sentences in D. To determine the difficulty
level l*(s) (l*(s) E [1,η]) of a sentence s, the
range [rmin, rmax] is divided into η intervals, so
that each interval contains the reading scores of 1η
of all the sentences. The assumption is that all the
sentences are equally distributed among the diffi-
culty levels. l*(s) will be i, if the reading score
r(s) resides in the i-th interval.
For each of the three aspects, we compute one
l*(s) for a sentence s by combining the heuristic
functions using the following equations. The as-
sumption is that the reading difficulty of a sentence
may be determined by the maximum measure on
the text features.
</bodyText>
<equation confidence="0.999925">
lsur(s) = max [llen(s), lans(s), lanc(s)]
llex(s) = max [llv(s), latr(s), lntr(s)] (2)
lsyn(s) = max [lpth(s), lanp(s)]
</equation>
<bodyText confidence="0.9999456">
where nt refers to the number of sentences in
which t appears. The indicator function δ(x) re-
turns 1 if x is true and 0 otherwise. l*(s) refers to
one of the functions lsur(s), llex(s) or lsyn(s).
Step 2. Given two words (terms) t1 and
t2, whose level distributions are pt1 and pt2 re-
spectively, we measure the distribution difference
cKL(t1, t2) using the Kullback-Leibler divergence
(Kullback and Leibler, 1951), computed by the
following formula.
</bodyText>
<equation confidence="0.995019333333333">
1
cKL(t1,t2) = 2 (KL(pt1||pt2) + KL(pt2||pt1))
(4)
</equation>
<bodyText confidence="0.9350792">
where KL(p||q) = Ei p(i) log p(i)
q(i). After that,
the logistic function is applied on the computed
difference to get the normalized distribution simi-
larity, i.e.
</bodyText>
<equation confidence="0.98537">
2
sim(t1, t2) = (5)
1 + ecKL(t1,t2)
</equation>
<bodyText confidence="0.999593">
Given a word ti, only λ other words with high-
est correlation (similarity) are selected to build the
</bodyText>
<page confidence="0.996379">
413
</page>
<bodyText confidence="0.9881065">
neighbor set of ti, denoted as N(ti). If a word tj
is not selected (i.e. tj ∈/ N(ti)), the correspond-
ing sim(ti7 tj) will be assigned 0. After that, the
word coupling matrix (i.e. C*) with sim(ti7 tj)
as elements is normalized along the rows so that
the sum of each row is 1. Based on three different
l*(s), we construct three word coupling matrices
Csur, Clex and Csyn.
</bodyText>
<subsectionHeader confidence="0.940625">
3.2.3 Coupled TF-IDF Matrix Calculation
</subsectionHeader>
<bodyText confidence="0.9999592">
In the general bag-of-words model, the words are
treated as independent of each other. However, for
readability assessment, words may be correlated
according to the similarity of their difficulty dis-
tributions. To improve the TF-IDF matrix M de-
scribed in Section 3.1, we multiply it by the word
coupling matrix C*, so that the term frequencies
are shared among the highly correlated (coupled)
words. We denote the coupled TF-IDF matrix as
M*, obtained by the following formula.
</bodyText>
<equation confidence="0.998131">
M* = C* · M (6)
</equation>
<bodyText confidence="0.999861666666667">
Specifically, three homogenous coupled TF-
IDF matrices Msur, Mlex and Msyn can be built
according to the three word coupling matrices C*.
</bodyText>
<subsectionHeader confidence="0.999506">
3.3 Graph-based Readability Assessment
</subsectionHeader>
<bodyText confidence="0.9999963">
We employ the coupled bag-of-words model for
readability assessment under the graph-based clas-
sification framework as described in the previ-
ous work (Zhu and Ghahramani, 2002). Firstly,
we construct a graph representing the readabil-
ity relationship among documents by using the
coupled bag-of-words model to compute the rela-
tions among these documents. Secondly, we esti-
mate reading levels of documents by applying la-
bel propagation on the graph.
</bodyText>
<subsectionHeader confidence="0.625093">
3.3.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999986666666667">
We build a directed graph G* to represent the read-
ability relation among documents, where nodes
represent documents, and edges are weighted by
the similarities between pairs of documents. Given
a similarity function, we link documents di to dj
with an edge of weight G*ij, defined as:
</bodyText>
<equation confidence="0.971758333333333">
Gi, * = I sim(di7 dj) if dj ∈ N(di)z)
j (7)
0 otherwise
</equation>
<bodyText confidence="0.999904">
where N(di) is the set of k-nearest neighbors of
di determined by the similarity function.
</bodyText>
<figure confidence="0.834383">
candidate neighbors
common neighbors
</figure>
<figureCaption confidence="0.999822">
Figure 2: Illustration of the graph merging strategy
</figureCaption>
<bodyText confidence="0.999909">
Given the coupled matrix M* ∈ Rmx|D |which
maps each document into a m-dimension feature
space, the similarity function sim(di7 dj) can be
defined by the Euclidean distance as follows.
</bodyText>
<equation confidence="0.993752">
1
sim(di7 dj) = V�Emk=1 (Mk,i − Mk,j)2 + c
(8)
</equation>
<bodyText confidence="0.998938">
where c is a small constant to avoid zero denomi-
nators.
Merge the three graphs Refer to Section 3.2,
the three coupled TF-IDF matrices will lead to
three different document graphs, denoted as Gsur,
Glex and Gsyn respectively. To take advantage of
the three aspects at one time, we need to merge the
three graphs into one, denoted as Gc.
In Gc, each node also keeps k neighbors, and
some edges shall be filtered out from the three
graphs. The basic idea is to remove edges con-
taining redundant information, as shown in Fig-
ure 2. For each node v, we firstly select the neigh-
bors which are common in all the three graphs (i.e.
Nsur(v) ∩Nlex(v) ∩Nsyn(v)). Secondly, for the
rest candidate nodes, which are the neighbors of
v in at least one graph, we select one by one the
node which possesses the least number of com-
mon neighbors (from all the three graphs) with the
nodes that are already selected in Nc(v). The ob-
jective is to keep the number of triangles in Gc to
a minimum. The edge weights of Gc are averaged
on the corresponding edges appeared in the three
graphs.
Combine with the feature-based graph Previ-
ous studies usually extract text features from doc-
uments to assess the readability using classifica-
tion models. Here, we also take into consideration
the feature-based graph, where similarities among
documents are computed on text features. We use
the features defined in (Jiang et al., 2014), where
the model based features are eliminated since the
</bodyText>
<figure confidence="0.3400865">
v
0
2 4 0 1
3
</figure>
<page confidence="0.993761">
414
</page>
<bodyText confidence="0.999968888888889">
computation depends on pre-assigned class labels,
and represent a document as a vector of the feature
values. We compute the similarity between any
pair of documents using the Euclidean distance,
and built the feature-based graph (denoted as Gf)
in the same way as above.
Additionally, to take advantage of both graphs,
we combine them into one (denoted as Gcf) using
the following formula.
</bodyText>
<equation confidence="0.874125">
Gcf
i,j = max [Gci,j, Gfi,j] (9)
</equation>
<subsubsectionHeader confidence="0.519886">
3.3.2 Graph Propagation
</subsubsectionHeader>
<bodyText confidence="0.999900272727273">
Given a graph G∗ constructed in previous sections,
its nodes are divided into two sets: the labeled set
Vl and the unlabeled set Vu. The goal of label
propagation is to propagate class labels from the
labeled nodes (i.e. documents) to the entire graph.
Here, we use a simplified version of the label prop-
agation method presented in (Subramanya et al.,
2010), which has been proved effective (Kim et
al., 2013). The method iteratively updates the la-
bel distribution on a document node using the fol-
lowing equation.
</bodyText>
<equation confidence="0.980638666666667">
⎛
pdi)(l) = 1 red pd(l)δ(d ∈ Vl) +
v∈N(d)
</equation>
<bodyText confidence="0.9853545">
At the left side of Eq.10, p(i)
d (l) is the afterward
probability of l (i.e. the class label) on a node d at
the i-th iteration. At the right side, κd is the nor-
malizing constant to make sure the sum of all the
probabilities is 1, and p0d(l) is the initial probabil-
ity of l on d if d is initially labeled (i.e. belonging
to the labeled set Vl). δ(x) is the indicator func-
tion. ,N(d) denotes the set of neighbors of d. The
iteration stops when the changes in p(i)
d (l) for all
the nodes and label values are small enough (e.g.
less than e−3), or i exceeds a predefined number
(e.g. greater than 30).
</bodyText>
<sectionHeader confidence="0.985907" genericHeader="method">
4 Empirical Studies
</sectionHeader>
<bodyText confidence="0.999731666666667">
In this section, we conduct experiments on
datasets of both English and Chinese, to investi-
gate the following three research questions:
</bodyText>
<listItem confidence="0.87998475">
RQ1: Whether the proposed method (i.e.
GRAW) outperforms the state-of-the-art methods
for readability assessment?
RQ2: What are the effects of adding the
word coupling matrix to the general bag-of-words
model?
RQ3: Whether the graph merging strategy is
effective, and whether the performance can be
</listItem>
<bodyText confidence="0.9836365">
further improved by combining the feature-based
graph.
</bodyText>
<subsectionHeader confidence="0.995008">
4.1 Corpus and Metrics
</subsectionHeader>
<bodyText confidence="0.9926067">
To evaluate our proposed method, we collected
two datasets. The first is CPT (Chinese primary
textbook) (Jiang et al., 2014), which contains Chi-
nese documents of six reading levels. The second
is ENCT (English New Concept textbook) which
contains English documents of four reading levels.
Both datasets are built from well-known textbooks
where documents are labeled as grade levels by
credible educationists. The details of the datasets
are listed in Table 2.
</bodyText>
<table confidence="0.999343666666667">
Dataset Language #Grade #Doc #Sent #Word
CPT Chinese 6 637 16145 234372
ENCT English 4 279 4671 62921
</table>
<tableCaption confidence="0.9304645">
Table 2: Statistics of the datasets on both English
and Chinese
</tableCaption>
<bodyText confidence="0.999940545454545">
We conduct experiments on both datasets us-
ing the cross-validation which randomly divides a
dataset into labeled (training) and unlabeled (test)
sets. The labeling proportion is varied to inves-
tigate the performance of GRAW under differ-
ent circumstances. To reduce variability, given
certain labeling proportion, 100 rounds of cross-
validation are performed, and the validation re-
sults are averaged over all the rounds. We choose
the precision (P), recall (R) and F1-measure (F1)
as the performance metrics.
</bodyText>
<sectionHeader confidence="0.6843265" genericHeader="method">
4.2 Comparison to the State-of-the-Art
Methods
</sectionHeader>
<bodyText confidence="0.9999648125">
To address RQ1, we implement the follow-
ing readability assessment methods and compare
GRAW to them: (1) SMOG (McLaughlin, 1969)
and FK (Kincaid et al., 1975) are two widely used
readability formulae. We reserve their core mea-
sures (i.e. text features, and number of strokes for
Chinese instead of number of syllables), and refine
the coefficients on both datasets to befit the read-
ing (grade) levels. (2) SUM (Collins-Thompson
and Callan, 2004) is a word-based method, which
trains one unigram model for each grade level, and
applies model smoothing both inter and intra the
grade levels. (3) LR and SVM refer to two feature-
based methods which incorporate text features de-
fined in (Jiang et al., 2014) to represent documents
as instances. The logistic regression model and
</bodyText>
<equation confidence="0.986666">
Gd,vp,(,�−1)(l)) (10)
</equation>
<page confidence="0.991559">
415
</page>
<table confidence="0.999987421052631">
Dataset Level Metric Methods
SMOG FK SUM LR SVM GRAWc GRAWcf
P 57.48 74.07 71.76 71.87 73.18 73.26 75.29
Gr.1 R 17.31 9.69 36.31 71.17 67.28 73.28 83.17
F1 26.14 15.25 47.67 71.23 69.70 72.98 78.89
P 34.73 31.44 37.94 51.62 50.78 52.05 55.83
Gr.2 R 31.06 28.00 29.73 56.48 59.45 57.36 66.06
F1 32.66 29.42 33.13 53.67 54.53 54.40 60.37
P 20.05 20.79 28.12 44.15 48.89 46.33 51.72
Gr.3 R 58.84 75.53 25.06 43.94 49.94 58.59 68.41
F1 29.89 32.40 26.35 43.72 49.04 51.57 58.74
P 25.06 28.94 25.60 33.35 33.92 39.90 44.15
CPT (Chinese) Gr.4 R 41.06 31.82 28.76 31.82 33.64 35.42 28.88
F1 31.03 29.69 26.91 32.24 33.58 37.32 34.57
P 33.57 45.00 28.71 37.70 37.30 45.02 37.33
Gr.5 R 4.00 2.71 34.41 36.12 34.29 27.12 19.00
F1 7.02 4.95 31.10 36.61 35.47 33.35 24.45
P 0.00 6.67 32.21 40.47 46.53 45.91 44.24
Gr.6 R 0.00 0.35 45.81 39.03 43.48 51.81 54.06
F1 0.00 0.67 37.55 39.48 44.65 48.38 48.15
P 28.48 34.48 37.39 46.53 48.43 50.41 51.43
Avg. R 25.38 24.68 33.35 46.43 48.01 50.60 53.26
F1 21.12 18.73 33.78 46.16 47.83 49.67 50.86
P 54.65 60.79 96.59 88.60 90.74 95.42 95.53
Gr.1 R 67.50 73.50 84.77 89.32 85.45 83.77 83.95
F1 60.18 66.36 90.09 88.64 87.76 89.01 89.18
P 50.11 56.23 78.30 85.51 90.80 88.60 89.03
Gr.2 R 59.28 63.93 35.07 86.07 92.86 96.76 96.86
F1 54.17 59.69 48.11 85.54 91.68 92.42 92.70
P 29.49 32.09 40.53 88.31 89.08 85.36 89.73
ENCT (English) Gr.3 R 24.22 26.94 68.33 86.17 84.78 94.17 96.56
F1 26.40 29.15 50.77 86.94 86.16 89.40 92.92
P 85.73 94.00 69.30 89.79 81.20 91.70 95.26
Gr.4 R 14.64 18.21 97.64 87.07 85.21 77.93 85.36
F1 24.06 29.46 80.81 88.02 81.79 83.84 89.81
P 55.00 60.78 71.18 88.05 87.95 90.27 92.39
Avg. R 41.41 45.65 71.45 87.16 87.08 88.16 90.68
F1 41.20 46.16 67.44 87.28 86.85 88.67 91.15
</table>
<tableCaption confidence="0.9462745">
Table 3: The average Precision, Recall and F1-measure (%) per reading level of the seven methods for
readability assessment on both datasets when the labeling proportion is 0.7
</tableCaption>
<bodyText confidence="0.997385379310345">
support vector machine are used as the classifiers
respectively.
For GRAW, we implement label propagation on
both the merged graph Gc and the final graph Gcf
(Section 3.3), denoted as GRAWc and GRAWcf
respectively. Table 3 gives the average perfor-
mance measure per reading level resulted by the
implemented methods on both datasets. Unless
otherwise specified, we fixed q to 3, and A to 2800
for CPT and 2000 for ENCT. The proportion of
the labeled (training) set is set to 0.7.
In Table 3, the precision, recall and F1-measure
of all the seven methods are calculated per read-
ing (grade) level on both English and Chinese
datasets. The values marked in bold in each row
refer to the maximum (best) measure gained by
the methods.
From Table 3, the readability formulae (SMOG
and FK) perform poorly on either the precision
or recall measure, and their F1-measure values
are generally the poorest. Both SMOG and FK
are designed for English, and have acceptable per-
formance on the English dataset ENCT. The un-
igram model (SUM) performs a little better than
the readability formulae. On ENCT, It has rel-
atively good performance on grade levels 1 and
4, while on the Chinese dataset CPT, the perfor-
mance is not satisfactory. The feature-based meth-
ods (LR and SVM) perform well on both ENCT
</bodyText>
<figureCaption confidence="0.765744">
Figure 3: The average F1-measure of the seven
methods on both datasets with the labeling pro-
portion varied from 0.1 to 0.9
</figureCaption>
<bodyText confidence="0.998898941176471">
and CPT, which means both the text features de-
veloped and the classifiers trained are useful. In
general, GRAWc performs better than both LR and
SVM, which demonstrates the effectiveness of our
method. In addition, by combining the feature-
based graph (GRAWcf), GRAW can be improved,
and performs the best on all the three metrics over
the majority of reading levels on both datasets.
The only exception is on level 5 in CPT, which
suggests the requirement of further improvements.
We study the effect of labeling proportion on the
performance of these methods on both datasets.
The F1-measure averaged over the reading levels
is used, since it is a good representative of the three
metrics according to Table 3. Figure 3 depicts the
performance trends of all the methods.
From Figure 3, neither SMOG nor FK benefits
</bodyText>
<figure confidence="0.998317117647059">
0.6
1
0.9
F1−measure (Chinese)
0.5
0.4
0.3
0.2
SMOG
FK
SUM
SVM
LR
GRAWc
GRAWcf
F1−measure (English)
0.8
0.7
0.6
0.5
0.4
SMOG
FK
SUM
SVM
LR
GRAWc
GRAWcf
0.1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Proportion of the labeled set
0.3
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Proportion of the labeled set
</figure>
<page confidence="0.658849">
416
</page>
<figure confidence="0.997356875">
(a) Comparison of the average F1-measure between the cou- (b) Comparison of the Recall rate per reading level between
pling and general TF-IDF matrices the coupling and general TF-IDF matrices
(c) The effects of η and λ on the performance of the word
(c) The effects of η and λ on the performance of the word (d) The effect of corpus size on the performance of the word
(d) The effect of corpus size on the performance of the word
coupling matrix
coupling matrix
coupling matrix coupling matrix
</figure>
<figureCaption confidence="0.989903">
Figure 4: Four perspectives on the effectiveness of the word coupling matrices
</figureCaption>
<figure confidence="0.999259453781513">
1
1
1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Proportion of the labeled set
F1−measure (Chinese)
F1−measure (English)
0.6
0.5
0.4
0.3
0.2
0.1
TF−IDF
Msur
Mlex
Msyn
0.8
0.8
TF−IDF
Msur
Mlex
Msyn
Recall (Chinese)
0.6
0.4
0.2
0
G1
G2
G3
Msyn
Recall (English)
0.6
0.4
0.2
0
G1
G2
Msyn
0.65
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.
Proportion of the labeled set
Reading level
G4
G5
G6
TF−IDF
Msur
Mlex
Graph
Reading level
G3
G4
TF−IDF
Mlex
0.9
0.85
0.8
0.75
0.7
Msur
Graph
Number of neighbors (λ)
Number of neighbors (λ)
10 20 30 40 50 60 70
Ratio of removed documents
Ratio of removed documents
0.9
0.5
0.9
0.5
0.88
0.475
F1−measure (Chinese)
0.85
F1−measure (English)
F1−measure (Chinese)
F1−measure (English)
0.4
0.86
0.45
0.8
0.84
0.3
η=2
η=3
η=4
η=5
η=6
η=7
η=8
η=9
0.425
0.75
0.2
0.4
0.7
0.1
0.375
0.78
Msyn
Msyn
0.35
400 800 1200 1600 2000 2400 2800 3200 3600 4000
0
0.65
0 10 20 30 40 50 60 70 80
η=2
η=3
0.82
η=4
η=5
0.8
η=6
η=7
η=8
η=9
0.76
200 400 600 800 1000 1200 1400 1600 1800 2000
</figure>
<bodyText confidence="0.998920260869565">
from the increasing size of the labeled set. This
suggests that the performance of the readability
formulae can hardly be improved by accumulat-
ing training data. The other 5 methods achieve
better performance on larger labeled set, and out-
perform the two formulae even if the labeling pro-
portion is small. Both LR and SVM perform bet-
ter than SUM, but the performance is not good
when the labeling proportion is less than 0.3, es-
pecially on the Chinese dataset. On the Chinese
dataset, SVM performs better than LR, while on
the English dataset, the situation is reversed. Both
versions of GRAW outperform the other methods
over the labeling ranges on both datasets. In ad-
dition, GRAW performs well when the labeling
proportion is still small. Again, by combining the
feature-based graph, the performance of GRAW is
consistently improved.
In summary, GRAW can outperform the state-
of-the-art methods for readability assessment on
both English and Chinese datasets. By combin-
ing the feature-based graph, the performance of
GRAW can be further improved.
</bodyText>
<subsectionHeader confidence="0.998175">
4.3 Effects of the Word Coupling Matrix
</subsectionHeader>
<bodyText confidence="0.999979285714285">
For RQ2, we firstly compare the coupled bag-of-
words model to the general model in the process
of graph construction. Four graphs are built by us-
ing each of the three word coupling matrices (i.e.
Msur, Mlex and Msyn) and the TF-IDF matrix
respectively. Label propagation is applied on each
graph to predict reading levels of unlabeled docu-
ments. The labeling proportion is varied from 0.1
to 0.9 on both the English and Chinese datasets.
Figure 4(a) depicts the average F1-measure re-
sulted from the four graphs.
From Figure 4(a), the three word coupling ma-
trices greatly outperform the TF-IDF matrix, espe-
cially on the Chinese dataset. This demonstrates
that the word coupling matrices are very effective
in improving the performance of the general bag-
of-words model for readability assessment.
Secondly, we investigate the performance of the
four matrices per reading level. Figure 4(b) de-
picts the recall rate per reading level of the four
corresponding graphs in bar charts. The labeling
proportion is set to 0.7. The recall rate is used
because it makes the reason evident that the TF-
IDF matrix performs poorly. From Figure 4(b), on
the Chinese dataset, nearly all the unlabeled docu-
ments are classified as level 1 by the TF-IDF ma-
trix, in which the word frequencies are too few to
make meaningful discrimination among the read-
ing levels. On the English dataset, the TF-IDF
matrix performs better, but still prefers to classify
documents into lower levels.
As described in Section 3.2.2, q (the number of
difficulty levels of sentences) and A (the number
of neighbors pertained for each document node)
are two parameters in building the word coupling
</bodyText>
<page confidence="0.995963">
417
</page>
<bodyText confidence="0.999991611111111">
matrices. To investigate their effects on the per-
formance of the built matrices, we vary the val-
ues of both q and A, and compute the average F1-
measure on the two datasets. Figure 4(c) depicts
the results in line charts, where q varies from 2 to 9
step by 1, while A varies from 400 to 4000 step by
400 on Chinese and from 200 to 2000 step by 200
on English (the difference is due to the dissimilar
number of documents between the two datasets).
The three word coupling matrices exhibit similar
behavior during experiments, hence, only Msyn is
depicted.
From Figure 4(c), a small q (e.g. 2 or 3) is good
on the Chinese dataset. However, on the English
dataset, q = 2 leads to the poorest performance. It
seems the increasing of q causes vibrated perfor-
mance, and the trend is further complicated when
involving A. Above all, q = 3 gives a prefer-
able option on both datasets. For A, most of the
lines exhibit a similar trend that rises first and then
keeps stable on both datasets, although some may
drop when A is too large. This suggests that mak-
ing a relatively large number of the other words as
the neighbors of one (i.e. A = 2800 on the Chi-
nese dataset and A = 2000 on the English dataset)
will make an effective word coupling matrix.
The word coupling matrix constructed in
GRAW uses the whole corpus on either English
or Chinese. To investigate if the corpus size takes
effects on the performance of GRAW, we vary
the proportion of the corpus used by randomly re-
moving documents from each reading level. Fig-
ure 4(d) depicts the average F1-measure resulted
by Msyn. The removing ratio is selected from
{0, 0.05, 0.1, 0.2, 0.4, 0.8}. Both the mean values
and deviations are shown on the line chart.
From Figure 4(d), on the Chinese dataset, the
performance of GRAW suffers little from remov-
ing documents, even if only 20% documents are
left for building the word coupling matrix. How-
ever, on the English dataset, the mean perfor-
mance drops sharply and the deviation increases
evidently. This suggests that cumulating sufficient
corpus is required for building a suitable word
coupling matrix in GRAW, and factors other than
number of documents may influence the corpus
quality, which deserves further study.
In summary, the word coupling matrix plays an
essential role in GRAW. For building a suitable
word coupling matrix, the number of difficulty
levels of sentences (q) can be set to 3, and a rel-
atively large number of the other words should be
selected as the neighbors of a word. A sufficient
corpus is required for refining the matrix.
</bodyText>
<subsectionHeader confidence="0.998953">
4.4 Effectiveness of Graph Combination
</subsectionHeader>
<bodyText confidence="0.993969541666667">
For RQ3, we compare graphs built on each sin-
gular word coupling matrix (i.e. Msur, Mlex and
Msyn) to the merged graph (i.e. GRAWc) and the
combined graph (i.e. GRAWcf). Figure 5 depicts
the average F1-measure resulted after applying la-
bel propagation on these graphs with labeling pro-
portion varied from 0.1 to 0.9. The feature-based
graph (i.e. Gf) is also depicted for comparison.
Figure 5: The average F1-measure of differ-
ent types of graphs on the English and Chinese
datasets
From Figure 5, the merged graph GRAWc out-
performs the three basic graphs on both datasets
in most cases. Within the three, Msyn performs
best, especially on the English dataset, where it
can outperform GRAWc slightly when the label-
ing proportion is small (0.2 − 0.4). By combining
the feature-based graph, GRAWcf performs even
better on both datasets, although Gf performs
poorest among all the graphs. In summary, the
graph merging strategy is effective, and by com-
bining the feature-based graph, the performance
of GRAW can be improved. This demonstrates the
potential of GRAW.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999859666666667">
In this paper, we propose a graph-based readabil-
ity assessment method using word coupling. The
coupled bag-of-words model is designed, which
exploits the correlation of readability among the
words, and by applying TF-IDF, models the rela-
tionship among documents on reading levels. The
model is employed in the graph-based classifica-
tion framework for readability assessment, which
involves graph building, merging, and label prop-
agation. Experiments are conducted on both Chi-
nese and English datasets. The results show that
our method can outperform the commonly used
</bodyText>
<figure confidence="0.992570444444445">
Proportion of the labeled set Proportion of the labeled set
F1−measure (Chinese)
0.55
0.45
Msur
0.4 Mlex
Msyn
Gf
0.35
GRAWc
GRAWcf
0.3
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
F1−measure (English)
0.85
0.75
0.9
0.8
0.7
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0
Msur
Mlex
Msyn
Gf
GRAWc
GRAWcf
</figure>
<page confidence="0.995093">
418
</page>
<bodyText confidence="0.999895230769231">
methods for readability assessment. In addition,
the evaluation demonstrates the potential of the
coupled bag-of-words model and the graph com-
bination/merging strategies.
In our future work, we plan to verify the sound-
ness of the results by applying our method on large
volume corpus of both English and Chinese. In ad-
dition, we will investigate other ways of comput-
ing the word coupling matrices, such as incorpo-
rating word coherency or semantics, and develop
efficient merging strategies which can be used for
training classification models, as well as for build-
ing graphs.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.979936666666667">
This work was supported by the National NSFC
projects under Grant Nos. 61373012, 61321491,
and 91218302.
</bodyText>
<sectionHeader confidence="0.998272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999509397727272">
Rebekah George Benjamin. 2012. Reconstructing
readability: Recent developments and recommenda-
tions in the analysis of text difficulty. Educational
Psychology Review, 24(1):63–88.
Xin Cheng, Duoqian Miao, Can Wang, and Longbing
Cao. 2013. Coupled term-term relation analysis for
document clustering. In Proceedings of the 2013
International Joint Conference on Neural Networks,
pages 1–8. IEEE.
Kevyn Collins-Thompson and James P Callan. 2004.
A language modeling approach to predicting reading
difficulty. In Proceedings of the 2004 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 193–200. Association for Computa-
tional Linguistics.
Samuel I Daitch, Jonathan A Kelner, and Daniel A
Spielman. 2009. Fitting a graph to vector data.
In Proceedings of the 26th Annual International
Conference on Machine Learning, pages 201–208.
ACM.
Edgar Dale and Jeanne S. Chall. 1948. A formula for
predicting readability. Educational Research Bul-
letin, 27(1):11–28.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No´emie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Posters, pages 276–284. As-
sociation for Computational Linguistics.
Thomas Franc¸ois and C´edrick Fairon. 2012. An ai
readability formula for french as a foreign language.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466–477. Association for Computational Lin-
guistics.
Itziar Gonzalez-Dios, Marıa Jes´us Aranzabe,
Arantza Dıaz de Ilarraza, and Haritz Salaberri.
2014. Simple or complex? assessing the readability
of basque texts. In Proceedings of the 25th Inter-
national Conference on Computational Linguistics,
pages 334–344. Association for Computational
Linguistics.
Julia Hancke, Sowmya Vajjala, and Detmar Meurers.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics, pages 1063–1080. Asso-
ciation for Computational Linguistics.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of the 26th An-
nual International Conference on Machine Learn-
ing, pages 441–448. ACM.
Zhiwei Jiang, Gang Sun, Qing Gu, and Daoxu Chen.
2014. An ordinal multi-class classification method
for readability assessment of chinese documents. In
Knowledge Science, Engineering and Management,
pages 61–72. Springer.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word ac-
quisition with application to readability prediction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
900–909. Association for Computational Linguis-
tics.
Doo Soon Kim, Kunal Verma, and Peter Z Yeh. 2013.
Joint extraction and labeling via graph propaga-
tion for dictionary construction. In Proceedings of
the 27th AAAI Conference on Artificial Intelligence,
pages 510–517.
J Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. 1975. Derivation of
new readability formulas (automated readability in-
dex, fog count and flesch reading ease formula) for
navy enlisted personnel. Technical report, Naval Air
Station, Memphis, TN.
Solomon Kullback and Richard A Leibler. 1951. On
information and sufficiency. The annals of mathe-
matical statistics, 22(1):79–86.
G Harry McLaughlin. 1969. Smog grading: A new
readability formula. Journal of reading, 12(8):639–
646.
Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help?: an exploration of graph-based al-
gorithms for cross-domain sentiment classification.
In Proceedings of the 2012 Joint Conference on
</reference>
<page confidence="0.989146">
419
</page>
<reference confidence="0.999784075">
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 655–665. Association for Computational Lin-
guistics.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing and management, 24(5):513–
523.
Sarah E Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 523–530. Association
for Computational Linguistics.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167–176. Association for Computational Linguis-
tics.
S. K. Michael Wong, Wojciech Ziarko, and P. C. N.
Wong. 1985. Generalized vector space model in
information retrieval. In Proceedings of the 8th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 18–25. ACM.
Beverley L. Zakaluk and S. Jay Samuels. 1988. Read-
ability: Its Past, Present, and Future. ERIC.
Xiaodong Zeng, Derek F Wong, Lidia S Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceeding of the 51stAnnual
Meeting of the Association for Computational Lin-
guistics, pages 770–779. Association for Computa-
tional Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical Report CMU-CALD-02-107,
Carnegie Mellon University.
</reference>
<page confidence="0.998376">
420
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476524">
<title confidence="0.999742">A Graph-based Readability Assessment Method using Word Coupling</title>
<author confidence="0.98862">Gang Sun Jiang</author>
<author confidence="0.98862">Qing Tao Bai</author>
<author confidence="0.98862">Daoxu</author>
<affiliation confidence="0.879058">State Key Laboratory for Novel Software Nanjing University, Nanjing 210023,</affiliation>
<email confidence="0.8251775">jiangzhiwei@outlook.com,guq@nju.edu.cn,bt@xjau.edu.cn,cdx@nju.edu.cn</email>
<abstract confidence="0.995770454545455">This paper proposes a graph-based readability assessment method using word coupling. Compared to the state-of-theart methods such as the readability formulae, the word-based and feature-based methods, our method develops a coupled bag-of-words model which combines the merits of word frequencies and text features. Unlike the general bag-of-words model which assumes words are independent, our model correlates the words based on their similarities on readability. By applying TF-IDF (Term Frequency and Inverse Document Frequency), the coupled TF-IDF matrix is built, and used in the graph-based classification framework, which involves graph building, merging and label propagation. Experiments are conducted on both English and Chinese datasets. The results demonstrate both effectiveness and potential of the method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rebekah George Benjamin</author>
</authors>
<title>Reconstructing readability: Recent developments and recommendations in the analysis of text difficulty.</title>
<date>2012</date>
<journal>Educational Psychology Review,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1916" citStr="Benjamin, 2012" startWordPosition="276" endWordPosition="277"> to select texts appropriate to the reading/grade levels of the students, and for web designers to organize texts on web pages for the users doing personalized searches for information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assessment. We design a coupled bag-of-words </context>
</contexts>
<marker>Benjamin, 2012</marker>
<rawString>Rebekah George Benjamin. 2012. Reconstructing readability: Recent developments and recommendations in the analysis of text difficulty. Educational Psychology Review, 24(1):63–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Cheng</author>
<author>Duoqian Miao</author>
<author>Can Wang</author>
<author>Longbing Cao</author>
</authors>
<title>Coupled term-term relation analysis for document clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 International Joint Conference on Neural Networks,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5312" citStr="Cheng et al., 2013" startWordPosition="794" endWordPosition="797">s. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains all the distinct words in a language (or the document set). A document is represented by a vector, whose components reflect the weight of every distinct word contained in the document. Normally, it assumes the words are independent. Now the capturing of the relationship among words has attracted considerable attention (Wong et al., 1985; Cheng et al., 2013). Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty. 2.3 The Graph-based Label Propagation Method Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-ba</context>
</contexts>
<marker>Cheng, Miao, Wang, Cao, 2013</marker>
<rawString>Xin Cheng, Duoqian Miao, Can Wang, and Longbing Cao. 2013. Coupled term-term relation analysis for document clustering. In Proceedings of the 2013 International Joint Conference on Neural Networks, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>James P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>193--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4322" citStr="Collins-Thompson and Callan, 2004" startWordPosition="636" endWordPosition="639">ge Processing, pages 411–420, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). Suitable text features are usually essential to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs</context>
<context position="19342" citStr="Collins-Thompson and Callan, 2004" startWordPosition="3149" endWordPosition="3152">e validation results are averaged over all the rounds. We choose the precision (P), recall (R) and F1-measure (F1) as the performance metrics. 4.2 Comparison to the State-of-the-Art Methods To address RQ1, we implement the following readability assessment methods and compare GRAW to them: (1) SMOG (McLaughlin, 1969) and FK (Kincaid et al., 1975) are two widely used readability formulae. We reserve their core measures (i.e. text features, and number of strokes for Chinese instead of number of syllables), and refine the coefficients on both datasets to befit the reading (grade) levels. (2) SUM (Collins-Thompson and Callan, 2004) is a word-based method, which trains one unigram model for each grade level, and applies model smoothing both inter and intra the grade levels. (3) LR and SVM refer to two featurebased methods which incorporate text features defined in (Jiang et al., 2014) to represent documents as instances. The logistic regression model and Gd,vp,(,�−1)(l)) (10) 415 Dataset Level Metric Methods SMOG FK SUM LR SVM GRAWc GRAWcf P 57.48 74.07 71.76 71.87 73.18 73.26 75.29 Gr.1 R 17.31 9.69 36.31 71.17 67.28 73.28 83.17 F1 26.14 15.25 47.67 71.23 69.70 72.98 78.89 P 34.73 31.44 37.94 51.62 50.78 52.05 55.83 Gr.</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Kevyn Collins-Thompson and James P Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193–200. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel I Daitch</author>
<author>Jonathan A Kelner</author>
<author>Daniel A Spielman</author>
</authors>
<title>Fitting a graph to vector data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>201--208</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6171" citStr="Daitch et al., 2009" startWordPosition="924" endWordPosition="927">propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, effective algorithms have been developed to propagate the label distributions to all the nodes (Subramanya et al., 2010; Kim et al., 2013). 3 The Proposed Method In this section, we present GRAW (Graph-based Readability Assessment method using Word coupling), which constructs a coupled bag-of-words model by exploiting the correlation of readability among the words. Unlike the general bag-ofwords model which models document relationship on topic, the coupled bag-of-words model extends it to model the </context>
</contexts>
<marker>Daitch, Kelner, Spielman, 2009</marker>
<rawString>Samuel I Daitch, Jonathan A Kelner, and Daniel A Spielman. 2009. Fitting a graph to vector data. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 201–208. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Dale</author>
<author>Jeanne S Chall</author>
</authors>
<title>A formula for predicting readability.</title>
<date>1948</date>
<journal>Educational Research Bulletin,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="1590" citStr="Dale and Chall, 1948" startWordPosition="226" endWordPosition="229">ging and label propagation. Experiments are conducted on both English and Chinese datasets. The results demonstrate both effectiveness and potential of the method. 1 Introduction Readability assessment is a task that aims to evaluate the reading difficulty or comprehending easiness of text documents. It is helpful for educationists to select texts appropriate to the reading/grade levels of the students, and for web designers to organize texts on web pages for the users doing personalized searches for information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models</context>
<context position="3970" citStr="Dale and Chall, 1948" startWordPosition="583" endWordPosition="586">ation method. 2.1 Readability Assessment Research on readability assessment has developed three types of methods: the readability formula, the word-based methods and the featurebased methods (Kincaid et al., 1975; CollinsThompson and Callan, 2004; Schwarm and Os411 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 411–420, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Fr</context>
</contexts>
<marker>Dale, Chall, 1948</marker>
<rawString>Edgar Dale and Jeanne S. Chall. 1948. A formula for predicting readability. Educational Research Bulletin, 27(1):11–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
<author>Martin Jansche</author>
<author>Matt Huenerfauth</author>
<author>No´emie Elhadad</author>
</authors>
<title>A comparison of features for automatic readability assessment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>276--284</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2266" citStr="Feng et al., 2010" startWordPosition="324" endWordPosition="327">e then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assessment. We design a coupled bag-of-words model, which correlates words based on their similarities on sentence-level readability computed using text features. The model is used in a graph-based classification framework, which involves graph building, graph merging/combination, and label propagation. We perform experiments on datasets of both English and Chinese. The results demonstrate bo</context>
<context position="4566" citStr="Feng et al., 2010" startWordPosition="674" endWordPosition="677">(Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). Suitable text features are usually essential to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains all the distinct words in a language (or the document set). A document is represented by a vector, whose components reflect the weight of every distinct word contained in the document. Normally, it assumes the wor</context>
</contexts>
<marker>Feng, Jansche, Huenerfauth, Elhadad, 2010</marker>
<rawString>Lijun Feng, Martin Jansche, Matt Huenerfauth, and No´emie Elhadad. 2010. A comparison of features for automatic readability assessment. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 276–284. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Franc¸ois</author>
<author>C´edrick Fairon</author>
</authors>
<title>An ai readability formula for french as a foreign language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>466--477</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Franc¸ois, Fairon, 2012</marker>
<rawString>Thomas Franc¸ois and C´edrick Fairon. 2012. An ai readability formula for french as a foreign language. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 466–477. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Itziar Gonzalez-Dios</author>
<author>Marıa Jes´us Aranzabe</author>
<author>Arantza Dıaz de Ilarraza</author>
<author>Haritz Salaberri</author>
</authors>
<title>Simple or complex? assessing the readability of basque texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<pages>334--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gonzalez-Dios, Aranzabe, de Ilarraza, Salaberri, 2014</marker>
<rawString>Itziar Gonzalez-Dios, Marıa Jes´us Aranzabe, Arantza Dıaz de Ilarraza, and Haritz Salaberri. 2014. Simple or complex? assessing the readability of basque texts. In Proceedings of the 25th International Conference on Computational Linguistics, pages 334–344. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hancke</author>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>Readability classification for german using lexical, syntactic, and morphological features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>1063--1080</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2316" citStr="Hancke et al., 2012" startWordPosition="332" endWordPosition="335"> Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assessment. We design a coupled bag-of-words model, which correlates words based on their similarities on sentence-level readability computed using text features. The model is used in a graph-based classification framework, which involves graph building, graph merging/combination, and label propagation. We perform experiments on datasets of both English and Chinese. The results demonstrate both effectiveness and potential of our method. The </context>
<context position="4616" citStr="Hancke et al., 2012" startWordPosition="682" endWordPosition="685">d et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). Suitable text features are usually essential to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains all the distinct words in a language (or the document set). A document is represented by a vector, whose components reflect the weight of every distinct word contained in the document. Normally, it assumes the words are independent. Now the capturing of the relat</context>
</contexts>
<marker>Hancke, Vajjala, Meurers, 2012</marker>
<rawString>Julia Hancke, Sowmya Vajjala, and Detmar Meurers. 2012. Readability classification for german using lexical, syntactic, and morphological features. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1063–1080. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Jebara</author>
<author>Jun Wang</author>
<author>Shih-Fu Chang</author>
</authors>
<title>Graph construction and b-matching for semisupervised learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>441--448</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6252" citStr="Jebara et al., 2009" startWordPosition="939" endWordPosition="942"> unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, effective algorithms have been developed to propagate the label distributions to all the nodes (Subramanya et al., 2010; Kim et al., 2013). 3 The Proposed Method In this section, we present GRAW (Graph-based Readability Assessment method using Word coupling), which constructs a coupled bag-of-words model by exploiting the correlation of readability among the words. Unlike the general bag-ofwords model which models document relationship on topic, the coupled bag-of-words model extends it to model the relationship among documents on readability. In the following sections, we descri</context>
</contexts>
<marker>Jebara, Wang, Chang, 2009</marker>
<rawString>Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009. Graph construction and b-matching for semisupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 441–448. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiwei Jiang</author>
<author>Gang Sun</author>
<author>Qing Gu</author>
<author>Daoxu Chen</author>
</authors>
<title>An ordinal multi-class classification method for readability assessment of chinese documents.</title>
<date>2014</date>
<booktitle>In Knowledge Science, Engineering and Management,</booktitle>
<pages>61--72</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="15503" citStr="Jiang et al., 2014" startWordPosition="2507" endWordPosition="2510">sesses the least number of common neighbors (from all the three graphs) with the nodes that are already selected in Nc(v). The objective is to keep the number of triangles in Gc to a minimum. The edge weights of Gc are averaged on the corresponding edges appeared in the three graphs. Combine with the feature-based graph Previous studies usually extract text features from documents to assess the readability using classification models. Here, we also take into consideration the feature-based graph, where similarities among documents are computed on text features. We use the features defined in (Jiang et al., 2014), where the model based features are eliminated since the v 0 2 4 0 1 3 414 computation depends on pre-assigned class labels, and represent a document as a vector of the feature values. We compute the similarity between any pair of documents using the Euclidean distance, and built the feature-based graph (denoted as Gf) in the same way as above. Additionally, to take advantage of both graphs, we combine them into one (denoted as Gcf) using the following formula. Gcf i,j = max [Gci,j, Gfi,j] (9) 3.3.2 Graph Propagation Given a graph G∗ constructed in previous sections, its nodes are divided int</context>
<context position="17841" citStr="Jiang et al., 2014" startWordPosition="2913" endWordPosition="2916">e conduct experiments on datasets of both English and Chinese, to investigate the following three research questions: RQ1: Whether the proposed method (i.e. GRAW) outperforms the state-of-the-art methods for readability assessment? RQ2: What are the effects of adding the word coupling matrix to the general bag-of-words model? RQ3: Whether the graph merging strategy is effective, and whether the performance can be further improved by combining the feature-based graph. 4.1 Corpus and Metrics To evaluate our proposed method, we collected two datasets. The first is CPT (Chinese primary textbook) (Jiang et al., 2014), which contains Chinese documents of six reading levels. The second is ENCT (English New Concept textbook) which contains English documents of four reading levels. Both datasets are built from well-known textbooks where documents are labeled as grade levels by credible educationists. The details of the datasets are listed in Table 2. Dataset Language #Grade #Doc #Sent #Word CPT Chinese 6 637 16145 234372 ENCT English 4 279 4671 62921 Table 2: Statistics of the datasets on both English and Chinese We conduct experiments on both datasets using the cross-validation which randomly divides a datas</context>
<context position="19599" citStr="Jiang et al., 2014" startWordPosition="3194" endWordPosition="3197">are GRAW to them: (1) SMOG (McLaughlin, 1969) and FK (Kincaid et al., 1975) are two widely used readability formulae. We reserve their core measures (i.e. text features, and number of strokes for Chinese instead of number of syllables), and refine the coefficients on both datasets to befit the reading (grade) levels. (2) SUM (Collins-Thompson and Callan, 2004) is a word-based method, which trains one unigram model for each grade level, and applies model smoothing both inter and intra the grade levels. (3) LR and SVM refer to two featurebased methods which incorporate text features defined in (Jiang et al., 2014) to represent documents as instances. The logistic regression model and Gd,vp,(,�−1)(l)) (10) 415 Dataset Level Metric Methods SMOG FK SUM LR SVM GRAWc GRAWcf P 57.48 74.07 71.76 71.87 73.18 73.26 75.29 Gr.1 R 17.31 9.69 36.31 71.17 67.28 73.28 83.17 F1 26.14 15.25 47.67 71.23 69.70 72.98 78.89 P 34.73 31.44 37.94 51.62 50.78 52.05 55.83 Gr.2 R 31.06 28.00 29.73 56.48 59.45 57.36 66.06 F1 32.66 29.42 33.13 53.67 54.53 54.40 60.37 P 20.05 20.79 28.12 44.15 48.89 46.33 51.72 Gr.3 R 58.84 75.53 25.06 43.94 49.94 58.59 68.41 F1 29.89 32.40 26.35 43.72 49.04 51.57 58.74 P 25.06 28.94 25.60 33.35 33</context>
</contexts>
<marker>Jiang, Sun, Gu, Chen, 2014</marker>
<rawString>Zhiwei Jiang, Gang Sun, Qing Gu, and Daoxu Chen. 2014. An ordinal multi-class classification method for readability assessment of chinese documents. In Knowledge Science, Engineering and Management, pages 61–72. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kidwell</author>
<author>Guy Lebanon</author>
<author>Kevyn CollinsThompson</author>
</authors>
<title>Statistical estimation of word acquisition with application to readability prediction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>900--909</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2094" citStr="Kidwell et al., 2009" startWordPosition="299" endWordPosition="302">r information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assessment. We design a coupled bag-of-words model, which correlates words based on their similarities on sentence-level readability computed using text features. The model is used in a graph-based classification framework,</context>
<context position="4406" citStr="Kidwell et al., 2009" startWordPosition="649" endWordPosition="652">putational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). Suitable text features are usually essential to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains all the distinct words in a language (or the document</context>
</contexts>
<marker>Kidwell, Lebanon, CollinsThompson, 2009</marker>
<rawString>Paul Kidwell, Guy Lebanon, and Kevyn CollinsThompson. 2009. Statistical estimation of word acquisition with application to readability prediction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 900–909. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doo Soon Kim</author>
<author>Kunal Verma</author>
<author>Peter Z Yeh</author>
</authors>
<title>Joint extraction and labeling via graph propagation for dictionary construction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 27th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>510--517</pages>
<contexts>
<context position="5666" citStr="Kim et al., 2013" startWordPosition="847" endWordPosition="850">r, whose components reflect the weight of every distinct word contained in the document. Normally, it assumes the words are independent. Now the capturing of the relationship among words has attracted considerable attention (Wong et al., 1985; Cheng et al., 2013). Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty. 2.3 The Graph-based Label Propagation Method Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, </context>
<context position="16436" citStr="Kim et al., 2013" startWordPosition="2668" endWordPosition="2671">s Gf) in the same way as above. Additionally, to take advantage of both graphs, we combine them into one (denoted as Gcf) using the following formula. Gcf i,j = max [Gci,j, Gfi,j] (9) 3.3.2 Graph Propagation Given a graph G∗ constructed in previous sections, its nodes are divided into two sets: the labeled set Vl and the unlabeled set Vu. The goal of label propagation is to propagate class labels from the labeled nodes (i.e. documents) to the entire graph. Here, we use a simplified version of the label propagation method presented in (Subramanya et al., 2010), which has been proved effective (Kim et al., 2013). The method iteratively updates the label distribution on a document node using the following equation. ⎛ pdi)(l) = 1 red pd(l)δ(d ∈ Vl) + v∈N(d) At the left side of Eq.10, p(i) d (l) is the afterward probability of l (i.e. the class label) on a node d at the i-th iteration. At the right side, κd is the normalizing constant to make sure the sum of all the probabilities is 1, and p0d(l) is the initial probability of l on d if d is initially labeled (i.e. belonging to the labeled set Vl). δ(x) is the indicator function. ,N(d) denotes the set of neighbors of d. The iteration stops when the chang</context>
</contexts>
<marker>Kim, Verma, Yeh, 2013</marker>
<rawString>Doo Soon Kim, Kunal Verma, and Peter Z Yeh. 2013. Joint extraction and labeling via graph propagation for dictionary construction. In Proceedings of the 27th AAAI Conference on Artificial Intelligence, pages 510–517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peter Kincaid</author>
<author>Robert P Fishburne Jr</author>
<author>Richard L Rogers</author>
<author>Brad S Chissom</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel.</title>
<date>1975</date>
<tech>Technical report,</tech>
<institution>Naval Air Station,</institution>
<location>Memphis, TN.</location>
<contexts>
<context position="1718" citStr="Kincaid et al., 1975" startWordPosition="247" endWordPosition="250">veness and potential of the method. 1 Introduction Readability assessment is a task that aims to evaluate the reading difficulty or comprehending easiness of text documents. It is helpful for educationists to select texts appropriate to the reading/grade levels of the students, and for web designers to organize texts on web pages for the users doing personalized searches for information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). </context>
<context position="3562" citStr="Kincaid et al., 1975" startWordPosition="525" endWordPosition="528">nized as follows: Section 2 introduces backgrounds of our work. Section 3 presents the details of the method. Section 4 designs the experiments and explains the results. Finally, Section 5 concludes the paper with planned future work. 2 Background In this section, we introduce briefly three research topics relevant to our work: readability assessment, the bag-of-words model and the graphbased label propagation method. 2.1 Readability Assessment Research on readability assessment has developed three types of methods: the readability formula, the word-based methods and the featurebased methods (Kincaid et al., 1975; CollinsThompson and Callan, 2004; Schwarm and Os411 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 411–420, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus </context>
<context position="19055" citStr="Kincaid et al., 1975" startWordPosition="3103" endWordPosition="3106">a dataset into labeled (training) and unlabeled (test) sets. The labeling proportion is varied to investigate the performance of GRAW under different circumstances. To reduce variability, given certain labeling proportion, 100 rounds of crossvalidation are performed, and the validation results are averaged over all the rounds. We choose the precision (P), recall (R) and F1-measure (F1) as the performance metrics. 4.2 Comparison to the State-of-the-Art Methods To address RQ1, we implement the following readability assessment methods and compare GRAW to them: (1) SMOG (McLaughlin, 1969) and FK (Kincaid et al., 1975) are two widely used readability formulae. We reserve their core measures (i.e. text features, and number of strokes for Chinese instead of number of syllables), and refine the coefficients on both datasets to befit the reading (grade) levels. (2) SUM (Collins-Thompson and Callan, 2004) is a word-based method, which trains one unigram model for each grade level, and applies model smoothing both inter and intra the grade levels. (3) LR and SVM refer to two featurebased methods which incorporate text features defined in (Jiang et al., 2014) to represent documents as instances. The logistic regre</context>
</contexts>
<marker>Kincaid, Jr, Rogers, Chissom, 1975</marker>
<rawString>J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, Naval Air Station, Memphis, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<title>On information and sufficiency. The annals of mathematical statistics,</title>
<date>1951</date>
<pages>22--1</pages>
<contexts>
<context position="11402" citStr="Kullback and Leibler, 1951" startWordPosition="1810" endWordPosition="1813">ading difficulty of a sentence may be determined by the maximum measure on the text features. lsur(s) = max [llen(s), lans(s), lanc(s)] llex(s) = max [llv(s), latr(s), lntr(s)] (2) lsyn(s) = max [lpth(s), lanp(s)] where nt refers to the number of sentences in which t appears. The indicator function δ(x) returns 1 if x is true and 0 otherwise. l*(s) refers to one of the functions lsur(s), llex(s) or lsyn(s). Step 2. Given two words (terms) t1 and t2, whose level distributions are pt1 and pt2 respectively, we measure the distribution difference cKL(t1, t2) using the Kullback-Leibler divergence (Kullback and Leibler, 1951), computed by the following formula. 1 cKL(t1,t2) = 2 (KL(pt1||pt2) + KL(pt2||pt1)) (4) where KL(p||q) = Ei p(i) log p(i) q(i). After that, the logistic function is applied on the computed difference to get the normalized distribution similarity, i.e. 2 sim(t1, t2) = (5) 1 + ecKL(t1,t2) Given a word ti, only λ other words with highest correlation (similarity) are selected to build the 413 neighbor set of ti, denoted as N(ti). If a word tj is not selected (i.e. tj ∈/ N(ti)), the corresponding sim(ti7 tj) will be assigned 0. After that, the word coupling matrix (i.e. C*) with sim(ti7 tj) as elem</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Harry McLaughlin</author>
</authors>
<title>Smog grading: A new readability formula.</title>
<date>1969</date>
<journal>Journal of reading,</journal>
<volume>12</volume>
<issue>8</issue>
<pages>646</pages>
<contexts>
<context position="1695" citStr="McLaughlin, 1969" startWordPosition="244" endWordPosition="246">trate both effectiveness and potential of the method. 1 Introduction Readability assessment is a task that aims to evaluate the reading difficulty or comprehending easiness of text documents. It is helpful for educationists to select texts appropriate to the reading/grade levels of the students, and for web designers to organize texts on web pages for the users doing personalized searches for information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012;</context>
<context position="3988" citStr="McLaughlin, 1969" startWordPosition="587" endWordPosition="588">ability Assessment Research on readability assessment has developed three types of methods: the readability formula, the word-based methods and the featurebased methods (Kincaid et al., 1975; CollinsThompson and Callan, 2004; Schwarm and Os411 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 411–420, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon</context>
<context position="19025" citStr="McLaughlin, 1969" startWordPosition="3099" endWordPosition="3100">on which randomly divides a dataset into labeled (training) and unlabeled (test) sets. The labeling proportion is varied to investigate the performance of GRAW under different circumstances. To reduce variability, given certain labeling proportion, 100 rounds of crossvalidation are performed, and the validation results are averaged over all the rounds. We choose the precision (P), recall (R) and F1-measure (F1) as the performance metrics. 4.2 Comparison to the State-of-the-Art Methods To address RQ1, we implement the following readability assessment methods and compare GRAW to them: (1) SMOG (McLaughlin, 1969) and FK (Kincaid et al., 1975) are two widely used readability formulae. We reserve their core measures (i.e. text features, and number of strokes for Chinese instead of number of syllables), and refine the coefficients on both datasets to befit the reading (grade) levels. (2) SUM (Collins-Thompson and Callan, 2004) is a word-based method, which trains one unigram model for each grade level, and applies model smoothing both inter and intra the grade levels. (3) LR and SVM refer to two featurebased methods which incorporate text features defined in (Jiang et al., 2014) to represent documents as</context>
</contexts>
<marker>McLaughlin, 1969</marker>
<rawString>G Harry McLaughlin. 1969. Smog grading: A new readability formula. Journal of reading, 12(8):639– 646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia Ponomareva</author>
<author>Mike Thelwall</author>
</authors>
<title>Do neighbours help?: an exploration of graph-based algorithms for cross-domain sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5889" citStr="Ponomareva and Thelwall, 2012" startWordPosition="878" endWordPosition="882">ble attention (Wong et al., 1985; Cheng et al., 2013). Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty. 2.3 The Graph-based Label Propagation Method Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, effective algorithms have been developed to propagate the label distributions to all the nodes (Subramanya et al., 2010; Kim et al., 2013). 3 The Proposed Method In this section, we present GRAW (Graph-based Readability Ass</context>
</contexts>
<marker>Ponomareva, Thelwall, 2012</marker>
<rawString>Natalia Ponomareva and Mike Thelwall. 2012. Do neighbours help?: an exploration of graph-based algorithms for cross-domain sentiment classification. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 655–665. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval. Information processing and management,</title>
<date>1988</date>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="7304" citStr="Salton and Buckley, 1988" startWordPosition="1102" endWordPosition="1105">dels document relationship on topic, the coupled bag-of-words model extends it to model the relationship among documents on readability. In the following sections, we describe in detail how to build the coupled bag-of-words model. The model is then used in the graphbased classification framework for readability assessment. 3.1 The General Bag-of-Words Model TF-IDF (Term Frequency and Inverse Document Frequency) is the most popular scheme of the bagof-words model. Given the set of documents D, the TF-IDF matrix M can be calculated based on the logarithmically scaled term (i.e. word) frequency (Salton and Buckley, 1988) as follows. Mt,d = tft,d · idft,d = (1 + log f(t, d)) · log j{djt E d}j where f(t, d) is the number of times that a term (word) t occurs in a document d E D. 3.2 The Coupled Bag-of-Words Model As shown in Figure 1, three main stages are required to construct the coupled bag-of-words model: per-sentence readability estimation, word coupling matrix construction and coupled TF-IDF matrix calculation. The following sections describe the details of these stages. 3.2.1 Per-Sentence Readability Estimation Two steps are required for the per-sentence readability estimation. The first is to compute a r</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information processing and management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2247" citStr="Schwarm and Ostendorf, 2005" startWordPosition="319" endWordPosition="323">ulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assessment. We design a coupled bag-of-words model, which correlates words based on their similarities on sentence-level readability computed using text features. The model is used in a graph-based classification framework, which involves graph building, graph merging/combination, and label propagation. We perform experiments on datasets of both English and Chinese. The res</context>
<context position="4352" citStr="Schwarm and Ostendorf, 2005" startWordPosition="640" endWordPosition="643">n, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tendorf, 2005). During the early time, many well-known readability formulae have been developed to assess the readability of text documents (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Surface text features are defined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their frequencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005) and the word acquisition model (Kidwell et al., 2009). The feature-based methods focus on extracting text features from a document and training a classification model to classify its readability (Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). Suitable text features are usually essential to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah E Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semisupervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6385" citStr="Subramanya et al., 2010" startWordPosition="959" endWordPosition="962"> et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, effective algorithms have been developed to propagate the label distributions to all the nodes (Subramanya et al., 2010; Kim et al., 2013). 3 The Proposed Method In this section, we present GRAW (Graph-based Readability Assessment method using Word coupling), which constructs a coupled bag-of-words model by exploiting the correlation of readability among the words. Unlike the general bag-ofwords model which models document relationship on topic, the coupled bag-of-words model extends it to model the relationship among documents on readability. In the following sections, we describe in detail how to build the coupled bag-of-words model. The model is then used in the graphbased classification framework for reada</context>
<context position="16384" citStr="Subramanya et al., 2010" startWordPosition="2659" endWordPosition="2662">dean distance, and built the feature-based graph (denoted as Gf) in the same way as above. Additionally, to take advantage of both graphs, we combine them into one (denoted as Gcf) using the following formula. Gcf i,j = max [Gci,j, Gfi,j] (9) 3.3.2 Graph Propagation Given a graph G∗ constructed in previous sections, its nodes are divided into two sets: the labeled set Vl and the unlabeled set Vu. The goal of label propagation is to propagate class labels from the labeled nodes (i.e. documents) to the entire graph. Here, we use a simplified version of the label propagation method presented in (Subramanya et al., 2010), which has been proved effective (Kim et al., 2013). The method iteratively updates the label distribution on a document node using the following equation. ⎛ pdi)(l) = 1 red pd(l)δ(d ∈ Vl) + v∈N(d) At the left side of Eq.10, p(i) d (l) is the afterward probability of l (i.e. the class label) on a node d at the i-th iteration. At the right side, κd is the normalizing constant to make sure the sum of all the probabilities is 1, and p0d(l) is the initial probability of l on d if d is initially labeled (i.e. belonging to the labeled set Vl). δ(x) is the indicator function. ,N(d) denotes the set o</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semisupervised learning of structured tagging models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167–176. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Michael Wong</author>
<author>Wojciech Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>18--25</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5291" citStr="Wong et al., 1985" startWordPosition="790" endWordPosition="793">ess of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods. 2.2 The Bag-of-Words Model The bag-of-words model is mostly used for document classification. It constructs a feature space that contains all the distinct words in a language (or the document set). A document is represented by a vector, whose components reflect the weight of every distinct word contained in the document. Normally, it assumes the words are independent. Now the capturing of the relationship among words has attracted considerable attention (Wong et al., 1985; Cheng et al., 2013). Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty. 2.3 The Graph-based Label Propagation Method Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). </context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S. K. Michael Wong, Wojciech Ziarko, and P. C. N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 8th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 18–25. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beverley L Zakaluk</author>
<author>S Jay Samuels</author>
</authors>
<title>Readability: Its Past, Present, and Future.</title>
<date>1988</date>
<publisher>ERIC.</publisher>
<contexts>
<context position="1877" citStr="Zakaluk and Samuels, 1988" startWordPosition="270" endWordPosition="273">of text documents. It is helpful for educationists to select texts appropriate to the reading/grade levels of the students, and for web designers to organize texts on web pages for the users doing personalized searches for information retrieval. Research on readability assessment starts from the early 20th century (Dale and Chall, 1948). Many useful readability formulae have been developed since then (Dale and Chall, 1948; McLaughlin, 1969; Kincaid et al., 1975). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress (Zakaluk and Samuels, 1988; ∗Corresponding author. Benjamin, 2012; Gonzalez-Dios et al., 2014). The word-based methods compute word frequencies in documents to estimate their readability (CollinsThompson and Callan, 2004; Kidwell et al., 2009). The feature-based methods extract text features from documents and train classification models to classify the readability (Schwarm and Ostendorf, 2005; Feng et al., 2010; Franc¸ois and Fairon, 2012; Hancke et al., 2012). In this paper, we propose a graph-based method using word coupling, which combines the merits of both word frequencies and text features for readability assess</context>
</contexts>
<marker>Zakaluk, Samuels, 1988</marker>
<rawString>Beverley L. Zakaluk and S. Jay Samuels. 1988. Readability: Its Past, Present, and Future. ERIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging.</title>
<date>2013</date>
<booktitle>In Proceeding of the 51stAnnual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>770--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5827" citStr="Zeng et al., 2013" startWordPosition="871" endWordPosition="874">e relationship among words has attracted considerable attention (Wong et al., 1985; Cheng et al., 2013). Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty. 2.3 The Graph-based Label Propagation Method Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones (Kim et al., 2013). It has been successfully applied in various applications, such as dictionary construction (Kim et al., 2013), word segmentation and tagging (Zeng et al., 2013), and sentiment classification (Ponomareva and Thelwall, 2012). Typically, a graph-based label propagation method consists of two main steps: graph construction and label propagation (Zeng et al., 2013). During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes (Daitch et al., 2009). Some form of edge pruning is required to refine the graph (Jebara et al., 2009). After that, effective algorithms have been developed to propagate the label distributions to all the nodes (Subramanya et al., 2010; Kim et al., 2013). 3 The Proposed Method</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F Wong, Lidia S Chao, and Isabel Trancoso. 2013. Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging. In Proceeding of the 51stAnnual Meeting of the Association for Computational Linguistics, pages 770–779. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="13050" citStr="Zhu and Ghahramani, 2002" startWordPosition="2086" endWordPosition="2089">e the TF-IDF matrix M described in Section 3.1, we multiply it by the word coupling matrix C*, so that the term frequencies are shared among the highly correlated (coupled) words. We denote the coupled TF-IDF matrix as M*, obtained by the following formula. M* = C* · M (6) Specifically, three homogenous coupled TFIDF matrices Msur, Mlex and Msyn can be built according to the three word coupling matrices C*. 3.3 Graph-based Readability Assessment We employ the coupled bag-of-words model for readability assessment under the graph-based classification framework as described in the previous work (Zhu and Ghahramani, 2002). Firstly, we construct a graph representing the readability relationship among documents by using the coupled bag-of-words model to compute the relations among these documents. Secondly, we estimate reading levels of documents by applying label propagation on the graph. 3.3.1 Graph Construction We build a directed graph G* to represent the readability relation among documents, where nodes represent documents, and edges are weighted by the similarities between pairs of documents. Given a similarity function, we link documents di to dj with an edge of weight G*ij, defined as: Gi, * = I sim(di7 </context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
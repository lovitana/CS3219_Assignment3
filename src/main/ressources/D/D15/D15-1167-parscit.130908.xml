<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001690">
<title confidence="0.9980055">
Document Modeling with Gated Recurrent Neural Network
for Sentiment Classification
</title>
<author confidence="0.999301">
Duyu Tang, Bing Qin‚àó, Ting Liu
</author>
<affiliation confidence="0.998364">
Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.957264">
{dytang, qinb, tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.997299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999787958333333">
Document level sentiment classification
remains a challenge: encoding the intrin-
sic relations between sentences in the se-
mantic meaning of a document. To ad-
dress this, we introduce a neural network
model to learn vector-based document rep-
resentation in a unified, bottom-up fash-
ion. The model first learns sentence rep-
resentation with convolutional neural net-
work or long short-term memory. After-
wards, semantics of sentences and their
relations are adaptively encoded in docu-
ment representation with gated recurren-
t neural network. We conduct documen-
t level sentiment classification on four
large-scale review datasets from IMDB
and Yelp Dataset Challenge. Experimen-
tal results show that: (1) our neural mod-
el shows superior performances over sev-
eral state-of-the-art algorithms; (2) gat-
ed recurrent neural network dramatically
outperforms standard recurrent neural net-
work in document modeling for sentiment
classification.1
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9983789">
Document level sentiment classification is a fun-
damental task in sentiment analysis, and is cru-
cial to understand user generated content in so-
cial networks or product reviews (Manning and
Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang
and Lee, 2008; Liu, 2012). The task calls for iden-
tifying the overall sentiment polarity (e.g. thumbs
up or thumbs down, 1-5 stars on review sites) of a
document. In literature, dominant approaches fol-
low (Pang et al., 2002) and exploit machine learn-
</bodyText>
<footnote confidence="0.676848666666667">
‚àóCorresponding author.
1 Codes and datasets are publicly available at
http://ir.hit.edu.cn/Àúdytang.
</footnote>
<bodyText confidence="0.999696634146342">
ing algorithm to build sentiment classifier. Many
of them focus on designing hand-crafted features
(Qu et al., 2010; Paltoglou and Thelwall, 2010) or
learning discriminate features from data, since the
performance of a machine learner is heavily de-
pendent on the choice of data representation (Ben-
gio et al., 2015).
Document level sentiment classification re-
mains a significant challenge: how to encode the
intrinsic (semantic or syntactic) relations between
sentences in the semantic meaning of documen-
t. This is crucial for sentiment classification be-
cause relations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have
great influences on determining the meaning and
the overall polarity of a document. However, ex-
isting studies typically fail to effectively capture
such information. For example, Pang et al. (2002)
and Wang and Manning (2012) represent docu-
ments with bag-of-ngrams features and build SVM
classifier upon that. Although such feature-driven
SVM is an extremely strong performer and hardly
to be transcended, its ‚Äúsparse‚Äù and ‚Äúdiscrete‚Äù char-
acteristics make it clumsy in taking into account of
side information like relations between sentences.
Recently, Le and Mikolov (2014) exploit neural
networks to learn continuous document represen-
tation from data. Essentially, they use local ngram
information and do not capture semantic relations
between sentences. Furthermore, a person asked
to do this task will naturally carry it out in a se-
quential, bottom-up fashion, analyze the meanings
of sentences before considering semantic relation-
s between them. This motivates us to develop an
end-to-end and bottom-up algorithm to effectively
model document representation.
In this paper, we introduce a neural network ap-
proach to learn continuous document representa-
tion for sentiment classification. The method is
on the basis of the principle of compositionality
(Frege, 1892), which states that the meaning of
a longer expression (e.g. a sentence or a docu-
</bodyText>
<page confidence="0.940138">
1422
</page>
<note confidence="0.991377">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422‚Äì1432,
Lisbon, Portugal, 17-21 September 2015. cÔøΩ2015 Association for Computational Linguistics.
</note>
<figure confidence="0.998614068965517">
Word Representation
Sentence Representation
Sentence Composition
Document Representation
Document Composition
1
w1 1 w2 1 w3 1 wùëô1‚àí1
1 wùëô1
Forward Gated
Neural Network
CNN/LSTM
Backward Gated
Neural Network
2
w1 2 w2 2 w3 2 wùëô2‚àí1
2 wùëô2
Forward Gated
Neural Network
CNN/LSTM
Backward Gated
Neural Network
Softmax
ùëõ ùëõ ùëõ ùëõ ùëõ
w1 w2 w3 wùëôùëõ‚àí1 wùëôùëõ
Forward Gated
Neural Network
CNN/LSTM
Backward Gated
Neural Network
</figure>
<figureCaption confidence="0.9907725">
Figure 1: The neural network model for document level sentiment classification. wz stands for the i-th
word in the n-th sentence, l,, is sentence length.
</figureCaption>
<bodyText confidence="0.999889592592592">
ment) depends on the meanings of its constituents.
Specifically, the approach models document rep-
resentation in two steps. In the first step, it us-
es convolutional neural network (CNN) or long
short-term memory (LSTM) to produce sentence
representations from word representations. After-
wards, gated recurrent neural network is exploit-
ed to adaptively encode semantics of sentences
and their inherent relations in document represen-
tations. These representations are naturally used
as features to classify the sentiment label of each
document. The entire model is trained end-to-end
with stochastic gradient descent, where the loss
function is the cross-entropy error of supervised
sentiment classification2.
We conduct document level sentiment classi-
fication on four large-scale review datasets from
IMDB3 and Yelp Dataset Challenge4. We com-
pare to neural network models such as paragraph
vector (Le and Mikolov, 2014), convolutional neu-
ral network, and baselines such as feature-based
SVM (Pang et al., 2002), recommendation algo-
rithm JMARS (Diao et al., 2014). Experimental
results show that: (1) the proposed neural model
shows superior performances over all baseline al-
gorithms; (2) gated recurrent neural network dra-
matically outperforms standard recurrent neural
</bodyText>
<footnote confidence="0.999966">
2A similar work can be found at: http:
//deeplearning.net/tutorial/lstm.html
3http://www.imdb.com/
4http://www.yelp.com/dataset_challenge
</footnote>
<bodyText confidence="0.7842685">
network in document modeling. The main con-
tributions of this work are as follows:
</bodyText>
<listItem confidence="0.993331545454545">
‚Ä¢ We present a neural network approach to en-
code relations between sentences in document rep-
resentation for sentiment classification.
‚Ä¢ We report empirical results on four large-scale
datasets, and show that the approach outperforms
state-of-the-art methods for document level senti-
ment classification.
‚Ä¢ We report empirical results that traditional re-
current neural network is weak in modeling docu-
ment composition, while adding neural gates dra-
matically improves the classification performance.
</listItem>
<sectionHeader confidence="0.932325" genericHeader="introduction">
2 The Approach
</sectionHeader>
<bodyText confidence="0.998286625">
We introduce the proposed neural model in this
section, which computes continuous vector repre-
sentations for documents of variable length. These
representations are further used as features to clas-
sify the sentiment label of each document. An
overview of the approach is displayed in Figure 1.
Our approach models document semantics
based on the principle of compositionality (Frege,
1892), which states that the meaning of a longer
expression (e.g. a sentence or a document) comes
from the meanings of its constituents and the rules
used to combine them. Since a document consist-
s of a list of sentences and each sentence is made
up of a list of words, the approach models docu-
ment representation in two stages. It first produces
continuous sentence vectors from word represen-
</bodyText>
<page confidence="0.963647">
1423
</page>
<bodyText confidence="0.999638833333333">
tations with sentence composition (Section 2.1).
Afterwards, sentence vectors are treated as inputs
of document composition to get document repre-
sentation (Section 2.2). Document representations
are then used as features for document level senti-
ment classification (Section 2.3).
</bodyText>
<subsectionHeader confidence="0.996428">
2.1 Sentence Composition
</subsectionHeader>
<bodyText confidence="0.992668953488372">
We first describe word vector representation, be-
fore presenting a convolutional neural network
with multiple filters for sentence composition.
Each word is represented as a low dimension-
al, continuous and real-valued vector, also known
as word embedding (Bengio et al., 2003). Al-
l the word vectors are stacked in a word embed-
ding matrix Lw E Rd√ó|V |, where d is the dimen-
sion of word vector and |V  |is vocabulary size.
These word vectors can be randomly initialized
from a uniform distribution (Socher et al., 2013b),
or be pre-trained from text corpus with embedding
learning algorithms (Mikolov et al., 2013; Pen-
nington et al., 2014; Tang et al., 2014). We adopt
the latter strategy to make better use of semantic
and grammatical associations of words.
We use convolutional neural network (CNN)
and long short-term memory (LSTM) to compute
continuous representations of sentences with se-
mantic composition. CNN and LSTM are state-
of-the-art semantic composition models for senti-
ment classification (Kim, 2014; Kalchbrenner et
al., 2014; Johnson and Zhang, 2015; Li et al.,
2015a). They learn fixed-length vectors for sen-
tences of varying length, captures words order in
a sentence and does not depend on external de-
pendency or constituency parse results. One could
also use tree-based composition method such as
Recursive Neural Tensor Network (Socher et al.,
2013b) or Tree-Structured LSTM (Tai et al., 2015;
Zhu et al., 2015) as alternatives.
Specifically, we try CNN with multiple con-
volutional filters of different widths (Tang et al.,
2015) to produce sentence representation. Fig-
ure 2 displays the method. We use multiple con-
volutional filters in order to capture local seman-
tics of n-grams of various granularities, which
have been proven effective for sentiment classifi-
cation. For example, a convolutional filter with a
width of 2 essentially captures the semantics of bi-
grams in a sentence. In this work, we use three
convolutional filters whose widths are 1, 2 and
3 to encode the semantics of unigrams, bigram-
</bodyText>
<figureCaption confidence="0.928129">
Figure 2: Sentence composition with convolution-
al neural network.
</figureCaption>
<bodyText confidence="0.969669">
s and trigrams in a sentence. Each filter consists
of a list of linear layers with shared parameter-
s. Formally, let us denote a sentence consisting
of n words as {w1, w2, ...wi, ...wnl, let lc be the
width of a convolutional filter, and let Wc, bc be
the shared parameters of linear layers in the fil-
ter. Each word wi is mapped to its embedding
representation ei E Rd. The input of a linear lay-
er is the concatenation of word embeddings in a
fixed-length window size lc, which is denoted as
Ic = [ei; ei+1; ...; ei+l-‚àí1] E Rd¬∑l-. The output of
a linear layer is calculated as
</bodyText>
<equation confidence="0.991702">
Oc = Wc ¬∑ Ic + bc (1)
</equation>
<bodyText confidence="0.999930454545454">
where Wc E Rl¬∞-√ód¬∑l-, bc E Rl¬∞-, loc is the output
length of linear layer. To capture global semantics
of a sentence, we feed the outputs of linear layers
to an average pooling layer, resulting in an output
vector with fixed-length. We further add hyperbol-
ic tangent (tanh) to incorporate pointwise nonlin-
earity, and average the outputs of multiple filters
to get sentence representation.
We also try lstm as the sentence level semantic
calculator, the performance comparison between
these two variations is given in Section 3.
</bodyText>
<subsectionHeader confidence="0.950075">
2.2 Document Composition with Gated
Recurrent Neural Network
</subsectionHeader>
<bodyText confidence="0.999907333333333">
The obtained sentence vectors are fed to a docu-
ment composition component to calculate the doc-
ument representation. We present a gated recur-
rent neural network approach for document com-
position in this part.
Given the vectors of sentences of variable
length as input, document composition produces
a fixed-length document vector as output. To this
end, a simple strategy is ignoring the order of sen-
</bodyText>
<figure confidence="0.994779875">
Average
Lookup
Pooling
Convolution
Tanh
Filter 1 Filter 2 Filter 3
W1
W2 W3 W4 ‚Ä¢‚Ä¢‚Ä¢&amp;quot;&apos; Wn‚àí1 Wn
</figure>
<page confidence="0.705004">
1424
</page>
<figureCaption confidence="0.99707">
Figure 3: Document modeling with gated recurrent neural network. GNN stands for the basic computa-
tional unit of gated recurrent neural network.
</figureCaption>
<figure confidence="0.994965705882353">
(a) GatedNN
GNN
GNN
Softmax
GNN
S2
Sn
S1
(b) GatedNN Avg
GNN
GNN
GNN
Softmax
Average
S2
Sn
S1
</figure>
<bodyText confidence="0.985168466666667">
tences and averaging sentence vectors as docu-
ment vector. Despite its computational efficiency,
it fails to capture complex linguistic relations (e.g.
‚Äúcause‚Äù and ‚Äúcontrast‚Äù) between sentences. Con-
volutional neural network (Denil et al., 2014) is an
alternative for document composition, which mod-
els local sentence relations with shared parameters
of linear layers.
Standard recurrent neural network (RNN) can
map vectors of sentences of variable length to
a fixed-length vector by recursively transforming
current sentence vector st with the output vector
of the previous step ht‚àí1. The transition function
is typically a linear layer followed by pointwise
non-linearity layer such as tanh.
</bodyText>
<equation confidence="0.973942">
ht = tanh(Wr ¬∑ [ht‚àí1; st] + br) (2)
</equation>
<bodyText confidence="0.988540263157894">
where Wr E Rlhx(lh+loc), br E Rlh, lh and loc are
dimensions of hidden vector and sentence vector,
respectively. Unfortunately, standard RNN suffer-
s the problem of gradient vanishing or exploding
(Bengio et al., 1994; Hochreiter and Schmidhu-
ber, 1997), where gradients may grow or decay
exponentially over long sequences. This makes
it difficult to model long-distance correlations in
a sequence. To address this problem, we devel-
op a gated recurrent neural network for documen-
t composition, which works in a sequential way
and adaptively encodes sentence semantics in doc-
ument representations. The approach is analo-
gous to the recently emerged LSTM (Graves et
al., 2013; Zaremba and Sutskever, 2014; Sutskev-
er et al., 2014; Xu et al., 2015) and gated neural
network (Cho et al., 2014; Chung et al., 2015).
Specifically, the transition function of the gated
RNN used in this work is calculated as follows.
</bodyText>
<equation confidence="0.99973975">
it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3)
ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4)
gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5)
ht = tanh(it O gt + ft O ht‚àí1) (6)
</equation>
<bodyText confidence="0.99996615">
where O stands for element-wise multiplication,
Wi, Wf, bi, bf adaptively select and remove histo-
ry vector and input vector for semantic composi-
tion. The model can be viewed as a LSTM whose
output gate is alway on, since we prefer not to dis-
carding any part of the semantics of sentences to
get a better document representation. Figure 3 (a)
displays a standard sequential way where the last
hidden vector is regarded as the document rep-
resentation for sentiment classification. We can
make further extensions such as averaging hidden
vectors as document representation, which takes
considerations of a hierarchy of historical seman-
tics with different granularities. The method is il-
lustrated in Figure 3 (b), which shares some char-
acteristics with (Zhao et al., 2015). We can go
one step further to use preceding histories and fol-
lowing evidences in the same way, and exploit bi-
directional (Graves et al., 2013) gated RNN as the
calculator. The model is embedded in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999358">
2.3 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999396111111111">
The composed document representations can be
naturally regarded as features of documents for
sentiment classification without feature engineer-
ing. Specifically, we first add a linear layer to
transform document vector to real-valued vector
whose length is class number C. Afterwards, we
add a softmax layer to convert real values to con-
ditional probabilities, which is calculated as fol-
lows.
</bodyText>
<equation confidence="0.9989595">
Pi = c xp(xi) (7)
,i,=1 exp(xi,)
</equation>
<bodyText confidence="0.999685666666667">
We conduct experiments in a supervised learn-
ing setting, where each document in the training
data is accompanied with its gold sentiment label.
</bodyText>
<page confidence="0.913232">
1425
</page>
<table confidence="0.999145">
Corpus #docs #s/d #w/d |V  |#class Class Distribution
Yelp 2013 335,018 8.90 151.6 211,245 5 .09/.09/.14/.33/.36
Yelp 2014 1,125,457 9.22 156.9 476,191 5 .10/.09/.15/.30/.36
Yelp 2015 1,569,264 8.97 151.9 612,636 5 .10/.09/.14/.30/.37
IMDB 348,415 14.02 325.6 115,831 10 .07/.04/.05/.05/.08/.11/.15/.17/.12/.18
</table>
<tableCaption confidence="0.997748">
Table 1: Statistical information of Yelp 2013/2014/2015 and IMDB datasets. #docs is the number of
</tableCaption>
<bodyText confidence="0.851926166666667">
documents, #s/d and #w/d represent average number of sentences and average number of words contained
in per document, |V  |is the vocabulary size of words, #class is the number of classes.
For model training, we use the cross-entropy er-
ror between gold sentiment distribution P9(d) and
predicted sentiment distribution P(d) as the loss
function.
</bodyText>
<equation confidence="0.998145">
C
ÔøΩloss = ‚àí Pi9(d) ¬∑ log(Pi(d)) (8)
d‚ààT i=1
</equation>
<bodyText confidence="0.999715764705882">
where T is the training data, C is the number
of classes, d represents a document. P9(d) has
a 1-of-K coding scheme, which has the same
dimension as the number of classes, and only the
dimension corresponding to the ground truth is
1, with all others being 0. We take the deriva-
tive of loss function through back-propagation
with respect to the whole set of parameters Œ∏ =
[Wc; bc; Wi; bi; Wf; bf; Wr; br; Wsoftmax, bsoftmaxl,
and update parameters with stochastic gradient
descent. We set the widths of three convolutional
filters as 1, 2 and 3, output length of convolutional
filter as 50. We learn 200-dimensional word em-
beddings with SkipGram (Mikolov et al., 2013)
on each dataset separately, randomly initialize
other parameters from a uniform distribution
U(‚àí0.01, 0.01), and set learning rate as 0.03.
</bodyText>
<sectionHeader confidence="0.999368" genericHeader="background">
3 Experiment
</sectionHeader>
<bodyText confidence="0.99972175">
We conduct experiments to empirically evaluate
our method by applying it to document level senti-
ment classification. We describe experimental set-
tings and report empirical results in this section.
</bodyText>
<subsectionHeader confidence="0.991996">
3.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.99994475">
We conduct experiments on large-scale datasets
consisting of document reviews. Specifically, we
use one movie review dataset from IMDB (Diao
et al., 2014) and three restaurant review dataset-
s from Yelp Dataset Challenge in 2013, 2014 and
2015. Human labeled review ratings are regarded
as gold standard sentiment labels, so that we do
not need to manually annotate sentiment labels of
documents. We do not consider the cases that rat-
ing does not match with review texts (Zhang et al.,
2014).
Statistical information of these datasets are giv-
en in Table 1. We use the same dataset split as
in (Diao et al., 2014) on IMDB dataset, and split
Yelp datasets into training, development and test-
ing sets with 80/10/10. We run tokenization and
sentence splitting with Stanford CoreNLP (Man-
ning et al., 2014) on all these datasets. We use
accuracy (Manning and Sch¬®utze, 1999; Jurafsky
and Martin, 2000) and MSE (Diao et al., 2014)
as evaluation metrics, where accuracy is a stan-
dard metric to measure the overall sentiment clas-
sification performance. We use MSE to measure
the divergences between predicted sentiment la-
bels and ground truth sentiment labels because re-
view labels reflect sentiment strengths (e.g. one
star means strong negative and five star means
strong positive).
</bodyText>
<equation confidence="0.997803666666667">
ÔøΩN i (goldi ‚àí predictedi)2
MSE = 9
N ()
</equation>
<subsectionHeader confidence="0.725064">
3.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999946">
We compare our methods (Conv-GRNN and
LSTM-GRNN) with the following baseline meth-
ods for document level sentiment classification.
</bodyText>
<listItem confidence="0.996739">
(1) Majority is a heuristic baseline, which as-
signs the majority sentiment label in training set
to each document in test set.
(2) In SVM+Ngrams, we use bag-of-unigrams
and bag-of-bigrams as features and train SVM
classifier with LibLinear (Fan et al., 2008)5.
(3) In TextFeatures, we implement sophisticated
features (Kiritchenko et al., 2014) including word
ngrams, character ngrams, sentiment lexicon fea-
tures, cluster features, et al.
</listItem>
<footnote confidence="0.851243">
5We also try discretized regression (Pang and Lee, 2005)
with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). Howev-
er, its performance is obviously worse than SVM classifier.
</footnote>
<page confidence="0.835021">
1426
</page>
<table confidence="0.999587615384615">
Yelp 2013 Yelp 2014 Yelp 2015 IMDB
Accuracy MSE Accuracy MSE Accuracy MSE Accuracy MSE
Majority 0.356 3.06 0.361 3.28 0.369 3.30 0.179 17.46
SVM + Unigrams 0.589 0.79 0.600 0.78 0.611 0.75 0.399 4.23
SVM + Bigrams 0.576 0.75 0.616 0.65 0.624 0.63 0.409 3.74
SVM + TextFeatures 0.598 0.68 0.618 0.63 0.624 0.60 0.405 3.56
SVM + AverageSG 0.543 1.11 0.557 1.08 0.568 1.04 0.319 5.57
SVM + SSWE 0.535 1.12 0.543 1.13 0.554 1.11 0.262 9.16
JMARS N/A ‚Äì N/A ‚Äì N/A ‚Äì N/A 4.97
Paragraph Vector 0.577 0.86 0.592 0.70 0.605 0.61 0.341 4.69
Convolutional NN 0.597 0.76 0.610 0.68 0.615 0.68 0.376 3.30
Conv-GRNN 0.637 0.56 0.655 0.51 0.660 0.50 0.425 2.71
LSTM-GRNN 0.651 0.50 0.671 0.48 0.676 0.49 0.453 3.00
</table>
<tableCaption confidence="0.845368">
Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are
accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
</tableCaption>
<listItem confidence="0.952274947368421">
(4) In AverageSG, we learn 200-dimensional
word vectors with word2vec6 (Mikolov et al.,
2013), average word embeddings to get document
representation, and train a SVM classifier.
(5) We learn sentiment-specific word embed-
dings (SSWE), and use max/min/average pooling
(Tang et al., 2014) to get document representation.
(6) We compare with a state-of-the-art recom-
mendation algorithm JMARS (Diao et al., 2014),
which utilizes user and aspects of a review with
collaborative filtering and topic modeling.
(7) We implement a convolutional neural net-
work (CNN) baseline as it is a state-of-the-art se-
mantic composition method for sentiment analysis
(Kim, 2014; Denil et al., 2014).
(8) We implement a state-of-the-art neural net-
work baseline Paragraph Vector (Le and Mikolov,
2014) because its codes are not officially provided.
Window size is tuned on the development set.
</listItem>
<subsectionHeader confidence="0.999149">
3.3 Comparison to Other Methods
</subsectionHeader>
<bodyText confidence="0.965653833333333">
Experimental results are given in Table 2. We e-
valuate each dataset with two metrics, namely ac-
curacy (higher is better) and MSE (lower is better).
The best method in each dataset and each evalua-
tion metric is in bold.
From Table 2, we can see that majority is the
worst method because it does not capture any tex-
tual semantics. SVM classifiers with unigram and
bigram features (Pang et al., 2002) are extremely
strong, which are almost the strongest performers
6We use Skipgram as it performs slightly better than
CBOW in the experiment. We also try off-the-shell word em-
beddings from Glove, but its performance is slightly worse
than tailored word embedding from each corpus.
among all baseline methods. Designing complex
features are also effective for document level sen-
timent classification, however, it does not surpass
the bag-of-ngram features significantly as on Twit-
ter corpora (Kiritchenko et al., 2014). Further-
more, the aforementioned bag-of-features are dis-
crete and sparse. For example, the feature dimen-
sion of bigrams and TextFeatures on Yelp 2015
dataset are 899K and 4.81M after we filter out low
frequent features. Based on them, we try to con-
catenate several discourse-driven features, but the
classification performances remain unchanged.
AverageSG is a straight forward way to com-
pose document representation without feature en-
gineering. Unfortunately, we can see that it does
not work in this scenario, which appeals for pow-
erful semantic composition models for documen-
t level sentiment classification. We try to make
better use of the sentiment information to learn
a better SSWE (Tang et al., 2014), e.g. setting
a large window size. However, its performance
is still worse than context-based word embedding.
This stems from the fact that there are many sen-
timent shifters (e.g. negation or contrast words) in
document level reviews, while Tang et al. (2014)
learn SSWE by assigning sentiment label of a tex-
t to each phrase it contains. How to learn SSWE
effectively with document level sentiment super-
vision remains as an interesting future work.
Since JMARS outputs real-valued outputs, we
only evaluate it in terms of MSE. We can see that
sophisticated baseline methods such as JMARS,
paragraph vector and convolutional NN obtain sig-
nificant performance boosts over AverageSG by
</bodyText>
<page confidence="0.985104">
1427
</page>
<table confidence="0.998221111111111">
Yelp 2013 Yelp 2014 Yelp 2015 IMDB
Accuracy MSE Accuracy MSE Accuracy MSE Accuracy MSE
Average 0.598 0.65 0.605 0.75 0.614 0.67 0.366 3.91
Recurrent 0.377 1.37 0.306 1.75 0.383 1.67 0.176 12.29
Recurrent Avg 0.582 0.69 0.591 0.70 0.597 0.74 0.344 3.71
Bi Recurrent Avg 0.587 0.73 0.597 0.73 0.577 0.82 0.372 3.32
GatedNN 0.636 0.58 0.656 0.52 0.651 0.51 0.430 2.95
GatedNN Avg 0.635 0.57 0.659 0.52 0.657 0.56 0.416 2.78
Bi GatedNN Avg 0.637 0.56 0.655 0.51 0.660 0.50 0.425 2.71
</table>
<tableCaption confidence="0.9187895">
Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu-
racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
</tableCaption>
<bodyText confidence="0.9992205">
capturing deeper semantics of texts. Comparing
between CNN and AverageSG, we can conclude
that deep semantic compositionality is crucial for
understanding the semantics and the sentiment of
documents. However, it is somewhat disappoint-
ing that these models do not significantly outper-
form discrete bag-of-ngrams and bag-of-features.
The reason might lie in that semantic meanings of
documents, e.g. relations between sentences, are
not well captured. We can see that the proposed
method Conv-GRNN and LSTM-GRNN yield the
best performance on all four datasets in two evalu-
ation metrics. Compared with CNN, Conv-GRNN
shows its superior power in document composi-
tion component, which encodes semantics of sen-
tences and their relations in document representa-
tion with gated recurrent neural network. We al-
so find that LSTM (almost) consistently performs
better than CNN in modeling the sentence repre-
sentation.
</bodyText>
<subsectionHeader confidence="0.997901">
3.4 Model Analysis
</subsectionHeader>
<bodyText confidence="0.999679333333333">
As discussed before, document composition con-
tributes a lot to the superior performance of Conv-
GRNN and LSTM-GRNN. Therefore, we take
Conv-GRNN as an example and compare differen-
t neural models for document composition in this
part. Specifically, after obtaining sentence vectors
with convolutional neural network as described in
Section 2.1, we carry out experiments in following
settings.
</bodyText>
<listItem confidence="0.993307090909091">
(1) Average. Sentence vectors are averaged to
get the document vector.
(2) Recurrent / GatedNN. Sentence vectors are
fed to standard (or gated) recurrent neural network
in a sequential way from the beginning of the input
document. The last hidden vector is regarded as
document representation.
(3) Recurrent Avg / GatedNN Avg. We extend
setting (2) by averaging hidden vectors of recur-
rent neural network as document vector.
(4) Bi Recurrent Avg / Bi GatedNN Avg. We ex-
</listItem>
<bodyText confidence="0.963138407407407">
tend setting (3) by calculating hidden vectors from
both preceding histories and following contexts.
Bi-directional hidden vectors are averaged as doc-
ument representation.
Table 3 shows the experimental results. We can
see that standard recurrent neural network (RN-
N) is the worst method, even worse than the sim-
ple vector average. This is because RNN suf-
fers from the vanishing gradient problem, stating
that the influence of a given input on the hidden
layer decays exponentially over time on the net-
work output. In this paper, it means that doc-
ument representation encodes rare semantics of
the beginning sentences. This is further justified
by the great improvement of Recurrent Avg over
Recurrent. Bi Recurrent Avg and Recurrent Avg
perform comparably, but disappointingly both of
them fail to transcend Average. After adding neu-
ral gates, GatedNN obtains dramatic accuracy im-
provements over Recurrent and significantly out-
performs previous settings. The results indicate
that Gated RNN is capable of handling the van-
ishing gradient problem to some extend, and it is
practical to adaptively model sentence semantics
in document representation. GatedNN Avg and Bi
GatedNN Avg obtains comparable performances
with GatedNN.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9740796">
Document level sentiment classification is a fun-
damental problem in sentiment analysis (Pang and
Lee, 2008; Liu, 2012), which aims at identifying
the sentiment label of a document (Pang et al.,
2002; Turney, 2002). Pang and Lee (2002; 2005)
</bodyText>
<page confidence="0.984791">
1428
</page>
<bodyText confidence="0.999985582089552">
cast this problem as a classification task, and use
machine learning method in a supervised learning
framework. Turney (2002) introduces an unsuper-
vised approach by using sentiment words/phrases
extracted from syntactic patterns to determine the
document polarity. Goldberg and Zhu (2006)
place this task in a semi-supervised setting, and
use unlabelled reviews with graph-based method.
Dominant studies in literature follow Pang et al.
(2002) and work on designing effective features
for building a powerful sentiment classifier. Rep-
resentative features include word ngrams (Wang
and Manning, 2012), text topic (Ganu et al., 2009),
bag-of-opinions (Qu et al., 2010), syntactic rela-
tions (Xia and Zong, 2010), sentiment lexicon fea-
tures (Kiritchenko et al., 2014).
Despite the effectiveness of feature engineering,
it is labor intensive and unable to extract and or-
ganize the discriminative information from data
(Bengio et al., 2015). Recently, neural network e-
merges as an effective way to learn continuous text
representation for sentiment classification. Exist-
ing studies in this direction can be divided into two
groups. One line of research focuses on learning
continuous word embedding. Traditional embed-
ding learning algorithms typically leverage con-
texts of words in a context-prediction way (Ben-
gio et al., 2003; Mikolov et al., 2013; Baroni et al.,
2014). Since these methods typically map word-
s with similar contexts but opposite polarity (e.g.
‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, sever-
al studies (Maas et al., 2011; Labutov and Lipson,
2013; Tang et al., 2014) learn sentiment-specific
word embeddings by taking sentiment of texts in-
to account. Another line of research concentrates
on semantic composition (Mitchell and Lapata,
2010). Yessenalina and Cardie (2011) represent
each word as a matrix and use iterated matrix mul-
tiplication as phrase-level composition function.
Socher et al. (2013b) introduce a family of recur-
sive neural networks for sentence-level semantic
composition. Recursive neural network is extend-
ed with global feedbackward (Paulus et al., 2014),
feature weight tuning (Li, 2014), deep recursive
layer (Irsoy and Cardie, 2014), adaptive composi-
tion functions (Dong et al., 2014), combined with
Combinatory Categorial Grammar (Hermann and
Blunsom, 2013), and used for opinion relation de-
tection (Xu et al., 2014). Glorot et al. (2011) use s-
tacked denoising autoencoder. Convolutional neu-
ral networks are widely used for semantic compo-
sition (Kim, 2014; Kalchbrenner et al., 2014; De-
nil et al., 2014; Johnson and Zhang, 2015) by auto-
matically capturing local and global semantics. Le
and Mikolov (2014) introduce Paragraph Vector to
learn document representation from semantics of
words. Sequential model like recurrent neural net-
work or long short-term memory (LSTM) are also
verified as strong approaches for semantic compo-
sition (Li et al., 2015a).
In this work, we represent document with
convolutional-gated recurrent neural network,
which adaptively encodes semantics of sentences
and their relations. A recent work in (Li et al.,
2015b) also investigate LSTM to model document
meaning. They verify the effectiveness of LSTM
in text generation task.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999976878787879">
We introduce neural network models (Conv-
GRNN and LSTM-GRNN) for document level
sentiment classification. The approach encodes
semantics of sentences and their relations in doc-
ument representation, and is effectively trained
end-to-end with supervised sentiment classifica-
tion objectives. We conduct extensive experiments
on four review datasets with two evaluation met-
rics. Empirical results show that our approaches
achieve state-of-the-art performances on all these
datasets. We also find that (1) traditional recurren-
t neural network is extremely weak in modeling
document composition, while adding neural gates
dramatically boosts the performance, (2) LSTM
performs better than a multi-filtered CNN in mod-
eling sentence representation.
We briefly discuss some future plans. How to
effectively compose sentence meanings to docu-
ment meaning is a central problem in natural lan-
guage processing. In this work, we develop neu-
ral models in a sequential way, and encode sen-
tence semantics and their relations automatically
without using external discourse analysis result-
s. From one perspective, one could carefully de-
fine a set of sentiment-sensitive discourse relation-
s (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondi-
tion‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific
gated RNN can be developed to explicitly mod-
el semantic composition rules for each relation
(Socher et al., 2013a). However, defining such a
relation scheme is linguistic driven and time con-
suming, which we leave as future work. From an-
other perspective, one could compose document
</bodyText>
<page confidence="0.986266">
1429
</page>
<bodyText confidence="0.9999591">
representation over discourse tree structures rather
than in a sequential way. Accordingly, Recursive
Neural Network (Socher et al., 2013b) and Struc-
tured LSTM (Tai et al., 2015; Zhu et al., 2015)
can be used as composition algorithms. Howev-
er, existing discourse structure learning algorithm-
s are difficult to scale to massive review texts on
the web. How to simultaneously learn document
structure and composition function is an interest-
ing future work.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9986571">
The authors give great thanks to Yaming Sun and
Jiwei Li for the fruitful discussions. We also
would like to thank three anonymous reviewer-
s for their valuable comments and suggestions.
This work was supported by the National High
Technology Development 863 Program of Chi-
na (No. 2015AA015407), National Natural Sci-
ence Foundation of China (No. 61133012 and No.
61273321). Duyu Tang is supported by Baidu Fel-
lowship and IBM Ph.D. Fellowship.
</bodyText>
<sectionHeader confidence="0.998897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99796645">
Marco Baroni, Georgiana Dinu, and Germ¬¥an
Kruszewski. 2014. Don‚Äôt count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238‚Äì247.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157‚Äì166.
Yoshua Bengio, R¬¥ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137‚Äì1155.
Yoshua Bengio, Ian J. Goodfellow, and Aaron
Courville. 2015. Deep learning. Book in prepa-
ration for MIT Press.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder‚Äìdecoder
for statistical machine translation. In EMNLP, pages
1724‚Äì1734.
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,
and Yoshua Bengio. 2015. Gated feedback recur-
rent neural networks. ICML.
Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil
Blunsom, and Nando de Freitas. 2014. Mod-
elling, visualising and summarising documents with
a single convolutional neural network. arXiv
preprint:1406.3830.
Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In SIGKDD, pages
193‚Äì202. ACM.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In AAAI, pages 1537‚Äì1543.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. JMLR.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563‚Äì584.
Gayatree Ganu, Noemie Elhadad, and Am¬¥elie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In WebDB.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In ICML,
pages 513‚Äì520.
Andrew B Goldberg and Xiaojin Zhu. 2006. Seeing s-
tars when there aren‚Äôt many stars: graph-based semi-
supervised learning for sentiment categorization. In
GraphBased Method for NLP, pages 45‚Äì52.
Alex Graves, Navdeep Jaitly, and A-R Mohamed.
2013. Hybrid speech recognition with deep bidirec-
tional lstm. In Automatic Speech Recognition and
Understanding (ASRU), 2013 IEEE Workshop on,
pages 273‚Äì278. IEEE.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In ACL, pages 894‚Äì904.
Sepp Hochreiter and J¬®urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In NIPS, pages 2096‚Äì2104.
Rie Johnson and Tong Zhang. 2015. Effective use of
word order for text categorization with convolution-
al neural networks. NAACL.
Dan Jurafsky and James H Martin. 2000. Speech &amp;
language processing. Pearson Education India.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In ACL, pages 655‚Äì665.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP, pages 1746‚Äì
1751.
</reference>
<page confidence="0.898649">
1430
</page>
<reference confidence="0.99910612264151">
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014. Sentiment analysis of short in-
formal texts. Journal of Artificial Intelligence Re-
search, pages 723‚Äì762.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Quoc V. Le and Tomas Mikolov. 2014. Distribut-
ed representations of sentences and documents. In
ICML, pages 1188‚Äì1196.
Jiwei Li, Dan Jurafsky, and Eudard Hovy. 2015a.
When are tree structures necessary for deep learn-
ing of representations? arXiv preprint arX-
iv:1503.00185.
Jiwei Li, Minh-Thang Luong, and Dan Jurafsky.
2015b. A hierarchical neural autoencoder for
paragraphs and documents. arXiv preprint arX-
iv:1506.01057.
Jiwei Li. 2014. Feature weight tuning for recursive
neural networks. Arxiv preprint, 1412.3714.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1‚Äì167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In A-
CL, pages 142‚Äì150.
Christopher D Manning and Hinrich Sch¬®utze. 1999.
Foundations of statistical natural language process-
ing. MIT press.
Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In ACL, pages 55‚Äì60.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111‚Äì3119.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388‚Äì1429.
Georgios Paltoglou and Mike Thelwall. 2010. A s-
tudy of information retrieval weighting schemes for
sentiment analysis. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1386‚Äì1395.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL, pages 115‚Äì
124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1‚Äì135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In EMNLP, pages 79‚Äì
86.
Romain Paulus, Richard Socher, and Christopher D
Manning. 2014. Global belief recursive neural net-
works. In NIPS, pages 2888‚Äì2896.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532‚Äì1543.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rating
prediction from sparse text patterns. In COLING,
pages 913‚Äì921.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In ACL.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP, pages 1631‚Äì1642.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In NIPS, pages 3104‚Äì3112.
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representation-
s from tree-structured long short-term memory net-
works. In ACL.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Li-
u, and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In ACL, pages 1555‚Äì1565.
Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn-
ing semantic representations of users and products
for document level sentiment classification. In ACL,
pages 1014‚Äì1023.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL, pages 417‚Äì424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In ACL, pages 90‚Äì94.
Rui Xia and Chengqing Zong. 2010. Exploring the use
of word relation features for sentiment classification.
In COLING, pages 1336‚Äì1344.
Liheng Xu, Kang Liu, and Jun Zhao. 2014. Joint opin-
ion relation detection using one-class deep neural
network. In COLING, pages 677‚Äì687.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural image
caption generation with visual attention. ICML.
</reference>
<page confidence="0.832315">
1431
</page>
<reference confidence="0.993094947368421">
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In EMNLP, pages 172‚Äì182.
Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.
Yongfeng Zhang, Haochen Zhang, Min Zhang, Y-
iqun Liu, and Shaoping Ma. 2014. Do users rate
or review?: boost phrase-level sentiment labeling
with review-level sentiment classification. In SIGIR,
pages 1027‚Äì1030. ACM.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In IJCAI.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In EMNLP, pages 162‚Äì171, .
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
ICML.
</reference>
<page confidence="0.994548">
1432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696403">
<title confidence="0.9992125">Document Modeling with Gated Recurrent Neural for Sentiment Classification</title>
<author confidence="0.987748">Bing Ting Tang</author>
<affiliation confidence="0.972518">Harbin Institute of Technology, Harbin, China</affiliation>
<email confidence="0.979432">qinb,</email>
<abstract confidence="0.999694347826087">Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural net-</abstract>
<intro confidence="0.74314">work in document modeling for sentiment</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ¬¥an Kruszewski</author>
</authors>
<title>Don‚Äôt count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="28264" citStr="Baroni et al., 2014" startWordPosition="4459" endWordPosition="4462">enko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks f</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ¬¥an Kruszewski. 2014. Don‚Äôt count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238‚Äì247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="12488" citStr="Bengio et al., 1994" startWordPosition="1935" endWordPosition="1938">with shared parameters of linear layers. Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht‚àí1. The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh. ht = tanh(Wr ¬∑ [ht‚àí1; st] + br) (2) where Wr E Rlhx(lh+loc), br E Rlh, lh and loc are dimensions of hidden vector and sentence vector, respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifica</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157‚Äì166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R¬¥ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="7808" citStr="Bengio et al., 2003" startWordPosition="1157" endWordPosition="1160">sentence vectors from word represen1423 tations with sentence composition (Section 2.1). Afterwards, sentence vectors are treated as inputs of document composition to get document representation (Section 2.2). Document representations are then used as features for document level sentiment classification (Section 2.3). 2.1 Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sen</context>
<context position="28220" citStr="Bengio et al., 2003" startWordPosition="4450" endWordPosition="4454"> 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) intro</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R¬¥ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137‚Äì1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Ian J Goodfellow</author>
<author>Aaron Courville</author>
</authors>
<date>2015</date>
<booktitle>Deep learning. Book in preparation for</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2046" citStr="Bengio et al., 2015" startWordPosition="300" endWordPosition="304">r identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Al</context>
<context position="27833" citStr="Bengio et al., 2015" startWordPosition="4392" endWordPosition="4395">-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011;</context>
</contexts>
<marker>Bengio, Goodfellow, Courville, 2015</marker>
<rawString>Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville. 2015. Deep learning. Book in preparation for MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1724--1734</pages>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder‚Äìdecoder for statistical machine translation. In EMNLP, pages 1724‚Äì1734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Gated feedback recurrent neural networks.</title>
<date>2015</date>
<publisher>ICML.</publisher>
<contexts>
<context position="13077" citStr="Chung et al., 2015" startWordPosition="2031" endWordPosition="2034">exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows. it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3) ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4) gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5) ht = tanh(it O gt + ft O ht‚àí1) (6) where O stands for element-wise multiplication, Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation. Figure 3 (a) displays</context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2015</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2015. Gated feedback recurrent neural networks. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Misha Denil</author>
<author>Alban Demiraj</author>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
<author>Nando de Freitas</author>
</authors>
<title>Modelling, visualising and summarising documents with</title>
<date>2014</date>
<marker>Denil, Demiraj, Kalchbrenner, Blunsom, de Freitas, 2014</marker>
<rawString>Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. 2014. Modelling, visualising and summarising documents with</rawString>
</citation>
<citation valid="false">
<title>a single convolutional neural network. arXiv preprint:1406.3830.</title>
<marker></marker>
<rawString>a single convolutional neural network. arXiv preprint:1406.3830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Minghui Qiu</author>
<author>Chao-Yuan Wu</author>
<author>Alexander J Smola</author>
<author>Jing Jiang</author>
<author>Chong Wang</author>
</authors>
<title>Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars).</title>
<date>2014</date>
<booktitle>In SIGKDD,</booktitle>
<pages>193--202</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5506" citStr="Diao et al., 2014" startWordPosition="820" endWordPosition="823">se representations are naturally used as features to classify the sentiment label of each document. The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2. We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4. We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural 2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html 3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge network in document modeling. The main contributions of this work are as follows: ‚Ä¢ We present a neural network approach to encode relations between sentences in document representation for sentiment classification. ‚Ä¢ We report empirical results on four </context>
<context position="16903" citStr="Diao et al., 2014" startWordPosition="2653" endWordPosition="2656">ter as 50. We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(‚àí0.01, 0.01), and set learning rate as 0.03. 3 Experiment We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification. We describe experimental settings and report empirical results in this section. 3.1 Experimental Setting We conduct experiments on large-scale datasets consisting of document reviews. Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015. Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of documents. We do not consider the cases that rating does not match with review texts (Zhang et al., 2014). Statistical information of these datasets are given in Table 1. We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and sentence splitting</context>
<context position="20144" citStr="Diao et al., 2014" startWordPosition="3182" endWordPosition="3185"> 0.453 3.00 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold. (4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier. (5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation. (6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling. (7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014). (8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set. 3.3 Comparison to Other Methods Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (hig</context>
</contexts>
<marker>Diao, Qiu, Wu, Smola, Jiang, Wang, 2014</marker>
<rawString>Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In SIGKDD, pages 193‚Äì202. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis.</title>
<date>2014</date>
<booktitle>In AAAI,</booktitle>
<pages>1537--1543</pages>
<contexts>
<context position="29120" citStr="Dong et al., 2014" startWordPosition="4587" endWordPosition="4590">c word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also ver</context>
</contexts>
<marker>Dong, Wei, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014. Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis. In AAAI, pages 1537‚Äì1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="18473" citStr="Fan et al., 2008" startWordPosition="2912" endWordPosition="2915">t labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive). ÔøΩN i (goldi ‚àí predictedi)2 MSE = 9 N () 3.2 Baseline Methods We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5. (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Yelp 2013 Yelp 2014 Yelp 2015 IMDB Accuracy MSE Accuracy MSE Accuracy MSE Accuracy MSE Majority 0.356 3.06 0.361 3.28 0.369 3.30 0.179 17.46 SVM + Unigrams 0.589 0.79 0.600 0.78 0.611 0.75 0.399 4.23 SVM + Bigrams 0.576 0.75 0.616 0.6</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. JMLR.</rawString>
</citation>
<citation valid="true">
<title>On sense and reference.</title>
<date>1997</date>
<pages>563--584</pages>
<location>Ludlow</location>
<marker>1997</marker>
<rawString>Gottlob Frege. 1892. On sense and reference. Ludlow (1997), pages 563‚Äì584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gayatree Ganu</author>
<author>Noemie Elhadad</author>
<author>Am¬¥elie Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In WebDB.</booktitle>
<contexts>
<context position="27530" citStr="Ganu et al., 2009" startWordPosition="4346" endWordPosition="4349">blem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algori</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>Gayatree Ganu, Noemie Elhadad, and Am¬¥elie Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In WebDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In ICML,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="29275" citStr="Glorot et al. (2011)" startWordPosition="4611" endWordPosition="4614">essenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural netwo</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, pages 513‚Äì520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Goldberg</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Seeing stars when there aren‚Äôt many stars: graph-based semisupervised learning for sentiment categorization. In GraphBased Method for NLP,</title>
<date>2006</date>
<pages>45--52</pages>
<contexts>
<context position="27187" citStr="Goldberg and Zhu (2006)" startWordPosition="4295" endWordPosition="4298">edNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminati</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Andrew B Goldberg and Xiaojin Zhu. 2006. Seeing stars when there aren‚Äôt many stars: graph-based semisupervised learning for sentiment categorization. In GraphBased Method for NLP, pages 45‚Äì52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Navdeep Jaitly</author>
<author>A-R Mohamed</author>
</authors>
<title>Hybrid speech recognition with deep bidirectional lstm.</title>
<date>2013</date>
<booktitle>In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on,</booktitle>
<pages>273--278</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="12942" citStr="Graves et al., 2013" startWordPosition="2006" endWordPosition="2009">imensions of hidden vector and sentence vector, respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows. it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3) ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4) gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5) ht = tanh(it O gt + ft O ht‚àí1) (6) where O stands for element-wise multiplication, Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, </context>
<context position="14239" citStr="Graves et al., 2013" startWordPosition="2236" endWordPosition="2239">et a better document representation. Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification. We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities. The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015). We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator. The model is embedded in Figure 1. 2.3 Sentiment Classification The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering. Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, which is calculated as follows. Pi = c xp(xi) (7) ,i,=1 exp(xi,) We conduct experiments in a supervised learning setting, where each document in t</context>
</contexts>
<marker>Graves, Jaitly, Mohamed, 2013</marker>
<rawString>Alex Graves, Navdeep Jaitly, and A-R Mohamed. 2013. Hybrid speech recognition with deep bidirectional lstm. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273‚Äì278. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>894--904</pages>
<contexts>
<context position="29194" citStr="Hermann and Blunsom, 2013" startWordPosition="4596" endWordPosition="4599">er line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In ACL, pages 894‚Äì904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¬®urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="12523" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1939" endWordPosition="1943">s of linear layers. Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht‚àí1. The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh. ht = tanh(Wr ¬∑ [ht‚àí1; st] + br) (2) where Wr E Rlhx(lh+loc), br E Rlh, lh and loc are dimensions of hidden vector and sentence vector, respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¬®urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735‚Äì1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language. In</title>
<date>2014</date>
<booktitle>NIPS,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="29068" citStr="Irsoy and Cardie, 2014" startWordPosition="4579" endWordPosition="4582"> Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural n</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In NIPS, pages 2096‚Äì2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>Effective use of word order for text categorization with convolutional neural networks.</title>
<date>2015</date>
<publisher>NAACL.</publisher>
<contexts>
<context position="8594" citStr="Johnson and Zhang, 2015" startWordPosition="1286" endWordPosition="1289">can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order t</context>
<context position="29465" citStr="Johnson and Zhang, 2015" startWordPosition="4642" endWordPosition="4645">rsive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning. They verify the effectiveness of</context>
</contexts>
<marker>Johnson, Zhang, 2015</marker>
<rawString>Rie Johnson and Tong Zhang. 2015. Effective use of word order for text categorization with convolutional neural networks. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech &amp; language processing.</title>
<date>2000</date>
<publisher>Pearson Education</publisher>
<contexts>
<context position="1375" citStr="Jurafsky and Martin, 2000" startWordPosition="196" endWordPosition="199">cument level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavi</context>
<context position="17643" citStr="Jurafsky and Martin, 2000" startWordPosition="2779" endWordPosition="2782">ings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of documents. We do not consider the cases that rating does not match with review texts (Zhang et al., 2014). Statistical information of these datasets are given in Table 1. We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014) on all these datasets. We use accuracy (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance. We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive). ÔøΩN i (goldi ‚àí predictedi)2 MSE = 9 N () 3.2 Baseline Methods We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic </context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Dan Jurafsky and James H Martin. 2000. Speech &amp; language processing. Pearson Education India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences. In</title>
<date>2014</date>
<booktitle>ACL,</booktitle>
<pages>655--665</pages>
<contexts>
<context position="8569" citStr="Kalchbrenner et al., 2014" startWordPosition="1282" endWordPosition="1285">y size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolu</context>
<context position="29419" citStr="Kalchbrenner et al., 2014" startWordPosition="4633" endWordPosition="4636">ocher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model docu</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In ACL, pages 655‚Äì665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. In</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1746--1751</pages>
<contexts>
<context position="8542" citStr="Kim, 2014" startWordPosition="1280" endWordPosition="1281">s vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the meth</context>
<context position="20391" citStr="Kim, 2014" startWordPosition="3222" endWordPosition="3223">d vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier. (5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation. (6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling. (7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014). (8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set. 3.3 Comparison to Other Methods Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better). The best method in each dataset and each evaluation metric is in bold. From Table 2, we can see that majority is the worst method because it does not capture any textual semantics. SVM classifiers with uni</context>
<context position="29392" citStr="Kim, 2014" startWordPosition="4631" endWordPosition="4632">function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also inv</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP, pages 1746‚Äì 1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>723--762</pages>
<contexts>
<context position="18559" citStr="Kiritchenko et al., 2014" startWordPosition="2923" endWordPosition="2926">ment strengths (e.g. one star means strong negative and five star means strong positive). ÔøΩN i (goldi ‚àí predictedi)2 MSE = 9 N () 3.2 Baseline Methods We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5. (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Yelp 2013 Yelp 2014 Yelp 2015 IMDB Accuracy MSE Accuracy MSE Accuracy MSE Accuracy MSE Majority 0.356 3.06 0.361 3.28 0.369 3.30 0.179 17.46 SVM + Unigrams 0.589 0.79 0.600 0.78 0.611 0.75 0.399 4.23 SVM + Bigrams 0.576 0.75 0.616 0.65 0.624 0.63 0.409 3.74 SVM + TextFeatures 0.598 0.68 0.618 0.63 0.624 0.60 0.405 3.56</context>
<context position="21552" citStr="Kiritchenko et al., 2014" startWordPosition="3409" endWordPosition="3412">does not capture any textual semantics. SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers 6We use Skipgram as it performs slightly better than CBOW in the experiment. We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus. among all baseline methods. Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014). Furthermore, the aforementioned bag-of-features are discrete and sparse. For example, the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features. Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged. AverageSG is a straight forward way to compose document representation without feature engineering. Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment cla</context>
<context position="27662" citStr="Kiritchenko et al., 2014" startWordPosition="4366" endWordPosition="4369">n unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 201</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mohammad. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, pages 723‚Äì762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="28458" citStr="Labutov and Lipson, 2013" startWordPosition="4490" endWordPosition="4493"> Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Car</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In ICML,</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="2907" citStr="Mikolov (2014)" startWordPosition="430" endWordPosition="431">lations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its ‚Äúsparse‚Äù and ‚Äúdiscrete‚Äù characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings of sentences before considering semantic relations between them. This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation. In this paper, we introduce a neural network approach to learn continuous document representation for sentimen</context>
<context position="5363" citStr="Mikolov, 2014" startWordPosition="800" endWordPosition="801">rrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations. These representations are naturally used as features to classify the sentiment label of each document. The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2. We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4. We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural 2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html 3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge network in document modeling. The main contributions of this work are as follows: ‚Ä¢ We present a neural network</context>
<context position="20513" citStr="Mikolov, 2014" startWordPosition="3241" endWordPosition="3242">SVM classifier. (5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation. (6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling. (7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014). (8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set. 3.3 Comparison to Other Methods Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better). The best method in each dataset and each evaluation metric is in bold. From Table 2, we can see that majority is the worst method because it does not capture any textual semantics. SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers 6We use Skipg</context>
<context position="29542" citStr="Mikolov (2014)" startWordPosition="4656" endWordPosition="4657"> is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning. They verify the effectiveness of LSTM in text generation task. 5 Conclusion We introduce neural network model</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In ICML, pages 1188‚Äì1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Dan Jurafsky</author>
<author>Eudard Hovy</author>
</authors>
<title>When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185.</title>
<date>2015</date>
<contexts>
<context position="8611" citStr="Li et al., 2015" startWordPosition="1290" endWordPosition="1293">ed from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local s</context>
<context position="29788" citStr="Li et al., 2015" startWordPosition="4691" endWordPosition="4694">nn and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning. They verify the effectiveness of LSTM in text generation task. 5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification obj</context>
</contexts>
<marker>Li, Jurafsky, Hovy, 2015</marker>
<rawString>Jiwei Li, Dan Jurafsky, and Eudard Hovy. 2015a. When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Minh-Thang Luong</author>
<author>Dan Jurafsky</author>
</authors>
<title>A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057.</title>
<date>2015</date>
<contexts>
<context position="8611" citStr="Li et al., 2015" startWordPosition="1290" endWordPosition="1293">ed from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local s</context>
<context position="29788" citStr="Li et al., 2015" startWordPosition="4691" endWordPosition="4694">nn and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning. They verify the effectiveness of LSTM in text generation task. 5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification obj</context>
</contexts>
<marker>Li, Luong, Jurafsky, 2015</marker>
<rawString>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015b. A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
</authors>
<title>Feature weight tuning for recursive neural networks.</title>
<date>2014</date>
<tech>Arxiv preprint,</tech>
<pages>1412--3714</pages>
<contexts>
<context position="29021" citStr="Li, 2014" startWordPosition="4574" endWordPosition="4575">s (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of </context>
</contexts>
<marker>Li, 2014</marker>
<rawString>Jiwei Li. 2014. Feature weight tuning for recursive neural networks. Arxiv preprint, 1412.3714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1407" citStr="Liu, 2012" startWordPosition="204" endWordPosition="205">arge-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of da</context>
<context position="26771" citStr="Liu, 2012" startWordPosition="4234" endWordPosition="4235">parably, but disappointingly both of them fail to transcend Average. After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effect</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1‚Äì167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="28432" citStr="Maas et al., 2011" startWordPosition="4486" endWordPosition="4489">ngio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recu</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL, pages 142‚Äì150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¬®utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT press.</publisher>
<marker>Manning, Sch¬®utze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Sch¬®utze. 1999. Foundations of statistical natural language processing. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="17548" citStr="Manning et al., 2014" startWordPosition="2763" endWordPosition="2767">view datasets from Yelp Dataset Challenge in 2013, 2014 and 2015. Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of documents. We do not consider the cases that rating does not match with review texts (Zhang et al., 2014). Statistical information of these datasets are given in Table 1. We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014) on all these datasets. We use accuracy (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance. We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive). ÔøΩN i (goldi ‚àí predictedi)2 MSE = 9 N () 3.2 Baseline Methods We compare our methods (Conv-GRNN and LSTM-GRNN) with the follo</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In ACL, pages 55‚Äì60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="8142" citStr="Mikolov et al., 2013" startWordPosition="1217" endWordPosition="1220">omposition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on exte</context>
<context position="16373" citStr="Mikolov et al., 2013" startWordPosition="2576" endWordPosition="2579"> classes, d represents a document. P9(d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0. We take the derivative of loss function through back-propagation with respect to the whole set of parameters Œ∏ = [Wc; bc; Wi; bi; Wf; bf; Wr; br; Wsoftmax, bsoftmaxl, and update parameters with stochastic gradient descent. We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50. We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(‚àí0.01, 0.01), and set learning rate as 0.03. 3 Experiment We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification. We describe experimental settings and report empirical results in this section. 3.1 Experimental Setting We conduct experiments on large-scale datasets consisting of document reviews. Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2</context>
<context position="19829" citStr="Mikolov et al., 2013" startWordPosition="3136" endWordPosition="3139"> 0.319 5.57 SVM + SSWE 0.535 1.12 0.543 1.13 0.554 1.11 0.262 9.16 JMARS N/A ‚Äì N/A ‚Äì N/A ‚Äì N/A 4.97 Paragraph Vector 0.577 0.86 0.592 0.70 0.605 0.61 0.341 4.69 Convolutional NN 0.597 0.76 0.610 0.68 0.615 0.68 0.376 3.30 Conv-GRNN 0.637 0.56 0.655 0.51 0.660 0.50 0.425 2.71 LSTM-GRNN 0.651 0.50 0.671 0.48 0.676 0.49 0.453 3.00 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold. (4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier. (5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation. (6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling. (7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014). (8) We implemen</context>
<context position="28242" citStr="Mikolov et al., 2013" startWordPosition="4455" endWordPosition="4458">icon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recur</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111‚Äì3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="28652" citStr="Mitchell and Lapata, 2010" startWordPosition="4518" endWordPosition="4521">ne line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388‚Äì1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Paltoglou</author>
<author>Mike Thelwall</author>
</authors>
<title>A study of information retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1386--1395</pages>
<contexts>
<context position="1878" citStr="Paltoglou and Thelwall, 2010" startWordPosition="273" endWordPosition="276">understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capt</context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment analysis. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 1386‚Äì1395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="18710" citStr="Pang and Lee, 2005" startWordPosition="2945" endWordPosition="2948">pare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5. (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Yelp 2013 Yelp 2014 Yelp 2015 IMDB Accuracy MSE Accuracy MSE Accuracy MSE Accuracy MSE Majority 0.356 3.06 0.361 3.28 0.369 3.30 0.179 17.46 SVM + Unigrams 0.589 0.79 0.600 0.78 0.611 0.75 0.399 4.23 SVM + Bigrams 0.576 0.75 0.616 0.65 0.624 0.63 0.409 3.74 SVM + TextFeatures 0.598 0.68 0.618 0.63 0.624 0.60 0.405 3.56 SVM + AverageSG 0.543 1.11 0.557 1.08 0.568 1.04 0.319 5.57 SVM + SSWE 0.535 1.12 0.543 1.13 0.554 1.11 0.262 9.16 JMARS N/A ‚Äì N/A ‚Äì N/A ‚Äì N/A 4.97 Pa</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115‚Äì 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1395" citStr="Pang and Lee, 2008" startWordPosition="200" endWordPosition="203">sification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the </context>
<context position="26759" citStr="Pang and Lee, 2008" startWordPosition="4230" endWordPosition="4233">rent Avg perform comparably, but disappointingly both of them fail to transcend Average. After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on desi</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1‚Äì135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1606" citStr="Pang et al., 2002" startWordPosition="236" endWordPosition="239"> gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in th</context>
<context position="5454" citStr="Pang et al., 2002" startWordPosition="812" endWordPosition="815"> inherent relations in document representations. These representations are naturally used as features to classify the sentiment label of each document. The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2. We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4. We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural 2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html 3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge network in document modeling. The main contributions of this work are as follows: ‚Ä¢ We present a neural network approach to encode relations between sentences in document representation for sentiment cl</context>
<context position="21035" citStr="Pang et al., 2002" startWordPosition="3331" endWordPosition="3334"> We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set. 3.3 Comparison to Other Methods Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better). The best method in each dataset and each evaluation metric is in bold. From Table 2, we can see that majority is the worst method because it does not capture any textual semantics. SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers 6We use Skipgram as it performs slightly better than CBOW in the experiment. We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus. among all baseline methods. Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014). Furthermore, the aforementioned bag-of-features are discrete and sparse. For exam</context>
<context position="26851" citStr="Pang et al., 2002" startWordPosition="4246" endWordPosition="4249">r adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative featur</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP, pages 79‚Äì 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romain Paulus</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Global belief recursive neural networks.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>2888--2896</pages>
<contexts>
<context position="28987" citStr="Paulus et al., 2014" startWordPosition="4567" endWordPosition="4570">‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document</context>
</contexts>
<marker>Paulus, Socher, Manning, 2014</marker>
<rawString>Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global belief recursive neural networks. In NIPS, pages 2888‚Äì2896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation. In</title>
<date>2014</date>
<booktitle>EMNLP,</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="8167" citStr="Pennington et al., 2014" startWordPosition="1221" endWordPosition="1225">scribe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or consti</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532‚Äì1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lizhen Qu</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>The bag-of-opinions method for review rating prediction from sparse text patterns.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>913--921</pages>
<contexts>
<context position="1847" citStr="Qu et al., 2010" startWordPosition="269" endWordPosition="272">nd is crucial to understand user generated content in social networks or product reviews (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn‚àóCorresponding author. 1 Codes and datasets are publicly available at http://ir.hit.edu.cn/Àúdytang. ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have great influences on determining the meaning and the overall polarity of a document. However, existing studies typ</context>
<context position="27565" citStr="Qu et al., 2010" startWordPosition="4351" endWordPosition="4354">e machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of</context>
</contexts>
<marker>Qu, Ifrim, Weikum, 2010</marker>
<rawString>Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The bag-of-opinions method for review rating prediction from sparse text patterns. In COLING, pages 913‚Äì921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8047" citStr="Socher et al., 2013" startWordPosition="1203" endWordPosition="1206">hen used as features for document level sentiment classification (Section 2.3). 2.1 Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors </context>
<context position="28812" citStr="Socher et al. (2013" startWordPosition="4542" endWordPosition="4545">on way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al</context>
<context position="31488" citStr="Socher et al., 2013" startWordPosition="4940" endWordPosition="4943">ome future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondition‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms. However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function </context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013a. Parsing with compositional vector grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="8047" citStr="Socher et al., 2013" startWordPosition="1203" endWordPosition="1206">hen used as features for document level sentiment classification (Section 2.3). 2.1 Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors </context>
<context position="28812" citStr="Socher et al. (2013" startWordPosition="4542" endWordPosition="4545">on way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al</context>
<context position="31488" citStr="Socher et al., 2013" startWordPosition="4940" endWordPosition="4943">ome future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondition‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms. However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631‚Äì1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="12995" citStr="Sutskever et al., 2014" startWordPosition="2014" endWordPosition="2018">spectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows. it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3) ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4) gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5) ht = tanh(it O gt + ft O ht‚àí1) (6) where O stands for element-wise multiplication, Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the sem</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS, pages 3104‚Äì3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8942" citStr="Tai et al., 2015" startWordPosition="1342" endWordPosition="1345">NN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of uni</context>
<context position="31840" citStr="Tai et al., 2015" startWordPosition="4995" endWordPosition="4998">fine a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondition‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms. However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function is an interesting future work. Acknowledgments The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions. We also would like to thank three anonymous reviewers for their valuable comments and suggestions. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National N</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="8187" citStr="Tang et al., 2014" startWordPosition="1226" endWordPosition="1229">entation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw E Rd√ó|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results</context>
<context position="20022" citStr="Tang et al., 2014" startWordPosition="3164" endWordPosition="3167">.68 0.615 0.68 0.376 3.30 Conv-GRNN 0.637 0.56 0.655 0.51 0.660 0.50 0.425 2.71 LSTM-GRNN 0.651 0.50 0.671 0.48 0.676 0.49 0.453 3.00 Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold. (4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier. (5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation. (6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling. (7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014). (8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set. 3.3 Comparison t</context>
<context position="22262" citStr="Tang et al., 2014" startWordPosition="3521" endWordPosition="3524">feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features. Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged. AverageSG is a straight forward way to compose document representation without feature engineering. Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classification. We try to make better use of the sentiment information to learn a better SSWE (Tang et al., 2014), e.g. setting a large window size. However, its performance is still worse than context-based word embedding. This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in document level reviews, while Tang et al. (2014) learn SSWE by assigning sentiment label of a text to each phrase it contains. How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work. Since JMARS outputs real-valued outputs, we only evaluate it in terms of MSE. We can see that sophisticated baseline methods such as JMARS, paragraph </context>
<context position="28478" citStr="Tang et al., 2014" startWordPosition="4494" endWordPosition="4497">emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In ACL, pages 1555‚Äì1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Learning semantic representations of users and products for document level sentiment classification.</title>
<date>2015</date>
<booktitle>In ACL,</booktitle>
<pages>1014--1023</pages>
<contexts>
<context position="9079" citStr="Tang et al., 2015" startWordPosition="1364" endWordPosition="1367">ateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigramFigure 2: Sentence composition with convolutional neural network. s and trigrams in a sentence. Each filter consists of a li</context>
</contexts>
<marker>Tang, Qin, Liu, 2015</marker>
<rawString>Duyu Tang, Bing Qin, and Ting Liu. 2015. Learning semantic representations of users and products for document level sentiment classification. In ACL, pages 1014‚Äì1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="26866" citStr="Turney, 2002" startWordPosition="4250" endWordPosition="4251">es, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In ACL, pages 417‚Äì424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>90--94</pages>
<contexts>
<context position="2559" citStr="Wang and Manning (2012)" startWordPosition="377" endWordPosition="380">rformance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like ‚Äúcontrast‚Äù and ‚Äúcause‚Äù have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its ‚Äúsparse‚Äù and ‚Äúdiscrete‚Äù characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it o</context>
<context position="27498" citStr="Wang and Manning, 2012" startWordPosition="4340" endWordPosition="4343">d Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Tradi</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, pages 90‚Äì94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Xia</author>
<author>Chengqing Zong</author>
</authors>
<title>Exploring the use of word relation features for sentiment classification. In</title>
<date>2010</date>
<booktitle>COLING,</booktitle>
<pages>1336--1344</pages>
<contexts>
<context position="27607" citStr="Xia and Zong, 2010" startWordPosition="4358" endWordPosition="4361">ed learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio</context>
</contexts>
<marker>Xia, Zong, 2010</marker>
<rawString>Rui Xia and Chengqing Zong. 2010. Exploring the use of word relation features for sentiment classification. In COLING, pages 1336‚Äì1344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liheng Xu</author>
<author>Kang Liu</author>
<author>Jun Zhao</author>
</authors>
<title>Joint opinion relation detection using one-class deep neural network. In</title>
<date>2014</date>
<booktitle>COLING,</booktitle>
<pages>677--687</pages>
<contexts>
<context position="29253" citStr="Xu et al., 2014" startWordPosition="4607" endWordPosition="4610">d Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated </context>
</contexts>
<marker>Xu, Liu, Zhao, 2014</marker>
<rawString>Liheng Xu, Kang Liu, and Jun Zhao. 2014. Joint opinion relation detection using one-class deep neural network. In COLING, pages 677‚Äì687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelvin Xu</author>
<author>Jimmy Ba</author>
<author>Ryan Kiros</author>
<author>Aaron Courville</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard Zemel</author>
<author>Yoshua Bengio</author>
</authors>
<title>Show, attend and tell: Neural image caption generation with visual attention.</title>
<date>2015</date>
<publisher>ICML.</publisher>
<contexts>
<context position="13013" citStr="Xu et al., 2015" startWordPosition="2019" endWordPosition="2022">y, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows. it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3) ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4) gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5) ht = tanh(it O gt + ft O ht‚àí1) (6) where O stands for element-wise multiplication, Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentence</context>
</contexts>
<marker>Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, Bengio, 2015</marker>
<rawString>Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="28683" citStr="Yessenalina and Cardie (2011)" startWordPosition="4522" endWordPosition="4525">on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. ‚Äúgood‚Äù and ‚Äúbad‚Äù) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use sta</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In EMNLP, pages 172‚Äì182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Zaremba</author>
<author>Ilya Sutskever</author>
</authors>
<title>Learning to execute. arXiv preprint arXiv:1410.4615.</title>
<date>2014</date>
<contexts>
<context position="12971" citStr="Zaremba and Sutskever, 2014" startWordPosition="2010" endWordPosition="2013">ector and sentence vector, respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows. it = sigmoid(Wi ¬∑ [ht‚àí1; st] + bi) (3) ft = sigmoid(Wf ¬∑ [ht‚àí1; st] + bf) (4) gt = tanh(Wr ¬∑ [ht‚àí1; st] + br) (5) ht = tanh(it O gt + ft O ht‚àí1) (6) where O stands for element-wise multiplication, Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discar</context>
</contexts>
<marker>Zaremba, Sutskever, 2014</marker>
<rawString>Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongfeng Zhang</author>
<author>Haochen Zhang</author>
<author>Min Zhang</author>
<author>Yiqun Liu</author>
<author>Shaoping Ma</author>
</authors>
<title>Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification.</title>
<date>2014</date>
<booktitle>In SIGIR,</booktitle>
<pages>1027--1030</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17239" citStr="Zhang et al., 2014" startWordPosition="2711" endWordPosition="2714">ntiment classification. We describe experimental settings and report empirical results in this section. 3.1 Experimental Setting We conduct experiments on large-scale datasets consisting of document reviews. Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015. Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of documents. We do not consider the cases that rating does not match with review texts (Zhang et al., 2014). Statistical information of these datasets are given in Table 1. We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014) on all these datasets. We use accuracy (Manning and Sch¬®utze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance. We use MSE to measure the divergences between p</context>
</contexts>
<marker>Zhang, Zhang, Zhang, Liu, Ma, 2014</marker>
<rawString>Yongfeng Zhang, Haochen Zhang, Min Zhang, Yiqun Liu, and Shaoping Ma. 2014. Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification. In SIGIR, pages 1027‚Äì1030. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhao</author>
<author>Zhengdong Lu</author>
<author>Pascal Poupart</author>
</authors>
<title>Self-adaptive hierarchical sentence model.</title>
<date>2015</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="14095" citStr="Zhao et al., 2015" startWordPosition="2210" endWordPosition="2213">The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation. Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification. We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities. The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015). We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator. The model is embedded in Figure 1. 2.3 Sentiment Classification The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering. Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, wh</context>
</contexts>
<marker>Zhao, Lu, Poupart, 2015</marker>
<rawString>Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanjun Zhou</author>
<author>Binyang Li</author>
<author>Wei Gao</author>
<author>Zhongyu Wei</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>162--171</pages>
<contexts>
<context position="31297" citStr="Zhou et al., 2011" startWordPosition="4912" endWordPosition="4915">ument composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation. We briefly discuss some future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondition‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms</context>
</contexts>
<marker>Zhou, Li, Gao, Wei, Wong, 2011</marker>
<rawString>Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, and Kam-Fai Wong. 2011. Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. In EMNLP, pages 162‚Äì171, .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over tree structures.</title>
<date>2015</date>
<publisher>ICML.</publisher>
<contexts>
<context position="8961" citStr="Zhu et al., 2015" startWordPosition="1346" endWordPosition="1349">-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigramFigure</context>
<context position="31859" citStr="Zhu et al., 2015" startWordPosition="4999" endWordPosition="5002">iment-sensitive discourse relations (Zhou et al., 2011), such as ‚Äúcontrast‚Äù, ‚Äúcondition‚Äù, ‚Äúcause‚Äù, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms. However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function is an interesting future work. Acknowledgments The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions. We also would like to thank three anonymous reviewers for their valuable comments and suggestions. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foun</context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
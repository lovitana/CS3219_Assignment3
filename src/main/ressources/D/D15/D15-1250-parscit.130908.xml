<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.970847">
A Binarized Neural Network Joint Model for Machine Translation
</title>
<author confidence="0.9697435">
Jingyi Zhang1,2, Masao Utiyama1, Eiichro Sumita1
Graham Neubig2, Satoshi Nakamura2
</author>
<affiliation confidence="0.873186666666667">
1National Institute of Information and Communications Technology,
3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan
2Graduate School of Information Science, Nara Institute of Science and Technology,
</affiliation>
<address confidence="0.636528">
Takayama, Ikoma, Nara 630-0192, Japan
</address>
<email confidence="0.992533">
jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp
neubig/s-nakamura@is.naist.jp
</email>
<sectionHeader confidence="0.994652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926166666667">
The neural network joint model (NNJM),
which augments the neural network lan-
guage model (NNLM) with an m-word
source context window, has achieved large
gains in machine translation accuracy, but
also has problems with high normalization
cost when using large vocabularies. Train-
ing the NNJM with noise-contrastive es-
timation (NCE), instead of standard maxi-
mum likelihood estimation (MLE), can re-
duce computation cost. In this paper, we
propose an alternative to NCE, the bina-
rized NNJM (BNNJM), which learns a bi-
nary classifier that takes both the context
and target words as input, and can be ef-
ficiently trained using MLE. We compare
the BNNJM and NNJM trained by NCE
on various translation tasks.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999298421052632">
Neural network translation models, which learn
mappings over real-valued vector representations
in high-dimensional space, have recently achieved
large gains in translation accuracy (Hu et al., 2014;
Devlin et al., 2014; Sundermeyer et al., 2014;
Auli et al., 2013; Schwenk, 2012; Sutskever et al.,
2014; Bahdanau et al., 2015).
Notably, Devlin et al. (2014) proposed a neural
network joint model (NNJM), which augments the
n-gram neural network language model (NNLM)
with an m-word source context window, as shown
in Figure 1a. While this model is effective, the
computation cost of using it in a large-vocabulary
SMT task is quite expensive, as probabilities need
to be normalized over the entire vocabulary. To
solve this problem, Devlin et al. (2014) pre-
sented a technique to train the NNJM to be self-
normalized and avoided the expensive normaliza-
tion cost during decoding. However, they also
</bodyText>
<figureCaption confidence="0.9340775">
Figure 1: (a) the traditional NNJM and (b) the pro-
posed BNNJM
</figureCaption>
<bodyText confidence="0.968741466666667">
note that this self-normalization technique sacri-
fices neural network accuracy, and the training
process for the self-normalized neural network is
very slow, as with standard maximum likelihood
estimation (MLE).
To remedy the problem of long training times
in the context of NNLMs, Vaswani et al. (2013)
used a method called noise contrastive estimation
(NCE). Compared with MLE, NCE does not re-
quire repeated summations over the whole vocab-
ulary and performs nonlinear logistic regression to
discriminate between the observed data and artifi-
cially generated noise.
This paper proposes an alternative framework of
binarized NNJMs (BNNJM), which are similar to
the NNJM, but use the current target word not as
the output, but as the input of the neural network,
estimating whether the target word under exam-
ination is correct or not, as shown in Figure 1b.
Because the BNNJM uses the current target word
as input, the information about the current target
word can be combined with the context word in-
formation and processed in the hidden layers.
The BNNJM learns a simple binary classifier,
given the context and target words, therefore it
can be trained by MLE very efficiently. “Incor-
rect” target words for the BNNJM can be gen-
erated in the same way as NCE generates noise
m-word
source context
</bodyText>
<equation confidence="0.966953555555556">
ti-n+1~ti-1
P(ti=N)
P(ti=1)
P(ti=2)
P(ti is correct)
P(ti is wrong)
m-word
source context
ti-n+1~ti
</equation>
<page confidence="0.894379">
2094
</page>
<note confidence="0.6278075">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2094–2099,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998027">
for the NNJM. We present a novel noise distribu-
tion based on translation probabilities to train the
NNJM and the BNNJM efficiently.
</bodyText>
<sectionHeader confidence="0.892976" genericHeader="method">
2 Neural Network Joint Model
</sectionHeader>
<equation confidence="0.983772555555555">
|T
Let T = t1  |be a translation of S = s|S|
1 . The
NNJM (Devlin et al., 2014) defines the following
probability,
_ T|ai+(m−1)/2 i−1
P (T|S) P(ti|sai−(m−1)/2, ti−n+1
11=1) (1)
—
</equation>
<bodyText confidence="0.999765">
where target word ti is affiliated with source word
sai. Affiliation ai is derived from the word align-
ments using heuristics1. To estimate these prob-
abilities, the NNJM uses m source context words
and n − 1 target history words as input to a neural
network and performs estimation of unnormalized
probabilities p (ti|C) before normalizing over all
words in the target vocabulary V ,
</bodyText>
<equation confidence="0.9993488">
P (ti|C) = p(ti|C)
Z(C)
Z (C) = P (2)
p (ti0|C)
ti0∈V
</equation>
<bodyText confidence="0.999860933333333">
where C stands for source and target context
words as in Equation 1.
The NNJM can be trained on a word-aligned
parallel corpus using standard MLE, but the cost
of normalizing over the entire vocabulary to calcu-
late the denominator in Equation 2 is quite large.
Devlin et al. (2014)’s self-normalization technique
can avoid normalization cost during decoding, but
not during training.
NCE can be used to train NNLM-style models
(Vaswani et al., 2013) to reduce training times.
NCE creates a noise distribution q (ti), selects k
noise samples ti1, ..., tik for each ti and introduces
a random variable v which is 1 for training exam-
ples and 0 for noise samples,
</bodyText>
<equation confidence="0.99903375">
P
(v = 1, ti|C) = 1 1+k ·p(ti|C)Z(C)
P (v = 0,ti|C) = k
1+k · q (ti).
</equation>
<bodyText confidence="0.981988666666667">
NCE trains the model to distinguish training
data from noise by maximize the conditional like-
lihood,
</bodyText>
<equation confidence="0.995642666666667">
k
L = log P (v = 1IC,ti)+ P log P (v = 0|C, tik).
j=1
</equation>
<bodyText confidence="0.9840785">
The normalization cost can be avoided by using
p (ti|C) as an approximation of P (ti|C).2
</bodyText>
<footnote confidence="0.988840285714286">
1If ti aligns to exactly one source word, ai is the index of
this source word; If ti aligns to multiple source words, ai is
the index of the aligned word in the middle; If ti is unaligned,
they inherit its affiliation from the closest aligned word.
2The theoretical properties of self-normalization tech-
niques, including NCE and Devlin et al. (2014)’s method,
are investigated by Andreas and Klein (2015).
</footnote>
<sectionHeader confidence="0.978608" genericHeader="method">
3 Binarized NNJM
</sectionHeader>
<bodyText confidence="0.950921222222222">
In this paper, we propose a new framework of the
binarized NNJM (BNNJM), which is similar to
the NNJM but learns not to predict the next word
given the context, but solves a binary classifica-
tion problem by adding a variable v ∈ {0, 1} that
stands for whether the current target word ti is cor-
rectly/wrongly produced in terms of source con-
text words sai+(m−1)/2 ai−(m−1)/2 and target history words
i−1
</bodyText>
<equation confidence="0.821535666666667">
ti−n+1 ,
P( v  |sai+(m−1)/2 i−1 ) .
ai−(m−1)/2, ti−n+1, ti
The BNNJM is learned by a feed-
forward neural network with m + n inputs
n o
sai+(m−1)/2 ai−(m−1)/2, ti−1
i−n+1, ti and two outputs for
v = 1/0.
</equation>
<bodyText confidence="0.999899071428572">
Because the BNNJM uses the current target
word as input, the information about the current
target word can be combined with the context
word information and processed in the hidden lay-
ers. Thus, the hidden layers can be used to learn
the difference between correct target words and
noise in the BNNJM, while in the NNJM the hid-
den layers just contain information about context
words and only the output layer can be used to dis-
criminate between the training data and noise, giv-
ing the BNNJM more power to learn this classifi-
cation problem.
We can use the BNNJM probability in transla-
tion as an approximation for the NNJM as below,
</bodyText>
<equation confidence="0.983011714285714">
� �
ti|sai+(m−1)/2
P ai−(m−1)/2, ti−1
� i−n+1 �
≈ P v = 1|sai+(m−1)/2
ai−(m−1)/2, ti−1
i−n+1, ti .
</equation>
<bodyText confidence="0.8318844">
As a binary classifier, the gradient for a sin-
gle example in the BNNJM can be calculated
efficiently by MLE without it being necessary
to calculate the softmax over the full vocabu-
lary. On the other hand, we need to create
“positive” and “negative” examples for the clas-
sifier. Positive examples can be extracted di-
rectly from the word-aligned parallel corpus as
Dai+(m−1)/2i−1
sai−(m−1)/2, ti−n+1 , tiE; Negative examples can
be generated for each positive example in the
same way that NCE generates noise data as
D sai+(m−1)/2
ai−(m−1)/2, ti−1
i−n+1, ti0E , where ti0 ∈ V \ {ti}.
</bodyText>
<sectionHeader confidence="0.996799" genericHeader="method">
4 Noise Sampling
</sectionHeader>
<subsectionHeader confidence="0.999026">
4.1 Unigram Noise
</subsectionHeader>
<bodyText confidence="0.9991705">
Vaswani et al. (2013) adopted the unigram proba-
bility distribution (UPD) to sample noise for train-
</bodyText>
<page confidence="0.915469">
2095
</page>
<figure confidence="0.998562666666667">
a � PC* A Vr tip, t0� choose to combine these target/source words as a
new target/source word.3
I will arrange for someone to take you round
</figure>
<figureCaption confidence="0.9720405">
Figure 2: A parallel sentence pair.
ing NNLMs with NCE,
</figureCaption>
<equation confidence="0.992381333333333">
q (ti0) = cc
Eocc(ur(ti00)
ti00∈V
</equation>
<bodyText confidence="0.999952">
where occur (ti0) stands for how many times ti0
occurs in the training corpus.
</bodyText>
<subsectionHeader confidence="0.936676">
4.2 Translation Model Noise
</subsectionHeader>
<bodyText confidence="0.999839571428571">
In this paper, we propose a noise distribution spe-
cialized for translation models, such as the NNJM
or BNNJM.
Figure 2 gives a Chinese-to-English parallel
sentence pair with word alignments to demon-
strate the intuition behind our method. Focusing
on sai=“#��”, this is translated into ti =“ar-
range”. For this positive example, UPD is allowed
to sample any arbitrary noise, such as ti0 = “ba-
nana”. However, in this case, noise ti0 = “banana”
is not useful for model training, as constraints on
possible translations given by the phrase table en-
sure that “V�” will never be translated into “ba-
nana”. On the other hand, noise ti0 = “arranges”
and “arrangement” are both possible translations
of “#�41�” and therefore useful training data, that
we would like our model to penalize.
Based on this intuition, we propose the use
of another noise distribution that only uses ti0
that are possible translations of sai, i.e., ti0 E
U (sai) \ {ti}, where U (sai) contains all target
words aligned to sai in the parallel corpus.
Because U (sai) may be quite large and con-
tain many wrong translations caused by wrong
alignments, “banana” may actually be included
in U(“#� �”). To mitigate the effect of un-
common examples, we use a translation proba-
bility distribution (TPD) to sample noise ti0 from
</bodyText>
<equation confidence="0.851921">
U (sai) \ {ti} as follows,
align(sai,ti0)
�
q (tz s ai)= Et.�0EU� sai � align(sai,ti00)
a
</equation>
<bodyText confidence="0.998776428571429">
where align (sai, ti0) is how many times ti0 is
aligned to sai in the parallel corpus.
Note that ti could be unaligned, in which case
we assume that it is aligned to a special null word.
Noise for unaligned words is sampled according to
the TPD of the null word. If several target/source
words are aligned to one source/target word, we
</bodyText>
<sectionHeader confidence="0.998638" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98562">
5.1 Setting
</subsectionHeader>
<bodyText confidence="0.998973472222222">
We evaluated the effectiveness of the proposed ap-
proach for Chinese-to-English (CE), Japanese-to-
English (JE) and French-to-English (FE) transla-
tion tasks. The datasets officially provided for the
patent machine translation task at NTCIR-9 (Goto
et al., 2011) were used for the CE and JE tasks.
The development and test sets were both provided
for the CE task while only the test set was provided
for the JE task. Therefore, we used the sentences
from the NTCIR-8 JE test set as the development
set. Word segmentation was done by BaseSeg
(Zhao et al., 2006) for Chinese and Mecab4 for
Japanese. For the FE language pair, we used stan-
dard data for the WMT 2014 translation task. The
training sets for CE, JE and FE tasks contain 1M,
3M and 2M sentence pairs, respectively.
For each translation task, a recent version of
Moses HPB decoder (Koehn et al., 2007) with the
training scripts was used as the baseline (Base).
We used the default parameters for Moses, and
a 5-gram language model was trained on the tar-
get side of the training corpus using the IRSTLM
Toolkit5 with improved Kneser-Ney smoothing.
Feature weights were tuned by MERT (Och,
2003).
The word-aligned training set was used to learn
the NNJM and the BNNJM.6 For both NNJM and
BNNJM, we set m = 7 and n = 5. The NNJM
was trained by NCE using UPD and TPD as noise
distributions. The BNNJM was trained by stan-
dard MLE using UPD and TPD to generate nega-
tive examples.
The number of noise samples for NCE was set
to be 100. For the BNNJM, we used only one neg-
ative example for each positive example in each
training epoch, as the BNNJM needs to calculate
</bodyText>
<footnote confidence="0.978164">
3The processing for multiple alignments helps sample
more useful negative examples for TPD, and had little ef-
fect on the translation performance when UPD was used as
the noise distribution for the NNJM and the BNNJM in our
preliminary experiments.
4http://sourceforge.net/projects/mecab/files/
5http://hlt.fbk.eu/en/irstlm
6Both the NNJM and the BNNJM had one hidden layer,
100 hidden nodes, input embedding dimension 50, output
embedding dimension 50. A small set of training data was
used as validation data. The training process was stopped
when validation likelihood stopped increasing.
</footnote>
<page confidence="0.972226">
2096
</page>
<table confidence="0.9986035">
CE T JE T FE T
E E E
NNJM UPD 20 22 19 49 20 28
TPD 4 6 4
BNNJM UPD 14 16 12 34 11 22
TPD 11 9 9
</table>
<tableCaption confidence="0.9740735">
Table 1: Epochs (E) and time (T) in minutes per
epoch for each task.
</tableCaption>
<table confidence="0.999851333333333">
CE JE FE
Base UPD 32.95 30.13 24.56
NNJM TPD 34.36+ 31.30+ 24.68
BNNJM UPD 34.60+ 31.50+ 24.80
TPD 32.89 30.04 24.50
35.05+* 31.42+ 25.84+*
</table>
<tableCaption confidence="0.995255">
Table 2: Translation results. The symbol + and *
</tableCaption>
<bodyText confidence="0.9988281">
represent significant differences at the p &lt; 0.01
level against Base and NNJM+UPD, respectively.
Significance tests were conducted using bootstrap
resampling (Koehn, 2004).
the whole neural network (not just the output layer
like the NNJM) for each noise sample and thus
noise computation is more expensive. However,
for different epochs, we resampled the negative
example for each positive example, so the BNNJM
can make use of different negative examples.
</bodyText>
<subsectionHeader confidence="0.926418">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99912828">
Table 1 shows how many epochs these two models
needed and the training time for each epoch on a
10-core 3.47GHz Xeon X5690 machine.7 Trans-
lation results are shown in Table 2.
We can see that using TPD instead of UPD
as a noise distribution for the NNJM trained by
NCE can speed up the training process signifi-
cantly, with a small improvement in performance.
But for the BNNJM, using different noise distribu-
tions affects translation performance significantly.
The BNNJM with UPD does not improve over
the baseline system, likely due to the small num-
ber of noise samples used in training the BNNJM,
while the BNNJM with TPD achieves good per-
formance, even better than the NNJM with TPD
on the Chinese-to-English and French-to-English
translation tasks.
From Table 2, the NNJM does not improve
translation performance significantly on the FE
task. Note that the baseline BLEU for the FE
7The decoding time for the NNJM and the BNNJM were
similar, since the NNJM trained by NCE uses p (ti|C) as
an approximation of P (ti|C) without normalization and the
BNNJM only needs to be normalized over two output neu-
rons.
</bodyText>
<equation confidence="0.871511">
S: A(this) U¨(movement) n1(continued) 0(until)
Ät$.(parasite) FH(by) M(two) ^ t(tongues) &apos;r%, 21
jAk(each other) ¥æ(contact) ö(where) r7 1(point)
¥æ(touched)
R: this movement is continued until the parasite is
</equation>
<footnote confidence="0.305490833333333">
touched by the point where the two tongues 21 contact
each other.
T1: the mobile continues to the parasite from the two
tongue 21 contacts the points of contact with each other.
T2: this movement is continued until the parasite by two
tongue 21 contact points of contact with each other.
</footnote>
<tableCaption confidence="0.8008905">
Table 3: Translation examples. Here, S: source;
R: reference; T1 uses NNJM; T2 uses BNNJM.
</tableCaption>
<table confidence="0.99877675">
NNJM BNNJM
A− &gt;the 1.681 -0.126
U¨− &gt;mobile -4.506 -3.758
411− &gt;continues -1.550 -0.130
0− &gt;to 2.510 -0.220
SUM -1.865 -4.236
A− &gt;this -2.414 -0.649
U¨− &gt;movement -1.527 -0.200
null− &gt;is 0.006 -0.055
411− &gt;continued -0.292 -0.249
0− &gt;until -6.846 -0.186
SUM -11.075 -1.341
</table>
<tableCaption confidence="0.994302">
Table 4: Scores for different translations.
</tableCaption>
<bodyText confidence="0.999927407407407">
task is lower than CE and JE tasks, indicating that
learning is harder for the FE task than CE and JE
tasks. The validation perplexities of the NNJM
with UPD for CE, JE and FE tasks are 4.03, 3.49
and 8.37. Despite these difficult learning circum-
stances and lack of large gains for the NNJM, the
BNNJM improves translations significantly for the
FE task, suggesting that the BNNJM is more ro-
bust to difficult translation tasks that are hard for
the NNJM.
Table 3 gives Chinese-to-English translation ex-
amples to demonstrate how the BNNJM (with
TPD) helps to improve translations over the
NNJM (with TPD). In this case, the BNNJM helps
to translate the phrase “A U¨ W1 0” bet-
ter. Table 4 gives translation scores for these two
translations calculated by the NNJM and the BN-
NJM. Context words are used for predictions but
not shown in the table.
As can be seen, the BNNJM prefers T2 while
the NNJM prefers T1. Among these predictions,
the NNJM and the BNNJM predict the translation
for “0” most differently. The NNJM clearly pre-
dicts that in this case “0” should be translated into
“to” more than “until”, likely because this exam-
ple rarely occurs in the training corpus. However,
the BNNJM prefers “until” more than “to”, which
</bodyText>
<page confidence="0.986882">
2097
</page>
<bodyText confidence="0.990514">
demonstrates the BNNJM’s robustness to less fre-
quent examples.
</bodyText>
<subsectionHeader confidence="0.973241">
5.3 Analysis for JE Translation Results
</subsectionHeader>
<bodyText confidence="0.999699833333333">
Finally, we examine the translation results to ex-
plore why the BNNJM with TPD did not outper-
form the NNJM with TPD for the JE translation
task, as it did for the other translation tasks. We
found that using the BNNJM instead of the NNJM
on the JE task did improve translation quality sig-
nificantly for infrequent words, but not for fre-
quent words.
First, we describe how we estimate translation
quality for infrequent words. Suppose we have a
test set S, a reference set R and a translation set T
with I sentences,
</bodyText>
<equation confidence="0.9521678">
Si (1 &lt; i &lt; I) , Ri (1 &lt; i &lt; I) , Ti (1 &lt; i &lt; I)
Ti contains J individual words,
Wij E Words (Ti)
To (Wij) is how many times Wij occurs in Ti and
Ro (Wij) is how many times Wij occurs in Ri.
</equation>
<bodyText confidence="0.9160655">
The general 1-gram translation accuracy (Pap-
ineni et al., 2002) is calculated as,
This general 1-gram translation accuracy does not
distinguish word frequency.
We use a modified 1-gram translation accuracy
that weights infrequent words more heavily,
</bodyText>
<equation confidence="0.9959892">
1
min(To(Wij),Ro(Wij))·
Occur(Wij)
I J To(Wij)
i=1 j=1
</equation>
<bodyText confidence="0.998249875">
where Occur (Wij) is how many times Wij oc-
curs in the whole reference set. Note Pc will not
be 1 even in the case of completely accurate trans-
lations, but it can approximately reflect infrequent
word translation accuracy, since correct frequent
word translations contribute less to Pc.
Table 5 shows Pg and Pc for different transla-
tion tasks. It can be seen that the BNNJM im-
proves infrequent word translation quality simi-
larly for all translation tasks, but improves gen-
eral translation quality less for the JE task than the
other translation tasks. We conjecture that the rea-
son why the BNNJM is less useful for frequent
word translations on the JE task is the fact that
the JE parallel corpus has less accurate function
word alignments than other language pairs, as the
</bodyText>
<table confidence="0.9986934">
CE JE FE
Pg Pc Pg Pc Pg Pc
NNJM 70.3 5.79 68.2 4.15 61.2 6.70
BNNJM 70.9 5.97 68.4 4.30 61.7 6.86
Imp. (%) 0.85 3.1 0.29 3.6 0.81 2.4
</table>
<tableCaption confidence="0.999335">
Table 5: 1-gram precisions and improvements.
</tableCaption>
<bodyText confidence="0.999814">
grammatical features of Japanese and English are
quite different.8 Wrong function word alignments
will make noise sampling less effective and there-
fore lower the BNNJM performance for function
word translations. Although wrong word align-
ments will also make noise sampling less effec-
tive for the NNJM, the BNNJM only uses one
noise sample for each positive example, so wrong
word alignments affect the BNNJM more than the
NNJM.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999275625">
Xu et al. (2011) proposed a method to use binary
classifiers to learn NNLMs. But they also used
the current target word in the output, similarly to
NCE. The BNNJM uses the current target word as
input, so the information about the current target
word can be combined with the context word in-
formation and processed in hidden layers.
Mauser et al. (2009) presented discriminative
lexicon models to predict target words. They
train a separate classifier for each target word, as
these lexicon models use discrete representations
of words and different classifiers do not share fea-
tures. In contrast, the BNNJM uses real-valued
vector representations of words and shares fea-
tures, so we train one classifier and can use the
similarity information between words.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999886727272727">
This paper proposes an alternative to the NNJM,
the BNNJM, which learns a binary classifier that
takes both the context and target words as input
and combines all useful information in the hidden
layers. We also present a novel noise distribution
based on translation probabilities to train the BN-
NJM efficiently. With the improved noise sam-
pling method, the BNNJM can achieve compara-
ble performance with the NNJM and even improve
the translation results over the NNJM on Chinese-
to-English and French-to-English translations.
</bodyText>
<footnote confidence="0.945787">
8Infrequent words are usually content words and frequent
words are usually function words.
</footnote>
<figure confidence="0.995971">
min(To(Wij),Ro(Wij))
To(Wij)
Pg =
I
i=1
J
j=1
I
i=1
J
j=1
Pc =
I
i=1
J
j=1
</figure>
<page confidence="0.963614">
2098
</page>
<sectionHeader confidence="0.973294" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999309634408602">
Jacob Andreas and Dan Klein. 2015. When and why
are log-linear models self-normalizing? In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
244–249.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1370–1380.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of The 9th NII Test Collection for IR
Systems Workshop Meeting, pages 559–578.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recurrent
neural networks. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 20–29.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
210–218.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of International Conference on
Computational Linguistics : Posters, pages 1071–
1080.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 14–25.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392.
Puyang Xu, Asela Gunawardana, and Sanjeev Khudan-
pur. 2011. Efficient subsampling for training com-
plex language models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1128–1136.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165.
</reference>
<page confidence="0.997785">
2099
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.184423">
<title confidence="0.7006325">A Binarized Neural Network Joint Model for Machine Translation Masao Eiichro</title>
<author confidence="0.688586">Satoshi</author>
<affiliation confidence="0.967239">Institute of Information and Communications</affiliation>
<address confidence="0.527716">3-5Hikaridai, Keihanna Science City, Kyoto 619-0289,</address>
<affiliation confidence="0.998826">School of Information Science, Nara Institute of Science and</affiliation>
<address confidence="0.973646">Takayama, Ikoma, Nara 630-0192,</address>
<email confidence="0.975411">neubig/s-nakamura@is.naist.jp</email>
<abstract confidence="0.999810157894737">The neural network joint model (NNJM), which augments the neural network lanmodel (NNLM) with an source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>When and why are log-linear models self-normalizing?</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>244--249</pages>
<contexts>
<context position="5904" citStr="Andreas and Klein (2015)" startWordPosition="956" endWordPosition="959">istinguish training data from noise by maximize the conditional likelihood, k L = log P (v = 1IC,ti)+ P log P (v = 0|C, tik). j=1 The normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2 1If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source context words sai+(m−1)/2 ai−(m−1)/2 and target history words i−1 ti−n+1 , P( v |sai+(m−1)/2 i−1 ) . ai−(m−1)/2, ti−n+1, ti The BNNJM is learned by a feedforward neural network with m + n inputs n o sai+(m−1)/2 ai−(m−1)/2, ti−1 i−n+1, ti and two</context>
</contexts>
<marker>Andreas, Klein, 2015</marker>
<rawString>Jacob Andreas and Dan Klein. 2015. When and why are log-linear models self-normalizing? In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 244–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="1458" citStr="Auli et al., 2013" startWordPosition="202" endWordPosition="205">kelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost dur</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2015</date>
<booktitle>In International Conference on Learning Representations.</booktitle>
<contexts>
<context position="1521" citStr="Bahdanau et al., 2015" startWordPosition="212" endWordPosition="215"> this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also Figure 1: (a) the traditional </context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2015</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="1413" citStr="Devlin et al., 2014" startWordPosition="194" endWordPosition="197">stimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and</context>
<context position="3988" citStr="Devlin et al., 2014" startWordPosition="615" endWordPosition="618">JM can be generated in the same way as NCE generates noise m-word source context ti-n+1~ti-1 P(ti=N) P(ti=1) P(ti=2) P(ti is correct) P(ti is wrong) m-word source context ti-n+1~ti 2094 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2094–2099, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. for the NNJM. We present a novel noise distribution based on translation probabilities to train the NNJM and the BNNJM efficiently. 2 Neural Network Joint Model |T Let T = t1 |be a translation of S = s|S| 1 . The NNJM (Devlin et al., 2014) defines the following probability, _ T|ai+(m−1)/2 i−1 P (T|S) P(ti|sai−(m−1)/2, ti−n+1 11=1) (1) — where target word ti is affiliated with source word sai. Affiliation ai is derived from the word alignments using heuristics1. To estimate these probabilities, the NNJM uses m source context words and n − 1 target history words as input to a neural network and performs estimation of unnormalized probabilities p (ti|C) before normalizing over all words in the target vocabulary V , P (ti|C) = p(ti|C) Z(C) Z (C) = P (2) p (ti0|C) ti0∈V where C stands for source and target context words as in Equati</context>
<context position="5849" citStr="Devlin et al. (2014)" startWordPosition="948" endWordPosition="951">0,ti|C) = k 1+k · q (ti). NCE trains the model to distinguish training data from noise by maximize the conditional likelihood, k L = log P (v = 1IC,ti)+ P log P (v = 0|C, tik). j=1 The normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2 1If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source context words sai+(m−1)/2 ai−(m−1)/2 and target history words i−1 ti−n+1 , P( v |sai+(m−1)/2 i−1 ) . ai−(m−1)/2, ti−n+1, ti The BNNJM is learned by a feedforward neural network with m + n in</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="10310" citStr="Goto et al., 2011" startWordPosition="1724" endWordPosition="1727">ere align (sai, ti0) is how many times ti0 is aligned to sai in the parallel corpus. Note that ti could be unaligned, in which case we assume that it is aligned to a special null word. Noise for unaligned words is sampled according to the TPD of the null word. If several target/source words are aligned to one source/target word, we 5 Experiments 5.1 Setting We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) w</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="1392" citStr="Hu et al., 2014" startWordPosition="190" endWordPosition="193">ise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to </context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="10908" citStr="Koehn et al., 2007" startWordPosition="1834" endWordPosition="1837">9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="12789" citStr="Koehn, 2004" startWordPosition="2167" endWordPosition="2168"> The training process was stopped when validation likelihood stopped increasing. 2096 CE T JE T FE T E E E NNJM UPD 20 22 19 49 20 28 TPD 4 6 4 BNNJM UPD 14 16 12 34 11 22 TPD 11 9 9 Table 1: Epochs (E) and time (T) in minutes per epoch for each task. CE JE FE Base UPD 32.95 30.13 24.56 NNJM TPD 34.36+ 31.30+ 24.68 BNNJM UPD 34.60+ 31.50+ 24.80 TPD 32.89 30.04 24.50 35.05+* 31.42+ 25.84+* Table 2: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling (Koehn, 2004). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. 5.2 Results and Discussion Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2. We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--218</pages>
<contexts>
<context position="19213" citStr="Mauser et al. (2009)" startWordPosition="3275" endWordPosition="3278">nce for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses real-valued vector representations of words and shares features, so we train one classifier and can use the similarity information between words. 7 Conclusion This paper proposes an alternative to the NNJM, the BNNJM, which learns a binary classifier that takes both the context and target words as input and combines all useful information</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11200" citStr="Och, 2003" startWordPosition="1885" endWordPosition="1886">Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate 3The processing for multiple alignments helps sample more useful negative examples for TPD, and had little effect on the translation </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="17223" citStr="Papineni et al., 2002" startWordPosition="2935" endWordPosition="2939">it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words. First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 &lt; i &lt; I) , Ri (1 &lt; i &lt; I) , Ti (1 &lt; i &lt; I) Ti contains J individual words, Wij E Words (Ti) To (Wij) is how many times Wij occurs in Ti and Ro (Wij) is how many times Wij occurs in Ri. The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as, This general 1-gram translation accuracy does not distinguish word frequency. We use a modified 1-gram translation accuracy that weights infrequent words more heavily, 1 min(To(Wij),Ro(Wij))· Occur(Wij) I J To(Wij) i=1 j=1 where Occur (Wij) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to Pc. Table 5 shows Pg and Pc for different translation tasks. It can be seen </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics : Posters,</booktitle>
<pages>1071--1080</pages>
<contexts>
<context position="1473" citStr="Schwenk, 2012" startWordPosition="206" endWordPosition="207"> (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. H</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of International Conference on Computational Linguistics : Posters, pages 1071– 1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>14--25</pages>
<contexts>
<context position="1439" citStr="Sundermeyer et al., 2014" startWordPosition="198" endWordPosition="201">ead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive nor</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="1497" citStr="Sutskever et al., 2014" startWordPosition="208" endWordPosition="211">uce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also Figure</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<contexts>
<context position="2456" citStr="Vaswani et al. (2013)" startWordPosition="363" endWordPosition="366">e, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE). To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise. This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word </context>
<context position="4974" citStr="Vaswani et al., 2013" startWordPosition="781" endWordPosition="784">orms estimation of unnormalized probabilities p (ti|C) before normalizing over all words in the target vocabulary V , P (ti|C) = p(ti|C) Z(C) Z (C) = P (2) p (ti0|C) ti0∈V where C stands for source and target context words as in Equation 1. The NNJM can be trained on a word-aligned parallel corpus using standard MLE, but the cost of normalizing over the entire vocabulary to calculate the denominator in Equation 2 is quite large. Devlin et al. (2014)’s self-normalization technique can avoid normalization cost during decoding, but not during training. NCE can be used to train NNLM-style models (Vaswani et al., 2013) to reduce training times. NCE creates a noise distribution q (ti), selects k noise samples ti1, ..., tik for each ti and introduces a random variable v which is 1 for training examples and 0 for noise samples, P (v = 1, ti|C) = 1 1+k ·p(ti|C)Z(C) P (v = 0,ti|C) = k 1+k · q (ti). NCE trains the model to distinguish training data from noise by maximize the conditional likelihood, k L = log P (v = 1IC,ti)+ P log P (v = 0|C, tik). j=1 The normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2 1If ti aligns to exactly one source word, ai is the index of this source w</context>
<context position="7894" citStr="Vaswani et al. (2013)" startWordPosition="1311" endWordPosition="1314">ifier, the gradient for a single example in the BNNJM can be calculated efficiently by MLE without it being necessary to calculate the softmax over the full vocabulary. On the other hand, we need to create “positive” and “negative” examples for the classifier. Positive examples can be extracted directly from the word-aligned parallel corpus as Dai+(m−1)/2i−1 sai−(m−1)/2, ti−n+1 , tiE; Negative examples can be generated for each positive example in the same way that NCE generates noise data as D sai+(m−1)/2 ai−(m−1)/2, ti−1 i−n+1, ti0E , where ti0 ∈ V \ {ti}. 4 Noise Sampling 4.1 Unigram Noise Vaswani et al. (2013) adopted the unigram probability distribution (UPD) to sample noise for train2095 a � PC* A Vr tip, t0� choose to combine these target/source words as a new target/source word.3 I will arrange for someone to take you round Figure 2: A parallel sentence pair. ing NNLMs with NCE, q (ti0) = cc Eocc(ur(ti00) ti00∈V where occur (ti0) stands for how many times ti0 occurs in the training corpus. 4.2 Translation Model Noise In this paper, we propose a noise distribution specialized for translation models, such as the NNJM or BNNJM. Figure 2 gives a Chinese-to-English parallel sentence pair with word a</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Asela Gunawardana</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient subsampling for training complex language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1128--1136</pages>
<contexts>
<context position="18876" citStr="Xu et al. (2011)" startWordPosition="3216" endWordPosition="3219">g Pc NNJM 70.3 5.79 68.2 4.15 61.2 6.70 BNNJM 70.9 5.97 68.4 4.30 61.7 6.86 Imp. (%) 0.85 3.1 0.29 3.6 0.81 2.4 Table 5: 1-gram precisions and improvements. grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses rea</context>
</contexts>
<marker>Xu, Gunawardana, Khudanpur, 2011</marker>
<rawString>Puyang Xu, Asela Gunawardana, and Sanjeev Khudanpur. 2011. Efficient subsampling for training complex language models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>162--165</pages>
<contexts>
<context position="10608" citStr="Zhao et al., 2006" startWordPosition="1780" endWordPosition="1783">igned to one source/target word, we 5 Experiments 5.1 Setting We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The wo</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.9988365">
Hierarchical Back-off Modeling of Hiero Grammar based on
Non-parametric Bayesian Model
</title>
<author confidence="0.962025">
Hidetaka Kamigaito&apos; Taro Watanabe2 Hiroya Takamura&apos;
</author>
<email confidence="0.913556">
kamigaito@lr.pi.titech.ac.jp tarow@google.com takamura@pi.titech.ac.jp
</email>
<author confidence="0.802354">
Manabu Okumura&apos; Eiichiro Sumita3
</author>
<email confidence="0.799654">
oku@pi.titech.ac.jp eiichiro.sumita@nict.go.jp
</email>
<affiliation confidence="0.9950635">
&apos;Tokyo Institute of Technology 2Google Japan Inc.
3National Institute of Information and Communication Technology
</affiliation>
<sectionHeader confidence="0.978761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970848484848">
In hierarchical phrase-based machine
translation, a rule table is automatically
learned by heuristically extracting syn-
chronous rules from a parallel corpus.
As a result, spuriously many rules are
extracted which may be composed of
various incorrect rules. The larger rule
table incurs more run time for decoding
and may result in lower translation quality.
To resolve the problems, we propose a
hierarchical back-off model for Hiero
grammar, an instance of a synchronous
context free grammar (SCFG), on the
basis of the hierarchical Pitman-Yor
process. The model can extract a compact
rule and phrase table without resorting to
any heuristics by hierarchically backing
off to smaller phrases under SCFG.
Inference is efficiently carried out using
two-step synchronous parsing of Xiao et
al., (2012) combined with slice sampling.
In our experiments, the proposed model
achieved higher or at least comparable
translation quality against a previous
Bayesian model on various language
pairs; German/French/Spanish/Japanese-
English. When compared against heuristic
models, our model achieved comparable
translation quality on a full size German-
English language pair in Europarl v7
corpus with significantly smaller grammar
size; less than 10% of that for heuristic
model.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968888888889">
Hierarchical phrase-based statistical machine
translation (HPBSMT) (Chiang, 2007) is a popu-
lar alternative to phrase-based SMT (PBSMT), in
which synchronous context free grammar (SCFG)
is used as the basis of the machine translation
model. With HPBSMT, a restricted form of an
SCFG, i.e., Hiero grammar, is usually used and is
especially suited for linguistically divergent lan-
guage pairs, such as Japanese and English. How-
ever, a rule table, i.e., a synchronous grammar,
may be composed of spuriously many rules with
potential errors especially when it was automati-
cally acquired from a parallel corpus. As a result,
the increase in the rule table incurs a large amount
of time for decoding and may result in lower trans-
lation quality.
Pruning a rule table either on the basis of signif-
icance test (Johnson et al., 2007) or entropy (Ling
et al., 2012; Zens et al., 2012) used in PBSMT can
be easily applied for HPBSMT. However, these
methods still rely on a heuristically determined
threshold parameter. Bayesian SCFG methods
(Blunsom et al., 2009) solve the spurious rule
extraction problem by directly inducing a com-
pact rule table from a parallel corpus on the basis
of a non-parametric Bayesian model without any
heuristics. Training for Bayesian SCFG models
infers a derivation tree for each training instance,
which demands the time complexity of O(|f|3|e|3)
when we use dynamic programming SCFG bi-
parsing (Wu, 1997). Gibbs sampling without bi-
parsing (Levenberg et al., 2012) can avoid this
problem, though the induced derivation trees may
strongly depend on initial derivation trees. Even
though we may learn a statistically sound model
on the basis of non-parametric Bayesian methods,
current approaches for an SCFG still rely on ex-
haustive heuristic rule extraction from the word-
alignment decided by derivation trees since the
learned models cannot handle rules and phrases of
various granularities.
We propose a model on the basis of the previ-
ous work on the non-parametric Inversion Trans-
duction Grammar (ITG) model (Neubig et al.,
2011) wherein phrases of various granularities are
</bodyText>
<page confidence="0.938316">
1217
</page>
<note confidence="0.9848295">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999732">
learned in a hierarchical back-off process. We
extend it by incorporating arbitrary Hiero rules
when backing off to smaller spans. For efficient
inference, we use a fast two-step bi-parsing ap-
proach (Xiao et al., 2012) which basically runs in a
time complexity of O(|f|3). Slice sampling for an
SCFG (Blunsom and Cohn, 2010) is used for effi-
ciently sampling a derivation tree from a reduced
space of possible derivations.
Our model achieved higher or at least com-
parable BLEU scores against the previous
Bayesian SCFG model on language pairs;
German/French/Spanish-English in the News-
Commentary corpus, and Japanese-English in
the NTCIR10 corpus. When compared against
heuristically extracted model through the GIZA++
pipeline, our model achieved comparable score on
a full size Germany-English language pair in Eu-
roparl v7 corpus with significantly less grammar
size.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961397260274">
Various criteria have been proposed to prune a
phrase table without decreasing translation qual-
ity, e.g., Fisher’s exact test (Johnson et al., 2007)
or relative entropy (Ling et al., 2012; Zens et al.,
2012). Although those methods are easily ap-
plied for pruning a rule table, they heavily rely
on the heuristically determined threshold parame-
ter to trade off the translation quality and decoding
speed of an MT system.
Previously, EM-algorithm based generative
models were exploited for generating compact
phrase and rule tables. Joint phrase alignment
model (Marcu and Wong, 2002) can directly
express many-to-many word aligments without
heuristic phrase extraction. DeNero et al. (2006)
proposed IBM Model 3 based many-to-many
alignment model. Rule arithmetic method (Cme-
jrek and Zhou, 2010) can generate SCFG rules
by combining other rule pairs through an inside-
outside algorithm. However, those previous at-
tempts were restricted in that the rules and phrases
were induced by heuristic combination.
Bayesian SCFG models can induce a com-
pact model by incorporating sophisticated non-
parametric Bayesian models for an SCFG, such as
a dirichlet process (DeNero et al., 2008; Blunsom
et al., 2009; Chung et al., 2014) or Pitman-Yor
process (Levenberg et al., 2012; Peng and Gildea,
2014). A model is learned by sampling derivation
trees in a parallel corpus and by accumulating the
rules in the sampled trees into the model. Due to
the O(|f|3|e|3) time complexity for bi-parsing a
bilingual sentence, previous studies relied on bi-
parsing at the initialization step, and conducted
Gibbs sampling by local operators (Blunsom et al.,
2009; Levenberg et al., 2012) or sampling on fixed
word alignments (Chung et al., 2014; Peng and
Gildea, 2014). As a result, the inference can easily
result in local optimum, wherein induced deriva-
tion trees may strongly depend on the initial trees.
Xiao et al. (2012) proposed a two-step approach
for bi-parsing a bilingual sentence in O(|f|3) in the
context of inducing SCFG rules discriminatively;
however, their approach violates the detailed bal-
ance due to its heuristic k-best pruning. Blun-
som and Cohn (2010) proposed a slice sampling
for an SCFG, in the same manner as that for Infi-
nite Hiden Markov Model (iHMM) (Van Gael et
al., 2008), which can efficiently prune a space of
possible derivations on the basis of dynamic pro-
gramming. Although slice sampling can prune
spans without violating the detailed balance, its
time complexity of O(|f|3|e|3) is still impractical
for a large-scale experiment. We efficiently car-
ried out large-scale experiments on the basis of the
two-step bi-parsing of Xiao et al. combined with
slice sampling of Blunsom and Cohn.
After learning a Bayesian model, it is not di-
rectly used in a decoder since it is composed of
only minimum rules without considering phrases
of various granularities. As a consequence, it is
a standard practice to obtain word alignment from
derivation trees and to extract SCFG rules heuris-
tically from the word-aligned data (Cohn and Haf-
fari, 2013). The work by Neubig et al. (2011) was
the first attempt to directly use the learned model
on the basis of a Bayesian ITG in which phrases
of many granularities were encoded in the model
by employing a hierarchical back-off procedure.
Our work is strongly motivated by their work, but
greatly differs in that our model can incorporate
many arbitrary Hiero rules, not limited to ITG-
style binary branching rules.
</bodyText>
<sectionHeader confidence="0.994891" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9996116">
We use Hiero grammar (Chiang, 2007), an in-
stance of an SCFG, which is defined as a context-
free grammar for two languages. Let E denote a
set of terminal symbols in the source language, ∆
a set of terminal symbols in the target language,
</bodyText>
<page confidence="0.879856">
1218
</page>
<figure confidence="0.501335">
erative process is represented as follows:
GX ∼ Prule(dr, θr, Gr0),
X → ⟨α/β⟩ ∼ GX, (1)
</figure>
<figureCaption confidence="0.976045">
Figure 1: Derivation tree generated from Bayesian
SCFG model
</figureCaption>
<bodyText confidence="0.996306363636364">
V a set of non-terminal symbols, S a start symbol
and R a set of rewrite rules. An SCFG is denoted
as a tuple of ⟨E, ∆, V, S, R⟩. Each rewrite rule
in R is represented as X → ⟨α/β⟩ in which α
is a string of non-terminals and source side termi-
nals (V ∪ E)* and β is a string of non-terminals
and target side terminals (V ∪ ∆)*. An example
derivation in an SCFG for the sentence pair “ni-
hongo wo eigo ni honyaku suru koto wa muzukasii
。 /Japanese is difficult to translate into English .”
is represented as follows:
</bodyText>
<equation confidence="0.991705">
S → X1 eigo X2 muzukasii 。 / X1 difficult X2
English.
X1 → X3 wo / X3 is
X2 → X4 honyaku suru X5 wa / X4 translate X5
X3 → nihongo / Japanese
X4 → ni / into
X5 → koto / to .
</equation>
<bodyText confidence="0.999968285714286">
A Hiero grammar has additional constraints
over a general SCFG; the number of terminal sym-
bols in each rule for both source and target sides
is limited to 5. Each rule may contain at most
two non-terminal symbols; adjacent non-terminal
symbols in the source side are prohibited. For de-
tails, refer to (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.99886">
3.1 Bayesian SCFG Models
</subsectionHeader>
<bodyText confidence="0.962127238095238">
Previous Bayesian SCFG Models, for instance a
model proposed by Levenberg et al. (2012), are
based on the Pitman-Yor process (Pitman and Yor,
1997) and learn SCFG rules by sampling a deriva-
tion tree for each bilingual sentence. Figure 1
shows an example derivation tree for our running
example sentence pair under the model. The gen-
where GX is a derivation tree and
Prule(dr, θr, Gro) is a Pitman-Yor process
(Pitman and Yor, 1997), which is a generalization
of a Dirichlet process parametrized by a discount
parameter dr, a strength parameter θr and a base
measure Gro. The output probability of a Pitman-
Yor process obeys the power-law distribution with
the discount parameter, which is very common in
standard NLP tasks.
The probability that a rule rk is drawn from a
model Prule(dr, θr, Gro) is determined by a Chi-
nese restaurant process which is decomposed into
two probability distributions. If rk already exists
in a table, we draw rk with probability
</bodyText>
<equation confidence="0.997726">
ck − dr · |φrk |,(2)
θr + nr
</equation>
<bodyText confidence="0.99993025">
where ck is the number of customers of rk, nr is
the number of all customers and φrk is a number
of rk’s tables. On the other hand, if rk is a new
rule, we draw rk with probability
</bodyText>
<equation confidence="0.99914">
θr + dr · |φr|
θr + nr
</equation>
<bodyText confidence="0.966966">
where |φr |is the number of tables in the model.
</bodyText>
<subsectionHeader confidence="0.998461">
3.2 Hierarchical Back-off Model
</subsectionHeader>
<bodyText confidence="0.99999625">
In the previous models, the generative process is
represented as a rewrite process starting from the
symbol S, which can incorporate only minimal
rules. Following Neubig et al. (2011), our model
reverses the process by recursively backing off to
smaller phrase pairs as shown in Figure 2. First,
our model attempts to generate a phrase pair, i.e.,
a sentence pair, as a derivation tree. If the model
successfully generates the phrase pair, we will fin-
ish the generation process. Otherwise, a Hiero
rule is generated to fallback to smaller spans rep-
resented in each non-terminal symbol X in the
rule. Then, each phrase pair corresponding to each
smaller span is recursively generated through our
model. In Figure 2, a phrase pair with “nil” indi-
cates those not in our model; therefore the phrase
pair is forced to back-off either by generating a
new phrase pair from a base measure (base) or by
falling back to smaller phrases using a Hiero rule
(back-off). The recursive procedure is done until
</bodyText>
<equation confidence="0.955881">
· Gr0, (3)
</equation>
<page confidence="0.935282">
1219
</page>
<bodyText confidence="0.999644857142857">
where cback and cbase are the number of customers
sampled from the back-off and base phrases, re-
spectively, with a base measure Gb and hyper-
parameter γb. We use a uniform distribution for
Gb = 0.5 since we consider only two states, back-
off and base. Unlike the model state, Pphrase
may reach this state even when a phrase pair is
not in the model. The phrase pair is backed-off
to smaller phrase pairs using Pphrase through the
non-terminals in the generated rule X ∈ ⟨α/β⟩.
base: As an alternative to the back-off state, we
may reach the base state which follows the proba-
bility distribution on the basis of the base measure
Gp0,
</bodyText>
<figureCaption confidence="0.999323">
Figure 2: Derivation tree generated from the hier- θp + dp ·|φp |cbase + γb · Gb
archical back-off model θp + np cback + cbase + γb Gp0. ( 7)
</figureCaption>
<bodyText confidence="0.9997164">
we reach phrase pairs which are generated without
any back-offs. Let a discount parameter be dp, a
strength parameter be θp, and a base measure be
Gp0. More formally, the generative process is rep-
resented as follows:
</bodyText>
<equation confidence="0.99535025">
GX ∼ Prule(dr, θr, Gphrase),
Gphrase ∼ Pphrase(dp, θp, GX),
X → ⟨s/t⟩ ∼ Gphrase,
X → ⟨α/β⟩ ∼ GX, (4)
</equation>
<bodyText confidence="0.9997412">
where s is source side terminals and t is target side
terminals in phrase pair ⟨s/t⟩. Pphrase is com-
posed of three states, i.e., model, back-off, and
base, and follows a hierarchical Pitman-Yor pro-
cess (Teh, 2006).
</bodyText>
<equation confidence="0.564256">
θp + np
</equation>
<bodyText confidence="0.999746714285714">
where ck is the numbers of customers of a phrase
pair pk and np is the number of all customers Note
that this state is reachable when the phrase pair
⟨s/t⟩ exists in the model in the same manner as
Equation (2).
back-off: We will back off to smaller phrases
using a rule generated by Prule as follows:
</bodyText>
<equation confidence="0.975127">
cback + γb · Gb
cback + cbase + γb
·Prule(dr, θr, Gphrase)
</equation>
<bodyText confidence="0.9980845">
In summary, Pphrase(dp, θp, GX) is defined as a
joint probability of Equations (5) through (7).
</bodyText>
<subsectionHeader confidence="0.999816">
3.3 Base Measure
</subsectionHeader>
<bodyText confidence="0.999982545454545">
Similar to Levenberg et al. (2012), the base mea-
sure for rule probability Gr0 is composed of
four generative processes. First, a number of
symbols in a source side of a rule |α |is gen-
erated from a Poisson distribution, i.e., |α |∼
Poisson(0.1). Let t(x) denote a function that re-
turns terminals from a string x. The number of
target side terminal symbols |t(β) |is also gener-
ated from a Poisson distribution and represented
as |t(β) |∼ Poisson(α + λ0)1. The type of
symbol αi in the source side, typei, either ter-
minal or non-terminal symbol, is determined by
typei ∼ Bernoulli(ϕ|α|) where ϕ is a hyper-
parameter taking 0 &lt; ϕ &lt; 1. ϕ|α |is based
on an intuition that shorter rules should be rela-
tively more likely to contain terminal symbols than
longer rules. Source and target terminal symbol
pair ⟨t(α), t(β)⟩ are generated from the geomet-
ric means of two directional IBM Model 1 word
alignment probabilities and monolingual unigram
probabilities for two languages, and represented
as:
</bodyText>
<equation confidence="0.5597055">
⟨t(α), t(β)⟩ ∼ (Puni(t(α))P−−→�1(t(α), t(β)) ·
Puni(t(β))P←−− �1(t(α),t(β)))� 2. (8)
</equation>
<bodyText confidence="0.99830325">
When the t(α) or t(β) is empty, we use the con-
stant 0.01 instead of the Model1 probabilities.
model: We draw a phrase pair ⟨s/t⟩ with the
probability similar to Equation (2):
</bodyText>
<equation confidence="0.919416333333333">
ck − dp ·|φpk|
, (5)
θp + dp · |φp|
θp + np
11 · Pphrase(dp,θp, GX), (6) 1Note that a0 is a small constant for the input distribution
XE(α//3) greater than zero.
</equation>
<page confidence="0.745419">
1220
</page>
<bodyText confidence="0.9999521">
The base measure for phrases Gp� is composed
of three generative processes, in a similar man-
ner as Levenberg et al. (2012), the number of
terminal symbols in a phrase pair in the source
side, |s|, is generated from a Poisson distribution
|s |∼ Poisson(0.1). The length for the target side
|t |is generated in the same manner as the source
side of the phrase pair. The alignments between
s and t are also generated in the same manner as
those for the base measure in a rule.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.99995">
In inference, we use a sentence-wise block sam-
pling of Blunsom and Cohn (2010), which has a
better convergence property when compared with
a step-wise Gibbs sampling. We repeat the follow-
ing steps given a sentence pair.
</bodyText>
<listItem confidence="0.975974444444444">
1. Decrement customers of the rules and phrase
pairs used in the current derivation for the
sentence pair.
2. Bi-parse the sentence pair in a bottom up
manner.
3. Sample a new derivation tree in a top-down
manner.
4. Increment customers of the rules and phrase
pairs in the sampled derivation tree.
</listItem>
<bodyText confidence="0.999966086956522">
The most time-consuming step during the infer-
ence procedure is bi-parsing of a sentence pair
which essentially takes O(|f|3|a|3) time using a
bottom up dynamic programming algorithm (Wu,
1997). When a span is very large, it can easily suf-
fer combinatorial explosion. To avoid this prob-
lem, we use a two-step slice sampling by perform-
ing the two-step bi-parsing of Xiao et al. (2012)
and by pruning possible derivation space (Blun-
som and Cohn, 2010) in each step (Algorithm 1).
From lines 1 to 7, a set of word alignment is enu-
merated and put into cubea. In addition to the ar-
bitrary word alignment of sourcei to targetj, null
word alignment is also merged into cubea (line
5). Note that word alignment considered in the
algorithm is restricted to one-to-many. The set of
word alignments in cubea is pruned and added to
the charta by SliceSampling. From lines 8 to 15,
all possible phrases and rules for each span con-
strained by the pruned word alignment are enu-
merated and temporally stored into cube. The
phrases and rules in cube are pruned by SliceSam-
pling and the remainders are added to chart. The
</bodyText>
<listItem confidence="0.950897842105263">
Algorithm 1 Two-step slice sampling
1: for i ← 1, · · · , |source |do
2: for j ← 1, · · · , |target |do
3: cubea ← {soucei,targetj}
4: end for
5: cubea ← {soucei, null}
6: charta ← 5lice5ampling(cubea)
7: clear cubea
8: end for
9: for h ← 1, · · · , |source |do
10: for all the i, j s.t j − i = h do
11: for inferable rule, phrase from the sub-
spans of [i, j] of all charts do
12: cube ← rule, phrase
13: end for
14: chart ← 5lice5ampling(cube)
15: clear cube
16: end for
17: end for
</listItem>
<bodyText confidence="0.999691615384615">
time complexity for the word alignment enumera-
tion from lines 1 to 7 is O(|f||e|) and that for the
phrase and rule enumeration from lines 8 to 15 is
O(|f|3).
The key difference to the slice sampling of
Blunsom and Cohn (2010) lies in lines 6 and 3 of
Algorithm 1. Let d denote a set of derivation trees
d and u be a set of slice variables u. In slice sam-
pling, we prune the rules r,,p in each source span
sp based on a slice variable usp corresponding to
that sp. After pruning, we sample trees from the
pruned space of r. The above process is formally
represented as:
</bodyText>
<equation confidence="0.8951075">
u ∼ P(u|d),
d ∼ P(d|u), (9)
</equation>
<bodyText confidence="0.972970875">
where P(d|u) is computed through sampling in
a top-down manner after parsing in a bottom-
up manner with Algorithm 1, and is equal to
Hd P(d |u). The probability P(u|d) is equal to
Hsp P(usp|d). Let r*denote a currently adopted
sp
rule in the span sp and P(usp|d) be defined using
a pruning score 5core(r* ) as follows:
</bodyText>
<equation confidence="0.9197645">
sp
5core(rspi) = Inside(rspi) · Future(rspi), (10)
</equation>
<bodyText confidence="0.9988646">
where Inside(rsp) and Future(rsp) are inside
and outside probabilities for sp, respectively. Let
srsp denote a set of source side words in rsp, trsp
a set of target side words in rsp, ssp a set of words
in a source sentence without srsp and tsp, a set of
</bodyText>
<page confidence="0.932831">
1221
</page>
<bodyText confidence="0.903721666666667">
words in a target sentence without tr3p. By us-
ing IBM Model 1 probabilities in two directions,
Inside(rsp) is calculated by
</bodyText>
<equation confidence="0.848242">
(P−−→M1(ssp, tsp) &apos; P←−−M1(ssp, tsp))� 2. (11)
</equation>
<bodyText confidence="0.9940255">
We use IBM Model1 outside probability for future
score Future(rsp). Similarly, the future score
Future(rsp) is computed using the two direc-
tional models:
</bodyText>
<equation confidence="0.975343142857143">
(P−−→M1(ssp, tsp) &apos; P←−−M1(ssp.tsp))� 2. (12)
When sp is used in the current derivation d, slice
variable usp is sampled from a uniform distribu-
tion2:
otherwise, usp is sampled from a beta distribution
if sp is not in the current derivation d:
P(usp|d) = Beta(usp; a,1.0), (14)
</equation>
<bodyText confidence="0.999896227272727">
where a &lt; 1 is a parameter for the beta distribu-
tion. If the Score(rspz) is less than usp, we prune
the rspz from cube. Similar to Blunsom and Cohn
(2010), if the span sp is not in the current deriva-
tion, the rules with low probability are pruned ac-
cording to Equation (14). Let rdsp denotes a rule in
d with span sp, P(d|u) is calculated by:
In our experiments discussed in Section 6, slice
sampling parameter a was set to 0.02 when in-
corporating the future score of Equation (12). In
contrast, we used a = 0.1 when performing slice
sampling without the future score. We empirically
found that setting a lower value for a led to slower
progress in learning due to a combinatorial explo-
sion when inferencing a derivation for each sen-
tence pair.
In the beginning of training, we do not have
any derivation trees for given training data, al-
though the derivation trees are required for esti-
mating parameters for Bayesian models. We use
the two-step parsing for generating initial deriva-
tion trees from only base measures. The k-best
</bodyText>
<footnote confidence="0.948784">
2H(·) is a function returns 1 if the condition is satisfied and
0 otherwise
</footnote>
<bodyText confidence="0.99978395">
pruning is conducted against the score denoted by
the equation 10 , which is very similar to Xiao et
al. (2012).3
For faster bi-parsing, we run sampling in paral-
lel in the same way as Zhao and Huang (2013), in
which bi-parsing is performed in parallel among
the bilingual sentences in a mini-batch. The up-
dates to the model are synchronized by increment-
ing and decrementing customers for the bilingual
sentences in the mini-batch. Note that the bi-
parsing for each mini-batch is conducted on the
fixed model parameters after the synchronised pa-
rameter updates.
In addition to the model parameters, hyperpa-
rameters are re-sampled after each training itera-
tion following the discount and strength hyperpa-
rameter resampling in a hierarchical Pitman-Yor
process (Teh, 2006). In particular, we resample
(dp, Bp), the pair of discount and strength parame-
ters for phrases from a distribution:
</bodyText>
<equation confidence="0.997968">
[1 — dp]�c⟨s,t⟩−1� (16)
1
</equation>
<bodyText confidence="0.999881">
where [ ] denotes a generalized Pochhammer sym-
bol, and c(,,,t) the number of customers of phrase
pair (s, t). We resample the pair (dr, Br) in the
same way as (dp, Bp). The hyperparameter &apos;Yb is
resampled from distribution:
</bodyText>
<equation confidence="0.97484">
(cback + &apos;Yb &apos; Gb)(cbase + &apos;Yb &apos; Gb)
(cback + cbase + &apos;Yb)2
</equation>
<bodyText confidence="0.999423333333333">
where O, used in the generative process for ei-
ther terminal or non-terminal symbol typei —
Bernoulli(Oα), is resampled from a distribution:
</bodyText>
<equation confidence="0.9067825">
H Bernoulli(O|α|)c⟨«/a⟩, (18)
(α/β)EBase
</equation>
<bodyText confidence="0.9999705">
where c(α/β) denotes the number of customers of
rule (α/,(i), and Base denotes a set of rules gener-
ated from the base measure. All the hyperparame-
ters are inferred by slice sampling (Neal, 2003).
</bodyText>
<sectionHeader confidence="0.949319" genericHeader="method">
5 Extraction of Translation Model
</sectionHeader>
<bodyText confidence="0.9998986">
In the previous work on Bayesian approaches
(Blunsom and Cohn, 2010; Levenberg et al.,
2012), it is a standard practice to heuristically ex-
tract rules and phrase pairs from the word align-
ment derived from the derivation trees sampled
</bodyText>
<footnote confidence="0.640346">
3Note that we use k = 30 for k-best pruning.
</footnote>
<equation confidence="0.9987252">
P(usp|d) =
Score(r*sp) , (13)
ff(usp &lt; Score(r* sp))
(15)
ErjErsn P(rj)ff(usp &lt; Score(rj))
H
spEd
P(rdsp)
[Bp] II PP |
n H
[Bp] 1 P ($,t)
|φp|
H
k=1
, (17)
</equation>
<page confidence="0.939509">
1222
</page>
<bodyText confidence="0.999209269230769">
from the Bayesian models. Instead of the heuris-
tic method, we directly extract rules and phrase
pairs from the learned models which are repre-
sented as Chinese restaurant tables. To limit gram-
mar size, we include only phrase pairs that are se-
lected at least once in the sample. During this ex-
traction process, we limit the source or target ter-
minal symbol size of phrase pairs to 5.
For each extracted rule or phase pair, we com-
pute a set of feature scores used for a HPBSMT
decoder; a weighted combination of multiple fea-
tures is necessary in SMT since the model learned
from training data may not fit well to translate an
unseen test data (Och, 2003). We use the follow-
ing six features; the joint model probability Pmodel
is calculated by Equation (2) for rules and by
Equation (5) for phrase pairs. The joint posterior
probability Pposterior(f, e) is estimated from the
posterior probabilities for every rule and phrase
pair in derivation trees through relative count es-
timation, motivated by Neubig et al. (2011) 4.
The joint posterior probability is considered as
an approximation for those back-off scores. The
conditional model probabilities in two directions,
Pmodel(f|e) and Pmodel(e|f), are estimated by
marginalizing the joint probability Pmodel(f, e):
</bodyText>
<equation confidence="0.993661">
Pmodel(f|e) = Pmodel (f, e) (19)
</equation>
<bodyText confidence="0.98838695">
Ef′ Pmodel (f′ , e)
The inverse direction Pmodel(e|f) is estimated,
similarly. The lexical probabilities in two direc-
tions, Plex(f|e) and Plex(e|f), are scored by IBM
Model probabilities between the source and target
terminal symbols in rules and phrase pairs. In ad-
dition to the above features, we use Word penalty
for each rule and phrase pair used in the cdec de-
coder (Dyer et al., 2010).
As indicated in previous studies (Koehn et al.,
2003; DeNero et al., 2006), the translation quality
of generative models is lower than that of mod-
els with heuristically extracted rules and phrase
pairs. DeNero et al. (2006) reported that con-
sidering multiple phrase boundaries is important
for improving translation quality. The generative
models, in particular Bayesian models, are strict in
determining phrase boundaries since their models
are usually estimated from sampled derivations.
As a result, translation quality is poorer when
</bodyText>
<footnote confidence="0.670038">
4Note that the correct way to decode from our model is to
score every phrase pair created during decoding with back-off
states, which is computationally intractable
</footnote>
<bodyText confidence="0.999905428571429">
compared with a model estimated using a heuristic
method. The Hiero grammar severely suffers from
the phrase granularity problem and can overfit to
the training data due to the flexibility of the rules.
To alleviate this problem, Neubig et al. (2011)
combined the derivation trees across training it-
erations by averaging the features for each rule
and phrase pair. During the sampling process,
each training iteration draws a different deriva-
tion tree for each sentence pair, and the combi-
nation of those different derivation trees can pro-
vide multiple possible phrase boundaries to the
model. Inspired by the averaging over the mod-
els from different iterations, we combine them as a
part of a sampling process; we treat the derivation
trees acquired from different iterations as addi-
tional training data, and increment the correspond-
ing customers into our model. Hyperparameters
are resampled after the merging process. The new
features are directly computed from the merged
model.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.981603">
6.1 Comparison with Previous Bayesian
Model
</subsectionHeader>
<bodyText confidence="0.999602958333333">
First, we compared the previous Bayesian model
(Gen) with our hierarchical back-off model
(Back). We used the first 100K sentence
pairs of the WMT10 News-Commentary cor-
pus for German/Spanish/French-to-English pairs
(Callison-Burch et al., 2010) and NTCIR10 cor-
pus for Japanese-English (Goto et al., 2013) for
the translation model. All sentences are lower-
cased and filtered to preserve at most 40 words on
both source and target sides. We sampled 20 it-
erations for Gen and Back and combined the last
10 iterations for extracting the translation model.5
The batch size was set to 64. The language mod-
els were estimated from the all-English side of
the WMT News-Commentary and europarl-v7. In
NTCIR10, we simply used the all-English side of
the training data. All the 5-gram language mod-
els were estimated using SRILM (Stolcke and oth-
ers, 2002) with interpolated Kneser-Ney smooth-
ing. The details of the corpus are presented in Ta-
ble 2. For detailed analysis, we also evaluate Hiero
grammars extracted from GIZA++ (Och and Ney,
2003) grow-diag-final bidirectional alignments us-
ing Moses (Koehn et al., 2007) with Hiero options.
</bodyText>
<footnote confidence="0.9523195">
5Gen and Back took 1 day, Back+future took 1.5 days for
inference.
</footnote>
<page confidence="0.735803">
1223
</page>
<table confidence="0.9997842">
News-Commentary NTCIR10
de-en es-en fr-en ja-en
Model Sample BLEU SIZE BLEU SIZE BLEU SIZE BLEU SIZE
*GIZA++ - 16.66 7.07M 23.16 6.07M 20.79 6.25M 26.08 3.45M
Gen 1 15.36 397.63k 21.10 295.69k 19.45 311.76k 25.73 262.45k
10 15.39 529.46k 20.83 384.55k 19.24 419.33k 25.79 344.67k
Back 1 15.30 410.92k 21.43 314.95k 19.74 362.22k 25.69 294.90k
10 15.42 563.80k 21.53 420.15k 19.51 497.51k 25.63 388.87k
Back + future 1 15.49 384.69k 21.63 296.30k 19.97 340.70k 25.82 268.38k
10 15.55 579.12k 21.74 429.33k 19.97 513.41k 25.41 390.23k
</table>
<tableCaption confidence="0.998733">
Table 1: Results of translation evaluation in 100k corpus
</tableCaption>
<table confidence="0.999552625">
de-en es-en fr-en ja-en
TM(en) 1.85M 1.67M 1.54M 1.80M
TM(other) 1.86M 1.86M 1.83M 2.03M
LM(en) 55.6M 55.6M 55.6M 27.8M
Dev(en) 65.5k 65.5k 65.5k 67.3k
Dev(other) 62.7k 68.1k 72.5k 73.0k
Test(en) 61.9k 61.9k 61.9k 310k
Test(other) 61.3k 65.5k 70.5k 333k
</table>
<tableCaption confidence="0.990592">
Table 2: The number of words in training data
</tableCaption>
<table confidence="0.960469333333333">
TM LM Dev Test
de 31.3M - 55.1k 59.4k
en 32.8M 50.5M 58.8k 55.5k
</table>
<tableCaption confidence="0.999851">
Table 3: The number of words in training data
</tableCaption>
<bodyText confidence="0.999958521739131">
We use GIZA++ and Moses default parameters for
training. Decoding was carried out using the cdec
decoder (?). Feature weights were tuned on the de-
velopment data by running MIRA (Chiang, 2012)
for 20 iterations with 16 parallel. For other param-
eters, we used cdec’s default values. The numbers
reported here are the average of three tuning runs
(Hopkins and May, 2011).
Table 1 lists the results measured using BLEU
(Papineni et al., 2002).The term Sample denotes
the combination size for each model. The term
SIZE in the table denotes the number of the ex-
tracted grammar types composed of Hiero rules
and phrase pairs. The numbers in italic denotes
the score of Back, significantly improved from
the score of 1 sampled combinated Gen. The
numbers in bold denotes the score of Back +
future, significantly improved from the score of
1 sampled combinated Back. All significance
test are performed using Clark et al. (2011) un-
der p-value of 0.05. Back performed better than
Gen on Spanish-English and French-English lan-
guage pairs. Note that the gains were achieved
with the comparable grammar size. When com-
paring German-English and Japanese-English lan-
guage pairs, there are no significant differences
between Back and Gen. The combination of our
Back with future score during slice sampling (+fu-
ture) achieved further gains over the slice sam-
pling without future scores, and slightly decrese
the grammar size, compared to Back. However,
there are still no significant difference between
Back+future and Gen on German-English and
Japanese-English language pairs. Sample combi-
nation has no or slight gain on BLEU score, in
spite of the increase in grammar size. From the
results, using last one sample as a grammar is suf-
ficient for translation quality. The performance of
the Bayesian model did not match with that for the
GIZA++ pipeline heuristic approach. In general,
complex model, such as Gen and Back, demands
larger corpus size for training, and the evaluation
on such smaller corpus may not be a fair com-
parison, since the sampling approach can rely on
only sampled derivations. Thus, we evaluate these
methods on large size corpus in the next section.
</bodyText>
<subsectionHeader confidence="0.999948">
6.2 Comparison with Heuristic Extraction
</subsectionHeader>
<bodyText confidence="0.991359333333333">
As reported in (Koehn et al., 2003; DeNero et
al., 2006), the comparison against heuristic ex-
traction is a challenging task. We compare the
Back+future and a baseline extracted from grow-
diag-final alignments of GIZA++ using Moses
with Hiero options. We use GIZA++ and Moses
default parameters for training. In addition, we
present heuristic extraction from the last 1 sample
of Back+future in +Exhaustive.
We used the full europarl-v7 German-English
corpus as presented in Table 3. The experimen-
tal set up was similar to that in Section 6.1 with
the following exceptions; Slice sampling parame-
ter a was set to 0.05. Mini-batch size was set to
1024 and sampling was performed 5 iterations.6
The translation model was extracted by last 1 iter-
ations.
Table 4 lists the results7. Our Back+future can
</bodyText>
<footnote confidence="0.9982385">
6Inference took 5 days.
7The row mark up with * indicate the model using word
</footnote>
<page confidence="0.943629">
1224
</page>
<table confidence="0.9990578">
Model BLEU SIZE
* GIZA++ Model 4 27.21 73.24M (x14.0)
GIZA++ Model 3 26.78 59.26M (x11.3)
Back + future 26.83 5.25M (x1.0)
Back + future + exhasustive 26.73 90.42M (x17.2)
</table>
<tableCaption confidence="0.999396">
Table 4: Results of translation evaluation in de-en
full size corpus.
Table 5: Example of a grammar
</tableCaption>
<figure confidence="0.4614158">
8 Conclusion
Gen gin X kamera / silver X camera
en / salt
Back + future
gin en kamera / silver salt camera
</figure>
<bodyText confidence="0.985356133333333">
As a result, Back+future infers better models by
avoiding over pruning spans.
The BLEU score of our back-off model did not
achieve gains over the heuristic baselines. The de-
tail analysis of the learned Hiero grammar’s CRP
tables reveals that the grammar is very sparse and
may have little generalization capacity. The ex-
pansion of back-off process and the use of word
classes will solve the sparsity and increase the
translation quality.
decrease the grammar size against GIZA++ with
comparable BLEU score. Surprisingly, exhaustive
extraction had no gains, probably because of the
word alignment in each Hiero rules relied on the
IBM Model 1.
</bodyText>
<sectionHeader confidence="0.978534" genericHeader="conclusions">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999745363636364">
Intuitively, the use of the hierarchical back-off in-
creases the Hiero grammar size, since the phrases
of all the granularities in the derivation trees are
incorporated in the grammar. In contrast, our hier-
archical back-off model achieved gains in transla-
tion quality without increasing the size of the ex-
tracted grammar when compared to the previous
generative model. The major differences were the
use of the minimal phrase pairs used in the previ-
ous work in which only minimal phrase pairs in
the leaves of derivation trees were included in the
model. As a result, larger phrase pairs were forced
to be constructed from those minimal rules. On the
other hand, our back-off model could directly ex-
press phrase pairs of multiple granularities. In par-
ticular, a complex noun may be composed of sev-
eral Hiero rules in the previous model, but it can
be directly expressed by a single phrase pair in our
model. Table 5 gives an example of a Japanese-
English phrase pair which is represented by two
Hiero rules in the previous model; it is directly ex-
pressed by a single phrase pair in our model.
The BLEU score of Back+future was higher
than the generative baseline with comparable
grammar size. We observed that a very different
word alignment was sampled in every training it-
eration; the tendency was very frequent for func-
tion words. Our future score for inferring the slice
variables may take into account the context in a
sentence better than those without the future score.
class informations. Model 3 and our Back-off model dose not
use any word class informations.
We proposed a hierarchical back-off model for
Hiero grammar. Our back-off model achieved
higher or equal translation quality against a previ-
ous Bayesian model under BLEU score on various
language pairs;German/French/Spanish/Japanese-
English. In addition to the hierarchical back-off
model, we also proposed a two-step slice sampling
approach. We showed that the two-step slice sam-
pling approach can avoid over-pruning by incor-
porating a future score for estimating slice vari-
ables, which led to increase in translation quality
through the experiments. The joint use of hierar-
chical back-off model and two step slice sampling
approach achieved comparable translation quality
on a full size Germany-English language pair in
Europarl v7 corpus with with significantly smaller
grammar size; 10% less than that for he heuristic
baseline.
For future work, we plan to embed a back-off
feature to decoder which is computed for all the
phrase pairs constructed in a derivation during the
decoding process. We will reflect the change of a
probability as a statefull feature for decoding step.
</bodyText>
<sectionHeader confidence="0.997854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9898122">
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238–241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
782–790, Suntec, Singapore, August. Association
for Computational Linguistics.
</reference>
<page confidence="0.901725">
1225
</page>
<reference confidence="0.999580097345133">
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 17–53. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. The Jour-
nal of Machine Learning Research, 13(1):1159–
1187.
Tagyoung Chung, Licheng Fang, Daniel Gildea, and
Daniel ˇStefankoviˇc. 2014. Sampling tree frag-
ments from forests. Computational Linguistics,
40(1):203–229.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Martin Cmejrek and Bowen Zhou. 2010. Two meth-
ods for extending hierarchical rules from the bilin-
gual chart parsing. In Coling 2010: Posters, pages
180–188, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
Trevor Cohn and Gholamreza Haffari. 2013. An infi-
nite hierarchical bayesian model of phrasal transla-
tion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 780–790, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In Proceedings on the
Workshop on Statistical Machine Translation, pages
31–38, New York City, June. Association for Com-
putational Linguistics.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure un-
der a Bayesian translation model. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 314–323, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7–12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223–232, Jeju Island, Korea, July.
Association for Computational Linguistics.
Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 962–971, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Daniel Marcu and Daniel Wong. 2002. A phrase-
based,joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
</reference>
<page confidence="0.796137">
1226
</page>
<reference confidence="0.99985125">
on Empirical Methods in Natural Language Pro-
cessing, pages 133–139. Association for Computa-
tional Linguistics, July.
Radford M Neal. 2003. Slice sampling. Annals of
statistics, pages 705–741.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 632–
641, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Xiaochang Peng and Daniel Gildea. 2014. Type-based
mcmc for sampling tree fragments from forests.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1735–1745, Doha, Qatar, October.
Association for Computational Linguistics.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, pages 855–
900.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 985–992, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden markov model. In Proceedings of the
25th international conference on Machine learning,
pages 1088–1095. ACM.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and
Shouxun Lin. 2012. Unsupervised discrimina-
tive induction of synchronous grammar for machine
translation. In Proceedings of COLING 2012, pages
2883–2898, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972–983, Jeju Island, Korea, July.
Association for Computational Linguistics.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 370–379, Atlanta, Georgia, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.992383">
1227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417645">
<title confidence="0.997319">Hierarchical Back-off Modeling of Hiero Grammar based</title>
<author confidence="0.482601">Non-parametric Bayesian Model</author>
<abstract confidence="0.918201">kamigaito@lr.pi.titech.ac.jp tarow@google.com takamura@pi.titech.ac.jp oku@pi.titech.ac.jp eiichiro.sumita@nict.go.jp</abstract>
<affiliation confidence="0.99788">Institute of Technology Japan Inc. Institute of Information and Communication Technology</affiliation>
<abstract confidence="0.998062705882353">In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/Japanese- English. When compared against heuristic models, our model achieved comparable translation quality on a full size German- English language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>238--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="4314" citStr="Blunsom and Cohn, 2010" startWordPosition="639" endWordPosition="642">ransduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f|3). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to p</context>
<context position="7019" citStr="Blunsom and Cohn (2010)" startWordPosition="1060" endWordPosition="1064">d on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xiao et al. (2012) proposed a two-step approach for bi-parsing a bilingual sentence in O(|f|3) in the context of inducing SCFG rules discriminatively; however, their approach violates the detailed balance due to its heuristic k-best pruning. Blunsom and Cohn (2010) proposed a slice sampling for an SCFG, in the same manner as that for Infinite Hiden Markov Model (iHMM) (Van Gael et al., 2008), which can efficiently prune a space of possible derivations on the basis of dynamic programming. Although slice sampling can prune spans without violating the detailed balance, its time complexity of O(|f|3|e|3) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directl</context>
<context position="15808" citStr="Blunsom and Cohn (2010)" startWordPosition="2651" endWordPosition="2654">or the input distribution XE(α//3) greater than zero. 1220 The base measure for phrases Gp� is composed of three generative processes, in a similar manner as Levenberg et al. (2012), the number of terminal symbols in a phrase pair in the source side, |s|, is generated from a Poisson distribution |s |∼ Poisson(0.1). The length for the target side |t |is generated in the same manner as the source side of the phrase pair. The alignments between s and t are also generated in the same manner as those for the base measure in a rule. 4 Inference In inference, we use a sentence-wise block sampling of Blunsom and Cohn (2010), which has a better convergence property when compared with a step-wise Gibbs sampling. We repeat the following steps given a sentence pair. 1. Decrement customers of the rules and phrase pairs used in the current derivation for the sentence pair. 2. Bi-parse the sentence pair in a bottom up manner. 3. Sample a new derivation tree in a top-down manner. 4. Increment customers of the rules and phrase pairs in the sampled derivation tree. The most time-consuming step during the inference procedure is bi-parsing of a sentence pair which essentially takes O(|f|3|a|3) time using a bottom up dynamic</context>
<context position="18060" citStr="Blunsom and Cohn (2010)" startWordPosition="3072" endWordPosition="3075"> · · , |target |do 3: cubea ← {soucei,targetj} 4: end for 5: cubea ← {soucei, null} 6: charta ← 5lice5ampling(cubea) 7: clear cubea 8: end for 9: for h ← 1, · · · , |source |do 10: for all the i, j s.t j − i = h do 11: for inferable rule, phrase from the subspans of [i, j] of all charts do 12: cube ← rule, phrase 13: end for 14: chart ← 5lice5ampling(cube) 15: clear cube 16: end for 17: end for time complexity for the word alignment enumeration from lines 1 to 7 is O(|f||e|) and that for the phrase and rule enumeration from lines 8 to 15 is O(|f|3). The key difference to the slice sampling of Blunsom and Cohn (2010) lies in lines 6 and 3 of Algorithm 1. Let d denote a set of derivation trees d and u be a set of slice variables u. In slice sampling, we prune the rules r,,p in each source span sp based on a slice variable usp corresponding to that sp. After pruning, we sample trees from the pruned space of r. The above process is formally represented as: u ∼ P(u|d), d ∼ P(d|u), (9) where P(d|u) is computed through sampling in a top-down manner after parsing in a bottomup manner with Algorithm 1, and is equal to Hd P(d |u). The probability P(u|d) is equal to Hsp P(usp|d). Let r*denote a currently adopted sp</context>
<context position="19818" citStr="Blunsom and Cohn (2010)" startWordPosition="3391" endWordPosition="3394">p, tsp) &apos; P←−−M1(ssp, tsp))� 2. (11) We use IBM Model1 outside probability for future score Future(rsp). Similarly, the future score Future(rsp) is computed using the two directional models: (P−−→M1(ssp, tsp) &apos; P←−−M1(ssp.tsp))� 2. (12) When sp is used in the current derivation d, slice variable usp is sampled from a uniform distribution2: otherwise, usp is sampled from a beta distribution if sp is not in the current derivation d: P(usp|d) = Beta(usp; a,1.0), (14) where a &lt; 1 is a parameter for the beta distribution. If the Score(rspz) is less than usp, we prune the rspz from cube. Similar to Blunsom and Cohn (2010), if the span sp is not in the current derivation, the rules with low probability are pruned according to Equation (14). Let rdsp denotes a rule in d with span sp, P(d|u) is calculated by: In our experiments discussed in Section 6, slice sampling parameter a was set to 0.02 when incorporating the future score of Equation (12). In contrast, we used a = 0.1 when performing slice sampling without the future score. We empirically found that setting a lower value for a led to slower progress in learning due to a combinatorial explosion when inferencing a derivation for each sentence pair. In the be</context>
<context position="22443" citStr="Blunsom and Cohn, 2010" startWordPosition="3838" endWordPosition="3841">in the same way as (dp, Bp). The hyperparameter &apos;Yb is resampled from distribution: (cback + &apos;Yb &apos; Gb)(cbase + &apos;Yb &apos; Gb) (cback + cbase + &apos;Yb)2 where O, used in the generative process for either terminal or non-terminal symbol typei — Bernoulli(Oα), is resampled from a distribution: H Bernoulli(O|α|)c⟨«/a⟩, (18) (α/β)EBase where c(α/β) denotes the number of customers of rule (α/,(i), and Base denotes a set of rules generated from the base measure. All the hyperparameters are inferred by slice sampling (Neal, 2003). 5 Extraction of Translation Model In the previous work on Bayesian approaches (Blunsom and Cohn, 2010; Levenberg et al., 2012), it is a standard practice to heuristically extract rules and phrase pairs from the word alignment derived from the derivation trees sampled 3Note that we use k = 30 for k-best pruning. P(usp|d) = Score(r*sp) , (13) ff(usp &lt; Score(r* sp)) (15) ErjErsn P(rj)ff(usp &lt; Score(rj)) H spEd P(rdsp) [Bp] II PP | n H [Bp] 1 P ($,t) |φp| H k=1 , (17) 1222 from the Bayesian models. Instead of the heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pai</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 238–241, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>782--790</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2744" citStr="Blunsom et al., 2009" startWordPosition="397" endWordPosition="400"> a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f|3|e|3) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on t</context>
<context position="6069" citStr="Blunsom et al., 2009" startWordPosition="907" endWordPosition="910">, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f|3|e|3) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 782–790, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26492" citStr="Callison-Burch et al., 2010" startWordPosition="4496" endWordPosition="4499">ions, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) w</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="1776" citStr="Chiang, 2007" startWordPosition="236" endWordPosition="237">o-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model. 1 Introduction Hierarchical phrase-based statistical machine translation (HPBSMT) (Chiang, 2007) is a popular alternative to phrase-based SMT (PBSMT), in which synchronous context free grammar (SCFG) is used as the basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time fo</context>
<context position="8372" citStr="Chiang, 2007" startWordPosition="1290" endWordPosition="1291"> a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let E denote a set of terminal symbols in the source language, ∆ a set of terminal symbols in the target language, 1218 erative process is represented as follows: GX ∼ Prule(dr, θr, Gr0), X → ⟨α/β⟩ ∼ GX, (1) Figure 1: Derivation tree generated from Bayesian SCFG model V a set of non-terminal symbols, S a start symbol and R a set of rewrite rules. An SCFG is denoted as a tuple of ⟨E, ∆, V, S, R⟩. Each rewrite rule in R is represented as X → ⟨α/β⟩ in which α is a string of non-terminals and source side termina</context>
<context position="9731" citStr="Chiang, 2007" startWordPosition="1560" endWordPosition="1561"> wo eigo ni honyaku suru koto wa muzukasii 。 /Japanese is difficult to translate into English .” is represented as follows: S → X1 eigo X2 muzukasii 。 / X1 difficult X2 English. X1 → X3 wo / X3 is X2 → X4 honyaku suru X5 wa / X4 translate X5 X3 → nihongo / Japanese X4 → ni / into X5 → koto / to . A Hiero grammar has additional constraints over a general SCFG; the number of terminal symbols in each rule for both source and target sides is limited to 5. Each rule may contain at most two non-terminal symbols; adjacent non-terminal symbols in the source side are prohibited. For details, refer to (Chiang, 2007). 3.1 Bayesian SCFG Models Previous Bayesian SCFG Models, for instance a model proposed by Levenberg et al. (2012), are based on the Pitman-Yor process (Pitman and Yor, 1997) and learn SCFG rules by sampling a derivation tree for each bilingual sentence. Figure 1 shows an example derivation tree for our running example sentence pair under the model. The genwhere GX is a derivation tree and Prule(dr, θr, Gro) is a Pitman-Yor process (Pitman and Yor, 1997), which is a generalization of a Dirichlet process parametrized by a discount parameter dr, a strength parameter θr and a base measure Gro. Th</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>1187</pages>
<contexts>
<context position="28639" citStr="Chiang, 2012" startWordPosition="4855" endWordPosition="4856">00k corpus de-en es-en fr-en ja-en TM(en) 1.85M 1.67M 1.54M 1.80M TM(other) 1.86M 1.86M 1.83M 2.03M LM(en) 55.6M 55.6M 55.6M 27.8M Dev(en) 65.5k 65.5k 65.5k 67.3k Dev(other) 62.7k 68.1k 72.5k 73.0k Test(en) 61.9k 61.9k 61.9k 310k Test(other) 61.3k 65.5k 70.5k 333k Table 2: The number of words in training data TM LM Dev Test de 31.3M - 55.1k 59.4k en 32.8M 50.5M 58.8k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, </context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. The Journal of Machine Learning Research, 13(1):1159– 1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Licheng Fang</author>
<author>Daniel Gildea</author>
<author>Daniel ˇStefankoviˇc</author>
</authors>
<title>Sampling tree fragments from forests.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Chung, Fang, Gildea, ˇStefankoviˇc, 2014</marker>
<rawString>Tagyoung Chung, Licheng Fang, Daniel Gildea, and Daniel ˇStefankoviˇc. 2014. Sampling tree fragments from forests. Computational Linguistics, 40(1):203–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="29368" citStr="Clark et al. (2011)" startWordPosition="4976" endWordPosition="4979"> here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that the gains were achieved with the comparable grammar size. When comparing German-English and Japanese-English language pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+future) achieved further gains over the slice sampling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-Eng</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Two methods for extending hierarchical rules from the bilingual chart parsing.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>180--188</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5668" citStr="Cmejrek and Zhou, 2010" startWordPosition="842" endWordPosition="846">ing et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the </context>
</contexts>
<marker>Cmejrek, Zhou, 2010</marker>
<rawString>Martin Cmejrek and Bowen Zhou. 2010. Two methods for extending hierarchical rules from the bilingual chart parsing. In Coling 2010: Posters, pages 180–188, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Gholamreza Haffari</author>
</authors>
<title>An infinite hierarchical bayesian model of phrasal translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>780--790</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7918" citStr="Cohn and Haffari, 2013" startWordPosition="1209" endWordPosition="1213">violating the detailed balance, its time complexity of O(|f|3|e|3) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let E denote a set of terminal symbols in the source langua</context>
</contexts>
<marker>Cohn, Haffari, 2013</marker>
<rawString>Trevor Cohn and Gholamreza Haffari. 2013. An infinite hierarchical bayesian model of phrasal translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 780–790, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="5563" citStr="DeNero et al. (2006)" startWordPosition="827" endWordPosition="830">ecreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A</context>
<context position="24578" citStr="DeNero et al., 2006" startWordPosition="4205" endWordPosition="4208">ities in two directions, Pmodel(f|e) and Pmodel(e|f), are estimated by marginalizing the joint probability Pmodel(f, e): Pmodel(f|e) = Pmodel (f, e) (19) Ef′ Pmodel (f′ , e) The inverse direction Pmodel(e|f) is estimated, similarly. The lexical probabilities in two directions, Plex(f|e) and Plex(e|f), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4Note that the correct way to decode from our model is to score every phrase pair created during decoding with back-off states, which is</context>
<context position="30705" citStr="DeNero et al., 2006" startWordPosition="5192" endWordPosition="5195">rease in grammar size. From the results, using last one sample as a grammar is sufficient for translation quality. The performance of the Bayesian model did not match with that for the GIZA++ pipeline heuristic approach. In general, complex model, such as Gen and Back, demands larger corpus size for training, and the evaluation on such smaller corpus may not be a fair comparison, since the sampling approach can rely on only sampled derivations. Thus, we evaluate these methods on large size corpus in the next section. 6.2 Comparison with Heuristic Extraction As reported in (Koehn et al., 2003; DeNero et al., 2006), the comparison against heuristic extraction is a challenging task. We compare the Back+future and a baseline extracted from growdiag-final alignments of GIZA++ using Moses with Hiero options. We use GIZA++ and Moses default parameters for training. In addition, we present heuristic extraction from the last 1 sample of Back+future in +Exhaustive. We used the full europarl-v7 German-English corpus as presented in Table 3. The experimental set up was similar to that in Section 6.1 with the following exceptions; Slice sampling parameter a was set to 0.05. Mini-batch size was set to 1024 and samp</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings on the Workshop on Statistical Machine Translation, pages 31–38, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<contexts>
<context position="24502" citStr="Dyer et al., 2010" startWordPosition="4192" endWordPosition="4195">an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel(f|e) and Pmodel(e|f), are estimated by marginalizing the joint probability Pmodel(f, e): Pmodel(f|e) = Pmodel (f, e) (19) Ef′ Pmodel (f′ , e) The inverse direction Pmodel(e|f) is estimated, similarly. The lexical probabilities in two directions, Plex(f|e) and Plex(e|f), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4Note that the correct way to decode from our model is to sc</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker></marker>
<rawString>In Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Ka Po Chow</author>
<author>Bin Lu</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the ntcir-10 workshop.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</booktitle>
<contexts>
<context position="26552" citStr="Goto et al., 2013" startWordPosition="4506" endWordPosition="4509">rivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) with interpolated Kneser-Ney smoothing. The details of the co</context>
</contexts>
<marker>Goto, Chow, Lu, Sumita, Tsou, 2013</marker>
<rawString>Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K Tsou. 2013. Overview of the patent machine translation task at the ntcir-10 workshop. In Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="28815" citStr="Hopkins and May, 2011" startWordPosition="4883" endWordPosition="4886">other) 62.7k 68.1k 72.5k 73.0k Test(en) 61.9k 61.9k 61.9k 310k Test(other) 61.3k 65.5k 70.5k 333k Table 2: The number of words in training data TM LM Dev Test de 31.3M - 55.1k 59.4k en 32.8M 50.5M 58.8k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better t</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>967--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2516" citStr="Johnson et al., 2007" startWordPosition="360" endWordPosition="363">basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f|3|e|3) when we use dynamic programming SCFG biparsing (Wu, 199</context>
<context position="5022" citStr="Johnson et al., 2007" startWordPosition="745" endWordPosition="748">rivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. R</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 967–975, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24556" citStr="Koehn et al., 2003" startWordPosition="4201" endWordPosition="4204">ional model probabilities in two directions, Pmodel(f|e) and Pmodel(e|f), are estimated by marginalizing the joint probability Pmodel(f, e): Pmodel(f|e) = Pmodel (f, e) (19) Ef′ Pmodel (f′ , e) The inverse direction Pmodel(e|f) is estimated, similarly. The lexical probabilities in two directions, Plex(f|e) and Plex(e|f), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4Note that the correct way to decode from our model is to score every phrase pair created during decoding with bac</context>
<context position="30683" citStr="Koehn et al., 2003" startWordPosition="5188" endWordPosition="5191"> in spite of the increase in grammar size. From the results, using last one sample as a grammar is sufficient for translation quality. The performance of the Bayesian model did not match with that for the GIZA++ pipeline heuristic approach. In general, complex model, such as Gen and Back, demands larger corpus size for training, and the evaluation on such smaller corpus may not be a fair comparison, since the sampling approach can rely on only sampled derivations. Thus, we evaluate these methods on large size corpus in the next section. 6.2 Comparison with Heuristic Extraction As reported in (Koehn et al., 2003; DeNero et al., 2006), the comparison against heuristic extraction is a challenging task. We compare the Back+future and a baseline extracted from growdiag-final alignments of GIZA++ using Moses with Hiero options. We use GIZA++ and Moses default parameters for training. In addition, we present heuristic extraction from the last 1 sample of Back+future in +Exhaustive. We used the full europarl-v7 German-English corpus as presented in Table 3. The experimental set up was similar to that in Section 6.1 with the following exceptions; Slice sampling parameter a was set to 0.05. Mini-batch size wa</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27353" citStr="Koehn et al., 2007" startWordPosition="4640" endWordPosition="4643">ined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) with interpolated Kneser-Ney smoothing. The details of the corpus are presented in Table 2. For detailed analysis, we also evaluate Hiero grammars extracted from GIZA++ (Och and Ney, 2003) grow-diag-final bidirectional alignments using Moses (Koehn et al., 2007) with Hiero options. 5Gen and Back took 1 day, Back+future took 1.5 days for inference. 1223 News-Commentary NTCIR10 de-en es-en fr-en ja-en Model Sample BLEU SIZE BLEU SIZE BLEU SIZE BLEU SIZE *GIZA++ - 16.66 7.07M 23.16 6.07M 20.79 6.25M 26.08 3.45M Gen 1 15.36 397.63k 21.10 295.69k 19.45 311.76k 25.73 262.45k 10 15.39 529.46k 20.83 384.55k 19.24 419.33k 25.79 344.67k Back 1 15.30 410.92k 21.43 314.95k 19.74 362.22k 25.69 294.90k 10 15.42 563.80k 21.53 420.15k 19.51 497.51k 25.63 388.87k Back + future 1 15.49 384.69k 21.63 296.30k 19.97 340.70k 25.82 268.38k 10 15.55 579.12k 21.74 429.33k 19</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A bayesian model for learning scfgs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>223--232</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3177" citStr="Levenberg et al., 2012" startWordPosition="465" endWordPosition="468">t al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f|3|e|3) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granu</context>
<context position="6136" citStr="Levenberg et al., 2012" startWordPosition="918" endWordPosition="921">heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f|3|e|3) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on t</context>
<context position="9845" citStr="Levenberg et al. (2012)" startWordPosition="1576" endWordPosition="1579">esented as follows: S → X1 eigo X2 muzukasii 。 / X1 difficult X2 English. X1 → X3 wo / X3 is X2 → X4 honyaku suru X5 wa / X4 translate X5 X3 → nihongo / Japanese X4 → ni / into X5 → koto / to . A Hiero grammar has additional constraints over a general SCFG; the number of terminal symbols in each rule for both source and target sides is limited to 5. Each rule may contain at most two non-terminal symbols; adjacent non-terminal symbols in the source side are prohibited. For details, refer to (Chiang, 2007). 3.1 Bayesian SCFG Models Previous Bayesian SCFG Models, for instance a model proposed by Levenberg et al. (2012), are based on the Pitman-Yor process (Pitman and Yor, 1997) and learn SCFG rules by sampling a derivation tree for each bilingual sentence. Figure 1 shows an example derivation tree for our running example sentence pair under the model. The genwhere GX is a derivation tree and Prule(dr, θr, Gro) is a Pitman-Yor process (Pitman and Yor, 1997), which is a generalization of a Dirichlet process parametrized by a discount parameter dr, a strength parameter θr and a base measure Gro. The output probability of a PitmanYor process obeys the power-law distribution with the discount parameter, which is</context>
<context position="13863" citStr="Levenberg et al. (2012)" startWordPosition="2305" endWordPosition="2308"> three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). θp + np where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: cback + γb · Gb cback + cbase + γb ·Prule(dr, θr, Gphrase) In summary, Pphrase(dp, θp, GX) is defined as a joint probability of Equations (5) through (7). 3.3 Base Measure Similar to Levenberg et al. (2012), the base measure for rule probability Gr0 is composed of four generative processes. First, a number of symbols in a source side of a rule |α |is generated from a Poisson distribution, i.e., |α |∼ Poisson(0.1). Let t(x) denote a function that returns terminals from a string x. The number of target side terminal symbols |t(β) |is also generated from a Poisson distribution and represented as |t(β) |∼ Poisson(α + λ0)1. The type of symbol αi in the source side, typei, either terminal or non-terminal symbol, is determined by typei ∼ Bernoulli(ϕ|α|) where ϕ is a hyperparameter taking 0 &lt; ϕ &lt; 1. ϕ|α</context>
<context position="15366" citStr="Levenberg et al. (2012)" startWordPosition="2569" endWordPosition="2572"> monolingual unigram probabilities for two languages, and represented as: ⟨t(α), t(β)⟩ ∼ (Puni(t(α))P−−→�1(t(α), t(β)) · Puni(t(β))P←−− �1(t(α),t(β)))� 2. (8) When the t(α) or t(β) is empty, we use the constant 0.01 instead of the Model1 probabilities. model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation (2): ck − dp ·|φpk| , (5) θp + dp · |φp| θp + np 11 · Pphrase(dp,θp, GX), (6) 1Note that a0 is a small constant for the input distribution XE(α//3) greater than zero. 1220 The base measure for phrases Gp� is composed of three generative processes, in a similar manner as Levenberg et al. (2012), the number of terminal symbols in a phrase pair in the source side, |s|, is generated from a Poisson distribution |s |∼ Poisson(0.1). The length for the target side |t |is generated in the same manner as the source side of the phrase pair. The alignments between s and t are also generated in the same manner as those for the base measure in a rule. 4 Inference In inference, we use a sentence-wise block sampling of Blunsom and Cohn (2010), which has a better convergence property when compared with a step-wise Gibbs sampling. We repeat the following steps given a sentence pair. 1. Decrement cus</context>
<context position="22468" citStr="Levenberg et al., 2012" startWordPosition="3842" endWordPosition="3845">Bp). The hyperparameter &apos;Yb is resampled from distribution: (cback + &apos;Yb &apos; Gb)(cbase + &apos;Yb &apos; Gb) (cback + cbase + &apos;Yb)2 where O, used in the generative process for either terminal or non-terminal symbol typei — Bernoulli(Oα), is resampled from a distribution: H Bernoulli(O|α|)c⟨«/a⟩, (18) (α/β)EBase where c(α/β) denotes the number of customers of rule (α/,(i), and Base denotes a set of rules generated from the base measure. All the hyperparameters are inferred by slice sampling (Neal, 2003). 5 Extraction of Translation Model In the previous work on Bayesian approaches (Blunsom and Cohn, 2010; Levenberg et al., 2012), it is a standard practice to heuristically extract rules and phrase pairs from the word alignment derived from the derivation trees sampled 3Note that we use k = 30 for k-best pruning. P(usp|d) = Score(r*sp) , (13) ff(usp &lt; Score(r* sp)) (15) ErjErsn P(rj)ff(usp &lt; Score(rj)) H spEd P(rdsp) [Bp] II PP | n H [Bp] 1 P ($,t) |φp| H k=1 , (17) 1222 from the Bayesian models. Instead of the heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at l</context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A bayesian model for learning scfgs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Jo˜ao Grac¸a</author>
<author>Isabel Trancoso</author>
<author>Alan Black</author>
</authors>
<title>Entropy-based pruning for phrasebased machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>962--971</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<marker>Ling, Grac¸a, Trancoso, Black, 2012</marker>
<rawString>Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso, and Alan Black. 2012. Entropy-based pruning for phrasebased machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 962–971, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Daniel Wong</author>
</authors>
<title>A phrasebased,joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="5456" citStr="Marcu and Wong, 2002" startWordPosition="813" endWordPosition="816">ntly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and Daniel Wong. 2002. A phrasebased,joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 133–139. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling. Annals of statistics,</title>
<date>2003</date>
<pages>705--741</pages>
<contexts>
<context position="22340" citStr="Neal, 2003" startWordPosition="3824" endWordPosition="3825">l, and c(,,,t) the number of customers of phrase pair (s, t). We resample the pair (dr, Br) in the same way as (dp, Bp). The hyperparameter &apos;Yb is resampled from distribution: (cback + &apos;Yb &apos; Gb)(cbase + &apos;Yb &apos; Gb) (cback + cbase + &apos;Yb)2 where O, used in the generative process for either terminal or non-terminal symbol typei — Bernoulli(Oα), is resampled from a distribution: H Bernoulli(O|α|)c⟨«/a⟩, (18) (α/β)EBase where c(α/β) denotes the number of customers of rule (α/,(i), and Base denotes a set of rules generated from the base measure. All the hyperparameters are inferred by slice sampling (Neal, 2003). 5 Extraction of Translation Model In the previous work on Bayesian approaches (Blunsom and Cohn, 2010; Levenberg et al., 2012), it is a standard practice to heuristically extract rules and phrase pairs from the word alignment derived from the derivation trees sampled 3Note that we use k = 30 for k-best pruning. P(usp|d) = Score(r*sp) , (13) ff(usp &lt; Score(r* sp)) (15) ErjErsn P(rj)ff(usp &lt; Score(rj)) H spEd P(rdsp) [Bp] II PP | n H [Bp] 1 P ($,t) |φp| H k=1 , (17) 1222 from the Bayesian models. Instead of the heuristic method, we directly extract rules and phrase pairs from the learned model</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M Neal. 2003. Slice sampling. Annals of statistics, pages 705–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>632--641</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3744" citStr="Neubig et al., 2011" startWordPosition="555" endWordPosition="558">s sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f|3). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampl</context>
<context position="7952" citStr="Neubig et al. (2011)" startWordPosition="1217" endWordPosition="1220">ime complexity of O(|f|3|e|3) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let E denote a set of terminal symbols in the source language, ∆ a set of terminal symbols in</context>
<context position="11213" citStr="Neubig et al. (2011)" startWordPosition="1823" endWordPosition="1826">ocess which is decomposed into two probability distributions. If rk already exists in a table, we draw rk with probability ck − dr · |φrk |,(2) θr + nr where ck is the number of customers of rk, nr is the number of all customers and φrk is a number of rk’s tables. On the other hand, if rk is a new rule, we draw rk with probability θr + dr · |φr| θr + nr where |φr |is the number of tables in the model. 3.2 Hierarchical Back-off Model In the previous models, the generative process is represented as a rewrite process starting from the symbol S, which can incorporate only minimal rules. Following Neubig et al. (2011), our model reverses the process by recursively backing off to smaller phrase pairs as shown in Figure 2. First, our model attempts to generate a phrase pair, i.e., a sentence pair, as a derivation tree. If the model successfully generates the phrase pair, we will finish the generation process. Otherwise, a Hiero rule is generated to fallback to smaller spans represented in each non-terminal symbol X in the rule. Then, each phrase pair corresponding to each smaller span is recursively generated through our model. In Figure 2, a phrase pair with “nil” indicates those not in our model; therefore</context>
<context position="23831" citStr="Neubig et al. (2011)" startWordPosition="4086" endWordPosition="4089">acted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior(f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4. The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel(f|e) and Pmodel(e|f), are estimated by marginalizing the joint probability Pmodel(f, e): Pmodel(f|e) = Pmodel (f, e) (19) Ef′ Pmodel (f′ , e) The inverse direction Pmodel(e|f) is estimated, similarly. The lexical probabilities in two directions, Plex(f|e) and Plex(e|f), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for</context>
<context position="25457" citStr="Neubig et al. (2011)" startWordPosition="4339" endWordPosition="4342">rative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4Note that the correct way to decode from our model is to score every phrase pair created during decoding with back-off states, which is computationally intractable compared with a model estimated using a heuristic method. The Hiero grammar severely suffers from the phrase granularity problem and can overfit to the training data due to the flexibility of the rules. To alleviate this problem, Neubig et al. (2011) combined the derivation trees across training iterations by averaging the features for each rule and phrase pair. During the sampling process, each training iteration draws a different derivation tree for each sentence pair, and the combination of those different derivation trees can provide multiple possible phrase boundaries to the model. Inspired by the averaging over the models from different iterations, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers in</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 632– 641, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="27279" citStr="Och and Ney, 2003" startWordPosition="4630" endWordPosition="4633">urce and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) with interpolated Kneser-Ney smoothing. The details of the corpus are presented in Table 2. For detailed analysis, we also evaluate Hiero grammars extracted from GIZA++ (Och and Ney, 2003) grow-diag-final bidirectional alignments using Moses (Koehn et al., 2007) with Hiero options. 5Gen and Back took 1 day, Back+future took 1.5 days for inference. 1223 News-Commentary NTCIR10 de-en es-en fr-en ja-en Model Sample BLEU SIZE BLEU SIZE BLEU SIZE BLEU SIZE *GIZA++ - 16.66 7.07M 23.16 6.07M 20.79 6.25M 26.08 3.45M Gen 1 15.36 397.63k 21.10 295.69k 19.45 311.76k 25.73 262.45k 10 15.39 529.46k 20.83 384.55k 19.24 419.33k 25.79 344.67k Back 1 15.30 410.92k 21.43 314.95k 19.74 362.22k 25.69 294.90k 10 15.42 563.80k 21.53 420.15k 19.51 497.51k 25.63 388.87k Back + future 1 15.49 384.69k 2</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="23467" citStr="Och, 2003" startWordPosition="4030" endWordPosition="4031">heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at least once in the sample. During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior(f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4. The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel(f|e) and Pmodel(e|f), are estimated by marginalizing the joint probability Pm</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="28886" citStr="Papineni et al., 2002" startWordPosition="4895" endWordPosition="4898">er) 61.3k 65.5k 70.5k 333k Table 2: The number of words in training data TM LM Dev Test de 31.3M - 55.1k 59.4k en 32.8M 50.5M 58.8k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochang Peng</author>
<author>Daniel Gildea</author>
</authors>
<title>Type-based mcmc for sampling tree fragments from forests.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1735--1745</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="6160" citStr="Peng and Gildea, 2014" startWordPosition="922" endWordPosition="925">ion. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f|3|e|3) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xiao e</context>
</contexts>
<marker>Peng, Gildea, 2014</marker>
<rawString>Xiaochang Peng and Daniel Gildea. 2014. Type-based mcmc for sampling tree fragments from forests. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1735–1745, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<pages>855--900</pages>
<contexts>
<context position="9905" citStr="Pitman and Yor, 1997" startWordPosition="1586" endWordPosition="1589">X2 English. X1 → X3 wo / X3 is X2 → X4 honyaku suru X5 wa / X4 translate X5 X3 → nihongo / Japanese X4 → ni / into X5 → koto / to . A Hiero grammar has additional constraints over a general SCFG; the number of terminal symbols in each rule for both source and target sides is limited to 5. Each rule may contain at most two non-terminal symbols; adjacent non-terminal symbols in the source side are prohibited. For details, refer to (Chiang, 2007). 3.1 Bayesian SCFG Models Previous Bayesian SCFG Models, for instance a model proposed by Levenberg et al. (2012), are based on the Pitman-Yor process (Pitman and Yor, 1997) and learn SCFG rules by sampling a derivation tree for each bilingual sentence. Figure 1 shows an example derivation tree for our running example sentence pair under the model. The genwhere GX is a derivation tree and Prule(dr, θr, Gro) is a Pitman-Yor process (Pitman and Yor, 1997), which is a generalization of a Dirichlet process parametrized by a discount parameter dr, a strength parameter θr and a base measure Gro. The output probability of a PitmanYor process obeys the power-law distribution with the discount parameter, which is very common in standard NLP tasks. The probability that a r</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability, pages 855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="13345" citStr="Teh, 2006" startWordPosition="2208" endWordPosition="2209"> |cbase + γb · Gb archical back-off model θp + np cback + cbase + γb Gp0. ( 7) we reach phrase pairs which are generated without any back-offs. Let a discount parameter be dp, a strength parameter be θp, and a base measure be Gp0. More formally, the generative process is represented as follows: GX ∼ Prule(dr, θr, Gphrase), Gphrase ∼ Pphrase(dp, θp, GX), X → ⟨s/t⟩ ∼ Gphrase, X → ⟨α/β⟩ ∼ GX, (4) where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. Pphrase is composed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). θp + np where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: cback + γb · Gb cback + cbase + γb ·Prule(dr, θr, Gphrase) In summary, Pphrase(dp, θp, GX) is defined as a joint probability of Equations (5) through (7). 3.3 Base Measure Similar to Levenberg et al. (2012), the base measure for rule probability Gr0 is composed of four generative process</context>
<context position="21538" citStr="Teh, 2006" startWordPosition="3686" endWordPosition="3687">rallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample (dp, Bp), the pair of discount and strength parameters for phrases from a distribution: [1 — dp]�c⟨s,t⟩−1� (16) 1 where [ ] denotes a generalized Pochhammer symbol, and c(,,,t) the number of customers of phrase pair (s, t). We resample the pair (dr, Br) in the same way as (dp, Bp). The hyperparameter &apos;Yb is resampled from distribution: (cback + &apos;Yb &apos; Gb)(cbase + &apos;Yb &apos; Gb) (cback + cbase + &apos;Yb)2 where O, used in the generative process for either terminal or non-terminal symbol typei — Bernoulli(Oα), is resampled from a distribution: H Bernoulli(O|α|)c⟨«/a⟩, (18) (α/</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yunus Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam sampling for the infinite hidden markov model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1088--1095</pages>
<publisher>ACM.</publisher>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite hidden markov model. In Proceedings of the 25th international conference on Machine learning, pages 1088–1095. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="3118" citStr="Wu, 1997" startWordPosition="458" endWordPosition="459">, 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f|3|e|3) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) m</context>
<context position="16441" citStr="Wu, 1997" startWordPosition="2757" endWordPosition="2758">vergence property when compared with a step-wise Gibbs sampling. We repeat the following steps given a sentence pair. 1. Decrement customers of the rules and phrase pairs used in the current derivation for the sentence pair. 2. Bi-parse the sentence pair in a bottom up manner. 3. Sample a new derivation tree in a top-down manner. 4. Increment customers of the rules and phrase pairs in the sampled derivation tree. The most time-consuming step during the inference procedure is bi-parsing of a sentence pair which essentially takes O(|f|3|a|3) time using a bottom up dynamic programming algorithm (Wu, 1997). When a span is very large, it can easily suffer combinatorial explosion. To avoid this problem, we use a two-step slice sampling by performing the two-step bi-parsing of Xiao et al. (2012) and by pruning possible derivation space (Blunsom and Cohn, 2010) in each step (Algorithm 1). From lines 1 to 7, a set of word alignment is enumerated and put into cubea. In addition to the arbitrary word alignment of sourcei to targetj, null word alignment is also merged into cubea (line 5). Note that word alignment considered in the algorithm is restricted to one-to-many. The set of word alignments in cu</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Unsupervised discriminative induction of synchronous grammar for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2883--2898</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1212" citStr="Xiao et al., (2012)" startWordPosition="159" endWordPosition="162">y many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model. 1 Introduction Hierarchical phrase-based statistical machine translation (HPBSMT) (Chiang, 2007) is a popular alternative to phrase-</context>
<context position="4208" citStr="Xiao et al., 2012" startWordPosition="621" endWordPosition="624">granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f|3). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europa</context>
<context position="6772" citStr="Xiao et al. (2012)" startWordPosition="1023" endWordPosition="1026"> 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f|3|e|3) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xiao et al. (2012) proposed a two-step approach for bi-parsing a bilingual sentence in O(|f|3) in the context of inducing SCFG rules discriminatively; however, their approach violates the detailed balance due to its heuristic k-best pruning. Blunsom and Cohn (2010) proposed a slice sampling for an SCFG, in the same manner as that for Infinite Hiden Markov Model (iHMM) (Van Gael et al., 2008), which can efficiently prune a space of possible derivations on the basis of dynamic programming. Although slice sampling can prune spans without violating the detailed balance, its time complexity of O(|f|3|e|3) is still i</context>
<context position="16631" citStr="Xiao et al. (2012)" startWordPosition="2790" endWordPosition="2793">e current derivation for the sentence pair. 2. Bi-parse the sentence pair in a bottom up manner. 3. Sample a new derivation tree in a top-down manner. 4. Increment customers of the rules and phrase pairs in the sampled derivation tree. The most time-consuming step during the inference procedure is bi-parsing of a sentence pair which essentially takes O(|f|3|a|3) time using a bottom up dynamic programming algorithm (Wu, 1997). When a span is very large, it can easily suffer combinatorial explosion. To avoid this problem, we use a two-step slice sampling by performing the two-step bi-parsing of Xiao et al. (2012) and by pruning possible derivation space (Blunsom and Cohn, 2010) in each step (Algorithm 1). From lines 1 to 7, a set of word alignment is enumerated and put into cubea. In addition to the arbitrary word alignment of sourcei to targetj, null word alignment is also merged into cubea (line 5). Note that word alignment considered in the algorithm is restricted to one-to-many. The set of word alignments in cubea is pruned and added to the charta by SliceSampling. From lines 8 to 15, all possible phrases and rules for each span constrained by the pruned word alignment are enumerated and temporall</context>
<context position="20881" citStr="Xiao et al. (2012)" startWordPosition="3580" endWordPosition="3583">ing a lower value for a led to slower progress in learning due to a combinatorial explosion when inferencing a derivation for each sentence pair. In the beginning of training, we do not have any derivation trees for given training data, although the derivation trees are required for estimating parameters for Bayesian models. We use the two-step parsing for generating initial derivation trees from only base measures. The k-best 2H(·) is a function returns 1 if the condition is satisfied and 0 otherwise pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter re</context>
</contexts>
<marker>Xiao, Xiong, Liu, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and Shouxun Lin. 2012. Unsupervised discriminative induction of synchronous grammar for machine translation. In Proceedings of COLING 2012, pages 2883–2898, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Daisy Stanton</author>
<author>Peng Xu</author>
</authors>
<title>A systematic comparison of phrase table pruning techniques.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>972--983</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2566" citStr="Zens et al., 2012" startWordPosition="370" endWordPosition="373">a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f|3|e|3) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et</context>
<context position="5081" citStr="Zens et al., 2012" startWordPosition="756" endWordPosition="759">EU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate</context>
</contexts>
<marker>Zens, Stanton, Xu, 2012</marker>
<rawString>Richard Zens, Daisy Stanton, and Peng Xu. 2012. A systematic comparison of phrase table pruning techniques. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 972–983, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Liang Huang</author>
</authors>
<title>Minibatch and parallelization for online large margin structured learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>370--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="20975" citStr="Zhao and Huang (2013)" startWordPosition="3598" endWordPosition="3601"> when inferencing a derivation for each sentence pair. In the beginning of training, we do not have any derivation trees for given training data, although the derivation trees are required for estimating parameters for Bayesian models. We use the two-step parsing for generating initial derivation trees from only base measures. The k-best 2H(·) is a function returns 1 if the condition is satisfied and 0 otherwise pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample (dp, Bp)</context>
</contexts>
<marker>Zhao, Huang, 2013</marker>
<rawString>Kai Zhao and Liang Huang. 2013. Minibatch and parallelization for online large margin structured learning. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 370–379, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
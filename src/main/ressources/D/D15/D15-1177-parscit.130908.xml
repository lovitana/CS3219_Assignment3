<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.9979795">
Syntax-Aware Multi-Sense Word Embeddings
for Deep Compositional Models of Meaning
</title>
<author confidence="0.998846">
Jianpeng Cheng
</author>
<affiliation confidence="0.981665666666667">
University of Oxford
Department of
Computer Science
</affiliation>
<email confidence="0.988685">
jianpeng.cheng@stcatz.oxon.org
</email>
<sectionHeader confidence="0.993684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965739130435">
Deep compositional models of meaning
acting on distributional representations of
words in order to produce vectors of larger
text constituents are evolving to a pop-
ular area of NLP research. We detail
a compositional distributional framework
based on a rich form of word embeddings
that aims at facilitating the interactions
between words in the context of a sen-
tence. Embeddings and composition lay-
ers are jointly learned against a generic
objective that enhances the vectors with
syntactic information from the surround-
ing context. Furthermore, each word is
associated with a number of senses, the
most plausible of which is selected dy-
namically during the composition process.
We evaluate the produced vectors qualita-
tively and quantitatively with positive re-
sults. At the sentence level, the effective-
ness of the framework is demonstrated on
the MSRPar task, for which we report re-
sults within the state-of-the-art range.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985026">
Representing the meaning of words by using their
distributional behaviour in a large text corpus is
a well-established technique in NLP research that
has been proved useful in numerous tasks. In
a distributional model of meaning, the semantic
representation of a word is given as a vector in
some high dimensional vector space, obtained ei-
ther by explicitly collecting co-occurrence statis-
tics of the target word with words belonging to a
representative subset of the vocabulary, or by di-
rectly optimizing the word vectors against an ob-
jective function in some neural-network based ar-
chitecture (Collobert and Weston, 2008; Mikolov
et al., 2013).
Regardless their method of construction, distri-
butional models of meaning do not scale up to
</bodyText>
<author confidence="0.8838">
Dimitri Kartsaklis
</author>
<affiliation confidence="0.913317">
Queen Mary University of London
School of Electronic Engineering
and Computer Science
</affiliation>
<email confidence="0.95835">
d.kartsaklis@qmul.ac.uk
</email>
<bodyText confidence="0.999975604651163">
larger text constituents such as phrases or sen-
tences, since the uniqueness of multi-word expres-
sions would inevitably lead to data sparsity prob-
lems, thus to unreliable vectorial representations.
The problem is usually addressed by the provision
of a compositional function, the purpose of which
is to prepare a vectorial representation for a phrase
or sentence by combining the vectors of the words
therein. While the nature and complexity of these
compositional models may vary, approaches based
on deep-learning architectures have been shown to
be especially successful in modelling the meaning
of sentences for a variety of tasks (Socher et al.,
2012; Kalchbrenner et al., 2014).
The mutual interaction of distributional word
vectors by a means of a compositional model pro-
vides many opportunities for interesting research,
the majority of which still remains to be explored.
One such direction is to investigate in what way
lexical ambiguity affects the compositional pro-
cess. In fact, recent work has shown that shal-
low multi-linear compositional models that explic-
itly handle extreme cases of lexical ambiguity in a
step prior to composition present consistently bet-
ter performance than their “ambiguous” counter-
parts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis
et al., 2014). A first attempt to test these obser-
vations in a deep compositional setting has been
presented by Cheng et al. (2014) with promising
results.
Furthermore, a second important question re-
lates to the very nature of the word embeddings
used in the context of a compositional model. In a
setting of this form, word vectors are not any more
just a means for discriminating words based on
their underlying semantic relationships; the main
goal of a word vector is to contribute to a bigger
whole—a task in which syntax, along with seman-
tics, also plays a very important role. It is a central
point of this paper, therefore, that in a composi-
tional distributional model of meaning word vec-
tors should be injected with information that re-
flects their syntactical roles in the training corpus.
</bodyText>
<page confidence="0.934739">
1531
</page>
<note confidence="0.9847135">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1531–1542,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999943836363636">
The purpose of this work is to improve the
current practice in deep compositional models of
meaning in relation to both the compositional pro-
cess itself and the quality of the word embed-
dings used therein. We propose an architecture
for jointly training a compositional model and a
set of word embeddings, in a way that imposes
dynamic word sense induction for each word dur-
ing the learning process. Note that this is in con-
trast with recent work in multi-sense neural word
embeddings (Neelakantan et al., 2014), in which
the word senses are learned without any composi-
tional considerations in mind.
Furthermore, we make the word embeddings
syntax-aware by introducing a variation of the
hinge loss objective function of Collobert and We-
ston (2008), in which the goal is not only to predict
the occurrence of a target word in a context, but to
also predict the position of the word within that
context. A qualitative analysis shows that our vec-
tors reflect both semantic and syntactic features in
a concise way.
In all current deep compositional distributional
settings, the word embeddings are internal param-
eters of the model with no use for any other pur-
pose than the task for which they were specifically
trained. In this work, one of our main consid-
erations is that the joint training step should be
generic enough to not be tied in any particular
task. In this way the word embeddings and the de-
rived compositional model can be learned on data
much more diverse than any task-specific dataset,
reflecting a wider range of linguistic features. In-
deed, experimental evaluation shows that the pro-
duced word embeddings can serve as a high qual-
ity general-purpose semantic word space, present-
ing performance on the Stanford Contextual Word
Similarity (SCWS) dataset of Huang et al. (2012)
competitive to and even better of the performance
of well-established neural word embeddings sets.
Finally, we propose a dynamic disambiguation
framework for a number of existing deep compo-
sitional models of meaning, in which the multi-
sense word embeddings and the compositional
model of the original training step are further re-
fined according to the purposes of a specific task
at hand. In the context of paraphrase detection, we
achieve a result very close to the current state-of-
the-art on the Microsoft Research Paraphrase Cor-
pus (Dolan and Brockett, 2005). An interesting
aspect at the sideline of the paraphrase detection
experiment is that, in contrast to mainstream ap-
proaches that mainly rely on simple forms of clas-
sifiers, we approach the problem by following a
siamese architecture (Bromley et al., 1993).
</bodyText>
<sectionHeader confidence="0.60457" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<subsectionHeader confidence="0.956737">
2.1 Distributional models of meaning
</subsectionHeader>
<bodyText confidence="0.999984">
Distributional models of meaning follow the dis-
tributional hypothesis (Harris, 1954), which states
that two words that occur in similar contexts have
similar meanings. Traditional approaches for con-
structing a word space rely on simple counting: a
word is represented by a vector of numbers (usu-
ally smoothed by the application of some func-
tion such as point-wise mutual information) which
show how frequently this word co-occurs with
other possible context words in a corpus of text.
In contrast to these methods, a recent class of
distributional models treat word representations as
parameters directly optimized on a word predic-
tion task (Bengio et al., 2003; Collobert and We-
ston, 2008; Mikolov et al., 2013; Pennington et
al., 2014). Instead of relying on observed co-
occurrence counts, these models aim to maximize
the objective function of a neural net-based ar-
chitecture; Mikolov et al. (2013), for example,
compute the conditional probability of observ-
ing words in a context around a target word (an
approach known as the skip-gram model). Re-
cent studies have shown that, compared to their
co-occurrence counterparts, neural word vectors
reflect better the semantic relationships between
words (Baroni et al., 2014) and are more effective
in compositional settings (Milajevs et al., 2014).
</bodyText>
<subsectionHeader confidence="0.998919">
2.2 Syntactic awareness
</subsectionHeader>
<bodyText confidence="0.999981263157895">
Since the main purpose of distributional models
until now was to measure the semantic relatedness
of words, relatively little effort has been put into
making word vectors aware of information regard-
ing the syntactic role under which a word occurs
in a sentence. In some cases the vectors are POS-
tag specific, so that ‘book’ as noun and ‘book’
as verb are represented by different vectors (Kart-
saklis and Sadrzadeh, 2013). Furthermore, word
spaces in which the context of a target word is de-
termined by means of grammatical dependencies
(Pad´o and Lapata, 2007) are more effective in cap-
turing syntactic relations than approaches based
on simple word proximity.
For word embeddings trained in neural settings,
syntactic information is not usually taken explic-
itly into account, with some notable exceptions.
At the lexical level, Levy and Goldberg (2014)
propose an extension of the skip-gram model
</bodyText>
<page confidence="0.990723">
1532
</page>
<bodyText confidence="0.999736785714286">
based on grammatical dependencies. Following a
different approach, Mnih and Kavukcuoglu (2013)
weight the vector of each context word depending
on its distance from the target word. With regard
to compositional settings (discussed in the next
section), Hashimoto et al. (2014) use dependency-
based word embeddings by employing a hinge loss
objective, while Hermann and Blunsom (2013)
condition their objectives on the CCG types of the
involved words.
As we will see in Section 3, the current paper
offers an appealing alternative to those approaches
that does not depend on grammatical relations or
types of any form.
</bodyText>
<subsectionHeader confidence="0.996934">
2.3 Compositionality in distributional models
</subsectionHeader>
<bodyText confidence="0.999981674418605">
The methods that aim to equip distributional mod-
els of meaning with compositional abilities come
in many different levels of sophistication, from
simple element-wise vector operators such as ad-
dition and multiplication (Mitchell and Lapata,
2008) to category theory (Coecke et al., 2010).
In this latter work relational words (such as verbs
or adjectives) are represented as multi-linear maps
acting on vectors representing their arguments
(nouns and noun phrases). In general, the above
models are shallow in the sense that they do not
have functional parameters and the output is pro-
duced by the direct interaction of the inputs; yet
they have been shown to capture the compositional
meaning of sentences to an adequate degree.
The idea of using neural networks for compo-
sitionality in language appeared 25 years ago in
a seminal paper by Pollack (1990), and has been
recently re-popularized by Socher and colleagues
(Socher et al., 2011a; Socher et al., 2012). The
compositional architecture used in these works
is that of a recursive neural network (RecNN)
(Socher et al., 2011b), where the words get com-
posed by following a parse tree. A particular
variant of the RecNN is the recurrent neural net-
work (RNN), in which a sentence is assumed to
be generated by aggregating words in sequence
(Mikolov et al., 2010). Furthermore, some re-
cent work (Kalchbrenner et al., 2014) models the
meaning of sentences by utilizing the concept of a
convolutional neural network (LeCun et al., 1998),
the main characteristic of which is that it acts on
small overlapping parts of the input vectors. In all
the above models, the word embeddings and the
weights of the compositional layers are optimized
against a task-specific objective function.
In Section 3 we will show how to remove
the restriction of a supervised setting, introduc-
ing a generic objective that can be trained on any
general-purpose text corpus. While we focus on
recursive and recurrent neural network architec-
tures, the general ideas we will discuss are in prin-
ciple model-independent.
</bodyText>
<subsectionHeader confidence="0.997986">
2.4 Disambiguation in composition
</subsectionHeader>
<bodyText confidence="0.999971567567568">
Regardless of the way they address composition,
all the models of Section 2.3 rely on ambiguous
word spaces, in which every meaning of a poly-
semous word is merged into a single vector. Es-
pecially for cases of homonymy (such as ‘bank’,
‘organ’ and so on), where the same word is used
to describe two or more completely unrelated con-
cepts, this approach is problematic: the semantic
representation of the word becomes the average of
all senses, inadequate to express any of them in a
reliable way.
To address this problem, a prior disambiguation
step on the word vectors is often introduced, the
purpose of which is to find the word representa-
tions that best fit to the given context, before com-
position takes place (Reddy et al., 2011; Kartsak-
lis et al., 2013; Kartsaklis and Sadrzadeh, 2013;
Kartsaklis et al., 2014). This idea has been tested
on algebraic and tensor-based compositional func-
tions with very positive results. Furthermore, it
has been also found to provide minimal benefits
for a RecNN compositional architecture in a num-
ber of phrase and sentence similarity tasks (Cheng
et al., 2014). This latter work clearly suggests that
explicitly dealing with lexical ambiguity in a deep
compositional setting is an idea that is worth to be
further explored. While treating disambiguation
as only a preprocessing step is a strategy less than
optimal for a neural setting, one would expect that
the benefits should be greater for an architecture
in which the disambiguation takes place in a dy-
namic fashion during training.
We are now ready to start detailing a compo-
sitional model that takes into account the above
considerations. The issue of lexical ambiguity is
covered in Section 4; Section 3 below deals with
generic training and syntactic awareness.
</bodyText>
<sectionHeader confidence="0.960007" genericHeader="method">
3 Syntax-based generic training
</sectionHeader>
<bodyText confidence="0.999981857142857">
We propose a novel architecture for learning word
embeddings and a compositional model to use
them in a single step. The learning takes places
in the context of a RecNN (or an RNN), and both
word embeddings and parameters of the composi-
tional layer are optimized against a generic objec-
tive function that uses a hinge loss function.
</bodyText>
<page confidence="0.903557">
1533
</page>
<figure confidence="0.9511649">
output
input
(a) RecNN
output
input
�
SES
�
S//ES//
max(0, m − f(s) + f(s&apos;&apos;)) (3)
</figure>
<bodyText confidence="0.9994701">
where 5 is the set of sentences, f the composi-
tional layer, and m a margin we wish to retain
between the scores of the positive training ex-
amples and the negative ones. During training,
all parameters in the scoring layer, the composi-
tional layers and word representations are jointly
updated by error back-propagation. As output,
we get both general-purpose syntax-aware word
representations and weights for the corresponding
compositional model.
</bodyText>
<figure confidence="0.985142">
(b) RNN
</figure>
<figureCaption confidence="0.987053">
Figure 1: Recursive (a) and recurrent (b) neural
networks.
</figureCaption>
<bodyText confidence="0.636186">
Figure 1 shows the general form of recursive
and recurrent neural networks. In architectures of
this form, a compositional layer is applied on each
pair of inputs x1 and x2 in the following way:
</bodyText>
<equation confidence="0.994324">
p = g(Wx[1:2] + b) (1)
</equation>
<bodyText confidence="0.999827111111111">
where x[1:2] denotes the concatenation of the two
vectors, g is a non-linear function, and W, b are
the parameters of the model. In the RecNN case,
the compositional process continues recursively
by following a parse tree until a vector for the
whole sentence or phrase is produced; on the other
hand, an RNN assumes that a sentence is gener-
ated in a left-to-right fashion, taking into consider-
ation no dependencies other than word adjacency.
We amend the above setting by introducing a
novel layer on the top of the compositional one,
which scores the linguistic plausibility of the com-
posed sentence or phrase vector with regard to
both syntax and semantics. Following Collobert
and Weston (2008), we convert the unsupervised
learning problem to a supervised one by corrupt-
ing training sentences. Specifically, for each sen-
tence s we create two sets of negative examples.
In the first set, 5&apos;, the target word within a given
context is replaced by a random word; as in the
original C&amp;W paper, this set is used to enforce
semantic coherence in the word vectors. Syntac-
tic coherence is enforced by a second set of nega-
tive examples, 5&apos;&apos;, in which the words of the con-
text have been randomly shuffled. The objective
function is defined in terms of the following hinge
losses:
</bodyText>
<equation confidence="0.555683">
max(0, m − f(s) + f(s&apos;)) (2)
</equation>
<sectionHeader confidence="0.961756" genericHeader="method">
4 From words to senses
</sectionHeader>
<bodyText confidence="0.9999904">
We now extend our model to address lexical ambi-
guity. We achieve that by applying a gated archi-
tecture, similar to the one used in the multi-sense
model of Neelakantan et al. (2014), but advancing
the main idea to the compositional setting detailed
in Section 3.
We assume a fixed number of n senses per
word.1 Each word is associated with amain vector
(obtained for example by using an existing vector
set, or by simply applying the process of Section
3 in a separate step), as well as with n vectors de-
noting cluster centroids and an equal number of
sense vectors. Both cluster centroids and sense
vectors are randomly initialized in the beginning
of the process. For each word wt in a training sen-
tence, we prepare a context vector by averaging
the main vectors of all other words in the same
context. This context vector is compared with the
cluster centroids of wt by cosine similarity, and
the sense corresponding to the closest cluster is se-
lected as the most representative of wt in the cur-
rent context. The selected cluster centroid is up-
dated by the addition of the context vector, and the
associated sense vector is passed as input to the
compositional layer. The selected sense vectors
for each word in the sentence are updated by back-
propagation, based on the objectives of Equations
2 and 3. The overall architecture of our model, as
described in this and the previous section, is illus-
trated in Figure 2.
</bodyText>
<sectionHeader confidence="0.943227" genericHeader="method">
5 Task-specific dynamic disambiguation
</sectionHeader>
<bodyText confidence="0.8312883">
The model of Figure 2 decouples the training of
word vectors and compositional parameters from
1Note that in principle the fixed number of senses assump-
tion is not necessary; Neelakantan et al. (2014), for exam-
ple, present a version of their model in which new senses are
added dynamically when appropriate.
�
SES
�
S/ES/
</bodyText>
<page confidence="0.972033">
1534
</page>
<figureCaption confidence="0.9831115">
Figure 2: Training of syntax-aware multi-sense
embeddings in the context of a RecNN.
</figureCaption>
<bodyText confidence="0.999946518518518">
a specific task, and as a consequence from any
task-specific training dataset. However, note that
by replacing the plausibility layer with a classi-
fier trained for some task at hand, you get a task-
specific network that transparently trains multi-
sense word embeddings and applies dynamic dis-
ambiguation on the fly. While this idea of a single-
step direct training seems appealing, one consid-
eration is that the task-specific dataset used for the
training will not probably reflect the linguistic va-
riety that is required to exploit the expressiveness
of the setting in its full. Additionally, in many
cases the size of datasets tied to specific tasks is
prohibiting for training a deep architecture.
It is a merit of this proposal that, in cases like
these, it is possible for one to train the generic
model of Figure 2 on any large corpus of text, and
then use the produced word vectors and compo-
sitional weights to initialize the parameters of a
more specific version of the architecture. As a
result, the trained parameters will be further re-
fined according to the task-specific objective. Fig-
ure 3 illustrates the generic case of a composi-
tional framework applying dynamic disambigua-
tion. Note that here sense selection takes place by
a soft-max layer, which can be directly optimized
on the task objective.
</bodyText>
<sectionHeader confidence="0.9953285" genericHeader="method">
6 A siamese network for paraphrase
detection
</sectionHeader>
<bodyText confidence="0.999982548387097">
We will test the dynamic disambiguation frame-
work of Section 5 in a paraphrase detection task.
A paraphrase is a restatement of the meaning of a
sentence using different words and/or syntax. The
goal of a paraphrase detection model, thus, is to
examine two sentences and decide if they express
the same meaning.
While the usual way to approach this problem is
to utilize a classifier that acts (for example) on the
concatenation of the two sentence vectors, in this
work we follow a novel perspective: specifically,
we apply a siamese architecture (Bromley et al.,
1993), a concept that has been extensively used
in computer vision (Hadsell et al., 2006; Sun et
al., 2014). While siamese networks have been also
used in the past for NLP purposes (for example,
by Yih et al. (2011)), to the best of our knowledge
this is the first time that such a setting is applied
for paraphrase detection.
In our model, two networks sharing the same
parameters are used to compute the vectorial rep-
resentations of two sentences, the paraphrase rela-
tion of which we wish to detect; this is achieved by
employing a cost function that compares the two
vectors. There are two commonly used cost func-
tions: the first is based on the L2 norm (Hadsell
et al., 2006; Sun et al., 2014), while the second on
the cosine similarity (Nair and Hinton, 2010; Sun
et al., 2014). The L2 norm variation is capable of
handling differences in the magnitude of the vec-
tors. Formally, the cost function is defined as:
</bodyText>
<figure confidence="0.879660272727273">
�1 2 kf(s1) − f(s2)k2 2 , if y = 1
Ef = 1
classifier
sentence vector
compositional layer(s)
selected sense
vectors
soft-max layer
sense vectors
main (ambiguous)
vectors
</figure>
<figureCaption confidence="0.9980755">
Figure 3: Dynamic disambiguation in a generic
compositional deep net.
</figureCaption>
<figure confidence="0.992380916666667">
plausibility layer
sentence vector
compositional layer
plausibility layer
phrase vector
compositional
layer
gate
sense vectors
main
(ambiguous)
vectors
</figure>
<bodyText confidence="0.8306568">
where s1, s2 are the input sentences, f the com-
positional layer (so f(s1) and f(s2) refer to sen-
tence vectors), and y = 1 denotes a paraphrase re-
lationship between the sentences; m stands for the
margin, a hyper-parameter chosen in advance. On
</bodyText>
<equation confidence="0.5872225">
max(0, m − kf (s1) − f(s2)k2 )2
2 , O.W.
</equation>
<page confidence="0.919223">
1535
</page>
<figureCaption confidence="0.98199">
Figure 4: A siamese network for paraphrase detec-
tion.
</figureCaption>
<bodyText confidence="0.98574075">
the other hand, the cost function based on cosine
similarity handles only directional differences, as
follows:
where d = f(s1)·f(s2) is the cosine similar-
</bodyText>
<equation confidence="0.468731">
11f(s1)11211f(s2)112
</equation>
<bodyText confidence="0.999962153846154">
ity of the two sentence vectors, w and b are the
scaling and shifting parameters to be optimized, σ
is the sigmoid function and y is the label. In the
experiments that will follow in Section 7.4, both
of these cost functions are evaluated. The overall
architecture is shown in Figure 4.
In Section 7.4 we will use the pre-trained vec-
tors and compositional weights for deriving sen-
tence representations that will be subsequently fed
to the siamese network. When the dynamic disam-
biguation framework is used, the sense vectors of
the words are updated during training so that the
sense selection process is gradually refined.
</bodyText>
<sectionHeader confidence="0.999447" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99997975">
We evaluate the quality of the compositional
word vectors and the proposed deep compositional
framework in the tasks of word similarity and
paraphrase detection, respectively.
</bodyText>
<subsectionHeader confidence="0.992856">
7.1 Model pre-training
</subsectionHeader>
<bodyText confidence="0.999895576923077">
In all experiments the word representations and
compositional models are pre-trained on the
British National Corpus (BNC), a general-purpose
text corpus that contains 6 million sentences of
written and spoken English. For comparison we
train two sets of word vectors and compositional
models, one ambiguous and one multi-sense (fix-
ing 3 senses per word). The dimension of the em-
beddings is set to 300.
As our compositional architectures we use a
RecNN and an RNN. In the RecNN case, the
words are composed by following the result of an
external parser, while for the RNN the composi-
tion takes place in sequence from left to right. To
avoid the exploding or vanishing gradient problem
(Bengio et al., 1994) for long sentences, we em-
ploy a long short-term memory (LSTM) network
(Hochreiter and Schmidhuber, 1997). During the
training of each model, we minimize the hinge loss
in Equations 2 and 3. The plausibility layer is im-
plemented as a 2-layer network, with 150 units at
the hidden layer, and is applied at each individ-
ual node (as opposed to a single application at the
sentence level). All parameters are updated with
mini-batches by AdaDelta (Zeiler, 2012) gradient
descent method (A = 0.03, initial α = 0.05).
</bodyText>
<subsectionHeader confidence="0.991363">
7.2 Qualitative evaluation of the word vectors
</subsectionHeader>
<bodyText confidence="0.99985884375">
As a first step, we qualitatively evaluate the trained
word embeddings by examining the nearest neigh-
bours lists of a few selected words. We com-
pare the results with those produced by the skip-
gram model (SG) of Mikolov et al. (2013) and
the language model (CW) of Collobert and Weston
(2008). We refer to our model as SAMS (Syntax-
Aware Multi-Sense). The results in Table 1 show
clearly that our model tends to group words that
are both semantically and syntactically related; for
example, and in contrast with the compared mod-
els which group words only at the semantic level,
our model is able to retain tenses, numbers (singu-
lars and plurals), and gerunds.
The observed behaviour is comparable to that of
embedding models with objective functions con-
ditioned on grammatical relations between words;
Levy and Goldberg (2014), for example, present a
similar table for their dependency-based extension
of the skip-gram model. The advantage of our ap-
proach against such models is twofold: firstly, the
word embeddings are accompanied by a generic
compositional model that can be used for creat-
ing sentence representations independently of any
specific task; and secondly, the training is quite
forgiving to data sparsity problems that in gen-
eral a dependency-based approach would intensify
(since context words are paired with the grammati-
cal relations they occur with the target word). As a
result, a small corpus such as the BNC is sufficient
for producing high quality syntax-aware word em-
beddings.
</bodyText>
<figure confidence="0.830962">
deep
compositional
layer(s)
deep
compositional
layer(s)
2
1(y − σ(wd + b))2 (4)
Ef =
</figure>
<page confidence="0.968477">
1536
</page>
<table confidence="0.936373363636363">
SG CW SAMS
begged beg, begging, cried begging, pretended, beg persuaded, asked, cried
refused refusing, refuses, refusal refusing , declined, refuse declined, rejected, denied
interrupted interrupting, punctuated, interrupts, interrupt, interrupting punctuated, preceded, disrupted
themes interrupt theme, concepts, subtext meanings, concepts, ideas
patiently thematic, theme, notions impatiently, queue, expectantly impatiently, siliently, anxiously
player impatiently, waited, waits game, club, team athlete, sportsman, team
prompting players, football, league prompt, amid, triggered sparking, triggering, forcing
reproduce prompted, prompt, sparking reproducing, thrive, survive replicate, produce, repopulate
predictions reproducing, replicate, humans predicting, assumption, expectations, projections,
prediction, predict, forecasts predicted forecasts
</table>
<tableCaption confidence="0.999752">
Table 1: Nearest neighbours for a number of words with various embedding models.
</tableCaption>
<subsectionHeader confidence="0.993955">
7.3 Word similarity
</subsectionHeader>
<bodyText confidence="0.999979173913043">
We now proceed to a quantitative evaluation of
our embeddings on the Stanford Contextual Word
Similarity (SCWS) dataset of Huang et al. (2012).
The dataset contains 2,003 pairs of words and the
contexts they occur in. We can therefore make
use of the contextual information in order to select
the most appropriate sense for each ambiguous
word. Similarly to Neelakantan et al. (2014), we
use three different metrics: globalSim measures
the similarity between two ambiguous word vec-
tors; localSim selects a single sense for each word
based on the context and computes the similarity
between the two sense vectors; avgSim represents
each word as a weighted average of all senses in
the given context and computes the similarity be-
tween the two weighted sense vectors.
We compute and report the Spearman’s corre-
lation between the embedding similarities and hu-
man judgments (Table 2). In addition to the skip-
gram and Collobert and Weston models, we also
compare against the CBOW model (Mikolov et
al., 2013) and the multi-sense skip-gram (MSSG)
model of Neelakantan et al. (2014).
</bodyText>
<table confidence="0.999556166666667">
Model globalSim localSim avgSim
CBOW 59.5 – –
SG 61.8 – –
CW 55.3 – –
MSSG 61.3 56.7 62.1
SAMS 59.9 58.5 62.5
</table>
<tableCaption confidence="0.8796145">
Table 2: Results for the word similarity task
(Spearman’s ρ × 100).
</tableCaption>
<bodyText confidence="0.999543363636364">
Among all methods, only the MSSG model and
ours are capable of learning multi-prototype word
representations. Our embeddings show top per-
formance for localSim and avgSim measures, and
performance competitive to that of MSSG and SG
for globalSim, both of which use a hierarchical
soft-max as their objective function. Compared to
the original C&amp;W model, our version presents an
improvement of 4.6%—a clear indication for the
effectiveness of the proposed learning method and
the enhanced objective.
</bodyText>
<subsectionHeader confidence="0.988081">
7.4 Paraphrase detection
</subsectionHeader>
<bodyText confidence="0.999965424242424">
In the last set of experiments, the proposed com-
positional distributional framework is evaluated
on the Microsoft Research Paraphrase Corpus
(MSRPC) (Dolan and Brockett, 2005), which con-
tains 5,800 pairs of sentences. This is a binary
classification task, with labels provided by human
annotators. We apply the siamese network detailed
in Section 6.
While MSRPC is one of the most used datasets
for evaluating paraphrase detection models, its
size is prohibitory for any attempt of training a
deep architecture. Therefore, for our training
we rely on a much larger external dataset, the
Paraphrase Database (PPDB) (Ganitkevitch et al.,
2013). The PPDB contains more than 220 million
paraphrase pairs, of which 73 million are phrasal
paraphrases and 140 million are paraphrase pat-
terns that capture syntactic transformations of sen-
tences. We use these phrase- and sentence-level
paraphrase pairs as additional training contexts
to fine-tune the generic compositional model pa-
rameters and word embeddings and to train the
baseline models. The original training set of the
MSRPC is used as validation set for deciding hy-
perparameters, such as the margin of the error
function and the number of training epochs.
The evaluations were conducted on various as-
pects, and the models are gradually refined to
demonstrate performance within the state-of-the-
art range.
Comparison of the two error functions In the
first evaluation, we compare the two error func-
tions of the siamese network using only ambigu-
</bodyText>
<page confidence="0.98699">
1537
</page>
<bodyText confidence="0.998816857142857">
ous vectors. As we can see in Table 3, the co-
sine error function consistently outperforms the
L2 norm-based one for both compositional mod-
els, providing a yet another confirmation of the
already well-established fact that similarity in se-
mantic vector spaces is better reflected by length-
invariant measures.
</bodyText>
<table confidence="0.928296666666667">
Model L2 Cosine
RecNN 73.8 74.9
RNN 73.0 74.3
</table>
<tableCaption confidence="0.804135">
Table 3: Results with different error functions for
the paraphrase detection task (accuracy × 100).
</tableCaption>
<bodyText confidence="0.998461076923077">
Effectiveness of disambiguation We now pro-
ceed to compare the effectiveness of the two com-
positional models when using ambiguous vectors
and multi-sense vectors, respectively. Our error
function is set to cosine similarity, following the
results of the previous evaluation. When dynamic
disambiguation is applied, we test two methods of
selecting sense vectors: in the hard case the vector
of the most plausible sense is selected, while in the
soft case a new vector is prepared as the weighted
average of all sense vectors according to proba-
bilities returned by the soft-max layer (see Figure
3). As a baseline we use a simple compositional
model based on vector addition.
The dynamic disambiguation models and the
additive baseline are compared with variations that
use a simple prior disambiguation step applied on
the word vectors. This is achieved by first se-
lecting for each word the sense vector that is the
closest to the average of all other word vectors
in the same sentence, and then composing the se-
lected sense vectors without further considerations
regarding ambiguity. The baseline model and the
prior disambiguation variants are trained as sepa-
rate logistic regression classifiers. The results are
shown in Table 4.
</bodyText>
<table confidence="0.99465075">
Model Ambig. Prior Hard DD Soft DD
Addition 69.9 71.3 – –
RecNN 74.9 75.3 75.7 76.0
RNN 74.3 74.6 75.1 75.2
</table>
<tableCaption confidence="0.982301">
Table 4: Different disambiguation choices for the
</tableCaption>
<bodyText confidence="0.995211589285715">
paraphrase detection task (accuracy × 100).
Overall, disambiguated vectors work better than
the ambiguous ones, with the improvement to be
more significant for the additive model; there, a
simple prior disambiguation step produces 1.4%
gains. For the deep compositional models, simple
prior disambiguation is still helpful with small im-
provements, a result which is consistent with the
findings of Cheng et al. (2014). The small gains
of the prior disambiguation models over the am-
biguous models clearly show that deep architec-
tures are quite capable of performing this elemen-
tary form of sense selection intrinsically, as part
of the learning process itself. However, the situ-
ation changes when the dynamic disambiguation
framework is used, where the gains over the am-
biguous version become more significant. Com-
paring the two ways of dynamic disambiguation
(hard method and soft method), the numbers that
the soft method gives are slightly higher, produc-
ing a total gain of 1.1% over the ambiguous ver-
sion for the RecNN case.2
Note that, at this stage, the advantage of us-
ing the dynamic disambiguation framework over
simple prior disambiguation is still small (0.7%
for the case of RecNN). We seek the reason be-
hind this in the recursive nature of our architecture,
which tends to progressively “hide” local features
of word vectors, thus diminishing the effect of the
fine-tuned sense vectors produced by the dynamic
disambiguation mechanism. The next section dis-
cusses the problem and provides a solution.
The role of pooling One of the problems of the
recursive and recurrent compositional architec-
tures, especially in grammars with strict branching
structure such as in English, is that any given com-
position is usually the product of a terminal and a
non-terminal; i.e. a single word can contribute to
the meaning of a sentence to the same extent as the
rest of a sentence on its whole, as below:
[[kids]NP [play ball games in the park]VP]S
In the above case, the contribution of the words
within the verb phrase to the final sentence rep-
resentation will be faded out due to the recursive
composition mechanism. Inspired by related work
in computer vision (Sun et al., 2014), we attempt
to alleviate this problem by introducing an aver-
age pooling layer at the sense vector level and
adding the resulting vector to the sentence repre-
sentation. By doing this we expect that the new
sentence vector will reflect local features from all
words in the sentence that can help in the clas-
sification in a more direct way. The results for
the new deep architectures are shown in Table 5,
where we see substantial improvements for both
deep nets. More importantly, the effect of dynamic
</bodyText>
<footnote confidence="0.921926">
2For all subsequent experiments, the reported results are
based on the soft selection method.
</footnote>
<page confidence="0.994316">
1538
</page>
<bodyText confidence="0.998909071428572">
disambiguation now becomes more significant, as
expected by our analysis.
Table 5 also includes results for two models
trained in a single step, with word and sense vec-
tors randomly initialized at the beginning of the
process. We see that, despite the large size of the
training set, the results are much lower than the
ones obtained when using the pre-training step.
This demonstrates the importance of the initial
training on a general-purpose corpus: the result-
ing vectors reflect linguistic information that, al-
though not obtainable from the task-specific train-
ing, can make great difference in the result of the
classification.
</bodyText>
<table confidence="0.9991704">
Model Ambig. Prior Dynamic
RecNN+pooling 75.5 76.3 77.6
RNN+pooling 74.8 75.9 76.6
1-step RecNN+pooling 74.4 – 72.9
1-step RNN+pooling 73.6 – 73.1
</table>
<tableCaption confidence="0.996731">
Table 5: Results with average pooling for the para-
</tableCaption>
<bodyText confidence="0.97761215625">
phrase detection task (accuracy × 100).
Cross-model comparison In this section we
propose a method to further improve the perfor-
mance of our models, and we present an evaluation
against some of the previously reported results.
We notice that using distributional properties
alone cannot capture efficiently subtle aspects of a
sentence, for example numbers or human names.
However, even small differences on those aspects
between two sentences can lead to a different clas-
sification result. Therefore, we train (using the
MSPRC training data) an additional logistic re-
gression classifier which is based not only on the
embeddings similarity, but also on a few hand-
engineered features. We then ensemble the new
classifier (C1) with the original one. In terms of
feature selection, we follow Socher et al. (2011a)
and Blacoe and Lapata (2012) and add the fol-
lowing features: the difference in sentence length,
the unigram overlap among the two sentences, fea-
tures related to numbers (including the presence or
absence of numbers from a sentence and whether
or not the numbers in the two sentences are the
same). In Table 6 we report results of the original
model and the ensembled model, and we compare
with the performance of other existing models.
In all of the implemented models (including the
additive baseline), disambiguation is performed to
guarantee the best performance. We see that by
ensembling the original classifier with C1, we im-
prove the result of the previous section by another
1%. This is the second best result reported so far
</bodyText>
<table confidence="0.958612333333333">
BL Model Acc. F1
All positive 66.5 79.9
Addition (disamb.) 71.3 81.1
Dynamic Dis. RecNN 76.0 84.0
RecNN+Pooling 77.6 84.7
RecNN+Pooling+C1 78.6 85.3
RNN 75.2 83.6
RNN+Pooling 76.6 84.3
RNN+Pooling+C1 77.5 84.6
</table>
<note confidence="0.9542598">
Published results Mihalcea et al. (2006) 70.3 81.3
Rus et al. (2008) 70.6 80.5
Qiu et al. (2006) 72.0 81.6
Islam and Inkpen (2009) 72.6 81.3
Fernando and Stevenson (2008) 74.1 82.4
Wan et al. (2006) 75.6 83.0
Das and Smith (2009) 76.1 82.7
Socher et al. (2011a) 76.8 83.6
Madnani et al. (2012) 77.4 84.1
Ji and Eisenstein (2013) 80.4 85.9
</note>
<tableCaption confidence="0.772701">
Table 6: Cross-model comparison in the para-
</tableCaption>
<bodyText confidence="0.911437666666667">
phrase detection task.
for the specific task, with a 0.6 difference in F-
score from the first (Ji and Eisenstein, 2013).3
</bodyText>
<sectionHeader confidence="0.90831" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999988705882353">
The main contribution of this paper is a deep com-
positional distributional model acting on linguis-
tically motivated word embeddings.4 The effec-
tiveness of the syntax-aware, multi-sense word
vectors and the dynamic compositional disam-
biguation framework in which they are used was
demonstrated by appropriate tasks at the lexical
and sentence level, respectively, with very posi-
tive results. As an aside, we also demonstrated the
benefits of a siamese architecture in the context of
a paraphrase detection task. While the architec-
tures tested in this work were limited to a RecNN
and an RNN, the ideas we presented are in prin-
ciple directly applicable to any kind of deep net-
work. As a future step, we aim to test the proposed
models on a convolutional compositional architec-
ture, similar to that of Kalchbrenner et al. (2014).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999211571428571">
The authors would like to thank the three anony-
mous reviewers for their useful comments, as well
as Nal Kalchbrenner and Ed Grefenstette for early
discussions and suggestions on the paper, and Si-
mon ˇSuster for comments on the final draft. Dim-
itri Kartsaklis gratefully acknowledges financial
support by AFOSR.
</bodyText>
<footnote confidence="0.9955965">
3Source: ACL Wiki (http://www.aclweb.org/acl-
wiki), August 2015.
4Code in Python/Theano and the word embeddings can be
found at https://github.com/cheng6076.
</footnote>
<page confidence="0.996295">
1539
</page>
<sectionHeader confidence="0.990043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998375345794392">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.
Jane Bromley, James W Bentz, L´eon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature
verification using a siamese time delay neural net-
work. International Journal of Pattern Recognition
and Artificial Intelligence, 7(04):669–688.
Jianpeng Cheng, Dimitri Kartsaklis, and Edward
Grefenstette. 2014. Investigating the role of
prior disambiguation in deep-learning composi-
tional models of meaning. In 2nd Workshop of
Learning Semantics, NIPS 2014, Montreal, Canada,
December.
B Coecke, M Sadrzadeh, and S Clark. 2010. Math-
ematical foundations for a distributional composi-
tional model of meaning. lambek festschrift. Lin-
guistic Analysis, 36:345–384.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Dipanjan Das and Noah A Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 468–476. Association for Computational
Linguistics.
W.B. Dolan and C. Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Third International Workshop on Paraphrasing
(IWP2005).
Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection.
In Proceedings of the 11th Annual Research Collo-
quium of the UK Special Interest Group for Compu-
tational Linguistics, pages 45–52. Citeseer.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In HLT-NAACL, pages 758–764.
Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006.
Dimensionality reduction by learning an invariant
mapping. In Computer vision and pattern recog-
nition, 2006 IEEE computer society conference on,
volume 2, pages 1735–1742. IEEE.
Zellig S Harris. 1954. Distributional structure. Word.
Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2014. Jointly learn-
ing word representations and composition functions
using predicate-argument structures. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1544–1555, Doha, Qatar, October. Association for
Computational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 894–904,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2009. Semantic sim-
ilarity of short texts. Recent Advances in Natural
Language Processing V, 309:227–236.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimi-
native improvements to distributional sentence sim-
ilarity. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 891–896, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
</reference>
<page confidence="0.791074">
1540
</page>
<reference confidence="0.998882648648648">
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1590–1601, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In Pro-
ceedings of 17th Conference on Natural Language
Learning (CoNLL), pages 114–123, Sofia, Bulgaria,
August.
Dimitri Kartsaklis, Nal Kalchbrenner, and Mehrnoosh
Sadrzadeh. 2014. Resolving lexical ambiguity in
tensor regression models of meaning. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Vol. 2: Short Pa-
pers), pages 212–217, Baltimore, USA, June. Asso-
ciation for Computational Linguistics.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
302–308, Baltimore, Maryland, June. Association
for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182–190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 708–719, Doha, Qatar,
October. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems, pages 2265–2273.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807–814.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
S. Pad´o and M. Lapata. 2007. Dependency-based
Construction of Semantic Space Models. Compu-
tational Linguistics, 33(2):161–199.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 18–26. Association for Compu-
tational Linguistics.
Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In IJC-
NLP, pages 705–713.
Vasile Rus, Philip M McCarthy, Mihai C Lintean,
Danielle S McNamara, and Arthur C Graesser.
2008. Paraphrase identification with lexico-
syntactic graph subsumption. In FLAIRS confer-
ence, pages 201–206.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 129–136.
</reference>
<page confidence="0.849289">
1541
</page>
<reference confidence="0.999391043478261">
R. Socher, B. Huval, C. Manning, and Ng. A.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Conference on Empirical
Methods in Natural Language Processing 2012.
Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou
Tang. 2014. Deep learning face representation by
joint identification-verification. In Advances in Neu-
ral Information Processing Systems, pages 1988–
1996.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2006. Using dependency-based features
to take the para-farce out of paraphrase. In Pro-
ceedings of the Australasian Language Technology
Workshop, volume 2006.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247–256, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.
</reference>
<page confidence="0.995527">
1542
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798563">
<title confidence="0.9975855">Syntax-Aware Multi-Sense Word for Deep Compositional Models of Meaning</title>
<author confidence="0.903478">Jianpeng</author>
<affiliation confidence="0.969548666666667">University of Department Computer</affiliation>
<email confidence="0.974769">jianpeng.cheng@stcatz.oxon.org</email>
<abstract confidence="0.999366416666667">Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<contexts>
<context position="8167" citStr="Baroni et al., 2014" startWordPosition="1291" endWordPosition="1294">y optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014). 2.2 Syntactic awareness Since the main purpose of distributional models until now was to measure the semantic relatedness of words, relatively little effort has been put into making word vectors aware of information regarding the syntactic role under which a word occurs in a sentence. In some cases the vectors are POStag specific, so that ‘book’ as noun and ‘book’ as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="23345" citStr="Bengio et al., 1994" startWordPosition="3826" endWordPosition="3829">on the British National Corpus (BNC), a general-purpose text corpus that contains 6 million sentences of written and spoken English. For comparison we train two sets of word vectors and compositional models, one ambiguous and one multi-sense (fixing 3 senses per word). The dimension of the embeddings is set to 300. As our compositional architectures we use a RecNN and an RNN. In the RecNN case, the words are composed by following the result of an external parser, while for the RNN the composition takes place in sequence from left to right. To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). During the training of each model, we minimize the hinge loss in Equations 2 and 3. The plausibility layer is implemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (A = 0.03, initial α = 0.05). 7.2 Qualitative evaluation of the word vectors As a first step, we qualitatively evaluat</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="7605" citStr="Bengio et al., 2003" startWordPosition="1203" endWordPosition="1206">e distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text. In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in composition</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36028" citStr="Blacoe and Lapata (2012)" startWordPosition="5842" endWordPosition="5845">d results. We notice that using distributional properties alone cannot capture efficiently subtle aspects of a sentence, for example numbers or human names. However, even small differences on those aspects between two sentences can lead to a different classification result. Therefore, we train (using the MSPRC training data) an additional logistic regression classifier which is based not only on the embeddings similarity, but also on a few handengineered features. We then ensemble the new classifier (C1) with the original one. In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same). In Table 6 we report results of the original model and the ensembled model, and we compare with the performance of other existing models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Bromley</author>
<author>James W Bentz</author>
<author>L´eon Bottou</author>
<author>Isabelle Guyon</author>
<author>Yann LeCun</author>
<author>Cliff Moore</author>
<author>Eduard S¨ackinger</author>
<author>Roopak Shah</author>
</authors>
<title>Signature verification using a siamese time delay neural network.</title>
<date>1993</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>7</volume>
<issue>04</issue>
<marker>Bromley, Bentz, Bottou, Guyon, LeCun, Moore, S¨ackinger, Shah, 1993</marker>
<rawString>Jane Bromley, James W Bentz, L´eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard S¨ackinger, and Roopak Shah. 1993. Signature verification using a siamese time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence, 7(04):669–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianpeng Cheng</author>
<author>Dimitri Kartsaklis</author>
<author>Edward Grefenstette</author>
</authors>
<title>Investigating the role of prior disambiguation in deep-learning compositional models of meaning.</title>
<date>2014</date>
<booktitle>In 2nd Workshop of Learning Semantics, NIPS 2014,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="3400" citStr="Cheng et al. (2014)" startWordPosition="515" endWordPosition="518">s many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional process. In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their “ambiguous” counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results. Furthermore, a second important question relates to the very nature of the word embeddings used in the context of a compositional model. In a setting of this form, word vectors are not any more just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole—a task in which syntax, along with semantics, also plays a very important role. It is a central point of this paper, therefore, that in a compositional distributional model of meaning word vectors should be injected with inform</context>
<context position="13012" citStr="Cheng et al., 2014" startWordPosition="2076" endWordPosition="2079">reliable way. To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting, one would expect that the benefits should be greater for an architecture in which the disambiguation takes place in a dynamic fashion during training. We are now ready to start detailing a compositional model that takes into account the above considerations. The issue of lexical ambiguity is covered in Section 4; Section </context>
<context position="31992" citStr="Cheng et al. (2014)" startWordPosition="5180" endWordPosition="5183">ssifiers. The results are shown in Table 4. Model Ambig. Prior Hard DD Soft DD Addition 69.9 71.3 – – RecNN 74.9 75.3 75.7 76.0 RNN 74.3 74.6 75.1 75.2 Table 4: Different disambiguation choices for the paraphrase detection task (accuracy × 100). Overall, disambiguated vectors work better than the ambiguous ones, with the improvement to be more significant for the additive model; there, a simple prior disambiguation step produces 1.4% gains. For the deep compositional models, simple prior disambiguation is still helpful with small improvements, a result which is consistent with the findings of Cheng et al. (2014). The small gains of the prior disambiguation models over the ambiguous models clearly show that deep architectures are quite capable of performing this elementary form of sense selection intrinsically, as part of the learning process itself. However, the situation changes when the dynamic disambiguation framework is used, where the gains over the ambiguous version become more significant. Comparing the two ways of dynamic disambiguation (hard method and soft method), the numbers that the soft method gives are slightly higher, producing a total gain of 1.1% over the ambiguous version for the R</context>
</contexts>
<marker>Cheng, Kartsaklis, Grefenstette, 2014</marker>
<rawString>Jianpeng Cheng, Dimitri Kartsaklis, and Edward Grefenstette. 2014. Investigating the role of prior disambiguation in deep-learning compositional models of meaning. In 2nd Workshop of Learning Semantics, NIPS 2014, Montreal, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical foundations for a distributional compositional model of meaning. lambek festschrift. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="10119" citStr="Coecke et al., 2010" startWordPosition="1596" endWordPosition="1599">nge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words. As we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form. 2.3 Compositionality in distributional models The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010). In this latter work relational words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree. The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popul</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B Coecke, M Sadrzadeh, and S Clark. 2010. Mathematical foundations for a distributional compositional model of meaning. lambek festschrift. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1752" citStr="Collobert and Weston, 2008" startWordPosition="263" endWordPosition="266">ion Representing the meaning of words by using their distributional behaviour in a large text corpus is a well-established technique in NLP research that has been proved useful in numerous tasks. In a distributional model of meaning, the semantic representation of a word is given as a vector in some high dimensional vector space, obtained either by explicitly collecting co-occurrence statistics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013). Regardless their method of construction, distributional models of meaning do not scale up to Dimitri Kartsaklis Queen Mary University of London School of Electronic Engineering and Computer Science d.kartsaklis@qmul.ac.uk larger text constituents such as phrases or sentences, since the uniqueness of multi-word expressions would inevitably lead to data sparsity problems, thus to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation for a phrase or</context>
<context position="5016" citStr="Collobert and Weston (2008)" startWordPosition="778" endWordPosition="782">he compositional process itself and the quality of the word embeddings used therein. We propose an architecture for jointly training a compositional model and a set of word embeddings, in a way that imposes dynamic word sense induction for each word during the learning process. Note that this is in contrast with recent work in multi-sense neural word embeddings (Neelakantan et al., 2014), in which the word senses are learned without any compositional considerations in mind. Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of Collobert and Weston (2008), in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context. A qualitative analysis shows that our vectors reflect both semantic and syntactic features in a concise way. In all current deep compositional distributional settings, the word embeddings are internal parameters of the model with no use for any other purpose than the task for which they were specifically trained. In this work, one of our main considerations is that the joint training step should be generic enough to not be tied in any partic</context>
<context position="7633" citStr="Collobert and Weston, 2008" startWordPosition="1207" endWordPosition="1211">thesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text. In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al.</context>
<context position="15556" citStr="Collobert and Weston (2008)" startWordPosition="2501" endWordPosition="2504">inear function, and W, b are the parameters of the model. In the RecNN case, the compositional process continues recursively by following a parse tree until a vector for the whole sentence or phrase is produced; on the other hand, an RNN assumes that a sentence is generated in a left-to-right fashion, taking into consideration no dependencies other than word adjacency. We amend the above setting by introducing a novel layer on the top of the compositional one, which scores the linguistic plausibility of the composed sentence or phrase vector with regard to both syntax and semantics. Following Collobert and Weston (2008), we convert the unsupervised learning problem to a supervised one by corrupting training sentences. Specifically, for each sentence s we create two sets of negative examples. In the first set, 5&apos;, the target word within a given context is replaced by a random word; as in the original C&amp;W paper, this set is used to enforce semantic coherence in the word vectors. Syntactic coherence is enforced by a second set of negative examples, 5&apos;&apos;, in which the words of the context have been randomly shuffled. The objective function is defined in terms of the following hinge losses: max(0, m − f(s) + f(s&apos;)</context>
<context position="24195" citStr="Collobert and Weston (2008)" startWordPosition="3972" endWordPosition="3975">ented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (A = 0.03, initial α = 0.05). 7.2 Qualitative evaluation of the word vectors As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words. We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared models which group words only at the semantic level, our model is able to retain tenses, numbers (singulars and plurals), and gerunds. The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for example, present a similar table for their dependency-based e</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>468--476</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37173" citStr="Das and Smith (2009)" startWordPosition="6035" endWordPosition="6038"> performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambiguation framework in which they are used was demonstrated by a</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 468–476. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<contexts>
<context position="6619" citStr="Dolan and Brockett, 2005" startWordPosition="1047" endWordPosition="1050">ontextual Word Similarity (SCWS) dataset of Huang et al. (2012) competitive to and even better of the performance of well-established neural word embeddings sets. Finally, we propose a dynamic disambiguation framework for a number of existing deep compositional models of meaning, in which the multisense word embeddings and the compositional model of the original training step are further refined according to the purposes of a specific task at hand. In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005). An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of classifiers, we approach the problem by following a siamese architecture (Bromley et al., 1993). 2 Background and related work 2.1 Distributional models of meaning Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a ve</context>
<context position="28406" citStr="Dolan and Brockett, 2005" startWordPosition="4607" endWordPosition="4610">ulti-prototype word representations. Our embeddings show top performance for localSim and avgSim measures, and performance competitive to that of MSSG and SG for globalSim, both of which use a hierarchical soft-max as their objective function. Compared to the original C&amp;W model, our version presents an improvement of 4.6%—a clear indication for the effectiveness of the proposed learning method and the enhanced objective. 7.4 Paraphrase detection In the last set of experiments, the proposed compositional distributional framework is evaluated on the Microsoft Research Paraphrase Corpus (MSRPC) (Dolan and Brockett, 2005), which contains 5,800 pairs of sentences. This is a binary classification task, with labels provided by human annotators. We apply the siamese network detailed in Section 6. While MSRPC is one of the most used datasets for evaluating paraphrase detection models, its size is prohibitory for any attempt of training a deep architecture. Therefore, for our training we rely on a much larger external dataset, the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). The PPDB contains more than 220 million paraphrase pairs, of which 73 million are phrasal paraphrases and 140 million are paraphrase</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>W.B. Dolan and C. Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics,</booktitle>
<pages>45--52</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="37114" citStr="Fernando and Stevenson (2008)" startWordPosition="6023" endWordPosition="6026">dditive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambigua</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. In Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics, pages 45–52. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Ppdb: The paraphrase database.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>758--764</pages>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In HLT-NAACL, pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raia Hadsell</author>
<author>Sumit Chopra</author>
<author>Yann LeCun</author>
</authors>
<title>Dimensionality reduction by learning an invariant mapping. In Computer vision and pattern recognition,</title>
<date>2006</date>
<booktitle>IEEE computer society conference on,</booktitle>
<volume>2</volume>
<pages>1735--1742</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="20064" citStr="Hadsell et al., 2006" startWordPosition="3276" endWordPosition="3279">uation framework of Section 5 in a paraphrase detection task. A paraphrase is a restatement of the meaning of a sentence using different words and/or syntax. The goal of a paraphrase detection model, thus, is to examine two sentences and decide if they express the same meaning. While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection. In our model, two networks sharing the same parameters are used to compute the vectorial representations of two sentences, the paraphrase relation of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et a</context>
</contexts>
<marker>Hadsell, Chopra, LeCun, 2006</marker>
<rawString>Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735–1742. IEEE.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Distributional structure. Word. Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, and Yoshimasa Tsuruoka.</title>
<date>1954</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1544--1555</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="7028" citStr="Harris, 1954" startWordPosition="1111" endWordPosition="1112">ses of a specific task at hand. In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005). An interesting aspect at the sideline of the paraphrase detection experiment is that, in contrast to mainstream approaches that mainly rely on simple forms of classifiers, we approach the problem by following a siamese architecture (Bromley et al., 1993). 2 Background and related work 2.1 Distributional models of meaning Distributional models of meaning follow the distributional hypothesis (Harris, 1954), which states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text. In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston,</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word. Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, and Yoshimasa Tsuruoka. 2014. Jointly learning word representations and composition functions using predicate-argument structures. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544–1555, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>894--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9551" citStr="Hermann and Blunsom (2013)" startWordPosition="1508" endWordPosition="1511">. For word embeddings trained in neural settings, syntactic information is not usually taken explicitly into account, with some notable exceptions. At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model 1532 based on grammatical dependencies. Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word. With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words. As we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form. 2.3 Compositionality in distributional models The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010). In this latter work relational</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 894–904, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="23450" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="3842" endWordPosition="3845">sentences of written and spoken English. For comparison we train two sets of word vectors and compositional models, one ambiguous and one multi-sense (fixing 3 senses per word). The dimension of the embeddings is set to 300. As our compositional architectures we use a RecNN and an RNN. In the RecNN case, the words are composed by following the result of an external parser, while for the RNN the composition takes place in sequence from left to right. To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). During the training of each model, we minimize the hinge loss in Equations 2 and 3. The plausibility layer is implemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (A = 0.03, initial α = 0.05). 7.2 Qualitative evaluation of the word vectors As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words. We compa</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6057" citStr="Huang et al. (2012)" startWordPosition="957" endWordPosition="960"> task for which they were specifically trained. In this work, one of our main considerations is that the joint training step should be generic enough to not be tied in any particular task. In this way the word embeddings and the derived compositional model can be learned on data much more diverse than any task-specific dataset, reflecting a wider range of linguistic features. Indeed, experimental evaluation shows that the produced word embeddings can serve as a high quality general-purpose semantic word space, presenting performance on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012) competitive to and even better of the performance of well-established neural word embeddings sets. Finally, we propose a dynamic disambiguation framework for a number of existing deep compositional models of meaning, in which the multisense word embeddings and the compositional model of the original training step are further refined according to the purposes of a specific task at hand. In the context of paraphrase detection, we achieve a result very close to the current state-ofthe-art on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005). An interesting aspect at the sidelin</context>
<context position="26595" citStr="Huang et al. (2012)" startWordPosition="4314" endWordPosition="4317">s game, club, team athlete, sportsman, team prompting players, football, league prompt, amid, triggered sparking, triggering, forcing reproduce prompted, prompt, sparking reproducing, thrive, survive replicate, produce, repopulate predictions reproducing, replicate, humans predicting, assumption, expectations, projections, prediction, predict, forecasts predicted forecasts Table 1: Nearest neighbours for a number of words with various embedding models. 7.3 Word similarity We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic similarity of short texts.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing V,</booktitle>
<pages>309--227</pages>
<contexts>
<context position="37074" citStr="Islam and Inkpen (2009)" startWordPosition="6017" endWordPosition="6020">mplemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors </context>
</contexts>
<marker>Islam, Inkpen, 2009</marker>
<rawString>Aminul Islam and Diana Inkpen. 2009. Semantic similarity of short texts. Recent Advances in Natural Language Processing V, 309:227–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Discriminative improvements to distributional sentence similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>891--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="37272" citStr="Ji and Eisenstein (2013)" startWordPosition="6053" endWordPosition="6056">of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambiguation framework in which they are used was demonstrated by appropriate tasks at the lexical and sentence level, respectively, with very positive results. As an</context>
</contexts>
<marker>Ji, Eisenstein, 2013</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2013. Discriminative improvements to distributional sentence similarity. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2682" citStr="Kalchbrenner et al., 2014" startWordPosition="403" endWordPosition="406">nce the uniqueness of multi-word expressions would inevitably lead to data sparsity problems, thus to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation for a phrase or sentence by combining the vectors of the words therein. While the nature and complexity of these compositional models may vary, approaches based on deep-learning architectures have been shown to be especially successful in modelling the meaning of sentences for a variety of tasks (Socher et al., 2012; Kalchbrenner et al., 2014). The mutual interaction of distributional word vectors by a means of a compositional model provides many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional process. In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their “ambiguous” counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014)</context>
<context position="11207" citStr="Kalchbrenner et al., 2014" startWordPosition="1777" endWordPosition="1780">eural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts on small overlapping parts of the input vectors. In all the above models, the word embeddings and the weights of the compositional layers are optimized against a task-specific objective function. In Section 3 we will show how to remove the restriction of a supervised setting, introducing a generic objective that can be trained on any general-purpose text corpus. While we focus on recursive and recurrent neural network architectures, </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior disambiguation of word tensors for constructing sentence vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1590--1601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3256" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="490" endWordPosition="493">f tasks (Socher et al., 2012; Kalchbrenner et al., 2014). The mutual interaction of distributional word vectors by a means of a compositional model provides many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional process. In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their “ambiguous” counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results. Furthermore, a second important question relates to the very nature of the word embeddings used in the context of a compositional model. In a setting of this form, word vectors are not any more just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole—a task in which syntax, along with semantics, also plays a very important role. It </context>
<context position="8686" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="1375" endWordPosition="1379"> counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014). 2.2 Syntactic awareness Since the main purpose of distributional models until now was to measure the semantic relatedness of words, relatively little effort has been put into making word vectors aware of information regarding the syntactic role under which a word occurs in a sentence. In some cases the vectors are POStag specific, so that ‘book’ as noun and ‘book’ as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad´o and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity. For word embeddings trained in neural settings, syntactic information is not usually taken explicitly into account, with some notable exceptions. At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model 1532 based on grammatical dependencies. Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of e</context>
<context position="12701" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="2027" endWordPosition="2030">ed into a single vector. Especially for cases of homonymy (such as ‘bank’, ‘organ’ and so on), where the same word is used to describe two or more completely unrelated concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way. To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting, one would ex</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior disambiguation of word tensors for constructing sentence vectors. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>Separating disambiguation from composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of 17th Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>114--123</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="12669" citStr="Kartsaklis et al., 2013" startWordPosition="2022" endWordPosition="2026">a polysemous word is merged into a single vector. Especially for cases of homonymy (such as ‘bank’, ‘organ’ and so on), where the same word is used to describe two or more completely unrelated concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way. To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal fo</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composition in distributional semantics. In Proceedings of 17th Conference on Natural Language Learning (CoNLL), pages 114–123, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Nal Kalchbrenner</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Resolving lexical ambiguity in tensor regression models of meaning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Papers),</booktitle>
<pages>212--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<contexts>
<context position="3282" citStr="Kartsaklis et al., 2014" startWordPosition="494" endWordPosition="497">lchbrenner et al., 2014). The mutual interaction of distributional word vectors by a means of a compositional model provides many opportunities for interesting research, the majority of which still remains to be explored. One such direction is to investigate in what way lexical ambiguity affects the compositional process. In fact, recent work has shown that shallow multi-linear compositional models that explicitly handle extreme cases of lexical ambiguity in a step prior to composition present consistently better performance than their “ambiguous” counterparts (Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). A first attempt to test these observations in a deep compositional setting has been presented by Cheng et al. (2014) with promising results. Furthermore, a second important question relates to the very nature of the word embeddings used in the context of a compositional model. In a setting of this form, word vectors are not any more just a means for discriminating words based on their underlying semantic relationships; the main goal of a word vector is to contribute to a bigger whole—a task in which syntax, along with semantics, also plays a very important role. It is a central point of this</context>
<context position="12727" citStr="Kartsaklis et al., 2014" startWordPosition="2031" endWordPosition="2034">lly for cases of homonymy (such as ‘bank’, ‘organ’ and so on), where the same word is used to describe two or more completely unrelated concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way. To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a strategy less than optimal for a neural setting, one would expect that the benefits sho</context>
</contexts>
<marker>Kartsaklis, Kalchbrenner, Sadrzadeh, 2014</marker>
<rawString>Dimitri Kartsaklis, Nal Kalchbrenner, and Mehrnoosh Sadrzadeh. 2014. Resolving lexical ambiguity in tensor regression models of meaning. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Papers), pages 212–217, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>86--11</pages>
<contexts>
<context position="11319" citStr="LeCun et al., 1998" startWordPosition="1795" endWordPosition="1798">n recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts on small overlapping parts of the input vectors. In all the above models, the word embeddings and the weights of the compositional layers are optimized against a task-specific objective function. In Section 3 we will show how to remove the restriction of a supervised setting, introducing a generic objective that can be trained on any general-purpose text corpus. While we focus on recursive and recurrent neural network architectures, the general ideas we will discuss are in principle model-independent. 2.4 Disambiguation in composition Regardle</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>302--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="9119" citStr="Levy and Goldberg (2014)" startWordPosition="1443" endWordPosition="1446">ich a word occurs in a sentence. In some cases the vectors are POStag specific, so that ‘book’ as noun and ‘book’ as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad´o and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity. For word embeddings trained in neural settings, syntactic information is not usually taken explicitly into account, with some notable exceptions. At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model 1532 based on grammatical dependencies. Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word. With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words. As we will see in Section 3, the current paper offers an appealing alternative to those approaches t</context>
<context position="24728" citStr="Levy and Goldberg (2014)" startWordPosition="4059" endWordPosition="4062">el (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared models which group words only at the semantic level, our model is able to retain tenses, numbers (singulars and plurals), and gerunds. The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for example, present a similar table for their dependency-based extension of the skip-gram model. The advantage of our approach against such models is twofold: firstly, the word embeddings are accompanied by a generic compositional model that can be used for creating sentence representations independently of any specific task; and secondly, the training is quite forgiving to data sparsity problems that in general a dependency-based approach would intensify (since context words are paired with the grammatical relations they occur with the target word). As a result, a small corpus such as the </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302–308, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37237" citStr="Madnani et al. (2012)" startWordPosition="6047" endWordPosition="6050"> with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambiguation framework in which they are used was demonstrated by appropriate tasks at the lexical and sentence level, respectively</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="36984" citStr="Mihalcea et al. (2006)" startWordPosition="5999" endWordPosition="6002">bled model, and we compare with the performance of other existing models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically mo</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1775" citStr="Mikolov et al., 2013" startWordPosition="267" endWordPosition="270"> of words by using their distributional behaviour in a large text corpus is a well-established technique in NLP research that has been proved useful in numerous tasks. In a distributional model of meaning, the semantic representation of a word is given as a vector in some high dimensional vector space, obtained either by explicitly collecting co-occurrence statistics of the target word with words belonging to a representative subset of the vocabulary, or by directly optimizing the word vectors against an objective function in some neural-network based architecture (Collobert and Weston, 2008; Mikolov et al., 2013). Regardless their method of construction, distributional models of meaning do not scale up to Dimitri Kartsaklis Queen Mary University of London School of Electronic Engineering and Computer Science d.kartsaklis@qmul.ac.uk larger text constituents such as phrases or sentences, since the uniqueness of multi-word expressions would inevitably lead to data sparsity problems, thus to unreliable vectorial representations. The problem is usually addressed by the provision of a compositional function, the purpose of which is to prepare a vectorial representation for a phrase or sentence by combining </context>
<context position="7655" citStr="Mikolov et al., 2013" startWordPosition="1212" endWordPosition="1215"> states that two words that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text. In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014). 2.2 Syntactic</context>
<context position="24136" citStr="Mikolov et al. (2013)" startWordPosition="3962" endWordPosition="3965">n Equations 2 and 3. The plausibility layer is implemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (A = 0.03, initial α = 0.05). 7.2 Qualitative evaluation of the word vectors As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words. We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in contrast with the compared models which group words only at the semantic level, our model is able to retain tenses, numbers (singulars and plurals), and gerunds. The observed behaviour is comparable to that of embedding models with objective functions conditioned on grammatical relations between words; Levy and Goldberg (2014), for ex</context>
<context position="27456" citStr="Mikolov et al., 2013" startWordPosition="4456" endWordPosition="4459">), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two weighted sense vectors. We compute and report the Spearman’s correlation between the embedding similarities and human judgments (Table 2). In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (Mikolov et al., 2013) and the multi-sense skip-gram (MSSG) model of Neelakantan et al. (2014). Model globalSim localSim avgSim CBOW 59.5 – – SG 61.8 – – CW 55.3 – – MSSG 61.3 56.7 62.1 SAMS 59.9 58.5 62.5 Table 2: Results for the word similarity task (Spearman’s ρ × 100). Among all methods, only the MSSG model and ours are capable of learning multi-prototype word representations. Our embeddings show top performance for localSim and avgSim measures, and performance competitive to that of MSSG and SG for globalSim, both of which use a hierarchical soft-max as their objective function. Compared to the original C&amp;W mo</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitrijs Milajevs</author>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Matthew Purver</author>
</authors>
<title>Evaluating neural word representations in tensor-based compositional settings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>708--719</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="8240" citStr="Milajevs et al., 2014" startWordPosition="1302" endWordPosition="1305">nd Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014). 2.2 Syntactic awareness Since the main purpose of distributional models until now was to measure the semantic relatedness of words, relatively little effort has been put into making word vectors aware of information regarding the syntactic role under which a word occurs in a sentence. In some cases the vectors are POStag specific, so that ‘book’ as noun and ‘book’ as verb are represented by different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad´o and Lapata, 2007) are more effe</context>
</contexts>
<marker>Milajevs, Kartsaklis, Sadrzadeh, Purver, 2014</marker>
<rawString>Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver. 2014. Evaluating neural word representations in tensor-based compositional settings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="10078" citStr="Mitchell and Lapata, 2008" startWordPosition="1589" endWordPosition="1592">pendencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words. As we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form. 2.3 Compositionality in distributional models The methods that aim to equip distributional models of meaning with compositional abilities come in many different levels of sophistication, from simple element-wise vector operators such as addition and multiplication (Mitchell and Lapata, 2008) to category theory (Coecke et al., 2010). In this latter work relational words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree. The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Polla</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2265--2273</pages>
<contexts>
<context position="9263" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="1463" endWordPosition="1466">different vectors (Kartsaklis and Sadrzadeh, 2013). Furthermore, word spaces in which the context of a target word is determined by means of grammatical dependencies (Pad´o and Lapata, 2007) are more effective in capturing syntactic relations than approaches based on simple word proximity. For word embeddings trained in neural settings, syntactic information is not usually taken explicitly into account, with some notable exceptions. At the lexical level, Levy and Goldberg (2014) propose an extension of the skip-gram model 1532 based on grammatical dependencies. Following a different approach, Mnih and Kavukcuoglu (2013) weight the vector of each context word depending on its distance from the target word. With regard to compositional settings (discussed in the next section), Hashimoto et al. (2014) use dependencybased word embeddings by employing a hinge loss objective, while Hermann and Blunsom (2013) condition their objectives on the CCG types of the involved words. As we will see in Section 3, the current paper offers an appealing alternative to those approaches that does not depend on grammatical relations or types of any form. 2.3 Compositionality in distributional models The methods that aim to equip d</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems, pages 2265–2273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning (ICML-10),</booktitle>
<pages>807--814</pages>
<contexts>
<context position="20739" citStr="Nair and Hinton, 2010" startWordPosition="3398" endWordPosition="3401">also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection. In our model, two networks sharing the same parameters are used to compute the vectorial representations of two sentences, the paraphrase relation of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014). The L2 norm variation is capable of handling differences in the magnitude of the vectors. Formally, the cost function is defined as: �1 2 kf(s1) − f(s2)k2 2 , if y = 1 Ef = 1 classifier sentence vector compositional layer(s) selected sense vectors soft-max layer sense vectors main (ambiguous) vectors Figure 3: Dynamic disambiguation in a generic compositional deep net. plausibility layer sentence vector compositional layer plausibility layer phrase vector compositional layer gate sense vectors main (ambiguous) vectors where s1, s2 are the input sentences, f the composition</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4779" citStr="Neelakantan et al., 2014" startWordPosition="742" endWordPosition="745">ssing, pages 1531–1542, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. The purpose of this work is to improve the current practice in deep compositional models of meaning in relation to both the compositional process itself and the quality of the word embeddings used therein. We propose an architecture for jointly training a compositional model and a set of word embeddings, in a way that imposes dynamic word sense induction for each word during the learning process. Note that this is in contrast with recent work in multi-sense neural word embeddings (Neelakantan et al., 2014), in which the word senses are learned without any compositional considerations in mind. Furthermore, we make the word embeddings syntax-aware by introducing a variation of the hinge loss objective function of Collobert and Weston (2008), in which the goal is not only to predict the occurrence of a target word in a context, but to also predict the position of the word within that context. A qualitative analysis shows that our vectors reflect both semantic and syntactic features in a concise way. In all current deep compositional distributional settings, the word embeddings are internal paramet</context>
<context position="16366" citStr="Neelakantan et al. (2014)" startWordPosition="2648" endWordPosition="2651">rst set, 5&apos;, the target word within a given context is replaced by a random word; as in the original C&amp;W paper, this set is used to enforce semantic coherence in the word vectors. Syntactic coherence is enforced by a second set of negative examples, 5&apos;&apos;, in which the words of the context have been randomly shuffled. The objective function is defined in terms of the following hinge losses: max(0, m − f(s) + f(s&apos;)) (2) 4 From words to senses We now extend our model to address lexical ambiguity. We achieve that by applying a gated architecture, similar to the one used in the multi-sense model of Neelakantan et al. (2014), but advancing the main idea to the compositional setting detailed in Section 3. We assume a fixed number of n senses per word.1 Each word is associated with amain vector (obtained for example by using an existing vector set, or by simply applying the process of Section 3 in a separate step), as well as with n vectors denoting cluster centroids and an equal number of sense vectors. Both cluster centroids and sense vectors are randomly initialized in the beginning of the process. For each word wt in a training sentence, we prepare a context vector by averaging the main vectors of all other wor</context>
<context position="17844" citStr="Neelakantan et al. (2014)" startWordPosition="2903" endWordPosition="2906">entroid is updated by the addition of the context vector, and the associated sense vector is passed as input to the compositional layer. The selected sense vectors for each word in the sentence are updated by backpropagation, based on the objectives of Equations 2 and 3. The overall architecture of our model, as described in this and the previous section, is illustrated in Figure 2. 5 Task-specific dynamic disambiguation The model of Figure 2 decouples the training of word vectors and compositional parameters from 1Note that in principle the fixed number of senses assumption is not necessary; Neelakantan et al. (2014), for example, present a version of their model in which new senses are added dynamically when appropriate. � SES � S/ES/ 1534 Figure 2: Training of syntax-aware multi-sense embeddings in the context of a RecNN. a specific task, and as a consequence from any task-specific training dataset. However, note that by replacing the plausibility layer with a classifier trained for some task at hand, you get a taskspecific network that transparently trains multisense word embeddings and applies dynamic disambiguation on the fly. While this idea of a singlestep direct training seems appealing, one consi</context>
<context position="26836" citStr="Neelakantan et al. (2014)" startWordPosition="4354" endWordPosition="4357">ictions reproducing, replicate, humans predicting, assumption, expectations, projections, prediction, predict, forecasts predicted forecasts Table 1: Nearest neighbours for a number of words with various embedding models. 7.3 Word similarity We now proceed to a quantitative evaluation of our embeddings on the Stanford Contextual Word Similarity (SCWS) dataset of Huang et al. (2012). The dataset contains 2,003 pairs of words and the contexts they occur in. We can therefore make use of the contextual information in order to select the most appropriate sense for each ambiguous word. Similarly to Neelakantan et al. (2014), we use three different metrics: globalSim measures the similarity between two ambiguous word vectors; localSim selects a single sense for each word based on the context and computes the similarity between the two sense vectors; avgSim represents each word as a weighted average of all senses in the given context and computes the similarity between the two weighted sense vectors. We compute and report the Spearman’s correlation between the embedding similarities and human judgments (Table 2). In addition to the skipgram and Collobert and Weston models, we also compare against the CBOW model (M</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based Construction of Semantic Space Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>S. Pad´o and M. Lapata. 2007. Dependency-based Construction of Semantic Space Models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="7681" citStr="Pennington et al., 2014" startWordPosition="1216" endWordPosition="1219"> that occur in similar contexts have similar meanings. Traditional approaches for constructing a word space rely on simple counting: a word is represented by a vector of numbers (usually smoothed by the application of some function such as point-wise mutual information) which show how frequently this word co-occurs with other possible context words in a corpus of text. In contrast to these methods, a recent class of distributional models treat word representations as parameters directly optimized on a word prediction task (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). Instead of relying on observed cooccurrence counts, these models aim to maximize the objective function of a neural net-based architecture; Mikolov et al. (2013), for example, compute the conditional probability of observing words in a context around a target word (an approach known as the skip-gram model). Recent studies have shown that, compared to their co-occurrence counterparts, neural word vectors reflect better the semantic relationships between words (Baroni et al., 2014) and are more effective in compositional settings (Milajevs et al., 2014). 2.2 Syntactic awareness Since the main </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="10687" citStr="Pollack (1990)" startWordPosition="1692" endWordPosition="1693">2008) to category theory (Coecke et al., 2010). In this latter work relational words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree. The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neu</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37040" citStr="Qiu et al. (2006)" startWordPosition="6011" endWordPosition="6014">ting models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the synta</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Ioannis P Klapaftis</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>Dynamic and static prototype vectors for semantic composition. In</title>
<date>2011</date>
<booktitle>IJCNLP,</booktitle>
<pages>705--713</pages>
<contexts>
<context position="12644" citStr="Reddy et al., 2011" startWordPosition="2018" endWordPosition="2021">ch every meaning of a polysemous word is merged into a single vector. Especially for cases of homonymy (such as ‘bank’, ‘organ’ and so on), where the same word is used to describe two or more completely unrelated concepts, this approach is problematic: the semantic representation of the word becomes the average of all senses, inadequate to express any of them in a reliable way. To address this problem, a prior disambiguation step on the word vectors is often introduced, the purpose of which is to find the word representations that best fit to the given context, before composition takes place (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013; Kartsaklis et al., 2014). This idea has been tested on algebraic and tensor-based compositional functions with very positive results. Furthermore, it has been also found to provide minimal benefits for a RecNN compositional architecture in a number of phrase and sentence similarity tasks (Cheng et al., 2014). This latter work clearly suggests that explicitly dealing with lexical ambiguity in a deep compositional setting is an idea that is worth to be further explored. While treating disambiguation as only a preprocessing step is a stra</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011. Dynamic and static prototype vectors for semantic composition. In IJCNLP, pages 705–713.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Philip M McCarthy</author>
<author>Mihai C Lintean</author>
<author>Danielle S McNamara</author>
<author>Arthur C Graesser</author>
</authors>
<title>Paraphrase identification with lexicosyntactic graph subsumption.</title>
<date>2008</date>
<booktitle>In FLAIRS conference,</booktitle>
<pages>201--206</pages>
<contexts>
<context position="37012" citStr="Rus et al. (2008)" startWordPosition="6005" endWordPosition="6008">he performance of other existing models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 Th</context>
</contexts>
<marker>Rus, McCarthy, Lintean, McNamara, Graesser, 2008</marker>
<rawString>Vasile Rus, Philip M McCarthy, Mihai C Lintean, Danielle S McNamara, and Arthur C Graesser. 2008. Paraphrase identification with lexicosyntactic graph subsumption. In FLAIRS conference, pages 201–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="10771" citStr="Socher et al., 2011" startWordPosition="1703" endWordPosition="1706">words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree. The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts o</context>
<context position="35997" citStr="Socher et al. (2011" startWordPosition="5837" endWordPosition="5840"> of the previously reported results. We notice that using distributional properties alone cannot capture efficiently subtle aspects of a sentence, for example numbers or human names. However, even small differences on those aspects between two sentences can lead to a different classification result. Therefore, we train (using the MSPRC training data) an additional logistic regression classifier which is based not only on the embeddings similarity, but also on a few handengineered features. We then ensemble the new classifier (C1) with the original one. In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same). In Table 6 we report results of the original model and the ensembled model, and we compare with the performance of other existing models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the </context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th international conference on machine learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="10771" citStr="Socher et al., 2011" startWordPosition="1703" endWordPosition="1706">words (such as verbs or adjectives) are represented as multi-linear maps acting on vectors representing their arguments (nouns and noun phrases). In general, the above models are shallow in the sense that they do not have functional parameters and the output is produced by the direct interaction of the inputs; yet they have been shown to capture the compositional meaning of sentences to an adequate degree. The idea of using neural networks for compositionality in language appeared 25 years ago in a seminal paper by Pollack (1990), and has been recently re-popularized by Socher and colleagues (Socher et al., 2011a; Socher et al., 2012). The compositional architecture used in these works is that of a recursive neural network (RecNN) (Socher et al., 2011b), where the words get composed by following a parse tree. A particular variant of the RecNN is the recurrent neural network (RNN), in which a sentence is assumed to be generated by aggregating words in sequence (Mikolov et al., 2010). Furthermore, some recent work (Kalchbrenner et al., 2014) models the meaning of sentences by utilizing the concept of a convolutional neural network (LeCun et al., 1998), the main characteristic of which is that it acts o</context>
<context position="35997" citStr="Socher et al. (2011" startWordPosition="5837" endWordPosition="5840"> of the previously reported results. We notice that using distributional properties alone cannot capture efficiently subtle aspects of a sentence, for example numbers or human names. However, even small differences on those aspects between two sentences can lead to a different classification result. Therefore, we train (using the MSPRC training data) an additional logistic regression classifier which is based not only on the embeddings similarity, but also on a few handengineered features. We then ensemble the new classifier (C1) with the original one. In terms of feature selection, we follow Socher et al. (2011a) and Blacoe and Lapata (2012) and add the following features: the difference in sentence length, the unigram overlap among the two sentences, features related to numbers (including the presence or absence of numbers from a sentence and whether or not the numbers in the two sentences are the same). In Table 6 we report results of the original model and the ensembled model, and we compare with the performance of other existing models. In all of the implemented models (including the additive baseline), disambiguation is performed to guarantee the best performance. We see that by ensembling the </context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing</booktitle>
<marker>A, 2012</marker>
<rawString>R. Socher, B. Huval, C. Manning, and Ng. A. 2012. Semantic compositionality through recursive matrix-vector spaces. In Conference on Empirical Methods in Natural Language Processing 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Sun</author>
<author>Yuheng Chen</author>
<author>Xiaogang Wang</author>
<author>Xiaoou Tang</author>
</authors>
<title>Deep learning face representation by joint identification-verification.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>pages</pages>
<contexts>
<context position="20083" citStr="Sun et al., 2014" startWordPosition="3280" endWordPosition="3283">ction 5 in a paraphrase detection task. A paraphrase is a restatement of the meaning of a sentence using different words and/or syntax. The goal of a paraphrase detection model, thus, is to examine two sentences and decide if they express the same meaning. While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection. In our model, two networks sharing the same parameters are used to compute the vectorial representations of two sentences, the paraphrase relation of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while th</context>
<context position="33757" citStr="Sun et al., 2014" startWordPosition="5475" endWordPosition="5478">ve and recurrent compositional architectures, especially in grammars with strict branching structure such as in English, is that any given composition is usually the product of a terminal and a non-terminal; i.e. a single word can contribute to the meaning of a sentence to the same extent as the rest of a sentence on its whole, as below: [[kids]NP [play ball games in the park]VP]S In the above case, the contribution of the words within the verb phrase to the final sentence representation will be faded out due to the recursive composition mechanism. Inspired by related work in computer vision (Sun et al., 2014), we attempt to alleviate this problem by introducing an average pooling layer at the sense vector level and adding the resulting vector to the sentence representation. By doing this we expect that the new sentence vector will reflect local features from all words in the sentence that can help in the classification in a more direct way. The results for the new deep architectures are shown in Table 5, where we see substantial improvements for both deep nets. More importantly, the effect of dynamic 2For all subsequent experiments, the reported results are based on the soft selection method. 1538</context>
</contexts>
<marker>Sun, Chen, Wang, Tang, 2014</marker>
<rawString>Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. 2014. Deep learning face representation by joint identification-verification. In Advances in Neural Information Processing Systems, pages 1988– 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the para-farce out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<volume>volume</volume>
<contexts>
<context position="37142" citStr="Wan et al. (2006)" startWordPosition="6029" endWordPosition="6032">formed to guarantee the best performance. We see that by ensembling the original classifier with C1, we improve the result of the previous section by another 1%. This is the second best result reported so far BL Model Acc. F1 All positive 66.5 79.9 Addition (disamb.) 71.3 81.1 Dynamic Dis. RecNN 76.0 84.0 RecNN+Pooling 77.6 84.7 RecNN+Pooling+C1 78.6 85.3 RNN 75.2 83.6 RNN+Pooling 76.6 84.3 RNN+Pooling+C1 77.5 84.6 Published results Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2009) 72.6 81.3 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Madnani et al. (2012) 77.4 84.1 Ji and Eisenstein (2013) 80.4 85.9 Table 6: Cross-model comparison in the paraphrase detection task. for the specific task, with a 0.6 difference in Fscore from the first (Ji and Eisenstein, 2013).3 8 Conclusion and future work The main contribution of this paper is a deep compositional distributional model acting on linguistically motivated word embeddings.4 The effectiveness of the syntax-aware, multi-sense word vectors and the dynamic compositional disambiguation framework in which they</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the para-farce out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, volume 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>247--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="20191" citStr="Yih et al. (2011)" startWordPosition="3300" endWordPosition="3303">ferent words and/or syntax. The goal of a paraphrase detection model, thus, is to examine two sentences and decide if they express the same meaning. While the usual way to approach this problem is to utilize a classifier that acts (for example) on the concatenation of the two sentence vectors, in this work we follow a novel perspective: specifically, we apply a siamese architecture (Bromley et al., 1993), a concept that has been extensively used in computer vision (Hadsell et al., 2006; Sun et al., 2014). While siamese networks have been also used in the past for NLP purposes (for example, by Yih et al. (2011)), to the best of our knowledge this is the first time that such a setting is applied for paraphrase detection. In our model, two networks sharing the same parameters are used to compute the vectorial representations of two sentences, the paraphrase relation of which we wish to detect; this is achieved by employing a cost function that compares the two vectors. There are two commonly used cost functions: the first is based on the L2 norm (Hadsell et al., 2006; Sun et al., 2014), while the second on the cosine similarity (Nair and Hinton, 2010; Sun et al., 2014). The L2 norm variation is capabl</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247–256, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</title>
<date>2012</date>
<contexts>
<context position="23802" citStr="Zeiler, 2012" startWordPosition="3905" endWordPosition="3906">er, while for the RNN the composition takes place in sequence from left to right. To avoid the exploding or vanishing gradient problem (Bengio et al., 1994) for long sentences, we employ a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). During the training of each model, we minimize the hinge loss in Equations 2 and 3. The plausibility layer is implemented as a 2-layer network, with 150 units at the hidden layer, and is applied at each individual node (as opposed to a single application at the sentence level). All parameters are updated with mini-batches by AdaDelta (Zeiler, 2012) gradient descent method (A = 0.03, initial α = 0.05). 7.2 Qualitative evaluation of the word vectors As a first step, we qualitatively evaluate the trained word embeddings by examining the nearest neighbours lists of a few selected words. We compare the results with those produced by the skipgram model (SG) of Mikolov et al. (2013) and the language model (CW) of Collobert and Weston (2008). We refer to our model as SAMS (SyntaxAware Multi-Sense). The results in Table 1 show clearly that our model tends to group words that are both semantically and syntactically related; for example, and in co</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D Zeiler. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
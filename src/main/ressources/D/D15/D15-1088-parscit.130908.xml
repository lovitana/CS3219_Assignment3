<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.042185">
<title confidence="0.985810333333333">
Mr. Bennet, his coachman, and the Archbishop walk into a bar but only
one of them gets recognized:
On The Difficulty of Detecting Characters in Literary Texts
</title>
<author confidence="0.999686">
Hardik Vala&apos;, David Jurgens&apos;, Andrew Piper2, Derek Ruths&apos;
</author>
<affiliation confidence="0.924598666666667">
&apos;School of Computer Science
2Department of Languages, Literatures, and Cultures
McGill University, Montreal, QC, Canada
</affiliation>
<email confidence="0.984518">
hardik.vala@mail.mcgill.ca,jurgens@cs.mcgill.ca
andrew.piper@mcgill.ca,derek.ruths@mcgill.ca
</email>
<sectionHeader confidence="0.993704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911285714286">
Characters are fundamental to literary
analysis. Current approaches are heav-
ily reliant on NER to identify characters,
causing many to be overlooked. We pro-
pose a novel technique for character detec-
tion, achieving significant improvements
over state of the art on multiple datasets.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838628571429">
How many literary characters appear in a novel?
Despite the seeming simplicity of the question,
precisely identifying which characters appear in a
story remains an open question in literary and nar-
rative analysis. Characters form the core of many
computational analyses, from inferring prototypi-
cal character types (Bamman et al., 2014) to iden-
tifying the structure of social networks in literature
(Elson et al., 2010; Lee and Yeung, 2012; Agar-
wal et al., 2013; Ardanuy and Sporleder, 2014;
Jayannavar et al., 2015). These current approaches
have largely assumed that characters can be reli-
ably identified in text using standard techniques
such as Named Entity Recognition (NER) and that
the variations in how a character is named can be
found through coreference resolution. However,
such treatment of character identity often over-
looks minor characters that serve to enrich the so-
cial structure and serve as foils for the identities of
major characters (Eder et al., 2010).
This work provides a comprehensive exami-
nation of literary character detection, with three
key contributions. First, we formalize the task
with evaluation criteria and offer two datasets, in-
cluding a complete, manually-annotated list of all
characters in 58 literary works. Second, we pro-
pose a new technique for character detection based
on inducing character prototypes, and in compar-
isons with three state-of-the-art methods, demon-
strate superior performance, achieving significant
improvements in F1 over the next-best method.
Third, as practical applications, we analyze liter-
ary trends in character density over 20 decades
and revisit the character-based literary hypothesis
tested by Elson et al. (2010).
</bodyText>
<sectionHeader confidence="0.999746" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998530214285714">
Character detection has primarily been performed
in the context of mining literary social networks.
Elson et al. (2010) extract character mentions
from conversational segments, using the Stan-
ford CoreNLP NER system to discover character
names (Manning et al., 2014). To account for vari-
ability in character naming, alternate forms of a
name are generated using the method of Davis et
al. (2003) and merged together as a single char-
acter. Furthermore, the set of aliases for a char-
acter is expanded by creating coreference chains
originating from these proper names and merging
all coreferent expressions. Agarwal et al. (2013)
also rely on the CoreNLP NER and coreference
resolution systems for character detection; how-
ever for literary analysis, they use gold character
mentions that have been marked and resolved by a
team of trained annotators, highlighting the diffi-
culty of the task.
He et al. (2013) propose an alternate approach
for identifying speaker references in novels, using
a probabilistic model to identify which character
is speaking. However, to account for the multiple
aliases used to refer to a character, the authors first
manually constructed a list of characters and their
aliases, which is the task proposed in this work and
underscores the need for automated methods.
Two approaches mined social interaction net-
</bodyText>
<page confidence="0.973657">
769
</page>
<note confidence="0.6584425">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999926368421053">
works without relying on dialogue, unlike the
methods of Elson et al. (2010) and He et al. (2013).
Lee and Yeung (2012) build social networks by
recognizing characters from explicit markers (e.g.,
kinship) and implicit markers (e.g., physical collo-
cation). Similarly, Agarwal and Rambow (2010)
build character networks using tree kernels on
parse trees to identify interacting agents.
In the two most-related works, Bamman et al.
(2014) and Ardanuy and Sporleder (2014), char-
acter names are extracted and clustered under a set
of constraints. In the BookNLP system developed
by Bamman et al. (2014), NER-identified names
are retained and merged based on animacy, deter-
mined through dependencies with ”sentient” lem-
mas from a small dictionary (including for exam-
ple, say and smile), and gender, assigned through
pronomial resolution and a dictionary of gender-
specific honorifics. Ardanuy and Sporleder (2014)
similarly use NER to identify character name men-
tions. These names are grouped through the appli-
cation of a series of deterministic rules, beginning
with recognizing gender constraints, where gen-
der assignments are based off of gender-specific
honorifics and names. If a gender can’t be as-
signed, then one is derived from the majority count
of gender-specific pronouns (e.g. he, herself) ap-
pearing in the immediate context of the name men-
tions. The extracted names are then clustered,
while respecting the gender impositions, based on
a sieve of name variant heuristics. In the final step,
any remaining ambiguous referents , i.e., those
that can be matched to multiple characters, are
assigned to the more prominent character in the
story. The authors achieve F1-scores &gt; 0.9 for ex-
tracting the 10 most relevant characters in a small
collection of novels, but the performance on all
characters is unknown.
</bodyText>
<sectionHeader confidence="0.97178" genericHeader="method">
3 Detecting Characters
</sectionHeader>
<bodyText confidence="0.998140166666667">
We propose an eight stage pipeline for detecting
characters, which builds a graph where nodes are
names and edges connect names belonging to the
same character. The vertices in the graph are ini-
tially populated by running NER over the corpus
and also incorporating names following an hon-
orific. Second, coreference resolution is run to
identify names that occur together in a corefer-
ence chain and edges are added where two nodes’
names co-occur in a chain. Stanford CoreNLP
is used for both NER and co-reference. Third,
we apply a series of name variation rules to link
</bodyText>
<note confidence="0.574679">
Mr. Bennet Miss Bennet Mr. Bennet Miss Bennet
</note>
<figureCaption confidence="0.996747333333333">
Figure 1: Resolving names in a character graph. The circles
represent individual names and the thin and thick lines denote
edges and anti-edges, respectively.
</figureCaption>
<bodyText confidence="0.985334604651163">
names potentially referring to the same charac-
ter (e.g., by removing an honorific). Fourth, a
gazetteer of 1859 hypocorisms for 560 names is
used to link variations (e.g., Tim and Timmy).
Stages 2–4 potentially introduce edges connect-
ing names of different characters. Therefore, in
the fifth stage, three heuristics are applied to add
prohibitions on merging two names into the same
character. Two vertices cannot be merged if (1)
the inferred genders of both names differ, (2) both
names share a common surname but different first
names, or (3) the honorific of both names differ,
e.g., “Miss” and “Mrs.” Similarly, the sixth stage
inserts prohibitions by extracting pairs of names
from the novel where (1) both names appear con-
nected by a conjunction, (2) one name appears as
the speaker mentioning the other name in direct
speech, and (3) both names appear together in a
quotation.
Together, Stage 1–6 are applied by first linking
all nodes by edges and following, identifying pairs
prohibited from being connected and remove the
edges along the shortest path between those two
nodes, effectively creating two new disconnected
components in the name graph. Figure 1 illustrates
this transformation on a simple character graph.
Next, the seventh step attempts to identify char-
acters whose names may not be recognized by
NER. For example, many minor characters do not
appear as named entities and instead have general
role-based referents such as “the governor” or “the
archbishop.” However, despite the lack of proper
names, such characters behave and interact in sim-
ilar ways as major characters, including having di-
alogue. Therefore, to discover such characters,
we adopt a bootstrapping technique aimed at un-
covering prototypical character behaviors from the
novels themselves, inspired by the semantic predi-
cate work of Flati and Navigli (2013). The Project
Gutenberg fiction corpus was dependency parsed
to identify all verbs in a dependency relation with
nouns, where each noun was categorized as (a) a
Bennet Elizabeth Bennet Bennet Elizabeth Bennet
</bodyText>
<page confidence="0.903558">
770
</page>
<bodyText confidence="0.999599541666667">
named entity (b) having its first sense in Word-
Net refer to an animate entity (Fellbaum, 1998), or
(c) neither of the above. All verbs associated with
these were then ranked according to their ratio of
types (a) and (c) to identify verbs strongly asso-
ciated with character-like behaviors, which avoids
including the behavior of nouns in (b) which may
refer to minor characters. Ultimately, 2,073 verbs-
and-dependency pairs with a ratio of &gt;0.25 were
retained as predicates selecting for character-like
entities, after limited experimental testing showed
this threshold extracted sensible verbs such as “re-
joice,” “accost,” and “frown.” Using this set of
predicates, nouns appearing with the verb in the
appropriate dependency relation are added as char-
acters. We prohibit adding names contained in a
small stop list of 22 generic nouns (e.g., “man”).
Finally, the eighth and last stage removes nodes
that are disconnected from the rest of the graph
and represent a name that is a portion of one or
more names for other nodes. These nodes are typ-
ically ambiguous first or last names. Thus, the re-
maining set of nodes are merged to create sets of
names, each associated with a different character.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996165102564103">
Given a literary novel, our objective is to produce
a list of characters, where each character may be
associated with one or more names.
Datasets Two datasets are used. The first is
a manually-annotated collection of 58 works with
complete lists of all characters and their possible
referents in texts. Of these works, 56 were gener-
ated as a part of an on-going longitudinal study of
author style for all Sherlock Holmes stories writ-
ten by Sir Arthur Conan Doyle. The remaining
two works are the full length novels Pride and
Prejudice by Jane Austen and The Moonstone by
Wilkie Collins. Characters and their alias were
manually coded by expert annotators, with multi-
ple passes to ensure completeness. The Moonstone
was treated as truly held-out test data and results
were only generated once prior to submission.
The second dataset consists of 30 novels listed
on Sparknotes (sparknotes.com) and their cor-
responding lists of characters, with supplemental
naming variations of these characters provided by
our annotators. These character lists often contain
only the major characters in a novel; for example,
their list for Pride and Prejudice contains only 17
characters, where as our manually-annotated list
identifies 73 characters. Nevertheless, the Spar-
knotes data serves as a baseline of those characters
any method should be able to detect.
Evaluation Character recognition systems
produce a list of sets, each containing the names
associated with one character, denoted E _
{E1, ... , E,,,} where EZ is a set of names for a
character. These lists are evaluated against a gold
standard list, denoted G, containing all naming
variations for each character. To evaluate, we for-
malize the problem as finding a maximum bipar-
tite matching where the sets of names in E and
those in G constitute the two node types. For
precision, matching is measured in the purity of
an extracted set of names, EZ, with respect to the
gold-standard names, Gj: 1 — |E�—G�|
|E� |; simply, a
match is maximal when the set of extracted names
is a subset of the gold standard names, with penal-
ties for including wrong names. Recall uses a
looser definition of matching with the aim of mea-
suring whether a character Gj was found at all;
matching is measured as a binary function that is
1 if EZ n Gj 7� 0 and 0 otherwise.
Comparison Systems The task of character
recognition has largely been subsumed into the
task of extracting the social network of novels.
Therefore, three state-of-the-art systems for social
network extraction were selected: the method de-
scribed in Elson et al. (2010), BookNLP (Bamman
et al., 2014), and the method described in Ardanuy
and Sporleder (2014). For each method, we follow
their procedures for identifying the characters in
the social network, which produces sets of one or
more aliases associated with each identified char-
acter. As a baseline, we use the output of Stanford
NER, where every name is considered a separate
character; this baseline represents the upper-bound
in recall from any system using only NER to iden-
tify character names.
Experiment 1: Accuracy Table 1 shows the
results for the manually-annotated and SparkNotes
corpora. The Sherlock Holmes corpus presents a
notable challenge due to the presence of many mi-
nor characters, which are not detected by NER. An
error analysis for our approach revealed that while
many characters were extracted, the coreference
resolution did not link a characters’ different ref-
erents together and hence, each name was reported
as a separate character, which caused a drop in per-
formance. Nevertheless, our system provided the
highest performance for character recognition.
The Pride and Prejudice novel presents a dif-
</bodyText>
<page confidence="0.995031">
771
</page>
<table confidence="0.999035142857143">
System Sherlock Holmes Stories Pride and Prejudice The Moonstone F1 SparkNotes
Precision Recall F1 Precision Recall F1 Precision Recall Recall
NER Baseline 0.3327 0.6535 0.4332 0.3910 0.8356 0.5328 0.2460 0.5441 0.3388 0.6762
Elson et al. (2010) 0.3757 0.6042 0.4485 0.3100 0.5205 0.3886 0.2612 0.4931 0.3415 0.4723
BookNLP (Bamman et al., 2014) 0.6084 0.4832 0.5219 0.4855 0.5205 0.5024 0.3662 0.4706 0.4119 0.5880
Ardanuy and Sporleder (2014) 0.5744 0.4719 0.5181 0.4610 0.5108 0.4846 0.3623 0.4691 0.4088 0.5898
This work 0.5109 0.6099 0.5404 0.7245 0.7945 0.7579 0.3673 0.5735 0.4478 0.5990
</table>
<tableCaption confidence="0.997882">
Table 1: Accuracy of character detection on different portions of the two datasets.
</tableCaption>
<table confidence="0.99998275">
Precision Recall F1
Sherlock Holmes Stories 0.5910 0.5335 0.5608
Pride and Prejudice 0.7635 0.6879 0.7237
The Moonstone 0.3943 0.4613 0.4294
</table>
<tableCaption confidence="0.999722">
Table 2: Accuracy of proposed system without stage 7.
</tableCaption>
<bodyText confidence="0.999236447058824">
ferent set of challenges due to multiple charac-
ters sharing the same last name or the same first
name. Here, coreference resolution frequently cre-
ates incorrect links between the similar names of
different characters, creating a drop in precision
for most systems. Our precision value particularly
benefited from the heuristics for distinguishing
characters by gender and stringent name-merging
constraints. BookNLP and the approach of Ar-
danuy and Sporleder (2014) performed quite sim-
ilarly in identifying characters, which is expected
given the overlap in rules applied by both systems.
Moonstone contains a unique novel structure
with multiple first-person narrators, group-based
characters (e.g., “the jugglers”) that present a chal-
lenge to co-reference systems, and 419 different
names for the 78 unique characters. An error anal-
ysis of our system revealed that majority of mis-
takes were due to the multiple names for a charac-
ter not being merged into a single identity. Never-
theless, our system performs best of those tested.
For the SparkNotes data, the NER baseline
achieves the highest recall, indicating that many
of the major character names listed in SparkNotes’
data can be directly found by NER. Nevertheless,
in reality, the baseline’s performance is offset by
its significantly lower precision, as shown in its
performance on the other novels; indeed the base-
line grossly overestimates the number of charac-
ters for the SparkNotes novels, reporting 339 char-
acters per novel on average.
Table 2 shows our system’s performance with-
out stage 7, which involved the extraction of minor
characters. Stage 7 overall improves recall with
a slight hindrance to precision. For the Sherlock
Holmes corpus, stage 7 is slightly detrimental to
overall performance, which as we stipulated ear-
lier is caused by missing co-referent links.
Finally, returning to the initially-posed question
of how many characters are present, we find that
despite the detection error in our method, the over-
all predicted number of characters is quite close
to the actual: for Sherlock Holmes stories, the
number of characters was estimated within 2.4
on average, for Pride and Prejudice our method
predicted 72 compared with 73 actual charac-
ters, and for The Moonstone our method predicted
87 compared with 78. Thus, we argue that our
procedure can provide a reasonable estimate for
the total number of characters. (For compari-
son, BookNLP, the next best system, extracted 69
and 72 characters for Pride and Prejudice and The
Moonstone, respectively, and within 1.2, on aver-
age, on the Sherlock Holmes set.)
Experiment 2: Literary Theories Elson et al.
(2010) analyze 60 novels to computationally test
literary theories for novels in urban and rural set-
tings (Williams, 1975; Moretti, 1999). Recently,
Jayannavar et al. (2015) challenged this analy-
sis, showing their improved method for social net-
work extraction did not support the same conclu-
sions. While our work focuses only on character
detection, we are nevertheless able to test the re-
lated hypothesis of whether the number of char-
acters in novels with urban settings is more than
those in rural. Character detection was run on
the same novels from Elson et al. (2010) and we
found no statistically-significant difference in the
mean number of characters in urban and rural set-
tings, even when accounting for text size. Thus,
our work raises questions about how these char-
acter interact and whether the setting influences
the structure of the social network, despite simi-
lar numbers of characters.
Experiment 3: Historical Trends As a sec-
ond application of our technique, we examine his-
torical trends in how many characters appear in a
novel. All fiction novels listed on Project Guten-
berg were compiled and publication dates were au-
tomatically extracted for 1066 and manually en-
tered for an additional 637. This set was combined
with a corpus of 6333 novels, including works
such as To The Lighthouse by Virginia Woolf, not
available on Project Gutenberg. Books were then
partitioned into the decade in which they were au-
</bodyText>
<page confidence="0.974599">
772
</page>
<figure confidence="0.993173333333333">
Ratio of number of characters
to book tokens
0.0045
0.0035
0.0025
0.0015
0.0005
0.004
0.003
0.002
0.001
0
</figure>
<figureCaption confidence="0.999986">
Figure 2: Distributions of the size-normalized number of characters per novel per decade.
</figureCaption>
<bodyText confidence="0.99976052631579">
thored. We limit our focus to trends starting in
1800 to 1990, when at least 11 books are available
for each decade.
To account for variability in novel length, we
normalize the novel’s number of characters by its
number of tokens. Figure 2 shows the box-and-
whisker plot of the normalized number of char-
acters per novel, where the box denotes the first
and third quartile and the bar denotes the me-
dian. Surprisingly, we did not observe any signif-
icant change in the relative number of characters
per novel, despite the underlying socio-economic
changes that accompanied this time period. While
novels written before 1850 had slightly more char-
acters on average, this effect may be due to the
smaller number of works available from this pe-
riod. However, our finding raises many questions
about whether the social networks for these char-
acters obey similar trends in their size and density.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998926">
Although a fundamental task to character analysis,
identifying the number of characters in a literary
novel presents a significant challenge to current
state of the art. To lay the foundation towards solv-
ing the task, we provide three contributions: (1)
an annotated corpus of 58 books, (2) an evaluation
framework for measuring performance on the task,
(3) a new state-of-the-art method for character ex-
traction. Furthermore, to promote future work we
make all software and data available upon request.
</bodyText>
<sectionHeader confidence="0.99935" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997895">
We would like to thank the three annotators for
their diligent reading and coding of novels.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999069794871795">
Apoorv Agarwal and Owen Rambow. 2010. Auto-
matic detection and classification of social events.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1024–1034.
Apoorv Agarwal, Anup Kotalwar, and Owen Rambow.
2013. Automatic extraction of social networks from
literary text: A case study on alice in wonderland. In
the Proceedings of the 6th International Joint Con-
ference on Natural Language Processing.
Mariona Coll Ardanuy and Caroline Sporleder. 2014.
Structure-based clustering of novels. In Proceedings
of the EACL Workshop on Computational Linguis-
tics for Literature, pages 31–39.
David Bamman, Ted Underwood, and Noah A Smith.
2014. A bayesian mixed effects model of literary
character. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 370–379.
Peter T Davis, David K Elson, and Judith L Klavans.
2003. Methods for precise named entity matching
in digital collections. In Proceedings of the 3rd
ACM/IEEE-CS joint conference on Digital libraries,
pages 125–127.
Jens Eder, Fotis Jannidis, and Ralf Schneider. 2010.
Characters in fictional worlds: Understanding
imaginary beings in literature, film, and other me-
dia, volume 3. Walter de Gruyter.
David K Elson, Nicholas Dames, and Kathleen R
McKeown. 2010. Extracting social networks from
literary fiction. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 138–147.
Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.
Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings of the The 51st Annual Meeting of the
Association for Computational Linguistics.
</reference>
<page confidence="0.985995">
773
</page>
<reference confidence="0.99929652">
Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. In Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1312–
1320.
Prashant Arun Jayannavar, Apoorv Agarwal, Melody
Ju, and Owen Rambow. 2015. Validating literary
theories using automatic social network extraction.
In Proceedings of the NAACL-2015 Workshop on
Computational Linguistics for Literature.
John Lee and Chak Yan Yeung. 2012. Extracting net-
works of people and places from literary texts. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Franco Moretti. 1999. Atlas of the European novel,
1800-1900. Verso.
Raymond Williams. 1975. The country and the city,
volume 423. Oxford University Press.
</reference>
<page confidence="0.998358">
774
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308601">
<title confidence="0.73614">Mr. Bennet, his coachman, and the Archbishop walk into a bar but one of them gets recognized: On The Difficulty of Detecting Characters in Literary Texts</title>
<author confidence="0.999906">David Andrew Derek</author>
<affiliation confidence="0.894898">of Computer of Languages, Literatures, and McGill University, Montreal, QC,</affiliation>
<abstract confidence="0.99930275">Characters are fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic detection and classification of social events.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1024--1034</pages>
<contexts>
<context position="4267" citStr="Agarwal and Rambow (2010)" startWordPosition="636" endWordPosition="639">es, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net769 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined through dependencies with ”sentient” lemmas from a small dictionary (including for example, say and smile), and gender, assigned through pronomial resolution and a dictionary of genderspecific honorifics. Ardanuy and Sp</context>
</contexts>
<marker>Agarwal, Rambow, 2010</marker>
<rawString>Apoorv Agarwal and Owen Rambow. 2010. Automatic detection and classification of social events. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024–1034.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Anup Kotalwar</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic extraction of social networks from literary text: A case study on alice in wonderland.</title>
<date>2013</date>
<booktitle>In the Proceedings of the 6th International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1198" citStr="Agarwal et al., 2013" startWordPosition="167" endWordPosition="171">verlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with</context>
<context position="3067" citStr="Agarwal et al. (2013)" startWordPosition="454" endWordPosition="457">cter detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task pro</context>
</contexts>
<marker>Agarwal, Kotalwar, Rambow, 2013</marker>
<rawString>Apoorv Agarwal, Anup Kotalwar, and Owen Rambow. 2013. Automatic extraction of social networks from literary text: A case study on alice in wonderland. In the Proceedings of the 6th International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Coll Ardanuy</author>
<author>Caroline Sporleder</author>
</authors>
<title>Structure-based clustering of novels.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL Workshop on Computational Linguistics for Literature,</booktitle>
<pages>31--39</pages>
<contexts>
<context position="1227" citStr="Ardanuy and Sporleder, 2014" startWordPosition="172" endWordPosition="175">a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with three key contributions. Fir</context>
<context position="4443" citStr="Ardanuy and Sporleder (2014)" startWordPosition="663" endWordPosition="666"> on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined through dependencies with ”sentient” lemmas from a small dictionary (including for example, say and smile), and gender, assigned through pronomial resolution and a dictionary of genderspecific honorifics. Ardanuy and Sporleder (2014) similarly use NER to identify character name mentions. These names are grouped through the application of a series of deterministic rules, beginning with recogni</context>
<context position="12517" citStr="Ardanuy and Sporleder (2014)" startWordPosition="1982" endWordPosition="1985">ubset of the gold standard names, with penalties for including wrong names. Recall uses a looser definition of matching with the aim of measuring whether a character Gj was found at all; matching is measured as a binary function that is 1 if EZ n Gj 7� 0 and 0 otherwise. Comparison Systems The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in Elson et al. (2010), BookNLP (Bamman et al., 2014), and the method described in Ardanuy and Sporleder (2014). For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character. As a baseline, we use the output of Stanford NER, where every name is considered a separate character; this baseline represents the upper-bound in recall from any system using only NER to identify character names. Experiment 1: Accuracy Table 1 shows the results for the manually-annotated and SparkNotes corpora. The Sherlock Holmes corpus presents a notable challenge due to the presence of many minor characters</context>
<context position="13992" citStr="Ardanuy and Sporleder (2014)" startWordPosition="2210" endWordPosition="2213">ate character, which caused a drop in performance. Nevertheless, our system provided the highest performance for character recognition. The Pride and Prejudice novel presents a dif771 System Sherlock Holmes Stories Pride and Prejudice The Moonstone F1 SparkNotes Precision Recall F1 Precision Recall F1 Precision Recall Recall NER Baseline 0.3327 0.6535 0.4332 0.3910 0.8356 0.5328 0.2460 0.5441 0.3388 0.6762 Elson et al. (2010) 0.3757 0.6042 0.4485 0.3100 0.5205 0.3886 0.2612 0.4931 0.3415 0.4723 BookNLP (Bamman et al., 2014) 0.6084 0.4832 0.5219 0.4855 0.5205 0.5024 0.3662 0.4706 0.4119 0.5880 Ardanuy and Sporleder (2014) 0.5744 0.4719 0.5181 0.4610 0.5108 0.4846 0.3623 0.4691 0.4088 0.5898 This work 0.5109 0.6099 0.5404 0.7245 0.7945 0.7579 0.3673 0.5735 0.4478 0.5990 Table 1: Accuracy of character detection on different portions of the two datasets. Precision Recall F1 Sherlock Holmes Stories 0.5910 0.5335 0.5608 Pride and Prejudice 0.7635 0.6879 0.7237 The Moonstone 0.3943 0.4613 0.4294 Table 2: Accuracy of proposed system without stage 7. ferent set of challenges due to multiple characters sharing the same last name or the same first name. Here, coreference resolution frequently creates incorrect links bet</context>
</contexts>
<marker>Ardanuy, Sporleder, 2014</marker>
<rawString>Mariona Coll Ardanuy and Caroline Sporleder. 2014. Structure-based clustering of novels. In Proceedings of the EACL Workshop on Computational Linguistics for Literature, pages 31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Ted Underwood</author>
<author>Noah A Smith</author>
</authors>
<title>A bayesian mixed effects model of literary character.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>370--379</pages>
<contexts>
<context position="1073" citStr="Bamman et al., 2014" startWordPosition="145" endWordPosition="148"> fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities </context>
<context position="4410" citStr="Bamman et al. (2014)" startWordPosition="658" endWordPosition="661">gs of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined through dependencies with ”sentient” lemmas from a small dictionary (including for example, say and smile), and gender, assigned through pronomial resolution and a dictionary of genderspecific honorifics. Ardanuy and Sporleder (2014) similarly use NER to identify character name mentions. These names are grouped through the application of a series of determinis</context>
<context position="12459" citStr="Bamman et al., 2014" startWordPosition="1973" endWordPosition="1976"> is maximal when the set of extracted names is a subset of the gold standard names, with penalties for including wrong names. Recall uses a looser definition of matching with the aim of measuring whether a character Gj was found at all; matching is measured as a binary function that is 1 if EZ n Gj 7� 0 and 0 otherwise. Comparison Systems The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in Elson et al. (2010), BookNLP (Bamman et al., 2014), and the method described in Ardanuy and Sporleder (2014). For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character. As a baseline, we use the output of Stanford NER, where every name is considered a separate character; this baseline represents the upper-bound in recall from any system using only NER to identify character names. Experiment 1: Accuracy Table 1 shows the results for the manually-annotated and SparkNotes corpora. The Sherlock Holmes corpus presents a nota</context>
<context position="13893" citStr="Bamman et al., 2014" startWordPosition="2196" endWordPosition="2199">ink a characters’ different referents together and hence, each name was reported as a separate character, which caused a drop in performance. Nevertheless, our system provided the highest performance for character recognition. The Pride and Prejudice novel presents a dif771 System Sherlock Holmes Stories Pride and Prejudice The Moonstone F1 SparkNotes Precision Recall F1 Precision Recall F1 Precision Recall Recall NER Baseline 0.3327 0.6535 0.4332 0.3910 0.8356 0.5328 0.2460 0.5441 0.3388 0.6762 Elson et al. (2010) 0.3757 0.6042 0.4485 0.3100 0.5205 0.3886 0.2612 0.4931 0.3415 0.4723 BookNLP (Bamman et al., 2014) 0.6084 0.4832 0.5219 0.4855 0.5205 0.5024 0.3662 0.4706 0.4119 0.5880 Ardanuy and Sporleder (2014) 0.5744 0.4719 0.5181 0.4610 0.5108 0.4846 0.3623 0.4691 0.4088 0.5898 This work 0.5109 0.6099 0.5404 0.7245 0.7945 0.7579 0.3673 0.5735 0.4478 0.5990 Table 1: Accuracy of character detection on different portions of the two datasets. Precision Recall F1 Sherlock Holmes Stories 0.5910 0.5335 0.5608 Pride and Prejudice 0.7635 0.6879 0.7237 The Moonstone 0.3943 0.4613 0.4294 Table 2: Accuracy of proposed system without stage 7. ferent set of challenges due to multiple characters sharing the same la</context>
</contexts>
<marker>Bamman, Underwood, Smith, 2014</marker>
<rawString>David Bamman, Ted Underwood, and Noah A Smith. 2014. A bayesian mixed effects model of literary character. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter T Davis</author>
<author>David K Elson</author>
<author>Judith L Klavans</author>
</authors>
<title>Methods for precise named entity matching in digital collections.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>125--127</pages>
<contexts>
<context position="2835" citStr="Davis et al. (2003)" startWordPosition="417" endWordPosition="420">1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by Elson et al. (2010). 2 Related Work Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a prob</context>
</contexts>
<marker>Davis, Elson, Klavans, 2003</marker>
<rawString>Peter T Davis, David K Elson, and Judith L Klavans. 2003. Methods for precise named entity matching in digital collections. In Proceedings of the 3rd ACM/IEEE-CS joint conference on Digital libraries, pages 125–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Eder</author>
<author>Fotis Jannidis</author>
<author>Ralf Schneider</author>
</authors>
<title>Characters in fictional worlds: Understanding imaginary beings in literature, film, and other media, volume 3. Walter de Gruyter.</title>
<date>2010</date>
<contexts>
<context position="1712" citStr="Eder et al., 2010" startWordPosition="249" endWordPosition="252">tructure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in cha</context>
</contexts>
<marker>Eder, Jannidis, Schneider, 2010</marker>
<rawString>Jens Eder, Fotis Jannidis, and Ralf Schneider. 2010. Characters in fictional worlds: Understanding imaginary beings in literature, film, and other media, volume 3. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="1155" citStr="Elson et al., 2010" startWordPosition="159" endWordPosition="162">identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examin</context>
<context position="2424" citStr="Elson et al. (2010)" startWordPosition="352" endWordPosition="355">key contributions. First, we formalize the task with evaluation criteria and offer two datasets, including a complete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by Elson et al. (2010). 2 Related Work Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all co</context>
<context position="4050" citStr="Elson et al. (2010)" startWordPosition="604" endWordPosition="607">probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net769 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined th</context>
<context position="12428" citStr="Elson et al. (2010)" startWordPosition="1968" endWordPosition="1971">|E�—G�| |E� |; simply, a match is maximal when the set of extracted names is a subset of the gold standard names, with penalties for including wrong names. Recall uses a looser definition of matching with the aim of measuring whether a character Gj was found at all; matching is measured as a binary function that is 1 if EZ n Gj 7� 0 and 0 otherwise. Comparison Systems The task of character recognition has largely been subsumed into the task of extracting the social network of novels. Therefore, three state-of-the-art systems for social network extraction were selected: the method described in Elson et al. (2010), BookNLP (Bamman et al., 2014), and the method described in Ardanuy and Sporleder (2014). For each method, we follow their procedures for identifying the characters in the social network, which produces sets of one or more aliases associated with each identified character. As a baseline, we use the output of Stanford NER, where every name is considered a separate character; this baseline represents the upper-bound in recall from any system using only NER to identify character names. Experiment 1: Accuracy Table 1 shows the results for the manually-annotated and SparkNotes corpora. The Sherloc</context>
<context position="13793" citStr="Elson et al. (2010)" startWordPosition="2181" endWordPosition="2184">r approach revealed that while many characters were extracted, the coreference resolution did not link a characters’ different referents together and hence, each name was reported as a separate character, which caused a drop in performance. Nevertheless, our system provided the highest performance for character recognition. The Pride and Prejudice novel presents a dif771 System Sherlock Holmes Stories Pride and Prejudice The Moonstone F1 SparkNotes Precision Recall F1 Precision Recall F1 Precision Recall Recall NER Baseline 0.3327 0.6535 0.4332 0.3910 0.8356 0.5328 0.2460 0.5441 0.3388 0.6762 Elson et al. (2010) 0.3757 0.6042 0.4485 0.3100 0.5205 0.3886 0.2612 0.4931 0.3415 0.4723 BookNLP (Bamman et al., 2014) 0.6084 0.4832 0.5219 0.4855 0.5205 0.5024 0.3662 0.4706 0.4119 0.5880 Ardanuy and Sporleder (2014) 0.5744 0.4719 0.5181 0.4610 0.5108 0.4846 0.3623 0.4691 0.4088 0.5898 This work 0.5109 0.6099 0.5404 0.7245 0.7945 0.7579 0.3673 0.5735 0.4478 0.5990 Table 1: Accuracy of character detection on different portions of the two datasets. Precision Recall F1 Sherlock Holmes Stories 0.5910 0.5335 0.5608 Pride and Prejudice 0.7635 0.6879 0.7237 The Moonstone 0.3943 0.4613 0.4294 Table 2: Accuracy of prop</context>
<context position="17061" citStr="Elson et al. (2010)" startWordPosition="2691" endWordPosition="2694">s is quite close to the actual: for Sherlock Holmes stories, the number of characters was estimated within 2.4 on average, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories Elson et al. (2010) analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999). Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from Elson et al. (2010) and we found no statistically-sig</context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David K Elson, Nicholas Dames, and Kathleen R McKeown. 2010. Extracting social networks from literary fiction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="8746" citStr="Fellbaum, 1998" startWordPosition="1356" endWordPosition="1357"> behave and interact in similar ways as major characters, including having dialogue. Therefore, to discover such characters, we adopt a bootstrapping technique aimed at uncovering prototypical character behaviors from the novels themselves, inspired by the semantic predicate work of Flati and Navigli (2013). The Project Gutenberg fiction corpus was dependency parsed to identify all verbs in a dependency relation with nouns, where each noun was categorized as (a) a Bennet Elizabeth Bennet Bennet Elizabeth Bennet 770 named entity (b) having its first sense in WordNet refer to an animate entity (Fellbaum, 1998), or (c) neither of the above. All verbs associated with these were then ranked according to their ratio of types (a) and (c) to identify verbs strongly associated with character-like behaviors, which avoids including the behavior of nouns in (b) which may refer to minor characters. Ultimately, 2,073 verbsand-dependency pairs with a ratio of &gt;0.25 were retained as predicates selecting for character-like entities, after limited experimental testing showed this threshold extracted sensible verbs such as “rejoice,” “accost,” and “frown.” Using this set of predicates, nouns appearing with the verb</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>Spred: Large-scale harvesting of semantic predicates.</title>
<date>2013</date>
<booktitle>In Proceedings of the The 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8439" citStr="Flati and Navigli (2013)" startWordPosition="1304" endWordPosition="1307">ph. Next, the seventh step attempts to identify characters whose names may not be recognized by NER. For example, many minor characters do not appear as named entities and instead have general role-based referents such as “the governor” or “the archbishop.” However, despite the lack of proper names, such characters behave and interact in similar ways as major characters, including having dialogue. Therefore, to discover such characters, we adopt a bootstrapping technique aimed at uncovering prototypical character behaviors from the novels themselves, inspired by the semantic predicate work of Flati and Navigli (2013). The Project Gutenberg fiction corpus was dependency parsed to identify all verbs in a dependency relation with nouns, where each noun was categorized as (a) a Bennet Elizabeth Bennet Bennet Elizabeth Bennet 770 named entity (b) having its first sense in WordNet refer to an animate entity (Fellbaum, 1998), or (c) neither of the above. All verbs associated with these were then ranked according to their ratio of types (a) and (c) to identify verbs strongly associated with character-like behaviors, which avoids including the behavior of nouns in (b) which may refer to minor characters. Ultimatel</context>
</contexts>
<marker>Flati, Navigli, 2013</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2013. Spred: Large-scale harvesting of semantic predicates. In Proceedings of the The 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua He</author>
<author>Denilson Barbosa</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Identification of speakers in novels.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1312--1320</pages>
<contexts>
<context position="3346" citStr="He et al. (2013)" startWordPosition="500" endWordPosition="503">lity in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net769 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for</context>
</contexts>
<marker>He, Barbosa, Kondrak, 2013</marker>
<rawString>Hua He, Denilson Barbosa, and Grzegorz Kondrak. 2013. Identification of speakers in novels. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312– 1320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Arun Jayannavar</author>
<author>Apoorv Agarwal</author>
<author>Melody Ju</author>
<author>Owen Rambow</author>
</authors>
<title>Validating literary theories using automatic social network extraction.</title>
<date>2015</date>
<booktitle>In Proceedings of the NAACL-2015 Workshop on Computational Linguistics for Literature.</booktitle>
<contexts>
<context position="1253" citStr="Jayannavar et al., 2015" startWordPosition="176" endWordPosition="179">er detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task </context>
<context position="17228" citStr="Jayannavar et al. (2015)" startWordPosition="2716" endWordPosition="2719">icted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories Elson et al. (2010) analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999). Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from Elson et al. (2010) and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size. Thus, our work raises questions about how these c</context>
</contexts>
<marker>Jayannavar, Agarwal, Ju, Rambow, 2015</marker>
<rawString>Prashant Arun Jayannavar, Apoorv Agarwal, Melody Ju, and Owen Rambow. 2015. Validating literary theories using automatic social network extraction. In Proceedings of the NAACL-2015 Workshop on Computational Linguistics for Literature.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Chak Yan Yeung</author>
</authors>
<title>Extracting networks of people and places from literary texts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1176" citStr="Lee and Yeung, 2012" startWordPosition="163" endWordPosition="166"> causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 Introduction How many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary cha</context>
<context position="4093" citStr="Lee and Yeung (2012)" startWordPosition="613" endWordPosition="616">acter is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction net769 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined through dependencies with ”sentient” lemmas f</context>
</contexts>
<marker>Lee, Yeung, 2012</marker>
<rawString>John Lee and Chak Yan Yeung. 2012. Extracting networks of people and places from literary texts. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2706" citStr="Manning et al., 2014" startWordPosition="394" endWordPosition="397">, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by Elson et al. (2010). 2 Related Work Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco Moretti</author>
</authors>
<date>1999</date>
<journal>Atlas of the European</journal>
<volume>novel,</volume>
<pages>1800--1900</pages>
<publisher>Verso.</publisher>
<contexts>
<context position="17192" citStr="Moretti, 1999" startWordPosition="2713" endWordPosition="2714"> Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories Elson et al. (2010) analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999). Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from Elson et al. (2010) and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text size. Thus, our wor</context>
</contexts>
<marker>Moretti, 1999</marker>
<rawString>Franco Moretti. 1999. Atlas of the European novel, 1800-1900. Verso.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Williams</author>
</authors>
<title>The country and the city, volume 423.</title>
<date>1975</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="17176" citStr="Williams, 1975" startWordPosition="2711" endWordPosition="2712">e, for Pride and Prejudice our method predicted 72 compared with 73 actual characters, and for The Moonstone our method predicted 87 compared with 78. Thus, we argue that our procedure can provide a reasonable estimate for the total number of characters. (For comparison, BookNLP, the next best system, extracted 69 and 72 characters for Pride and Prejudice and The Moonstone, respectively, and within 1.2, on average, on the Sherlock Holmes set.) Experiment 2: Literary Theories Elson et al. (2010) analyze 60 novels to computationally test literary theories for novels in urban and rural settings (Williams, 1975; Moretti, 1999). Recently, Jayannavar et al. (2015) challenged this analysis, showing their improved method for social network extraction did not support the same conclusions. While our work focuses only on character detection, we are nevertheless able to test the related hypothesis of whether the number of characters in novels with urban settings is more than those in rural. Character detection was run on the same novels from Elson et al. (2010) and we found no statistically-significant difference in the mean number of characters in urban and rural settings, even when accounting for text siz</context>
</contexts>
<marker>Williams, 1975</marker>
<rawString>Raymond Williams. 1975. The country and the city, volume 423. Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012063">
<title confidence="0.9978725">
Combining Discrete and Continuous Features for Deterministic
Transition-based Dependency Parsing
</title>
<author confidence="0.996802">
Meishan Zhang and Yue Zhang
</author>
<affiliation confidence="0.997628">
Singapore University of Technology and Design
</affiliation>
<email confidence="0.907197">
{meishan zhang, yue zhang}@sutd.edu.sg
</email>
<sectionHeader confidence="0.996223" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943428571428">
We investigate a combination of a tra-
ditional linear sparse feature model and
a multi-layer neural network model for
deterministic transition-based dependency
parsing, by integrating the sparse features
into the neural model. Correlations are
drawn between the hybrid model and pre-
vious work on integrating word embed-
ding features into a discrete linear model.
By analyzing the results of various parsers
on web-domain parsing, we show that the
integrated model is a better way to com-
bine traditional and embedding features
compared with previous methods.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998977782608696">
Transition-based parsing algorithms construct out-
put syntax trees using a sequence of shift-reduce
actions. They are attractive in computational ef-
ficiency, allowing linear time decoding with de-
terministic (Nivre, 2008) or beam-search (Zhang
and Clark, 2008) algorithms. Using rich non-local
features, transition-based parsers achieve state-of-
the-art accuracies for dependency parsing (Zhang
and Nivre, 2011; Zhang and Nivre, 2012; Bohnet
and Nivre, 2012; Choi and McCallum, 2013;
Zhang et al., 2014).
Deterministic transition-based parsers works
by making a sequence of greedy local deci-
sions (Nivre et al., 2004; Honnibal et al., 2013;
Goldberg et al., 2014; G´omez-Rodriguez and
Fern´andez-Gonz´alez, 2015). They are attractive
by very fast speeds. Traditionally, a linear model
has been used for the local action classifier. Re-
cently, Chen and Manning (2014) use a neural net-
work (NN) to replace linear models, and report im-
proved accuracies.
A contrast between a neural network model and
a linear model is shown in Figure 1 (a) and (b).
</bodyText>
<figure confidence="0.9884582">
(a) discrete linear (b) continuous NN
(eg. MaltParser) (eg. Chen and Manning (2014))
(c) Turian et al. (2010)
(d) Guo et al. (2014)
(e) this paper
</figure>
<figureCaption confidence="0.994395">
Figure 1: Five deterministic transition-based
parsers with discrete and continuous features.
</figureCaption>
<bodyText confidence="0.999958294117647">
A neural network model takes continuous vector
representations of words as inputs, which can be
pre-trained using large amounts of unlabeled data,
thus containing more information. In addition, us-
ing an extra hidden layer, a neural network is ca-
pable of learning non-linear relations between au-
tomatic features, achieving feature combinations
automatically.
Discrete manual features and continuous fea-
tures complement each other. A natural question
that arises from the contrast is whether traditional
discrete features and continuous neural features
can be integrated for better accuracies. We study
this problem by constructing the neural network
shown in Figure 1 (e), which incorporates the dis-
crete input layer of the linear model (Figure 1 (a))
into the NN model (Figure 1 (b)) by conjoining
</bodyText>
<equation confidence="0.9986176">
· · · · · ·
· · · · · ·
· · · · · · · · ·
transform
· · · · · ·
</equation>
<page confidence="0.795064">
1316
</page>
<note confidence="0.6035365">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1316–1321,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997192903225806">
it with the hidden layer. This architecture is con-
nected with previous work on incorporating word
embeddings into a linear model.
In particular, Turian et al. (2010) incorporate
word embeddings as real-valued features into a
CRF model. The architecture is shown in Figure
1(c), which can be regarded as Figure 1(e) with-
out the hidden layer. Guo et al. (2014) find that
the accuracies of Turian et al can be enhanced by
discretizing the embedding features before com-
bining them with the traditional features. They use
simple binarization and clustering to this end, find-
ing that the latter works better. The architecture is
shown in Figure 1(d). In contrast, Figure 1(e) di-
rectly combines discrete and continuous features,
replacing the hard-coded transformation function
of Guo et al. (2014) with a hidden layer, which
can be tuned by supervised training.1
We correlate and compare all the five systems
in Figure 1 empirically, using the SANCL 2012
data (Petrov and McDonald, 2012) and the stan-
dard Penn Treebank data. Results show that
the method of this paper gives higher accura-
cies than the other methods. In addition, the
method of Guo et al. (2014) gives slightly better
accuracies compared to the method of Turian et
al. (2010) for parsing task, consistent with Guo
et al’s observation on named entity recognition
(NER). We make our C++ code publicly avail-
able under GPL at https://github.com/
SUTDNLP/NNTransitionParser.
</bodyText>
<sectionHeader confidence="0.995955" genericHeader="introduction">
2 Parser
</sectionHeader>
<bodyText confidence="0.999809">
We take Chen and Manning (2014), which uses
the arc-standard transition system (Nivre, 2008).
Given an POS-tagged input sentence, it builds a
projective output y by performing a sequence of
state transition actions using greedy search. Chen
and Manning (2014) can be viewed as a neutral
alternative of MaltParser (Nivre, 2008).
Although not giving state-of-the-art accuracies,
deterministic parsing is attractive for its high pars-
ing speed (1000+ sentences per second). Our in-
corporation of discrete features does not harm the
overall speed significantly. In addition, determin-
istic parsers use standard neural classifiers, which
allows isolated study of feature influences.
</bodyText>
<footnote confidence="0.9992122">
1Yet another alternative structure is to directly combine
the two types of inputs, and replacing the input layer of (b)
using them. Wang and Manning (2013) compared this archi-
tecture with (c) using a CRF network, finding that the latter
works better for NER and chunking.
</footnote>
<sectionHeader confidence="0.986887" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9995242">
Following Chen and Manning (2014), training of
all the models using a cross-entropy loss objective
with a L2-regularization, and mini-batched Ada-
Grad (Duchi et al., 2011). We unify below the five
deterministic parsing models in Figure 1.
</bodyText>
<subsectionHeader confidence="0.99823">
3.1 Baseline linear (L)
</subsectionHeader>
<bodyText confidence="0.999748142857143">
We build a baseline linear model using logistic re-
gression (Figure 1(a)). Given a parsing state x, a
vector of discrete features Φd(x) is extracted ac-
cording to the arc-standard feature templates of
Ma et al. (2014a), which is based on the arc-eager
templates of Zhang and Nivre (2011). The score
of an action a is defined by
</bodyText>
<equation confidence="0.979255">
Score(a) = Q(Φd(x) · →− 0 d,a),
</equation>
<bodyText confidence="0.9178956">
where Q represents the sigmoid activation func-
→−
tion, 0 d is the set of model parameters, denoting
the feature weights with respect to actions, a can
be SHIFT, LEFT(l) and RIGHT(l).
</bodyText>
<subsectionHeader confidence="0.998835">
3.2 Baseline Neural (NN)
</subsectionHeader>
<bodyText confidence="0.9999725">
We take the Neural model of Chen and Manning
(2014) as another baseline (Figure 1(b)). Given
a parsing state x, the words are first mapped into
continuous vectors by using a set of pre-trained
word embeddings. Denote the mapping as Φe(x).
In addition, denote the hidden layer as Φh, and the
ith node in the hidden as Φh,i (0 ≤ i ≤ |Φh|). The
hidden layer is defined as
</bodyText>
<equation confidence="0.9885525">
Φh,i = (Φe(x) · →− 3
0 h,i) ,
</equation>
<bodyText confidence="0.999809666666667">
where 0 h is the set of parameters between the in-
put and hidden layers. The score of an action a is
defined as
</bodyText>
<equation confidence="0.9661595">
Score(a) = Q(Φh(x) · →−0 c,a),
→−
</equation>
<bodyText confidence="0.999727">
where 0 c,a is the set of parameters between the
hidden and output layers. We use the arc-standard
features Φe as Chen and Manning (2014), which
is also based on the arc-eager templates of Zhang
and Nivre (2011), similar to those of the baseline
model L.
</bodyText>
<subsectionHeader confidence="0.8490505">
3.3 Linear model with real-valued
embeddings (Turian)
</subsectionHeader>
<bodyText confidence="0.999785666666667">
We apply the method of Turian et al. (2010), com-
bining real-valued embeddings with discrete fea-
tures in the linear baseline (Figure 1(c)). Given a
</bodyText>
<page confidence="0.904971">
1317
</page>
<bodyText confidence="0.724625">
state x, the score of an action a is defined as
</bodyText>
<equation confidence="0.943834">
Score(a) = σ �(bd(x) ⊕ be(x))
�
→−
9 c,a) ,
</equation>
<bodyText confidence="0.975267">
where ⊕ is the vector concatenation operator.
</bodyText>
<subsectionHeader confidence="0.572717">
3.4 Linear model with transformed
embeddings (Guo)
</subsectionHeader>
<bodyText confidence="0.9999695">
We apply the method of Guo et al. (2014), com-
bining embeddings into the linear baseline by first
transforming into discrete values. Given a state x,
the score of an action is defined as
</bodyText>
<equation confidence="0.99827375">
Score(a) = σ ( (bd(x) ⊕ d(be(x) ))
�
→−
9 c,a) ,
</equation>
<bodyText confidence="0.9993895">
where d is a transformation function from real-
value to binary features. We use clustering of em-
beddings for d as it gives better performances ac-
cording to Guo et al. (2014). Following Guo et
al. (2014), we use compounded clusters learnt by
K-means algorithm of different granularities.
</bodyText>
<subsectionHeader confidence="0.9937805">
3.5 Directly combining linear and neural
features (This)
</subsectionHeader>
<bodyText confidence="0.999684666666667">
We directly combine linear and neural features
(Figure 1(e)). Given a state x, the score of an ac-
tion is defined as
</bodyText>
<equation confidence="0.985813333333333">
Score(a) = σ( (bd(x) ⊕ bh(x))
�
→−9 c,a) ,
</equation>
<bodyText confidence="0.999944">
where bh is the same as the NN baseline. Note
that like d in Guo, bh is also a function that trans-
forms embeddings be. The main difference is that
it can be tuned in supervised training.
</bodyText>
<sectionHeader confidence="0.971098" genericHeader="method">
4 Web Domain Experiments
</sectionHeader>
<subsectionHeader confidence="0.997154">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.998160181818182">
We perform experiments on the SANCL 2012 web
data (Petrov and McDonald, 2012), using the Wall
Street Journal (WSJ) training corpus to train the
models and the WSJ development corpus to tune
parameters. We clean the web domain texts fol-
lowing the method of Ma et al. (2014b). Au-
tomatic POS tags are produced by using a CRF
model trained on the WSJ training corpus. The
POS tags are assigned automatically on the train-
ing corpus by ten-fold jackknifing. Table 1 shows
the corpus details.
</bodyText>
<table confidence="0.999789571428571">
Domain #Sent #Word TA
WSJ-train 30,060 731,678 97.03
WSJ-dev 1,336 32,092 96.88
WSJ-test 1,640 35,590 97.51
answers 1,744 28,823 91.93
newsgroups 1,195 20,651 93.75
reviews 1,906 28,086 92.66
</table>
<tableCaption confidence="0.7955905">
Table 1: Corpus statistics of our experiments,
where TA denotes POS tagging accuracy.
</tableCaption>
<figureCaption confidence="0.994685">
Figure 2: Dev results on fine-tuning (UAS).
</figureCaption>
<bodyText confidence="0.9997306">
Following Chen and Manning (2014), we use
the pre-trained word embedding released by Col-
lobert et al. (2011), and set h = 200 for the hidden
layer size, A = 10−8 for L2 regularization, and
α = 0.01 for the initial learning rate of Adagrad.
</bodyText>
<subsectionHeader confidence="0.989843">
4.2 Development Results
</subsectionHeader>
<bodyText confidence="0.999266875">
Fine-tuning of embeddings. Chen and Man-
ning (2014) fine-tune word embeddings in su-
pervised training, consistent with Socher et al.
(2013). Intuitively, fine-tuning embeddings allows
in-vocabulary words to join the parameter space,
thereby giving better fitting to in-domain data.
However, it also forfeits the benefit of large-scale
pre-training, because out-of-vocabulary (OOV)
words do not have their embeddings fine-tuned.
In this sense, the method of Chen and Manning
resembles a traditional supervised sparse linear
model, which can be weak on OOV.
On the other hand, the semi-supervised learning
methods such as Turian et al. (2010) and Guo et
al. (2014), do not fine-tune the word embeddings.
Embeddings are taken as inputs rather than model
</bodyText>
<figure confidence="0.995978393939394">
NN Turian This
-T +T
82
81
80
79
NN Turian This
-T +T
91
90
89
88
NN Turian This
-T +T
83
82
81
80
NN Turian This
-T +T
(c) newsgroups (d) reviews
82
81
80
79
(a) WSJ
(b) answers
· (
−→9 d,a ⊕
· (
−→9 d,a ⊕
· (
−→9 d,a ⊕
</figure>
<page confidence="0.865462">
1318
</page>
<table confidence="0.999219666666667">
Model WSJ answers newsgroups reviews
UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE
L 88.19 86.16 83.72 —– 79.30 74.24 68.43 —– 82.55 79.06 69.07 —– 80.77 76.16 72.20 —–
NN 89.81 87.83 84.94 84.94 79.27 74.30 69.18 69.18 83.71 80.35 69.60 69.60 81.63 77.22 72.04 72.04
Turian 89.17 87.21 84.13 91.35 79.57 74.57 69.60 54.21 82.89 79.65 68.48 52.63 81.33 77.04 72.30 55.03
Guo 89.33 87.21 83.82 90.83 79.32 74.22 67.36 51.76 82.75 79.31 68.18 55.06 81.87 77.25 73.03 56.80
This 90.61 88.68 88.00 93.77 80.08 75.18 69.97 54.21 84.64 81.35 69.66 53.44 82.53 78.15 73.39 57.20
ZPar-local 88.95 86.90 84.63 —– 78.98 73.81 68.15 —– 82.43 79.01 67.30 —– 81.21 76.45 70.38 —–
C&amp;M(2014) 89.56 87.55 79.15 79.15 79.82 74.63 67.78 67.78 83.39 79.72 67.95 67.95 81.60 76.91 68.83 68.83
</table>
<tableCaption confidence="0.999132">
Table 2: Main results on SANCL. All systems are deterministic.
</tableCaption>
<bodyText confidence="0.999880846153846">
parameters. Therefore, such methods can expect
better cross-domain accuracies.
We empirically compare the models NN, Turian
and This by fine-tuning (+T) and not fine-tuning
(-T) word embeddings, and the results are shown
in Figure 2. As expected, the baseline NN model
gives better accuracies on WSJ with fine-tuning,
but worse cross-domain accuracies. Interestingly,
our combined model gives consistently better ac-
curacies with fine-tuning. We attribute this to
the use of sparse discrete features, which allows
the model to benefit from large-scale pre-trained
embeddings without sacrificing in-domain perfor-
mance. The observation on Turian is similar. For
the final experiments, we apply fine-tuning on the
NN model, but not to the Turian and This. Note
also tat for all experiments, the POS and label em-
bedding features of Chen and Manning (2014) are
fine-tuned, consistent with their original method.
Dropout rate. We test the effect of dropout (Hin-
ton et al., 2012) during training, using a default ra-
tio of 0.5 according to Chen and Manning (2014).
In our experiments, we find that the dense NN
model and our combined model achieve better per-
formances by using dropout, but the other models
do not benefit from dropout.
</bodyText>
<subsectionHeader confidence="0.998879">
4.3 Final Results
</subsectionHeader>
<bodyText confidence="0.998962714285714">
The final results across web domains are shown
in Table 2. Our logistic regression linear parser
and re-implementation of Chen and Manning
(2014) give comparable accuracies to the percep-
tron ZPar2 and Stanford NN Parser3, respectively.
It can be seen from the table that both Turian
and Guo4 outperform L by incorporating embed-
</bodyText>
<footnote confidence="0.9969495">
2https://sourceforge.net/projects/zpar/, version 0.7.
3http://nlp.stanford.edu/software/nndep.shtml
4We compound six clusters of granularities 500, 1000,
1500, 2000, 2500, 3000.
</footnote>
<bodyText confidence="0.996921277777778">
ding features. Guo gives overall higher improve-
ments, consistent with the observation of Guo et
al. (2014) on NER. Our methods gives signifi-
cantly5 better results compared with Turian and
Guo, thanks to the extra hidden layer.
Our OOV performance is higher than NN,
because the embeddings of OOV words are
not tuned, and hence the model can handle
them effectively. Interestingly, NN gives higher
accuracies on web domain out-of-embedding-
vocabulary (OOE) words, out of which 54% are
in-vocabulary.
Note that the accuracies of our parsers are lower
than the best systems in the SANCL shared task,
which use ensemble models. Our parser enjoys the
fast speed of deterministic parsers, and in partic-
ular the baseline NN parser (Chen and Manning,
2014).
</bodyText>
<sectionHeader confidence="0.996489" genericHeader="method">
5 WSJ Experiments
</sectionHeader>
<bodyText confidence="0.9999791875">
For comparison with related work, we conduct ex-
periments on Penn Treebank corpus also. We use
the WSJ sections 2-21 for training, section 22 for
development and section 23 for testing. WSJ con-
stituent trees are converted to dependency trees us-
ing Penn2Malt6. We use auto POS tags consistent
with previous work. The ZPar POS-tagger is used
to assign POS tags. Ten-fold jackknifing is per-
formed on the training data to assign POS auto-
matically. For this set of experiments, the parser
hyper-parameters are taken directly from the best
settings in the Web Domain experiments.
The results are shown in Table 3, together with
some state-of-the-art deterministic parsers. Com-
paring the L, NN and This models, the observa-
tions are consistent with the web domain.
</bodyText>
<footnote confidence="0.999962">
5The p-values are below 0.01 using pairwise t-test.
6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
</footnote>
<page confidence="0.961894">
1319
</page>
<table confidence="0.988979888888889">
System UAS LAS
L 89.36 88.33
NN 91.15 90.04
This 91.80 90.68
ZPar-local 89.94 88.92
Ma et al. (2014a) 90.38 –
Chen and Manning (2014) 91.17 89.99
Honnibal et al. (2013) 91.30 90.00
Ma et al. (2014a)* 91.32 –
</table>
<tableCaption confidence="0.9442895">
Table 3: Main results on WSJ. All systems are de-
terministic.
</tableCaption>
<bodyText confidence="0.999518846153846">
Our combined parser gives accuracies competi-
tive to state-of-the-art deterministic parsers in the
literature. In particular, the method of Chen and
Manning (2014) is the same as our NN baseline.
Note that Zhou et al. (2015) reports a UAS of
91.47% by this parser, which is higher than the
results we obtained. The main results include the
use of different batch size during, while Zhou et
al. (2015) used a batch size of 100,000, we used a
batch size of 10,000 in all experiments. Honnibal
et al. (2013) applies dynamic oracle to the deter-
ministic transition-based parsing, giving a UAS of
91.30%. Ma et al. (2014a) is similar to ZPar local,
except that they use the arc-standard transitions,
while ZPar-local is based on arc-eager transitions.
Ma et al. (2014a)* uses a special method to process
punctuations, leading to about 1% UAS improve-
ments over the vanilla system.
Recently, Dyer et al. (2015) proposed a de-
terministic transition-based parser using LSTM,
which gives a UAS of 93.1% on Stanford conver-
sion of the Penn Treebank. Their work shows that
more sophisticated neural network structures with
long term memories can significantly improve the
accuracy over local classifiers. Their work is or-
thogonal to ours.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999977666666666">
As discussed in the introduction, our work is re-
lated to previous work on integrating word embed-
dings into discrete models (Turian et al., 2010; Yu
et al., 2013; Guo et al., 2014). Along this line,
there has also been work that uses a neural net-
work to automatically vectorize the structures of
a sentence, and then taking the resulting vector
as features in a linear NLP model (Socher et al.,
2012; Tang et al., 2014; Yu et al., 2015). Our re-
sults show that the use of a hidden neural layer
gives superior results compared with both direct
integration and integration via a hard-coded trans-
formation function (e.g binarization or clustering).
There has been recent work integrating contin-
uous and discrete features for the task of POS
tagging (Ma et al., 2014b; Tsuboi, 2014). Both
models have essentially the same structure as our
model. In contrast to their work, we systemati-
cally compare various ways to integrate discrete
and continuous features, for the dependency pars-
ing task. Our model is also different from Ma et
al. (2014b) in the hidden layer. While they use a
form of restricted Boltzmann machine to pre-train
the embeddings and hidden layer from large-scale
ngrams, we fully rely on supervised learning to
train complex feature combinations.
Wang and Manning (2013) consider integrat-
ing embeddings and discrete features into a neu-
ral CRF. They show that combined neural and dis-
crete features work better without a hidden layer
(i.e. Turian et al. (2010)). They argue that non-
linear structures do not work well with high di-
mensional features. We find that using a hid-
den layer specifically for embedding features gives
better results compared with using no hidden lay-
ers.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999939">
We studied the combination of discrete and con-
tinuous features for deterministic transition-based
dependency parsing, comparing several methods
to incorporate word embeddings and traditional
sparse features in the same model. Experiments
on both in-domain and cross-domain parsing show
that directly adding sparse features into a neural
network gives higher accuracies compared with all
previous methods to incorporate word embeddings
into a traditional sparse linear model.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999618">
We thank the anonymous reviewers for their con-
structive comments, which helped to improve the
paper. This work is supported by the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
</bodyText>
<sectionHeader confidence="0.989901" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.6161215">
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
</bodyText>
<page confidence="0.970284">
1320
</page>
<reference confidence="0.994229490566037">
labeled non-projective dependency parsing. In
EMNLP-CONLL, pages 1455–1465.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In ACL, pages 1052–1062.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In ACL, pages 334–343.
Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Asso-
ciation for Computational Linguistics, 2:119–130.
Carlos G´omez-Rodriguez and Daniel Fern´andez-
Gonz´alez. 2015. An efficient dynamic oracle
for unrestricted non-projective parsing. In ACL-
IJCNLP, pages 256–261.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of the
2014 Conference on EMNLP, pages 110–120.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Matthew Honnibal, Yoav Goldberg, and Mark Johnson.
2013. A non-monotonic arc-eager transition system
for dependency parsing. In Proceedings of the 17th
CONLL, pages 163–172.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014a. Punctu-
ation processing for projective dependency parsing.
In ACL (Volume 2: Short Papers), pages 791–796.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014b. Tagging
the web: Building a robust web tagger with neural
network. In ACL, pages 144–154.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on EMNLP-
CONLL, pages 1201–1211.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In ACL, pages 455–465.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In ACL, pages 1555–1565.
Yuta Tsuboi. 2014. Neural networks leverage corpus-
wide information for part-of-speech tagging. In Pro-
ceedings of the 2014 EMNLP, pages 938–950.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th ACL, pages 384–394.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence
labeling. In IJCNLP, pages 1285–1291.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In NAACL, pages 563–
568.
Mo Yu, Matthew R Gormley, and Mark Dredze.
2015. Combining word embeddings and feature
embeddings for fine-grained relation extraction. In
NAACL.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP, pages 562–571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188–193.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In COLING
2012: Posters, pages 1391–1400.
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Jaakkola, and Amir Globerson. 2014. Steps to ex-
cellence: Simple inference with refined scoring of
dependency trees. In ACL, pages 197–207.
Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A neural probabilistic structured-
prediction model for transition-based dependency
parsing. In ACL, pages 1213–1222.
</reference>
<page confidence="0.992931">
1321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.228782">
<title confidence="0.998084">Combining Discrete and Continuous Features for Transition-based Dependency Parsing</title>
<author confidence="0.977151">Zhang</author>
<affiliation confidence="0.987934">Singapore University of Technology and</affiliation>
<address confidence="0.248337">zhang, yue</address>
<abstract confidence="0.9959582">We investigate a combination of a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>labeled non-projective dependency parsing.</title>
<booktitle>In EMNLP-CONLL,</booktitle>
<pages>1455--1465</pages>
<marker></marker>
<rawString>labeled non-projective dependency parsing. In EMNLP-CONLL, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>740--750</pages>
<contexts>
<context position="1652" citStr="Chen and Manning (2014)" startWordPosition="233" endWordPosition="236">ng and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information.</context>
<context position="4603" citStr="Chen and Manning (2014)" startWordPosition="719" endWordPosition="722">training.1 We correlate and compare all the five systems in Figure 1 empirically, using the SANCL 2012 data (Petrov and McDonald, 2012) and the standard Penn Treebank data. Results show that the method of this paper gives higher accuracies than the other methods. In addition, the method of Guo et al. (2014) gives slightly better accuracies compared to the method of Turian et al. (2010) for parsing task, consistent with Guo et al’s observation on named entity recognition (NER). We make our C++ code publicly available under GPL at https://github.com/ SUTDNLP/NNTransitionParser. 2 Parser We take Chen and Manning (2014), which uses the arc-standard transition system (Nivre, 2008). Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. Chen and Manning (2014) can be viewed as a neutral alternative of MaltParser (Nivre, 2008). Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second). Our incorporation of discrete features does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which a</context>
<context position="6407" citStr="Chen and Manning (2014)" startWordPosition="1012" endWordPosition="1015">r (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by Score(a) = Q(Φd(x) · →− 0 d,a), where Q represents the sigmoid activation func→− tion, 0 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe(x). In addition, denote the hidden layer as Φh, and the ith node in the hidden as Φh,i (0 ≤ i ≤ |Φh|). The hidden layer is defined as Φh,i = (Φe(x) · →− 3 0 h,i) , where 0 h is the set of parameters between the input and hidden layers. The score of an action a is defined as Score(a) = Q(Φh(x) · →−0 c,a), →− where 0 c,a is the set of parameters between the hidden and output layers. We use the arc-standard features</context>
<context position="9352" citStr="Chen and Manning (2014)" startWordPosition="1542" endWordPosition="1545"> domain texts following the method of Ma et al. (2014b). Automatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Table 1 shows the corpus details. Domain #Sent #Word TA WSJ-train 30,060 731,678 97.03 WSJ-dev 1,336 32,092 96.88 WSJ-test 1,640 35,590 97.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy. Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word embedding released by Collobert et al. (2011), and set h = 200 for the hidden layer size, A = 10−8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabul</context>
<context position="12262" citStr="Chen and Manning (2014)" startWordPosition="2038" endWordPosition="2041">ure 2. As expected, the baseline NN model gives better accuracies on WSJ with fine-tuning, but worse cross-domain accuracies. Interestingly, our combined model gives consistently better accuracies with fine-tuning. We attribute this to the use of sparse discrete features, which allows the model to benefit from large-scale pre-trained embeddings without sacrificing in-domain performance. The observation on Turian is similar. For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of Chen and Manning (2014) are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to Chen and Manning (2014). In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. 4.3 Final Results The final results across web domains are shown in Table 2. Our logistic regression linear parser and re-implementation of Chen and Manning (2014) give comparable accuracies to the perceptron ZPar2 and Stan</context>
<context position="13909" citStr="Chen and Manning, 2014" startWordPosition="2297" endWordPosition="2300">ds gives significantly5 better results compared with Turian and Guo, thanks to the extra hidden layer. Our OOV performance is higher than NN, because the embeddings of OOV words are not tuned, and hence the model can handle them effectively. Interestingly, NN gives higher accuracies on web domain out-of-embeddingvocabulary (OOE) words, out of which 54% are in-vocabulary. Note that the accuracies of our parsers are lower than the best systems in the SANCL shared task, which use ensemble models. Our parser enjoys the fast speed of deterministic parsers, and in particular the baseline NN parser (Chen and Manning, 2014). 5 WSJ Experiments For comparison with related work, we conduct experiments on Penn Treebank corpus also. We use the WSJ sections 2-21 for training, section 22 for development and section 23 for testing. WSJ constituent trees are converted to dependency trees using Penn2Malt6. We use auto POS tags consistent with previous work. The ZPar POS-tagger is used to assign POS tags. Ten-fold jackknifing is performed on the training data to assign POS automatically. For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The res</context>
<context position="15230" citStr="Chen and Manning (2014)" startWordPosition="2509" endWordPosition="2512">e L, NN and This models, the observations are consistent with the web domain. 5The p-values are below 0.01 using pairwise t-test. 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 1319 System UAS LAS L 89.36 88.33 NN 91.15 90.04 This 91.80 90.68 ZPar-local 89.94 88.92 Ma et al. (2014a) 90.38 – Chen and Manning (2014) 91.17 89.99 Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a)* 91.32 – Table 3: Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments. Honnibal et al. (2013) applies dynamic oracle to the deterministic transition-based parsing, giving a UAS of 91.30%. Ma et al. (2014a) is similar to ZPar local, except that they use the arc-standard transitions, while ZPar-local is based on arc-eager transitions. Ma et al. (2014a)</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1052--1062</pages>
<contexts>
<context position="1269" citStr="Choi and McCallum, 2013" startWordPosition="175" endWordPosition="178"> we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In ACL, pages 1052–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="9427" citStr="Collobert et al. (2011)" startWordPosition="1554" endWordPosition="1558"> are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Table 1 shows the corpus details. Domain #Sent #Word TA WSJ-train 30,060 731,678 97.03 WSJ-dev 1,336 32,092 96.88 WSJ-test 1,640 35,590 97.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy. Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word embedding released by Collobert et al. (2011), and set h = 200 for the hidden layer size, A = 10−8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="5698" citStr="Duchi et al., 2011" startWordPosition="886" endWordPosition="889">s does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 1Yet another alternative structure is to directly combine the two types of inputs, and replacing the input layer of (b) using them. Wang and Manning (2013) compared this architecture with (c) using a CRF network, finding that the latter works better for NER and chunking. 3 Models Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by Score(a) = Q(Φd(x) · →− 0 d,a), where Q represents the sigmoid activation func→− tion, 0 d is the set of model parameters, denoting the feature weights with respect to actions, a can</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory.</title>
<date>2015</date>
<booktitle>In ACL,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="15969" citStr="Dyer et al. (2015)" startWordPosition="2638" endWordPosition="2641">n the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments. Honnibal et al. (2013) applies dynamic oracle to the deterministic transition-based parsing, giving a UAS of 91.30%. Ma et al. (2014a) is similar to ZPar local, except that they use the arc-standard transitions, while ZPar-local is based on arc-eager transitions. Ma et al. (2014a)* uses a special method to process punctuations, leading to about 1% UAS improvements over the vanilla system. Recently, Dyer et al. (2015) proposed a deterministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conversion of the Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatica</context>
</contexts>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In ACL, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Francesco Sartorio</author>
<author>Giorgio Satta</author>
</authors>
<title>A tabular method for dynamic oracles in transition-based parsing.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--119</pages>
<contexts>
<context position="1449" citStr="Goldberg et al., 2014" startWordPosition="204" endWordPosition="207">nstruct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five deterministic transition-based parsers with discrete an</context>
</contexts>
<marker>Goldberg, Sartorio, Satta, 2014</marker>
<rawString>Yoav Goldberg, Francesco Sartorio, and Giorgio Satta. 2014. A tabular method for dynamic oracles in transition-based parsing. Transactions of the Association for Computational Linguistics, 2:119–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Daniel Fern´andezGonz´alez</author>
</authors>
<title>An efficient dynamic oracle for unrestricted non-projective parsing.</title>
<date>2015</date>
<booktitle>In ACLIJCNLP,</booktitle>
<pages>256--261</pages>
<marker>G´omez-Rodriguez, Fern´andezGonz´alez, 2015</marker>
<rawString>Carlos G´omez-Rodriguez and Daniel Fern´andezGonz´alez. 2015. An efficient dynamic oracle for unrestricted non-projective parsing. In ACLIJCNLP, pages 256–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on EMNLP,</booktitle>
<pages>110--120</pages>
<contexts>
<context position="1963" citStr="Guo et al. (2014)" startWordPosition="290" endWordPosition="293"> a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically. Discrete manual features and continuous features complement each other. A natural question that arises from the contrast is whether t</context>
<context position="3492" citStr="Guo et al. (2014)" startWordPosition="539" endWordPosition="542"> · · · · · · · · · · · · · · · · · · transform · · · · · · 1316 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1316–1321, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. it with the hidden layer. This architecture is connected with previous work on incorporating word embeddings into a linear model. In particular, Turian et al. (2010) incorporate word embeddings as real-valued features into a CRF model. The architecture is shown in Figure 1(c), which can be regarded as Figure 1(e) without the hidden layer. Guo et al. (2014) find that the accuracies of Turian et al can be enhanced by discretizing the embedding features before combining them with the traditional features. They use simple binarization and clustering to this end, finding that the latter works better. The architecture is shown in Figure 1(d). In contrast, Figure 1(e) directly combines discrete and continuous features, replacing the hard-coded transformation function of Guo et al. (2014) with a hidden layer, which can be tuned by supervised training.1 We correlate and compare all the five systems in Figure 1 empirically, using the SANCL 2012 data (Pet</context>
<context position="7590" citStr="Guo et al. (2014)" startWordPosition="1235" endWordPosition="1238">. We use the arc-standard features Φe as Chen and Manning (2014), which is also based on the arc-eager templates of Zhang and Nivre (2011), similar to those of the baseline model L. 3.3 Linear model with real-valued embeddings (Turian) We apply the method of Turian et al. (2010), combining real-valued embeddings with discrete features in the linear baseline (Figure 1(c)). Given a 1317 state x, the score of an action a is defined as Score(a) = σ �(bd(x) ⊕ be(x)) � →− 9 c,a) , where ⊕ is the vector concatenation operator. 3.4 Linear model with transformed embeddings (Guo) We apply the method of Guo et al. (2014), combining embeddings into the linear baseline by first transforming into discrete values. Given a state x, the score of an action is defined as Score(a) = σ ( (bd(x) ⊕ d(be(x) )) � →− 9 c,a) , where d is a transformation function from realvalue to binary features. We use clustering of embeddings for d as it gives better performances according to Guo et al. (2014). Following Guo et al. (2014), we use compounded clusters learnt by K-means algorithm of different granularities. 3.5 Directly combining linear and neural features (This) We directly combine linear and neural features (Figure 1(e)). </context>
<context position="10243" citStr="Guo et al. (2014)" startWordPosition="1684" endWordPosition="1687"> (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather than model NN Turian This -T +T 82 81 80 79 NN Turian This -T +T 91 90 89 88 NN Turian This -T +T 83 82 81 80 NN Turian This -T +T (c) newsgroups (d) reviews 82 81 80 79 (a) WSJ (b) answers · ( −→9 d,a ⊕ · ( −→9 d,a ⊕ · ( −→9 d,a ⊕ 1318 Model WSJ answers newsgroups reviews UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE L 88.19 86.16 83.72 —– 79.30 74.24 68.43 —– 82.55 79.06 69.07 —– 80.77 76.16 72.20 —– NN 89.81 87.83 84.94 84.94 79.27 74.30 69.18 69.18 83.71 80.35 69.60 69.60 81.63 77.22 72.04 72.04</context>
<context position="13268" citStr="Guo et al. (2014)" startWordPosition="2192" endWordPosition="2195">sults The final results across web domains are shown in Table 2. Our logistic regression linear parser and re-implementation of Chen and Manning (2014) give comparable accuracies to the perceptron ZPar2 and Stanford NN Parser3, respectively. It can be seen from the table that both Turian and Guo4 outperform L by incorporating embed2https://sourceforge.net/projects/zpar/, version 0.7. 3http://nlp.stanford.edu/software/nndep.shtml 4We compound six clusters of granularities 500, 1000, 1500, 2000, 2500, 3000. ding features. Guo gives overall higher improvements, consistent with the observation of Guo et al. (2014) on NER. Our methods gives significantly5 better results compared with Turian and Guo, thanks to the extra hidden layer. Our OOV performance is higher than NN, because the embeddings of OOV words are not tuned, and hence the model can handle them effectively. Interestingly, NN gives higher accuracies on web domain out-of-embeddingvocabulary (OOE) words, out of which 54% are in-vocabulary. Note that the accuracies of our parsers are lower than the best systems in the SANCL shared task, which use ensemble models. Our parser enjoys the fast speed of deterministic parsers, and in particular the ba</context>
<context position="16485" citStr="Guo et al., 2014" startWordPosition="2724" endWordPosition="2727">ctuations, leading to about 1% UAS improvements over the vanilla system. Recently, Dyer et al. (2015) proposed a deterministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conversion of the Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). B</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proceedings of the 2014 Conference on EMNLP, pages 110–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="12383" citStr="Hinton et al., 2012" startWordPosition="2057" endWordPosition="2061">. Interestingly, our combined model gives consistently better accuracies with fine-tuning. We attribute this to the use of sparse discrete features, which allows the model to benefit from large-scale pre-trained embeddings without sacrificing in-domain performance. The observation on Turian is similar. For the final experiments, we apply fine-tuning on the NN model, but not to the Turian and This. Note also tat for all experiments, the POS and label embedding features of Chen and Manning (2014) are fine-tuned, consistent with their original method. Dropout rate. We test the effect of dropout (Hinton et al., 2012) during training, using a default ratio of 0.5 according to Chen and Manning (2014). In our experiments, we find that the dense NN model and our combined model achieve better performances by using dropout, but the other models do not benefit from dropout. 4.3 Final Results The final results across web domains are shown in Table 2. Our logistic regression linear parser and re-implementation of Chen and Manning (2014) give comparable accuracies to the perceptron ZPar2 and Stanford NN Parser3, respectively. It can be seen from the table that both Turian and Guo4 outperform L by incorporating embe</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Yoav Goldberg</author>
<author>Mark Johnson</author>
</authors>
<title>A non-monotonic arc-eager transition system for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 17th CONLL,</booktitle>
<pages>163--172</pages>
<contexts>
<context position="1426" citStr="Honnibal et al., 2013" startWordPosition="200" endWordPosition="203">d parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five deterministic transition-based p</context>
<context position="14967" citStr="Honnibal et al. (2013)" startWordPosition="2467" endWordPosition="2470"> assign POS automatically. For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The results are shown in Table 3, together with some state-of-the-art deterministic parsers. Comparing the L, NN and This models, the observations are consistent with the web domain. 5The p-values are below 0.01 using pairwise t-test. 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 1319 System UAS LAS L 89.36 88.33 NN 91.15 90.04 This 91.80 90.68 ZPar-local 89.94 88.92 Ma et al. (2014a) 90.38 – Chen and Manning (2014) 91.17 89.99 Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a)* 91.32 – Table 3: Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments. Honnibal et al. (2</context>
</contexts>
<marker>Honnibal, Goldberg, Johnson, 2013</marker>
<rawString>Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system for dependency parsing. In Proceedings of the 17th CONLL, pages 163–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Yue Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Punctuation processing for projective dependency parsing.</title>
<date>2014</date>
<booktitle>In ACL (Volume 2: Short Papers),</booktitle>
<pages>791--796</pages>
<contexts>
<context position="6004" citStr="Ma et al. (2014" startWordPosition="938" endWordPosition="941">anning (2013) compared this architecture with (c) using a CRF network, finding that the latter works better for NER and chunking. 3 Models Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by Score(a) = Q(Φd(x) · →− 0 d,a), where Q represents the sigmoid activation func→− tion, 0 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe(x). In additi</context>
<context position="8782" citStr="Ma et al. (2014" startWordPosition="1452" endWordPosition="1455">eatures (Figure 1(e)). Given a state x, the score of an action is defined as Score(a) = σ( (bd(x) ⊕ bh(x)) � →−9 c,a) , where bh is the same as the NN baseline. Note that like d in Guo, bh is also a function that transforms embeddings be. The main difference is that it can be tuned in supervised training. 4 Web Domain Experiments 4.1 Setting We perform experiments on the SANCL 2012 web data (Petrov and McDonald, 2012), using the Wall Street Journal (WSJ) training corpus to train the models and the WSJ development corpus to tune parameters. We clean the web domain texts following the method of Ma et al. (2014b). Automatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Table 1 shows the corpus details. Domain #Sent #Word TA WSJ-train 30,060 731,678 97.03 WSJ-dev 1,336 32,092 96.88 WSJ-test 1,640 35,590 97.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy. Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word </context>
<context position="14898" citStr="Ma et al. (2014" startWordPosition="2455" endWordPosition="2458">ags. Ten-fold jackknifing is performed on the training data to assign POS automatically. For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The results are shown in Table 3, together with some state-of-the-art deterministic parsers. Comparing the L, NN and This models, the observations are consistent with the web domain. 5The p-values are below 0.01 using pairwise t-test. 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 1319 System UAS LAS L 89.36 88.33 NN 91.15 90.04 This 91.80 90.68 ZPar-local 89.94 88.92 Ma et al. (2014a) 90.38 – Chen and Manning (2014) 91.17 89.99 Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a)* 91.32 – Table 3: Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, </context>
<context position="17066" citStr="Ma et al., 2014" startWordPosition="2824" endWordPosition="2827">Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They </context>
</contexts>
<marker>Ma, Zhang, Zhu, 2014</marker>
<rawString>Ji Ma, Yue Zhang, and Jingbo Zhu. 2014a. Punctuation processing for projective dependency parsing. In ACL (Volume 2: Short Papers), pages 791–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Yue Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Tagging the web: Building a robust web tagger with neural network. In</title>
<date>2014</date>
<booktitle>ACL,</booktitle>
<pages>144--154</pages>
<contexts>
<context position="6004" citStr="Ma et al. (2014" startWordPosition="938" endWordPosition="941">anning (2013) compared this architecture with (c) using a CRF network, finding that the latter works better for NER and chunking. 3 Models Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by Score(a) = Q(Φd(x) · →− 0 d,a), where Q represents the sigmoid activation func→− tion, 0 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe(x). In additi</context>
<context position="8782" citStr="Ma et al. (2014" startWordPosition="1452" endWordPosition="1455">eatures (Figure 1(e)). Given a state x, the score of an action is defined as Score(a) = σ( (bd(x) ⊕ bh(x)) � →−9 c,a) , where bh is the same as the NN baseline. Note that like d in Guo, bh is also a function that transforms embeddings be. The main difference is that it can be tuned in supervised training. 4 Web Domain Experiments 4.1 Setting We perform experiments on the SANCL 2012 web data (Petrov and McDonald, 2012), using the Wall Street Journal (WSJ) training corpus to train the models and the WSJ development corpus to tune parameters. We clean the web domain texts following the method of Ma et al. (2014b). Automatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Table 1 shows the corpus details. Domain #Sent #Word TA WSJ-train 30,060 731,678 97.03 WSJ-dev 1,336 32,092 96.88 WSJ-test 1,640 35,590 97.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy. Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word </context>
<context position="14898" citStr="Ma et al. (2014" startWordPosition="2455" endWordPosition="2458">ags. Ten-fold jackknifing is performed on the training data to assign POS automatically. For this set of experiments, the parser hyper-parameters are taken directly from the best settings in the Web Domain experiments. The results are shown in Table 3, together with some state-of-the-art deterministic parsers. Comparing the L, NN and This models, the observations are consistent with the web domain. 5The p-values are below 0.01 using pairwise t-test. 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 1319 System UAS LAS L 89.36 88.33 NN 91.15 90.04 This 91.80 90.68 ZPar-local 89.94 88.92 Ma et al. (2014a) 90.38 – Chen and Manning (2014) 91.17 89.99 Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a)* 91.32 – Table 3: Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, </context>
<context position="17066" citStr="Ma et al., 2014" startWordPosition="2824" endWordPosition="2827">Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They </context>
</contexts>
<marker>Ma, Zhang, Zhu, 2014</marker>
<rawString>Ji Ma, Yue Zhang, and Jingbo Zhu. 2014b. Tagging the web: Building a robust web tagger with neural network. In ACL, pages 144–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1403" citStr="Nivre et al., 2004" startWordPosition="196" endWordPosition="199">tion Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five determini</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1009" citStr="Nivre, 2008" startWordPosition="141" endWordPosition="142">ting the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action cl</context>
<context position="4664" citStr="Nivre, 2008" startWordPosition="729" endWordPosition="730">irically, using the SANCL 2012 data (Petrov and McDonald, 2012) and the standard Penn Treebank data. Results show that the method of this paper gives higher accuracies than the other methods. In addition, the method of Guo et al. (2014) gives slightly better accuracies compared to the method of Turian et al. (2010) for parsing task, consistent with Guo et al’s observation on named entity recognition (NER). We make our C++ code publicly available under GPL at https://github.com/ SUTDNLP/NNTransitionParser. 2 Parser We take Chen and Manning (2014), which uses the arc-standard transition system (Nivre, 2008). Given an POS-tagged input sentence, it builds a projective output y by performing a sequence of state transition actions using greedy search. Chen and Manning (2014) can be viewed as a neutral alternative of MaltParser (Nivre, 2008). Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second). Our incorporation of discrete features does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 1Yet another alte</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="4115" citStr="Petrov and McDonald, 2012" startWordPosition="639" endWordPosition="642">14) find that the accuracies of Turian et al can be enhanced by discretizing the embedding features before combining them with the traditional features. They use simple binarization and clustering to this end, finding that the latter works better. The architecture is shown in Figure 1(d). In contrast, Figure 1(e) directly combines discrete and continuous features, replacing the hard-coded transformation function of Guo et al. (2014) with a hidden layer, which can be tuned by supervised training.1 We correlate and compare all the five systems in Figure 1 empirically, using the SANCL 2012 data (Petrov and McDonald, 2012) and the standard Penn Treebank data. Results show that the method of this paper gives higher accuracies than the other methods. In addition, the method of Guo et al. (2014) gives slightly better accuracies compared to the method of Turian et al. (2010) for parsing task, consistent with Guo et al’s observation on named entity recognition (NER). We make our C++ code publicly available under GPL at https://github.com/ SUTDNLP/NNTransitionParser. 2 Parser We take Chen and Manning (2014), which uses the arc-standard transition system (Nivre, 2008). Given an POS-tagged input sentence, it builds a p</context>
<context position="8588" citStr="Petrov and McDonald, 2012" startWordPosition="1417" endWordPosition="1420">lowing Guo et al. (2014), we use compounded clusters learnt by K-means algorithm of different granularities. 3.5 Directly combining linear and neural features (This) We directly combine linear and neural features (Figure 1(e)). Given a state x, the score of an action is defined as Score(a) = σ( (bd(x) ⊕ bh(x)) � →−9 c,a) , where bh is the same as the NN baseline. Note that like d in Guo, bh is also a function that transforms embeddings be. The main difference is that it can be tuned in supervised training. 4 Web Domain Experiments 4.1 Setting We perform experiments on the SANCL 2012 web data (Petrov and McDonald, 2012), using the Wall Street Journal (WSJ) training corpus to train the models and the WSJ development corpus to tune parameters. We clean the web domain texts following the method of Ma et al. (2014b). Automatic POS tags are produced by using a CRF model trained on the WSJ training corpus. The POS tags are assigned automatically on the training corpus by ten-fold jackknifing. Table 1 shows the corpus details. Domain #Sent #Word TA WSJ-train 30,060 731,678 97.03 WSJ-dev 1,336 32,092 96.88 WSJ-test 1,640 35,590 97.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on EMNLPCONLL,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="16704" citStr="Socher et al., 2012" startWordPosition="2763" endWordPosition="2766">he Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on EMNLPCONLL, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="9720" citStr="Socher et al. (2013)" startWordPosition="1606" endWordPosition="1609">7.51 answers 1,744 28,823 91.93 newsgroups 1,195 20,651 93.75 reviews 1,906 28,086 92.66 Table 1: Corpus statistics of our experiments, where TA denotes POS tagging accuracy. Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word embedding released by Collobert et al. (2011), and set h = 200 for the hidden layer size, A = 10−8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In ACL, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentimentspecific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="16723" citStr="Tang et al., 2014" startWordPosition="2767" endWordPosition="2770">ir work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different fro</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentimentspecific word embedding for twitter sentiment classification. In ACL, pages 1555–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
</authors>
<title>Neural networks leverage corpuswide information for part-of-speech tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 EMNLP,</booktitle>
<pages>938--950</pages>
<contexts>
<context position="17082" citStr="Tsuboi, 2014" startWordPosition="2828" endWordPosition="2829">uo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They show that combin</context>
</contexts>
<marker>Tsuboi, 2014</marker>
<rawString>Yuta Tsuboi. 2014. Neural networks leverage corpuswide information for part-of-speech tagging. In Proceedings of the 2014 EMNLP, pages 938–950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1941" citStr="Turian et al. (2010)" startWordPosition="285" endWordPosition="288">d parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg. Chen and Manning (2014)) (c) Turian et al. (2010) (d) Guo et al. (2014) (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trained using large amounts of unlabeled data, thus containing more information. In addition, using an extra hidden layer, a neural network is capable of learning non-linear relations between automatic features, achieving feature combinations automatically. Discrete manual features and continuous features complement each other. A natural question that arises from the</context>
<context position="3299" citStr="Turian et al. (2010)" startWordPosition="506" endWordPosition="509">roblem by constructing the neural network shown in Figure 1 (e), which incorporates the discrete input layer of the linear model (Figure 1 (a)) into the NN model (Figure 1 (b)) by conjoining · · · · · · · · · · · · · · · · · · · · · transform · · · · · · 1316 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1316–1321, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. it with the hidden layer. This architecture is connected with previous work on incorporating word embeddings into a linear model. In particular, Turian et al. (2010) incorporate word embeddings as real-valued features into a CRF model. The architecture is shown in Figure 1(c), which can be regarded as Figure 1(e) without the hidden layer. Guo et al. (2014) find that the accuracies of Turian et al can be enhanced by discretizing the embedding features before combining them with the traditional features. They use simple binarization and clustering to this end, finding that the latter works better. The architecture is shown in Figure 1(d). In contrast, Figure 1(e) directly combines discrete and continuous features, replacing the hard-coded transformation fun</context>
<context position="7252" citStr="Turian et al. (2010)" startWordPosition="1173" endWordPosition="1176">as Φh, and the ith node in the hidden as Φh,i (0 ≤ i ≤ |Φh|). The hidden layer is defined as Φh,i = (Φe(x) · →− 3 0 h,i) , where 0 h is the set of parameters between the input and hidden layers. The score of an action a is defined as Score(a) = Q(Φh(x) · →−0 c,a), →− where 0 c,a is the set of parameters between the hidden and output layers. We use the arc-standard features Φe as Chen and Manning (2014), which is also based on the arc-eager templates of Zhang and Nivre (2011), similar to those of the baseline model L. 3.3 Linear model with real-valued embeddings (Turian) We apply the method of Turian et al. (2010), combining real-valued embeddings with discrete features in the linear baseline (Figure 1(c)). Given a 1317 state x, the score of an action a is defined as Score(a) = σ �(bd(x) ⊕ be(x)) � →− 9 c,a) , where ⊕ is the vector concatenation operator. 3.4 Linear model with transformed embeddings (Guo) We apply the method of Guo et al. (2014), combining embeddings into the linear baseline by first transforming into discrete values. Given a state x, the score of an action is defined as Score(a) = σ ( (bd(x) ⊕ d(be(x) )) � →− 9 c,a) , where d is a transformation function from realvalue to binary featu</context>
<context position="10221" citStr="Turian et al. (2010)" startWordPosition="1679" endWordPosition="1682">eddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather than model NN Turian This -T +T 82 81 80 79 NN Turian This -T +T 91 90 89 88 NN Turian This -T +T 83 82 81 80 NN Turian This -T +T (c) newsgroups (d) reviews 82 81 80 79 (a) WSJ (b) answers · ( −→9 d,a ⊕ · ( −→9 d,a ⊕ · ( −→9 d,a ⊕ 1318 Model WSJ answers newsgroups reviews UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE UAS LAS OOV OOE L 88.19 86.16 83.72 —– 79.30 74.24 68.43 —– 82.55 79.06 69.07 —– 80.77 76.16 72.20 —– NN 89.81 87.83 84.94 84.94 79.27 74.30 69.18 69.18 83.71 80.35 69.60 69.60 8</context>
<context position="16449" citStr="Turian et al., 2010" startWordPosition="2716" endWordPosition="2719">* uses a special method to process punctuations, leading to about 1% UAS improvements over the vanilla system. Recently, Dyer et al. (2015) proposed a deterministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conversion of the Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging</context>
<context position="17775" citStr="Turian et al. (2010)" startWordPosition="2939" endWordPosition="2942">st to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They show that combined neural and discrete features work better without a hidden layer (i.e. Turian et al. (2010)). They argue that nonlinear structures do not work well with high dimensional features. We find that using a hidden layer specifically for embedding features gives better results compared with using no hidden layers. 7 Conclusion We studied the combination of discrete and continuous features for deterministic transition-based dependency parsing, comparing several methods to incorporate word embeddings and traditional sparse features in the same model. Experiments on both in-domain and cross-domain parsing show that directly adding sparse features into a neural network gives higher accuracies </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of non-linear deep architecture in sequence labeling.</title>
<date>2013</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1285--1291</pages>
<contexts>
<context position="5402" citStr="Wang and Manning (2013)" startWordPosition="838" endWordPosition="841">actions using greedy search. Chen and Manning (2014) can be viewed as a neutral alternative of MaltParser (Nivre, 2008). Although not giving state-of-the-art accuracies, deterministic parsing is attractive for its high parsing speed (1000+ sentences per second). Our incorporation of discrete features does not harm the overall speed significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 1Yet another alternative structure is to directly combine the two types of inputs, and replacing the input layer of (b) using them. Wang and Manning (2013) compared this architecture with (c) using a CRF network, finding that the latter works better for NER and chunking. 3 Models Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (20</context>
<context position="17587" citStr="Wang and Manning (2013)" startWordPosition="2907" endWordPosition="2910"> recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They show that combined neural and discrete features work better without a hidden layer (i.e. Turian et al. (2010)). They argue that nonlinear structures do not work well with high dimensional features. We find that using a hidden layer specifically for embedding features gives better results compared with using no hidden layers. 7 Conclusion We studied the combination of discrete and continuous features for deterministic transition-based dependency parsing, comparing several methods to incorporate word embeddings and tr</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In IJCNLP, pages 1285–1291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Tiejun Zhao</author>
<author>Daxiang Dong</author>
<author>Hao Tian</author>
<author>Dianhai Yu</author>
</authors>
<title>Compound embedding features for semi-supervised learning.</title>
<date>2013</date>
<booktitle>In NAACL,</booktitle>
<pages>563--568</pages>
<contexts>
<context position="16466" citStr="Yu et al., 2013" startWordPosition="2720" endWordPosition="2723">od to process punctuations, leading to about 1% UAS improvements over the vanilla system. Recently, Dyer et al. (2015) proposed a deterministic transition-based parser using LSTM, which gives a UAS of 93.1% on Stanford conversion of the Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014</context>
</contexts>
<marker>Yu, Zhao, Dong, Tian, Yu, 2013</marker>
<rawString>Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-supervised learning. In NAACL, pages 563– 568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew R Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Combining word embeddings and feature embeddings for fine-grained relation extraction.</title>
<date>2015</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="16741" citStr="Yu et al., 2015" startWordPosition="2771" endWordPosition="2774">more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2015</marker>
<rawString>Mo Yu, Matthew R Gormley, and Mark Dredze. 2015. Combining word embeddings and feature embeddings for fine-grained relation extraction. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="1048" citStr="Zhang and Clark, 2008" startWordPosition="145" endWordPosition="148"> the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing. In EMNLP, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="1197" citStr="Zhang and Nivre, 2011" startWordPosition="163" endWordPosition="166">el. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model</context>
<context position="6075" citStr="Zhang and Nivre (2011)" startWordPosition="950" endWordPosition="953">twork, finding that the latter works better for NER and chunking. 3 Models Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd(x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by Score(a) = Q(Φd(x) · →− 0 d,a), where Q represents the sigmoid activation func→− tion, 0 d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe(x). In addition, denote the hidden layer as Φh, and the ith node in the hidden as Φh</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th ACL, pages 188–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing the effect of global learning and beam-search on transition-based dependency parsing.</title>
<date>2012</date>
<booktitle>In COLING 2012: Posters,</booktitle>
<pages>1391--1400</pages>
<contexts>
<context position="1220" citStr="Zhang and Nivre, 2012" startWordPosition="167" endWordPosition="170">sults of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (</context>
</contexts>
<marker>Zhang, Nivre, 2012</marker>
<rawString>Yue Zhang and Joakim Nivre. 2012. Analyzing the effect of global learning and beam-search on transition-based dependency parsing. In COLING 2012: Posters, pages 1391–1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Zhang</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
<author>Amir Globerson</author>
</authors>
<title>Steps to excellence: Simple inference with refined scoring of dependency trees.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>197--207</pages>
<contexts>
<context position="1290" citStr="Zhang et al., 2014" startWordPosition="179" endWordPosition="182">ted model is a better way to combine traditional and embedding features compared with previous methods. 1 Introduction Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodriguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). (a) discrete linear (b) continuous NN (eg. MaltParser) (eg</context>
</contexts>
<marker>Zhang, Lei, Barzilay, Jaakkola, Globerson, 2014</marker>
<rawString>Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola, and Amir Globerson. 2014. Steps to excellence: Simple inference with refined scoring of dependency trees. In ACL, pages 197–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhou</author>
<author>Yue Zhang</author>
<author>Shujian Huang</author>
<author>Jiajun Chen</author>
</authors>
<title>A neural probabilistic structuredprediction model for transition-based dependency parsing.</title>
<date>2015</date>
<booktitle>In ACL,</booktitle>
<pages>1213--1222</pages>
<contexts>
<context position="15291" citStr="Zhou et al. (2015)" startWordPosition="2522" endWordPosition="2525">web domain. 5The p-values are below 0.01 using pairwise t-test. 6http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 1319 System UAS LAS L 89.36 88.33 NN 91.15 90.04 This 91.80 90.68 ZPar-local 89.94 88.92 Ma et al. (2014a) 90.38 – Chen and Manning (2014) 91.17 89.99 Honnibal et al. (2013) 91.30 90.00 Ma et al. (2014a)* 91.32 – Table 3: Main results on WSJ. All systems are deterministic. Our combined parser gives accuracies competitive to state-of-the-art deterministic parsers in the literature. In particular, the method of Chen and Manning (2014) is the same as our NN baseline. Note that Zhou et al. (2015) reports a UAS of 91.47% by this parser, which is higher than the results we obtained. The main results include the use of different batch size during, while Zhou et al. (2015) used a batch size of 100,000, we used a batch size of 10,000 in all experiments. Honnibal et al. (2013) applies dynamic oracle to the deterministic transition-based parsing, giving a UAS of 91.30%. Ma et al. (2014a) is similar to ZPar local, except that they use the arc-standard transitions, while ZPar-local is based on arc-eager transitions. Ma et al. (2014a)* uses a special method to process punctuations, leading to a</context>
</contexts>
<marker>Zhou, Zhang, Huang, Chen, 2015</marker>
<rawString>Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen. 2015. A neural probabilistic structuredprediction model for transition-based dependency parsing. In ACL, pages 1213–1222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
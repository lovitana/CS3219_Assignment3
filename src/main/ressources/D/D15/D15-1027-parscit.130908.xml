<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026245">
<title confidence="0.992607">
A Model of Zero-Shot Learning of Spoken Language Understanding
</title>
<author confidence="0.98324">
Majid Yazdani James Henderson
</author>
<affiliation confidence="0.953606">
Computer Science Department Xerox Research Center Europe
University of Geneva james.henderson@xrce.xerox.com
</affiliation>
<email confidence="0.988347">
majid.yazdani@unige.ch
</email>
<sectionHeader confidence="0.993661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932888888889">
When building spoken dialogue systems
for a new domain, a major bottleneck is
developing a spoken language understand-
ing (SLU) module that handles the new
domain’s terminology and semantic con-
cepts. We propose a statistical SLU model
that generalises to both previously unseen
input words and previously unseen out-
put classes by leveraging unlabelled data.
After mapping the utterance into a vector
space, the model exploits the structure of
the output labels by mapping each label
to a hyperplane that separates utterances
with and without that label. Both these
mappings are initialised with unsupervised
word embeddings, so they can be com-
puted even for words or concepts which
were not in the SLU training data.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838557377049">
Spoken Language Understanding (SLU) in dia-
logue systems is the task of taking the utterance
output by a speech recognizer and assigning it a
semantic label that represents the dialogue actions
of that utterance accompanied with their associ-
ated attributes and values. For example, the utter-
ance ”I would like Chinese food” is labelled with
inform(food=Chinese), in which inform is the dia-
logue action that provides the value of the attribute
food that is Chinese.
Dialogue systems often use hand-crafted gram-
mars for SLU, such as Phoenix (Ward, 1994),
which are expensive to develop, and expensive
to extend or adapt to new attributes and values.
Statistical SLU models are usually trained on the
data obtained from a specific domain and loca-
tion, using a structured output classifier that can
be discriminative (Pradhan et al., 2004; Kate and
Mooney, 2006; Henderson et al., 2012) or genera-
tive (Schwartz et al., 1996; He and Young, 2005).
Gathering and annotating SLU data is costly and
time consuming and therefore SLU datasets are
small compare to the number of possible labels.
Because training sets for a new domain are
small, or non-existent, learning is often an in-
stance of Zero-shot or One-shot learning prob-
lems (Palatucci et al., 2009; L. Fei-Fei; Fergus,
2006), in which zero or few examples of some
output classes are available during the training.
For example, in the restaurant reservation domain,
not all possible combinations of foods and dia-
logue actions may be included in the training set.
The general idea to solve this type of problems is
to map the input and class labels to a semantic
space of usually lower dimension in which simi-
lar classes are represented by closer points in the
space (Palatucci et al., 2009; Weston et al., 2011;
Weston et al., 2010). Usually unsupervised knowl-
edge sources are used to form semantic codes of
the labels that helps us to generalize to unseen la-
bels.
On the other hand, there are also different ways
to express the same meaning, and similarly, most
of them can not be included in the training set.
For instance, the system may have seen ”Please
give me the telephone number” in training, but the
user might ask ”Please give me the phone” at test
time. This problem, feature sparsity, is a common
issue in many NLP tasks. Decomposition of in-
put feature parameters using vector-matrix mul-
tiplication (Bengio et al., 2003; Collobert et al.,
2011; Collobert and Weston, 2008) has addressed
this sparsity issue successfully in previous work.
In this way, by sharing the word representations
and composition matrices, we can overcome fea-
ture sparsity by producing similar representations
for similar utterances.
In order to represent words and concepts we
use word embeddings, which are a form of vec-
tor space model. Word embeddings have proven
to be effective models of semantic representation
</bodyText>
<page confidence="0.975828">
244
</page>
<note confidence="0.6576085">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999783025641026">
of words in various NLP tasks (Baroni et al., 2014;
Yazdani and Popescu-Belis, 2013; Collobert et al.,
2011; Collobert and Weston, 2008; Huang et al.,
2012; Mikolov et al., 2013b). In addition to pa-
rameter sharing, these representations enable us
to leverage large scale unlabelled data. Because
word embeddings trained on unlabeled data reflect
the similarity between words, they help the model
generalize from the words in the original training
corpora to the words in the new extended domain,
and help generalize from small amounts of data in
the extended domain.
The contribution of this paper is to build a rep-
resentation learning classifier for the SLU task that
can generalize to unseen words and labels. For ev-
ery utterance we learn how to compose the word
vectors to form the semantics of that utterance for
this task of language understanding. Furthermore,
we learn how to compose the semantics of each la-
bel from the semantics of the words used to name
that label. This enables us to generalize to unseen
labels.
In this work we use the word2vec software of
Mikolov et al. (2013a)1 to induce unsupervised
word embeddings that are used to initialize word
embedding parameters. For this, we use an En-
glish Wikipedia dump as our unlabelled training
corpus, which is a diverse broad-coverage corpus.
It has been shown (Baroni et al., 2014; Mikolov
et al., 2013b) that these embeddings capture lex-
ical similarities even when they are trained on a
diverse corpus like Wikipedia. We test our models
on a restaurant booking domain. We investigate
domain adaptation by adding new attribute types
(e.g. goodformeal) and new attribute values (e.g.
Hayes Valley as a restaurant location). Our exper-
iments indicate that our model has better perfor-
mance compared to a hand-crafted system as well
as a SVM baseline.
</bodyText>
<sectionHeader confidence="0.98745" genericHeader="method">
2 SLU Datasets
</sectionHeader>
<bodyText confidence="0.999955444444444">
The dialogue utterances used to build the SLU
dataset were collected during a trial of online di-
alogue policy adaptation for a restaurant reserva-
tion system based in San Francisco. The trial be-
gan with (area, pricerange and food), and adapted
the Interaction Manager online to handle the ad-
ditional attribute types near, allowedforkids, and
goodformeal (Gaˇsic et al., 2014). User utterances
from these trials were transcribed and annotated
</bodyText>
<footnote confidence="0.905886">
1https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.9999826">
with dialogue acts by an expert, and afterwards
edited by another expert2. Each user utterance was
annotated with a set of labels, where each label
consists of an act type (e.g. inform, request), an
attribute type (e.g. foodtype, pricerange), and an
attribute value (e.g. Chinese, Cheap).
The dataset is separated into four subsets,
SFCore, SF1Ext, SF2Ext and SF3Ext, each
with an increasing set of attribute types, as speci-
fied in Table 1. This table also gives the total num-
ber of utterances in each data set. For our first ex-
periment, we split each dataset into about 15% for
the testing set and 85% for the training set. For our
second experiment we use each extended subset
for testing and its preceding subsets for training.
</bodyText>
<table confidence="0.995567">
Ontology Attribute types ( # of values ) # of utterances
SFCore food(59), area(155), pricerange(3) 1103
SF1Ext SFCore + near(39) 1810
SF2Ext SF1Ext + allowedforkids(2) 1571
SF3Ext SF2Ext +goodformeal(4) 1518
</table>
<tableCaption confidence="0.892253">
Table 1: Domains for San Francisco (SF) restau-
rants expanding in complexity
</tableCaption>
<sectionHeader confidence="0.94103" genericHeader="method">
3 A Dialogue Act Representation
</sectionHeader>
<subsectionHeader confidence="0.877589">
Learning Classifier
</subsectionHeader>
<bodyText confidence="0.999991190476191">
The SLU model is run on each hypothesis output
by the ASR component, and tries to predict the
correct set of dialogue act labels for each hypoth-
esis. This problem is in general an instance of
multi-label classification, because a single utter-
ance can have multiple dialogue act labels. Also,
these labels are structured, since each label consist
of an act type, an attribute type, and an attribute
value. Each label component also has canonical
text associated with it, which is the text used to
name the label component (e.g. “Chinese” as a
value).
The number of possible dialogue acts grows
rapidly as the domain is extended with new at-
tribute types and values, making this task one of
multi-label classification with a very large number
of labels. One natural approach to this task is to
train one binary classifier for each possible label,
to decide whether or not to include it in the output.
In our case, this requires training a large number
of classifiers, and it is impossible to generalize to
</bodyText>
<footnote confidence="0.99397525">
2This data is publically available from
https://sites.google.com/site/
parlanceprojectofficial/home/
datarepository
</footnote>
<page confidence="0.997902">
245
</page>
<bodyText confidence="0.999956444444444">
dialogue acts that include attributes or values that
were not in the training set since there won’t be
any parameter sharing among label classifiers.
In our alternative approach, we build the rep-
resentation of the utterance and the representation
of the label from their constituent words, then we
check if these representations match or not. In the
following we explain in details this representation
learning model.
</bodyText>
<subsectionHeader confidence="0.999672">
3.1 Utterance Representation Learning
</subsectionHeader>
<bodyText confidence="0.999928875">
In this section we explain how to build the utter-
ance representation from its constituent words. In
addition to words, we use bigrams, since they have
been shown previously to be effective features for
this task (Henderson et al., 2012). Following the
success in transfer learning from parsing to under-
standing tasks (Henderson et al., 2013; Socher et
al., 2013), we use dependency parse bigrams in
our features as well. We learn to build a local rep-
resentation at each word position in the utterance
by using the word representation, adjacent word
representations, and the head word representation.
Let φ(w) be a d dimensional vector representing
the word w, and φ(Ui) be a h dimensional vector
which is the local representation at word position
i. We compute the local representation as follows:
</bodyText>
<equation confidence="0.999666">
φ(Ui) = σ(φ(wi)Wword + φ(wh)WparseRk+
φ(wj)Wprevious + φ(wk)Wnext) (1)
</equation>
<bodyText confidence="0.9999835">
in which wh is the head word with the depen-
dency relation Rk to wi, and wj and wk are the
previous and next words. Wword is a d x h ma-
trix that transforms the word embedding to hidden
representation inputs. WparseRk is a d x h ma-
trix for the relation Rk that similarly transforms
the head word embedding (so Wparse is a tensor),
and Wprevious and Wnext similarly transform the
previous and next words’ embeddings. Figure 1
depicts this representation building at each word.
</bodyText>
<subsectionHeader confidence="0.999247">
3.2 Label Representation Learning
</subsectionHeader>
<bodyText confidence="0.999654666666667">
One standard way to address the problem of multi-
label classification is building binary classifiers
for each possible label. Large margin classifiers
have been shown to be an effective tool for this
task (Pradhan et al., 2004; Kate and Mooney,
2006). We use the same idea of binary classifiers
to learn one hyperplane per label, which separates
the utterances with this label from all other utter-
ances, with a large margin. In the standard way of
</bodyText>
<figureCaption confidence="0.999809">
Figure 1: The multi-label classifier
</figureCaption>
<bodyText confidence="0.999876842105263">
building the classifier, each label’s hyperplane is
independent of other labels. To extend this model
to a zero-shot learning classifier, we use parame-
ter sharing among label hyperplanes so that similar
labels have similar hyperplanes.
We exploit the structure of labels by assuming
that each hyperplane representation is a compo-
sition of representations of the label’s constituent
components, namely dialogue action, attribute and
attribute value. We learn the composition function
and the constituent representations while training
the classifiers, using the labelled SLU data. The
constituent representations are initialised as the
word embeddings for the label constituent’s name
string, such as “inform”, “food” and “Chinese”,
where these embeddings are trained on the unla-
belled data. Figure 1 depicts the classifier model.
We define the hyperplane of the label aj(attk =
vall) with its normal vector Waj,attk,vall as:
</bodyText>
<equation confidence="0.595867">
Waj,attk,vall = σ([φ(aj), φ(attk), φ(vall)]Wih)Who
</equation>
<bodyText confidence="0.999367214285715">
where φ(·) is the same mapping to d dimensional
word vectors that is used above in the utterance
representation, Wih is a 3d x h matrix and Who is
a h x h matrix. The score of each local represen-
tation vector φ(Ui) is its distance from this label
hyperplane, which is computed as the dot product
of the local vector φ(Ui) with the normal vector
Waj,attk,vall.
We sum these local scores for each po-
sition i to build the whole utterance score:
Ei φ(Ui)WTaj,attk,vall. Alternatively we can think
of this computation as summing the local vectors
to get a whole-utterance representation φ(U) =
Ei φ(Ui) and then doing the dot product. The
</bodyText>
<page confidence="0.992196">
246
</page>
<bodyText confidence="0.999802727272727">
pooling method (sum) used in the model is (inten-
tionally) over-simplistic. We did not want to dis-
tract from the main contribution of the paper, and
our dataset did not justify any more complex solu-
tion since utterances are short. It can be replaced
by more powerful approaches if it is needed.
To train a large margin classifier, we train all
the parameters such that the score of an utterance
is bigger than a margin for its labels and less than
the negative margin for all other labels. Thus, the
loss function is as follows:
</bodyText>
<equation confidence="0.723788">
O(Ui)WTaj,attk,vall)
</equation>
<bodyText confidence="0.998320666666667">
(2)
where θ is all the parameters of the model, namely
O(wi) (word embeddings), Wword, WParse,
Wprevious, Wnext, Wih, and Who. y is either 1 or
−1 depending whether the input U has that label
or not.
To optimize this large margin classifier we per-
form stochastic gradient descent by using the ada-
grad algorithm on this primal loss function, sim-
ilarly to Pegasos SVM (Shalev-Shwartz et al.,
2007), but here we backpropagate the errors to
the representations to train the word embeddings
and composition functions. In each iteration of the
stochastic training algorithm, we randomly select
an utterance and its labels as positive examples
and choose randomly another utterance with a dif-
ferent label as a negative example. When choos-
ing the negative sample randomly, we sample ut-
terances with the same dialogue act but different
attribute or value with 4 times higher probability
than utterances with a different dialogue act. This
biased negative sampling speeds up the training
process since it provides more difficult training ex-
amples to the learner.
The model is able to address the adaptivity is-
sues because the utterance and the dialogue act
representations are in the same space using the
same shared parameters O(w), which are ini-
tialised with unsupervised word embeddings. It
has been shown that such word embeddings cap-
ture word similarities and hence the classifier is
no longer ignorant about any new attribute type or
attribute value. Also, there is parameter sharing
between dialogue acts because these word/label
embeddings are shared, and the matrices for the
composition of these representations are the same
across all dialogue acts. This can help overcome
sparsity in the SLU training set by transferring
learning between similar situations and similar
dialogue act triples. For example, if the train-
ing set does not contain any examples of the act
”request(postcode)”, but many examples of ”re-
quest(phone)”, sharing the parameters can help
with the recognition of ”request(postcode)” in ut-
terances similar to ”request(phone)”. Moreover,
the SLU model is to some extent robust against
paraphrasing in the input utterance because it
maps the utterance to a semantic space, and uses
parse bigrams. More sophisticated vector-space
semantic representations of the utterance are an
area for future work, but should be largely orthog-
onal to the contribution of this paper.
To find the set of compatible dialogue acts for a
given utterance, we should check all possible dia-
logue acts. This can severely slow down SLU. To
avoid testing all possible dialogue combinations,
we build three different classifiers: The first one
recognises the act types in the utterance, the sec-
ond one recognises the attribute types for each of
the chosen act types, and the third classifier recog-
nises the full dialogue acts as we described above,
but only for the chosen pairs of act types and at-
tribute types.
</bodyText>
<sectionHeader confidence="0.991752" genericHeader="method">
4 SLU Experiments
</sectionHeader>
<bodyText confidence="0.999596142857143">
In the first experiment, we measure SLU perfor-
mance trained on all available data, by building a
dataset that is the union of all the above datasets.
This measures the performance of SLU when there
is a small amount of data for an extended do-
main. This dataset, similarly to SF3Ext, has 6
main attribute types. Table 2 shows the perfor-
mance of this model. We report as baselines the
performance of the Phoenix system (hand crafted
for this domain) and a binary linear SVM trained
on the same data. The hidden layers have size
h=d=50. For this experiment, we split each dataset
into about 15% for the testing set and 85% for the
training set.
</bodyText>
<table confidence="0.99590575">
System Outputs Precision Recall F-core
Phoenix 516 84.10 41.65 55.71
SVM 690 65.03 52.45 58.06
Our 932 90.24 81.15 85.45
</table>
<tableCaption confidence="0.953567">
Table 2: Performance on union of data (SF-
Core+SF1Ext+SF2Ext+SF3Ext)
</tableCaption>
<bodyText confidence="0.5264835">
Our SLU model can adapt well to the extended
domain with more attribute types. We observe
</bodyText>
<figure confidence="0.437435">
min λ θ2 �max(0,1−y
B 2+� i
U
</figure>
<page confidence="0.548588">
247
</page>
<table confidence="0.997960777777778">
model, train set SF1Ext Test set SF3Ext
P—R—F SF2Ext P—R—F
P—R—F
Our SFcore 73.36—66.11—69.54 74.61—59.73—66.34 72.54—53.86—61.81
SVM SFcore 50.66— 38.7— 43.87 49.64—34.70— 40.84 48.99—30.91—37.90
Our SF1Ext 83.18—66.08—73.65 78.32—59.98—67.93
SVM SF1Ext 58.72—41.71—48.77 53.25—34.88—42.15
Our SF2Ext 84.12—67.78—75.07
SVM SF2Ext 59.27—42.80—49.70
</table>
<tableCaption confidence="0.999952">
Table 3: SLU performance: trained on a smaller domain and tested on more inclusive domains.
</tableCaption>
<bodyText confidence="0.999986875">
particularly that the recall is almost twice as high
as the hand-crafted baseline. This shows that our
SLU can recognise most of the dialogue acts in
an utterance, where the rule-based Phoenix sys-
tem and a classifier without composed output can-
not. Overall there are 1042 dialogue acts in the
test set. SLU recall is very important in the over-
all dialogue system performance, as the effect of a
missed dialogue act is hard to handle for the Inter-
action Manager. Both hand-crafted and our system
show relatively high precision.
In the next experiment, we measure how well
the new SLU model performs in an extended do-
main without any training examples from that ex-
tended domain. We train a SLU model on each
subset, and test it on each of the more inclusive
subsets. Table 3 shows the results.
Not surprisingly, the performance is better if
SLU is trained on a similar domain to the test do-
main, and adding more attribute types and values
decreases the performance more. But our SLU
can generalise very well to the extended domain,
achieving much better generalisation that the SVM
model.
</bodyText>
<subsectionHeader confidence="0.994774">
4.1 Conclusion
</subsectionHeader>
<bodyText confidence="0.999994576923077">
In this paper, we describe a new SLU model
that is designed for improved domain adaptation.
The multi-label classification problem of dialogue
act recognition is addressed with a classifier that
learns to build an utterance representation and a
dialogue act representation, and decides whether
or not they are compatible. The dialogue act repre-
sentation is a vector composition of its constituent
labels’ embeddings, and is trained as the hyper-
plane of a large margin binary classifier for that di-
alogue act. The utterance representation is trained
as a composition of word embeddings. Since the
utterance and the dialogue act representations are
both built using unsupervised word embeddings
and share these embedding parameters, the model
can address the issues of domain adaptation. Word
embeddings capture word similarities, and hence
the classifier is able to generalise from known at-
tribute types or values to similar novel attribute
types or values. We tested this SLU model on
datasets where the number of attribute types and
values is increased, and show much better re-
sults than the baselines, especially in recall. The
model succeeds in both adapting to an extended
domain using relatively few training examples and
in recognising novel attribute types and values.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999980375">
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE), and Hasler
foundation project no. 15019, Deep Neural Net-
work Dependency Parser for Context-aware Rep-
resentation Learning. The authors also would like
to thank Dr.Helen Hastie for her help in annotating
the dataset.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977484928571428">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 238–247.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 160–167.
</reference>
<page confidence="0.986532">
248
</page>
<reference confidence="0.998679662790697">
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
M Ga&amp;quot;sic, D Kim, P Tsiakoulis, C Breslin, M Hender-
son, M Szummer, B Thomson, and S Young. 2014.
Incremental on-line adaptation of pomdp-based dia-
logue managers to extended domains.
Yulan He and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. Computer
Speech and Language, 19:85–106.
Matthew Henderson, Milica Ga&amp;quot;si´c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Comput. Linguist., 39(4):949–998.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, ACL-44, pages 913–920.
R.; Perona L. Fei-Fei; Fergus. 2006. One-shot learning
of object categories. IEEE Transactions on Pattern
Analysis Machine Intelligence, 28:594–611, April.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT-2013).
Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning
with semantic output codes. In Advances in Neu-
ral Information Processing Systems 22, pages 1410–
1418.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, and
James H. Martin. 2004. Shallow semantic parsing
using support vector machines.
Richard Schwartz, Scott Miller, David Stallard, and
John Makhoul. 1996. Language understanding us-
ing hidden understanding models. In Spoken Lan-
guage, 1996. ICSLP 96. Proceedings., Fourth Inter-
national Conference on, volume 2, pages 997–1000.
IEEE.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of the 24th Interna-
tional Conference on Machine Learning, pages 807–
814.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642.
Wayne Ward. 1994. Extracting information in sponta-
neous speech. In ICSLP.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Mach.
Learn., 81.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary image
annotation. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intelli-
gence - Volume Volume Three, pages 2764–2770.
Majid Yazdani and Andrei Popescu-Belis. 2013. Com-
puting text semantic relatedness using the contents
and links of a hypertext encyclopedia. Artif. Intell.,
194:176–202.
</reference>
<page confidence="0.998904">
249
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934248">
<title confidence="0.999945">A Model of Zero-Shot Learning of Spoken Language Understanding</title>
<author confidence="0.999957">Majid Yazdani James Henderson</author>
<affiliation confidence="0.992662">Computer Science Department Xerox Research Center Europe of Geneva</affiliation>
<email confidence="0.956745">majid.yazdani@unige.ch</email>
<abstract confidence="0.999428684210526">When building spoken dialogue systems for a new domain, a major bottleneck is developing a spoken language understanding (SLU) module that handles the new domain’s terminology and semantic concepts. We propose a statistical SLU model that generalises to both previously unseen input words and previously unseen output classes by leveraging unlabelled data. After mapping the utterance into a vector space, the model exploits the structure of the output labels by mapping each label to a hyperplane that separates utterances with and without that label. Both these mappings are initialised with unsupervised word embeddings, so they can be computed even for words or concepts which were not in the SLU training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="4063" citStr="Baroni et al., 2014" startWordPosition="648" endWordPosition="651">y in previous work. In this way, by sharing the word representations and composition matrices, we can overcome feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for</context>
<context position="5358" citStr="Baroni et al., 2014" startWordPosition="864" endWordPosition="867">rance we learn how to compose the word vectors to form the semantics of that utterance for this task of language understanding. Furthermore, we learn how to compose the semantics of each label from the semantics of the words used to name that label. This enables us to generalize to unseen labels. In this work we use the word2vec software of Mikolov et al. (2013a)1 to induce unsupervised word embeddings that are used to initialize word embedding parameters. For this, we use an English Wikipedia dump as our unlabelled training corpus, which is a diverse broad-coverage corpus. It has been shown (Baroni et al., 2014; Mikolov et al., 2013b) that these embeddings capture lexical similarities even when they are trained on a diverse corpus like Wikipedia. We test our models on a restaurant booking domain. We investigate domain adaptation by adding new attribute types (e.g. goodformeal) and new attribute values (e.g. Hayes Valley as a restaurant location). Our experiments indicate that our model has better performance compared to a hand-crafted system as well as a SVM baseline. 2 SLU Datasets The dialogue utterances used to build the SLU dataset were collected during a trial of online dialogue policy adaptati</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="3345" citStr="Bengio et al., 2003" startWordPosition="541" endWordPosition="544">eston et al., 2010). Usually unsupervised knowledge sources are used to form semantic codes of the labels that helps us to generalize to unseen labels. On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen ”Please give me the telephone number” in training, but the user might ask ”Please give me the phone” at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008) has addressed this sparsity issue successfully in previous work. In this way, by sharing the word representations and composition matrices, we can overcome feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-2</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="3398" citStr="Collobert and Weston, 2008" startWordPosition="549" endWordPosition="552">wledge sources are used to form semantic codes of the labels that helps us to generalize to unseen labels. On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen ”Please give me the telephone number” in training, but the user might ask ”Please give me the phone” at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008) has addressed this sparsity issue successfully in previous work. In this way, by sharing the word representations and composition matrices, we can overcome feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computationa</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3369" citStr="Collobert et al., 2011" startWordPosition="545" endWordPosition="548">Usually unsupervised knowledge sources are used to form semantic codes of the labels that helps us to generalize to unseen labels. On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen ”Please give me the telephone number” in training, but the user might ask ”Please give me the phone” at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003; Collobert et al., 2011; Collobert and Weston, 2008) has addressed this sparsity issue successfully in previous work. In this way, by sharing the word representations and composition matrices, we can overcome feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasic</author>
<author>D Kim</author>
<author>P Tsiakoulis</author>
<author>C Breslin</author>
<author>M Henderson</author>
<author>M Szummer</author>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Incremental on-line adaptation of pomdp-based dialogue managers to extended domains.</title>
<date>2014</date>
<marker>Gasic, Kim, Tsiakoulis, Breslin, Henderson, Szummer, Thomson, Young, 2014</marker>
<rawString>M Ga&amp;quot;sic, D Kim, P Tsiakoulis, C Breslin, M Henderson, M Szummer, B Thomson, and S Young. 2014. Incremental on-line adaptation of pomdp-based dialogue managers to extended domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Steve Young</author>
</authors>
<title>Semantic processing using the hidden vector state model. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="1904" citStr="He and Young, 2005" startWordPosition="293" endWordPosition="296">d” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dialogue actions may be included in the training set. The general idea to solve th</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Yulan He and Steve Young. 2005. Semantic processing using the hidden vector state model. Computer Speech and Language, 19:85–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Milica Gasi´c</author>
<author>Blaise Thomson</author>
<author>Pirros Tsiakoulis</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Discriminative Spoken Language Understanding Using Word Confusion Networks.</title>
<date>2012</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<marker>Henderson, Gasi´c, Thomson, Tsiakoulis, Yu, Young, 2012</marker>
<rawString>Matthew Henderson, Milica Ga&amp;quot;si´c, Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012. Discriminative Spoken Language Understanding Using Word Confusion Networks. In Spoken Language Technology Workshop, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
<author>Gabriele Musillo</author>
</authors>
<title>Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model.</title>
<date>2013</date>
<journal>Comput. Linguist.,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="9304" citStr="Henderson et al., 2013" startWordPosition="1495" endWordPosition="1498">ach, we build the representation of the utterance and the representation of the label from their constituent words, then we check if these representations match or not. In the following we explain in details this representation learning model. 3.1 Utterance Representation Learning In this section we explain how to build the utterance representation from its constituent words. In addition to words, we use bigrams, since they have been shown previously to be effective features for this task (Henderson et al., 2012). Following the success in transfer learning from parsing to understanding tasks (Henderson et al., 2013; Socher et al., 2013), we use dependency parse bigrams in our features as well. We learn to build a local representation at each word position in the utterance by using the word representation, adjacent word representations, and the head word representation. Let φ(w) be a d dimensional vector representing the word w, and φ(Ui) be a h dimensional vector which is the local representation at word position i. We compute the local representation as follows: φ(Ui) = σ(φ(wi)Wword + φ(wh)WparseRk+ φ(wj)Wprevious + φ(wk)Wnext) (1) in which wh is the head word with the dependency relation Rk to wi, and</context>
</contexts>
<marker>Henderson, Merlo, Titov, Musillo, 2013</marker>
<rawString>James Henderson, Paola Merlo, Ivan Titov, and Gabriele Musillo. 2013. Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model. Comput. Linguist., 39(4):949–998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4168" citStr="Huang et al., 2012" startWordPosition="664" endWordPosition="667">come feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize to unseen words and labels. For every utterance we learn how to compose</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>913--920</pages>
<contexts>
<context position="1821" citStr="Kate and Mooney, 2006" startWordPosition="278" endWordPosition="281">ssociated attributes and values. For example, the utterance ”I would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and</context>
<context position="10590" citStr="Kate and Mooney, 2006" startWordPosition="1712" endWordPosition="1715">rix that transforms the word embedding to hidden representation inputs. WparseRk is a d x h matrix for the relation Rk that similarly transforms the head word embedding (so Wparse is a tensor), and Wprevious and Wnext similarly transform the previous and next words’ embeddings. Figure 1 depicts this representation building at each word. 3.2 Label Representation Learning One standard way to address the problem of multilabel classification is building binary classifiers for each possible label. Large margin classifiers have been shown to be an effective tool for this task (Pradhan et al., 2004; Kate and Mooney, 2006). We use the same idea of binary classifiers to learn one hyperplane per label, which separates the utterances with this label from all other utterances, with a large margin. In the standard way of Figure 1: The multi-label classifier building the classifier, each label’s hyperplane is independent of other labels. To extend this model to a zero-shot learning classifier, we use parameter sharing among label hyperplanes so that similar labels have similar hyperplanes. We exploit the structure of labels by assuming that each hyperplane representation is a composition of representations of the lab</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 913–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Perona L Fei-Fei R</author>
<author>Fergus</author>
</authors>
<title>One-shot learning of object categories.</title>
<date>2006</date>
<journal>IEEE Transactions on Pattern Analysis Machine Intelligence,</journal>
<volume>28</volume>
<marker>R, Fergus, 2006</marker>
<rawString>R.; Perona L. Fei-Fei; Fergus. 2006. One-shot learning of object categories. IEEE Transactions on Pattern Analysis Machine Intelligence, 28:594–611, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="4190" citStr="Mikolov et al., 2013" startWordPosition="668" endWordPosition="671">y by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize to unseen words and labels. For every utterance we learn how to compose the word vectors to f</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013).</booktitle>
<contexts>
<context position="4190" citStr="Mikolov et al., 2013" startWordPosition="668" endWordPosition="671">y by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize to unseen words and labels. For every utterance we learn how to compose the word vectors to f</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey E Hinton</author>
<author>Tom M Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1410--1418</pages>
<contexts>
<context position="2211" citStr="Palatucci et al., 2009" startWordPosition="344" endWordPosition="347">attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dialogue actions may be included in the training set. The general idea to solve this type of problems is to map the input and class labels to a semantic space of usually lower dimension in which similar classes are represented by closer points in the space (Palatucci et al., 2009; Weston et al., 2011; Weston et al., 2010). Usually unsupervised knowledge sources are used to form semantic</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, and Tom M. Mitchell. 2009. Zero-shot learning with semantic output codes. In Advances in Neural Information Processing Systems 22, pages 1410– 1418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<contexts>
<context position="1798" citStr="Pradhan et al., 2004" startWordPosition="274" endWordPosition="277">companied with their associated attributes and values. For example, the utterance ”I would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible co</context>
<context position="10566" citStr="Pradhan et al., 2004" startWordPosition="1708" endWordPosition="1711">. Wword is a d x h matrix that transforms the word embedding to hidden representation inputs. WparseRk is a d x h matrix for the relation Rk that similarly transforms the head word embedding (so Wparse is a tensor), and Wprevious and Wnext similarly transform the previous and next words’ embeddings. Figure 1 depicts this representation building at each word. 3.2 Label Representation Learning One standard way to address the problem of multilabel classification is building binary classifiers for each possible label. Large margin classifiers have been shown to be an effective tool for this task (Pradhan et al., 2004; Kate and Mooney, 2006). We use the same idea of binary classifiers to learn one hyperplane per label, which separates the utterances with this label from all other utterances, with a large margin. In the standard way of Figure 1: The multi-label classifier building the classifier, each label’s hyperplane is independent of other labels. To extend this model to a zero-shot learning classifier, we use parameter sharing among label hyperplanes so that similar labels have similar hyperplanes. We exploit the structure of labels by assuming that each hyperplane representation is a composition of re</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, and James H. Martin. 2004. Shallow semantic parsing using support vector machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Schwartz</author>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>John Makhoul</author>
</authors>
<title>Language understanding using hidden understanding models.</title>
<date>1996</date>
<booktitle>In Spoken Language,</booktitle>
<volume>2</volume>
<pages>997--1000</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1883" citStr="Schwartz et al., 1996" startWordPosition="289" endWordPosition="292"> would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dialogue actions may be included in the training set. The gen</context>
</contexts>
<marker>Schwartz, Miller, Stallard, Makhoul, 1996</marker>
<rawString>Richard Schwartz, Scott Miller, David Stallard, and John Makhoul. 1996. Language understanding using hidden understanding models. In Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, volume 2, pages 997–1000. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="13383" citStr="Shalev-Shwartz et al., 2007" startWordPosition="2172" endWordPosition="2175">n classifier, we train all the parameters such that the score of an utterance is bigger than a margin for its labels and less than the negative margin for all other labels. Thus, the loss function is as follows: O(Ui)WTaj,attk,vall) (2) where θ is all the parameters of the model, namely O(wi) (word embeddings), Wword, WParse, Wprevious, Wnext, Wih, and Who. y is either 1 or −1 depending whether the input U has that label or not. To optimize this large margin classifier we perform stochastic gradient descent by using the adagrad algorithm on this primal loss function, similarly to Pegasos SVM (Shalev-Shwartz et al., 2007), but here we backpropagate the errors to the representations to train the word embeddings and composition functions. In each iteration of the stochastic training algorithm, we randomly select an utterance and its labels as positive examples and choose randomly another utterance with a different label as a negative example. When choosing the negative sample randomly, we sample utterances with the same dialogue act but different attribute or value with 4 times higher probability than utterances with a different dialogue act. This biased negative sampling speeds up the training process since it </context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of the 24th International Conference on Machine Learning, pages 807– 814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="9326" citStr="Socher et al., 2013" startWordPosition="1499" endWordPosition="1502">entation of the utterance and the representation of the label from their constituent words, then we check if these representations match or not. In the following we explain in details this representation learning model. 3.1 Utterance Representation Learning In this section we explain how to build the utterance representation from its constituent words. In addition to words, we use bigrams, since they have been shown previously to be effective features for this task (Henderson et al., 2012). Following the success in transfer learning from parsing to understanding tasks (Henderson et al., 2013; Socher et al., 2013), we use dependency parse bigrams in our features as well. We learn to build a local representation at each word position in the utterance by using the word representation, adjacent word representations, and the head word representation. Let φ(w) be a d dimensional vector representing the word w, and φ(Ui) be a h dimensional vector which is the local representation at word position i. We compute the local representation as follows: φ(Ui) = σ(φ(wi)Wword + φ(wh)WparseRk+ φ(wj)Wprevious + φ(wk)Wnext) (1) in which wh is the head word with the dependency relation Rk to wi, and wj and wk are the pre</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Ward</author>
</authors>
<title>Extracting information in spontaneous speech.</title>
<date>1994</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="1515" citStr="Ward, 1994" startWordPosition="230" endWordPosition="231">epts which were not in the SLU training data. 1 Introduction Spoken Language Understanding (SLU) in dialogue systems is the task of taking the utterance output by a speech recognizer and assigning it a semantic label that represents the dialogue actions of that utterance accompanied with their associated attributes and values. For example, the utterance ”I would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). Gathering and annotating SLU data is costly and time consuming and therefore SLU datasets are small compare to the number of possible labels. Because training sets for a new domain are small, or non-existent, </context>
</contexts>
<marker>Ward, 1994</marker>
<rawString>Wayne Ward. 1994. Extracting information in spontaneous speech. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Large scale image annotation: Learning to rank with joint word-image embeddings.</title>
<date>2010</date>
<journal>Mach. Learn.,</journal>
<volume>81</volume>
<contexts>
<context position="2745" citStr="Weston et al., 2010" startWordPosition="437" endWordPosition="440">s often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dialogue actions may be included in the training set. The general idea to solve this type of problems is to map the input and class labels to a semantic space of usually lower dimension in which similar classes are represented by closer points in the space (Palatucci et al., 2009; Weston et al., 2011; Weston et al., 2010). Usually unsupervised knowledge sources are used to form semantic codes of the labels that helps us to generalize to unseen labels. On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen ”Please give me the telephone number” in training, but the user might ask ”Please give me the phone” at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of input feature parameters using vector-matrix multiplication (Bengio et al., 2003</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2010</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. Mach. Learn., 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three,</booktitle>
<pages>2764--2770</pages>
<contexts>
<context position="2723" citStr="Weston et al., 2011" startWordPosition="433" endWordPosition="436">-existent, learning is often an instance of Zero-shot or One-shot learning problems (Palatucci et al., 2009; L. Fei-Fei; Fergus, 2006), in which zero or few examples of some output classes are available during the training. For example, in the restaurant reservation domain, not all possible combinations of foods and dialogue actions may be included in the training set. The general idea to solve this type of problems is to map the input and class labels to a semantic space of usually lower dimension in which similar classes are represented by closer points in the space (Palatucci et al., 2009; Weston et al., 2011; Weston et al., 2010). Usually unsupervised knowledge sources are used to form semantic codes of the labels that helps us to generalize to unseen labels. On the other hand, there are also different ways to express the same meaning, and similarly, most of them can not be included in the training set. For instance, the system may have seen ”Please give me the telephone number” in training, but the user might ask ”Please give me the phone” at test time. This problem, feature sparsity, is a common issue in many NLP tasks. Decomposition of input feature parameters using vector-matrix multiplicatio</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three, pages 2764–2770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Yazdani</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Computing text semantic relatedness using the contents and links of a hypertext encyclopedia.</title>
<date>2013</date>
<journal>Artif. Intell.,</journal>
<pages>194--176</pages>
<contexts>
<context position="4096" citStr="Yazdani and Popescu-Belis, 2013" startWordPosition="652" endWordPosition="655">n this way, by sharing the word representations and composition matrices, we can overcome feature sparsity by producing similar representations for similar utterances. In order to represent words and concepts we use word embeddings, which are a form of vector space model. Word embeddings have proven to be effective models of semantic representation 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize</context>
</contexts>
<marker>Yazdani, Popescu-Belis, 2013</marker>
<rawString>Majid Yazdani and Andrei Popescu-Belis. 2013. Computing text semantic relatedness using the contents and links of a hypertext encyclopedia. Artif. Intell., 194:176–202.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
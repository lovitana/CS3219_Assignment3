<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000380">
<title confidence="0.989394">
Sentence Modeling with Gated Recursive Neural Network
</title>
<author confidence="0.998223">
Xinchi Chen, Xipeng Qiu; Chenxi Zhu, Shiyu Wu, Xuanjing Huang
</author>
<affiliation confidence="0.999349">
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.962991">
825 Zhangheng Road, Shanghai, China
</address>
<email confidence="0.999465">
{xinchichen13,xpqiu,czhu13,syu13,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858444444445">
Recently, neural network based sentence
modeling methods have achieved great
progress. Among these methods, the re-
cursive neural networks (RecNNs) can ef-
fectively model the combination of the
words in sentence. However, RecNNs
need a given external topological struc-
ture, like syntactic tree. In this paper,
we propose a gated recursive neural net-
work (GRNN) to model sentences, which
employs a full binary tree (FBT) struc-
ture to control the combinations in re-
cursive structure. By introducing two
kinds of gates, our model can better model
the complicated combinations of features.
Experiments on three text classification
datasets show the effectiveness of our
model.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999455">
Recently, neural network based sentence modeling
approaches have been increasingly focused on for
their ability to minimize the efforts in feature en-
gineering, such as Neural Bag-of-Words (NBoW),
Recurrent Neural Network (RNN) (Mikolov et al.,
2010), Recursive Neural Network (RecNN) (Pol-
lack, 1990; Socher et al., 2013b; Socher et al.,
2012) and Convolutional Neural Network (CNN)
(Kalchbrenner et al., 2014; Hu et al., 2014).
Among these methods, recursive neural net-
works (RecNNs) have shown their excellent abil-
ities to model the word combinations in sentence.
However, RecNNs require a pre-defined topolog-
ical structure, like parse tree, to encode sentence,
which limits the scope of its application. Cho et al.
(2014) proposed the gated recursive convolutional
neural network (grConv) by utilizing the directed
acyclic graph (DAG) structure instead of parse tree
</bodyText>
<note confidence="0.59593">
*Corresponding author.
</note>
<figure confidence="0.601039">
I cannot agree with you more I cannot agree with you more
</figure>
<figureCaption confidence="0.987172">
Figure 1: Example of Gated Recursive Neural
</figureCaption>
<bodyText confidence="0.990927363636364">
Networks (GRNNs). Left is a GRNN using a di-
rected acyclic graph (DAG) structure. Right is a
GRNN using a full binary tree (FBT) structure.
(The green nodes, gray nodes and white nodes
illustrate the positive, negative and neutral senti-
ments respectively.)
to model sentences. However, DAG structure is
relatively complicated. The number of the hidden
neurons quadraticly increases with the length of
sentences so that grConv cannot effectively deal
with long sentences.
Inspired by grConv, we propose a gated recur-
sive neural network (GRNN) for sentence model-
ing. Different with grConv, we use the full binary
tree (FBT) as the topological structure to recur-
sively model the word combinations, as shown in
Figure 1. The number of the hidden neurons lin-
early increases with the length of sentences. An-
other difference is that we introduce two kinds of
gates, reset and update gates (Chung et al., 2014),
to control the combinations in recursive structure.
With these two gating mechanisms, our model can
better model the complicated combinations of fea-
tures and capture the long dependency interac-
tions.
In our previous works, we have investigated
several different topological structures (tree and
directed acyclic graph) to recursively model the
semantic composition from the bottom layer to the
top layer, and applied them on Chinese word seg-
mentation (Chen et al., 2015a) and dependency
parsing (Chen et al., 2015b) tasks. However, these
structures are not suitable for modeling sentences.
</bodyText>
<page confidence="0.985007">
793
</page>
<note confidence="0.933973">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–798,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.8797745">
Figure 2: Architecture of Gated Recursive Neural
Network (GRNN).
</figureCaption>
<bodyText confidence="0.999471428571429">
In this paper, we adopt the full binary tree as the
topological structure to reduce the model com-
plexity.
Experiments on the Stanford Sentiment Tree-
bank dataset (Socher et al., 2013b) and the TREC
questions dataset (Li and Roth, 2002) show the ef-
fectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.983948" genericHeader="method">
2 Gated Recursive Neural Network
</sectionHeader>
<subsectionHeader confidence="0.985597">
2.1 Architecture
</subsectionHeader>
<bodyText confidence="0.999320428571429">
The recursive neural network (RecNN) need a
topological structure to model a sentence, such as
a syntactic tree. In this paper, we use a full binary
tree (FBT), as showing in Figure 2, to model the
combinations of features for a given sentence.
In fact, the FBT structure can model the com-
binations of features by continuously mixing the
information from the bottom layer to the top layer.
Each neuron can be regarded as a complicated
feature composition of its governed sub-sentence.
When the children nodes combine into their parent
node, the combination information of two children
nodes is also merged and preserved by their par-
ent node. As shown in Figure 2, we put all-zero
padding vectors after the last word of the sentence
until the length of 2⌈log2⌉, where n is the length of
the given sentence.
Inspired by the success of the gate mechanism
of Chung et al. (2014), we further propose a gated
recursive neural network (GRNN) by introducing
two kinds of gates, namely “reset gate” and “up-
date gate”. Specifically, there are two reset gates,
rL and rR, partially reading the information from
Figure 3: Our proposed gated recursive unit.
left child and right child respectively. And the up-
date gates zN, zL and zR decide what to preserve
when combining the children’s information. Intu-
itively, these gates seem to decide how to update
and exploit the combination information.
In the case of text classification, for each given
sentence xi = w(i)
1:N(i) and the corresponding class
yi, we first represent each word w(i)
j into its corre-
sponding embedding ww(z) ∈ Rd, where N(i) in-
dicates the length of i-th sentence and d is dimen-
sionality of word embeddings. Then, the embed-
dings are sent to the first layer of GRNN as inputs,
whose outputs are recursively applied to upper lay-
ers until it outputs a single fixed-length vector.
Next, we receive the class distribution P(·|xi; 0)
for the given sentence xi by a softmax transforma-
tion of ui, where ui is the top node of the network
(a fixed length vectorial representation):
P(·|xi; 0) = softmax(Ws × ui + bs), (1)
where bs ∈ R|T|, Ws ∈ R|T|xd. d is the dimen-
sionality of the top node ui, which is same with
the word embedding size and T represents the set
of possible classes. 0 represents the parameter set.
</bodyText>
<subsectionHeader confidence="0.991246">
2.2 Gated Recursive Unit
</subsectionHeader>
<bodyText confidence="0.999322888888889">
GRNN consists of the minimal structures, gated
recursive units, as showing in Figure 3.
By assuming that the length of sentence is n, we
will have recursion layer l ∈ [1, ⌈logn2 ⌉+1], where
symbol ⌈q⌉ indicates the minimal integer q* ≥ q.
At each recursion layer l, the activation of the j-
th (j ∈ [0, 2⌈log�⌉−l)) hidden node h(l)
j ∈ Rd is
computed as
</bodyText>
<figure confidence="0.9439695">
h(q = {corresponding
Zv ⊙ h�� + ZL ⊙ h2�1 + ZR ⊙ h2�+1, l &gt; word embedding, l = 1,
(2)
...
...
...
...
...
...
...
...
P(·|xi;θ)
Softmax(Ws × ui + bjui
0
(i) (i) (i)
w1 w2
(i) (i) w3 w4 w5
...
0 0
M,1) 41+11
Gate r, Gate rR
h�(�)
h�^(�)
Gate z
</figure>
<page confidence="0.991478">
794
</page>
<bodyText confidence="0.928059">
where zN, zL and zR E Rd are update gates
hlj, left child node hl−1
</bodyText>
<sectionHeader confidence="0.329324" genericHeader="method">
2j and
</sectionHeader>
<bodyText confidence="0.9882445">
right child node hl−1
2j+1 respectively, and O indi-
cates element-wise multiplication.
The update gates can be formalized as:
</bodyText>
<equation confidence="0.992515166666667">
� � � � �
1/Z hl.
1/Z ⊙ exp(U h2-1 ), (3)
j
1/Z hl−1
2j+1
</equation>
<bodyText confidence="0.998646">
where U E R3d×3d is the coefficient of update
gates, and Z E Rd is the vector of the normaliza-
tion coefficients,
</bodyText>
<equation confidence="0.99028475">
⎡ 3 hl•
Zk = ∑[exp(U⎢h2�
i=1 hl−1
2j+1
</equation>
<bodyText confidence="0.931741">
where 1 G k G d.
The new activation hlj is computed as:
</bodyText>
<equation confidence="0.953920666666667">
[ h� = tanh(Wˆh rL O hl−1
h-.
1),
(5)
rR O hl−1
2j+1
</equation>
<bodyText confidence="0.9098345">
where Wˆh E Rd×2d, rL E Rd, rR E Rd. rL and
rR are the reset gates for left child node hl−1
</bodyText>
<sectionHeader confidence="0.584617" genericHeader="method">
2j and
</sectionHeader>
<bodyText confidence="0.849107">
right child node hl−1
</bodyText>
<equation confidence="0.939463125">
2j+1 respectively, which can be
formalized as:
[ ]
[ rL ] hl−1
2j
= σ(G ), (6)
rR hl−1
2j+1
</equation>
<bodyText confidence="0.998237916666667">
where G E R2d×2d is the coefficient of two reset
gates and σ indicates the sigmoid function.
Intuiativly, the reset gates control how to select
the output information of the left and right chil-
dren, which result to the current new activation h.
By the update gates, the activation of a parent neu-
ron can be regarded as a choice among the the cur-
rent new activation h, the left child, and the right
child. This choice allows the overall structure to
change adaptively with respect to the inputs.
This gate mechanism is effective to model the
combinations of features.
</bodyText>
<subsectionHeader confidence="0.995412">
2.3 Training
</subsectionHeader>
<bodyText confidence="0.958511714285714">
We use the Maximum Likelihood (ML) criterion
to train our model. Given training set (xi, yi) and
the parameter set of our model θ, the goal is to
minimize the loss function:
Initial learning rate
Regularization
Dropout rate on input layer
</bodyText>
<tableCaption confidence="0.993361">
Table 1: Hyper-parameter settings.
</tableCaption>
<bodyText confidence="0.999860444444444">
where m is number of training sentences.
Following (Socher et al., 2013a), we use the di-
agonal variant of AdaGrad (Duchi et al., 2011)
with minibatchs to minimize the objective.
For parameter initialization, we use random ini-
tialization within (-0.01, 0.01) for all parameters
except the word embeddings. We adopt the pre-
trained English word embeddings from (Collobert
et al., 2011) and fine-tune them during training.
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.936903">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9599975">
To evaluate our approach, we test our model on
three datasets:
</bodyText>
<listItem confidence="0.9955305">
• SST-1 The movie reviews with five classes
in the Stanford Sentiment Treebank1 (Socher
et al., 2013b): negative, somewhat negative,
neutral, somewhat positive, positive.
• SST-2 The movie reviews with binary classes
in the Stanford Sentiment Treebank1 (Socher
et al., 2013b): negative, positive.
• QC The TREC questions dataset2 (Li and
Roth, 2002) involves six different question
types.
</listItem>
<subsectionHeader confidence="0.999889">
3.2 Hyper-parameters
</subsectionHeader>
<bodyText confidence="0.999919571428571">
Table 1 lists the hyper-parameters of our model. In
this paper, we also exploit dropout strategy (Sri-
vastava et al., 2014) to avoid overfitting. In ad-
dition, we set the batch size to 20. We set word
embedding size d = 50 on the TREC dataset
and d = 100 on the Stanford Sentiment Treebank
dataset.
</bodyText>
<subsectionHeader confidence="0.999123">
3.3 Experiment Results
</subsectionHeader>
<bodyText confidence="0.829123">
Table 2 shows the performance of our GRNN on
three datasets.
for new activation
</bodyText>
<equation confidence="0.900568363636364">
� ZN
Z � � ZL
ZR
⎤
⎦⎥)1d×(i−1)+k, (4)
α = 0.3
λ = 10−4
p = 20%
1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment
J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/
QA/QC/
</equation>
<page confidence="0.998634">
795
</page>
<note confidence="0.955115583333333">
Methods SST-1 SST-2 QC
NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2
PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8*
CNN-non-static (Kim, 2014) 48.0 87.2 93.6
CNN-multichannel (Kim, 2014) 47.4 88.1 92.2
MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4
DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0
RecNTN (Socher et al., 2013b) 45.7 85.4 -
RAE (Socher et al., 2011) 43.2 82.4 -
MV-RecNN (Socher et al., 2012) 44.4 82.9 -
AdaSent (Zhao et al., 2015) - - 92.4
GRNN (our approach) 47.5 85.5 93.8
</note>
<tableCaption confidence="0.9244835">
Table 2: Performances of the different models. The result of PV is from our own implementation based
on Gensim.
</tableCaption>
<bodyText confidence="0.998401764705882">
Competitor Models Neural Bag-of-Words
(NBOW) model is a simple and intuitive method
which ignores the word order. Paragraph Vector
(PV) (Le and Mikolov, 2014) learns continuous
distributed vector representations for pieces of
texts, which can be regarded as a long term
memory of sentences as opposed to short memory
in recurrent neural network. Here, we use the
popular open source implementation of PV in
Gensim1. Methods in the third block are CNN
based models. Kim (2014) reports 4 different
CNN models using max-over-time pooling, where
CNN-non-static and CNN-multichannel are more
sophisticated. MaxTDNN sentence model is
based on the architecture of the Time-Delay
Neural Network (TDNN) (Waibel et al., 1989;
Collobert and Weston, 2008). Dynamic convo-
lutional neural network (DCNN) (Kalchbrenner
et al., 2014) uses the dynamic k-max pooling
operator as a non-linear sub-sampling function, in
which the choice of k depends on the length of
given sentence. Methods in the fourth block are
RecNN based models. Recursive Neural Tensor
Network (RecNTN) (Socher et al., 2013b) is an
extension of plain RecNN, which also depends
on a external syntactic structure. Recursive
Autoencoder (RAE) (Socher et al., 2011) learns
the representations of sentences by minimizing
the reconstruction error. Matrix-Vector Recursive
Neural Network (MV-RecNN) (Socher et al.,
2012) is a extension of RecNN by assigning a
vector and a matrix to every node in the parse
tree. AdaSent (Zhao et al., 2015) adopts recursive
neural network using DAG structure.
</bodyText>
<footnote confidence="0.947849">
1https://github.com/piskvorky/gensim/
</footnote>
<bodyText confidence="0.999785576923077">
Moreover, the plain GRNN which does not in-
corporate the gate mechanism cannot outperform
the GRNN model. Theoretically, the plain GRNN
can be regarded as a special case of GRNN, whose
parameters are constrained or truncated. As a re-
sult, GRNN is a more powerful model which out-
performs the plain GRNN. Thus, we mainly focus
on the GRNN model in this paper.
Result Discussion Generally, our model is bet-
ter than the previous recursive neural network
based models (RecNTN, RAE, MV-RecNN and
AdaSent), which indicates our model can better
model the combinations of features with the FBT
and our gating mechanism, even without an exter-
nal syntactic tree.
Although we just use the top layer outputs as
the feature for classification, our model still out-
performs AdaSent.
Compared with the CNN based methods
(MaxTDNN, DCNN and CNNs), our model
achieves the comparable performances with much
fewer parameters. Although CNN based methods
outperform our model on SST-1 and SST-2, the
number of parameters2 of GRNN ranges from 40K
to 160K while the number of parameters is about
400K in CNN.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999563">
Cho et al. (2014) proposed grConv to model sen-
tences for machine translation. Unlike our model,
grConv uses the DAG structure as the topological
structure to model sentences. The number of the
</bodyText>
<footnote confidence="0.905824">
2We only take parameters of network into account, leav-
ing out word embeddings.
</footnote>
<page confidence="0.996974">
796
</page>
<bodyText confidence="0.999969142857143">
internal nodes is n2/2, where n is the length of the
sentence. Zhao et al. (2015) uses the same struc-
ture to model sentences (called AdaSent), and uti-
lizes the information of internal nodes to model
sentences for text classification. Unlike grConv
and AdaSent, our model uses full binary tree as
the topological structure. The number of the in-
ternal nodes is 2n in our model. Therefore, our
model is more efficient for long sentences. In ad-
dition, we just use the top layer neurons for text
classification.
Moreover, grConv and AdaSent only exploit
one gating mechanism (update gate), which cannot
sufficiently model the complicated feature com-
binations. Unlike them, our model incorporates
two kind of gates and can better model the feature
combinations.
Hu et al. (2014) also proposed a similar archi-
tecture for matching problems, but they employed
the convolutional neural network which might be
coarse in modeling the feature combinations.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999970444444444">
In this paper, we propose a gated recursive neu-
ral network (GRNN) to recursively summarize the
meaning of sentence. GRNN uses full binary tree
as the recursive topological structure instead of an
external syntactic tree. In addition, we introduce
two kinds of gates to model the complicated com-
binations of features. In future work, we would
like to investigate the other gating mechanisms for
better modeling the feature combinations.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999944375">
We would like to thank the anonymous review-
ers for their valuable comments. This work was
partially funded by the National Natural Science
Foundation of China (61472088, 61473092), Na-
tional High Technology Research and Develop-
ment Program of China (2015AA015408), Shang-
hai Science and Technology Development Funds
(14ZR1403200).
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99647170967742">
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
Chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics.
Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu,
and Xuanjing Huang. 2015b. Transition-based de-
pendency parsing using two heterogeneous gated re-
cursive neural networks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.
Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics, pages 556–
562.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
</reference>
<page confidence="0.970216">
797
</page>
<reference confidence="0.999355870967742">
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP).
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.
Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hin-
ton, Kiyohiro Shikano, and Kevin J Lang. 1989.
Phoneme recognition using time-delay neural net-
works. Acoustics, Speech and Signal Processing,
IEEE Transactions on, 37(3):328–339.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. arXiv
preprint arXiv:1504.05070.
</reference>
<page confidence="0.997045">
798
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717154">
<title confidence="0.999633">Sentence Modeling with Gated Recursive Neural Network</title>
<author confidence="0.8732815">Xipeng Zhu Chen</author>
<author confidence="0.8732815">Shiyu Wu</author>
<author confidence="0.8732815">Xuanjing Shanghai Key Laboratory of Intelligent Information Processing</author>
<author confidence="0.8732815">Fudan</author>
<affiliation confidence="0.996615">School of Computer Science, Fudan</affiliation>
<address confidence="0.995502">825 Zhangheng Road, Shanghai,</address>
<abstract confidence="0.998000368421053">Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Gated recursive neural network for Chinese word segmentation.</title>
<date>2015</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3393" citStr="Chen et al., 2015" startWordPosition="511" endWordPosition="514">ly increases with the length of sentences. Another difference is that we introduce two kinds of gates, reset and update gates (Chung et al., 2014), to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions. In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–798, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (L</context>
</contexts>
<marker>Chen, Qiu, Zhu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015a. Gated recursive neural network for Chinese word segmentation. In Proceedings of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xinchi Chen</author>
<author>Yaqian Zhou</author>
<author>Chenxi Zhu</author>
</authors>
<title>Xipeng Qiu, and Xuanjing Huang. 2015b. Transition-based dependency parsing using two heterogeneous gated recursive neural networks.</title>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Chen, Zhou, Zhu, </marker>
<rawString>Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu, and Xuanjing Huang. 2015b. Transition-based dependency parsing using two heterogeneous gated recursive neural networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merri¨enboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</title>
<date>2014</date>
<marker>Cho, van Merri¨enboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</title>
<date>2014</date>
<contexts>
<context position="2922" citStr="Chung et al., 2014" startWordPosition="439" endWordPosition="442">r, DAG structure is relatively complicated. The number of the hidden neurons quadraticly increases with the length of sentences so that grConv cannot effectively deal with long sentences. Inspired by grConv, we propose a gated recursive neural network (GRNN) for sentence modeling. Different with grConv, we use the full binary tree (FBT) as the topological structure to recursively model the word combinations, as shown in Figure 1. The number of the hidden neurons linearly increases with the length of sentences. Another difference is that we introduce two kinds of gates, reset and update gates (Chung et al., 2014), to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions. In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. 793 Proc</context>
<context position="4975" citStr="Chung et al. (2014)" startWordPosition="768" endWordPosition="771">he FBT structure can model the combinations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition of its governed sub-sentence. When the children nodes combine into their parent node, the combination information of two children nodes is also merged and preserved by their parent node. As shown in Figure 2, we put all-zero padding vectors after the last word of the sentence until the length of 2⌈log2⌉, where n is the length of the given sentence. Inspired by the success of the gate mechanism of Chung et al. (2014), we further propose a gated recursive neural network (GRNN) by introducing two kinds of gates, namely “reset gate” and “update gate”. Specifically, there are two reset gates, rL and rR, partially reading the information from Figure 3: Our proposed gated recursive unit. left child and right child respectively. And the update gates zN, zL and zR decide what to preserve when combining the children’s information. Intuitively, these gates seem to decide how to update and exploit the combination information. In the case of text classification, for each given sentence xi = w(i) 1:N(i) and the corres</context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10336" citStr="Collobert and Weston, 2008" startWordPosition="1740" endWordPosition="1743"> d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of tex</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8959" citStr="Collobert et al., 2011" startWordPosition="1512" endWordPosition="1515">) criterion to train our model. Given training set (xi, yi) and the parameter set of our model θ, the goal is to minimize the loss function: Initial learning rate Regularization Dropout rate on input layer Table 1: Hyper-parameter settings. where m is number of training sentences. Following (Socher et al., 2013a), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training. 3 Experiments 3.1 Datasets To evaluate our approach, we test our model on three datasets: • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, positive. • SST-2 The movie reviews with binary classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, positive. • QC The TREC questions dataset2 (Li and Roth, 2002) involves six different question types. 3.2 Hyper-parameters Table 1 lists the hyper-parameters of our model. In this paper,</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="8711" citStr="Duchi et al., 2011" startWordPosition="1475" endWordPosition="1478">he left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs. This gate mechanism is effective to model the combinations of features. 2.3 Training We use the Maximum Likelihood (ML) criterion to train our model. Given training set (xi, yi) and the parameter set of our model θ, the goal is to minimize the loss function: Initial learning rate Regularization Dropout rate on input layer Table 1: Hyper-parameter settings. where m is number of training sentences. Following (Socher et al., 2013a), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training. 3 Experiments 3.1 Datasets To evaluate our approach, we test our model on three datasets: • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, positive. • SST-2 The movie reviews with binary classes in the S</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1453" citStr="Hu et al., 2014" startWordPosition="204" endWordPosition="207">wo kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cannot agree with you more I cannot agree with you more Figure 1: Example of Gated Recursive Neural Networks (GRNNs). Left is a GRNN</context>
<context position="14429" citStr="Hu et al. (2014)" startWordPosition="2402" endWordPosition="2405">the information of internal nodes to model sentences for text classification. Unlike grConv and AdaSent, our model uses full binary tree as the topological structure. The number of the internal nodes is 2n in our model. Therefore, our model is more efficient for long sentences. In addition, we just use the top layer neurons for text classification. Moreover, grConv and AdaSent only exploit one gating mechanism (update gate), which cannot sufficiently model the complicated feature combinations. Unlike them, our model incorporates two kind of gates and can better model the feature combinations. Hu et al. (2014) also proposed a similar architecture for matching problems, but they employed the convolutional neural network which might be coarse in modeling the feature combinations. 5 Conclusion In this paper, we propose a gated recursive neural network (GRNN) to recursively summarize the meaning of sentence. GRNN uses full binary tree as the recursive topological structure instead of an external syntactic tree. In addition, we introduce two kinds of gates to model the complicated combinations of features. In future work, we would like to investigate the other gating mechanisms for better modeling the f</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1435" citStr="Kalchbrenner et al., 2014" startWordPosition="200" endWordPosition="203">structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cannot agree with you more I cannot agree with you more Figure 1: Example of Gated Recursive Neural Networks (GRNN</context>
<context position="10154" citStr="Kalchbrenner et al., 2014" startWordPosition="1712" endWordPosition="1715">rs of our model. In this paper, we also exploit dropout strategy (Srivastava et al., 2014) to avoid overfitting. In addition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) mo</context>
<context position="11523" citStr="Kalchbrenner et al., 2014" startWordPosition="1931" endWordPosition="1934">ector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use the popular open source implementation of PV in Gensim1. Methods in the third block are CNN based models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</title>
<date>2014</date>
<contexts>
<context position="10240" citStr="Kim, 2014" startWordPosition="1728" endWordPosition="1729">erfitting. In addition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (P</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10195" citStr="Mikolov, 2014" startWordPosition="1722" endWordPosition="1723">ut strategy (Srivastava et al., 2014) to avoid overfitting. In addition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method whic</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="4010" citStr="Li and Roth, 2002" startWordPosition="602" endWordPosition="605">5a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–798, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach. 2 Gated Recursive Neural Network 2.1 Architecture The recursive neural network (RecNN) need a topological structure to model a sentence, such as a syntactic tree. In this paper, we use a full binary tree (FBT), as showing in Figure 2, to model the combinations of features for a given sentence. In fact, the FBT structure can model the combinations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition of its governed sub-sentence. When the children nodes </context>
<context position="9435" citStr="Li and Roth, 2002" startWordPosition="1585" endWordPosition="1588">hin (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training. 3 Experiments 3.1 Datasets To evaluate our approach, we test our model on three datasets: • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, positive. • SST-2 The movie reviews with binary classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, positive. • QC The TREC questions dataset2 (Li and Roth, 2002) involves six different question types. 3.2 Hyper-parameters Table 1 lists the hyper-parameters of our model. In this paper, we also exploit dropout strategy (Srivastava et al., 2014) to avoid overfitting. In addition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � </context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics, pages 556– 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="1325" citStr="Pollack, 1990" startWordPosition="184" endWordPosition="186">tences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cann</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="10467" citStr="Socher et al., 2011" startWordPosition="1764" endWordPosition="1767">f our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use t</context>
<context position="11920" citStr="Socher et al., 2011" startWordPosition="1994" endWordPosition="1997">ticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure. 1https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN model. Theoretically, the plain GRNN can be regarded as a special case of GRNN, whose parameters are constrained or truncated. As a result</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="1369" citStr="Socher et al., 2012" startWordPosition="191" endWordPosition="194">ree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cannot agree with you more I cannot agree with y</context>
<context position="10510" citStr="Socher et al., 2012" startWordPosition="1772" endWordPosition="1775">ation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use the popular open source implementation of PV</context>
<context position="12072" citStr="Socher et al., 2012" startWordPosition="2013" endWordPosition="2016">Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure. 1https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN model. Theoretically, the plain GRNN can be regarded as a special case of GRNN, whose parameters are constrained or truncated. As a result, GRNN is a more powerful model which outperforms the plain GRNN. Thus, we mainly focus on the GRNN model in this paper. Result Discussion Generally, ou</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="1346" citStr="Socher et al., 2013" startWordPosition="187" endWordPosition="190">mploys a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cannot agree with you mor</context>
<context position="3957" citStr="Socher et al., 2013" startWordPosition="593" endWordPosition="596">ed them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–798, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach. 2 Gated Recursive Neural Network 2.1 Architecture The recursive neural network (RecNN) need a topological structure to model a sentence, such as a syntactic tree. In this paper, we use a full binary tree (FBT), as showing in Figure 2, to model the combinations of features for a given sentence. In fact, the FBT structure can model the combinations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition o</context>
<context position="8648" citStr="Socher et al., 2013" startWordPosition="1463" endWordPosition="1466"> regarded as a choice among the the current new activation h, the left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs. This gate mechanism is effective to model the combinations of features. 2.3 Training We use the Maximum Likelihood (ML) criterion to train our model. Given training set (xi, yi) and the parameter set of our model θ, the goal is to minimize the loss function: Initial learning rate Regularization Dropout rate on input layer Table 1: Hyper-parameter settings. where m is number of training sentences. Following (Socher et al., 2013a), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training. 3 Experiments 3.1 Datasets To evaluate our approach, we test our model on three datasets: • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, p</context>
<context position="10427" citStr="Socher et al., 2013" startWordPosition="1756" endWordPosition="1759">Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in </context>
<context position="11782" citStr="Socher et al., 2013" startWordPosition="1973" endWordPosition="1976"> models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure. 1https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN </context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP).</booktitle>
<contexts>
<context position="1346" citStr="Socher et al., 2013" startWordPosition="187" endWordPosition="190">mploys a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model. 1 Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree *Corresponding author. I cannot agree with you mor</context>
<context position="3957" citStr="Socher et al., 2013" startWordPosition="593" endWordPosition="596">ed them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–798, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach. 2 Gated Recursive Neural Network 2.1 Architecture The recursive neural network (RecNN) need a topological structure to model a sentence, such as a syntactic tree. In this paper, we use a full binary tree (FBT), as showing in Figure 2, to model the combinations of features for a given sentence. In fact, the FBT structure can model the combinations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition o</context>
<context position="8648" citStr="Socher et al., 2013" startWordPosition="1463" endWordPosition="1466"> regarded as a choice among the the current new activation h, the left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs. This gate mechanism is effective to model the combinations of features. 2.3 Training We use the Maximum Likelihood (ML) criterion to train our model. Given training set (xi, yi) and the parameter set of our model θ, the goal is to minimize the loss function: Initial learning rate Regularization Dropout rate on input layer Table 1: Hyper-parameter settings. where m is number of training sentences. Following (Socher et al., 2013a), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training. 3 Experiments 3.1 Datasets To evaluate our approach, we test our model on three datasets: • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, p</context>
<context position="10427" citStr="Socher et al., 2013" startWordPosition="1756" endWordPosition="1759">Results Table 2 shows the performance of our GRNN on three datasets. for new activation � ZN Z � � ZL ZR ⎤ ⎦⎥)1d×(i−1)+k, (4) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in </context>
<context position="11782" citStr="Socher et al., 2013" startWordPosition="1973" endWordPosition="1976"> models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure. 1https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Waibel</author>
<author>Toshiyuki Hanazawa</author>
<author>Geoffrey Hinton</author>
<author>Kiyohiro Shikano</author>
<author>Kevin J Lang</author>
</authors>
<title>Phoneme recognition using time-delay neural networks.</title>
<date>1989</date>
<journal>Acoustics, Speech and Signal Processing, IEEE Transactions on,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="11421" citStr="Waibel et al., 1989" startWordPosition="1917" endWordPosition="1920">es the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use the popular open source implementation of PV in Gensim1. Methods in the third block are CNN based models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recurs</context>
</contexts>
<marker>Waibel, Hanazawa, Hinton, Shikano, Lang, 1989</marker>
<rawString>Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang. 1989. Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):328–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhao</author>
<author>Zhengdong Lu</author>
<author>Pascal Poupart</author>
</authors>
<title>Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070.</title>
<date>2015</date>
<contexts>
<context position="10550" citStr="Zhao et al., 2015" startWordPosition="1780" endWordPosition="1783">) α = 0.3 λ = 10−4 p = 20% 1 ∑m log P(yi|xi; θ) + 2m11θ112 1http://nlp.stanford.edu/sentiment J(θ) = � m i=1 λ 2, (7) 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 795 Methods SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use the popular open source implementation of PV in Gensim1. Methods in the third block </context>
<context position="12192" citStr="Zhao et al., 2015" startWordPosition="2037" endWordPosition="2040">inear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure. 1https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN model. Theoretically, the plain GRNN can be regarded as a special case of GRNN, whose parameters are constrained or truncated. As a result, GRNN is a more powerful model which outperforms the plain GRNN. Thus, we mainly focus on the GRNN model in this paper. Result Discussion Generally, our model is better than the previous recursive neural network based models (RecNTN, RAE, MV-RecNN and AdaSent), which ind</context>
<context position="13738" citStr="Zhao et al. (2015)" startWordPosition="2290" endWordPosition="2293">l achieves the comparable performances with much fewer parameters. Although CNN based methods outperform our model on SST-1 and SST-2, the number of parameters2 of GRNN ranges from 40K to 160K while the number of parameters is about 400K in CNN. 4 Related Work Cho et al. (2014) proposed grConv to model sentences for machine translation. Unlike our model, grConv uses the DAG structure as the topological structure to model sentences. The number of the 2We only take parameters of network into account, leaving out word embeddings. 796 internal nodes is n2/2, where n is the length of the sentence. Zhao et al. (2015) uses the same structure to model sentences (called AdaSent), and utilizes the information of internal nodes to model sentences for text classification. Unlike grConv and AdaSent, our model uses full binary tree as the topological structure. The number of the internal nodes is 2n in our model. Therefore, our model is more efficient for long sentences. In addition, we just use the top layer neurons for text classification. Moreover, grConv and AdaSent only exploit one gating mechanism (update gate), which cannot sufficiently model the complicated feature combinations. Unlike them, our model inc</context>
</contexts>
<marker>Zhao, Lu, Poupart, 2015</marker>
<rawString>Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.857351">
Hierarchical Low-Rank Tensors for Multilingual Transfer Parsing
</title>
<author confidence="0.717389">
Yuan Zhang Regina Barzilay
</author>
<affiliation confidence="0.503752">
CSAIL, MIT CSAIL, MIT
</affiliation>
<email confidence="0.99174">
yuanzh@csail.mit.edu regina@csail.mit.edu
</email>
<sectionHeader confidence="0.994601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846291666667">
Accurate multilingual transfer parsing typ-
ically relies on careful feature engineer-
ing. In this paper, we propose a hierar-
chical tensor-based approach for this task.
This approach induces a compact feature
representation by combining atomic fea-
tures. However, unlike traditional tensor
models, it enables us to incorporate prior
knowledge about desired feature interac-
tions, eliminating invalid feature combi-
nations. To this end, we use a hierar-
chical structure that uses intermediate em-
beddings to capture desired feature com-
binations. Algebraically, this hierarchi-
cal tensor is equivalent to the sum of tra-
ditional tensors with shared components,
and thus can be effectively trained with
standard online algorithms. In both unsu-
pervised and semi-supervised transfer sce-
narios, our hierarchical tensor consistently
improves UAS and LAS over state-of-the-
art multilingual transfer parsers and the
traditional tensor model across 10 differ-
ent languages.1
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878545454545">
The goal of multilingual syntactic transfer is to
parse a resource lean target language utilizing an-
notations available in other languages. Recent ap-
proaches have demonstrated that such transfer is
possible, even in the absence of parallel data. As
a main source of guidance, these methods rely on
the commonalities in dependency structures across
languages. These commonalities manifest them-
selves through a broad and diverse set of indi-
cators, ranging from standard arc features used
in monolingual parsers to typological properties
</bodyText>
<footnote confidence="0.858444">
1The source code is available at https://github.
com/yuanzh/TensorTransfer.
</footnote>
<figure confidence="0.955066714285714">
Verb-subject:
{head POS=VERB} ∧ {modifier POS=NOUN}
∧{label=subj} ∧ {direction=LEFT}∧
{82A=SV}
Noun-adjective:
{head POS=NOUN} ∧ {modifier POS=ADJ}∧
{direction=LEFT} ∧ {87A=Adj-Noun}
</figure>
<tableCaption confidence="0.695081">
Table 1: Example verb-subject and noun-adjective
</tableCaption>
<bodyText confidence="0.9407333">
typological features. 82A and 87A denote the
WALS (Dryer et al., 2005) feature codes for verb-
subject and noun-adjective ordering preferences.
needed to guide cross-lingual sharing (e.g., verb-
subject ordering preference). In fact, careful fea-
ture engineering has been shown to play a cru-
cial role in state-of-the-art multilingual transfer
parsers (T¨ackstr¨om et al., 2013).
Tensor-based models are an appealing alterna-
tive to manual feature design. These models auto-
matically induce a compact feature representation
by factorizing a tensor constructed from atomic
features (e.g., the head POS). No prior knowledge
about feature interactions is assumed. As a result,
the model considers all possible combinations of
atomic features, and addresses the parameter ex-
plosion problem via a low-rank assumption.
In the multilingual transfer setting, however, we
have some prior knowledge about legitimate fea-
ture combinations. Consider for instance a ty-
pological feature that encodes verb-subject pref-
erences. As Table 1 shows, it is expressed as a
conjunction of five atomic features. Ideally, we
would like to treat this composition as a single
non-decomposable feature. However, the tradi-
tional tensor model decomposes this feature into
multiple dimensions, and considers various com-
binations of these features as well as their indi-
vidual interactions with other features. Moreover,
we want to avoid invalid combinations that con-
</bodyText>
<page confidence="0.950013">
1857
</page>
<note confidence="0.9848645">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1857–1867,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.996084346153846">
join the above feature with unrelated atomic fea-
tures. For instance, there is no point to construct-
ing features of the form {head POS=ADJ}∧{head
POS=VERB} ∧ · · · ∧ {82A=SV} as the head POS
takes a single value. However, the traditional
tensor technique still considers these unobserved
feature combinations, and assigns them non-zero
weights (see Section 7). This inconsistency be-
tween prior knowledge and the low-rank assump-
tion results in a sub-optimal parameter estimation.
To address this issue, we introduce a hierarchi-
cal tensor model that constrains parameter repre-
sentation. The model encodes prior knowledge
by explicitly excluding undesired feature combi-
nations over the same atomic features. At the bot-
tom level of the hierarchy, the model constructs
combinations of atomic features, generating inter-
mediate embeddings that represent the legitimate
feature groupings. For instance, these groupings
will not combine the verb-subject ordering feature
and the POS head feature. At higher levels of
the hierarchy, the model combines these embed-
dings as well as the expert-defined typological fea-
tures over the same atomic features. The hierar-
chical tensor is thereby able to capture the interac-
tion between features at various subsets of atomic
features. Algebraically, the hierarchical tensor is
equivalent to the sum of traditional tensors with
shared components. Thus, we can use standard
online algorithms for optimizing the low-rank hi-
erarchical tensor.
We evaluate our model on labeled dependency
transfer parsing using the newly released multi-
lingual universal dependency treebank (McDonald
et al., 2013). We compare our model against the
state-of-the-art multilingual transfer dependency
parser (T¨ackstr¨om et al., 2013) and the direct
transfer model (McDonald et al., 2011). All the
parsers utilize the same training resources but with
different feature representations. When trained on
source languages alone, our model outperforms
the baselines for 7 out of 10 languages on both
unlabeled attachment score (UAS) and labeled at-
tachment score (LAS). On average, it achieves
1.1% UAS improvement over T¨ackstr¨om et al.
(2013)’s model and 4.8% UAS over the direct
transfer. We also consider a semi-supervised set-
ting where multilingual data is augmented with 50
annotated sentences in the target language. In this
case, our model achieves improvement of 1.7%
UAS over T¨ackstr¨om et al. (2013)’s model and
4.5% UAS over the direct transfer.
</bodyText>
<sectionHeader confidence="0.997079" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999220625">
Multilingual Parsing The lack of annotated
parsing resources for the vast majority of world
languages has kindled significant interest in multi-
source parsing transfer (Hwa et al., 2005; Dur-
rett et al., 2012; Zeman and Resnik, 2008; Yu
et al., 2013b; Cohen et al., 2011; Rasooli and
Collins, 2015). Recent research has focused on
the non-parallel setting, where transfer is driven
by cross-lingual commonalities in syntactic struc-
ture (Naseem et al., 2010; T¨ackstr¨om et al., 2013;
Berg-Kirkpatrick and Klein, 2010; Cohen and
Smith, 2009; Duong et al., 2015).
Our work is closely related to the selective-
sharing approaches (Naseem et al., 2012;
T¨ackstr¨om et al., 2013). The core of these
methods is the assumption that head-modifier
attachment preferences are universal across
different languages. However, the sharing of arc
direction is selective and is based on typological
features. While this selective sharing idea was
first realized in the generative model (Naseem et
al., 2012), higher performance was achieved in
a discriminative arc-factored model (T¨ackstr¨om
et al., 2013). These gains were obtained by a
careful construction of features templates that
combine standard dependency parsing features
and typological features. In contrast, we propose
an automated, tensor-based approach that can
effectively capture the interaction between these
features, yielding a richer representation for cross-
lingual transfer. Moreover, our model handles
labeled dependency parsing while previous work
only focused on the unlabeled dependency parsing
task.
Tensor-based Models Our approach also relates
to prior work on tensor-based modeling. Lei et
al. (2014) employ three-way tensors to obtain a
low-dimensional input representation optimized
for parsing performance. Srikumar and Manning
(2014) learn a multi-class label embedding tai-
lored for document classification and POS tag-
ging in the tensor framework. Yu and Dredze
(2015), Fried et al. (2015) apply low-rank ten-
sor decompositions to learn task-specific word and
phrase embeddings. Other applications of tensor
framework include low-rank regularization (Pri-
madhanty et al., 2015; Quattoni et al., 2014; Singh
et al., 2015) and neural tensor networks (Socher et
</bodyText>
<page confidence="0.976919">
1858
</page>
<figure confidence="0.9995734">
Hcφhc
Lφl
MCφmC
Dφd
Hφh Mφm
Hφh Mφm Dφd
e4
e3
Tlφtl
Hcφhc
Mcφmc
Lφl
Tuφtu
ez
e,
</figure>
<figureCaption confidence="0.990199">
Figure 1: Visual representation for traditional mul-
tiway tensor.
</figureCaption>
<bodyText confidence="0.638063666666667">
+ φ
al., 2013; Yu et al., 2013a). While these methods
can automatically combine atomic features into
</bodyText>
<equation confidence="0.694464">
φh φm φd
</equation>
<bodyText confidence="0.999693571428571">
a compact composite representation, they cannot
take into account constraints on feature combina-
tion. In contrast, our method can capture features
at different composition levels, and more gener-
ally can incorporate structural constraints based on
prior knowledge. As our experiments show, this
approach delivers higher transfer accuracy.
</bodyText>
<sectionHeader confidence="0.804184" genericHeader="method">
3 Hierarchical Low-rank Scoring for
Transfer Parsing
</sectionHeader>
<subsectionHeader confidence="0.999646">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.999973545454546">
We start by briefly reviewing the traditional three-
way tensor scoring function (Lei et al., 2014). The
three-way tensor characterizes each arc h —* m
using the tensor-product over three feature vec-
tors: the head vector (Oh E Rn), the modifier vec-
tor (Om E Rn) and the arc vector (Oh→m E Rl).
Oh captures atomic features associated with the
head, such as its POS tag and its word form. Simi-
larly, Om and Oh→m capture atomic features asso-
ciated with the modifier and the arc respectively.
The tensor-product of these three vectors is a rank-
</bodyText>
<equation confidence="0.713579">
1 tensor:
Oh ® Om ® Oh→m E Rn×n×l
</equation>
<bodyText confidence="0.999781333333333">
This rank-1 tensor captures all possible combina-
tions of the atomic features in each vector, and
therefore significantly expands the feature set. The
tensor score is the inner product between a three-
way parameter tensor A E Rn×n×l and this rank-1
feature tensor:
</bodyText>
<equation confidence="0.934867">
vec(A) · vec(Oh ® Om ® Oh→m)
</equation>
<bodyText confidence="0.990034">
where vec(·) denotes the vector representation of a
tensor. This tensor scoring method avoids the pa-
rameter explosion and overfitting problem by as-
suming a low-rank factorization of the parameters
Figure 2: Visual representation for hierarchical
tensor, represented as a tree structure. The ten-
sor first captures the low-level interaction (HOh,
MOm and DOd) by an element-wise product, and
then combines the intermediate embedding with
other components higher in the hierarchy, e.g. e2
and LOl. The equations show that we composite
two representations by an element-wise sum.
</bodyText>
<equation confidence="0.875169">
A. Specifically, A is decomposed into the sum of
r rank-1 components:
r
A = U(i) ® V (i) ® W (i)
i=1
</equation>
<bodyText confidence="0.9999362">
where r is the rank of the tensor, U, V E Rr×n
and W E Rr×l are parameter matrices. U(i) de-
notes the i-th row of matrix U and similarly for
V (i) and W (i). Figure 1 shows the representation
of a more general multiway factorization. With
this factorization, the model effectively alleviates
the feature explosion problem by projecting sparse
feature vectors into dense r-dimensional embed-
dings via U, V and W. Subsequently, the score is
computed as follows:
</bodyText>
<equation confidence="0.99192">
r
Stensor(h —* m) = [UOh]i[V Om]i[W Oh→m]i
i=1
</equation>
<bodyText confidence="0.99998675">
where [·]i denotes the i-th element of the matrix.
In multilingual transfer, however, we want to
incorporate typological features that do not fit in
any of the components. For example, if we add
the verb-subject ordering preference into Oh→m,
the tensor will represent the concatenation of this
preference with a noun-adjective arc, even though
this feature should never trigger.
</bodyText>
<subsectionHeader confidence="0.999624">
3.2 Hierarchical Low-rank Tensor
</subsectionHeader>
<bodyText confidence="0.999814">
To address this issue, we propose the hierarchi-
cal factorization of tensor parameters.2 The key
idea is to generate intermediate embeddings that
capture the interaction of the same set of atomic
</bodyText>
<footnote confidence="0.990243">
2In this section we focus on delexicalized transfer, and
describe the lexicalization process in Section 3.3.
</footnote>
<page confidence="0.995906">
1859
</page>
<bodyText confidence="0.999464384615385">
features as other expert-defined features. As Fig-
ure 2 shows, this design enables the model to han-
dle expert-defined features over various subsets of
the atomic features.
Now, we will illustrate this idea in the context of
multilingual parsing. Table 2 summarizes the no-
tations of the feature vectors and the correspond-
ing parameters. Specifically, for each arc h → m
with label l, we first compute the intermediate fea-
ture embedding e1 that captures the interaction be-
tween the head φh, the modifier φm and the arc
direction and length φd, by an element-wise prod-
uct.
</bodyText>
<equation confidence="0.996394">
[e1]i = [Hφh]i[Mφm]i[Dφd]i (1)
</equation>
<bodyText confidence="0.999911882352941">
where [·]i denotes the i-th value of the feature em-
bedding, and H, M and D are the parameter ma-
trices as in Table 2. The embedding e1 cap-
tures the unconstrained interaction over the head,
the modifier and the arc. Note that φtu includes
expert-defined typological features that rely on the
specific values of the head POS, the modifier POS
and the arc direction, such as the example noun-
adjective feature in Table 1. Therefore, the em-
bedding Tuφtu captures an expert-defined interac-
tion over the head, the modifier and the arc. Thus
e1 and Tuφtu provide two different representations
of the same set of atomic features (e.g. the head)
and our prior knowledge motivates us to exclude
the interaction between them since the low-rank
assumption would not apply. Thus, we combine
e1 and Tuφtu as e2 using an element-wise sum
</bodyText>
<equation confidence="0.997457">
[e2]i = [e1]i + [Tuφtu]i (2)
</equation>
<bodyText confidence="0.99997">
and thereby avoid such combinations. As Fig-
ure 2 shows, e2 in turn is used to capture the higher
level interaction with arc label features φl,
</bodyText>
<equation confidence="0.996842">
[e3]i = [Lφl]i[e2]i (3)
</equation>
<bodyText confidence="0.994411666666667">
Now e3 captures the interaction between head,
modifier, arc direction, length and label. It is over
the same set of atomic features as the typological
features that depend on arc labels φtl, such as the
example verb-subject ordering feature in Table 1.
Therefore, we sum over these embeddings as
</bodyText>
<equation confidence="0.997411">
[e4]i = [e3]i + [Tlφtl]i (4)
</equation>
<bodyText confidence="0.9975315">
Finally, we capture the interaction between
e4 and context feature embeddings Hcφhc and
</bodyText>
<table confidence="0.984356363636364">
Notation Description
H, φh Head/modifier POS tag
M, φm
D, φd Arc length and direction
L, φl Arc label
Tu, φtu Typological features that depend on
head/modifier POS but not arc label
Tl, φtl Typological features that depend
on arc label
Hc, φhc POS tags of head/modifier
Mc, φmc neighboring words
</table>
<tableCaption confidence="0.865348333333333">
Table 2: Notations and descriptions of parame-
ter matrices and feature vectors in our hierarchical
tensor model.
</tableCaption>
<bodyText confidence="0.774182">
Mcφmc and compute the tensor score as
</bodyText>
<equation confidence="0.984873666666667">
r
Stensor(h →−l m) = [Hcφhc]i[Mcφmc][e4]i
i=1
</equation>
<bodyText confidence="0.807339666666667">
By combining Equation 1 to 5, we observe
that our hierarchical tensor score decomposes into
three multiway tensor scoring functions.
</bodyText>
<equation confidence="0.985282363636364">
r
Stensor(h →−l m) = [Hcφhc]i[Mcφmc]i
i=1
{ [Tlφtl]i + [Lφl]i
( /�
[Tuφtu]i + [Hφh]i[Mφm]i[Dφd]i
r
{[Hcφhc]i[Mcφmc]i[Tlφtl]i
i=1
+[Hcφhc]i[Mcφmc]i[Lφl]i[Tuφtu]i l
+[Hcφhc]i [Mcφmc]i [Lφl]i [Hφh]i [Mφm]i [Dφd]i J
</equation>
<bodyText confidence="0.999942571428571">
This decomposition provides another view of
our tensor model. That is, our hierarchical tensor
is algebraically equivalent to the sum of three mul-
tiway tensors, where Hc, Mc and L are shared.3
From this perspective, we can see that our tensor
model effectively captures the following three sets
of combinations over atomic features:
</bodyText>
<listItem confidence="0.904230666666667">
f1: φhc ⊗ φmc ⊗ φtl
f2: φhc ⊗ φmc ⊗ φl ⊗ φtu
f3: φhc ⊗ φmc ⊗ φl ⊗ φh ⊗ φm ⊗ φd
</listItem>
<footnote confidence="0.6848945">
3We could also associate each multiway tensor with a dif-
ferent weight. In our work, we keep them weighted equally.
</footnote>
<page confidence="0.98199">
1860
</page>
<bodyText confidence="0.999962714285714">
The last set of features f3 captures the interac-
tion across standard atomic features. The other
two sets of features f1 and f2 focus on combin-
ing atomic typological features with atomic label
and context features. Consequently, we explicitly
assign zero weights for invalid assignments, by ex-
cluding the combination of φtu with φh and φm.
</bodyText>
<subsectionHeader confidence="0.999113">
3.3 Lexicalization Components
</subsectionHeader>
<bodyText confidence="0.999677">
In order to encode lexical information in our
tensor-based model, we add two additional com-
ponents, Hwφhw and Mwφmw, for head and mod-
ifier lexicalization respectively. We compute the
final score as the interaction between the delexi-
calized feature embedding in Equation 5 and the
lexical components. Specifically:
</bodyText>
<equation confidence="0.9995905">
[e5]i = [Hcφhc]i[Mcφmc]i[e4]i
Stensor(h *−l m) = 1:r [Hwφhw]i[Mwφmw]i[e5]i
i=1
(7)
</equation>
<bodyText confidence="0.999147666666667">
where e5 is the embedding that represents the
delexicalized transfer results. We describe the fea-
tures in φhw and φmw in Section 5.
</bodyText>
<subsectionHeader confidence="0.975662">
3.4 Combined Scoring
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Similar to previous work on low-rank tensor scor-
ing models (Lei et al., 2014; Lei et al., 2015), we
combine the traditional scoring and the low-rank
tensor scoring. More formally, for a sentence x
and a dependency tree y, our final scoring func-
tion has the form
</bodyText>
<equation confidence="0.9929165">
S(x, y) = γ 1: w · φ(h *− l m)
hl−*mEy
+ (1 − γ) 1: Stensor(h *−l m) (8)
hl−*mEy
</equation>
<bodyText confidence="0.999859625">
where φ(h *−l m) is the traditional features for
arc h * m with label l and w is the correspond-
ing parameter vector. γ E [0, 1] is the balanc-
ing hyper-parameter and we tune the value on the
development set. The parameters in our model
are θ = (w, H, M, D, L, Tu, Tl, Hc, Mc), and our
goal is to optimize all parameters given the train-
ing set.
</bodyText>
<sectionHeader confidence="0.992044" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999986105263158">
In this section, we describe our learning method.4
Following standard practice, we optimize the pa-
rameters θ = (w, H, M, D, L, Tu, Tl, Hc, Mc) in
a maximum soft-margin framework, using online
passive-aggressive (PA) updates (Crammer et al.,
2006).
For tensor parameter update, we employ the
joint update method originally used by Lei et al.
(2015) in the context of four-way tensors. While
our tensor has a very high order (8 components for
the delexicalized parser and 10 for the lexicalized
parser) and is hierarchical, the gradient computa-
tion is nevertheless similar to that of traditional
tensors. As described in Section 3.2, we can view
our hierarchical tensor as the combination of three
multiway tensors with parameter sharing. There-
fore, we can compute the gradient of each mul-
tiway tensor and take the sum accordingly. For
example, the gradient of the label component is
</bodyText>
<equation confidence="0.999700444444444">
∂L = 1: ((Hcφhc) O (Mcφmc) O [(Tuφtu)
hl−*mEy∗
+ (Hφh) O (Mφm) O (Dφd)] ® φl
)
((Hcφhc) O (Mcφmc) O [(Tuφtu)
1: −
hl−*mE˜y
)
+ (Hφh) O (Mφm) O (Dφd)] ® φl (9)
</equation>
<bodyText confidence="0.999922888888889">
where O is the element-wise product and + de-
notes the element-wise addition. y∗ and y˜ are the
gold tree and the maximum violated tree respec-
tively. For each sentence x, we find y˜ via cost-
augmented decoding.
Tensor Initialization Given the high tensor or-
der, initialization has a significant impact on the
learning quality. We extend the previous power
method for high-order tensor initialization (Lei et
al., 2015) to the hierarchical structure using the al-
gebraic view as in computing the gradient.
Briefly, the power method incrementally com-
putes the most important rank-1 component for
H(i), M(i) etc, for i = 1... r. In each iteration,
the algorithm updates each component by taking
the multiplication between the tensor T and the
rest of the components. When we update the label
component l, we do the multiplication for different
</bodyText>
<footnote confidence="0.9874755">
4Our description focuses on delexicalized transfer, and we
can easily extend the method to the lexicalized case.
</footnote>
<page confidence="0.97633">
1861
</page>
<bodyText confidence="0.724025">
Feature Description
</bodyText>
<note confidence="0.85297">
82A Order of Subject and Verb
83A Order of Object and Verb
85A Order of Adposition and Noun Phrase
86A Order of Genitive and Noun
87A Order of Adjective and Noun
</note>
<tableCaption confidence="0.94212">
Table 3: Typological features from WALS (Dryer
</tableCaption>
<bodyText confidence="0.77192975">
et al., 2005) used to build the feature tem-
plates in our work, inspired by Naseem et al.
(2012). Unlike previous work (Naseem et al.,
2012; T¨ackstr¨om et al., 2013), we use 82A and
83A instead of 81A (order of subject, object and
verb) because we can distinguish between subject
and object relations based on dependency labels.
multiway tensors and then take the sum.
</bodyText>
<equation confidence="0.514028">
l = (To, hc, mc, −, tu) + (Ti, hc, mc, −, h, m, d)
</equation>
<bodyText confidence="0.999561">
where the operator (To, hc, mc, −, tu) returns a
vector in which the i-th element is computed as
Euvw To(i, u, v, w)hc(u)mc(v)tu(w). The algo-
rithm updates other components in a similar fash-
ion until convergence.
</bodyText>
<sectionHeader confidence="0.999367" genericHeader="method">
5 Features
</sectionHeader>
<subsectionHeader confidence="0.595677">
Linear Scoring Features Our traditional lin-
</subsectionHeader>
<bodyText confidence="0.999878933333333">
ear scoring features in φ(h *−l m) are mainly
drawn from previous work (T¨ackstr¨om et al.,
2013). Table 3 lists the typological features
from “The World Atlas of Language Structure
(WALS)” (Dryer et al., 2005) used to build the fea-
ture templates in our work. We use 82A and 83A
for verb-subject and verb-object order respectively
because we can distinguish between these two re-
lations based on dependency labels. Table 4 sum-
marizes the typological feature templates we use.
In addition, we expand features with dependency
labels to enable labeled dependency parsing.
Tensor Scoring Features For our tensor model,
feature vectors listed in Table 2 capture the five
types of atomic features as follows:
</bodyText>
<listItem confidence="0.949207625">
(a) φh, φm: POS tags of the head or the modifier.
(b) φhc, φmc: POS tags of the left/right neighbor-
ing words.
(c) φl: dependency labels.
(d) φd: dependency length conjoined with direc-
tion.
(e) φtu, φtl: selectively shared typological fea-
tures, as described in Table 4.
</listItem>
<bodyText confidence="0.99081927027027">
Table 4: Typological feature templates used in our
work. hp/mp are POS tags of the head/modifier.
dir E {LEFT, RIGHT} denotes the arc direction.
82A-87A denote the WALS typological feature
value. δ(·) is the indicator function. subj E l
denotes that the arc label l indicates a subject rela-
tion, and similarly for obj E l.
We further conjoin atomic features (b) and (d) with
the family and the typological class of the lan-
guage, because the arc direction and the word or-
der distribution depends on the typological prop-
erty of languages (T¨ackstr¨om et al., 2013). We
also add a bias term into each feature vector.
Partial Lexicalization We utilize multilingual
word embeddings to incorporate partial lexical
information in our model. We use the CCA
method (Faruqui and Dyer, 2014) to generate
multilingual word embeddings. Specifically, we
project word vectors in each non-English language
to the English embedding space. To reduce the
noise from the automatic projection process, we
only incorporate lexical information for the top-
100 most frequent words in the following closed
classes: pronoun, determiner, adposition, conjunc-
tion, particle and punctuation mark. Therefore, we
call this feature extension partial lexicalization.5
We follow previous work (Lei et al., 2014) for
adding embedding features. For the linear scoring
model, we simply append the head and the modi-
fier word embeddings after the feature vector. For
the tensor-based model, we add each entry of the
word embedding as a feature value into φhw and
φmw. In addition, we add indicator features for the
English translation of words because this improves
performance in preliminary experiments. For ex-
ample, for the German word und, we add the word
and as a feature.
</bodyText>
<footnote confidence="0.962373666666667">
5In our preliminary experiments, we observe that our lexi-
calized model usually outperforms the unlexicalized counter-
parts by about 2%.
</footnote>
<equation confidence="0.9775889">
dir·82A·δ(hp=VERB∧mp=NOUN∧subjE l)
dir·82A·δ(hp=VERB∧mp=PRON∧subjE l)
dir·83A·δ(hp=VERB∧mp=NOUN∧objE l)
dir·83A·δ(hp=VERB∧mp=PRON∧objE l)
dir·85A·δ(hp=ADP∧mp=NOUN)
dir·85A·δ(hp=ADP∧mp=PRON)
dir·86A·δ(hp=NOUN∧mp=NOUN)
dir·87A·δ(hp=ADJ∧mp=NOUN)
φtl
φtu
</equation>
<page confidence="0.990992">
1862
</page>
<sectionHeader confidence="0.998134" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999811833333334">
Dataset We evaluate our model on the newly re-
leased multilingual universal dependency treebank
v2.0 (McDonald et al., 2013) that consists of 10
languages: English (EN), French (FR), German
(DE), Indonesian (ID), Italian (IT), Japanese (JA),
Korean (KO), Brazilian-Portuguese (PT), Spanish
(ES) and Swedish (SV). This multilingual tree-
bank is annotated with a universal POS tagset and
a universal dependency label set. Therefore, this
dataset is an excellent benchmark for cross-lingual
transfer evaluation. For POS tags, the gold uni-
versal annotation used the coarse tagset (Petrov et
al., 2011) that consists of 12 tags: noun, verb, ad-
jective, adverb, pronoun, determiner, adposition,
numeral, conjunction, particle, punctuation mark,
and a catch-all tag X. For dependency labels, the
universal annotation developed the Stanford de-
pendencies (De Marneffe and Manning, 2008) into
a rich set of 40 labels. This universal annota-
tion enables labeled dependency parsing in cross-
lingual transfer.
Evaluation Scenarios We first consider the un-
supervised transfer scenario, in which we assume
no target language annotations are available. Fol-
lowing the standard setup, for each target language
evaluated, we train our model on the concatenation
of the training data in all other source languages.
In addition, we consider the semi-supervised
transfer scenario, in which we assume 50 sen-
tences in the target language are available with an-
notation. However, we observe that random sen-
tence selection of the supervised sample results
in a big performance variance. Instead, we se-
lect sentences that contain patterns that are absent
or rare in source language treebanks. To this end,
each time we greedily select the sentence that min-
imizes the KL divergence between the trigram dis-
tribution of the target language and the trigram dis-
tribution of the training data after adding this sen-
tence. The training data includes both the target
and the source languages. The trigrams are based
on universal POS tags. Note that our method does
not require any dependency annotations. To incor-
porate the new supervision, we simply add the new
sentences into the original training set, weighing
their impact by a factor of 10.
Baselines We compare against different variants
of our model.
</bodyText>
<listItem confidence="0.952317133333333">
• Direct: a direct transfer baseline (McDonald et
al., 2011) that uses only delexicalized features
in the MSTParser (McDonald et al., 2005).
• NT-Select: our model without the tensor com-
ponent. This baseline corresponds to the prior
feature-based transfer method (T¨ackstr¨om et al.,
2013) with extensions to labeled parsing, lexi-
calization and semi-supervised parsing.6
• Multiway: tensor-based model where typolog-
ical features are added as an additional compo-
nent and parameters are factorized in the multi-
way structure similarly as in Figure 1.
• Sup50: our model trained only on the 50
sentences in the target language in the semi-
supervised scenario.
</listItem>
<bodyText confidence="0.99926832">
In all the experiments we incorporate partial lexi-
calization for all variants of our model and we fo-
cus on labeled dependency parsing.
Supervised Upper Bound As a performance
upper bound, we train the RBGParser (Lei et al.,
2014), the state-of-the-art tensor-based parser, on
the full target language training set. We train the
first-order model7 with default parameter settings,
using the current version of the code.8
Evaluation Measures Following standard prac-
tices, we report unlabeled attachment score (UAS)
and labeled attachment score (LAS), excluding
punctuation. For all experiments, we report results
on the test set and omit the development results be-
cause of space.
Experimental Details For all experiments, we
use the arc-factored model and use Eisner’s algo-
rithm (Eisner, 1996) to infer the projective Viterbi
parse. We train our model and the baselines for 10
epochs. We set a strong regularization C = 0.001
during learning because cross-lingual transfer con-
tains noise and the models can easily overfit. Other
hyper-parameters are set as γ = 0.3 and r = 200
(rank of the tensor). For partial lexicalization, we
set the embedding dimension to 50.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.986115666666667">
Table 5 and 7 summarize the results for the unsu-
pervised and the semi-supervised scenarios. Aver-
aged across languages, our model outperforms all
</bodyText>
<footnote confidence="0.999207166666667">
6We use this as a re-implementation of T¨ackstr¨om et al.
(2013)’s model because their code is not publicly available.
7All multilingual transfer models in our work and
in T¨ackstr¨om et al. (2013)’s work are first-order. Therefore,
we train first-order RBGParser for consistency.
8https://github.com/taolei87/RBGParser
</footnote>
<page confidence="0.718736">
1863
</page>
<table confidence="0.999832461538462">
Direct NT-Select Multiway Ours
UAS LAS UAS LAS UAS LAS UAS LAS
EN 65.7 56.7 67.6 55.3 69.8 56.3 70.5 59.8
FR 77.9 67.4 79.1 68.9 78.4 68.3 78.9 68.8
DE 62.1 53.1 62.1 53.3 62.1 54.0 62.5 54.1
ID 46.8 39.3 57.4 37.1 59.5 38.9 61.0 43.5
IT 77.9 67.9 79.4 69.4 79.0 69.0 79.3 69.4
JA 57.8 16.8 69.2 20.8 69.9 20.4 71.7 21.3
KO 59.9 34.3 70.4 29.1 70.5 28.1 70.7 30.5
PT 77.7 71.0 78.5 72.0 78.3 71.9 78.6 72.5
ES 76.8 65.9 77.2 67.7 77.6 68.0 78.0 68.3
SV 75.9 64.5 74.5 62.2 74.8 62.9 75.0 62.5
AVG 67.8 53.7 71.5 53.6 72.0 53.8 72.6 55.1
</table>
<tableCaption confidence="0.798418666666667">
Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of
different variants of our model with partial lexicalization in unsupervised scenario. “Direct” and “Multi-
way” indicate the direct transfer and the multiway variants of our model. “NT-Select” indicates our model
without tensor component, corresponding to a re-implementation of previous transfer model (T¨ackstr¨om
et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the
results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
</tableCaption>
<table confidence="0.999596833333333">
Feature Weight
87Anhp=NOUNnmp=ADJ 2.24 x 10−3
87Anhp=VERBnmp=NOUN 8.88 x 10−4
87Anhp=VERBnmp=PRON 1.21 x 10−4
87Anhp=NOUNnmp=NOUN 9.48 x 10−4
87Anhp=ADPnmp=NOUN 3.87 x 10−4
</table>
<tableCaption confidence="0.965445">
Table 6: Examples of weights for feature
</tableCaption>
<bodyText confidence="0.992256905660377">
combinations between the typological feature
87A=Adj-Noun and different types of arcs. The
first row shows the weight for the valid feature
(conjoined with noun-*adjective arcs) and the rest
show weights for the invalid features (conjoined
with other types of arcs).
the baselines in both cases. Moreover, it achieves
best UAS and LAS on 7 out of 10 languages.
The difference is more pronounced in the semi-
supervised case. Below, we summarize our find-
ings when comparing the model with the base-
lines.
Impact of Hierarchical Tensors We first ana-
lyze the impact of using a hierarchical tensor by
comparing against the Multiway baseline that im-
plements traditional tensor model. As Table 6
shows, this model learns non-zero weights even
for invalid feature combinations.
This disregard to known constraints impacts the
resulting performance. In the unsupervised sce-
nario, our hierarchical tensor achieves an aver-
age improvement of 0.5% on UAS and 1.3% on
LAS. Moreover, our model obtains better UAS on
all languages and better LAS on 9 out of 10 lan-
guages. This observation shows that the multi-
lingual transfer consistently benefits more from a
hierarchical tensor structure. In addition, we ob-
serve a similar gain over this baseline in the semi-
supervised scenario.
Impact of Tensor Models To evaluate the effec-
tiveness of tensor modeling in multilingual trans-
fer, we compare our model against the NT-Select
baseline. In the unsupervised scenario, our ten-
sor model yields a 1.1% gain on UAS and a 1.5%
on LAS. In the semi-supervised scenario, the im-
provement is more pronounced, reaching 1.7% on
UAS and 1.9% on LAS. The relative error reduc-
tion almost doubles, e.g. 7.1% vs. 3.8% on UAS.
While both our model and NT-Select outper-
form Direct baseline by a large margin on UAS,
we observe that NT-Select achieves a slightly
worse LAS than Direct. By adding a tensor com-
ponent, our model outperforms both baselines on
LAS, demonstrating that tensor scoring function is
able to capture better labeled features for transfer
comparing to Direct and NT-Select baselines.
Transfer Performance in the Context of Super-
vised Results To assess the contribution of mul-
tilingual transfer, we compare against the Sup50
results in which we train our model only on 50
target language sentences. As Table 7 shows,
our model improves UAS by 2.3% and LAS by
2.7%. We also provide a performance upper bound
</bodyText>
<page confidence="0.970713">
1864
</page>
<table confidence="0.999849428571429">
Semi-supervised Transfer Supervised Parsing (RBGParser)
Direct Sup50 NT-Select Multiway Ours Partial Lex. Full Lex.
UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
EN 76.8 70.3 79.6 74.2 81.0 75.0 81.5 75.9 82.5 77.2 88.7 84.5 92.3 90.3
FR 78.8 70.2 76.9 66.8 79.4 71.0 79.0 71.1 79.6 71.8 83.3 76.5 83.3 76.5
DE 68.4 59.8 71.0 62.4 71.3 62.1 72.1 63.2 74.2 65.6 82.0 72.8 84.5 78.2
ID 63.7 56.1 78.2 68.9 76.9 68.2 77.8 69.3 79.1 70.4 85.0 77.1 85.8 79.8
IT 78.9 70.3 77.1 69.3 80.2 72.2 80.8 72.6 80.9 72.6 85.5 79.8 87.9 84.7
JA 68.2 42.1 76.6 61.0 73.0 58.8 75.6 60.9 76.4 61.3 79.0 64.0 82.1 70.3
KO 65.3 45.2 70.1 54.7 66.5 50.2 67.8 52.8 70.2 54.2 74.0 59.1 90.9 86.1
PT 78.6 72.9 76.0 70.0 78.7 73.1 79.3 73.9 79.3 73.5 85.2 80.8 88.5 86.5
ES 77.0 68.5 75.2 66.5 77.0 69.0 77.6 69.5 78.4 70.5 82.0 75.0 85.8 81.6
SV 77.7 67.2 74.9 64.7 77.6 66.8 77.8 67.5 78.3 67.9 84.4 75.4 87.3 82.3
AVG 73.4 62.3 75.6 65.8 76.2 66.6 76.9 67.7 77.9 68.5 82.9 74.5 87.3 83.5
</table>
<tableCaption confidence="0.995297">
Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50
</tableCaption>
<bodyText confidence="0.983928777777778">
annotated sentences in the target language are available. “Sup50” columns show the results of our model
when only supervised data in the target language is available. We also include in the last two columns
the supervised training results with partial or full lexicalization as the performance upper bound. Other
columns have the same meaning as in Table 5. Boldface numbers indicate the best UAS or LAS.
by training RBGParser on the full training set.9
When trained with partial lexical information as
in our model, RBGParser gives 82.9% on UAS
and 74.5% on LAS with partial lexical informa-
tion. By utilizing source language annotations, our
model closes the performance gap between train-
ing on the 50 sentences and on the full training set
by about 30% on both UAS and LAS. We further
compare to the performance upper bound with full
lexical information (87.3% UAS and 83.5% LAS).
In this case, our model still closes the performance
gap by 21% on UAS and 15% on LAS.
Time Efficiency of Hierarchical Tensors We
observe that our hierarchical structure retains the
time efficiency of tensor models. On the English
test set, the decoding speed of our hierarchical ten-
sor is close to the multiway counterpart (58.6 vs.
61.2 sentences per second), and is lower than the
three-way tensor by a factor of 3.1 (184.4 sen-
tences per second). The time complexity of ten-
sors is linear to the number of low-rank com-
ponents, and is independent of the factorization
structure.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.932776230769231">
In this paper, we introduce a hierarchical tensor
based-model which enables us to constrain learned
representation based on desired feature interac-
tions. We demonstrate that our model outperforms
state-of-the-art multilingual transfer parsers and
9On average, each language has more than 10,000 training
sentences.
traditional tensors. These observations, taken to-
gether with the fact that hierarchical tensors are
efficiently learnable, suggest that the approach can
be useful in a broader range of parsing applica-
tions; exploring the options is an appealing line of
future research.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999966769230769">
This research is developed in a collaboration
of MIT with the Arabic Language Technologies
(ALT) group at Qatar Computing Research In-
stitute (QCRI) within the Interactive sYstems for
Answer Search (IYAS) project. The authors ac-
knowledge the support of the U.S. Army Research
Office under grant number W911NF-10-1-0533.
We thank the MIT NLP group and the EMNLP
reviewers for their comments. Any opinions, find-
ings, conclusions, or recommendations expressed
in this paper are those of the authors, and do not
necessarily reflect the views of the funding orga-
nizations.
</bodyText>
<sectionHeader confidence="0.997056" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9065166">
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phy-
logenetic grammar induction. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1288–1297. Association
for Computational Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceed-
ings of the Annual Conference of the North Amer-
ican Chapter of the Association for Computational
</reference>
<page confidence="0.937528">
1865
</page>
<reference confidence="0.999590693693693">
Linguistics, pages 74–82. Association for Computa-
tional Linguistics.
Shay B Cohen, Dipanjan Das, and Noah A Smith.
2011. Unsupervised structure prediction with non-
parallel multilingual guidance. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 50–61. Association for
Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551–585.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8. Association for Com-
putational Linguistics.
Matthew S Dryer, David Gil, Bernard Comrie, Hagen
Jung, Claudia Schmidt, et al. 2005. The world atlas
of language structures.
Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Cross-lingual transfer for unsu-
pervised dependency parsing without parallel data.
Proceedings of the SIGNLL Conference on Compu-
tational Natural Language Learning, page 113.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.
Association for Computational Linguistics.
Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340–345. Association
for Computational Linguistics.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of the Annual Confer-
ence of the European Chapter of the Association for
Computational Linguistics., volume 2014.
Daniel Fried, Tamara Polajnar, and Stephen Clark.
2015. Low-rank tensors for verbs in compositional
distributional semantics. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering, 11(03):311–325.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, volume 1, pages 1381–1391.
Tao Lei, Yuan Zhang, Regina Barzilay, Llu´ıs M`arquez,
and Alessandro Moschitti. 2015. High-order low-
rank tensors for semantic role labeling. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91–98. Association for Computa-
tional Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62–72. Association for Computational Lin-
guistics.
Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Os-
car T¨ackstr¨om, et al. 2013. Universal dependency
annotation for multilingual parsing. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics, pages 92–97.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637. Asso-
ciation for Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Audi Primadhanty, Xavier Carreras, and Ariadna Quat-
toni. 2015. Low-rank regularization for sparse con-
junctive feature spaces: An application to named en-
tity classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics.
Ariadna Quattoni, Borja Balle, Xavier Carreras, and
Amir Globerson. 2014. Spectral regularization
for max-margin sequence tagging. In Proceedings
of the 31st International Conference on Machine
Learning (ICML-14), pages 1710–1718.
Mohammad Sadegh Rasooli and Michael Collins.
2015. Density-driven cross-lingual transfer of de-
pendency parsers. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics.
</reference>
<page confidence="0.837504">
1866
</page>
<reference confidence="0.99976423076923">
Sameer Singh, Tim Rocktaschel, and Sebastian Riedel.
2015. Towards combined matrix and tensor fac-
torization for universal schema relation extraction.
In NAACL Workshop on Vector Space Modeling for
NLP.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Proceedings of the Advances in Neural Information
Processing Systems, pages 926–934.
Vivek Srikumar and Christopher D Manning. 2014.
Learning distributed representations for structured
output prediction. In Proceedings of the Advances
in Neural Information Processing Systems, pages
3266–3274.
Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of the Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Mo Yu and Mark Dredze. 2015. Learning composition
models for phrase embeddings. Transactions of the
Association for Computational Linguistics, 3:227–
242.
Dong Yu, Li Deng, and Frank Seide. 2013a. The
deep tensor neural network with applications to
large vocabulary speech recognition. Audio, Speech,
and Language Processing, IEEE Transactions on,
21(2):388–396.
Mo Yu, Tiejun Zhao, Yalong Bai, Hao Tian, and Di-
anhai Yu. 2013b. Cross-lingual projections between
languages from different families. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 312–317.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the International Joint
Conference on Natural Language Processing, pages
35–42.
</reference>
<page confidence="0.994354">
1867
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658730">
<title confidence="0.999967">Hierarchical Low-Rank Tensors for Multilingual Transfer Parsing</title>
<author confidence="0.999739">Yuan Zhang Regina Barzilay</author>
<affiliation confidence="0.941484">CSAIL, MIT CSAIL, MIT</affiliation>
<email confidence="0.998102">yuanzh@csail.mit.eduregina@csail.mit.edu</email>
<abstract confidence="0.98647675">Accurate multilingual transfer parsing typically relies on careful feature engineering. In this paper, we propose a hierarchical tensor-based approach for this task. This approach induces a compact feature representation by combining atomic features. However, unlike traditional tensor models, it enables us to incorporate prior knowledge about desired feature interactions, eliminating invalid feature combinations. To this end, we use a hierarchical structure that uses intermediate embeddings to capture desired feature combinations. Algebraically, this hierarchical tensor is equivalent to the sum of traditional tensors with shared components, and thus can be effectively trained with standard online algorithms. In both unsupervised and semi-supervised transfer scenarios, our hierarchical tensor consistently improves UAS and LAS over state-of-theart multilingual transfer parsers and the traditional tensor model across 10 differ-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1288--1297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6587" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="963" endWordPosition="966"> improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored model (T¨ackstr¨om et al., 2013). These gains were obtained </context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288–1297. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6610" citStr="Cohen and Smith, 2009" startWordPosition="967" endWordPosition="970">kstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful constructi</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with nonparallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>50--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6341" citStr="Cohen et al., 2011" startWordPosition="928" endWordPosition="931">str¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on t</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with nonparallel multilingual guidance. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 50–61. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="16989" citStr="Crammer et al., 2006" startWordPosition="2684" endWordPosition="2687">h *−l m) (8) hl−*mEy where φ(h *−l m) is the traditional features for arc h * m with label l and w is the corresponding parameter vector. γ E [0, 1] is the balancing hyper-parameter and we tune the value on the development set. The parameters in our model are θ = (w, H, M, D, L, Tu, Tl, Hc, Mc), and our goal is to optimize all parameters given the training set. 4 Learning In this section, we describe our learning method.4 Following standard practice, we optimize the parameters θ = (w, H, M, D, L, Tu, Tl, Hc, Mc) in a maximum soft-margin framework, using online passive-aggressive (PA) updates (Crammer et al., 2006). For tensor parameter update, we employ the joint update method originally used by Lei et al. (2015) in the context of four-way tensors. While our tensor has a very high order (8 components for the delexicalized parser and 10 for the lexicalized parser) and is hierarchical, the gradient computation is nevertheless similar to that of traditional tensors. As described in Section 3.2, we can view our hierarchical tensor as the combination of three multiway tensors with parameter sharing. Therefore, we can compute the gradient of each multiway tensor and take the sum accordingly. For example, the</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew S Dryer</author>
<author>David Gil</author>
<author>Bernard Comrie</author>
<author>Hagen Jung</author>
<author>Claudia Schmidt</author>
</authors>
<title>The world atlas of language structures.</title>
<date>2005</date>
<contexts>
<context position="2045" citStr="Dryer et al., 2005" startWordPosition="281" endWordPosition="284"> on the commonalities in dependency structures across languages. These commonalities manifest themselves through a broad and diverse set of indicators, ranging from standard arc features used in monolingual parsers to typological properties 1The source code is available at https://github. com/yuanzh/TensorTransfer. Verb-subject: {head POS=VERB} ∧ {modifier POS=NOUN} ∧{label=subj} ∧ {direction=LEFT}∧ {82A=SV} Noun-adjective: {head POS=NOUN} ∧ {modifier POS=ADJ}∧ {direction=LEFT} ∧ {87A=Adj-Noun} Table 1: Example verb-subject and noun-adjective typological features. 82A and 87A denote the WALS (Dryer et al., 2005) feature codes for verbsubject and noun-adjective ordering preferences. needed to guide cross-lingual sharing (e.g., verbsubject ordering preference). In fact, careful feature engineering has been shown to play a crucial role in state-of-the-art multilingual transfer parsers (T¨ackstr¨om et al., 2013). Tensor-based models are an appealing alternative to manual feature design. These models automatically induce a compact feature representation by factorizing a tensor constructed from atomic features (e.g., the head POS). No prior knowledge about feature interactions is assumed. As a result, the </context>
<context position="18982" citStr="Dryer et al., 2005" startWordPosition="3025" endWordPosition="3028">mponent for H(i), M(i) etc, for i = 1... r. In each iteration, the algorithm updates each component by taking the multiplication between the tensor T and the rest of the components. When we update the label component l, we do the multiplication for different 4Our description focuses on delexicalized transfer, and we can easily extend the method to the lexicalized case. 1861 Feature Description 82A Order of Subject and Verb 83A Order of Object and Verb 85A Order of Adposition and Noun Phrase 86A Order of Genitive and Noun 87A Order of Adjective and Noun Table 3: Typological features from WALS (Dryer et al., 2005) used to build the feature templates in our work, inspired by Naseem et al. (2012). Unlike previous work (Naseem et al., 2012; T¨ackstr¨om et al., 2013), we use 82A and 83A instead of 81A (order of subject, object and verb) because we can distinguish between subject and object relations based on dependency labels. multiway tensors and then take the sum. l = (To, hc, mc, −, tu) + (Ti, hc, mc, −, h, m, d) where the operator (To, hc, mc, −, tu) returns a vector in which the i-th element is computed as Euvw To(i, u, v, w)hc(u)mc(v)tu(w). The algorithm updates other components in a similar fashion </context>
</contexts>
<marker>Dryer, Gil, Comrie, Jung, Schmidt, 2005</marker>
<rawString>Matthew S Dryer, David Gil, Bernard Comrie, Hagen Jung, Claudia Schmidt, et al. 2005. The world atlas of language structures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Duong</author>
<author>Trevor Cohn</author>
<author>Steven Bird</author>
<author>Paul Cook</author>
</authors>
<title>Cross-lingual transfer for unsupervised dependency parsing without parallel data.</title>
<date>2015</date>
<booktitle>Proceedings of the SIGNLL Conference on Computational Natural Language Learning,</booktitle>
<pages>113</pages>
<contexts>
<context position="6631" citStr="Duong et al., 2015" startWordPosition="971" endWordPosition="974"> model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful construction of features templa</context>
</contexts>
<marker>Duong, Cohn, Bird, Cook, 2015</marker>
<rawString>Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Cross-lingual transfer for unsupervised dependency parsing without parallel data. Proceedings of the SIGNLL Conference on Computational Natural Language Learning, page 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6279" citStr="Durrett et al., 2012" startWordPosition="915" endWordPosition="919">e (LAS). On average, it achieves 1.1% UAS improvement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. Howeve</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 1,</booktitle>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26487" citStr="Eisner, 1996" startWordPosition="4204" endWordPosition="4205">er bound, we train the RBGParser (Lei et al., 2014), the state-of-the-art tensor-based parser, on the full target language training set. We train the first-order model7 with default parameter settings, using the current version of the code.8 Evaluation Measures Following standard practices, we report unlabeled attachment score (UAS) and labeled attachment score (LAS), excluding punctuation. For all experiments, we report results on the test set and omit the development results because of space. Experimental Details For all experiments, we use the arc-factored model and use Eisner’s algorithm (Eisner, 1996) to infer the projective Viterbi parse. We train our model and the baselines for 10 epochs. We set a strong regularization C = 0.001 during learning because cross-lingual transfer contains noise and the models can easily overfit. Other hyper-parameters are set as γ = 0.3 and r = 200 (rank of the tensor). For partial lexicalization, we set the embedding dimension to 50. 7 Results Table 5 and 7 summarize the results for the unsupervised and the semi-supervised scenarios. Averaged across languages, our model outperforms all 6We use this as a re-implementation of T¨ackstr¨om et al. (2013)’s model </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340–345. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Conference of the European Chapter of the Association for Computational Linguistics.,</booktitle>
<volume>volume</volume>
<contexts>
<context position="21406" citStr="Faruqui and Dyer, 2014" startWordPosition="3438" endWordPosition="3441">te the WALS typological feature value. δ(·) is the indicator function. subj E l denotes that the arc label l indicates a subject relation, and similarly for obj E l. We further conjoin atomic features (b) and (d) with the family and the typological class of the language, because the arc direction and the word order distribution depends on the typological property of languages (T¨ackstr¨om et al., 2013). We also add a bias term into each feature vector. Partial Lexicalization We utilize multilingual word embeddings to incorporate partial lexical information in our model. We use the CCA method (Faruqui and Dyer, 2014) to generate multilingual word embeddings. Specifically, we project word vectors in each non-English language to the English embedding space. To reduce the noise from the automatic projection process, we only incorporate lexical information for the top100 most frequent words in the following closed classes: pronoun, determiner, adposition, conjunction, particle and punctuation mark. Therefore, we call this feature extension partial lexicalization.5 We follow previous work (Lei et al., 2014) for adding embedding features. For the linear scoring model, we simply append the head and the modifier </context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of the Annual Conference of the European Chapter of the Association for Computational Linguistics., volume 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fried</author>
<author>Tamara Polajnar</author>
<author>Stephen Clark</author>
</authors>
<title>Low-rank tensors for verbs in compositional distributional semantics.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8026" citStr="Fried et al. (2015)" startWordPosition="1171" endWordPosition="1174">eraction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constr</context>
</contexts>
<marker>Fried, Polajnar, Clark, 2015</marker>
<rawString>Daniel Fried, Tamara Polajnar, and Stephen Clark. 2015. Low-rank tensors for verbs in compositional distributional semantics. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering,</title>
<date>2005</date>
<pages>11--03</pages>
<contexts>
<context position="6257" citStr="Hwa et al., 2005" startWordPosition="911" endWordPosition="914">ed attachment score (LAS). On average, it achieves 1.1% UAS improvement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across diffe</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering, 11(03):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>1381--1391</pages>
<contexts>
<context position="7733" citStr="Lei et al. (2014)" startWordPosition="1128" endWordPosition="1131">c-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful construction of features templates that combine standard dependency parsing features and typological features. In contrast, we propose an automated, tensor-based approach that can effectively capture the interaction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφ</context>
<context position="9057" citStr="Lei et al., 2014" startWordPosition="1330" endWordPosition="1333">or. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experiments show, this approach delivers higher transfer accuracy. 3 Hierarchical Low-rank Scoring for Transfer Parsing 3.1 Background We start by briefly reviewing the traditional threeway tensor scoring function (Lei et al., 2014). The three-way tensor characterizes each arc h —* m using the tensor-product over three feature vectors: the head vector (Oh E Rn), the modifier vector (Om E Rn) and the arc vector (Oh→m E Rl). Oh captures atomic features associated with the head, such as its POS tag and its word form. Similarly, Om and Oh→m capture atomic features associated with the modifier and the arc respectively. The tensor-product of these three vectors is a rank1 tensor: Oh ® Om ® Oh→m E Rn×n×l This rank-1 tensor captures all possible combinations of the atomic features in each vector, and therefore significantly expa</context>
<context position="16122" citStr="Lei et al., 2014" startWordPosition="2514" endWordPosition="2517">o encode lexical information in our tensor-based model, we add two additional components, Hwφhw and Mwφmw, for head and modifier lexicalization respectively. We compute the final score as the interaction between the delexicalized feature embedding in Equation 5 and the lexical components. Specifically: [e5]i = [Hcφhc]i[Mcφmc]i[e4]i Stensor(h *−l m) = 1:r [Hwφhw]i[Mwφmw]i[e5]i i=1 (7) where e5 is the embedding that represents the delexicalized transfer results. We describe the features in φhw and φmw in Section 5. 3.4 Combined Scoring Similar to previous work on low-rank tensor scoring models (Lei et al., 2014; Lei et al., 2015), we combine the traditional scoring and the low-rank tensor scoring. More formally, for a sentence x and a dependency tree y, our final scoring function has the form S(x, y) = γ 1: w · φ(h *− l m) hl−*mEy + (1 − γ) 1: Stensor(h *−l m) (8) hl−*mEy where φ(h *−l m) is the traditional features for arc h * m with label l and w is the corresponding parameter vector. γ E [0, 1] is the balancing hyper-parameter and we tune the value on the development set. The parameters in our model are θ = (w, H, M, D, L, Tu, Tl, Hc, Mc), and our goal is to optimize all parameters given the trai</context>
<context position="21901" citStr="Lei et al., 2014" startWordPosition="3508" endWordPosition="3511">al word embeddings to incorporate partial lexical information in our model. We use the CCA method (Faruqui and Dyer, 2014) to generate multilingual word embeddings. Specifically, we project word vectors in each non-English language to the English embedding space. To reduce the noise from the automatic projection process, we only incorporate lexical information for the top100 most frequent words in the following closed classes: pronoun, determiner, adposition, conjunction, particle and punctuation mark. Therefore, we call this feature extension partial lexicalization.5 We follow previous work (Lei et al., 2014) for adding embedding features. For the linear scoring model, we simply append the head and the modifier word embeddings after the feature vector. For the tensor-based model, we add each entry of the word embedding as a feature value into φhw and φmw. In addition, we add indicator features for the English translation of words because this improves performance in preliminary experiments. For example, for the German word und, we add the word and as a feature. 5In our preliminary experiments, we observe that our lexicalized model usually outperforms the unlexicalized counterparts by about 2%. dir</context>
<context position="25925" citStr="Lei et al., 2014" startWordPosition="4120" endWordPosition="4123">ckstr¨om et al., 2013) with extensions to labeled parsing, lexicalization and semi-supervised parsing.6 • Multiway: tensor-based model where typological features are added as an additional component and parameters are factorized in the multiway structure similarly as in Figure 1. • Sup50: our model trained only on the 50 sentences in the target language in the semisupervised scenario. In all the experiments we incorporate partial lexicalization for all variants of our model and we focus on labeled dependency parsing. Supervised Upper Bound As a performance upper bound, we train the RBGParser (Lei et al., 2014), the state-of-the-art tensor-based parser, on the full target language training set. We train the first-order model7 with default parameter settings, using the current version of the code.8 Evaluation Measures Following standard practices, we report unlabeled attachment score (UAS) and labeled attachment score (LAS), excluding punctuation. For all experiments, we report results on the test set and omit the development results because of space. Experimental Details For all experiments, we use the arc-factored model and use Eisner’s algorithm (Eisner, 1996) to infer the projective Viterbi parse</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1381–1391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Llu´ıs M`arquez</author>
<author>Alessandro Moschitti</author>
</authors>
<title>High-order lowrank tensors for semantic role labeling.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Lei, Zhang, Barzilay, M`arquez, Moschitti, 2015</marker>
<rawString>Tao Lei, Yuan Zhang, Regina Barzilay, Llu´ıs M`arquez, and Alessandro Moschitti. 2015. High-order lowrank tensors for semantic role labeling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25180" citStr="McDonald et al., 2005" startWordPosition="4001" endWordPosition="4004"> the target language and the trigram distribution of the training data after adding this sentence. The training data includes both the target and the source languages. The trigrams are based on universal POS tags. Note that our method does not require any dependency annotations. To incorporate the new supervision, we simply add the new sentences into the original training set, weighing their impact by a factor of 10. Baselines We compare against different variants of our model. • Direct: a direct transfer baseline (McDonald et al., 2011) that uses only delexicalized features in the MSTParser (McDonald et al., 2005). • NT-Select: our model without the tensor component. This baseline corresponds to the prior feature-based transfer method (T¨ackstr¨om et al., 2013) with extensions to labeled parsing, lexicalization and semi-supervised parsing.6 • Multiway: tensor-based model where typological features are added as an additional component and parameters are factorized in the multiway structure similarly as in Figure 1. • Sup50: our model trained only on the 50 sentences in the target language in the semisupervised scenario. In all the experiments we incorporate partial lexicalization for all variants of our</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>62--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5391" citStr="McDonald et al., 2011" startWordPosition="775" endWordPosition="778">ereby able to capture the interaction between features at various subsets of atomic features. Algebraically, the hierarchical tensor is equivalent to the sum of traditional tensors with shared components. Thus, we can use standard online algorithms for optimizing the low-rank hierarchical tensor. We evaluate our model on labeled dependency transfer parsing using the newly released multilingual universal dependency treebank (McDonald et al., 2013). We compare our model against the state-of-the-art multilingual transfer dependency parser (T¨ackstr¨om et al., 2013) and the direct transfer model (McDonald et al., 2011). All the parsers utilize the same training resources but with different feature representations. When trained on source languages alone, our model outperforms the baselines for 7 out of 10 languages on both unlabeled attachment score (UAS) and labeled attachment score (LAS). On average, it achieves 1.1% UAS improvement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨acks</context>
<context position="25101" citStr="McDonald et al., 2011" startWordPosition="3989" endWordPosition="3992">e sentence that minimizes the KL divergence between the trigram distribution of the target language and the trigram distribution of the training data after adding this sentence. The training data includes both the target and the source languages. The trigrams are based on universal POS tags. Note that our method does not require any dependency annotations. To incorporate the new supervision, we simply add the new sentences into the original training set, weighing their impact by a factor of 10. Baselines We compare against different variants of our model. • Direct: a direct transfer baseline (McDonald et al., 2011) that uses only delexicalized features in the MSTParser (McDonald et al., 2005). • NT-Select: our model without the tensor component. This baseline corresponds to the prior feature-based transfer method (T¨ackstr¨om et al., 2013) with extensions to labeled parsing, lexicalization and semi-supervised parsing.6 • Multiway: tensor-based model where typological features are added as an additional component and parameters are factorized in the multiway structure similarly as in Figure 1. • Sup50: our model trained only on the 50 sentences in the target language in the semisupervised scenario. In al</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 62–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
<author>Yvonne QuirmbachBrundage</author>
<author>Yoav Goldberg</author>
<author>Dipanjan Das</author>
<author>Kuzman Ganchev</author>
<author>Keith B Hall</author>
<author>Slav Petrov</author>
<author>Hao Zhang</author>
<author>Oscar T¨ackstr¨om</author>
</authors>
<title>Universal dependency annotation for multilingual parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>92--97</pages>
<marker>McDonald, Nivre, QuirmbachBrundage, Goldberg, Das, Ganchev, Hall, Petrov, Zhang, T¨ackstr¨om, 2013</marker>
<rawString>Ryan T McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et al. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6527" citStr="Naseem et al., 2010" startWordPosition="955" endWordPosition="958">rget language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored </context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234–1244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>629--637</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6716" citStr="Naseem et al., 2012" startWordPosition="985" endWordPosition="988"> lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful construction of features templates that combine standard dependency parsing features and typological features. In co</context>
<context position="19064" citStr="Naseem et al. (2012)" startWordPosition="3041" endWordPosition="3044">tes each component by taking the multiplication between the tensor T and the rest of the components. When we update the label component l, we do the multiplication for different 4Our description focuses on delexicalized transfer, and we can easily extend the method to the lexicalized case. 1861 Feature Description 82A Order of Subject and Verb 83A Order of Object and Verb 85A Order of Adposition and Noun Phrase 86A Order of Genitive and Noun 87A Order of Adjective and Noun Table 3: Typological features from WALS (Dryer et al., 2005) used to build the feature templates in our work, inspired by Naseem et al. (2012). Unlike previous work (Naseem et al., 2012; T¨ackstr¨om et al., 2013), we use 82A and 83A instead of 81A (order of subject, object and verb) because we can distinguish between subject and object relations based on dependency labels. multiway tensors and then take the sum. l = (To, hc, mc, −, tu) + (Ti, hc, mc, −, h, m, d) where the operator (To, hc, mc, −, tu) returns a vector in which the i-th element is computed as Euvw To(i, u, v, w)hc(u)mc(v)tu(w). The algorithm updates other components in a similar fashion until convergence. 5 Features Linear Scoring Features Our traditional linear scori</context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629–637. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="23370" citStr="Petrov et al., 2011" startWordPosition="3714" endWordPosition="3717">862 6 Experimental Setup Dataset We evaluate our model on the newly released multilingual universal dependency treebank v2.0 (McDonald et al., 2013) that consists of 10 languages: English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese (JA), Korean (KO), Brazilian-Portuguese (PT), Spanish (ES) and Swedish (SV). This multilingual treebank is annotated with a universal POS tagset and a universal dependency label set. Therefore, this dataset is an excellent benchmark for cross-lingual transfer evaluation. For POS tags, the gold universal annotation used the coarse tagset (Petrov et al., 2011) that consists of 12 tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark, and a catch-all tag X. For dependency labels, the universal annotation developed the Stanford dependencies (De Marneffe and Manning, 2008) into a rich set of 40 labels. This universal annotation enables labeled dependency parsing in crosslingual transfer. Evaluation Scenarios We first consider the unsupervised transfer scenario, in which we assume no target language annotations are available. Following the standard setup, for each target language evaluate</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audi Primadhanty</author>
<author>Xavier Carreras</author>
<author>Ariadna Quattoni</author>
</authors>
<title>Low-rank regularization for sparse conjunctive feature spaces: An application to named entity classification.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8211" citStr="Primadhanty et al., 2015" startWordPosition="1195" endWordPosition="1199"> on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior know</context>
</contexts>
<marker>Primadhanty, Carreras, Quattoni, 2015</marker>
<rawString>Audi Primadhanty, Xavier Carreras, and Ariadna Quattoni. 2015. Low-rank regularization for sparse conjunctive feature spaces: An application to named entity classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Borja Balle</author>
<author>Xavier Carreras</author>
<author>Amir Globerson</author>
</authors>
<title>Spectral regularization for max-margin sequence tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1710--1718</pages>
<contexts>
<context position="8234" citStr="Quattoni et al., 2014" startWordPosition="1200" endWordPosition="1203">cy parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experimen</context>
</contexts>
<marker>Quattoni, Balle, Carreras, Globerson, 2014</marker>
<rawString>Ariadna Quattoni, Borja Balle, Xavier Carreras, and Amir Globerson. 2014. Spectral regularization for max-margin sequence tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1710–1718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Michael Collins</author>
</authors>
<title>Density-driven cross-lingual transfer of dependency parsers.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6369" citStr="Rasooli and Collins, 2015" startWordPosition="932" endWordPosition="935">’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While t</context>
</contexts>
<marker>Rasooli, Collins, 2015</marker>
<rawString>Mohammad Sadegh Rasooli and Michael Collins. 2015. Density-driven cross-lingual transfer of dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Tim Rocktaschel</author>
<author>Sebastian Riedel</author>
</authors>
<title>Towards combined matrix and tensor factorization for universal schema relation extraction.</title>
<date>2015</date>
<booktitle>In NAACL Workshop on Vector Space Modeling for NLP.</booktitle>
<contexts>
<context position="8255" citStr="Singh et al., 2015" startWordPosition="1204" endWordPosition="1207">-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experiments show, this approac</context>
</contexts>
<marker>Singh, Rocktaschel, Riedel, 2015</marker>
<rawString>Sameer Singh, Tim Rocktaschel, and Sebastian Riedel. 2015. Towards combined matrix and tensor factorization for universal schema relation extraction. In NAACL Workshop on Vector Space Modeling for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems,</booktitle>
<pages>926--934</pages>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Proceedings of the Advances in Neural Information Processing Systems, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning distributed representations for structured output prediction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems,</booktitle>
<pages>3266--3274</pages>
<contexts>
<context position="7870" citStr="Srikumar and Manning (2014)" startWordPosition="1145" endWordPosition="1148">bine standard dependency parsing features and typological features. In contrast, we propose an automated, tensor-based approach that can effectively capture the interaction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., </context>
</contexts>
<marker>Srikumar, Manning, 2014</marker>
<rawString>Vivek Srikumar and Christopher D Manning. 2014. Learning distributed representations for structured output prediction. In Proceedings of the Advances in Neural Information Processing Systems, pages 3266–3274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Target language adaptation of discriminative transfer parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>T¨ackstr¨om, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer parsers. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Learning composition models for phrase embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<pages>242</pages>
<contexts>
<context position="8005" citStr="Yu and Dredze (2015)" startWordPosition="1167" endWordPosition="1170">tively capture the interaction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot tak</context>
</contexts>
<marker>Yu, Dredze, 2015</marker>
<rawString>Mo Yu and Mark Dredze. 2015. Learning composition models for phrase embeddings. Transactions of the Association for Computational Linguistics, 3:227– 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Frank Seide</author>
</authors>
<title>The deep tensor neural network with applications to large vocabulary speech recognition. Audio, Speech, and Language Processing,</title>
<date>2013</date>
<journal>IEEE Transactions on,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="6320" citStr="Yu et al., 2013" startWordPosition="924" endWordPosition="927">ovement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is select</context>
<context position="8474" citStr="Yu et al., 2013" startWordPosition="1246" endWordPosition="1249">ning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experiments show, this approach delivers higher transfer accuracy. 3 Hierarchical Low-rank Scoring for Transfer Parsing 3.1 Background We start by briefly reviewing the traditional threeway tensor scoring function (Lei et al., 2014). The three-way t</context>
</contexts>
<marker>Yu, Deng, Seide, 2013</marker>
<rawString>Dong Yu, Li Deng, and Frank Seide. 2013a. The deep tensor neural network with applications to large vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 21(2):388–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Tiejun Zhao</author>
<author>Yalong Bai</author>
<author>Hao Tian</author>
<author>Dianhai Yu</author>
</authors>
<title>Cross-lingual projections between languages from different families.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>312--317</pages>
<contexts>
<context position="6320" citStr="Yu et al., 2013" startWordPosition="924" endWordPosition="927">ovement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is select</context>
<context position="8474" citStr="Yu et al., 2013" startWordPosition="1246" endWordPosition="1249">ning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 Hcφhc Lφl MCφmC Dφd Hφh Mφm Hφh Mφm Dφd e4 e3 Tlφtl Hcφhc Mcφmc Lφl Tuφtu ez e, Figure 1: Visual representation for traditional multiway tensor. + φ al., 2013; Yu et al., 2013a). While these methods can automatically combine atomic features into φh φm φd a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experiments show, this approach delivers higher transfer accuracy. 3 Hierarchical Low-rank Scoring for Transfer Parsing 3.1 Background We start by briefly reviewing the traditional threeway tensor scoring function (Lei et al., 2014). The three-way t</context>
</contexts>
<marker>Yu, Zhao, Bai, Tian, Yu, 2013</marker>
<rawString>Mo Yu, Tiejun Zhao, Yalong Bai, Hao Tian, and Dianhai Yu. 2013b. Cross-lingual projections between languages from different families. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 312–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="6303" citStr="Zeman and Resnik, 2008" startWordPosition="920" endWordPosition="923">t achieves 1.1% UAS improvement over T¨ackstr¨om et al. (2013)’s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised setting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over T¨ackstr¨om et al. (2013)’s model and 4.5% UAS over the direct transfer. 2 Related Work Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc di</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In Proceedings of the International Joint Conference on Natural Language Processing, pages 35–42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
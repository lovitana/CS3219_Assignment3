<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004945">
<title confidence="0.5895725">
Large-Scale Acquisition of Entailment Pattern Pairs by Exploiting
Transitivity
</title>
<author confidence="0.935414">
Julien Kloetzer∗ Kentaro Torisawat Chikara Hashimoto§ Jong-Hoon Oh¶
</author>
<affiliation confidence="0.97293">
Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Kyoto, Japan
</affiliation>
<email confidence="0.92955">
{∗ julien, ttorisawa, § ch, ¶rovellia}@nict.go.jp
</email>
<sectionHeader confidence="0.993109" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838529411765">
We propose a novel method for acquiring
entailment pairs of binary patterns on a
large-scale. This method exploits the tran-
sitivity of entailment and a self-training
scheme to improve the performance of an
already strong supervised classifier for en-
tailment, and unlike previous methods that
exploit transitivity, it works on a large-
scale. With it we acquired 138.1 million
pattern pairs with 70% precision with such
non-trivial lexical substitution as “use Y
to distribute X”—*“X is available on Y”
whose extraction is considered difficult.
This represents 50.4 million more pattern
pairs (a 57.5% increase) than what our
supervised baseline extracted at the same
precision.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999148333333333">
Recognizing textual entailment (Geffet and Da-
gan, 2005; Androutsopoulos and Malakasiotis,
2009; Zanzotto et al., 2009; Berant et al., 2011) is
an important task for many NLP applications, such
as relation extraction (Romano et al., 2006) or
question-answering (Harabagiu and Hickl, 2006).
Text L entails text R if the information written
in the latter can be deduced from the information
written in the former. As building blocks to rec-
ognize entailment relations between texts, numer-
ous works have focused on recognizing entailment
relations between patterns, such as “grew up in
X”—*“lived in X” or “X grew up in Y”—*“X lived in
Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a;
Hashimoto et al., 2009; Berant et al., 2011; Kloet-
zer et al., 2013b).
We propose in this paper a method for ac-
quiring on a very large-scale, entailment pairs of
</bodyText>
<table confidence="0.993238">
Quantity of training data Average
precision
Baseline plus 5,000 training data samples 49.0%
Baseline: 83,800 training data samples 48.8%
Baseline minus 10,000 training data samples 48.5%
Baseline minus 20,000 training data samples 47.8%
</table>
<tableCaption confidence="0.891161">
Table 1: Average precision for baseline method
with various amounts of training data
</tableCaption>
<bodyText confidence="0.997060310344828">
such class-dependent binary patterns as “under-
went Xexam on Ydate”—*“Xexam carried out on
Ydate”. Our starting point is a supervised baseline
trained with 83,800 manually labeled pattern pairs
detailed in Kloetzer et al. (2013b). Its top 205 mil-
lion output pairs have an estimated 80% precision,
but this baseline’s performance is saturated. Ta-
ble 1 shows the baseline’s average precision when
varying its amount of hand-labeled training data.
Since the average precision only improves slightly
with additional training data, the investment in
hand-labeling additional training data is difficult
to justify.
To improve our baseline further, we exploit the
transitivity property of entailment to automatically
generate new features for it. The entailment is
transitive; if we detect that L entails C and C en-
tails R, we can infer an entailment relation be-
tween L and R even if no such relation was de-
tected beforehand. Based on this idea, we pro-
pose a self-training scheme that works in the fol-
lowing way. For pattern pair (P,Q), we use the
baselines output to find all the chains of patterns
from P to Q that are linked by entailment rela-
tions, which we call transitivity paths, and encode
the information related to them as new features to
judge the validity of pair (P,Q). Our expectation
is that even if our supervised baseline fails to judge
(P,Q) as an entailment pair, the existence of paths
</bodyText>
<page confidence="0.953604">
1649
</page>
<note confidence="0.6535365">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1649–1655,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999971264705882">
from P to Q that are comprised of pairs judged as
entailments by our baseline might strongly suggest
that P entails Q; hence, adding our new features to
the baseline should help it make better decisions
based on the information encoded in the features.
This self-training approach is the first that encodes
the information contained in transitivity paths as
features for a classifier, and as such it differs from
previous state-of-the-art methods that exploit tran-
sitivity to extract new pairs using Integer Linear
Programming (Berant et al., 2011) or that auto-
generate training data (Kloetzer et al., 2013a).
From a corpus of 600 million web pages, we
show that our proposed method extracted 217.8
million entailment pairs in Japanese with 80% pre-
cision1, which is a 6% increase over the 205.3
million pairs output by our baseline with identi-
cal precision. It also extracted 138.1 million en-
tailment pairs with 70% precision with non-trivial
lexical substitution (generally deemed difficult to
extract), which is a 50.4 million pair increase
(57.5% size improvement) over the 87.7 million
pairs output by our baseline with the same preci-
sion. These include such pairs as “use X to dis-
tribute Y”—*“Y is available on X”, “underwent X
on Y”—*“X carried out on Y”, “start X at Y”—*“Y’s
X” or “attach X to Y”—*“put X on Y”. Even though
we only present results for the Japanese language,
we believe that our method should be applicable to
other languages as well. This is because none of
the few language dependent features of our classi-
fier are strictly needed by the baseline or our pro-
posed method, and its performance boost is unre-
lated to these features.
</bodyText>
<sectionHeader confidence="0.999036" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.9995205">
The task of recognizing entailment between texts
has been proposed by Dagan et al. (2006) and in-
tensively researched (Malakasiotis and Androut-
sopoulos, 2007; Szpektor et al., 2004; Androut-
sopoulos and Malakasiotis, 2009; Dagan et al.,
2009; Hashimoto et al., 2009; Berant et al.,
2011) using a various range of techniques, includ-
ing Integer Linear Programming (Berant et al.,
2011), machine learning with SVMs (Malakasio-
tis and Androutsopoulos, 2007), and probabilis-
tic models (Wang and Manning, 2010; Shnarch
et al., 2011). Entailment recognizer or entail-
ment data sets have been used in such fields
as relation extraction (Romano et al., 2006) and
</bodyText>
<subsectionHeader confidence="0.447508">
1Examples are given in English for convenience
</subsectionHeader>
<bodyText confidence="0.999935976190477">
question-answering (Harabagiu and Hickl, 2006;
Tanaka et al., 2013). In this work, we are inter-
ested into recognizing entailment between syntac-
tic patterns, which can then be used as building
blocks in a complete entailment recognition sys-
tem (Shnarch et al., 2011). Recognizing entail-
ment between patterns has generally been stud-
ied using unsupervised techniques (Szpektor et
al., 2004; Hashimoto et al., 2009; Weeds and
Weir, 2003b), although we showed that supervised
techniques naturally obtain stronger performance
(Kloetzer et al., 2013b).
The two works that are most closely related to
our work are Berant et al. (2011) and Kloetzer et
al. (2013a), both of which exploit transitivity to
improve the result of a baseline classifier. Berant
et al. (2011) proposed an entailment recognition
method for binary patterns that exploits Integer
Linear Programming techniques (ILP) to expand
the results of an SVM classifier. This method en-
codes into an ILP problem an entailment graph,
which is a valued graph where nodes and edges re-
spectively represent patterns and their entailment
relations, and the values equal the SVM classi-
fiers score output. The problems variables EPQ E
{0, 11 indicate whether pattern pairs (here, (P,Q))
have an entailment relation, and the goal is to max-
imize the sum of the scores of the pairs selected
as entailment relations {(P, Q)|EPQ = 11. In
(Kloetzer et al., 2013a), we proposed a contradic-
tion acquisition method that uses a training data
expansion scheme; it automatically generates new
contradictions by exploiting transitivity and adds
the highest scoring contradictions based on a novel
score (CDP) to the training data of the original
classifier. The score is based on the assumption
that if pair (P,Q), when chained by transitivity to
other pairs (Q,RZ), generally leads to correct en-
tailment pairs (P,RZ), then all pairs (P,RZ) should
be correct entailment pairs. Although this work
was designed for contradiction recognition, it is
easily adapted to entailment.
</bodyText>
<sectionHeader confidence="0.777407" genericHeader="method">
3 Target Data and Baseline Classifiers
</sectionHeader>
<bodyText confidence="0.999201571428572">
Target Pattern Pairs We extracted our binary
patterns from the TSUBAKI corpus (Shinzato et
al., 2008) of 600 million Japanese web pages. Bi-
nary patterns are defined as sequences of words on
the path of dependency relations connecting two
nouns in a sentence and have two variables. “use Y
to distribute X” and “X is available on Y” are such
</bodyText>
<page confidence="0.941093">
1650
</page>
<bodyText confidence="0.999484366666667">
binary patterns. Like previous works (De Saeger
et al., 2009; Berant et al., 2011; Kloetzer et al.,
2013a), we pose restrictions on the noun-pairs
that co-occur with each pattern using word classes
to disambiguate their various potential meanings:
“Xbook by Yauthor” and “Xbuilding by Ylocation”.
We used the EM-based noun clustering algorithm
presented inKazama and Torisawa (2008) to clas-
sify one million nouns into 500 semantic classes.
Our target set, to which we apply all of our classi-
fiers, is set E of around 11 billion class-dependent
pattern pairs for which both patterns share at least
three co-occurring noun-pairs.
Baseline Classifier Our baseline classifier
(BASE) is an SVM classifier trained with about
83,800 binary pattern pairs that were hand-labeled
as entailment (25,436 pairs, 30.4% of the total)
or non-entailment (58,361 pairs). We trained
the classifier using SVMlight software2 with a
polynomial kernel of degree 2.
Following previous work (Kloetzer et al.,
2013b), we used three types of features in BASE:
surface features indicate clues like the presence
of n-grams or measure the string overlap between
two patterns; database features exploit existing
language resources; and distributional similar-
ity scores measure the patterns’ semantic similar-
ity based on the nouns that co-occur with them.
See Kloetzer et al. (2013b) for more details about
BASE s features.
</bodyText>
<sectionHeader confidence="0.985431" genericHeader="method">
4 Proposed Method
</sectionHeader>
<bodyText confidence="0.994700916666667">
Our method consists of the following three steps:
Step 1 Chain together the entailment pairs pro-
vided by our baseline classifier BASE to
form transitivity paths; if P —* Q and Q —*
R, then create path P —* Q —* R.
Step 2 Train new classifiers with features that en-
code the information contained in the transi-
tivity paths obtained in Step 1.
Step 3 Combine the output of these classifiers
with that of baseline classifier BASE.
Figure 1 shows an overview of our method, and
we describe its details in the following sections.
</bodyText>
<footnote confidence="0.958677">
2http://svmlight.joachims.org/
</footnote>
<figureCaption confidence="0.999704">
Figure 1: Overview of proposed method
</figureCaption>
<subsectionHeader confidence="0.648408">
4.1 Step 1: Transitivity Paths
</subsectionHeader>
<bodyText confidence="0.986572">
We generate chains of entailment pairs (or tran-
sitivity paths) in the following way. First, we
extract from the output of the baseline classifier
BASE set E(0) of the pattern pairs for which
BASE returns a score over given threshold 0:
E(0) = {(P, Q) E E|SBASE(P, Q) &gt; 0}, where
SBASE(P, Q) is the score returned by BASE for
pattern pair (P,Q). The higher 0 is, the greater the
precision of the pairs in E(0) should be. Then by
chaining the entailment pairs from E(0) together,
we build sets of transitivity paths composed of two
entailment pairs Tr(0,1) for 0 E 10, −oo} and of
three entailment pairs Tr(0, 2) for 0 = 0. Since
additional chaining is computationally expensive,
we stopped at paths that consist of three pairs.
</bodyText>
<subsectionHeader confidence="0.957146">
4.2 Step 2: Training New Classifiers
</subsectionHeader>
<bodyText confidence="0.9972665">
In this step, we train new classifiers by adding
new features to BASE. The training data, the
classifier software, and the settings are the same
as for BASE. For given pattern pair (P,R),
Path(P, R, 0, N) is the set of all the transitivity
paths in Tr(0, N) that lead to pair (P,R). We en-
code the information contained in these paths in
three new feature sets.
Before explaining these three new feature sets,
we define three scoring functions for the transi-
tivity paths to assess their quality; the MinScore
of a path is the minimum of scores returned by
BASE for each pair in the path, and ArScore and
GeoScore are the arithmetic and geometric aver-
ages of the scores returned by BASE for each pair
in the path. Each of the three feature sets is com-
puted for each of the three scoring functions, but
we just mention MinScore in our explanations due
to space limitations.
Feature set 1: scores of top-ranked paths Here
we select the top ten paths of Path(P, R, 0, N)
ranked by MinScore and use as features a new vec-
</bodyText>
<page confidence="0.960173">
1651
</page>
<bodyText confidence="0.99993965">
tor that consists of the following values: (1) the
MinScore of each path and (2) the scores returned
by BASE for each of the pairs in the ten paths.
When there are fewer than ten paths, the missing
features are set to 0.
Feature set 2: BASE features of the pairs in the
highest ranking transitivity paths Here we se-
lect the transitivity path of Path(P, R, 0, N) with
the highest MinScore and use the BASE feature
values for each pair in the transitivity path as a new
feature vector for pair (P,R).
Feature set 3: score distribution Given thresh-
old α, we count the number of paths whose Min-
Score exceeds α. By varying α from lower
bound low to upper bound up, we derive vec-
tor [|{p E Path(P, R, 0, N)|MinScore(p) &gt;
α}|]α∈{low,low+β,low+2∗β,...,up} and use it as anew
feature vector for pair (P,R). We set β = 0.1 and
low and up such that the score values returned by
BASE are bounded by low and up.
</bodyText>
<subsectionHeader confidence="0.997359">
4.3 Step 3: Optimization and Weighted Sum
Classifier Combination
</subsectionHeader>
<bodyText confidence="0.999498142857143">
The final output of our method combines the out-
puts of BASE and two new classifiers: (1) a classi-
fier with new features computed with 1-step tran-
sitivity paths (N = 1), and (2) another with new
features computed with up to 2-step transitivity
paths (N E 11, 2}). We then use a weighted
sum and compute score SPROPOSED(P, Q)
</bodyText>
<equation confidence="0.8102405">
E
i ni * Si(P, Q) for each pair (P,Q). Si repre-
</equation>
<bodyText confidence="0.999877947368421">
sents the scores of the respective classifiers, and
we set n0 + n1 + n2 = 100 (ni are all natural
numbers).
For each potential combination of three weights
ni, we computed the average precision returned
by our method on DEV, our development set, and
selected for our final output the weight combina-
tion that gave the best average precision on DEV.
The final classifiers weights obtained in our ex-
periments were 62 for BASE, 30 for the classifier
with 1-step transitivity features, and 8 for the one
with the 1- and 2-step transitivity features.
Using the same method, we also performed ab-
lation tests to remove the features that harmed the
classifiers and ensured that every proposed new
feature set and every scoring function were useful.
Finally, we confirmed that using a weighted
sum for our proposed method returned higher av-
erage precision than Stacking (Wolpert, 1992),
</bodyText>
<figureCaption confidence="0.9065565">
Figure 2: Precision curves for PROPOSED and
baseline methods for non-similar pairs
</figureCaption>
<bodyText confidence="0.843612">
which is a more standard combination method, or
than using the output of any of the new classifiers
alone.
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99828996969697">
In this section, we evaluate our proposed method
in a large-scale setting and compare it to BASE
and to state-of-the-art methods based on ILP (Be-
rant et al., 2011) and automatic training data ex-
pansion (Kloetzer et al., 2013a). We also indicate
that our method shows the best performance gain
for pattern pairs with non-trivial lexical substitu-
tions, which are more difficult to acquire.
Evaluation Method For our evaluation, we pre-
pared test set TEST of 15,000 pattern pairs ran-
domly sampled from E (our target set of 11 bil-
lion pairs). The pairs were annotated by three hu-
mans (not the authors) who voted to settle labeling
discrepancies. We also prepared development set
DEV of 5, 000 pattern pairs from E for tuning our
method. The Kappa score was 0.55 for the anno-
tation of these two sets.
We measured the performance of each method
by computing its average precision (Manning and
Sch¨utze, 1999) on the TEST set. We used the av-
erage precision instead of the traditional F-value
because the latters value greatly varies depend-
ing on where the classification boundary is drawn,
even for similar rankings. We also drew precision
curves for each method using the same TEST set.
Proposed Methods Performance We first show
the performance of PROPOSED (our proposed
method) and BASE (the baseline classifier). As
another baseline, we consider BASE +DEV where
the 5,000 samples of the DEV set were added to
the BASE training data. We show the average pre-
cision for each of these three classifiers and the
=
</bodyText>
<page confidence="0.976521">
1652
</page>
<table confidence="0.999753142857143">
Classifier Average Million pairs
precision at 80% prec.
PROPOSED 50.64% 217.8
BASE + DEV 48.96% 202.4
BASE 48.79% 205.3
ILP N/A 205.2
CDP 48.42% 198.0
</table>
<tableCaption confidence="0.589941666666667">
Table 2: Average precision and entailment pairs
obtained (in millions) for proposed method, base-
line classifiers, and state-of-the-art methods
</tableCaption>
<table confidence="0.9995354">
Classifier Av. precision Av. precision
(similar) (non-similar)
PROPOSED 78.73% 39.53%
BASE + DEV 77.72% 37.24%
BASE 77.85% 36.98%
</table>
<tableCaption confidence="0.9796985">
Table 3: Average precision for similar and non-
similar pairs
</tableCaption>
<bodyText confidence="0.998758557142857">
number of pairs obtained at 80% precision in Ta-
ble 2. We also show the performance of these
classifiers over similar pattern pairs (both patterns
share a content word) and non-similar pairs (they
do not share a content word) in Table 3.
As mentioned in the introduction, BASE +DEV
shows that the addition of 5,000 hand-labeled sam-
ples to the training data of BASE (a 6% increase)
only improves the average precision performance
by 0.17%. Our proposed method, on the other
hand, exploits the same 5,000 new annotated sam-
ples for tuning its parameters and obtains a 1.85%
gain of average precision. Using PROPOSED, we
acquired 217.8 million pattern pairs with 80% pre-
cision, an improvement of 6.0% over BASE.
As shown in Table 3, BASE s performance is
much lower for non-similar pairs like “use Y to
distribute X”→“X is available on Y”, which have
non-trivial lexical substitutions and are more dif-
ficult to acquire than similar pairs. This is also
where PROPOSED obtains the biggest gain in
performance: an average precision of 39.53 com-
pared to 36.98 for BASE. We show the preci-
sion curves we obtained when ranking the non-
similar pairs with BASE and PROPOSED in
Fig. 2. PROPOSED acquired 138.1 million non-
similar pairs at 70% precision, which is an in-
crease of 50.4 million pairs (a 57.5% size im-
provement) compared to BASE with the same pre-
cision. We believe that the strong performance of
BASE for similar entailment pairs helped it dis-
cover, through transitivity, the variations of non-
similar entailment pairs it could already detect.
Comparison to State-of-the-art Methods We
also compared PROPOSED with two state-of-
the-art methods that exploit transitivity: the ILP-
based method of Berant et al. (2011) (ILP) and
the training data expansion method we proposed
in Kloetzer et al. (2013a) (CDP). The latter, which
was initially designed for acquiring contradiction
pairs, was adapted to acquire entailment for com-
parison purposes. The results of this compari-
son are summarized in Table 2, and the precision
curves for these two methods as well over non-
similar pairs are shown in Fig. 2. Our proposed
method is the only one that provides stable im-
provement in our large-scale setting; at best, the
other two just slightly outperform BASE. We be-
lieve that our feature encoding provides more in-
formation to the classifier than the raw scores in
the transitivity paths that are exploited by the other
state-of-the-art methods, and as such strengthens
the performance.
As for explaining the poor performance of the
state-of-the-art methods, ILP is unfortunately not
tractable for big problems; our ILP solver failed to
solve 82% of the independent problems we fed it
due to insufficient memory even on 64-Gb mem-
ory machines, making ILP just slightly better than
BASE. Our pattern graph is also sparser than that
in Berant et al. (2011), and as such ILP might
not be completely efficient. But we assume that
even if we had used the graphs whole closure, the
ILP problem instances would have become even
less tractable, resulting in performance that only
slightly exceeds BASE. As for CDP, since our
baseline classifier already has more than 80,000
hand-annotated samples as training data, the addi-
tion of automatically generated training samples is
actually harmful.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974583333333">
In this work, we proposed a method that exploits
the transitivity relation of entailment in a self-
training scheme and combines classifiers with a
weighted sum. In our large-scale setting, our
method outperforms state-of-the-art methods that
are also based on a transitivity approach, includ-
ing an ILP-based method. Using our proposed
method, we acquired 217.8 million Japanese en-
tailment pairs with 80% precision and 138.1 mil-
lion non-trivial pairs with 70% precision. We are
considering an extrinsic evaluation for these data
such as the RTE test in future research.
</bodyText>
<page confidence="0.982657">
1653
</page>
<sectionHeader confidence="0.982724" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999304596330275">
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entail-
ment methods. arXiv preprint arXiv:0912.3747.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL 2011, pages 610–619.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising tectual entailment, pages 177–
190. Springer.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):1–17.
Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM 2009, pages 764–
769.
Maayan Geffet and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 107–
114. Association for Computational Linguistics.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, pages 905–912. Association
for Computational Linguistics.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun’ichi
Kazama. 2009. Large-scale verb entailment ac-
quisition from the web. In Proceedings of EMNLP
2009, volume 3, pages 1172–1181.
Junichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. Proceed-
ings of ACL 2008, pages 407–415.
Julien Kloetzer, Stijn De Saeger, Kentaro Tori-
sawa, Chikara Hashimoto, Jong-Hoon Oh, Kiyonori
Ohtake, and Motoki Sano. 2013a. Two-stage
method for large-scale acquisition of contradiction
pattern pairs using entailment. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 693–703.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,
Motoki Sano, Chikara Hashimoto, and Jun Gotoh.
2013b. Large-scale acquisition of entailment pattern
pairs. In Information Processing Society of Japan
(IPSJ) Kansai-Branch Convention.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323–328.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using SVMs and
string similarity measures. In Proceedings of the
ACL- PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42–47.
Christopher D Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT press.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
and Ido Dagan. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proceedings of EACL, pages 409–416.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: an open search engine infrastructure for
developing new information access methodology.
Proceedings of IJCNLP2008.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical en-
tailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers
- Volume 2, HLT ’11, pages 558–563, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura
Coppola, et al. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP,
volume 4, pages 41–48.
Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,
Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,
and Kentaro Torisawa. 2013. Wisdom2013: A
large-scale web information analysis system. In
Sixth International Joint Conference on Natural
Language Processing, pages 58–61.
Mengqiu Wang and Christopher D Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
’10, pages 1164–1172, Beijing, China. Association
for Computational Linguistics. ACM ID: 1873912.
Julie Weeds and David Weir. 2003a. A general frame-
work for distributional similarity. In Proceedings of
EMNLP 2003, pages 81–88. Association for Com-
putational Linguistics.
Julie Weeds and David Weir. 2003b. A general frame-
work for distributional similarity. In Proceedings of
the 2003 conference on Empirical methods in natu-
ral language processing, pages 81–88. Association
for Computational Linguistics.
</reference>
<page confidence="0.865764">
1654
</page>
<reference confidence="0.999650666666667">
David H Wolpert. 1992. Stacked generalization. Neu-
ral networks, 5(2):241–259.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. Natural
Language Engineering, 15(04):551–582.
</reference>
<page confidence="0.992427">
1655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771845">
<title confidence="0.979962">Large-Scale Acquisition of Entailment Pattern Pairs by Exploiting Transitivity</title>
<author confidence="0.896844">Kentaro</author>
<affiliation confidence="0.9845765">Information Analysis Laboratory, National Institute of Information and Communications Technology (NICT), Kyoto, Japan</affiliation>
<abstract confidence="0.994054833333333">We propose a novel method for acquiring entailment pairs of binary patterns on a large-scale. This method exploits the transitivity of entailment and a self-training scheme to improve the performance of an already strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such lexical substitution as Y distribute is available on whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods. arXiv preprint arXiv:0912.3747.</title>
<date>2009</date>
<contexts>
<context position="1109" citStr="Androutsopoulos and Malakasiotis, 2009" startWordPosition="151" endWordPosition="154">nd a self-training scheme to improve the performance of an already strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et a</context>
<context position="5666" citStr="Androutsopoulos and Malakasiotis, 2009" startWordPosition="896" endWordPosition="900">ut on Y”, “start X at Y”—*“Y’s X” or “attach X to Y”—*“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing enta</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2009</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2009. A survey of paraphrasing and textual entailment methods. arXiv preprint arXiv:0912.3747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>610--619</pages>
<contexts>
<context position="1154" citStr="Berant et al., 2011" startWordPosition="159" endWordPosition="162">ady strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al</context>
<context position="4324" citStr="Berant et al., 2011" startWordPosition="670" endWordPosition="673">17-21 September 2015. c�2015 Association for Computational Linguistics. from P to Q that are comprised of pairs judged as entailments by our baseline might strongly suggest that P entails Q; hence, adding our new features to the baseline should help it make better decisions based on the information encoded in the features. This self-training approach is the first that encodes the information contained in transitivity paths as features for a classifier, and as such it differs from previous state-of-the-art methods that exploit transitivity to extract new pairs using Integer Linear Programming (Berant et al., 2011) or that autogenerate training data (Kloetzer et al., 2013a). From a corpus of 600 million web pages, we show that our proposed method extracted 217.8 million entailment pairs in Japanese with 80% precision1, which is a 6% increase over the 205.3 million pairs output by our baseline with identical precision. It also extracted 138.1 million entailment pairs with 70% precision with non-trivial lexical substitution (generally deemed difficult to extract), which is a 50.4 million pair increase (57.5% size improvement) over the 87.7 million pairs output by our baseline with the same precision. Thes</context>
<context position="5732" citStr="Berant et al., 2011" startWordPosition="909" endWordPosition="912">nly present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as build</context>
<context position="8617" citStr="Berant et al., 2011" startWordPosition="1371" endWordPosition="1374"> all pairs (P,RZ) should be correct entailment pairs. Although this work was designed for contradiction recognition, it is easily adapted to entailment. 3 Target Data and Baseline Classifiers Target Pattern Pairs We extracted our binary patterns from the TSUBAKI corpus (Shinzato et al., 2008) of 600 million Japanese web pages. Binary patterns are defined as sequences of words on the path of dependency relations connecting two nouns in a sentence and have two variables. “use Y to distribute X” and “X is available on Y” are such 1650 binary patterns. Like previous works (De Saeger et al., 2009; Berant et al., 2011; Kloetzer et al., 2013a), we pose restrictions on the noun-pairs that co-occur with each pattern using word classes to disambiguate their various potential meanings: “Xbook by Yauthor” and “Xbuilding by Ylocation”. We used the EM-based noun clustering algorithm presented inKazama and Torisawa (2008) to classify one million nouns into 500 semantic classes. Our target set, to which we apply all of our classifiers, is set E of around 11 billion class-dependent pattern pairs for which both patterns share at least three co-occurring noun-pairs. Baseline Classifier Our baseline classifier (BASE) is</context>
<context position="14932" citStr="Berant et al., 2011" startWordPosition="2465" endWordPosition="2469">that harmed the classifiers and ensured that every proposed new feature set and every scoring function were useful. Finally, we confirmed that using a weighted sum for our proposed method returned higher average precision than Stacking (Wolpert, 1992), Figure 2: Precision curves for PROPOSED and baseline methods for non-similar pairs which is a more standard combination method, or than using the output of any of the new classifiers alone. 5 Experiments In this section, we evaluate our proposed method in a large-scale setting and compare it to BASE and to state-of-the-art methods based on ILP (Berant et al., 2011) and automatic training data expansion (Kloetzer et al., 2013a). We also indicate that our method shows the best performance gain for pattern pairs with non-trivial lexical substitutions, which are more difficult to acquire. Evaluation Method For our evaluation, we prepared test set TEST of 15,000 pattern pairs randomly sampled from E (our target set of 11 billion pairs). The pairs were annotated by three humans (not the authors) who voted to settle labeling discrepancies. We also prepared development set DEV of 5, 000 pattern pairs from E for tuning our method. The Kappa score was 0.55 for th</context>
<context position="18469" citStr="Berant et al. (2011)" startWordPosition="3055" endWordPosition="3058">ves we obtained when ranking the nonsimilar pairs with BASE and PROPOSED in Fig. 2. PROPOSED acquired 138.1 million nonsimilar pairs at 70% precision, which is an increase of 50.4 million pairs (a 57.5% size improvement) compared to BASE with the same precision. We believe that the strong performance of BASE for similar entailment pairs helped it discover, through transitivity, the variations of nonsimilar entailment pairs it could already detect. Comparison to State-of-the-art Methods We also compared PROPOSED with two state-ofthe-art methods that exploit transitivity: the ILPbased method of Berant et al. (2011) (ILP) and the training data expansion method we proposed in Kloetzer et al. (2013a) (CDP). The latter, which was initially designed for acquiring contradiction pairs, was adapted to acquire entailment for comparison purposes. The results of this comparison are summarized in Table 2, and the precision curves for these two methods as well over nonsimilar pairs are shown in Fig. 2. Our proposed method is the only one that provides stable improvement in our large-scale setting; at best, the other two just slightly outperform BASE. We believe that our feature encoding provides more information to </context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of ACL 2011, pages 610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5536" citStr="Dagan et al. (2006)" startWordPosition="879" endWordPosition="882">n. These include such pairs as “use X to distribute Y”—*“Y is available on X”, “underwent X on Y”—*“X carried out on Y”, “start X at Y”—*“Y’s X” or “attach X to Y”—*“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for conv</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177– 190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="5686" citStr="Dagan et al., 2009" startWordPosition="901" endWordPosition="904">tach X to Y”—*“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between synta</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15(4):1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
</authors>
<title>Large scale relation acquisition using class dependent patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of ICDM 2009,</booktitle>
<pages>764--769</pages>
<marker>De Saeger, Torisawa, Kazama, Kuroda, Murata, 2009</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, and Masaki Murata. 2009. Large scale relation acquisition using class dependent patterns. In Proceedings of ICDM 2009, pages 764– 769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1069" citStr="Geffet and Dagan, 2005" startWordPosition="146" endWordPosition="150">sitivity of entailment and a self-training scheme to improve the performance of an already strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 200</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 107– 114. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>905--912</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1302" citStr="Harabagiu and Hickl, 2006" startWordPosition="181" endWordPosition="184">cquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al., 2013b). We propose in this paper a method for acquiring on a very large-scale, entailment pairs of Quantity of training data Average precision Ba</context>
<context position="6189" citStr="Harabagiu and Hickl, 2006" startWordPosition="978" endWordPosition="981">(Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu and Andrew Hickl. 2006. Methods for using textual entailment in open-domain question answering. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 905–912. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Stijn De Saeger</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Large-scale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009,</booktitle>
<volume>3</volume>
<pages>1172--1181</pages>
<marker>Hashimoto, Torisawa, Kuroda, De Saeger, Murata, Kazama, 2009</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquisition from the web. In Proceedings of EMNLP 2009, volume 3, pages 1172–1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by largescale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>407--415</pages>
<contexts>
<context position="8918" citStr="Kazama and Torisawa (2008)" startWordPosition="1414" endWordPosition="1417">of 600 million Japanese web pages. Binary patterns are defined as sequences of words on the path of dependency relations connecting two nouns in a sentence and have two variables. “use Y to distribute X” and “X is available on Y” are such 1650 binary patterns. Like previous works (De Saeger et al., 2009; Berant et al., 2011; Kloetzer et al., 2013a), we pose restrictions on the noun-pairs that co-occur with each pattern using word classes to disambiguate their various potential meanings: “Xbook by Yauthor” and “Xbuilding by Ylocation”. We used the EM-based noun clustering algorithm presented inKazama and Torisawa (2008) to classify one million nouns into 500 semantic classes. Our target set, to which we apply all of our classifiers, is set E of around 11 billion class-dependent pattern pairs for which both patterns share at least three co-occurring noun-pairs. Baseline Classifier Our baseline classifier (BASE) is an SVM classifier trained with about 83,800 binary pattern pairs that were hand-labeled as entailment (25,436 pairs, 30.4% of the total) or non-entailment (58,361 pairs). We trained the classifier using SVMlight software2 with a polynomial kernel of degree 2. Following previous work (Kloetzer et al.</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Junichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by largescale clustering of dependency relations. Proceedings of ACL 2008, pages 407–415.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Julien Kloetzer</author>
<author>Stijn De Saeger</author>
</authors>
<title>Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, Kiyonori Ohtake, and Motoki Sano. 2013a. Two-stage method for large-scale acquisition of contradiction pattern pairs using entailment.</title>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>693--703</pages>
<marker>Kloetzer, De Saeger, </marker>
<rawString>Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, Kiyonori Ohtake, and Motoki Sano. 2013a. Two-stage method for large-scale acquisition of contradiction pattern pairs using entailment. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 693–703.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Julien Kloetzer</author>
</authors>
<title>Stijn De Saeger, Kentaro Torisawa, Motoki Sano, Chikara Hashimoto, and Jun Gotoh. 2013b. Large-scale acquisition of entailment pattern pairs.</title>
<booktitle>In Information Processing Society of Japan (IPSJ) Kansai-Branch Convention.</booktitle>
<marker>Kloetzer, </marker>
<rawString>Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Motoki Sano, Chikara Hashimoto, and Jun Gotoh. 2013b. Large-scale acquisition of entailment pattern pairs. In Information Processing Society of Japan (IPSJ) Kansai-Branch Convention.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="1670" citStr="Lin and Pantel, 2001" startWordPosition="245" endWordPosition="248">fet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al., 2013b). We propose in this paper a method for acquiring on a very large-scale, entailment pairs of Quantity of training data Average precision Baseline plus 5,000 training data samples 49.0% Baseline: 83,800 training data samples 48.8% Baseline minus 10,000 training data samples 48.5% Baseline minus 20,000 training data samples 47.8% Table 1: Average precision for baseline method with various amounts of training data such class-dependent binary patterns as “underwent Xexam on Ydate”—*“Xexam carried out on Yd</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning textual entailment using SVMs and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL- PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="5603" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="887" endWordPosition="891">ute Y”—*“Y is available on X”, “underwent X on Y”—*“X carried out on Y”, “start X at Y”—*“Y’s X” or “attach X to Y”—*“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>Prodromos Malakasiotis and Ion Androutsopoulos. 2007. Learning textual entailment using SVMs and string similarity measures. In Proceedings of the ACL- PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Romano</author>
<author>Milen Kouylekov</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Investigating a generic paraphrase-based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>409--416</pages>
<contexts>
<context position="1252" citStr="Romano et al., 2006" startWordPosition="175" endWordPosition="178">vity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al., 2013b). We propose in this paper a method for acquiring on a very large-scale, entailment pairs</context>
<context position="6092" citStr="Romano et al., 2006" startWordPosition="965" endWordPosition="968">ntailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). Th</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, 2006</marker>
<rawString>Lorenza Romano, Milen Kouylekov, Idan Szpektor, and Ido Dagan. 2006. Investigating a generic paraphrase-based approach for relation extraction. In Proceedings of EACL, pages 409–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>TSUBAKI: an open search engine infrastructure for developing new information access methodology.</title>
<date>2008</date>
<booktitle>Proceedings of IJCNLP2008.</booktitle>
<contexts>
<context position="8291" citStr="Shinzato et al., 2008" startWordPosition="1312" endWordPosition="1315">contradictions by exploiting transitivity and adds the highest scoring contradictions based on a novel score (CDP) to the training data of the original classifier. The score is based on the assumption that if pair (P,Q), when chained by transitivity to other pairs (Q,RZ), generally leads to correct entailment pairs (P,RZ), then all pairs (P,RZ) should be correct entailment pairs. Although this work was designed for contradiction recognition, it is easily adapted to entailment. 3 Target Data and Baseline Classifiers Target Pattern Pairs We extracted our binary patterns from the TSUBAKI corpus (Shinzato et al., 2008) of 600 million Japanese web pages. Binary patterns are defined as sequences of words on the path of dependency relations connecting two nouns in a sentence and have two variables. “use Y to distribute X” and “X is available on Y” are such 1650 binary patterns. Like previous works (De Saeger et al., 2009; Berant et al., 2011; Kloetzer et al., 2013a), we pose restrictions on the noun-pairs that co-occur with each pattern using word classes to disambiguate their various potential meanings: “Xbook by Yauthor” and “Xbuilding by Ylocation”. We used the EM-based noun clustering algorithm presented i</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008. TSUBAKI: an open search engine infrastructure for developing new information access methodology. Proceedings of IJCNLP2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Jacob Goldberger</author>
<author>Ido Dagan</author>
</authors>
<title>A probabilistic modeling framework for lexical entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>558--563</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5970" citStr="Shnarch et al., 2011" startWordPosition="945" endWordPosition="948">or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir</context>
</contexts>
<marker>Shnarch, Goldberger, Dagan, 2011</marker>
<rawString>Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011. A probabilistic modeling framework for lexical entailment. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 558–563, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>41--48</pages>
<contexts>
<context position="5626" citStr="Szpektor et al., 2004" startWordPosition="892" endWordPosition="895">t X on Y”—*“X carried out on Y”, “start X at Y”—*“Y’s X” or “attach X to Y”—*“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work,</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP, volume 4, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiro Tanaka</author>
</authors>
<title>Stijn De Saeger, Kiyonori Ohtake, Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii, and Kentaro Torisawa.</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>58--61</pages>
<marker>Tanaka, 2013</marker>
<rawString>Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake, Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii, and Kentaro Torisawa. 2013. Wisdom2013: A large-scale web information analysis system. In Sixth International Joint Conference on Natural Language Processing, pages 58–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<journal>ACM ID:</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1164--1172</pages>
<location>Beijing,</location>
<contexts>
<context position="5947" citStr="Wang and Manning, 2010" startWordPosition="941" endWordPosition="944"> needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1164–1172, Beijing, China. Association for Computational Linguistics. ACM ID: 1873912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP 2003,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1692" citStr="Weeds and Weir, 2003" startWordPosition="249" endWordPosition="252">ndroutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al., 2013b). We propose in this paper a method for acquiring on a very large-scale, entailment pairs of Quantity of training data Average precision Baseline plus 5,000 training data samples 49.0% Baseline: 83,800 training data samples 48.8% Baseline minus 10,000 training data samples 48.5% Baseline minus 20,000 training data samples 47.8% Table 1: Average precision for baseline method with various amounts of training data such class-dependent binary patterns as “underwent Xexam on Ydate”—*“Xexam carried out on Ydate”. Our starting poi</context>
<context position="6576" citStr="Weeds and Weir, 2003" startWordPosition="1040" endWordPosition="1043"> et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al. (2013a), both of which exploit transitivity to improve the result of a baseline classifier. Berant et al. (2011) proposed an entailment recognition method for binary patterns that exploits Integer Linear Programming techniques (ILP) to expand the results of an SVM classifier. This method encodes into an ILP problem an entailment graph, which is a valued graph where nodes and edges r</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003a. A general framework for distributional similarity. In Proceedings of EMNLP 2003, pages 81–88. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1692" citStr="Weeds and Weir, 2003" startWordPosition="249" endWordPosition="252">ndroutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloetzer et al., 2013b). We propose in this paper a method for acquiring on a very large-scale, entailment pairs of Quantity of training data Average precision Baseline plus 5,000 training data samples 49.0% Baseline: 83,800 training data samples 48.8% Baseline minus 10,000 training data samples 48.5% Baseline minus 20,000 training data samples 47.8% Table 1: Average precision for baseline method with various amounts of training data such class-dependent binary patterns as “underwent Xexam on Ydate”—*“Xexam carried out on Ydate”. Our starting poi</context>
<context position="6576" citStr="Weeds and Weir, 2003" startWordPosition="1040" endWordPosition="1043"> et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al. (2013a), both of which exploit transitivity to improve the result of a baseline classifier. Berant et al. (2011) proposed an entailment recognition method for binary patterns that exploits Integer Linear Programming techniques (ILP) to expand the results of an SVM classifier. This method encodes into an ILP problem an entailment graph, which is a valued graph where nodes and edges r</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003b. A general framework for distributional similarity. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 81–88. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<date>1992</date>
<booktitle>Stacked generalization. Neural networks,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="14563" citStr="Wolpert, 1992" startWordPosition="2406" endWordPosition="2407">r our final output the weight combination that gave the best average precision on DEV. The final classifiers weights obtained in our experiments were 62 for BASE, 30 for the classifier with 1-step transitivity features, and 8 for the one with the 1- and 2-step transitivity features. Using the same method, we also performed ablation tests to remove the features that harmed the classifiers and ensured that every proposed new feature set and every scoring function were useful. Finally, we confirmed that using a weighted sum for our proposed method returned higher average precision than Stacking (Wolpert, 1992), Figure 2: Precision curves for PROPOSED and baseline methods for non-similar pairs which is a more standard combination method, or than using the output of any of the new classifiers alone. 5 Experiments In this section, we evaluate our proposed method in a large-scale setting and compare it to BASE and to state-of-the-art methods based on ILP (Berant et al., 2011) and automatic training data expansion (Kloetzer et al., 2013a). We also indicate that our method shows the best performance gain for pattern pairs with non-trivial lexical substitutions, which are more difficult to acquire. Evalua</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H Wolpert. 1992. Stacked generalization. Neural networks, 5(2):241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>04</issue>
<contexts>
<context position="1132" citStr="Zanzotto et al., 2009" startWordPosition="155" endWordPosition="158"> performance of an already strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”—*“X is available on Y” whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 1 Introduction Recognizing textual entailment (Geffet and Dagan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to recognize entailment relations between texts, numerous works have focused on recognizing entailment relations between patterns, such as “grew up in X”—*“lived in X” or “X grew up in Y”—*“X lived in Y” (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al.</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Natural Language Engineering, 15(04):551–582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
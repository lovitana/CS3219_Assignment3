<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005761">
<title confidence="0.9981875">
Finding Function in Form: Compositional Character Models for
Open Vocabulary Word Representation
</title>
<author confidence="0.9892705">
Wang Ling Tiago Luis Luis Marujo Ram´on Fernandez Astudillo
Silvio Amir Chris Dyer Alan W Black Isabel Trancoso
</author>
<affiliation confidence="0.979174333333333">
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior T´ecnico, Lisbon, Portugal
</affiliation>
<email confidence="0.797663">
{lingwang,lmarujo,cdyer,awb}@cs.cmu.edu
{ramon.astudillo,samir,tmcl,isabel.trancoso}@inesc-id.pt
</email>
<sectionHeader confidence="0.990368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895833333333">
We introduce a model for construct-
ing vector representations of words by
composing characters using bidirectional
LSTMs. Relative to traditional word rep-
resentation models that have independent
vectors for each word type, our model
requires only a single vector per char-
acter type and a fixed set of parame-
ters for the compositional model. De-
spite the compactness of this model and,
more importantly, the arbitrary nature
of the form–function relationship in lan-
guage, our “composed” word representa-
tions yield state-of-the-art results in lan-
guage modeling and part-of-speech tag-
ging. Benefits over traditional baselines
are particularly pronounced in morpholog-
ically rich languages (e.g., Turkish).
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999800240740741">
Good representations of words are important for
good generalization in natural language process-
ing applications. Of central importance are vec-
tor space models that capture functional (i.e., se-
mantic and syntactic) similarity in terms of ge-
ometric locality. However, when word vectors
are learned—a practice that is becoming increas-
ingly common—most models assume that each
word type has its own vector representation that
can vary independently of other model compo-
nents. This paper argues that this independence
assumption is inherently problematic, in particular
in morphologically rich languages (e.g., Turkish).
In such languages, a more reasonable assumption
would be that orthographic (formal) similarity is
evidence for functional similarity.
However, it is manifestly clear that similarity in
form is neither a necessary nor sufficient condi-
tion for similarity in function: small orthographic
differences may correspond to large semantic or
syntactic differences (butter vs. batter), and large
orthographic differences may obscure nearly per-
fect functional correspondence (rich vs. affluent).
Thus, any orthographically aware model must be
able to capture non-compositional effects in addi-
tion to more regular effects due to, e.g., morpho-
logical processes. To model the complex form-
function relationship, we turn to long short-term
memories (LSTMs), which are designed to be able
to capture complex non-linear and non-local dy-
namics in sequences (Hochreiter and Schmidhu-
ber, 1997). We use bidirectional LSTMs to “read”
the character sequences that constitute each word
and combine them into a vector representation of
the word. This model assumes that each charac-
ter type is associated with a vector, and the LSTM
parameters encode both idiosyncratic lexical and
regular morphological knowledge.
To evaluate our model, we use a vector-
based model for part-of-speech (POS) tagging
and for language modeling, and we report ex-
periments on these tasks in several languages
comparing to baselines that use more tradi-
tional, orthographically-unaware parameteriza-
tions. These experiments show: (i) our character-
based model is able to generate similar representa-
tions for words that are semantically and syntacti-
cally similar, even for words are orthographically
distant (e.g., October and January); our model
achieves improvements over word lookup tables
using only a fraction of the number of parameters
in two tasks; (iii) our model obtains state-of-the-
art performance on POS tagging (including estab-
lishing a new best performance in English); and
</bodyText>
<page confidence="0.918519">
1520
</page>
<note confidence="0.984731">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1520–1530,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997567875">
(iv) performance improvements are especially dra-
matic in morphologically rich languages.
The paper is organized as follows: Section 2
presents our character-based model to generate
word embeddings. Experiments on Language
Modeling and POS tagging are described in Sec-
tions 4 and 5. We present related work in Sec-
tion 6; and we conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.97283" genericHeader="introduction">
2 Word Vectors and Wordless Word
Vectors
</sectionHeader>
<bodyText confidence="0.9998844">
It is commonplace to represent words as vectors.
In contrast to naive models in which all word types
in a vocabulary V are equally different from each
other, vector space models capture the intuition
that words may be different or similar along a va-
riety of dimensions. Learning vector representa-
tions of words by treating them as optimizable pa-
rameters in various kinds of language models has
been found to be a remarkably effective means
for generating vector representations that perform
well in other tasks (Collobert et al., 2011; Kalch-
brenner and Blunsom, 2013; Liu et al., 2014; Chen
and Manning, 2014). Formally, such models de-
fine a matrix P E Rd×|V |, which contains d pa-
rameters for each word in the vocabulary V . For a
given word type w E V , a column is selected by
right-multiplying P by a one-hot vector of length
|V |, which we write 1w, that is zero in every di-
mension except for the element corresponding to
w. Thus, P is often referred to as word lookup
table and we shall denote by eWw E Rd the embed-
ding obtained from a word lookup table for w as
eWw = P · 1w. This allows tasks with low amounts
of annotated data to be trained jointly with other
tasks with large amounts of data and leverage the
similarities in these tasks. A common practice to
this end is to initialize the word lookup table with
the parameters trained on an unsupervised task.
Some examples of these include the skip-n-gram
and CBOW models of Mikolov et al. (2013).
</bodyText>
<subsectionHeader confidence="0.991082">
2.1 Problem: Independent Parameters
</subsectionHeader>
<bodyText confidence="0.999856023809524">
There are two practical problems with word
lookup tables. Firstly, while they can be pre-
trained with large amounts of data to learn se-
mantic and syntactic similarities between words,
each vector is independent. That is, even though
models based on word lookup tables are often ob-
served to learn that cats, kings and queens exist in
roughly the same linear correspondences to each
other as cat, king and queen do, the model does
not represent the fact that adding an s at the end
of the word is evidence for this transformation.
This means that word lookup tables cannot gen-
erate representations for previously unseen words,
such as Frenchification, even if the components,
French and -ification, are observed in other con-
texts.
Second, even if copious data is available, it is
impractical to actually store vectors for all word
types. As each word type gets a set of parameters
d, the total number of parameters is dx|V |, where
|V  |is the size of the vocabulary. Even in rela-
tively morphological poor English, the number of
word types tends to scale to the order of hundreds
of thousands, and in noisier domains, such as on-
line data, the number of word types raises con-
siderably. For instance, in the English wikipedia
dump with 60 million sentences, there are approx-
imately 20 million different lowercased and tok-
enized word types, each of which would need its
own vector. Intuitively, it is not sensible to use the
same number of parameters for each word type.
Finally, it is important to remark that it is
uncontroversial among cognitive scientists that
our lexicon is structured into related forms—i.e.,
their parameters are not independent. The well-
known “past tense debate” between connection-
ists and proponents of symbolic accounts con-
cerns disagreements about how humans represent
knowledge of inflectional processes (e.g., the for-
mation of the English past tense), not whether
such knowledge exists (Marslen-Wilson and Tyler,
1998).
</bodyText>
<subsectionHeader confidence="0.999544">
2.2 Solution: Compositional Models
</subsectionHeader>
<bodyText confidence="0.999958066666667">
Our solution to these problems is to construct
a vector representation of a word by composing
smaller pieces into a representation of the larger
form. This idea has been explored in prior work
by composing morphemes into representations of
words (Luong et al., 2013; Botha and Blunsom,
2014; Soricut and Och, 2015). Morphemes are an
ideal primitive for such a model since they are—
by definition—the minimal meaning-bearing (or
syntax-bearing) units of language. The drawback
to such approaches is they depend on a morpho-
logical analyzer.
In contrast, we would like to compose repre-
sentations of characters into representations of
words. However, the relationship between words
</bodyText>
<page confidence="0.986252">
1521
</page>
<bodyText confidence="0.997132142857143">
forms and their meanings is non-trivial (de Saus-
sure, 1916). While some compositional relation-
ships exist, e.g., morphological processes such as
adding -ing or -ly to a stem have relatively reg-
ular effects, many words with lexical similarities
convey different meanings, such as, the word pairs
lesson � lessen and coarse course.
</bodyText>
<sectionHeader confidence="0.999655" genericHeader="method">
3 C2W Model
</sectionHeader>
<bodyText confidence="0.999868379310345">
Our compositional character to word (C2W)
model is based on bidirectional LSTMs (Graves
and Schmidhuber, 2005), which are able to
learn complex non-local dependencies in sequence
models. An illustration is shown in Figure 1. The
input of the C2W model (illustrated on bottom) is
a single word type w, and we wish to obtain is
a d-dimensional vector used to represent w. This
model shares the same input and output of a word
lookup table (illustrated on top), allowing it to eas-
ily replace then in any network.
As input, we define an alphabet of characters
C. For English, this vocabulary would contain an
entry for each uppercase and lowercase letter as
well as numbers and punctuation. The input word
w is decomposed into a sequence of characters
c1, ... , cm, where m is the length of w. Each ci
is defined as a one hot vector 1ci, with one on the
index of ci in vocabulary M. We define a projec-
tion layer PC E RdC×|C|, where dC is the number
of parameters for each character in the character
set C. This of course just a character lookup table,
and is used to capture similarities between charac-
ters in a language (e.g., vowels vs. consonants).
Thus, we write the projection of each input char-
acter ci as eci = PC · 1ci.
Given the input vectors x1, ... , xm, a LSTM
computes the state sequence h1, ... , hm+1 by it-
eratively applying the following updates:
</bodyText>
<equation confidence="0.996850666666667">
it = Q(Wixxt + Wihht−1 + Wicct−1 + bi)
ft = Q(Wfxxt + Wfhht−1 + Wfcct−1 + bf)
ct = ft O ct−1+
it O tanh(Wcxxt + Wchht−1 + bc)
ot = Q(Woxxt + Wohht−1 + Wocct + bo)
ht = ot O tanh(ct),
</equation>
<bodyText confidence="0.99960975">
where Q is the component-wise logistic sig-
moid function, and O is the component-wise
(Hadamard) product. LSTMs define an extra cell
memory ct, which is combined linearly at each
</bodyText>
<figure confidence="0.8275635">
cats
cats
</figure>
<figureCaption confidence="0.997946">
Figure 1: Illustration of the word lookup tables
</figureCaption>
<bodyText confidence="0.939399210526316">
(top) and the lexical Composition Model (bottom).
Square boxes represent vectors of neuron activa-
tions. Shaded boxes indicate that a non-linearity.
timestamp t. The information that is propagated
from ct−1 to ct is controlled by the three gates it,
ft, and ot, which determine the what to include
from the input xt, the what to forget from ct−1 and
what is relevant to the current state ht. We write
W to refer to all parameters the LSTM (Wix,
Wfx, bf, ... ). Thus, given a sequence of charac-
ter representations eCc1, ... , eCcm as input, the for-
ward LSTM, yields the state sequence sf0, .. . , sfm,
while the backward LSTM receives as input the re-
verse sequence, and yields states sbm, ... , sb0. Both
LSTMs use a different set of parameters Wf and
Wb. The representation of the word w is obtained
by combining the forward and backward states:
eCw = Dfsf m + Dbsb0 + bd,
where Df, Db and bd are parameters that deter-
</bodyText>
<figure confidence="0.994837826086957">
Word
Lookup
Table
cats
job
cat
....
....
embeddings
for word &amp;quot;cats&amp;quot;
embeddings
for word &amp;quot;cats&amp;quot;
Bi-LSTM
Character
Lookup
Table
c a t s
....
....
c
a
s
t
</figure>
<page confidence="0.976012">
1522
</page>
<bodyText confidence="0.999060391304348">
mine how the states are combined.
Caching for Efficiency. Relative to eWw , com-
puting eCw is computational expensive, as it re-
quires two LSTMs traversals of length m. How-
ever, eCw only depends on the character sequence
of that word, which means that unless the parame-
ters are updated, it is possible to cache the value of
eCw for each different w’s that will be used repeat-
edly. Thus, the model can keep a list of the most
frequently occurring word types in memory and
run the compositional model only for rare words.
Obviously, caching all words would yield the same
performance as using a word lookup table eWw , but
also using the same amount of memory. Conse-
quently, the number of word types used in cache
can be adjusted to satisfy memory vs. perfor-
mance requirements of a particular application.
At training time, when parameters are changing,
repeated words within the same batch only need to
be computed once, and the gradient at the output
can be accumulated within the batch so that only
one update needs to be done per word type. For
this reason, it is preferable to define larger batches.
</bodyText>
<sectionHeader confidence="0.997411" genericHeader="method">
4 Experiments: Language Modeling
</sectionHeader>
<bodyText confidence="0.999975052631579">
Our proposed model is similar to models used to
compute composed representations of sentences
from words (Cho et al., 2014; Li et al., 2015).
However, the relationship between the meanings
of individual words and the composite meaning
of a phrase or sentence is arguably more regular
than the relationship of representations of charac-
ters and the meaning of a word. Is our model capa-
ble of learning such an irregular relationship? We
now explore this question empirically.
Language modeling is a task with many appli-
cations in NLP. An effective LM requires syntactic
aspects of language to be modeled, such as word
orderings (e.g., “John is smart” vs. “John smart
is”), but also semantic aspects (e.g., “John ate fish”
vs. “fish ate John”). Thus, if our C2W model
only captures regular aspects of words, such as,
prefixes and suffixes, the model will yield worse
results compared to word lookup tables.
</bodyText>
<subsectionHeader confidence="0.964399">
4.1 Language Model
</subsectionHeader>
<bodyText confidence="0.978585227272727">
Language modeling amounts to learning a func-
tion that computes the log probability, log p(w),
of a sentence w = (w1, ... , wn). This quantity
can be decomposed according to the chain rule
into the sum of the conditional log probabilities
Eni=1 log p(wi  |w1, ... , wi−1). Our language
model computes log p(wi  |w1, ... , wi−1) by
composing representations of words w1, ... , wi−1
using an recurrent LSTM model (Mikolov et al.,
2010; Sundermeyer et al., 2012).
The model is illustrated in Figure 2, where we
observe on the first level that each word wi is pro-
jected into their word representations. This can be
done by using word lookup tables eWwi, in which
case, we will have a regular recurrent language
model. To use our C2W model, we can sim-
ply replace the word lookup table with the model
f(wi) = eCwi. Each LSTM block si, is used to
predict word wi+1. This is performed by project-
ing the si into a vector of size of the vocabulary V
and performing a softmax.
cats eat fish
</bodyText>
<figureCaption confidence="0.9922375">
Figure 2: Illustration of our neural network for
Language Modeling.
</figureCaption>
<bodyText confidence="0.999877833333333">
The softmax is still simply a d × V table,
which encodes the likelihood of every word type
in a given context, which is a closed-vocabulary
model. Thus, at test time out-of-vocabulary
(OOV) words cannot be addressed. A strategy
that is generally applied is to prune the vocabu-
lary V by replacing word types with lower fre-
quencies as an OOV token. At test time, the prob-
ability of words not in vocabulary is estimated as
the OOV token. Thus, depending on the number
of word types that are pruned, the global perplexi-
ties may decrease, since there are fewer outcomes
in the softmax, which makes the absolute value of
perplexity not informative when comparing mod-
els of different vocabulary sizes. Yet, the rela-
tive perplexity between different models indicates
which models can better predict words based on
their contexts.
</bodyText>
<figure confidence="0.98788">
embedings
for words
LSTM
Softmax
over
Vocabulary
cats eat fish &lt;/s&gt;
Word Lookup
or
Lexical
Composition
Model
</figure>
<page confidence="0.935577">
1523
</page>
<bodyText confidence="0.999528285714286">
To address OOV words in the baseline setup,
these are replaced by an unknown token, and also
associated with a set of embeddings. During train-
ing, word types that occur once are replaced with
the unknown token stochastically with 0.5 proba-
bility. The same process is applied at the character
level for the C2W model.
</bodyText>
<subsectionHeader confidence="0.776267">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999971756097561">
Datasets We look at the language model perfor-
mance on English, Portuguese, Catalan, German
and Turkish, which have a broad range of morpho-
logical typologies. While all these languages con-
tain inflections, in agglutinative languages affixes
tend to be unchanged, while in fusional languages
they are not. For each language, Wikipedia articles
were randomly extracted until 1 million words are
obtained and these were used for training. For de-
velopment and testing, we extracted an additional
set of 20,000 words.
Setup We define the size of the word represen-
tation d to 50. In the C2W model requires set-
ting the dimensionality of characters dC and cur-
rent states dCS. We set dC = 50 and dCS = 150.
Each LSTM state used in the language model se-
quence sz is set to 150 for both states and cell
memories. Training is performed with mini-batch
gradient descent with 100 sentences. The learn-
ing rate and momentum were set to 0.2 and 0.95.
The softmax over words is always performed on
lowercased words. We restrict the output vocabu-
lary to the most frequent 5000 words. Remaining
word types will be replaced by an unknown token,
which must also be predicted. The word represen-
tation layer is still performed over all word types
(i.e., completely open vocabulary). When using
word lookup tables, the input words are also low-
ercased, as this setup produces the best results. In
the C2W, case information is preserved.
Evaluation is performed by computing the per-
plexities over the test data, and the parameters that
yield the highest perplexity over the development
data are used.
Perplexities Perplexities over the testset are re-
ported on Table 4. From these results, we can see
that in general, it is clear that C2W always outper-
forms word lookup tables (row “Word”), and that
improvements are especially pronounced in Turk-
ish, which is a highly morphological language,
where word meanings differ radically depending
</bodyText>
<table confidence="0.999755375">
Fusional Agglutinative
Perplexity EN PT CA DE TR
5-gram KN 70.72 58.73 39.83 59.07 52.87
Word 59.38 46.17 35.34 43.02 44.01
C2W 57.39 40.92 34.92 41.94 32.88
#Parameters
Word 4.3M 4.2M 4.3M 6.3M 5.7M
C2W 180K 178K 182K 183K 174K
</table>
<tableCaption confidence="0.999569">
Table 1: Language Modeling Results
</tableCaption>
<bodyText confidence="0.999420307692308">
on the suffixes used (evde —* in the house vs. ev-
den —*from the house).
Number of Parameters As for the number of
parameters (illustrated for block “#Parameters”),
the number of parameters in word lookup tables is
V xd. If a language contains 80,000 word types (a
conservative estimate in morphologically rich lan-
guages), 4 million parameters would be necessary.
On the other hand, the compositional model con-
sists of 8 matrices of dimensions dCSxdC+2dCS.
Additionally, there is also the matrix that com-
bines the forward and backward states of size
d x 2dCS. Thus, the number of parameters is
roughly 150,000 parameters—substantially fewer.
This model also needs a character lookup table
with dC parameters for each entry. For English,
there are 618 characters, for an additional 30,900
parameters. So the total number of parameters for
English is roughly 180,000 parameters (2 to 3 pa-
rameters per word type), which is an order of mag-
nitude lower than word lookup tables.
Performance As for efficiency, both representa-
tions can label sentences at a rate of approximately
300 words per second during training. While this
is surprising, due to the fact that the C2W model
requires a composition over characters, the main
bottleneck of the system is the softmax over the
vocabulary. Furthermore, caching is used to avoid
composing the same word type twice in the same
batch. This shows that the C2W model, is rela-
tively fast compared operations such as a softmax.
Representations of (nonce) words While is is
promising that the model is not simply learning
lexical features, what is most interesting is that the
model can propose embeddings for nonce words,
in stark contrast to the situation observed with
lookup table models. We show the 5-most-similar
in-vocabulary words (measured with cosine simi-
larity) as computed by our character model on two
</bodyText>
<page confidence="0.931998">
1524
</page>
<table confidence="0.999589666666667">
increased John Noahshire phding
reduced Richard Nottinghamshire mixing
improved George Bucharest modelling
expected James Saxony styling
decreased Robert Johannesburg blaming
targeted Edward Gloucestershire christening
</table>
<tableCaption confidence="0.971308">
Table 2: Most-similar in-vocabular words under
</tableCaption>
<bodyText confidence="0.969596">
the C2W model; the two query words on the left
are in the training vocabulary, those on the right
are nonce (invented) words.
in-vocabulary words and two nonce words1.This
makes our model generalize significantly better
than lookup tables that generally use unknown to-
kens for OOV words. Furthermore, this ability to
generalize is much more similar to that of human
beings, who are able to infer meanings for new
words based on its form.
</bodyText>
<sectionHeader confidence="0.996113" genericHeader="method">
5 Experiments: Part-of-speech Tagging
</sectionHeader>
<bodyText confidence="0.999983714285714">
As a second illustration of the utility of our model,
we turn to POS tagging. As morphology is a
strong indicator for syntax in many languages,
a much effort has been spent engineering fea-
tures (Nakagawa et al., 2001; Mueller et al., 2013).
We now show that some of these features can be
learnt automatically using our model.
</bodyText>
<subsectionHeader confidence="0.977961">
5.1 Bi-LSTM Tagging Model
</subsectionHeader>
<bodyText confidence="0.997895764705882">
Our tagging model is likewise novel, but very
straightforward. It builds a Bi-LSTM over words
as illustrated in Figure 3. The input of the model
is a sequence of features f(w1), ... , f(wn). Once
again, word vectors can either be generated us-
ing the C2W model f(wi) = eCw., or word
lookup tables f(wi) = eWw.. We also test the us-
age of hand-engineered features, in which case
f1(wi), ... , fn(wi). Then, the sequential fea-
tures f(w1), ... , f(wn) are fed into a bidirec-
tional LSTM model, obtaining the forward states
sf0, ... , sn and the backward states sb
f N+1, ... , sb0.
Thus, state sfi contains the information of all
words from 0 to i and sbi from n to i. The for-
ward and backward states are combined, for each
index from 1 to n, as follows:
</bodyText>
<equation confidence="0.940254">
li = tanh(Lfsfi + Lbsbi + bl),
</equation>
<bodyText confidence="0.989498">
where Lf, Lb and bl are parameters defining how
the forward and backward states are combined.
</bodyText>
<footnote confidence="0.953522">
1software submitted as supplementary material
</footnote>
<bodyText confidence="0.999655166666667">
The size of the forward sf and backward states
sb and the combined state l are hyperparameters
of the model, denoted as dfWS, dbWS and dWS, re-
spectively. Finally, the output labels for index i
are obtained as a softmax over the POS tagset, by
projecting the combined state li.
</bodyText>
<equation confidence="0.5039665">
cats eat fish
NNS VBP NN
</equation>
<figureCaption confidence="0.997016">
Figure 3: Illustration of our neural network for
POS tagging.
</figureCaption>
<subsectionHeader confidence="0.964437">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999932347826087">
Datasets For English, we conduct experiments
on the Wall Street Journal of the Penn Treebank
dataset (Marcus et al., 1993), using the standard
splits (sections 1–18 for train, 19–21 for tuning
and 22–24 for testing). We also perform tests on
4 other languages, which we obtained from the
CoNLL shared tasks (Mart´ı et al., 2007; Brants
et al., 2002; Afonso et al., 2002; Atalay et al.,
2003). While the PTB dataset provides standard
train, tuning and test splits, there are no tuning sets
in the datasets in other languages, so we withdraw
the last 100 sentences from the training dataset and
use them for tuning.
Setup The POS model requires two sets of hy-
perparameters. Firstly, words must be converted
into continuous representations and the same hy-
perparametrization as in language modeling (Sec-
tion 4) is used. Additionally, we also compare to
the convolutional model of Santos and Zadrozny
(2014), which also requires the dimensionality
for characters and the word representation size,
which are set to 50 and 150, respectively. Sec-
ondly, words representations are combined to en-
</bodyText>
<figure confidence="0.996478214285714">
embedings
for words
Bi-LSTM
embedings
for words
in context
Softmax
over
Labels
Word Lookup
or
Lexical
Composition
Model
</figure>
<page confidence="0.984447">
1525
</page>
<bodyText confidence="0.991855450980393">
code context. Our POS tagger has three hyperpa-
rameters dfWS, dbWS and dWS, which correspond
to the sizes of LSTM states, and are all set to 50.
As for the learning algorithm, use the same setup
(learning rate, momentum and mini-batch sizes) as
used in language modeling.
Once again, we replace OOV words with an un-
known token, in the setup that uses word lookup
tables, and the same with OOV characters in the
C2W model. In setups using pre-trained word em-
beddings, we consider a word an OOV if it was not
seen in the labelled training data as well as in the
unlabeled data used for pre-training.
Compositional Model Comparison A compar-
ison of different recurrent neural networks for the
C2W model is presented in Table 3. We used our
proposed tagger tagger in all experiments and re-
sults are reported for the English Penn Treebank.
Results on label accuracy test set is shown in the
column “acc”. The number of parameters in the
word composition model is shown in the column
“parameters”. Finally, the number of words pro-
cessed at test time per second are shown in column
“words/sec”.
We observe that approaches using RNN yield
worse results than their LSTM counterparts with
a difference of approximately 2%. This suggests
that while regular RNNs can learn shorter char-
acter sequence dependencies, they are not ideal
to learn longer dependencies. LSTMs, on the
other hand, seem to effectively obtain relatively
higher results, on par with using word look up ta-
bles (row “Word Lookup”), even when using for-
ward (row “Forward LSTM”) and backward (row
“Backward LSTM”) LSTMs individually. The
best results are obtained using the bidirectional
LSTM (row “Bi-LSTM”), which achieves an ac-
curacy of 97.29% on the test set, surpassing the
word lookup table. The convolution model (San-
tos and Zadrozny, 2014) obtained slightly lower
results (row “Convolutional (S&amp;Z)”), we think
this is because the convolutional model uses a
max-pooling layer over series of window convolu-
tions. As order is only perserved within windows,
longer distance dependences are unobserved.
There are approximately 40k lowercased word
types in the training data in the PTB dataset. Thus,
a word lookup table with 50 dimensions per type
contains approximately 2 million parameters. In
the C2W models, the number of characters types
(including uppercase and lowercase) is approxi-
</bodyText>
<table confidence="0.9986786">
acc parameters words/sec
Word Lookup 96.97 2000k 6K
Convolutional (S&amp;Z) 96.80 42.5k 4K
Forward RNN 95.66 17.5k 4K
Backward RNN 95.52 17.5k 4K
Bi-RNN 95.93 40k 3K
Forward LSTM 97.12 80k 3K
Backward LSTM 97.08 80k 3K
Bi-LSTM dCS = 50 97.22 70k 3K
Bi-LSTM 97.36 150k 2K
</table>
<tableCaption confidence="0.9961215">
Table 3: POS accuracy results for the English PTB
using word representation models.
</tableCaption>
<bodyText confidence="0.997950108108108">
mately 80. Thus, the character look up table con-
sists of only 4k parameters, which is negligible
compared to the number of parameters in the com-
positional model, which is once again 150k pa-
rameters. One could argue that results in the Bi-
LSTM model are higher than those achieved by
other models as it contains more parameters, so
we set the state size dCS = 50 (row “Bi-LSTM
dCS = 50”) and obtained similar results.
In terms of computational speed, we can ob-
serve that there is a more significant slowdown
when applying the C2W models compared to lan-
guage modeling. This is because there is no longer
a softmax over the whole word vocabulary as the
main bottleneck of the network. However, we can
observe that while the Bi-LSTM system is 3 times
slower, it is does not significantly hurt the perfor-
mance of the system.
Results on Multiple Languages Results on 5
languages are shown in Table 4. In general, we
can observe that the model using word lookup
tables (row “Word”) performs consistently worse
than the C2W model (row “C2W”). We also com-
pare our results with Stanford’s POS tagger, with
the default set of features, found in Table 4. Re-
sults using these tagger are comparable or bet-
ter than state-of-the-art systems. We can observe
that in most cases we can slightly outperform
the scores obtained using their tagger. This is a
promising result, considering that we use the same
training data and do not handcraft any features.
Furthermore, we can observe that for Turkish, our
results are significantly higher (&gt;4%).
Comparison with Benchmarks Most state-of-
the-art POS tagging systems are obtained by ei-
ther learning or handcrafting good lexical fea-
tures (Manning, 2011; Sun, 2014) or using ad-
</bodyText>
<page confidence="0.972336">
1526
</page>
<table confidence="0.9991056">
System Fusional Agglutinative
EN PT CA DE TR
Word 96.97 95.67 98.09 97.51 83.43
C2W 97.36 97.47 98.92 98.08 91.59
Stanford 97.32 97.54 98.76 97.92 87.31
</table>
<tableCaption confidence="0.999597">
Table 4: POS accuracies on different languages
</tableCaption>
<bodyText confidence="0.999908045454546">
ditional raw data to learn features in an unsuper-
vised fashion. Generally, optimal results are ob-
tained by performing both. Table 5 shows the
current Benchmarks in this task for the English
PTB. Accuracies on the test set is reported on col-
umn “acc”. Columns “+feat” and “+data” de-
fine whether hand-crafted features are used and
whether additional data was used. We can see that
even without feature engineering or unsupervised
pretraining, our C2W model (row “C2W”) is on
par with the current state-of-the-art system (row
“structReg”). However, if we add hand-crafted
features, we can obtain further improvements on
this dataset (row “C2W + features”).
However, there are many words that do not con-
tain morphological cues to their part-of-speech.
For instance, the word snake does not contain any
morphological cues that determine its tag. In these
cases, if they are not found labelled in the training
data, the model would be dependent on context to
determine their tags, which could lead to errors in
ambiguous contexts. Unsupervised training meth-
ods such as the Skip-n-gram model (Mikolov et
al., 2013) can be used to pretrain the word rep-
resentations on unannotated corpora. If such pre-
training places cat, dog and snake near each other
in vector space, and the supervised POS data con-
tains evidence that cat and dog are nouns, our
model will be likely to label snake with the same
tag.
We train embeddings using English wikipedia
with the dataset used in (Ling et al., 2015), and
the Structured Skip-n-gram model. Results using
pre-trained word lookup tables and the C2W with
the pre-trained word lookup tables as additional
parameters are shown in rows “word(sskip)” and
“C2W + word(sskip)”. We can observe that both
systems can obtain improvements over their ran-
dom initializations (rows “word” and (C2W)).
Finally, we also found that when using the C2W
model in conjunction pre-trained word embed-
dings, that adding a non-linearity to the repre-
sentations extracted from the C2W model eCw im-
proves the results over using a simple linear trans-
</bodyText>
<table confidence="0.999047083333333">
+feat +data acc
word no no 96.70
C2W no no 97.36
word+features yes no 97.34
C2W+features yes no 97.57
Stanford 2.0 (Manning, 2011) yes no 97.32
structReg (Sun, 2014) yes no 97.36
word (sskip) no yes 97.42
C2W+word (sskip) no yes 97.54
C2W(tanh)+word (sskip) no yes 97.78
Morˇce (Spoustov´a et al., 2009) yes yes 97.44
SCCN (Søgaard, 2011) yes yes 97.50
</table>
<tableCaption confidence="0.7549225">
Table 5: POS accuracy result comparison with
state-of-the-art systems for the English PTB.
</tableCaption>
<bodyText confidence="0.970448">
formation (row “C2W(tanh)+word (sskip)”). This
setup, obtains 0.28 points over the current state-of-
the-art system(row “SCCN”).
</bodyText>
<subsectionHeader confidence="0.983474">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999972">
It is important to refer here that these results do
not imply that our model always outperforms ex-
isting benchmarks, in fact in most experiments,
results are typically fairly similar to existing sys-
tems. Even in Turkish, using morphological anal-
ysers in order to extract additional features could
also accomplish similar results. The goal of our
work is not to overcome existing benchmarks, but
show that much of the feature engineering done in
the benchmarks can be learnt automatically from
the task specific data. More importantly, we wish
to show large dimensionality word look tables can
be compacted into a lookup table using characters
and a compositional model allowing the model
scale better with the size of the training data. This
is a desirable property of the model as data be-
comes more abundant in many NLP tasks.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999894307692308">
Our work, which learns representations without
relying on word lookup tables has not been ex-
plored to our knowledge. In essence, our model
attempts to learn lexical features automatically
while compacting the model by reducing the re-
dundancy found in word lookup tables. Individ-
ually, these problems have been the focus of re-
search in many areas.
Lexical information has been used to augment
word lookup tables. Word representation learn-
ing can be thought of as a process that takes a
string as input representing a word and outputs
a set of values that represent a word in vector
</bodyText>
<page confidence="0.981136">
1527
</page>
<bodyText confidence="0.999908394366197">
space. Using word lookup tables is one possi-
ble approach to accomplish this. Many meth-
ods have been used to augment this model to
learn lexical features with an additional model
that is jointly maximized with the word lookup
table. This is generally accomplished by either
performing a component-wise addition of the em-
beddings produced by word lookup tables (Chen
et al., 2015), and that generated by the additional
lexical model, or simply concatenating both rep-
resentations (Santos and Zadrozny, 2014). Many
models have been proposed, the work in (Col-
lobert et al., 2011) refers that additional features
sets FZ can be added to the one-hot representa-
tion and multiple lookup tables IF, can be learnt
to project each of the feature sets to the same
low-dimensional vector eWw . For instance, the
work in (Botha and Blunsom, 2014) shows that us-
ing morphological analyzers to generate morpho-
logical features, such as stems, prefixes and suf-
fixes can be used to learn better representations
for words. A problem with this approach is the
fact that the model can only learn from what has
been defined as feature sets. The models proposed
in (Santos and Zadrozny, 2014; Chen et al., 2015)
allow the model to arbitrary extract meaningful
lexical features from words by defining composi-
tional models over characters. The work in (Chen
et al., 2015) defines a simple compositional model
by summing over all characters in a given word,
while the work in (Santos and Zadrozny, 2014)
defines a convolutional network, which combines
windows of characters and a max-pooling layer to
find important morphological features. The main
drawback of these methods is that character or-
der is often neglected, that is, when summing over
all character embeddings, words such as dog and
god would have the same representation accord-
ing to the lexical model. Convolutional model are
less susceptible to these problems as they com-
bine windows of characters at each convolution,
where the order within the window is preserved.
However, the order between extracted windows is
not, so the problem still persists for longer words,
such as those found in agglutinative languages.
Yet, these approaches work in conjunction with a
word lookup table, as they compensate for this in-
ability. Aside from neural approaches, character-
based models have been applied to address mul-
tiple lexically oriented tasks, such as translitera-
tion (Kang and Choi, 2000) and twitter normaliza-
tion (Xu et al., 2013; Ling et al., 2013).
Compacting models has been a focus of re-
search in tasks, such as language modeling and
machine translation, as extremely large models
can be built with the large amounts of training
data that are available in these tasks. In language
modeling, it is frequent to prune higher order n-
grams that do not encode any additional infor-
mation (Seymore and Rosenfeld, 1996; Stolcke,
1998; Moore and Quirk, 2009). The same be ap-
plied in machine translation (Ling et al., 2012;
Zens et al., 2012) by removing longer translation
pairs that can be replicated using smaller ones. In
essence our model learns regularities at the sub-
word level that can be leveraged for building more
compact word representations.
Finally, our work has been applied to depen-
dency parsing and found similar improvements
over word models in morphologically rich lan-
guages (Ballesteros et al., 2015).
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999930473684211">
We propose a C2W model that builds word em-
beddings for words without an explicit word
lookup table. Thus, it benefits from being sen-
sitive to lexical aspects within words, as it takes
characters as atomic units to derive the embed-
dings for the word. On POS tagging, our mod-
els using characters alone can still achieve com-
parable or better results than state-of-the-art sys-
tems, without the need to manually engineer such
lexical features. Although both language model-
ing and POS tagging both benefit strongly from
morphological cues, the success of our models in
languages with impoverished morphological cues
shows that it is able to learn non-compositional as-
pects of how letters fit together.
The code for the C2W model and our language
model and POS tagger implementations is avail-
able from https://github.com/wlin12/
JNN.
</bodyText>
<sectionHeader confidence="0.99774" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999856375">
The PhD thesis of Wang Ling is supported by
FCT grant SFRH/BD/51157/2010. This research
was supported in part by the U.S. Army Re-
search Laboratory, the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533
and NSF IIS-1054319 and FCT through the pluri-
anual contract UID/CEC/50021/2013 and grant
number SFRH/BPD/68428/2010.
</bodyText>
<page confidence="0.991519">
1528
</page>
<sectionHeader confidence="0.996263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824683168317">
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. “Floresta sint´a(c)tica”: a tree-
bank for Portuguese. In Proc. LREC.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
In Proc. the 4th International Workshop on Linguis-
tically Interpreted Corpora (LINC).
Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with LSTMs. In
Proc. EMNLP.
Jan A. Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proc. ICML.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. EMNLP.
Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,
and Huanbo Luan. 2015. Joint learning of character
and word embeddings.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proc. EMNLP.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR.
Ferdinand de Saussure. 1916. Course in General Lin-
guistics.
Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. EMNLP.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
transliteration and back-transliteration by decision
tree learning. In LREC.
Jiwei Li, Dan Jurafsky, and Eduard H. Hovy. 2015.
When are tree structures necessary for deep learning
of representations? CoRR, abs/1503.00185.
Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proc. EMNLP.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proc. EMNLP.
Wang Ling, Chris Dyer, Alan Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proc. NAACL.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proc. ACL.
Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with re-
cursive neural networks for morphology. In Proc.
CoNLL.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proc. CICLing.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn treebank. Com-
put. Linguist.
William Marslen-Wilson and Lorraine K. Tyler. 1998.
Rules, representations, and the English past tense.
Trends in Cognitive Science, 2(11).
M. Antonia Marti, Mariona Taul´e, Lluis M´arquez, and
Manuel Bertran. 2007. CESS-ECE: A multilingual
and multilevel annotated corpus. In Proc. LREC.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Proc.
Interspeech.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. NIPS.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proc. EMNLP.
Thomas Mueller, Helmut Schmid, and Hinrich
Sch¨utze. 2013. Efficient higher-order CRFs for
morphological tagging. In Proc. EMNLP.
Tetsuji Nakagawa, Taku Kudoh, and Yuji Matsumoto.
2001. Unknown word guessing and part-of-speech
tagging using support vector machines. In In Proc.
the Sixth Natural Language Processing Pacific Rim
Symposium.
Cicero D. Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-of-
speech tagging. In Proc. ICML.
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able backoff language models. In Proc. ICSLP.
</reference>
<page confidence="0.885139">
1529
</page>
<reference confidence="0.999729642857143">
Anders Søgaard. 2011. Semisupervised condensed
nearest neighbor for part-of-speech tagging. In
Proc. ACL.
Radu Soricut and Franz Och. 2015. Unsupervised
morphology induction using word embeddings. In
Proc. NAACL.
Drahom´ıra Spoustov´a, Jan Hajiˇc, Jan Raab, and
Miroslav Spousta. 2009. Semi-supervised training
for the averaged perceptron POS tagger. In Proc.
EACL.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In In Proc. DARPA
Broadcast News Transcription and Understanding
Workshop.
Xu Sun. 2014. Structure regularization for struc-
tured prediction: Theories and experiments. CoRR,
abs/1411.6243.
Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Proc. Interspeech.
Wei Xu, Alan Ritter, and Ralph Grishman. 2013.
Gathering and generating paraphrases from twitter
with application to normalization. In Proceedings
of the Sixth Workshop on Building and Using Com-
parable Corpora.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proc. EMNLP.
</reference>
<page confidence="0.989476">
1530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408647">
<title confidence="0.9977855">Finding Function in Form: Compositional Character Models Open Vocabulary Word Representation</title>
<author confidence="0.990518">Wang Ling Tiago Luis Luis Marujo Ram´on Fernandez Silvio Amir Chris Dyer Alan W Black Isabel Trancoso</author>
<affiliation confidence="0.824043">Spoken Systems Lab, INESC-ID, Lisbon, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, Instituto Superior T´ecnico, Lisbon,</affiliation>
<abstract confidence="0.983690894736842">We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form–function relationship in language, our “composed” word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susana Afonso</author>
<author>Eckhard Bick</author>
<author>Renato Haber</author>
<author>Diana Santos</author>
</authors>
<title>Floresta sint´a(c)tica”: a treebank for Portuguese. In</title>
<date>2002</date>
<booktitle>Proc. LREC.</booktitle>
<contexts>
<context position="22861" citStr="Afonso et al., 2002" startWordPosition="3818" endWordPosition="3821"> and dWS, respectively. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state li. cats eat fish NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the wor</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>Susana Afonso, Eckhard Bick, Renato Haber, and Diana Santos. 2002. “Floresta sint´a(c)tica”: a treebank for Portuguese. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nart B Atalay</author>
<author>Kemal Oflazer</author>
<author>Bilge Say</author>
</authors>
<title>The annotation process in the Turkish treebank.</title>
<date>2003</date>
<booktitle>In In Proc. the 4th International Workshop on Linguistically Interpreted Corpora (LINC).</booktitle>
<contexts>
<context position="22883" citStr="Atalay et al., 2003" startWordPosition="3822" endWordPosition="3825">y. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state li. cats eat fish NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the word representation size,</context>
</contexts>
<marker>Atalay, Oflazer, Say, 2003</marker>
<rawString>Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003. The annotation process in the Turkish treebank. In In Proc. the 4th International Workshop on Linguistically Interpreted Corpora (LINC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Improved transition-based parsing by modeling characters instead of words with LSTMs. In</title>
<date>2015</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="35717" citStr="Ballesteros et al., 2015" startWordPosition="5951" endWordPosition="5954">eling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic units to derive the embeddings for the word. On POS tagging, our models using characters alone can still achieve comparable or better results than state-of-the-art systems, without the need to manually engineer such lexical features. Although both language modeling and POS tagging both benefit strongly from morphological cues, the success of our models in languages with impover</context>
</contexts>
<marker>Ballesteros, Dyer, Smith, 2015</marker>
<rawString>Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2015. Improved transition-based parsing by modeling characters instead of words with LSTMs. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional morphology for word representations and language modelling.</title>
<date>2014</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="8112" citStr="Botha and Blunsom, 2014" startWordPosition="1268" endWordPosition="1271">. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, </context>
<context position="33202" citStr="Botha and Blunsom, 2014" startWordPosition="5539" endWordPosition="5542">h the word lookup table. This is generally accomplished by either performing a component-wise addition of the embeddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both representations (Santos and Zadrozny, 2014). Many models have been proposed, the work in (Collobert et al., 2011) refers that additional features sets FZ can be added to the one-hot representation and multiple lookup tables IF, can be learnt to project each of the feature sets to the same low-dimensional vector eWw . For instance, the work in (Botha and Blunsom, 2014) shows that using morphological analyzers to generate morphological features, such as stems, prefixes and suffixes can be used to learn better representations for words. A problem with this approach is the fact that the model can only learn from what has been defined as feature sets. The models proposed in (Santos and Zadrozny, 2014; Chen et al., 2015) allow the model to arbitrary extract meaningful lexical features from words by defining compositional models over characters. The work in (Chen et al., 2015) defines a simple compositional model by summing over all characters in a given word, wh</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<date>2002</date>
<note>The TIGER treebank.</note>
<contexts>
<context position="22840" citStr="Brants et al., 2002" startWordPosition="3814" endWordPosition="3817">denoted as dfWS, dbWS and dWS, respectively. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state li. cats eat fish NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for c</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="4958" citStr="Chen and Manning, 2014" startWordPosition="724" endWordPosition="727"> Word Vectors It is commonplace to represent words as vectors. In contrast to naive models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P E Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w E V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w, that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup table and we shall denote by eWw E Rd the embedding obtained from a word lookup table for w as eWw = P · 1w. This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data and lev</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Lei Xu</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Huanbo Luan</author>
</authors>
<title>Joint learning of character and word embeddings.</title>
<date>2015</date>
<contexts>
<context position="32749" citStr="Chen et al., 2015" startWordPosition="5463" endWordPosition="5466"> Lexical information has been used to augment word lookup tables. Word representation learning can be thought of as a process that takes a string as input representing a word and outputs a set of values that represent a word in vector 1527 space. Using word lookup tables is one possible approach to accomplish this. Many methods have been used to augment this model to learn lexical features with an additional model that is jointly maximized with the word lookup table. This is generally accomplished by either performing a component-wise addition of the embeddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both representations (Santos and Zadrozny, 2014). Many models have been proposed, the work in (Collobert et al., 2011) refers that additional features sets FZ can be added to the one-hot representation and multiple lookup tables IF, can be learnt to project each of the feature sets to the same low-dimensional vector eWw . For instance, the work in (Botha and Blunsom, 2014) shows that using morphological analyzers to generate morphological features, such as stems, prefixes and suffixes can be used to learn better repr</context>
</contexts>
<marker>Chen, Xu, Liu, Sun, Luan, 2015</marker>
<rawString>Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. 2015. Joint learning of character and word embeddings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>Proc. EMNLP.</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="4883" citStr="Collobert et al., 2011" startWordPosition="711" endWordPosition="714">rk in Section 6; and we conclude in Section 7. 2 Word Vectors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to naive models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P E Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w E V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w, that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup table and we shall denote by eWw E Rd the embedding obtained from a word lookup table for w as eWw = P · 1w. This allows tasks with low amounts of annotated dat</context>
<context position="32945" citStr="Collobert et al., 2011" startWordPosition="5493" endWordPosition="5497">t of values that represent a word in vector 1527 space. Using word lookup tables is one possible approach to accomplish this. Many methods have been used to augment this model to learn lexical features with an additional model that is jointly maximized with the word lookup table. This is generally accomplished by either performing a component-wise addition of the embeddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both representations (Santos and Zadrozny, 2014). Many models have been proposed, the work in (Collobert et al., 2011) refers that additional features sets FZ can be added to the one-hot representation and multiple lookup tables IF, can be learnt to project each of the feature sets to the same low-dimensional vector eWw . For instance, the work in (Botha and Blunsom, 2014) shows that using morphological analyzers to generate morphological features, such as stems, prefixes and suffixes can be used to learn better representations for words. A problem with this approach is the fact that the model can only learn from what has been defined as feature sets. The models proposed in (Santos and Zadrozny, 2014; Chen et</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferdinand de Saussure</author>
</authors>
<date>1916</date>
<note>Course in General Linguistics.</note>
<marker>de Saussure, 1916</marker>
<rawString>Ferdinand de Saussure. 1916. Course in General Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks.</title>
<date>2005</date>
<contexts>
<context position="8957" citStr="Graves and Schmidhuber, 2005" startWordPosition="1398" endWordPosition="1401">end on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical similarities convey different meanings, such as, the word pairs lesson � lessen and coarse course. 3 C2W Model Our compositional character to word (C2W) model is based on bidirectional LSTMs (Graves and Schmidhuber, 2005), which are able to learn complex non-local dependencies in sequence models. An illustration is shown in Figure 1. The input of the C2W model (illustrated on bottom) is a single word type w, and we wish to obtain is a d-dimensional vector used to represent w. This model shares the same input and output of a word lookup table (illustrated on top), allowing it to easily replace then in any network. As input, we define an alphabet of characters C. For English, this vocabulary would contain an entry for each uppercase and lowercase letter as well as numbers and punctuation. The input word w is dec</context>
</contexts>
<marker>Graves, Schmidhuber, 2005</marker>
<rawString>Alex Graves and J¨urgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural Comput.,</journal>
<volume>9</volume>
<issue>8</issue>
<contexts>
<context position="2696" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="371" endWordPosition="375">or similarity in function: small orthographic differences may correspond to large semantic or syntactic differences (butter vs. batter), and large orthographic differences may obscure nearly perfect functional correspondence (rich vs. affluent). Thus, any orthographically aware model must be able to capture non-compositional effects in addition to more regular effects due to, e.g., morphological processes. To model the complex formfunction relationship, we turn to long short-term memories (LSTMs), which are designed to be able to capture complex non-linear and non-local dynamics in sequences (Hochreiter and Schmidhuber, 1997). We use bidirectional LSTMs to “read” the character sequences that constitute each word and combine them into a vector representation of the word. This model assumes that each character type is associated with a vector, and the LSTM parameters encode both idiosyncratic lexical and regular morphological knowledge. To evaluate our model, we use a vectorbased model for part-of-speech (POS) tagging and for language modeling, and we report experiments on these tasks in several languages comparing to baselines that use more traditional, orthographically-unaware parameterizations. These experiments </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="4915" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="715" endWordPosition="719">conclude in Section 7. 2 Word Vectors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to naive models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P E Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w E V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w, that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup table and we shall denote by eWw E Rd the embedding obtained from a word lookup table for w as eWw = P · 1w. This allows tasks with low amounts of annotated data to be trained jointly with oth</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic transliteration and back-transliteration by decision tree learning.</title>
<date>2000</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="34790" citStr="Kang and Choi, 2000" startWordPosition="5796" endWordPosition="5799">tion according to the lexical model. Convolutional model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be r</context>
</contexts>
<marker>Kang, Choi, 2000</marker>
<rawString>Byung-Ju Kang and Key-Sun Choi. 2000. Automatic transliteration and back-transliteration by decision tree learning. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Dan Jurafsky</author>
<author>Eduard H Hovy</author>
</authors>
<title>When are tree structures necessary for deep learning of representations? CoRR,</title>
<date>2015</date>
<contexts>
<context position="12974" citStr="Li et al., 2015" startWordPosition="2139" endWordPosition="2142">onsequently, the number of word types used in cache can be adjusted to satisfy memory vs. performance requirements of a particular application. At training time, when parameters are changing, repeated words within the same batch only need to be computed once, and the gradient at the output can be accumulated within the batch so that only one update needs to be done per word type. For this reason, it is preferable to define larger batches. 4 Experiments: Language Modeling Our proposed model is similar to models used to compute composed representations of sentences from words (Cho et al., 2014; Li et al., 2015). However, the relationship between the meanings of individual words and the composite meaning of a phrase or sentence is arguably more regular than the relationship of representations of characters and the meaning of a word. Is our model capable of learning such an irregular relationship? We now explore this question empirically. Language modeling is a task with many applications in NLP. An effective LM requires syntactic aspects of language to be modeled, such as word orderings (e.g., “John is smart” vs. “John smart is”), but also semantic aspects (e.g., “John ate fish” vs. “fish ate John”).</context>
</contexts>
<marker>Li, Jurafsky, Hovy, 2015</marker>
<rawString>Jiwei Li, Dan Jurafsky, and Eduard H. Hovy. 2015. When are tree structures necessary for deep learning of representations? CoRR, abs/1503.00185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Jo˜ao Grac¸a</author>
<author>Isabel Trancoso</author>
<author>Alan Black</author>
</authors>
<title>Entropy-based pruning for phrasebased machine translation.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Ling, Grac¸a, Trancoso, Black, 2012</marker>
<rawString>Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso, and Alan Black. 2012. Entropy-based pruning for phrasebased machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="34853" citStr="Ling et al., 2013" startWordPosition="5808" endWordPosition="5811"> susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regul</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="29787" citStr="Ling et al., 2015" startWordPosition="4977" endWordPosition="4980">y are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training methods such as the Skip-n-gram model (Mikolov et al., 2013) can be used to pretrain the word representations on unannotated corpora. If such pretraining places cat, dog and snake near each other in vector space, and the supervised POS data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eCw improves the results over using a simple linear trans+feat +data acc word no no 96.7</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4933" citStr="Liu et al., 2014" startWordPosition="720" endWordPosition="723">ctors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to naive models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P E Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w E V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w, that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup table and we shall denote by eWw E Rd the embedding obtained from a word lookup table for w as eWw = P · 1w. This allows tasks with low amounts of annotated data to be trained jointly with other tasks with larg</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="8087" citStr="Luong et al., 2013" startWordPosition="1264" endWordPosition="1267"> are not independent. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have rel</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In</title>
<date>2011</date>
<booktitle>Proc. CICLing.</booktitle>
<contexts>
<context position="28077" citStr="Manning, 2011" startWordPosition="4692" endWordPosition="4693">h Stanford’s POS tagger, with the default set of features, found in Table 4. Results using these tagger are comparable or better than state-of-the-art systems. We can observe that in most cases we can slightly outperform the scores obtained using their tagger. This is a promising result, considering that we use the same training data and do not handcraft any features. Furthermore, we can observe that for Turkish, our results are significantly higher (&gt;4%). Comparison with Benchmarks Most state-ofthe-art POS tagging systems are obtained by either learning or handcrafting good lexical features (Manning, 2011; Sun, 2014) or using ad1526 System Fusional Agglutinative EN PT CA DE TR Word 96.97 95.67 98.09 97.51 83.43 C2W 97.36 97.47 98.92 98.08 91.59 Stanford 97.32 97.54 98.76 97.92 87.31 Table 4: POS accuracies on different languages ditional raw data to learn features in an unsupervised fashion. Generally, optimal results are obtained by performing both. Table 5 shows the current Benchmarks in this task for the English PTB. Accuracies on the test set is reported on column “acc”. Columns “+feat” and “+data” define whether hand-crafted features are used and whether additional data was used. We can s</context>
<context position="30486" citStr="Manning, 2011" startWordPosition="5092" endWordPosition="5093">and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eCw improves the results over using a simple linear trans+feat +data acc word no no 96.70 C2W no no 97.36 word+features yes no 97.34 C2W+features yes no 97.57 Stanford 2.0 (Manning, 2011) yes no 97.32 structReg (Sun, 2014) yes no 97.36 word (sskip) no yes 97.42 C2W+word (sskip) no yes 97.54 C2W(tanh)+word (sskip) no yes 97.78 Morˇce (Spoustov´a et al., 2009) yes yes 97.44 SCCN (Søgaard, 2011) yes yes 97.50 Table 5: POS accuracy result comparison with state-of-the-art systems for the English PTB. formation (row “C2W(tanh)+word (sskip)”). This setup, obtains 0.28 points over the current state-ofthe-art system(row “SCCN”). 5.3 Discussion It is important to refer here that these results do not imply that our model always outperforms existing benchmarks, in fact in most experiments</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Proc. CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="22614" citStr="Marcus et al., 1993" startWordPosition="3775" endWordPosition="3778">e parameters defining how the forward and backward states are combined. 1software submitted as supplementary material The size of the forward sf and backward states sb and the combined state l are hyperparameters of the model, denoted as dfWS, dbWS and dWS, respectively. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state li. cats eat fish NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous r</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Marslen-Wilson</author>
<author>Lorraine K Tyler</author>
</authors>
<title>Rules, representations, and the English past tense.</title>
<date>1998</date>
<journal>Trends in Cognitive Science,</journal>
<volume>2</volume>
<issue>11</issue>
<contexts>
<context position="7785" citStr="Marslen-Wilson and Tyler, 1998" startWordPosition="1216" endWordPosition="1219">d tokenized word types, each of which would need its own vector. Intuitively, it is not sensible to use the same number of parameters for each word type. Finally, it is important to remark that it is uncontroversial among cognitive scientists that our lexicon is structured into related forms—i.e., their parameters are not independent. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would li</context>
</contexts>
<marker>Marslen-Wilson, Tyler, 1998</marker>
<rawString>William Marslen-Wilson and Lorraine K. Tyler. 1998. Rules, representations, and the English past tense. Trends in Cognitive Science, 2(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Antonia Marti</author>
<author>Mariona Taul´e</author>
<author>Lluis M´arquez</author>
<author>Manuel Bertran</author>
</authors>
<title>CESS-ECE: A multilingual and multilevel annotated corpus.</title>
<date>2007</date>
<booktitle>In Proc. LREC.</booktitle>
<marker>Marti, Taul´e, M´arquez, Bertran, 2007</marker>
<rawString>M. Antonia Marti, Mariona Taul´e, Lluis M´arquez, and Manuel Bertran. 2007. CESS-ECE: A multilingual and multilevel annotated corpus. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proc. Interspeech.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="5806" citStr="Mikolov et al. (2013)" startWordPosition="889" endWordPosition="892">which we write 1w, that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup table and we shall denote by eWw E Rd the embedding obtained from a word lookup table for w as eWw = P · 1w. This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data and leverage the similarities in these tasks. A common practice to this end is to initialize the word lookup table with the parameters trained on an unsupervised task. Some examples of these include the skip-n-gram and CBOW models of Mikolov et al. (2013). 2.1 Problem: Independent Parameters There are two practical problems with word lookup tables. Firstly, while they can be pretrained with large amounts of data to learn semantic and syntactic similarities between words, each vector is independent. That is, even though models based on word lookup tables are often observed to learn that cats, kings and queens exist in roughly the same linear correspondences to each other as cat, king and queen do, the model does not represent the fact that adding an s at the end of the word is evidence for this transformation. This means that word lookup tables</context>
<context position="29413" citStr="Mikolov et al., 2013" startWordPosition="4909" endWordPosition="4912">current state-of-the-art system (row “structReg”). However, if we add hand-crafted features, we can obtain further improvements on this dataset (row “C2W + features”). However, there are many words that do not contain morphological cues to their part-of-speech. For instance, the word snake does not contain any morphological cues that determine its tag. In these cases, if they are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training methods such as the Skip-n-gram model (Mikolov et al., 2013) can be used to pretrain the word representations on unannotated corpora. If such pretraining places cat, dog and snake near each other in vector space, and the supervised POS data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Less is more: significance-based n-gram selection for smaller, better language models.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="35256" citStr="Moore and Quirk, 2009" startWordPosition="5876" endWordPosition="5879"> from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from b</context>
</contexts>
<marker>Moore, Quirk, 2009</marker>
<rawString>Robert C. Moore and Chris Quirk. 2009. Less is more: significance-based n-gram selection for smaller, better language models. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mueller</author>
<author>Helmut Schmid</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Efficient higher-order CRFs for morphological tagging. In</title>
<date>2013</date>
<booktitle>Proc. EMNLP.</booktitle>
<marker>Mueller, Schmid, Sch¨utze, 2013</marker>
<rawString>Thomas Mueller, Helmut Schmid, and Hinrich Sch¨utze. 2013. Efficient higher-order CRFs for morphological tagging. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Taku Kudoh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Unknown word guessing and part-of-speech tagging using support vector machines.</title>
<date>2001</date>
<booktitle>In In Proc. the Sixth Natural Language Processing Pacific Rim Symposium.</booktitle>
<contexts>
<context position="21056" citStr="Nakagawa et al., 2001" startWordPosition="3492" endWordPosition="3495">bulary, those on the right are nonce (invented) words. in-vocabulary words and two nonce words1.This makes our model generalize significantly better than lookup tables that generally use unknown tokens for OOV words. Furthermore, this ability to generalize is much more similar to that of human beings, who are able to infer meanings for new words based on its form. 5 Experiments: Part-of-speech Tagging As a second illustration of the utility of our model, we turn to POS tagging. As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. 5.1 Bi-LSTM Tagging Model Our tagging model is likewise novel, but very straightforward. It builds a Bi-LSTM over words as illustrated in Figure 3. The input of the model is a sequence of features f(w1), ... , f(wn). Once again, word vectors can either be generated using the C2W model f(wi) = eCw., or word lookup tables f(wi) = eWw.. We also test the usage of hand-engineered features, in which case f1(wi), ... , fn(wi). Then, the sequential features f(w1), ... , f(wn) are fed into a bi</context>
</contexts>
<marker>Nakagawa, Kudoh, Matsumoto, 2001</marker>
<rawString>Tetsuji Nakagawa, Taku Kudoh, and Yuji Matsumoto. 2001. Unknown word guessing and part-of-speech tagging using support vector machines. In In Proc. the Sixth Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero D Santos</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning character-level representations for part-ofspeech tagging. In</title>
<date>2014</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="23394" citStr="Santos and Zadrozny (2014)" startWordPosition="3905" endWordPosition="3908">ed from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the word representation size, which are set to 50 and 150, respectively. Secondly, words representations are combined to enembedings for words Bi-LSTM embedings for words in context Softmax over Labels Word Lookup or Lexical Composition Model 1525 code context. Our POS tagger has three hyperparameters dfWS, dbWS and dWS, which correspond to the sizes of LSTM states, and are all set to 50. As for the learning algorithm, use the same setup (learning rate, momentum and mini-batch sizes) as used in language modeling. Once again, we replac</context>
<context position="25503" citStr="Santos and Zadrozny, 2014" startWordPosition="4257" endWordPosition="4261">h a difference of approximately 2%. This suggests that while regular RNNs can learn shorter character sequence dependencies, they are not ideal to learn longer dependencies. LSTMs, on the other hand, seem to effectively obtain relatively higher results, on par with using word look up tables (row “Word Lookup”), even when using forward (row “Forward LSTM”) and backward (row “Backward LSTM”) LSTMs individually. The best results are obtained using the bidirectional LSTM (row “Bi-LSTM”), which achieves an accuracy of 97.29% on the test set, surpassing the word lookup table. The convolution model (Santos and Zadrozny, 2014) obtained slightly lower results (row “Convolutional (S&amp;Z)”), we think this is because the convolutional model uses a max-pooling layer over series of window convolutions. As order is only perserved within windows, longer distance dependences are unobserved. There are approximately 40k lowercased word types in the training data in the PTB dataset. Thus, a word lookup table with 50 dimensions per type contains approximately 2 million parameters. In the C2W models, the number of characters types (including uppercase and lowercase) is approxiacc parameters words/sec Word Lookup 96.97 2000k 6K Con</context>
<context position="32875" citStr="Santos and Zadrozny, 2014" startWordPosition="5481" endWordPosition="5484">process that takes a string as input representing a word and outputs a set of values that represent a word in vector 1527 space. Using word lookup tables is one possible approach to accomplish this. Many methods have been used to augment this model to learn lexical features with an additional model that is jointly maximized with the word lookup table. This is generally accomplished by either performing a component-wise addition of the embeddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both representations (Santos and Zadrozny, 2014). Many models have been proposed, the work in (Collobert et al., 2011) refers that additional features sets FZ can be added to the one-hot representation and multiple lookup tables IF, can be learnt to project each of the feature sets to the same low-dimensional vector eWw . For instance, the work in (Botha and Blunsom, 2014) shows that using morphological analyzers to generate morphological features, such as stems, prefixes and suffixes can be used to learn better representations for words. A problem with this approach is the fact that the model can only learn from what has been defined as fe</context>
</contexts>
<marker>Santos, Zadrozny, 2014</marker>
<rawString>Cicero D. Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-ofspeech tagging. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable backoff language models.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="35217" citStr="Seymore and Rosenfeld, 1996" startWordPosition="5870" endWordPosition="5873">as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable backoff language models. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semisupervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="30694" citStr="Søgaard, 2011" startWordPosition="5127" endWordPosition="5128"> initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eCw improves the results over using a simple linear trans+feat +data acc word no no 96.70 C2W no no 97.36 word+features yes no 97.34 C2W+features yes no 97.57 Stanford 2.0 (Manning, 2011) yes no 97.32 structReg (Sun, 2014) yes no 97.36 word (sskip) no yes 97.42 C2W+word (sskip) no yes 97.54 C2W(tanh)+word (sskip) no yes 97.78 Morˇce (Spoustov´a et al., 2009) yes yes 97.44 SCCN (Søgaard, 2011) yes yes 97.50 Table 5: POS accuracy result comparison with state-of-the-art systems for the English PTB. formation (row “C2W(tanh)+word (sskip)”). This setup, obtains 0.28 points over the current state-ofthe-art system(row “SCCN”). 5.3 Discussion It is important to refer here that these results do not imply that our model always outperforms existing benchmarks, in fact in most experiments, results are typically fairly similar to existing systems. Even in Turkish, using morphological analysers in order to extract additional features could also accomplish similar results. The goal of our work i</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Franz Och</author>
</authors>
<title>Unsupervised morphology induction using word embeddings.</title>
<date>2015</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="8136" citStr="Soricut and Och, 2015" startWordPosition="1272" endWordPosition="1275">se debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical </context>
</contexts>
<marker>Soricut, Och, 2015</marker>
<rawString>Radu Soricut and Franz Och. 2015. Unsupervised morphology induction using word embeddings. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahom´ıra Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proc. EACL.</booktitle>
<marker>Spoustov´a, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahom´ıra Spoustov´a, Jan Hajiˇc, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models. In</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="35232" citStr="Stolcke, 1998" startWordPosition="5874" endWordPosition="5875">nability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. </context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In In Proc. DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
</authors>
<title>Structure regularization for structured prediction: Theories and experiments.</title>
<date>2014</date>
<location>CoRR, abs/1411.6243.</location>
<contexts>
<context position="28089" citStr="Sun, 2014" startWordPosition="4694" endWordPosition="4695">S tagger, with the default set of features, found in Table 4. Results using these tagger are comparable or better than state-of-the-art systems. We can observe that in most cases we can slightly outperform the scores obtained using their tagger. This is a promising result, considering that we use the same training data and do not handcraft any features. Furthermore, we can observe that for Turkish, our results are significantly higher (&gt;4%). Comparison with Benchmarks Most state-ofthe-art POS tagging systems are obtained by either learning or handcrafting good lexical features (Manning, 2011; Sun, 2014) or using ad1526 System Fusional Agglutinative EN PT CA DE TR Word 96.97 95.67 98.09 97.51 83.43 C2W 97.36 97.47 98.92 98.08 91.59 Stanford 97.32 97.54 98.76 97.92 87.31 Table 4: POS accuracies on different languages ditional raw data to learn features in an unsupervised fashion. Generally, optimal results are obtained by performing both. Table 5 shows the current Benchmarks in this task for the English PTB. Accuracies on the test set is reported on column “acc”. Columns “+feat” and “+data” define whether hand-crafted features are used and whether additional data was used. We can see that even</context>
<context position="30521" citStr="Sun, 2014" startWordPosition="5098" endWordPosition="5099">ookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eCw improves the results over using a simple linear trans+feat +data acc word no no 96.70 C2W no no 97.36 word+features yes no 97.34 C2W+features yes no 97.57 Stanford 2.0 (Manning, 2011) yes no 97.32 structReg (Sun, 2014) yes no 97.36 word (sskip) no yes 97.42 C2W+word (sskip) no yes 97.54 C2W(tanh)+word (sskip) no yes 97.78 Morˇce (Spoustov´a et al., 2009) yes yes 97.44 SCCN (Søgaard, 2011) yes yes 97.50 Table 5: POS accuracy result comparison with state-of-the-art systems for the English PTB. formation (row “C2W(tanh)+word (sskip)”). This setup, obtains 0.28 points over the current state-ofthe-art system(row “SCCN”). 5.3 Discussion It is important to refer here that these results do not imply that our model always outperforms existing benchmarks, in fact in most experiments, results are typically fairly simi</context>
</contexts>
<marker>Sun, 2014</marker>
<rawString>Xu Sun. 2014. Structure regularization for structured prediction: Theories and experiments. CoRR, abs/1411.6243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>LSTM neural networks for language modeling.</title>
<date>2012</date>
<booktitle>In Proc. Interspeech.</booktitle>
<marker>Sundermeyer, Schl¨uter, Ney, 2012</marker>
<rawString>Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. 2012. LSTM neural networks for language modeling. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Ralph Grishman</author>
</authors>
<title>Gathering and generating paraphrases from twitter with application to normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora.</booktitle>
<contexts>
<context position="34833" citStr="Xu et al., 2013" startWordPosition="5804" endWordPosition="5807">al model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence ou</context>
</contexts>
<marker>Xu, Ritter, Grishman, 2013</marker>
<rawString>Wei Xu, Alan Ritter, and Ralph Grishman. 2013. Gathering and generating paraphrases from twitter with application to normalization. In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Daisy Stanton</author>
<author>Peng Xu</author>
</authors>
<title>A systematic comparison of phrase table pruning techniques.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="35339" citStr="Zens et al., 2012" startWordPosition="5892" endWordPosition="5895">xically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic un</context>
</contexts>
<marker>Zens, Stanton, Xu, 2012</marker>
<rawString>Richard Zens, Daisy Stanton, and Peng Xu. 2012. A systematic comparison of phrase table pruning techniques. In Proc. EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.970725">
Exploring Markov Logic Networks for Question Answering
</title>
<author confidence="0.9667665">
Tushar Khot*, Niranjan Balasubramanian+, Eric Gribkoff$,
Ashish Sabharwal*, Peter Clark*, Oren Etzioni*
</author>
<affiliation confidence="0.955218">
*Allen Institute for AI, +Stony Brook University, $University of Washington
</affiliation>
<email confidence="0.9752425">
{tushark,ashishs,peterc,orene}@allenai.org, niranjan@cs.stonybrook.edu,
eagribko@cs.washington.edu
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971580645161">
Elementary-level science exams pose sig-
nificant knowledge acquisition and rea-
soning challenges for automatic question
answering. We develop a system that rea-
sons with knowledge derived from text-
books, represented in a subset of first-
order logic. Automatic extraction, while
scalable, often results in knowledge that
is incomplete and noisy, motivating use of
reasoning mechanisms that handle uncer-
tainty.
Markov Logic Networks (MLNs) seem a
natural model for expressing such knowl-
edge, but the exact way of leveraging
MLNs is by no means obvious. We in-
vestigate three ways of applying MLNs to
our task. First, we simply use the extracted
science rules directly as MLN clauses and
exploit the structure present in hard con-
straints to improve tractability. Second,
we interpret science rules as describing
prototypical entities, resulting in a drasti-
cally simplified but brittle network. Our
third approach, called Praline, uses MLNs
to align lexical elements as well as define
and control how inference should be per-
formed in this task. Praline demonstrates
a 15% accuracy boost and a 10x reduction
in runtime as compared to other MLN-
based methods, and comparable accuracy
to word-based baseline approaches.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926063829787">
We consider the problem of answering questions
in standardized science exams (Clark et al., 2013),
which are used as a benchmark for developing
knowledge acquisition and reasoning capabilities.
The 4th grade science exam dataset from Clark et
al. (2013) tests for a wide variety of knowledge
and its application to specific scenarios. In partic-
ular we focus on a subset that test students’ un-
derstanding of various kinds of general rules and
principles (e.g., gravity pulls objects towards the
Earth) and their ability to apply these rules to
reason about specific situations or scenarios (e.g.,
which force is responsible for a ball to drop?).
Answering these questions can be naturally for-
mulated as a reasoning task given the appropri-
ate form of knowledge. Prior work on reasoning
based approaches has largely relied on manually
input knowledge (Lenat, 1995). We present an in-
vestigation of a reasoning approach that operates
over knowledge automatically extracted from text.
In order to effectively reason over knowledge
derived from text, a QA system must handle in-
complete and potentially noisy knowledge, and
allow for reasoning under uncertainty. We cast
QA as a reasoning problem in weighted-first order
logic. While many probabilistic formalisms ex-
ist, we use Markov Logic Networks for the ease of
specification of weighted rules. MLNs have been
adopted for many NLP tasks (Singla and Domin-
gos, 2006a; Kok and Domingos, 2008; Poon and
Domingos, 2009). Recently, Beltagy et al. (2013)
and Beltagy and Mooney (2014) have shown that
MLNs can be used to reason with rules derived
from natural language. While MLNs appear to be
a natural fit, it is a priori unclear how to effectively
formulate the QA task as an MLN problem. We
find that unique characteristics of this domain pose
new challenges in efficient inference. Moreover, it
is unclear how MLNs might perform on automat-
ically extracted noisy rules and how they would
fare against simpler baselines that do not rely as
much on structured logical representations.
Our goal is to build a high accuracy reasoning-
based QA system that can answer a question in
near real time. To this end, we investigate three
MLN-based formulations: (1) A natural formula-
tion that is intuitive but suffers from inefficient in-
</bodyText>
<page confidence="0.984019">
685
</page>
<note confidence="0.9850885">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 685–694,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999857636363636">
ference (e.g., over 10 minutes on 31% of the ques-
tions); (2) an extension that improves efficiency by
using prototypical constants, but is brittle to vari-
ation in structure; and (3) a formulation with im-
proved flexibility to handle variation in text and
structure that is 15% more accurate and 10x faster
than the other approaches.
Despite significant improvements over two nat-
ural MLN formulations, the best reasoning-based
configuration still does not outperform a simpler
word-based baseline. We surmise that without ef-
fective salience models on text-derived rules, rea-
soning is unable to leverage the systematic ad-
vantages of the MLN-based models. The im-
proved flexibility in the MLN-based models es-
sentially appears to approximate word-based ap-
proaches due to the noisy and incomplete nature of
the input knowledge. Nevertheless, the reasoning
based method shows improved performance when
adding multiple rules, which provides a principled
way to inject additional knowledge and control in-
ference for further improvements.
</bodyText>
<sectionHeader confidence="0.936897" genericHeader="method">
2 Background: QA Task
</sectionHeader>
<bodyText confidence="0.995421428571429">
Following Clark et al. (2014), we formulate QA
as a reasoning task over knowledge derived from
textual sources. A multiple choice question with
k answer options is turned into k true-false ques-
tions, each of which asserts some known facts (the
setup) and posits a query. The reasoning task is
to determine whether the query is true given the
setup and the input knowledge.
The input knowledge is derived from 4th-grade
science texts and augmented with a web search for
terms appearing in the texts. Much of this knowl-
edge is in terms of generalities, expressed natu-
rally as IF-THEN rules. We use the representation
and extraction procedures of Clark et al. (2014),
recapitulated briefly here for completeness.
Rule Representation: The generalities in text
convey information about classes of entities and
events. Following the neo-davidsonian reified rep-
resentation (Curran et al., 2007), we encode infor-
mation about events (e.g, falling) and entities (e.g.,
ball or stone) using variables. Predicates such
as agent, cause, function, towards, and in define
semantic relationships between variables. Rather
than committing to a type ontology, the variables
are associated with their original string represen-
tation using an isa predicate.
The “if” or antecedent part of the rule is se-
mantically interpreted as being universally quanti-
fied (omitted below for conciseness) whereas ev-
ery entity or event mentioned only in the “then”
or consequent part of the rule is treated as
existentially quantified. Both antecedent and
consequent are interpreted as conjunctions. For
example, “Growing thicker fur in winter helps
some animals to stay warm” translates into:
</bodyText>
<construct confidence="0.9368324">
isa(g, grow), isa(a, some animals),
isa(f, thicker fur), isa(w, the winter),
agent(g, a), object(g, f), in(g, w)
⇒ ∃s, r : isa(s, stays), isa(r, warm),
enables(g, s), agent(s, a), object(s, r) (1)
</construct>
<bodyText confidence="0.879794333333333">
Question Representation: The question repre-
sentation is computed similarly except that we use
fixed constants (represented as block letters) rather
than variables. For example, consider the ques-
tion: “A fox grows thick fur as the season changes.
This helps the fox to (A) hide from danger (B) at-
tract a mate (C) find food (D) keep warm?” The
T/F question corresponding to option (D) trans-
lates into:
</bodyText>
<construct confidence="0.94931775">
setup :isa(F, fox), isa(G, grows), isa(T,
thick fur), agent(G, F), object(G, T)
query :isa(K, keep warm), enables(G, K),
agent(K, F)
</construct>
<bodyText confidence="0.985910888888889">
Lexical Reasoning: Since entity and event vari-
ables hold textual values, reasoning must accom-
modate lexical variability and textual entailment.
For example, the surface forms “thick fur” and
“thicker fur” are semantically equivalent. Also,
the string “fox” entails “some animal”. We use a
lexical reasoning component based on textual en-
tailment to establish lexical equivalence or entail-
ment between variables.
</bodyText>
<listItem confidence="0.562878818181818">
Most Likely Answer as Inference: Given KB
rules and the question as input, we formulate a
probabilistic reasoning problem by adding lexical
reasoning probabilities and incorporating uncer-
tainties in derived rules. Given setup facts 5 and k
answer options Qi, we seek the most likely answer
option: arg maxi∈{1,...,k} Pr[Qi  |5, KB]. This is
a Partial MAP computation which is known to be
#P-hard (Park, 2002). Hence methods such as In-
teger Linear Programming are not directly appli-
cable.
</listItem>
<page confidence="0.997361">
686
</page>
<subsectionHeader confidence="0.906398">
2.1 Challenges
</subsectionHeader>
<bodyText confidence="0.999915">
Reasoning with text-derived knowledge presents
challenges that expose the brittleness and rigidity
inherent in pure logic-based frameworks. Text-
derived rules are incomplete and include lexical
items as logical elements, making rule application
in a pure logical setting extremely brittle: Many
relevant rules cannot be applied because their pre-
conditions are not fully satisfied due to poor align-
ment. For example, naive matching of rule (1)
with the facts in the setup would not conclude
the query since the rule requires “in the winter”
to be true. A robust inference mechanism must
allow for rule application with partial evidence.
Further, a single text-derived rule may be insuf-
ficient to answer a question. For example, “An-
imals grow thick fur in winter” and “Thick fur
helps keep warm” may need to be chained.
</bodyText>
<sectionHeader confidence="0.993252" genericHeader="method">
3 Probabilistic Formulations
</sectionHeader>
<bodyText confidence="0.995871590909091">
Statistical Relational Learning (SRL) mod-
els (Getoor and Taskar, 2007) are a natural fit for
QA reasoning. They provide probabilistic seman-
tics over knowledge in first-order logic, thereby
handling uncertainty in lexical reasoning and
incomplete matching. While there are many SRL
formalisms including Stochastic Logic Programs
(SLPs) (Muggleton, 1996), ProbLog (Raedt et
al., 2007), and PRISM (Sato and Kameya, 2001),
we use Markov Logic Networks (MLNs) for
their ease of specification and ability to naturally
handle potentially cyclic rules.
Markov Logic Networks (MLNs) are rela-
tional models represented using weighted first-
order logic rules. The rules provide a template
for generating a Markov network by grounding the
variables to all the constants in the rules. Each rule
fi forms a clique in the ground network and its
weight wi determines the potential for the clique.
Since all cliques generated by grounding the same
clause have the same weight, the probability of a
given assignment is calculated as:
</bodyText>
<equation confidence="0.982697">
1 � P(X = x) = Z exp
i
</equation>
<bodyText confidence="0.999510666666667">
where ni(x) is the number of times the ith for-
mula is satisfied by the world x and Z is a normal-
ization constant. Intuitively, a rule with a positive
weight is more likely to be true than false.Higher
the weight of a rule, more likely it is to be true.
We explore three MLN formulations:
</bodyText>
<listItem confidence="0.763521342857143">
a) First-order MLN: Given a question and rel-
evant first-order KB rules, we convert them into an
MLN program and let MLN inference automati-
cally handle rule chaining. While a natural first-
order formulation of the QA task, this struggles
with long conjunctions and existentials in rules, as
well as relatively few atoms and little to no sym-
metries. This results in massive grounding sizes,
not remedied easily by existing solutions such as
lazy, lifted, or structured inference. We exploit
the structure imposed by hard constraints to vastly
simplify groundings and bring them to the realm
of feasibility, but performance remains poor.
b) Entity Resolution MLN: Instead of reason-
ing with rules that express generalities over classes
of individuals, we replace the variables in the
previous formulation with prototypical constants.
This reduces the number of groundings, while re-
taining the crux of the reasoning problem defined
over generalities. Combining this idea with exist-
ing entity resolution approaches substantially im-
proves scalability. However, this turns out to be
too brittle in handling lexical mismatches, espe-
cially in the presence of differences in parse struc-
tures
c) Praline MLN: Both of the above MLNs rely
on exactly matching the relations in the KB and
question representation, making them too sensitive
to syntactic differences. In response, PRobabilis-
tic ALignment and INferencE (Praline) performs
inference using primarily the string constants but
guided by the edge or relational structure. We
relax the rigidity in rule application by explicitly
modeling the desired QA inference behavior in-
stead of relying on MLN’s semantics.
</listItem>
<subsectionHeader confidence="0.9951">
3.1 First-Order MLN Formulation
</subsectionHeader>
<bodyText confidence="0.999982384615385">
For a set R of first-order KB rules, arguably
the most natural way to represent the QA task
of computing Pr[Qi I S, R] as an MLN pro-
gram M is to simply add R essentially verba-
tim as first-order rules in M. For all existen-
tially quantified variables, we introduce a new do-
main constant. Predicates of M are those in R,
along with a binary entails predicate represent-
ing the lexical entailment blackbox, which allows
M to probabilistically connect lexically related
constants such as “thick fur” and “thicker fur” or
“fox” and “some animals”. entails is defined to
be closed-world and is not necessarily transitive.
</bodyText>
<equation confidence="0.817309">
�wini(x)
</equation>
<page confidence="0.975814">
687
</page>
<bodyText confidence="0.999411666666667">
Evidence: Soft evidence for M consists
of entails relations between every or-
dered pair of entity (or event) strings, e.g.,
entails(fox, some animals). Hard evidence for
M comprises grounded atoms in S.
Query: The query atom in M is result(), anew
zero-arity predicate result() that is made equiva-
lent to the conjunction of the predicates in Qi that
have not been included in the evidence. We are
interested in computing Pr[result() = true].
Semantic Rules: In addition to KB science
rules, we add semantic rules that capture the in-
tended meaning of our predicates, such as ev-
ery event has a unique agent, cause(x, y) —*
effect(y, x), etc. Semantic predicates also en-
force natural restrictions such as non-reflexivity,
!r(x, x), and anti-symmetry, r(x, y) —*!r(y, x).
Finally, to help bridge lexical gaps more, we
use a simple external lexical alignment algo-
rithm to estimate how much does the setup en-
tail antecedentr of each KB rule r, and how much
does consequentr entail query. These are then
added as two additional MLN rules per KB rule.
Our rules have a specific first-order logic form:
</bodyText>
<equation confidence="0.9443065">
Vx1, .., xk / \ Ri(xi1, xi2)
i
—* ]xk+1, .., xk+m / \ Rj(xj1, xj2)
j
</equation>
<bodyText confidence="0.999832111111111">
Existentials spanning conjunctions in the conse-
quent of this rule form can neither be directly fed
into existing MLN systems nor naively translated
into a standard form without incurring an expo-
nential blowup. We introduce a new “existential”
predicate Eα j (x1, ... , xk, xk+j) for each existen-
tial variable xk+j in each such rule α. This predi-
cate becomes the consequent of α, and hard MLN
rules make it equivant to the original consequent.
</bodyText>
<subsectionHeader confidence="0.856921">
3.1.1 Boosting Inference Efficiency.
</subsectionHeader>
<bodyText confidence="0.999938105263158">
A bottleneck in using MLN solvers out-of-the-box
for this QA formulation is the prohibitively large
grounded network size. For example, 34 out of
108 runs timed out during MLN grounding phase
after 6 minutes. On average, the ground networks
in these runs were of the order of 1.4x106 ground
clauses. Such behavior has also been observed,
perhaps to a lesser degree, in related NLP tasks
such as RTE (Beltagy and Mooney, 2014) and STS
(Beltagy et al., 2014).
Existing techniques address large grounding
size by focusing on relevant atoms (Singla and
Domingos, 2006b; Shavlik and Natarajan, 2009)
or grouping atoms into large classes of inter-
changeable atoms (de Salvo Braz et al., 2005;
Gogate and Domingos, 2011; Venugopal and
Gogate, 2012). Our QA encoding has very few
atoms (often under 500) but very long clauses and
highly asymmetric structure. This makes existing
methods ineffective. For example, lazy inference
in Alchemy-11 reduced —70K ground clauses to
—56K on a question, while our method, described
next, brought it down to only 951 clauses. Fur-
ther, Lifted Blocked Gibbs and Probabilistic Theo-
rem Proving, as implemented in Alchemy-2, were
slower than basic Alchemy-1.
We utilize the combinatorial structure imposed
by the set H of hard constraints (e.g., semantic
rules, definition style rules, some science rules)
present in the MLN, and use it to simplify the
grounding of both hard and soft constraints. Im-
portantly, this does not alter the first-order MLN
semantics. The approach thus embraces hard
clauses rather than relaxing them, as is often done
in probabilistic inference techniques, especially
when avoiding infinite energy barriers in MCMC
based methods. Assuming an arbitrary constraint
ordering in H, let Fi denote the first i constraints.
Starting with i = 1, we generate the propositional
grounding Gi of Fi, use a propositional satisfia-
bility (SAT) solver to identify the set Bi of back-
bone variables of Gi (i.e., variables that take a
fixed value in all solutions to Gi), freeze values of
the corresponding atoms in Bi, increment i, and
repeat until GJH1 has been processed. Although
the end result can be described simply as freez-
ing atoms corresponding to the backbone variables
in the grounding of H, the incremental process
helps us control the intermediate grounding size
as a propositional variable is no longer generated
for a frozen atom. Once the freezing process is
complete, the full grounding of H is further sim-
plified by removing frozen variables. Finally, the
soft constraints S are grounded much more effi-
ciently by taking frozen atoms into account. Our
approach may also be seen as an extension of a
proposal by Papai et al. (2011).
</bodyText>
<footnote confidence="0.990403">
1http://alchemy.cs.washington.edu
</footnote>
<page confidence="0.981502">
688
</page>
<subsectionHeader confidence="0.983293">
3.2 Entity Resolution Based MLN
</subsectionHeader>
<bodyText confidence="0.999800086956522">
Representing generalities as quantified rules de-
fined over classes of entities or events appears to
be a natural formulation, but is also quite inef-
ficient leading to large grounded networks. De-
spite the drastically reduced number of groundings
by our inference approach, the first-order MLN
formulation still timed out on 31% of the ques-
tions. Hence we consider an alternative formula-
tion that treats generalities as relations expressed
over prototypical entities and events. This formu-
lation leverages the fact that elementary level sci-
ence questions can often be answered using rela-
tively simple logical reasoning over exemplar ob-
jects and homogeneous classes of objects. The
only uncertainty present in our system is what’s
introduced by lexical variations and extraction er-
rors, which we handle with probabilistic equality.
KB Rules and Question: We define rules over
prototypical entity/event constants, rather than
first-order variables. These constants are tied to
their respective string representations, with the un-
derstanding that two entities behave similarly if
they have lexically similar strings. For example,
</bodyText>
<construct confidence="0.733361">
agent(Grow, Animals), object(Grow, Fur) ⇒
enables(Grow, StayWarm)
</construct>
<bodyText confidence="0.999951333333333">
What was a first-order rule in FO-MLN is now al-
ready fully grounded! Entities/events in the ques-
tion are also similarly represented by constants.
Note that the efficiency boost using hard con-
straints (Section 3.1.1) is orthogonal to using pro-
totypical constants and can be applied here as well.
Equivalence or Resolution Rules: Using a sim-
ple probabilistic variant of existing Entity/Event
Resolution frameworks (Singla and Domingos,
2006a; Kok and Domingos, 2008), we ensure that
(a) two entities/events are considered similar when
they are tied to lexically similar strings and (b)
similar entities/events participate in similar rela-
tions w.r.t. other entities/events. This defines soft
clusters or equivalence classes of entities/events.
We use a probabilistic sameAs predicate which is
reflexive, symmetric, and transitive, and interacts
with the rest of the MLN as follows:
</bodyText>
<construct confidence="0.89201775">
isa(x, s), entails(s, s&apos;) → isa(x, s&apos;).
isa(x, s), isa(y, s) → sameAs(x, y).
w : isa(x, s), !isa(y, s) → !sameAs(x, y)
r(x, y), sameAs(y, z) → r(x, z).
</construct>
<bodyText confidence="0.999926666666667">
r in the last rule refers to any of the MLN pred-
icates other than entails and isa. The sameAs
predicate, as before, is implemented in a typed
fashion, separately for entities and events. We will
refer to this formulation as ER-MLN.
Partial Match Rules: Due to lexical variability,
often not all conjuncts in a rule’s antecedent are
present in the question’s setup. To handle incom-
plete matches, for each KB derived MLN rule of
the form (∧kZ=,LZ) → R, we also add k soft rules
of the form LZ → R. This adds flexibility, by help-
ing “fire” the rule in a soft manner. This differs
from FO-MLN which uses an external alignment
system to find parts of the antecedent mentioned
in the setup, L&apos; and creates one rule L&apos; ⇒ R.
Comparison with FO-MLN: Long KB rules
and question representation now no longer have
quantified variables, only the binary or ternary
rules above do. These mention at most 3 variables
each and thus have relatively manageable ground-
ings. On the other hand, as discussed in the next
section, ER-MLN can fail on questions that have
distinct entities with similar string representations
(e.g. two distinct plants in a question would map
to the same entity). Further, it fails to apply valid
rules in the presence of syntactic differences such
as agent(Fall, Things) generated by “things fall
due to gravity” and object(Dropped, Ball) for “a
student dropped a ball”. Although “drop” entails
“fall” and “ball” entails “object”, ER-MLN cannot
reliably bridge the structural difference involving
object and agent, as these two relationships typi-
cally aren’t equivalent. Despite these limitations,
ER-MLN provides a substantial scalability advan-
tage over FO-MLN on a vast majority of the ques-
tions that remain within its scope.
</bodyText>
<subsectionHeader confidence="0.999247">
3.3 PRobabilistic ALignment and INferencE
</subsectionHeader>
<bodyText confidence="0.99979925">
ER-MLN handles some of the word-level lexi-
cal variation via resolution and soft partial match
rules that break long antecedents. However, it is
still rigid in two respects:
</bodyText>
<listItem confidence="0.569949571428571">
(1) ER-MLN primarily relies on the predicates
(also referred to as links or edges) for inference.
As a result, even if the words in the antecedent
and setup have high entailment scores, the rule
will still not “fire” if the edges do not match.
(2) As entities bound to lexically equivalent
strings are forced to “behave” identically, ER-
</listItem>
<page confidence="0.998506">
689
</page>
<bodyText confidence="0.999881807692308">
MLN fails on questions that involve two different
entities that are bound to equivalent string repre-
sentations. Consider the question: “A student puts
two identical plants in the same type and amount
of soil. She puts one of these plants near a sunny
window and the other in a dark room. This exper-
iment tests how the plants respond to (A) light (B)
air (C) water (D) soil.” The entities correspond-
ing to the two plants will be bound to equivalent
string representations and hence will be treated as
the same entity. To avoid this, we do not force the
entailment-based clusters of constants to behave
similarly. Instead, as discussed below, we use the
clusters to guide inference in a softer manner.
To introduce such flexibility, we define an MLN
to directly control how new facts are inferred given
the KB rules. The flexibility to control inference
helps address two additional QA challenges:
Acyclic inference: While knowledge is ex-
tracted from text as a set of directed rules each
with an antecedent and a consequent, there is no
guarantee that the rules taken together are acyclic.
For example, a rule stating “Living things —* de-
pend on the sun” and “Sun —* source of energy for
living things” may exist side-by-side. Successful
inference for QA must avoid feedback loops.
False unless proven: While MLNs assume
atoms not mentioned in any rule to be true with
probability 0.5, elementary level science reason-
ing is better reflected in a system that assumes all
atoms to be false unless stated in the question or
proven through the application of a rule. This is
similar to the semantics of Problog (Raedt et al.,
2007) and PRISM (Sato and Kameya, 2001).
While acyclic inference and false unless proven
can be handled by setting high negative priors in
MLNs, inference behavior is susceptible to varia-
tions in these weights. By using hard rules to con-
trol the direction of inference, we can explicitly
enforce these constraints.
We introduce a unary predicate called holds
over string constants to capture the probability of
a string constant being true given the setup is true
(bx E setup, holds(x) = true) and the KB rules
hold. Instead of using edges for inference, we
use them as factors influencing alignment: similar
constants have similar local neighborhoods. With
n string constants, this reduces the number of un-
observed groundings from O(n2) edges in the ER-
MLN to O(n) existence predicates. For the exam-
ple rule (1), Praline can be viewed as using the
following rule for inference:
</bodyText>
<equation confidence="0.7430565">
holds(Grow), holds(Animals), holds(Fur),
holds(Winter) ==&gt;. holds(Stays), holds(Warm)
</equation>
<bodyText confidence="0.9998433">
If we view KB rules and question as a labeled
graph G (Figure 1), alignment between string con-
stants corresponds to node alignment in G. The
nodes and edges of G are the input to the MLN,
and the holds predicate on each node captures the
probability of it being true given the setup. We
now use MLNs (as described below) to define the
inference procedure for any such input graph G.
Evidence: We represent the graph struc-
ture of G using predicates node(nodeid)
and edge(nodeid, nodeid, label). We use
setup(nodeid) and query(nodeid) to represent
the question’s setup and query, resp. Similarly,
we use inLhs(nodeid) and inRhs(nodeid) to
represent rules’ antecedent and consequent, resp.
Graph Alignment Rules: Similar to the previ-
ous approaches, we use entailment scores between
words and short phrases to compute the alignment.
In addition, we also expect aligned nodes to have
similar edge structures:
</bodyText>
<equation confidence="0.8615795">
aligns(x, y), edge(x, u, r),edge(y, v, s)
==&gt;. aligns(u, v)
</equation>
<bodyText confidence="0.960367888888889">
That is, if node x aligns with y then their chil-
dren/ancestors should also align. We create copies
of these rules for edges with the same label, r = s,
with a higher weight and for edges with different
labels, r =� s, with a lower weight.
Inference Rules: We use MLNs to define the
inference procedure to prove the query using the
alignments from aligns. We assume that any node
y that aligns with a node x that holds, also holds:
</bodyText>
<equation confidence="0.91888">
holds(x), aligns(x, y) ==&gt;. holds(y) (2)
</equation>
<bodyText confidence="0.999831777777778">
For example, if the setup mentions “fox”, all
nodes that entail “fox” also hold. As we also
use the edge structure during alignment, we would
have a lower probability of “fox” in “fox finds
food” to align with “animal” in “animal grows fur”
as compared to “animal” in “animal finds food”.
We use KB rules to further infer new facts
that should hold based on the rule structure. We
compute lhsHolds, the probability of the rule’s
</bodyText>
<page confidence="0.983442">
690
</page>
<figure confidence="0.9986765625">
lhs_holds rhs_holds
Living things
agent
depend
energy
on from
the Sun
earth
Life
on
agent
depends
energy
on
from
the Sun
</figure>
<figureCaption confidence="0.969556666666667">
Figure 1: KB rule and question as a graph where blue:setup; green:query; orange:antecedent;
purple:consequent; dotted lines: alignments. lhsHolds combines individual probabilities of antecedent
nodes and rhsHolds captures the probability of the consequent.
</figureCaption>
<bodyText confidence="0.98508">
antecedent holding, and use it to infer rhsHolds,
the probability of the consequent. Similar to ER-
MLN, we break the rule into multiple small rules.2
</bodyText>
<equation confidence="0.9715672">
w :holds(x), inLhs(x, r) ⇒ lhsHolds(r)
w :!holds(x), inLhs(x, r) ⇒!lhsHolds(r)
lhsHolds(r) ⇒ rhsHolds(r).
rhsHolds(r), inRhs(r, x) ⇒ holds(x).
Acyclic inference: We use two pred-
</equation>
<bodyText confidence="0.998563166666667">
icates, proves(nodeid, nodeid) and
ruleProves(rule, rule), to capture the infer-
ence chain between nodes and rules, resp. To
ensure acyclicity in inference, we add transi-
tive clauses over these predicates and disallow
reflexivity, i.e., !proves(x, x), and update rule (2):
</bodyText>
<construct confidence="0.8498505">
wp :proves(x, y), holds(x) ⇒ holds(y)
wa :aligns(x, y) ⇒ proves(x, y)
</construct>
<bodyText confidence="0.999217">
We capture inference direction between rules by
checking consequent and antecedent alignments:
</bodyText>
<equation confidence="0.9401935">
proves(x, y), inrhs(x, r1), inlhs(y, r2)
⇒ ruleProves(r1, r2).
</equation>
<bodyText confidence="0.9998799">
False unless proven: To ensure that nodes hold
only if they can be proven from setup, we add
bidirectional implications to our rules. An alter-
native is to introduce a strong negative prior on
holds and have a higher positive weight on all
other clauses that conclude holds. However, the
performance of our MLNs was very sensitive to
the choice of the weight. We instead model this
constraint explicitly. Figure 1 shows a sample in-
ference chain using dotted lines.
</bodyText>
<footnote confidence="0.722356">
2An intuitive alternative for the 2nd clause doesn’t cap-
ture the intending meaning, −w :!holds(x), inLhs(x, r) ⇒
lhsHolds(r)
</footnote>
<bodyText confidence="0.999605272727273">
Praline defines a meta-inference procedure that
is easily modifiable to enforce desired QA infer-
ence behavior, e.g. w : aligns(x, y), setup(x) ⇒
!query(y) would prevent a term from the setup to
align with the query. Further, by representing the
input KB and question as evidence, we can define
a single static first-order MLN for all the questions
instead of a compiled MLN for every question.
This opens up the possibility of learning weights
of this static MLN, which would be challenging
for the previous two approaches.3
</bodyText>
<sectionHeader confidence="0.98963" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999985842105264">
We used Tuffy 0.44 (Niu et al., 2011) as the
base MLN solver5 and extended it to incorpo-
rate the hard-constraint based grounding reduc-
tion technique discussed earlier, implemented us-
ing the SAT solver Glucose 3.06 (Audemard and
Simon, 2009) exploiting its “solving under as-
sumptions” capability for efficiency. We used a
10 minute timelimit, including a max of 6 minutes
for grounding. Marginal inference was performed
using MC-SAT (Poon and Domingos, 2006), with
default parameters and 5000 flips per sample to
generate 500 samples for marginal estimation.
We used a 2-core 2.5 GHz Amazon EC2 linux
machine with 16 GB RAM. We selected 108
elementary-level science questions (non-diagram,
multiple-choice) from 4th grade New York Re-
gents exam as our benchmark (Dev-108) and used
another 68 questions from the same source as a
blind test set (Unseen-68)7.
</bodyText>
<footnote confidence="0.9999878">
3In this work, we have set the weights manually.
4http://i.stanford.edu/hazy/tuffy
5Alchemy 1.0 gave similar results.
6http://www.labri.fr/perso/lsimon/glucose
7http://allenai.org/content/data/Ariscienceexams.txt
</footnote>
<page confidence="0.990393">
691
</page>
<table confidence="0.999818125">
Question MLN #Answered Exam #MLN #Atoms #Ground Runtime
Set Formulation (some / all) Score Rules Clauses (all)
FO-MLN 106 / 82 33.6% 35 384∗ 524∗ 280 s
Dev-108 ER-MLN 107 / 107 34.5% 41 284 2,308 188 s
PRALINE 108 48.8% 51 182 219 17 s
FO-MLN 66 33.8% - - - 288 s
Unseen-68 ER-MLN 68 31.3% - - - 226 s
PRALINE 68 46.3% - - - 17 s
</table>
<tableCaption confidence="0.999775">
Table 1: QA performance of various MLN formulations. #MLN-Rules, #GroundClauses, and Runtime
</tableCaption>
<bodyText confidence="0.9738325">
per multiple-choice question are averaged over the corresponding dataset. #Answered column indicates
questions where at least one answer option didn’t time out (left) and where no answer option timed out
(right). Of the 432 Dev MLNs (108 × 4), #Atoms and #GroundClauses for FO-MLN are averaged over
the 398 MLNs where grounding finished; 34 remaining MLNs timed out after processing 1.4M clauses.
The KB, representing roughly 47,000 sen-
tences, was generated in advance by processing
the New York Regents 4th grade science exam
syllabus, the corresponding Barron’s study guide,
and documents obtained by querying the Inter-
net for relevant terms. Given a question, we
use a simple word-overlap based matching algo-
rithm, referred to as the rule selector, to retrieve
the top 30 matching sentences to be considered
for the question. Textual entailment scores be-
tween words and short phrases were computed us-
ing WordNet (Miller, 1995), and converted to “de-
sired” probabilities for soft entails evidence. The
accuracy reported for each approach is computed
as the number of multiple-choice questions it an-
swers correctly, with a partial credit of 1/k in case
of a k-way tie between the highest scoring options
if they include the correct answer.
</bodyText>
<subsectionHeader confidence="0.983917">
4.1 MLN Formulation Comparison
</subsectionHeader>
<bodyText confidence="0.999838351351351">
Table 1 compares the effectiveness of our three
MLN formulations: FO-MLN, ER-MLN, and Pra-
line. For each question and approach, we generate
an MLN program for each answer option using the
most promising KB rule for that answer option.
In the case of FO-MLN, Tuffy exceeded the 6
minute time limit when generating groundings for
34 of the 108 × 4 MLNs for the Dev-108 question
set, quitting after working with 1.4 × 106 clauses
on average, despite starting with only around 35
first-order MLN rules. In the remaining MLNs,
where our clause reduction technique successfully
finished, the ground network size reduced dramat-
ically to 524 clauses and 384 atoms on average.
Tuffy finished inference for all 4 answer options
for 82 of the 108 questions; for other questions, it
chose the most promising answer option among
the ones it finished processing. Overall, this re-
sulted in a score of 33.6% with an average of 280
seconds per multiple-choice question on Dev-108,
and similar performance on Unseen-68.
ER-MLN, as expected, did not result in any
timeouts during grounding. The number of ground
clauses here, 2,308 on average, is dominated not
by KB rules but by the binary and ternary en-
tity resolution clauses involving the sameAs pred-
icate. ER-MLN was roughly 33% faster than FO-
MLN, but overall achieved similar exam scores.
Praline resulted in a 10x speedup over ER-
MLN, explained in part by much smaller ground
networks with only 219 clauses on average. It
boosted exam performance by roughly 15%, push-
ing it up to 48.8% on Dev-108 and 46.3% on
Unseen-68 (statistically significantly better than
FO-MLN with p-value &lt; 0.05). This demonstrates
the value that the added flexibility and control Pra-
line brings.
</bodyText>
<subsectionHeader confidence="0.990504">
4.2 Praline: Improvements and Ablation
</subsectionHeader>
<bodyText confidence="0.999156764705882">
We evaluate Praline when using multiple KB rules
as a chain or multiple inference paths. Simply
using the top two rules for inference turns out to
be ineffective as they are often very similar. In-
stead, we use incremental inference where we add
one rule, perform inference to determine which
additional facts now hold and which setup facts
haven’t yet been used, and then use this infor-
mation to select the next best rule. This, as the
Chain=2 entries in the first row of Table 2 show,
improves Praline’s accuracy on both datasets. The
improvement comes at the cost of a modest run-
time increase from 17 seconds per question to 38.
Finally, we evaluate the impact of Praline’s ad-
ditional rules to handle acyclicity (Acyclic) and
the false unless proven (FUP) constraint. As Table
2 shows, Praline’s accuracy drops upon removing
</bodyText>
<page confidence="0.993451">
692
</page>
<table confidence="0.999708333333333">
MLN One rule Chain=2
Dev-108 Unseen Dev-108 Unseen
Praline 48.8% 46.3% 50.3% 52.7%
-Acyclic 44.7% 36.0% 43.6% 30.9%
-FUP 35.0% 30.9% 42.1% 29.4%
-FUP -Acyclic 37.3% 34.2% 36.6% 24.3%
</table>
<tableCaption confidence="0.9793085">
Table 2: QA performance of Praline MLN
variations.
</tableCaption>
<table confidence="0.998215666666667">
Dev-108 Unseen-68 Dev-170 Unseen-176
Praline 50.3% 52.7% 33.2% 36.6%
Word-based 57.4% 51.5% 40.3% 43.3%
</table>
<tableCaption confidence="0.999897">
Table 3: QA performance: Praline vs. word-based.
</tableCaption>
<bodyText confidence="0.9989264">
either of these constraints, highlighting their im-
portance. Specifically, when using only one KB
rule, dropping FUP clauses has a bigger influence
that dropping Acyclic constraint clauses. With a
single rule, there is still a possibility of cyclic in-
ference within a rule, leading to a small drop in
score there as well. When chaining multiple rules,
however, the possibility of incorrect cyclic infer-
ence is higher and we see a correspondingly larger
drop in score when dropping Acyclic constraints.
</bodyText>
<subsectionHeader confidence="0.998614">
4.3 Comparison to baseline approaches
</subsectionHeader>
<bodyText confidence="0.999972777777778">
Table 3 compares Praline to a baseline word-based
method on two question sets. The new set here is
from 4th and 5th grade, with 170 Dev and 176 un-
seen questions. The word-based approach calcu-
lates the entailment score, using the same methods
as for the soft entails evidence earlier, between
the words in the T/F question and words in a rule
in the KB. It then uses the maximum entailment
score from all selected rules as the confidence
measure i.e. maxr∈R entailment(q, r). While the
scores of the two approaches are statistically sim-
ilar (p-value &gt; 0.05), the simple word-based ap-
proach does have a slight edge over Praline. Au-
tomatic extraction of knowledge from text pro-
vides additional information (e.g., rule structure)
that MLNs are capable of exploiting. However,
we found this additional flexibility to not pay off
with the current knowledge-base and questions.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.994261305555555">
Reasoning with automatically extracted knowl-
edge presents significant challenges. Our
investigation of MLN-based formulations for
elementary-level science exams highlights two
key issues: 1) Natural translations of text de-
rived knowledge into first-order representations
are highly inefficient, resulting in large ground
networks. 2) When the logical elements in the
rules largely mirror the constructs in the source
text, reasoning is hampered because of structural
variability. In response, we proposed, Praline, an
alignment based solution that is both efficient and
accurate. Praline reasons with prototypical con-
stants, and provides greater flexibility in how in-
ference is performed and is therefore more robust
to structural mismatches.
MLNs provided a flexible, structured frame-
work to define inference for the QA task, while
also providing reasoning chains used to arrive at
an answer. While models such as MLNs seem
a perfect fit for textual reasoning tasks such as
RTE and QA, their performance on these tasks is
still not up to par with textual feature-based ap-
proaches (Beltagy and Mooney, 2014). We con-
jecture that the increased flexibility of complex
relational models results in increased susceptibil-
ity to noisy input, and the systematic advantages
of MLN models are difficult to exploit with text-
derived rules. Automatically learning weights of
these models may allow leveraging their flexibil-
ity to address these issues, but weight learning re-
mains challenging with only distant supervision.
We hope our datasets, knowledge bases, and
MLN models8 will help push NLP and SRL com-
munities towards designing improved structured
reasoning QA systems.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999235">
The authors would like thank Pedro Domingos
and Dan Weld for invaluable discussions and the
Aristo team at AI2, especially Jesse Kinkead, for
help with prototype development and evaluation.
</bodyText>
<sectionHeader confidence="0.998927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9919403">
Gilles Audemard and Laurent Simon. 2009. Predict-
ing learnt clauses quality in modern SAT solvers. In
21st IJCAI, pages 399–404, Pasadena, CA, July.
Islam Beltagy and Raymond J Mooney. 2014. Effi-
cient Markov logic inference for natural language
semantics. Quebec City, Canada, July.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. 2nd Joint Conference on
</reference>
<footnote confidence="0.978151">
8Available at http://allenai.org/software.html
</footnote>
<page confidence="0.997277">
693
</page>
<reference confidence="0.999810950617284">
Lexical and Computational Semantics: Proceeding
of the Main Conference and the Shared Task, At-
lanta, pages 11–21.
Islam Beltagy, Katrin Erk, and Raymond J. Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In 52nd ACL, pages 1210–1219, Balti-
more, MD, June.
Peter Clark, Phil Harrison, and Niranjan Balasubrama-
nian. 2013. A study of the AKBC/requirements for
passing an elementary science test. In Proc. of the
AKBC-WEKEX workshop at CIKM.
Peter Clark, Niranjan Balasubramanian, Sum-
ithra Bhakthavatsalam, Kevin Humphreys, Jesse
Kinkead, Ashish Sabharwal, and Oyvind Tafjord.
2014. Automatic construction of inference-
supporting knowledge bases. In 4th Workshop on
Automated Knowledge Base Construction (AKBC),
Montreal, Canada, December.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&amp;C
and Boxer. In 45th ACL, pages 33–36, Prague,
Czech Republic, June.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth.
2005. Lifted first-order probabilistic inference. In
19th IJCAI, pages 1319–1325.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press.
Vibhav Gogate and Pedro Domingos. 2011.
Probabilistic theorem proving. pages 256–265,
Barcelona, Spain, July.
Stanley Kok and Pedro Domingos. 2008. Extracting
semantic networks from text via relational cluster-
ing. In 19th ECML, pages 624–639, Antwerp, Bel-
gium, September.
Douglas Lenat. 1995. Cyc: A large-scale invest-
ment in knowledge infrastructure. Communications
ACM, 38(11):33–38.
George Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.
Stephen Muggleton. 1996. Stochastic logic programs.
In Advances in Inductive Logic Programming.
Feng Niu, Christopher R´e, AnHai Doan, and Jude W.
Shavlik. 2011. Tuffy: Scaling up statistical infer-
ence in Markov Logic Networks using an RDBMS.
In 37th VLDB, pages 373–384.
Tivadar Papai, Parag Singla, and Henry Kautz. 2011.
Constraint propagation for efficient inference in
Markov logic. In 17th CP, pages 691–705. Springer,
Perugia, Italy.
James Park. 2002. MAP complexity results and ap-
proximation methods. pages 388–396, Edmonton,
Canada, August.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determin-
istic dependencies. In 21st AAAI, pages 458–463,
Boston, MA.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1–10.
Luc De Raedt, Angelika Kimmig, and Hannu Toivo-
nen. 2007. Problog: A probabilistic Prolog and its
application in link discovery. In International Joint
Conference on Artificial Intelligence.
Taisuke Sato and Yoshitaka Kameya. 2001. Parameter
learning of logic programs for symbolic-statistical
modeling. Journal of Artificial Intelligence Re-
search, pages 391–454.
Jude Shavlik and Sriraam Natarajan. 2009. Speed-
ing up inference in Markov logic networks by
preprocessing to reduce the size of the resulting
grounded network. In 21st IJCAI, pages 1951–1956,
Pasadena, CA.
Parag Singla and Pedro Domingos. 2006a. Entity reso-
lution with Markov logic. In 6th ICDM, pages 572–
582, Hong Kong, China, December.
Parag Singla and Pedro Domingos. 2006b. Memory-
efficient inference in relational domains. In 21st
AAAI, pages 488–493, Boston, MA.
Deepak Venugopal and Vibhav Gogate. 2012. On
lifting the gibbs sampling algorithm. pages 1664–
1672, Lake Tahoe, NV, December.
</reference>
<page confidence="0.998605">
694
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635938">
<title confidence="0.999947">Exploring Markov Logic Networks for Question Answering</title>
<author confidence="0.829651">Niranjan Eric Peter Oren</author>
<affiliation confidence="0.993073">Institute for AI, Brook University, of</affiliation>
<email confidence="0.999184">eagribko@cs.washington.edu</email>
<abstract confidence="0.999069">Elementary-level science exams pose significant knowledge acquisition and reasoning challenges for automatic question answering. We develop a system that reasons with knowledge derived from textbooks, represented in a subset of firstorder logic. Automatic extraction, while scalable, often results in knowledge that is incomplete and noisy, motivating use of reasoning mechanisms that handle uncertainty. Markov Logic Networks (MLNs) seem a natural model for expressing such knowledge, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. First, we simply use the extracted science rules directly as MLN clauses and exploit the structure present in hard constraints to improve tractability. Second, we interpret science rules as describing prototypical entities, resulting in a drastically simplified but brittle network. Our third approach, called Praline, uses MLNs to align lexical elements as well as define and control how inference should be performed in this task. Praline demonstrates a 15% accuracy boost and a 10x reduction in runtime as compared to other MLNbased methods, and comparable accuracy to word-based baseline approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gilles Audemard</author>
<author>Laurent Simon</author>
</authors>
<title>Predicting learnt clauses quality in modern SAT solvers.</title>
<date>2009</date>
<booktitle>In 21st IJCAI,</booktitle>
<pages>399--404</pages>
<location>Pasadena, CA,</location>
<contexts>
<context position="28807" citStr="Audemard and Simon, 2009" startWordPosition="4628" endWordPosition="4631">event a term from the setup to align with the query. Further, by representing the input KB and question as evidence, we can define a single static first-order MLN for all the questions instead of a compiled MLN for every question. This opens up the possibility of learning weights of this static MLN, which would be challenging for the previous two approaches.3 4 Empirical Evaluation We used Tuffy 0.44 (Niu et al., 2011) as the base MLN solver5 and extended it to incorporate the hard-constraint based grounding reduction technique discussed earlier, implemented using the SAT solver Glucose 3.06 (Audemard and Simon, 2009) exploiting its “solving under assumptions” capability for efficiency. We used a 10 minute timelimit, including a max of 6 minutes for grounding. Marginal inference was performed using MC-SAT (Poon and Domingos, 2006), with default parameters and 5000 flips per sample to generate 500 samples for marginal estimation. We used a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. We selected 108 elementary-level science questions (non-diagram, multiple-choice) from 4th grade New York Regents exam as our benchmark (Dev-108) and used another 68 questions from the same source as a blind test set</context>
</contexts>
<marker>Audemard, Simon, 2009</marker>
<rawString>Gilles Audemard and Laurent Simon. 2009. Predicting learnt clauses quality in modern SAT solvers. In 21st IJCAI, pages 399–404, Pasadena, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Raymond J Mooney</author>
</authors>
<title>Efficient Markov logic inference for natural language semantics.</title>
<date>2014</date>
<location>Quebec City, Canada,</location>
<contexts>
<context position="3088" citStr="Beltagy and Mooney (2014)" startWordPosition="463" endWordPosition="466">asoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations. Our goal is to build a high accuracy reasoningbased QA system that can answer a question in near </context>
<context position="14932" citStr="Beltagy and Mooney, 2014" startWordPosition="2375" endWordPosition="2378">or each existential variable xk+j in each such rule α. This predicate becomes the consequent of α, and hard MLN rules make it equivant to the original consequent. 3.1.1 Boosting Inference Efficiency. A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought</context>
<context position="36830" citStr="Beltagy and Mooney, 2014" startWordPosition="5919" endWordPosition="5922"> response, we proposed, Praline, an alignment based solution that is both efficient and accurate. Praline reasons with prototypical constants, and provides greater flexibility in how inference is performed and is therefore more robust to structural mismatches. MLNs provided a flexible, structured framework to define inference for the QA task, while also providing reasoning chains used to arrive at an answer. While models such as MLNs seem a perfect fit for textual reasoning tasks such as RTE and QA, their performance on these tasks is still not up to par with textual feature-based approaches (Beltagy and Mooney, 2014). We conjecture that the increased flexibility of complex relational models results in increased susceptibility to noisy input, and the systematic advantages of MLN models are difficult to exploit with textderived rules. Automatically learning weights of these models may allow leveraging their flexibility to address these issues, but weight learning remains challenging with only distant supervision. We hope our datasets, knowledge bases, and MLN models8 will help push NLP and SRL communities towards designing improved structured reasoning QA systems. Acknowledgments The authors would like than</context>
</contexts>
<marker>Beltagy, Mooney, 2014</marker>
<rawString>Islam Beltagy and Raymond J Mooney. 2014. Efficient Markov logic inference for natural language semantics. Quebec City, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets Markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>2nd Joint Conference on Lexical and Computational Semantics: Proceeding of the Main Conference and the Shared Task,</booktitle>
<pages>11--21</pages>
<location>Atlanta,</location>
<contexts>
<context position="3058" citStr="Beltagy et al. (2013)" startWordPosition="458" endWordPosition="461">t an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations. Our goal is to build a high accuracy reasoningbased QA system that </context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets Markov: Deep semantics with probabilistic logical form. 2nd Joint Conference on Lexical and Computational Semantics: Proceeding of the Main Conference and the Shared Task, Atlanta, pages 11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond J Mooney</author>
</authors>
<title>Probabilistic soft logic for semantic textual similarity.</title>
<date>2014</date>
<booktitle>In 52nd ACL,</booktitle>
<pages>1210--1219</pages>
<location>Baltimore, MD,</location>
<contexts>
<context position="14963" citStr="Beltagy et al., 2014" startWordPosition="2381" endWordPosition="2384">n each such rule α. This predicate becomes the consequent of α, and hard MLN rules make it equivant to the original consequent. 3.1.1 Boosting Inference Efficiency. A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. F</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond J. Mooney. 2014. Probabilistic soft logic for semantic textual similarity. In 52nd ACL, pages 1210–1219, Baltimore, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
<author>Niranjan Balasubramanian</author>
</authors>
<title>A study of the AKBC/requirements for passing an elementary science test.</title>
<date>2013</date>
<booktitle>In Proc. of the AKBC-WEKEX workshop at CIKM.</booktitle>
<contexts>
<context position="1662" citStr="Clark et al., 2013" startWordPosition="235" endWordPosition="238">e present in hard constraints to improve tractability. Second, we interpret science rules as describing prototypical entities, resulting in a drastically simplified but brittle network. Our third approach, called Praline, uses MLNs to align lexical elements as well as define and control how inference should be performed in this task. Praline demonstrates a 15% accuracy boost and a 10x reduction in runtime as compared to other MLNbased methods, and comparable accuracy to word-based baseline approaches. 1 Introduction We consider the problem of answering questions in standardized science exams (Clark et al., 2013), which are used as a benchmark for developing knowledge acquisition and reasoning capabilities. The 4th grade science exam dataset from Clark et al. (2013) tests for a wide variety of knowledge and its application to specific scenarios. In particular we focus on a subset that test students’ understanding of various kinds of general rules and principles (e.g., gravity pulls objects towards the Earth) and their ability to apply these rules to reason about specific situations or scenarios (e.g., which force is responsible for a ball to drop?). Answering these questions can be naturally formulate</context>
</contexts>
<marker>Clark, Harrison, Balasubramanian, 2013</marker>
<rawString>Peter Clark, Phil Harrison, and Niranjan Balasubramanian. 2013. A study of the AKBC/requirements for passing an elementary science test. In Proc. of the AKBC-WEKEX workshop at CIKM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter Clark</author>
<author>Niranjan Balasubramanian</author>
<author>Sumithra Bhakthavatsalam</author>
<author>Kevin Humphreys</author>
<author>Jesse Kinkead</author>
</authors>
<title>Ashish Sabharwal, and Oyvind Tafjord.</title>
<marker>Clark, Balasubramanian, Bhakthavatsalam, Humphreys, Kinkead, </marker>
<rawString>Peter Clark, Niranjan Balasubramanian, Sumithra Bhakthavatsalam, Kevin Humphreys, Jesse Kinkead, Ashish Sabharwal, and Oyvind Tafjord.</rawString>
</citation>
<citation valid="true">
<title>Automatic construction of inferencesupporting knowledge bases.</title>
<date>2014</date>
<booktitle>In 4th Workshop on Automated Knowledge Base Construction (AKBC),</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="3088" citStr="(2014)" startWordPosition="466" endWordPosition="466">at operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations. Our goal is to build a high accuracy reasoningbased QA system that can answer a question in near </context>
<context position="5108" citStr="(2014)" startWordPosition="784" endWordPosition="784">r word-based baseline. We surmise that without effective salience models on text-derived rules, reasoning is unable to leverage the systematic advantages of the MLN-based models. The improved flexibility in the MLN-based models essentially appears to approximate word-based approaches due to the noisy and incomplete nature of the input knowledge. Nevertheless, the reasoning based method shows improved performance when adding multiple rules, which provides a principled way to inject additional knowledge and control inference for further improvements. 2 Background: QA Task Following Clark et al. (2014), we formulate QA as a reasoning task over knowledge derived from textual sources. A multiple choice question with k answer options is turned into k true-false questions, each of which asserts some known facts (the setup) and posits a query. The reasoning task is to determine whether the query is true given the setup and the input knowledge. The input knowledge is derived from 4th-grade science texts and augmented with a web search for terms appearing in the texts. Much of this knowledge is in terms of generalities, expressed naturally as IF-THEN rules. We use the representation and extraction</context>
</contexts>
<marker>2014</marker>
<rawString>2014. Automatic construction of inferencesupporting knowledge bases. In 4th Workshop on Automated Knowledge Base Construction (AKBC), Montreal, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<date>2007</date>
<booktitle>Linguistically motivated large-scale NLP with C&amp;C and Boxer. In 45th ACL,</booktitle>
<pages>33--36</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5966" citStr="Curran et al., 2007" startWordPosition="918" endWordPosition="921">ry. The reasoning task is to determine whether the query is true given the setup and the input knowledge. The input knowledge is derived from 4th-grade science texts and augmented with a web search for terms appearing in the texts. Much of this knowledge is in terms of generalities, expressed naturally as IF-THEN rules. We use the representation and extraction procedures of Clark et al. (2014), recapitulated briefly here for completeness. Rule Representation: The generalities in text convey information about classes of entities and events. Following the neo-davidsonian reified representation (Curran et al., 2007), we encode information about events (e.g, falling) and entities (e.g., ball or stone) using variables. Predicates such as agent, cause, function, towards, and in define semantic relationships between variables. Rather than committing to a type ontology, the variables are associated with their original string representation using an isa predicate. The “if” or antecedent part of the rule is semantically interpreted as being universally quantified (omitted below for conciseness) whereas every entity or event mentioned only in the “then” or consequent part of the rule is treated as existentially </context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In 45th ACL, pages 33–36, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Eyal Amir</author>
<author>Dan Roth</author>
</authors>
<title>Lifted first-order probabilistic inference.</title>
<date>2005</date>
<booktitle>In 19th IJCAI,</booktitle>
<pages>1319--1325</pages>
<contexts>
<context position="15191" citStr="Braz et al., 2005" startWordPosition="2416" endWordPosition="2419">ation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. Further, Lifted Blocked Gibbs and Probabilistic Theorem Proving, as implemented in Alchemy-2, were slower than basic Alchemy-1. We utilize the combinatorial structure imposed by the set H of hard constraints (e.g., semantic rules</context>
</contexts>
<marker>Braz, Amir, Roth, 2005</marker>
<rawString>Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005. Lifted first-order probabilistic inference. In 19th IJCAI, pages 1319–1325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<booktitle>Vibhav Gogate and</booktitle>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>MIT Press.</publisher>
<marker>Domingos, 2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press. Vibhav Gogate and Pedro Domingos. 2011.</rawString>
</citation>
<citation valid="true">
<title>Probabilistic theorem proving.</title>
<date></date>
<pages>256--265</pages>
<location>Barcelona, Spain,</location>
<marker></marker>
<rawString>Probabilistic theorem proving. pages 256–265, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Extracting semantic networks from text via relational clustering.</title>
<date>2008</date>
<booktitle>In 19th ECML,</booktitle>
<pages>624--639</pages>
<location>Antwerp, Belgium,</location>
<contexts>
<context position="2999" citStr="Kok and Domingos, 2008" startWordPosition="449" endWordPosition="452"> relied on manually input knowledge (Lenat, 1995). We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations. Our goal</context>
<context position="18915" citStr="Kok and Domingos, 2008" startWordPosition="2995" endWordPosition="2998">ding that two entities behave similarly if they have lexically similar strings. For example, agent(Grow, Animals), object(Grow, Fur) ⇒ enables(Grow, StayWarm) What was a first-order rule in FO-MLN is now already fully grounded! Entities/events in the question are also similarly represented by constants. Note that the efficiency boost using hard constraints (Section 3.1.1) is orthogonal to using prototypical constants and can be applied here as well. Equivalence or Resolution Rules: Using a simple probabilistic variant of existing Entity/Event Resolution frameworks (Singla and Domingos, 2006a; Kok and Domingos, 2008), we ensure that (a) two entities/events are considered similar when they are tied to lexically similar strings and (b) similar entities/events participate in similar relations w.r.t. other entities/events. This defines soft clusters or equivalence classes of entities/events. We use a probabilistic sameAs predicate which is reflexive, symmetric, and transitive, and interacts with the rest of the MLN as follows: isa(x, s), entails(s, s&apos;) → isa(x, s&apos;). isa(x, s), isa(y, s) → sameAs(x, y). w : isa(x, s), !isa(y, s) → !sameAs(x, y) r(x, y), sameAs(y, z) → r(x, z). r in the last rule refers to any </context>
</contexts>
<marker>Kok, Domingos, 2008</marker>
<rawString>Stanley Kok and Pedro Domingos. 2008. Extracting semantic networks from text via relational clustering. In 19th ECML, pages 624–639, Antwerp, Belgium, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Lenat</author>
</authors>
<title>Cyc: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2426" citStr="Lenat, 1995" startWordPosition="360" endWordPosition="361">013) tests for a wide variety of knowledge and its application to specific scenarios. In particular we focus on a subset that test students’ understanding of various kinds of general rules and principles (e.g., gravity pulls objects towards the Earth) and their ability to apply these rules to reason about specific situations or scenarios (e.g., which force is responsible for a ball to drop?). Answering these questions can be naturally formulated as a reasoning task given the appropriate form of knowledge. Prior work on reasoning based approaches has largely relied on manually input knowledge (Lenat, 1995). We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009).</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="30991" citStr="Miller, 1995" startWordPosition="4975" endWordPosition="4976">e grounding finished; 34 remaining MLNs timed out after processing 1.4M clauses. The KB, representing roughly 47,000 sentences, was generated in advance by processing the New York Regents 4th grade science exam syllabus, the corresponding Barron’s study guide, and documents obtained by querying the Internet for relevant terms. Given a question, we use a simple word-overlap based matching algorithm, referred to as the rule selector, to retrieve the top 30 matching sentences to be considered for the question. Textual entailment scores between words and short phrases were computed using WordNet (Miller, 1995), and converted to “desired” probabilities for soft entails evidence. The accuracy reported for each approach is computed as the number of multiple-choice questions it answers correctly, with a partial credit of 1/k in case of a k-way tie between the highest scoring options if they include the correct answer. 4.1 MLN Formulation Comparison Table 1 compares the effectiveness of our three MLN formulations: FO-MLN, ER-MLN, and Praline. For each question and approach, we generate an MLN program for each answer option using the most promising KB rule for that answer option. In the case of FO-MLN, T</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
</authors>
<title>Stochastic logic programs.</title>
<date>1996</date>
<booktitle>In Advances in Inductive Logic Programming.</booktitle>
<contexts>
<context position="9581" citStr="Muggleton, 1996" startWordPosition="1481" endWordPosition="1482">st allow for rule application with partial evidence. Further, a single text-derived rule may be insufficient to answer a question. For example, “Animals grow thick fur in winter” and “Thick fur helps keep warm” may need to be chained. 3 Probabilistic Formulations Statistical Relational Learning (SRL) models (Getoor and Taskar, 2007) are a natural fit for QA reasoning. They provide probabilistic semantics over knowledge in first-order logic, thereby handling uncertainty in lexical reasoning and incomplete matching. While there are many SRL formalisms including Stochastic Logic Programs (SLPs) (Muggleton, 1996), ProbLog (Raedt et al., 2007), and PRISM (Sato and Kameya, 2001), we use Markov Logic Networks (MLNs) for their ease of specification and ability to naturally handle potentially cyclic rules. Markov Logic Networks (MLNs) are relational models represented using weighted firstorder logic rules. The rules provide a template for generating a Markov network by grounding the variables to all the constants in the rules. Each rule fi forms a clique in the ground network and its weight wi determines the potential for the clique. Since all cliques generated by grounding the same clause have the same we</context>
</contexts>
<marker>Muggleton, 1996</marker>
<rawString>Stephen Muggleton. 1996. Stochastic logic programs. In Advances in Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Niu</author>
<author>Christopher R´e</author>
<author>AnHai Doan</author>
<author>Jude W Shavlik</author>
</authors>
<title>Tuffy: Scaling up statistical inference in Markov Logic Networks using an RDBMS.</title>
<date>2011</date>
<booktitle>In 37th VLDB,</booktitle>
<pages>373--384</pages>
<marker>Niu, R´e, Doan, Shavlik, 2011</marker>
<rawString>Feng Niu, Christopher R´e, AnHai Doan, and Jude W. Shavlik. 2011. Tuffy: Scaling up statistical inference in Markov Logic Networks using an RDBMS. In 37th VLDB, pages 373–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tivadar Papai</author>
<author>Parag Singla</author>
<author>Henry Kautz</author>
</authors>
<title>Constraint propagation for efficient inference in Markov logic.</title>
<date>2011</date>
<booktitle>In 17th CP,</booktitle>
<pages>691--705</pages>
<publisher>Springer,</publisher>
<location>Perugia, Italy.</location>
<contexts>
<context position="17188" citStr="Papai et al. (2011)" startWordPosition="2741" endWordPosition="2744"> i, and repeat until GJH1 has been processed. Although the end result can be described simply as freezing atoms corresponding to the backbone variables in the grounding of H, the incremental process helps us control the intermediate grounding size as a propositional variable is no longer generated for a frozen atom. Once the freezing process is complete, the full grounding of H is further simplified by removing frozen variables. Finally, the soft constraints S are grounded much more efficiently by taking frozen atoms into account. Our approach may also be seen as an extension of a proposal by Papai et al. (2011). 1http://alchemy.cs.washington.edu 688 3.2 Entity Resolution Based MLN Representing generalities as quantified rules defined over classes of entities or events appears to be a natural formulation, but is also quite inefficient leading to large grounded networks. Despite the drastically reduced number of groundings by our inference approach, the first-order MLN formulation still timed out on 31% of the questions. Hence we consider an alternative formulation that treats generalities as relations expressed over prototypical entities and events. This formulation leverages the fact that elementary</context>
</contexts>
<marker>Papai, Singla, Kautz, 2011</marker>
<rawString>Tivadar Papai, Parag Singla, and Henry Kautz. 2011. Constraint propagation for efficient inference in Markov logic. In 17th CP, pages 691–705. Springer, Perugia, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Park</author>
</authors>
<title>MAP complexity results and approximation methods.</title>
<date>2002</date>
<pages>388--396</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="8284" citStr="Park, 2002" startWordPosition="1282" endWordPosition="1283">e semantically equivalent. Also, the string “fox” entails “some animal”. We use a lexical reasoning component based on textual entailment to establish lexical equivalence or entailment between variables. Most Likely Answer as Inference: Given KB rules and the question as input, we formulate a probabilistic reasoning problem by adding lexical reasoning probabilities and incorporating uncertainties in derived rules. Given setup facts 5 and k answer options Qi, we seek the most likely answer option: arg maxi∈{1,...,k} Pr[Qi |5, KB]. This is a Partial MAP computation which is known to be #P-hard (Park, 2002). Hence methods such as Integer Linear Programming are not directly applicable. 686 2.1 Challenges Reasoning with text-derived knowledge presents challenges that expose the brittleness and rigidity inherent in pure logic-based frameworks. Textderived rules are incomplete and include lexical items as logical elements, making rule application in a pure logical setting extremely brittle: Many relevant rules cannot be applied because their preconditions are not fully satisfied due to poor alignment. For example, naive matching of rule (1) with the facts in the setup would not conclude the query si</context>
</contexts>
<marker>Park, 2002</marker>
<rawString>James Park. 2002. MAP complexity results and approximation methods. pages 388–396, Edmonton, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Sound and efficient inference with probabilistic and deterministic dependencies.</title>
<date>2006</date>
<booktitle>In 21st AAAI,</booktitle>
<pages>458--463</pages>
<location>Boston, MA.</location>
<contexts>
<context position="29024" citStr="Poon and Domingos, 2006" startWordPosition="4661" endWordPosition="4664">ry question. This opens up the possibility of learning weights of this static MLN, which would be challenging for the previous two approaches.3 4 Empirical Evaluation We used Tuffy 0.44 (Niu et al., 2011) as the base MLN solver5 and extended it to incorporate the hard-constraint based grounding reduction technique discussed earlier, implemented using the SAT solver Glucose 3.06 (Audemard and Simon, 2009) exploiting its “solving under assumptions” capability for efficiency. We used a 10 minute timelimit, including a max of 6 minutes for grounding. Marginal inference was performed using MC-SAT (Poon and Domingos, 2006), with default parameters and 5000 flips per sample to generate 500 samples for marginal estimation. We used a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. We selected 108 elementary-level science questions (non-diagram, multiple-choice) from 4th grade New York Regents exam as our benchmark (Dev-108) and used another 68 questions from the same source as a blind test set (Unseen-68)7. 3In this work, we have set the weights manually. 4http://i.stanford.edu/hazy/tuffy 5Alchemy 1.0 gave similar results. 6http://www.labri.fr/perso/lsimon/glucose 7http://allenai.org/content/data/Ariscienc</context>
</contexts>
<marker>Poon, Domingos, 2006</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2006. Sound and efficient inference with probabilistic and deterministic dependencies. In 21st AAAI, pages 458–463, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="3025" citStr="Poon and Domingos, 2009" startWordPosition="453" endWordPosition="456">t knowledge (Lenat, 1995). We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical representations. Our goal is to build a high accura</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc De Raedt</author>
<author>Angelika Kimmig</author>
<author>Hannu Toivonen</author>
</authors>
<title>Problog: A probabilistic Prolog and its application in link discovery.</title>
<date>2007</date>
<booktitle>In International Joint Conference on Artificial Intelligence.</booktitle>
<marker>De Raedt, Kimmig, Toivonen, 2007</marker>
<rawString>Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. 2007. Problog: A probabilistic Prolog and its application in link discovery. In International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taisuke Sato</author>
<author>Yoshitaka Kameya</author>
</authors>
<title>Parameter learning of logic programs for symbolic-statistical modeling.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>391--454</pages>
<contexts>
<context position="9646" citStr="Sato and Kameya, 2001" startWordPosition="1490" endWordPosition="1493">r, a single text-derived rule may be insufficient to answer a question. For example, “Animals grow thick fur in winter” and “Thick fur helps keep warm” may need to be chained. 3 Probabilistic Formulations Statistical Relational Learning (SRL) models (Getoor and Taskar, 2007) are a natural fit for QA reasoning. They provide probabilistic semantics over knowledge in first-order logic, thereby handling uncertainty in lexical reasoning and incomplete matching. While there are many SRL formalisms including Stochastic Logic Programs (SLPs) (Muggleton, 1996), ProbLog (Raedt et al., 2007), and PRISM (Sato and Kameya, 2001), we use Markov Logic Networks (MLNs) for their ease of specification and ability to naturally handle potentially cyclic rules. Markov Logic Networks (MLNs) are relational models represented using weighted firstorder logic rules. The rules provide a template for generating a Markov network by grounding the variables to all the constants in the rules. Each rule fi forms a clique in the ground network and its weight wi determines the potential for the clique. Since all cliques generated by grounding the same clause have the same weight, the probability of a given assignment is calculated as: 1 �</context>
<context position="23432" citStr="Sato and Kameya, 2001" startWordPosition="3754" endWordPosition="3757">arantee that the rules taken together are acyclic. For example, a rule stating “Living things —* depend on the sun” and “Sun —* source of energy for living things” may exist side-by-side. Successful inference for QA must avoid feedback loops. False unless proven: While MLNs assume atoms not mentioned in any rule to be true with probability 0.5, elementary level science reasoning is better reflected in a system that assumes all atoms to be false unless stated in the question or proven through the application of a rule. This is similar to the semantics of Problog (Raedt et al., 2007) and PRISM (Sato and Kameya, 2001). While acyclic inference and false unless proven can be handled by setting high negative priors in MLNs, inference behavior is susceptible to variations in these weights. By using hard rules to control the direction of inference, we can explicitly enforce these constraints. We introduce a unary predicate called holds over string constants to capture the probability of a string constant being true given the setup is true (bx E setup, holds(x) = true) and the KB rules hold. Instead of using edges for inference, we use them as factors influencing alignment: similar constants have similar local n</context>
</contexts>
<marker>Sato, Kameya, 2001</marker>
<rawString>Taisuke Sato and Yoshitaka Kameya. 2001. Parameter learning of logic programs for symbolic-statistical modeling. Journal of Artificial Intelligence Research, pages 391–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jude Shavlik</author>
<author>Sriraam Natarajan</author>
</authors>
<title>Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network.</title>
<date>2009</date>
<booktitle>In 21st IJCAI,</booktitle>
<pages>1951--1956</pages>
<location>Pasadena, CA.</location>
<contexts>
<context position="15101" citStr="Shavlik and Natarajan, 2009" startWordPosition="2400" endWordPosition="2403">.1 Boosting Inference Efficiency. A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. Further, Lifted Blocked Gibbs and Probabilistic Theorem Proving, as implemented in Alchemy-2, were slower than basic Alchemy-1. We utilize </context>
</contexts>
<marker>Shavlik, Natarajan, 2009</marker>
<rawString>Jude Shavlik and Sriraam Natarajan. 2009. Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network. In 21st IJCAI, pages 1951–1956, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Entity resolution with Markov logic.</title>
<date>2006</date>
<booktitle>In 6th ICDM,</booktitle>
<pages>572--582</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="2974" citStr="Singla and Domingos, 2006" startWordPosition="444" endWordPosition="448">based approaches has largely relied on manually input knowledge (Lenat, 1995). We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical </context>
<context position="15070" citStr="Singla and Domingos, 2006" startWordPosition="2396" endWordPosition="2399">the original consequent. 3.1.1 Boosting Inference Efficiency. A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. Further, Lifted Blocked Gibbs and Probabilistic Theorem Proving, as implemented in Alchemy-2, were slower th</context>
<context position="18889" citStr="Singla and Domingos, 2006" startWordPosition="2991" endWordPosition="2994">ntations, with the understanding that two entities behave similarly if they have lexically similar strings. For example, agent(Grow, Animals), object(Grow, Fur) ⇒ enables(Grow, StayWarm) What was a first-order rule in FO-MLN is now already fully grounded! Entities/events in the question are also similarly represented by constants. Note that the efficiency boost using hard constraints (Section 3.1.1) is orthogonal to using prototypical constants and can be applied here as well. Equivalence or Resolution Rules: Using a simple probabilistic variant of existing Entity/Event Resolution frameworks (Singla and Domingos, 2006a; Kok and Domingos, 2008), we ensure that (a) two entities/events are considered similar when they are tied to lexically similar strings and (b) similar entities/events participate in similar relations w.r.t. other entities/events. This defines soft clusters or equivalence classes of entities/events. We use a probabilistic sameAs predicate which is reflexive, symmetric, and transitive, and interacts with the rest of the MLN as follows: isa(x, s), entails(s, s&apos;) → isa(x, s&apos;). isa(x, s), isa(y, s) → sameAs(x, y). w : isa(x, s), !isa(y, s) → !sameAs(x, y) r(x, y), sameAs(y, z) → r(x, z). r in th</context>
</contexts>
<marker>Singla, Domingos, 2006</marker>
<rawString>Parag Singla and Pedro Domingos. 2006a. Entity resolution with Markov logic. In 6th ICDM, pages 572– 582, Hong Kong, China, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Memoryefficient inference in relational domains.</title>
<date>2006</date>
<booktitle>In 21st AAAI,</booktitle>
<pages>488--493</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2974" citStr="Singla and Domingos, 2006" startWordPosition="444" endWordPosition="448">based approaches has largely relied on manually input knowledge (Lenat, 1995). We present an investigation of a reasoning approach that operates over knowledge automatically extracted from text. In order to effectively reason over knowledge derived from text, a QA system must handle incomplete and potentially noisy knowledge, and allow for reasoning under uncertainty. We cast QA as a reasoning problem in weighted-first order logic. While many probabilistic formalisms exist, we use Markov Logic Networks for the ease of specification of weighted rules. MLNs have been adopted for many NLP tasks (Singla and Domingos, 2006a; Kok and Domingos, 2008; Poon and Domingos, 2009). Recently, Beltagy et al. (2013) and Beltagy and Mooney (2014) have shown that MLNs can be used to reason with rules derived from natural language. While MLNs appear to be a natural fit, it is a priori unclear how to effectively formulate the QA task as an MLN problem. We find that unique characteristics of this domain pose new challenges in efficient inference. Moreover, it is unclear how MLNs might perform on automatically extracted noisy rules and how they would fare against simpler baselines that do not rely as much on structured logical </context>
<context position="15070" citStr="Singla and Domingos, 2006" startWordPosition="2396" endWordPosition="2399">the original consequent. 3.1.1 Boosting Inference Efficiency. A bottleneck in using MLN solvers out-of-the-box for this QA formulation is the prohibitively large grounded network size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. Further, Lifted Blocked Gibbs and Probabilistic Theorem Proving, as implemented in Alchemy-2, were slower th</context>
<context position="18889" citStr="Singla and Domingos, 2006" startWordPosition="2991" endWordPosition="2994">ntations, with the understanding that two entities behave similarly if they have lexically similar strings. For example, agent(Grow, Animals), object(Grow, Fur) ⇒ enables(Grow, StayWarm) What was a first-order rule in FO-MLN is now already fully grounded! Entities/events in the question are also similarly represented by constants. Note that the efficiency boost using hard constraints (Section 3.1.1) is orthogonal to using prototypical constants and can be applied here as well. Equivalence or Resolution Rules: Using a simple probabilistic variant of existing Entity/Event Resolution frameworks (Singla and Domingos, 2006a; Kok and Domingos, 2008), we ensure that (a) two entities/events are considered similar when they are tied to lexically similar strings and (b) similar entities/events participate in similar relations w.r.t. other entities/events. This defines soft clusters or equivalence classes of entities/events. We use a probabilistic sameAs predicate which is reflexive, symmetric, and transitive, and interacts with the rest of the MLN as follows: isa(x, s), entails(s, s&apos;) → isa(x, s&apos;). isa(x, s), isa(y, s) → sameAs(x, y). w : isa(x, s), !isa(y, s) → !sameAs(x, y) r(x, y), sameAs(y, z) → r(x, z). r in th</context>
</contexts>
<marker>Singla, Domingos, 2006</marker>
<rawString>Parag Singla and Pedro Domingos. 2006b. Memoryefficient inference in relational domains. In 21st AAAI, pages 488–493, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Venugopal</author>
<author>Vibhav Gogate</author>
</authors>
<title>On lifting the gibbs sampling algorithm.</title>
<date>2012</date>
<pages>1664--1672</pages>
<location>Lake Tahoe, NV,</location>
<contexts>
<context position="15247" citStr="Venugopal and Gogate, 2012" startWordPosition="2424" endWordPosition="2427">ork size. For example, 34 out of 108 runs timed out during MLN grounding phase after 6 minutes. On average, the ground networks in these runs were of the order of 1.4x106 ground clauses. Such behavior has also been observed, perhaps to a lesser degree, in related NLP tasks such as RTE (Beltagy and Mooney, 2014) and STS (Beltagy et al., 2014). Existing techniques address large grounding size by focusing on relevant atoms (Singla and Domingos, 2006b; Shavlik and Natarajan, 2009) or grouping atoms into large classes of interchangeable atoms (de Salvo Braz et al., 2005; Gogate and Domingos, 2011; Venugopal and Gogate, 2012). Our QA encoding has very few atoms (often under 500) but very long clauses and highly asymmetric structure. This makes existing methods ineffective. For example, lazy inference in Alchemy-11 reduced —70K ground clauses to —56K on a question, while our method, described next, brought it down to only 951 clauses. Further, Lifted Blocked Gibbs and Probabilistic Theorem Proving, as implemented in Alchemy-2, were slower than basic Alchemy-1. We utilize the combinatorial structure imposed by the set H of hard constraints (e.g., semantic rules, definition style rules, some science rules) present in</context>
</contexts>
<marker>Venugopal, Gogate, 2012</marker>
<rawString>Deepak Venugopal and Vibhav Gogate. 2012. On lifting the gibbs sampling algorithm. pages 1664– 1672, Lake Tahoe, NV, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
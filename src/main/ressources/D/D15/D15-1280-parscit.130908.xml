<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9969815">
Multi-Timescale Long Short-Term Memory Neural Network
for Modelling Sentences and Documents
</title>
<author confidence="0.999087">
Pengfei Liu, Xipeng Qiu; Xinchi Chen, Shiyu Wu, Xuanjing Huang
</author>
<affiliation confidence="0.9993795">
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.962992">
825 Zhangheng Road, Shanghai, China
</address>
<email confidence="0.999465">
{pfliu14,xpqiu,xinchichen13,syu13,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836875">
Neural network based methods have ob-
tained great progress on a variety of nat-
ural language processing tasks. However,
it is still a challenge task to model long
texts, such as sentences and documents. In
this paper, we propose a multi-timescale
long short-term memory (MT-LSTM) neu-
ral network to model long texts. MT-
LSTM partitions the hidden states of the
standard LSTM into several groups. Each
group is activated at different time peri-
ods. Thus, MT-LSTM can model very
long documents as well as short sentences.
Experiments on four benchmark datasets
show that our model outperforms the other
neural models in text classification task.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.968837032258065">
Distributed representations of words have been
widely used in many natural language process-
ing (NLP) tasks (Collobert et al., 2011; Turian et
al., 2010; Mikolov et al., 2013b; Bengio et al.,
2003). Following this success, it is rising a sub-
stantial interest to learn the distributed represen-
tations of the continuous words, such as phrases,
sentences, paragraphs and documents (Mitchell
and Lapata, 2010; Socher et al., 2013; Mikolov
et al., 2013b; Le and Mikolov, 2014; Kalchbren-
ner et al., 2014). The primary role of these mod-
els is to represent the variable-length sentence or
document as a fixed-length vector. A good rep-
resentation of the variable-length text should fully
capture the semantics of natural language.
Recently, the long short-term memory neural
network (LSTM) (Hochreiter and Schmidhuber,
1997) has been applied successfully in many NLP
tasks, such as spoken language understanding
(Yao et al., 2014), sequence labeling (Chen et al.,
*Corresponding author
2015) and machine translation (Sutskever et al.,
2014). LSTM is an extension of the recurrent neu-
ral network (RNN) (Elman, 1990), which can cap-
ture the long-term and short-term dependencies
and is very suitable to model the variable-length
texts. Besides, LSTM is also sensitive to word
order and does not rely on the external syntactic
structure as recursive neural network (Socher et
al., 2013). However, when modeling long texts,
such as documents, LSTM need to keep the useful
features for a quite long period of time. The long-
term dependencies need to be transmitted one-by-
one along the sequence. Some important features
could be lost in transmission process. Besides,
the error signal is also back-propagated one-by-
one through multiple time steps in the training
phase with back-propagation through time (BPTT)
(Werbos, 1990) algorithm. The learning efficiency
could also be decreased for the long texts. For ex-
ample, if a valuable feature occurs at the begin of
a long document, we need to back-propagate the
error through the whole document.
In this paper, we propose a multi-timescale long
short-term memory (MT-LSTM) to capture the
valuable information with different timescales. In-
spired by the works of (El Hihi and Bengio, 1995)
and (Koutnik et al., 2014), we partition the hidden
states of the standard LSTM into several groups.
Each group is activated and updated at different
time periods. The fast-speed groups keep the
short-term memories, while the slow-speed groups
keep the long-term memories. We evaluate our
model on four benchmark datasets of text classifi-
cation. Experimental results show that our model
can not only handle short texts, but can model long
texts.
Our contributions can be summarized as fol-
lows.
• With the multiple different timescale memo-
ries, MT-LSTM easily carries the crucial in-
formation over a long distance. MT-LSTM
</bodyText>
<page confidence="0.897167">
2326
</page>
<note confidence="0.985778">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2326–2335,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.671932">
can well model both short and long texts.
</bodyText>
<listItem confidence="0.9061935">
• MT-LSTM has faster convergence speed than
the standard LSTM since the error signal
can be back-propagated through multiple
timescales in the training phase.
</listItem>
<sectionHeader confidence="0.995964" genericHeader="introduction">
2 Neural Models for Sentences and
Documents
</sectionHeader>
<bodyText confidence="0.99993268852459">
The primary role of the neural models is to repre-
sent the variable-length sentence or document as a
fixed-length vector. These models generally con-
sist of a projection layer that maps words, sub-
word units or n-grams to vector representations
(often trained beforehand with unsupervised meth-
ods), and then combine them with the different
architectures of neural networks. Most of these
models for distributed representations of sentences
or documents can be classified into four cate-
gories.
Bag-of-words models A simple and intuitive
method is the Neural Bag-of-Words (NBOW)
model, in which the representation of sentences
or documents can be generated by averaging con-
stituent word representations. However, the main
drawback of NBOW is that the word order is lost.
Although NBOW is effective for general docu-
ment classification, it is not suitable for short sen-
tences.
Sequence models Sequence models construct
the representation of sentences or documents
based on the recurrent neural network (RNN)
(Mikolov et al., 2010) or the gated versions of
RNN (Sutskever et al., 2014; Chung et al., 2014).
Sequence models are sensitive to word order, but
they have a bias towards the latest input words.
This gives the RNN excellent performance at lan-
guage modelling, but it is suboptimal for modeling
the whole sentence, especially for the long texts.
Le and Mikolov (2014) proposed a Paragraph Vec-
tor (PV) to learn continuous distributed vector rep-
resentations for pieces of texts, which can be re-
garded as a long-term memory of sentences as op-
posed to the short-memory in RNN.
Topological models Topological models com-
pose the sentence representation following a given
topological structure over the words (Socher et
al., 2011a; Socher et al., 2012; Socher et al.,
2013). Recursive neural network (RecNN) adopts
a more general structure to encode sentence (Pol-
lack, 1990; Socher et al., 2013). At every node in
the tree the contexts at the left and right children
of the node are combined by a classical layer. The
weights of the layer are shared across all nodes
in the tree. The layer computed at the top node
gives a representation for the sentence. However,
RecNN depends on external constituency parse
trees provided by an external topological structure,
such as parse tree.
Convolutional models Convolutional neural
network (CNN) is also used to model sentences
(Collobert et al., 2011; Kalchbrenner et al.,
2014; Hu et al., 2014). It takes as input the
embeddings of words in the sentence aligned
sequentially, and summarizes the meaning of
a sentence through layers of convolution and
pooling, until reaching a fixed length vectorial
representation in the final layer. CNN can main-
tain the word order information and learn more
abstract characteristics.
</bodyText>
<sectionHeader confidence="0.936067" genericHeader="method">
3 Long Short-Term Memory Networks
</sectionHeader>
<bodyText confidence="0.999968571428571">
A recurrent neural network (RNN) (Elman, 1990)
is able to process a sequence of arbitrary length by
recursively applying a transition function to its in-
ternal hidden state vector ht of the input sequence.
The activation of the hidden state ht at time-step t
is computed as a function f of the current input
symbol xt and the previous hidden state ht−1
</bodyText>
<equation confidence="0.984554666666667">
{
0 t = 0
ht = f(ht−1, xt) otherwise
</equation>
<bodyText confidence="0.999166722222222">
It is common to use the state-to-state transition
function f as the composition of an element-wise
nonlinearity with an affine transformation of both
xt and ht−1.
Traditionally, a simple strategy for modeling se-
quence is to map the input sequence to a fixed-
sized vector using one RNN, and then to feed the
vector to a softmax layer for classification or other
tasks (Sutskever et al., 2014; Cho et al., 2014).
Unfortunately, a problem with RNNs with tran-
sition functions of this form is that during training,
components of the gradient vector can grow or de-
cay exponentially over long sequences (Bengio et
al., 1994; Hochreiter et al., 2001; Hochreiter and
Schmidhuber, 1997). This problem with explod-
ing or vanishing gradients makes it difficult for the
RNN model to learn long-distance correlations in
a sequence.
</bodyText>
<equation confidence="0.895538">
(1)
</equation>
<page confidence="0.973849">
2327
</page>
<figureCaption confidence="0.981972">
Figure 1: A LSTM unit. The dashed line is the
recurrent connection, and the solid link is the con-
nection at the current time.
</figureCaption>
<bodyText confidence="0.991828647058824">
Long short-term memory network (LSTM) was
proposed by (Hochreiter and Schmidhuber, 1997)
to specifically address this issue of learning long-
term dependencies. The LSTM maintains a sepa-
rate memory cell inside it that updates and exposes
its content only when deemed necessary. A num-
ber of minor modifications to the standard LSTM
unit have been made. While there are numerous
LSTM variants, here we describe the implementa-
tion used by Graves (2013).
We define the LSTM units at each time step t
to be a collection of vectors in Rd: an input gate
it, a forget gate ft, an output gate ot, a memory
cell ct and a hidden state ht. d is the number of
the LSTM units. The entries of the gating vectors
it, ft and ot are in [0, 1]. The LSTM transition
equations are the following:
</bodyText>
<equation confidence="0.999646833333333">
it = Q(Wixt + Uiht−1 + Vict−1) (2)
ft = Q(Wfxt + Ufht−1 + Vfct−1),
ot = Q(Woxt + Uoht−1 + Voct),
Et = tanh(W�xt + U�ht−1),
ct = fit O ct−1 + it O ct,
ht = ot O tanh(ct),
</equation>
<bodyText confidence="0.998559647058824">
where xt is the input at the current time step, Q de-
notes the logistic sigmoid function and O denotes
elementwise multiplication. Intuitively, the forget
gate controls the amount of which each unit of the
memory cell is erased, the input gate controls how
much each unit is updated, and the output gate
controls the exposure of the internal memory state.
Figure 1 shows the structure of a LSTM unit. In
particular, these gates and the memory cell allow a
LSTM unit to adaptively forget, memorize and ex-
pose the memory content. If the detected feature,
i.e., the memory content, is deemed important, the
forget gate will be closed and carry the memory
content across many time-steps, which is equiva-
lent to capturing a long-term dependency. On the
other hand, the unit may decide to reset the mem-
ory content by opening the forget gate.
</bodyText>
<sectionHeader confidence="0.996332" genericHeader="method">
4 Multi-Timescale Long Short-Term
Memory Neural Network
</sectionHeader>
<figure confidence="0.999131666666667">
(a) Unfolded LSTM
(b) Unfolded MT-LSTM with Fast-to-Slow Feedback
Strategy
</figure>
<figureCaption confidence="0.900215">
Figure 2: Illustration of the unfolded LSTM and
unfolded MT-LSTM. The dotted node indicates
</figureCaption>
<bodyText confidence="0.98667555">
the unit which is inactivated at current time, while
the solid node indicates the unit which is activated.
The dotted lines indicate the units which kept un-
changed, while the solid lines indicate the units
which will be updated at the next time step.
LSTM can capture the long-term and short-term
dependencies in a sequence. But the long-term
dependencies need to be transmitted one-by-one
along the sequence. Some important informa-
tion could be lost in transmission process for long
texts, such as documents. Besides, the error sig-
nal is back-propagated through multiple time steps
when we use the back-propagation through time
(BPTT) (Werbos, 1990) algorithm. The training
efficiency could also be low for the long texts. For
example, if a valuable feature occurs at the begin
of a long document, we need to back-propagate
the error through the whole document.
Inspired by the works of (El Hihi and Bengio,
1995) and (Koutnik et al., 2014), which use de-
</bodyText>
<equation confidence="0.749756125">
h1
x1x2x3x4xT Y
h2
h3
h4
� � �
hT
softmax
g3 g3 g3
1 2 3
g2
1
g1
1
�1 �2 �3 �4T
g2
</equation>
<page confidence="0.225288">
2
</page>
<figure confidence="0.935286833333333">
g 2
1
g2
3
g1
3
g3
4
g2
4
g1
4
</figure>
<equation confidence="0.8608926">
� � �
� � �
� � �
g3
T
g2
T
g1
T
-ft—
</equation>
<page confidence="0.929052">
2328
</page>
<bodyText confidence="0.999186">
layed connections and units operating at different
timescales to improve the simple RNN, we sepa-
rate the LSTM units into several groups. Different
groups capture different timescales dependencies.
More formally, the LSTM units are parti-
tioned into g groups {G1, · · · , GgI. Each group
Gk, (1 &lt; k &lt; g) is activated at different time pe-
riods Tk. Accordingly, the gates and weight ma-
trices are also partitioned to maintain the corre-
sponding LSTM groups. The MT-LSTM with just
one group is the same to the standard LSTM.
At each time step t, only the groups Gk that sat-
isfy (t MOD Tk) = 0 are executed. The choice
of the set of periods Tk E {T1, · · · ,TgI is arbi-
trary. Here, we use the exponential series of peri-
ods: group Gk has the period of Tk = 2k−1. The
group G1 is the fastest one and can be executed
at every time step, which works like the standard
LSTM. The group Gk is the slowest one.
At time step t, the memory cell vector and hid-
den state vector of group Gk are calculate in two
</bodyText>
<listItem confidence="0.8005975">
cases:
(1) When group Gk is activated at time step t,
the LSMT units of this group are calculated by the
following equations:
</listItem>
<figure confidence="0.568975833333333">
g g Vj→kj 8
Uj→k i t−1), ( )
ikt = σ(Wki xt + E i hj t−1 + E Vj→k
j=1 j=1 f cj t−1),
g g Vj→k
Uj→k o cj t),
fkt = σ(Wkfxt + E f hj t−1 + E
j=1 j=1
g g
Uj→k
okt = σ(Wkoxt + E o hj t−1 + E
j=1 j=1
g
Uj→k
Ekt = tanh(Wkcxt + E c hjt−1),
j=1
ckt = fkt ⊙ ckt−1 + ikt ⊙ ckt ,
hkt = okt ⊙ tanh(ckt ),
</figure>
<bodyText confidence="0.9996502">
where itk, fkt and okt are the vectors of input gates,
forget gates, and output gates of group Gk at time
step t respectively; ckt and hkt are the memory cell
vector and hidden state vector of group Gk at time
step t respectively.
</bodyText>
<listItem confidence="0.509966">
(2) When group Gk is non-activated at time step
t, its LSMT units keep unchanged.
</listItem>
<equation confidence="0.9846085">
ck t = ck t−1, (14)
hkt = hkt−1. (15)
</equation>
<figureCaption confidence="0.658417">
Figure 3 shows the different between the stan-
dard LSTM and MT-LSTM.
</figureCaption>
<figure confidence="0.990948">
(a) Fast-to-Slow Strategy (b) Slow-to-Fast Strategy
</figure>
<figureCaption confidence="0.677907">
Figure 3: Two feedback strategies of our model.
The dashed line shows the feedback connection,
and the solid link shows the connection at current
time.
</figureCaption>
<subsectionHeader confidence="0.994282">
4.1 Two Feedback Strategies
</subsectionHeader>
<bodyText confidence="0.992022681818181">
The feedback mechanism of LSTM is imple-
mented by the recurrent connections from time
step t − 1 to t. Since the MT-LSTM groups are
updated with the different frequencies, we can re-
gard the different group as the human memory.
The fast-speed groups are short-term memories,
while the slow-speed groups are long-term mem-
ories. Therefore, an important consideration is
what feedback mechanism is between the short-
term and long-term memories.
For the proposed MT-LSTM, we consider two
feedback strategies to define the connectivity pat-
terns among the different groups.
Fast-to-Slow (F2S) Strategy Intuitively, when
we accumulate the short-term memory to a certain
degree, we store some valuable information from
the short-term memory into the long-term mem-
ory. Therefore, we firstly define a fast to slow
strategy, which updates the slower group using the
faster group. The connections from group j to
group k exist if and only if Tj &lt; Tk. The weight
matrices Uj→k
</bodyText>
<equation confidence="0.956190571428571">
i , Uj→k
f , Uj→k
o , Uj→k
c , Vj→k
i ,
Vj→k
f , Vj→k
</equation>
<bodyText confidence="0.9916736">
o are set to zero when Tj &gt; Tk.
The F2S updating strategy is shown in Figure
3a.
Slow-to-Fast (S2F) Strategy Following the
work of (Koutnik et al., 2014), we also investigate
another update scheme from slow-speed group to
fast-speed group. The motivation is that a long
term memory can be “distilled” into a short-term
memory. The connections from group j to group i
exist only if Tj &gt; Ti. The weight matrices Uj→k
</bodyText>
<equation confidence="0.431429166666667">
i ,
Uj→kf , Uj→k
o , Uj→k
c , Vj→k
i , Vj→k
f , Vj→k
</equation>
<bodyText confidence="0.966287666666667">
o are set
to zero when Tj &lt; Tk.
The S2F update strategy is shown in Figure 3b.
</bodyText>
<page confidence="0.927697">
2329
</page>
<table confidence="0.9998834">
Dataset Type Train Size Dev. Size Test Size Class Averaged Length Vocabulary Size
SST-1 Sentence 8544 1101 2210 5 19 18K
SST-2 Sentence 6920 872 1821 2 18 15K
QC Sentence 5452 - 500 6 10 9.4K
IMDB Document 25,000 - 25,000 2 294 392K
</table>
<tableCaption confidence="0.999682">
Table 1: Statistics of the four datasets used in this paper.
</tableCaption>
<subsectionHeader confidence="0.6660875">
4.2 Dynamic Selection of the Number of the
MT-LSTM Unit Groups
</subsectionHeader>
<bodyText confidence="0.999898333333333">
Another consideration is how many groups need
to be used. An intuitive way is that we need more
groups for long texts than short texts. The number
of the group depends the length of the texts.
Here, we use a simple dynamic strategy to
choose the maximum number of groups, and then
the best g is chosen as a hyperparameter according
to different tasks. The upper bound of the number
of groups is calculated by
</bodyText>
<equation confidence="0.992793">
g = 1o92 L − 1, (16)
</equation>
<bodyText confidence="0.999566">
where L is the average length of the corpus. Thus,
the slowest group is activated at least twice.
</bodyText>
<sectionHeader confidence="0.997777" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.9999792">
In each of the experiments, the hidden layer at
the last moment has a fully connected layer fol-
lowed by a softmax non-linear layer that predicts
the probability distribution over classes given the
input sentence. The network is trained to min-
imise the cross-entropy of the predicted and true
distributions; the objective includes an L2 regu-
larization term over the parameters. The network
is trained with backpropagation and the gradient-
based optimization is performed using the Ada-
grad update rule (Duchi et al., 2011).
The back propagation of the error propagation
is similar to LSTM as well. The only difference
is that the error propagates only from groups that
were executed at time step t. The error of non-
activated groups gets copied back in time (simi-
larly to copying the activations of nodes not ac-
tivated at the time step t during the correspond-
ing forward pass), where it is added to the back-
propagated error.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999204">
In this section, we investigate the empirical per-
formances of our proposed MT-LSTM model on
four benchmark datasets for sentence and docu-
ment classification and then compare it to other
competitor models.
</bodyText>
<subsectionHeader confidence="0.958922">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9998352">
We evaluate our model on four different datasets.
The first three datasets are sentence-level, and the
last dataset is document-level. The detailed statis-
tics about the four datasets are listed in Table 1.
Each dataset is briefly described as follows.
</bodyText>
<listItem confidence="0.899376823529412">
• SST-1 The movie reviews with five classes
(negative, somewhat negative, neutral, some-
what positive, positive) in the Stanford Senti-
ment Treebank1 (Socher et al., 2013).
• SST-2 The movie reviews with binary
classes. It is also from the Stanford Senti-
ment Treebank.
• QC The TREC questions dataset2 involves
six different question types, e.g. whether the
question is about a location, about a person
or about some numeric information (Li and
Roth, 2002).
• IMDB The IMDB dataset3 consists of
100,000 movie reviews with binary classes
(Maas et al., 2011). One key aspect of this
dataset is that each movie review has several
sentences.
</listItem>
<subsectionHeader confidence="0.990808">
6.2 Competitor Models
</subsectionHeader>
<bodyText confidence="0.999903">
We compare our model with the following models:
</bodyText>
<listItem confidence="0.94727225">
• NB-SVM and MNB. Naive Bayes SVM and
Multinomial Naive Bayes with uni and bi-
gram features (Wang and Manning, 2012).
• NBOW The NBOW sums the word vectors
and applies a non-linearity followed by a
softmax classification layer.
• RAE Recursive Autoencoders with pre-
trained word vectors from Wikipedia (Socher
et al., 2011b).
• MV-RNN Matrix-Vector Recursive Neural
Network with parse trees (Socher et al.,
2012).
</listItem>
<footnote confidence="0.9589666">
1http://nlp.stanford.edu/sentiment.
2http://cogcomp.cs.illinois.edu/Data/
QA/QC/.
3http://ai.stanford.edu/˜amaas/data/
sentiment/
</footnote>
<page confidence="0.937574">
2330
</page>
<table confidence="0.998965333333333">
SST-1 SST-2 QC IMDB
Embedding size 100 100 100 100
hidden layer size 60 60 55 100
Initial learning rate 0.1 0.1 0.1 0.1
Regularization 10−5 10−5 10−5 10−5
Number of Groups 3 3 3 5
</table>
<tableCaption confidence="0.9546965">
Table 2: Hyper-parameter settings for the LSTM
and MT-LSTM.
</tableCaption>
<equation confidence="0.156783">
Acc.(%)
</equation>
<listItem confidence="0.918872157894737">
• RNTN Recursive Neural Tensor Network
with tensor-based feature function and parse
trees (Socher et al., 2013).
• AdaSent Self-adaptive hierarchical sentence
model with gated mechanism (Zhao et al.,
2015).
• DCNN Dynamic Convolutional Neural Net-
work with dynamic k-max pooling (Kalch-
brenner et al., 2014).
• CNN-non-static and CNN-multichannel
Convolutional Neural Network (Kim, 2014).
• PV Logistic regression on top of paragraph
vectors (Le and Mikolov, 2014). Here, we
use the popular open source implementation
of PV in Gensim4.
• LSTM The standard LSTM for text classifi-
cation. We use the implementation of Graves
(2013). The unfolded illustration is shown in
Figure 2a.
</listItem>
<subsectionHeader confidence="0.990614">
6.3 Hyperparameters and Training
</subsectionHeader>
<bodyText confidence="0.999986571428571">
In all of our experiments, the word embeddings are
trained using word2vec (Mikolov et al., 2013a) on
the Wikipedia corpus (1B words). The vocabu-
lary size is about 500,000. The the word embed-
dings are fine-tuned during training to improve the
performance (Collobert et al., 2011). The other
parameters are initialized by randomly sampling
from uniform distribution in [-0.1, 0.1]. The hy-
perparameters which achieve the best performance
on the development set will be chosen for the fi-
nal evaluation. For datasets without development
set, we use 10-fold cross-validation (CV) instead.
The final hyper-parameters for the LSTM and MT-
LSTM are set as Figure 2.
</bodyText>
<sectionHeader confidence="0.648102" genericHeader="evaluation">
6.4 Results
</sectionHeader>
<bodyText confidence="0.9995072">
Table 3 shows the classification accuracies of the
standard LSTM, MT-LSTM compared with the
competitor models.
Firstly, we compare two feedback strategies
of MT-LSTM. The fast-to-slow feedback strat-
</bodyText>
<footnote confidence="0.6884765">
4https://github.com/piskvorky/gensim/
Number of Iteration
</footnote>
<figureCaption confidence="0.999701">
Figure 4: Convergence Speeds on IMDB dataset.
</figureCaption>
<bodyText confidence="0.999966611111111">
egy (MT-LSTM (F2S)) is better than the slow-to-
fast strategy (MT-LSTM (S2F)), which indicates
that MT-LSTM benefits from periodically stor-
ing some valuable information “purified” from the
short-term memory into the long-term memory. In
the following discussion, we use fast-to-slow feed-
back strategy as the default setting of MT-LSTM.
Compared with the standard LSTM, MT-LSTM
results in significantly improvements with the
same size of hidden layers.
MT-LSTM outperforms the competitor models
on the SST-1, QC and IMDB datasets, and is close
to the two best CNN based models on the SST-2
dataset. But MT-LSTM uses much fewer param-
eters than the CNN based models. The number
of parameters of LSTM range from 10K to 40K
while the number of parameters is about 400K in
CNN.
Moreover, MT-LSTM can not only handle short
texts, but can model long texts in classification
task.
Documents Modeling Most of the competitor
models cannot deal with the texts of with sev-
eral sentences (paragraphs, documents). For in-
stance, MV-RNN and RNTN (Socher et al., 2013)
are based on the parsing over each sentence and
it is unclear how to combine the representations
over many sentences. The convolutional models,
such as CNN (Kim, 2014) and AdaSent (Zhao et
al., 2015), need more hidden layers or nodes for
long texts and result in a very complicated model.
These models therefore are restricted to work-
ing on sentences instead of paragraphs or docu-
ments. Denil et al. (2014) used two-level version
of DCNN (Kalchbrenner et al., 2014) to model
documents. The first level uses a DCNN to trans-
</bodyText>
<figure confidence="0.99905025">
1 2 3 4 5 6 7 8
0.9
0.8
0.7
0.6
0.5
LSTM
MT-LSTM
</figure>
<page confidence="0.995858">
2331
</page>
<note confidence="0.940481461538461">
Model SST-1 SST-2 QC IMDB
NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 -
RAE (Socher et al., 2011b) 43.2 82.4 - -
MV-RNN (Socher et al., 2012) 44.4 82.9 - -
RNTN (Socher et al., 2013) 45.7 85.4 - -
DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 -
CNN-non-static (Kim, 2014) 48.0 87.2 93.6 -
CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 -
AdaSent (Zhao et al., 2015) - - 92.4 -
NBSVM (Wang and Manning, 2012) - - - 91.2
MNB (Wang and Manning, 2012) - - - 86.6
Two-level DCNN (Denil et al., 2014) - - - 89.4
PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7*
</note>
<table confidence="0.990139333333333">
LSTM 47.9 85.8 91.3 88.5
MT-LSTM (S2F) 48.9 86.7 93.3 90.2
MT-LSTM (F2S) 49.1 87.2 94.4 92.1
</table>
<tableCaption confidence="0.9562265">
Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without
marks are reported in the corresponding paper.
</tableCaption>
<figureCaption confidence="0.889854">
Figure 5: Performances of our model with the dif-
</figureCaption>
<bodyText confidence="0.992489094339623">
ferent numbers of memory groups g on four devel-
opment datasets: SST-1,SST-2, QC, and IMDB.
Y-axis represents the accuracy(%), and X-axis rep-
resents different numbers of memory groups. All
memory groups share a fixed-size memory layer
h, and here we set h=120.
form embeddings for the words in each sentence
into an embedding for the entire sentence. The
second level uses another DCNN to transform sen-
tence embeddings from the first level into a single
embedding vector that represents the entire docu-
ment. However, their result is unsatisfactory and
they reported that the IMDB dataset is too small
to train a CNN model.
The standard LSTM has an advantage to model
documents due to its simplification. However, it is
also difficult to train LSTM since the error signals
need to be back-propagated over a long distance
with the BPTT algorithm.
Our MT-LSTM can alleviate this problem with
multiple timescale memories. The experiment on
IMDB dataset demonstrates this advantage. MT-
LSTM achieves the accuracy of 92.1% , which are
better than the other models.
Moreover, MT-LSTM converges at a faster rate
than the standard LSTM. Figure 4 plots the con-
vergence on the IMDB dataset. In practice, MT-
LSTM is approximately three times faster than the
standard LSTM since the hidden states of low-
speed group often keep unchanged and need not
to be re-calculated at each time step.
Impact of the Different Number of Memory
Groups In our model, the number of memory
groups is a hyperparameter. Here we plotted the
accuracy curves of our model with the different
numbers of memory groups in Figure 5 to show
its impacts on the four datasets.
When the length of text (SST-1, SST-2 and
QC) is small, not all memory groups can be acti-
vated if we set too many groups, which may harm
the performance. When dealing with the long
texts (IMBD), more groups lead to a better per-
formance. The performance can be improved with
the increase of the number of memory groups.
According to our dynamic strategy, the maxi-
mum numbers of groups is 3, 3, 2, 7 for the four
datasets. The best numbers of groups from exper-
iments are 3, 3, 3, 5 respectively. Therefor, our
dynamic strategy is reasonable. All the datasets
except QC, the best number of groups is equal to
or smaller than our calculated upper bound. MT-
LSMT suffers underfitting when the number of
groups is larger than the upper bound.
</bodyText>
<figure confidence="0.991494200000001">
1 2 3 4 5
(a) SST-1
1 2 3 4
(c) QC
1 2 3 4 5
(b) SST-2
1 2 3 4 5 6 8
(d) IMDB
47
46.5
46
45.5
45
44.5
92.5
92
91.5
91
90.5
90
85.8
85.6
85.4
85.2
85
84.8
84.6
90
89.5
89
88.5
2332
0.5
0.4
0.3
0.2
0.8
0.6
0.4
0.2
0
&lt;s&gt; Is this progress ? &lt;/s&gt;
LSTM
MT-LSTM
0.8
0.6
0.4
0.2
0
&lt;s&gt; He ’d create a movie better than this . &lt;/s&gt;
LSTM
MT-LSTM
LSTM
MT-LSTM
&lt;s&gt; It &apos;s not exactly a gourmetmeal but the fare is fair , even coming from the drive . &lt;/s&gt;
</figure>
<figureCaption confidence="0.963259">
Figure 6: The dynamical changes of the predicted sentiment score over time. Y-axis represents the
sentiment score, while X-axis represents the input words in chronological order. The red horizontal line
gives a border between the positive and negative sentiments.
</figureCaption>
<subsectionHeader confidence="0.997812">
6.5 Case Study
</subsectionHeader>
<bodyText confidence="0.999804642857143">
To get an intuitive understanding of what is hap-
pening when we use LSTM or MT-LSTM to pre-
dict the class of text, we design an experiment
to analyze the output of LSTM and MT-LSTM at
each time step.
We sample three sentences from the SST-2 test
dataset, and the dynamical changes of the pre-
dicted sentiment score over time are shown in Fig-
ure 6. It is intriguing to notice that our model can
handle the rhetorical question well.
The first sentence “Is this progress ?”
has a negative sentiment. Although the word
“progress” is positive, our model can adjust the
sentiment correctly after seeing the question mark
“?”, and finally gets a correct prediction.
The second sentence “He ’d create a
movie better than this .” also has a
negative sentiment. The word “better” is posi-
tive. Our model finally gets a correct negative pre-
diction after seeing “than this”, while LSTM gets
a wrong prediction.
The third sentence “ It ’s not exactly
a gourmet meal but fare is fair
, even coming from the drive .”
is positive and has more complicated semantic
composition. Our model can still capture the
useful long-term features and gets the correct
prediction, while LSTM does not work well.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999987071428571">
There are many previous works to model the
variable-length text as a fixed-length vector. Spe-
cific to text classification task, most of the mod-
els cannot deal with the texts of several sen-
tences (paragraphs, documents), such as MV-RNN
(Socher et al., 2012), RNTN (Socher et al., 2013),
CNN (Kim, 2014), AdaSent (Zhao et al., 2015),
and so on. The simple neural bag-of-words model
can deal with long texts, but it loses the word order
information. PV (Le and Mikolov, 2014) works in
unsupervised way, and the learned vector cannot
be fine-tuned on the specific task.
Our proposed MT-LSTM can handle short texts
as well as long texts in classification task.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.994571461538461">
In this paper, we introduce the MT-LSTM, a gen-
eralization of LSTMs to capture the information
with different timescales. MT-LSTM can well
model both short and long texts. With the multi-
ple different timescale memories. Intuitively, MT-
LSTM easily carries the crucial information over
a long distance. Another advantage of MT-LSTM
is that the training speed is faster than the standard
LSTM (approximately three times faster in prac-
tice).
In future work, we would like to investigate the
other feedback mechanism between the short-term
and long-term memories.
</bodyText>
<page confidence="0.969926">
2333
</page>
<sectionHeader confidence="0.999168" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999205">
We would like to thank the anonymous review-
ers for their valuable comments. This work was
partially funded by the National Natural Science
Foundation of China (61472088, 61473092), Na-
tional High Technology Research and Develop-
ment Program of China (2015AA015408), Shang-
hai Science and Technology Development Funds
(14ZR1403200).
</bodyText>
<sectionHeader confidence="0.998851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999265368421053">
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015. Long short-term mem-
ory neural networks for chinese word segmenta-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of EMNLP.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil
Blunsom, and Nando de Freitas. 2014. Modelling,
visualising and summarising documents with a sin-
gle convolutional neural network. arXiv preprint
arXiv:1406.3830.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Salah El Hihi and Yoshua Bengio. 1995. Hierarchical
recurrent neural networks for long-term dependen-
cies. In NIPS, pages 493–499. Citeseer.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Alex Graves. 2013. Generating sequences
with recurrent neural networks. arXiv preprint
arXiv:1308.0850.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and
J¨urgen Schmidhuber. 2001. Gradient flow in recur-
rent nets: the difficulty of learning long-term depen-
dencies.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.
Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juer-
gen Schmidhuber. 2014. A clockwork rnn. In Pro-
ceedings of The 31st International Conference on
Machine Learning, pages 1863–1871.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics, pages 556–
562.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142–150. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.
</reference>
<page confidence="0.801975">
2334
</page>
<reference confidence="0.99982579245283">
Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011a. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129–136.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP).
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560.
Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-
offrey Zweig, and Yangyang Shi. 2014. Spoken lan-
guage understanding using long short-term memory
neural networks. IEEE SLT.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. arXiv
preprint arXiv:1504.05070.
</reference>
<page confidence="0.989307">
2335
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686676">
<title confidence="0.898294666666667">Multi-Timescale Long Short-Term Memory Neural for Modelling Sentences and Documents Liu, Xipeng Chen, Shiyu Wu, Xuanjing</title>
<author confidence="0.995905">Shanghai Key Laboratory of Intelligent Information Processing</author>
<author confidence="0.995905">Fudan</author>
<affiliation confidence="0.996658">School of Computer Science, Fudan</affiliation>
<address confidence="0.995534">825 Zhangheng Road, Shanghai,</address>
<abstract confidence="0.999519294117647">Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, it is still a challenge task to model long texts, such as sentences and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MT- LSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="8115" citStr="Bengio et al., 1994" startWordPosition="1282" endWordPosition="1285">ise It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1. Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014). Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. (1) 2327 Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time. Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1231" citStr="Bengio et al., 2003" startWordPosition="177" endWordPosition="180">ulti-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Pengfei Liu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Long short-term memory neural networks for chinese word segmentation.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Chen, Qiu, Zhu, Liu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, and Xuanjing Huang. 2015. Long short-term memory neural networks for chinese word segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</title>
<date>2014</date>
<contexts>
<context position="5416" citStr="Chung et al., 2014" startWordPosition="830" endWordPosition="833">categories. Bag-of-words models A simple and intuitive method is the Neural Bag-of-Words (NBOW) model, in which the representation of sentences or documents can be generated by averaging constituent word representations. However, the main drawback of NBOW is that the word order is lost. Although NBOW is effective for general document classification, it is not suitable for short sentences. Sequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) (Mikolov et al., 2010) or the gated versions of RNN (Sutskever et al., 2014; Chung et al., 2014). Sequence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the w</context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1165" citStr="Collobert et al., 2011" startWordPosition="165" endWordPosition="168">exts, such as sentences and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently,</context>
<context position="6706" citStr="Collobert et al., 2011" startWordPosition="1042" endWordPosition="1045">). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can maintain the word order information and learn more abstract characteristics. 3 Long Short-Term Memory Networks A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input </context>
<context position="19984" citStr="Collobert et al., 2011" startWordPosition="3403" endWordPosition="3406"> Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters for the LSTM and MTLSTM are set as Figure 2. 6.4 Results Table 3 shows the classification accuracies of the standard LSTM, MT-LSTM compared with the competitor models. Firstly, we compare two feedback strategies of MT-LSTM. The fast-to-slow feedback strat4https://gith</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Misha Denil</author>
<author>Alban Demiraj</author>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
<author>Nando de Freitas</author>
</authors>
<title>Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</title>
<date>2014</date>
<marker>Denil, Demiraj, Kalchbrenner, Blunsom, de Freitas, 2014</marker>
<rawString>Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. 2014. Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="16609" citStr="Duchi et al., 2011" startWordPosition="2865" endWordPosition="2868">s the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters. The network is trained with backpropagation and the gradientbased optimization is performed using the Adagrad update rule (Duchi et al., 2011). The back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error. 6 Experiments In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to othe</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salah El Hihi</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical recurrent neural networks for long-term dependencies. In</title>
<date>1995</date>
<booktitle>NIPS,</booktitle>
<pages>493--499</pages>
<publisher>Citeseer.</publisher>
<marker>El Hihi, Bengio, 1995</marker>
<rawString>Salah El Hihi and Yoshua Bengio. 1995. Hierarchical recurrent neural networks for long-term dependencies. In NIPS, pages 493–499. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2139" citStr="Elman, 1990" startWordPosition="319" endWordPosition="320">2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The longterm dependencies need to be transmitted one-byone along the sequence. Some important features could be lost in transmission process. Besides, the error signal is also back-propagated one-byone thr</context>
<context position="7157" citStr="Elman, 1990" startWordPosition="1114" endWordPosition="1115">ternal topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can maintain the word order information and learn more abstract characteristics. 3 Long Short-Term Memory Networks A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input sequence. The activation of the hidden state ht at time-step t is computed as a function f of the current input symbol xt and the previous hidden state ht−1 { 0 t = 0 ht = f(ht−1, xt) otherwise It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1. Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixed</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</title>
<date>2013</date>
<contexts>
<context position="8898" citStr="Graves (2013)" startWordPosition="1412" endWordPosition="1413">ce correlations in a sequence. (1) 2327 Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time. Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. A number of minor modifications to the standard LSTM unit have been made. While there are numerous LSTM variants, here we describe the implementation used by Graves (2013). We define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The entries of the gating vectors it, ft and ot are in [0, 1]. The LSTM transition equations are the following: it = Q(Wixt + Uiht−1 + Vict−1) (2) ft = Q(Wfxt + Ufht−1 + Vfct−1), ot = Q(Woxt + Uoht−1 + Voct), Et = tanh(W�xt + U�ht−1), ct = fit O ct−1 + it O ct, ht = ot O tanh(ct), where xt is the input at the current time step, Q denotes the logistic sigmoid function and O denotes el</context>
<context position="19622" citStr="Graves (2013)" startWordPosition="3347" endWordPosition="3348">ive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets with</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="1849" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="272" endWordPosition="275">o et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such </context>
<context position="8175" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1290" endWordPosition="1293">nsition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1. Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014). Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. (1) 2327 Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time. Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. A number of minor modifications to the standard </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Yoshua Bengio</author>
<author>Paolo Frasconi</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.</title>
<date>2001</date>
<contexts>
<context position="8140" citStr="Hochreiter et al., 2001" startWordPosition="1286" endWordPosition="1289">se the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1. Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014). Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. (1) 2327 Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time. Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. A number of m</context>
</contexts>
<marker>Hochreiter, Bengio, Frasconi, Schmidhuber, 2001</marker>
<rawString>Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J¨urgen Schmidhuber. 2001. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6751" citStr="Hu et al., 2014" startWordPosition="1050" endWordPosition="1053">eneral structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can maintain the word order information and learn more abstract characteristics. 3 Long Short-Term Memory Networks A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input sequence. The activation of the hidden state </context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1532" citStr="Kalchbrenner et al., 2014" startWordPosition="224" endWordPosition="228">s on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman</context>
<context position="6733" citStr="Kalchbrenner et al., 2014" startWordPosition="1046" endWordPosition="1049">ork (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can maintain the word order information and learn more abstract characteristics. 3 Long Short-Term Memory Networks A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input sequence. The activation of</context>
<context position="19301" citStr="Kalchbrenner et al., 2014" startWordPosition="3294" endWordPosition="3298">s.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings</context>
<context position="22189" citStr="Kalchbrenner et al., 2014" startWordPosition="3752" endWordPosition="3755"> Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents). For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</title>
<date>2014</date>
<contexts>
<context position="19381" citStr="Kim, 2014" startWordPosition="3306" endWordPosition="3307">edding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 20</context>
<context position="21891" citStr="Kim, 2014" startWordPosition="3703" endWordPosition="3704">M uses much fewer parameters than the CNN based models. The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN. Moreover, MT-LSTM can not only handle short texts, but can model long texts in classification task. Documents Modeling Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents). For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7</context>
<context position="27730" citStr="Kim, 2014" startWordPosition="4761" endWordPosition="4762">ediction. The third sentence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well. 7 Related Work There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task. Our proposed MT-LSTM can handle short texts as well as long texts in classification task. 8 Conclusion In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multiple different timescale memories. Intuitively, </context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Koutnik</author>
<author>Klaus Greff</author>
<author>Faustino Gomez</author>
<author>Juergen Schmidhuber</author>
</authors>
<title>A clockwork rnn.</title>
<date>2014</date>
<booktitle>In Proceedings of The 31st International Conference on Machine Learning,</booktitle>
<pages>1863--1871</pages>
<contexts>
<context position="3279" citStr="Koutnik et al., 2014" startWordPosition="500" endWordPosition="503">smission process. Besides, the error signal is also back-propagated one-byone through multiple time steps in the training phase with back-propagation through time (BPTT) (Werbos, 1990) algorithm. The learning efficiency could also be decreased for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales. Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), we partition the hidden states of the standard LSTM into several groups. Each group is activated and updated at different time periods. The fast-speed groups keep the short-term memories, while the slow-speed groups keep the long-term memories. We evaluate our model on four benchmark datasets of text classification. Experimental results show that our model can not only handle short texts, but can model long texts. Our contributions can be summarized as follows. • With the multiple different timescale memories, MT-LSTM easily carries the crucial information over a long distance. MT-LSTM 2326 </context>
<context position="11393" citStr="Koutnik et al., 2014" startWordPosition="1850" endWordPosition="1853">nce. But the long-term dependencies need to be transmitted one-by-one along the sequence. Some important information could be lost in transmission process for long texts, such as documents. Besides, the error signal is back-propagated through multiple time steps when we use the back-propagation through time (BPTT) (Werbos, 1990) algorithm. The training efficiency could also be low for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document. Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), which use deh1 x1x2x3x4xT Y h2 h3 h4 � � � hT softmax g3 g3 g3 1 2 3 g2 1 g1 1 �1 �2 �3 �4T g2 2 g 2 1 g2 3 g1 3 g3 4 g2 4 g1 4 � � � � � � � � � g3 T g2 T g1 T -ft— 2328 layed connections and units operating at different timescales to improve the simple RNN, we separate the LSTM units into several groups. Different groups capture different timescales dependencies. More formally, the LSTM units are partitioned into g groups {G1, · · · , GgI. Each group Gk, (1 &lt; k &lt; g) is activated at different time periods Tk. Accordingly, the gates and weight matrices are also partitioned to maintain the co</context>
<context position="14795" citStr="Koutnik et al., 2014" startWordPosition="2531" endWordPosition="2534">different groups. Fast-to-Slow (F2S) Strategy Intuitively, when we accumulate the short-term memory to a certain degree, we store some valuable information from the short-term memory into the long-term memory. Therefore, we firstly define a fast to slow strategy, which updates the slower group using the faster group. The connections from group j to group k exist if and only if Tj &lt; Tk. The weight matrices Uj→k i , Uj→k f , Uj→k o , Uj→k c , Vj→k i , Vj→k f , Vj→k o are set to zero when Tj &gt; Tk. The F2S updating strategy is shown in Figure 3a. Slow-to-Fast (S2F) Strategy Following the work of (Koutnik et al., 2014), we also investigate another update scheme from slow-speed group to fast-speed group. The motivation is that a long term memory can be “distilled” into a short-term memory. The connections from group j to group i exist only if Tj &gt; Ti. The weight matrices Uj→k i , Uj→kf , Uj→k o , Uj→k c , Vj→k i , Vj→k f , Vj→k o are set to zero when Tj &lt; Tk. The S2F update strategy is shown in Figure 3b. 2329 Dataset Type Train Size Dev. Size Test Size Class Averaged Length Vocabulary Size SST-1 Sentence 8544 1101 2210 5 19 18K SST-2 Sentence 6920 872 1821 2 18 15K QC Sentence 5452 - 500 6 10 9.4K IMDB Docu</context>
</contexts>
<marker>Koutnik, Greff, Gomez, Schmidhuber, 2014</marker>
<rawString>Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. 2014. A clockwork rnn. In Proceedings of The 31st International Conference on Machine Learning, pages 1863–1871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="1504" citStr="Mikolov, 2014" startWordPosition="222" endWordPosition="223">ces. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent</context>
<context position="5686" citStr="Mikolov (2014)" startWordPosition="877" endWordPosition="878">der is lost. Although NBOW is effective for general document classification, it is not suitable for short sentences. Sequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) (Mikolov et al., 2010) or the gated versions of RNN (Sutskever et al., 2014; Chung et al., 2014). Sequence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the </context>
<context position="19458" citStr="Mikolov, 2014" startWordPosition="3319" endWordPosition="3320">ng rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform d</context>
<context position="22834" citStr="Mikolov, 2014" startWordPosition="3887" endWordPosition="3888">level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al., 2014) - - - 89.4 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7* LSTM 47.9 85.8 91.3 88.5 MT-LSTM (S2F) 48.9 86.7 93.3 90.2 MT-LSTM (F2S) 49.1 87.2 94.4 92.1 Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without marks are reported in the corresponding paper. Figure 5: Performances of our model with the different numbers of memory groups g on four development datasets: SST-1,SST-2, QC, and IMDB. Y-axis represents the accuracy(%), and X-axis represents different numbers of memory groups. All memory groups share a fixed-size memory layer h, and here we set h=120. form embeddings for the w</context>
<context position="27901" citStr="Mikolov, 2014" startWordPosition="4791" endWordPosition="4792">ion. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well. 7 Related Work There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task. Our proposed MT-LSTM can handle short texts as well as long texts in classification task. 8 Conclusion In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multiple different timescale memories. Intuitively, MTLSTM easily carries the crucial information over a long distance. Another advantage of MT-LSTM is that the training speed is faster than the standard LSTM (approximately</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="17949" citStr="Li and Roth, 2002" startWordPosition="3088" endWordPosition="3091">level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector R</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics, pages 556– 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18049" citStr="Maas et al., 2011" startWordPosition="3105" endWordPosition="3108">isted in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. </context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1208" citStr="Mikolov et al., 2013" startWordPosition="173" endWordPosition="176">s paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network </context>
<context position="19801" citStr="Mikolov et al., 2013" startWordPosition="3373" endWordPosition="3376"> (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters for the LSTM and MTLSTM are set as Figure 2. 6.4 Results Table 3 shows the classifica</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1208" citStr="Mikolov et al., 2013" startWordPosition="173" endWordPosition="176">s paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network </context>
<context position="19801" citStr="Mikolov et al., 2013" startWordPosition="3373" endWordPosition="3376"> (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters for the LSTM and MTLSTM are set as Figure 2. 6.4 Results Table 3 shows the classifica</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1438" citStr="Mitchell and Lapata, 2010" startWordPosition="208" endWordPosition="211">e periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translati</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="6185" citStr="Pollack, 1990" startWordPosition="956" endWordPosition="958">elling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="6041" citStr="Socher et al., 2011" startWordPosition="932" endWordPosition="935">ence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural netw</context>
<context position="18521" citStr="Socher et al., 2011" startWordPosition="3184" endWordPosition="3187">ut some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Se</context>
<context position="22400" citStr="Socher et al., 2011" startWordPosition="3798" endWordPosition="3801">lear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al., 2014) - - - 89.4 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7* LSTM 47.9 85.8 91.3 88.5 MT-LSTM (S2F) 48.9 86.7 93.3 90.2 MT-LSTM (F2S) 49.1 87.2 94.4 92.1 Table 3: Results of our MT-LSTM model against st</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011a. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="6041" citStr="Socher et al., 2011" startWordPosition="932" endWordPosition="935">ence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural netw</context>
<context position="18521" citStr="Socher et al., 2011" startWordPosition="3184" endWordPosition="3187">ut some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Se</context>
<context position="22400" citStr="Socher et al., 2011" startWordPosition="3798" endWordPosition="3801">lear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al., 2014) - - - 89.4 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7* LSTM 47.9 85.8 91.3 88.5 MT-LSTM (S2F) 48.9 86.7 93.3 90.2 MT-LSTM (F2S) 49.1 87.2 94.4 92.1 Table 3: Results of our MT-LSTM model against st</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="6063" citStr="Socher et al., 2012" startWordPosition="936" endWordPosition="939">ive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used</context>
<context position="18611" citStr="Socher et al., 2012" startWordPosition="3197" endWordPosition="3200">,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN D</context>
<context position="22445" citStr="Socher et al., 2012" startWordPosition="3807" endWordPosition="3810"> many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al., 2014) - - - 89.4 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7* LSTM 47.9 85.8 91.3 88.5 MT-LSTM (S2F) 48.9 86.7 93.3 90.2 MT-LSTM (F2S) 49.1 87.2 94.4 92.1 Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results</context>
<context position="27685" citStr="Socher et al., 2012" startWordPosition="4751" endWordPosition="4754">on after seeing “than this”, while LSTM gets a wrong prediction. The third sentence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well. 7 Related Work There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task. Our proposed MT-LSTM can handle short texts as well as long texts in classification task. 8 Conclusion In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multipl</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP).</booktitle>
<contexts>
<context position="1459" citStr="Socher et al., 2013" startWordPosition="212" endWordPosition="215">n model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al.,</context>
<context position="6085" citStr="Socher et al., 2013" startWordPosition="940" endWordPosition="943">t they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used to model sentences (C</context>
<context position="17664" citStr="Socher et al., 2013" startWordPosition="3040" endWordPosition="3043"> investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models. 6.1 Datasets We evaluate our model on four different datasets. The first three datasets are sentence-level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes w</context>
<context position="19107" citStr="Socher et al., 2013" startWordPosition="3266" endWordPosition="3269">rom Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training I</context>
<context position="21723" citStr="Socher et al., 2013" startWordPosition="3673" endWordPosition="3676">ize of hidden layers. MT-LSTM outperforms the competitor models on the SST-1, QC and IMDB datasets, and is close to the two best CNN based models on the SST-2 dataset. But MT-LSTM uses much fewer parameters than the CNN based models. The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN. Moreover, MT-LSTM can not only handle short texts, but can model long texts in classification task. Documents Modeling Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents). For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IM</context>
<context position="27713" citStr="Socher et al., 2013" startWordPosition="4756" endWordPosition="4759"> while LSTM gets a wrong prediction. The third sentence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well. 7 Related Work There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task. Our proposed MT-LSTM can handle short texts as well as long texts in classification task. 8 Conclusion In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multiple different timescale memori</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="2065" citStr="Sutskever et al., 2014" startWordPosition="304" endWordPosition="307">cher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The longterm dependencies need to be transmitted one-byone along the sequence. Some important features could be lost in transmissio</context>
<context position="5395" citStr="Sutskever et al., 2014" startWordPosition="826" endWordPosition="829">be classified into four categories. Bag-of-words models A simple and intuitive method is the Neural Bag-of-Words (NBOW) model, in which the representation of sentences or documents can be generated by averaging constituent word representations. However, the main drawback of NBOW is that the word order is lost. Although NBOW is effective for general document classification, it is not suitable for short sentences. Sequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) (Mikolov et al., 2010) or the gated versions of RNN (Sutskever et al., 2014; Chung et al., 2014). Sequence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological</context>
<context position="7889" citStr="Sutskever et al., 2014" startWordPosition="1244" endWordPosition="1247">al hidden state vector ht of the input sequence. The activation of the hidden state ht at time-step t is computed as a function f of the current input symbol xt and the previous hidden state ht−1 { 0 t = 0 ht = f(ht−1, xt) otherwise It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1. Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014). Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. (1) 2327 Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time. Long short-term memory network (LSTM) </context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1186" citStr="Turian et al., 2010" startWordPosition="169" endWordPosition="172">and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18316" citStr="Wang and Manning, 2012" startWordPosition="3151" endWordPosition="3154">ith binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Gro</context>
<context position="22710" citStr="Wang and Manning, 2012" startWordPosition="3857" endWordPosition="3860">aragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 - CNN-non-static (Kim, 2014) 48.0 87.2 93.6 - CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 - AdaSent (Zhao et al., 2015) - - 92.4 - NBSVM (Wang and Manning, 2012) - - - 91.2 MNB (Wang and Manning, 2012) - - - 86.6 Two-level DCNN (Denil et al., 2014) - - - 89.4 PV (Le and Mikolov, 2014) 44.6* 82.7* 91.8* 91.7* LSTM 47.9 85.8 91.3 88.5 MT-LSTM (S2F) 48.9 86.7 93.3 90.2 MT-LSTM (F2S) 49.1 87.2 94.4 92.1 Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without marks are reported in the corresponding paper. Figure 5: Performances of our model with the different numbers of memory groups g on four development datasets: SST-1,SST-2, QC, and IMDB. Y-axis represents the accuracy(%), and X-axis represents different num</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Werbos</author>
</authors>
<title>Backpropagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>78--10</pages>
<contexts>
<context position="2842" citStr="Werbos, 1990" startWordPosition="430" endWordPosition="431">l the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The longterm dependencies need to be transmitted one-byone along the sequence. Some important features could be lost in transmission process. Besides, the error signal is also back-propagated one-byone through multiple time steps in the training phase with back-propagation through time (BPTT) (Werbos, 1990) algorithm. The learning efficiency could also be decreased for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales. Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), we partition the hidden states of the standard LSTM into several groups. Each group is activated and updated at different time periods. The fast-speed groups kee</context>
<context position="11102" citStr="Werbos, 1990" startWordPosition="1801" endWordPosition="1802">ent time, while the solid node indicates the unit which is activated. The dotted lines indicate the units which kept unchanged, while the solid lines indicate the units which will be updated at the next time step. LSTM can capture the long-term and short-term dependencies in a sequence. But the long-term dependencies need to be transmitted one-by-one along the sequence. Some important information could be lost in transmission process for long texts, such as documents. Besides, the error signal is back-propagated through multiple time steps when we use the back-propagation through time (BPTT) (Werbos, 1990) algorithm. The training efficiency could also be low for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document. Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), which use deh1 x1x2x3x4xT Y h2 h3 h4 � � � hT softmax g3 g3 g3 1 2 3 g2 1 g1 1 �1 �2 �3 �4T g2 2 g 2 1 g2 3 g1 3 g3 4 g2 4 g1 4 � � � � � � � � � g3 T g2 T g1 T -ft— 2328 layed connections and units operating at different timescales to improve the simple RNN, we separate the LSTM units into several groups.</context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul J Werbos. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisheng Yao</author>
<author>Baolin Peng</author>
<author>Yu Zhang</author>
<author>Dong Yu</author>
<author>Geoffrey Zweig</author>
<author>Yangyang Shi</author>
</authors>
<title>Spoken language understanding using long short-term memory neural networks.</title>
<date>2014</date>
<journal>IEEE SLT.</journal>
<contexts>
<context position="1955" citStr="Yao et al., 2014" startWordPosition="289" endWordPosition="292">he continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., *Corresponding author 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The longterm dependen</context>
</contexts>
<marker>Yao, Peng, Zhang, Yu, Zweig, Shi, 2014</marker>
<rawString>Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geoffrey Zweig, and Yangyang Shi. 2014. Spoken language understanding using long short-term memory neural networks. IEEE SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhao</author>
<author>Zhengdong Lu</author>
<author>Pascal Poupart</author>
</authors>
<title>Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070.</title>
<date>2015</date>
<contexts>
<context position="19201" citStr="Zhao et al., 2015" startWordPosition="3279" endWordPosition="3282"> trees (Socher et al., 2012). 1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/ 2330 SST-1 SST-2 QC IMDB Embedding size 100 100 100 100 hidden layer size 60 60 55 100 Initial learning rate 0.1 0.1 0.1 0.1 Regularization 10−5 10−5 10−5 10−5 Number of Groups 3 3 3 5 Table 2: Hyper-parameter settings for the LSTM and MT-LSTM. Acc.(%) • RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a. 6.3 Hyperparameters and Training In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013</context>
<context position="21923" citStr="Zhao et al., 2015" startWordPosition="3707" endWordPosition="3710">ters than the CNN based models. The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN. Moreover, MT-LSTM can not only handle short texts, but can model long texts in classification task. Documents Modeling Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents). For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans1 2 3 4 5 6 7 8 0.9 0.8 0.7 0.6 0.5 LSTM MT-LSTM 2331 Model SST-1 SST-2 QC IMDB NBOW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 - RAE (Socher et al., 2011b) 43.2 82.4 - - MV-RNN (Socher et al., 2012) 44.4 82.9 - - RNTN (Socher et al., 2013) 45.7 85.4 - - DCNN (Kalchbrenner et </context>
<context position="27759" citStr="Zhao et al., 2015" startWordPosition="4764" endWordPosition="4767">entence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well. 7 Related Work There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task. Our proposed MT-LSTM can handle short texts as well as long texts in classification task. 8 Conclusion In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multiple different timescale memories. Intuitively, MTLSTM easily carries the cru</context>
</contexts>
<marker>Zhao, Lu, Poupart, 2015</marker>
<rawString>Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
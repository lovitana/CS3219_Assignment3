<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001459">
<title confidence="0.994351">
Variable-Length Word Encodings for Neural Translation Models
</title>
<author confidence="0.987961">
Rohan Chitnis and John DeNero
</author>
<affiliation confidence="0.996262">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.996011">
{ronuchit,denero}@berkeley.edu
</email>
<sectionHeader confidence="0.993829" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976825">
Recent work in neural machine translation
has shown promising performance, but the
most effective architectures do not scale
naturally to large vocabulary sizes. We
propose and compare three variable-length
encoding schemes that represent a large
vocabulary corpus using a much smaller
vocabulary with no loss in information.
Common words are unaffected by our en-
coding, but rare words are encoded us-
ing a sequence of two pseudo-words. Our
method is simple and effective: it requires
no complete dictionaries, learning proce-
dures, increased training time, changes to
the model, or new parameters. Com-
pared to a baseline that replaces all rare
words with an unknown word symbol, our
best variable-length encoding strategy im-
proves WMT English-French translation
performance by up to 1.7 BLEU.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887516129032">
Bahdanau et al. (2014) propose a neural transla-
tion model that learns vector representations for in-
dividual words as well as word sequences. Their
approach jointly predicts a translation and a la-
tent word-level alignment for a sequence of source
words. However, the architecture of the network
does not scale naturally to large vocabularies (Jean
et al., 2014).
In this paper, we propose a novel approach to
circumvent the large-vocabulary challenge by pre-
processing the source and target word sequences,
encoding them as a longer token sequence drawn
from a small vocabulary that does not discard
any information. Common words are unaffected,
but rare words are encoded as a sequence of two
pseudo-words. The exact same learning and infer-
ence machinery applied to these transformed data
yields improved translations.
We evaluate a family of 3 different encoding
schemes based on Huffman codes. All of them
eliminate the need to replace rare words with the
unknown word symbol. Our approach is simpler
than other methods recently proposed to address
the same issue. It does not introduce new param-
eters into the model, change the model structure,
affect inference, require access to a complete dic-
tionary, or require any additional learning proce-
dures. Nonetheless, compared to a baseline system
that replaces all rare words with an unknown word
symbol, our encoding approach improves English-
French news translation by up to 1.7 BLEU.
</bodyText>
<sectionHeader confidence="0.994584" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.975969">
2.1 Neural Machine Translation
</subsectionHeader>
<bodyText confidence="0.9998052">
Neural machine translation describes approaches
to machine translation that learn from corpora in
a single integrated model that embeds words and
sentences into a vector space (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al.,
2014). We focus on one recent approach to neu-
ral machine translation, proposed by Bahdanau et
al. (2014), that predicts both a translation and its
alignment to the source sentence, though our tech-
nique is relevant to related approaches as well.
The architecture consists of an encoder and a de-
coder. The encoder receives a source sentence x
and encodes each prefix using a recurrent neural
network that recursively combines embeddings xj
for each word position j:
</bodyText>
<equation confidence="0.977995">
→− h j = f(xj, →−h j−1) (1)
</equation>
<bodyText confidence="0.9727356">
where f is a non-linear function. Reverse encod-
←−
ings h j are computed similarly to represent suf-
fixes of the sentence. These vector representa-
tions are stacked to form hj, a representation of the
</bodyText>
<page confidence="0.940232">
2088
</page>
<note confidence="0.678887">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2088–2093,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.962897666666667">
whole sentence focused on position j.
The decoder predicts each target word yi se-
quentially according to the distribution
</bodyText>
<equation confidence="0.986091">
P(yi|yi−1, ..., yi, X) = g(yi−1, si, ci) (2)
</equation>
<bodyText confidence="0.999936083333333">
where si is a hidden decoder state summarizing
the prefix of the translation generated so far, ci is
a summary of the entire input sequence, and g is
another non-linear function. Encoder and decoder
parameters are jointly optimized to maximize the
log-likelihood of a training corpus.
Depending on the approach to neural transla-
tion, c can take multiple forms. Bahdanau et al.
(2014) propose integrating an attention mechanism
in the decoder, which is trained to determine on
which portions of the source sentence to focus.
The decoder computes ci, the summarizing con-
text vector, as a convex combination of the hj.
The coefficients of this combination are propor-
tional (softmax) to an alignment model prediction
exp a(hj, si), where a is a non-linear function.
The speed of prediction scales with the output
vocabulary size, due to the denominator of Equa-
tion 2 (Jean et al., 2014). The input vocabulary size
is also a challenge for storage and learning. As a re-
sult, neural machine translation systems only con-
sider the top 30K to 100K most frequent words in
a training corpus, replacing the other words with
an unknown word symbol.
</bodyText>
<subsectionHeader confidence="0.670514">
2.2 Related Work
</subsectionHeader>
<bodyText confidence="0.999986704545455">
There has been much recent work in improving
translation quality by addressing these vocabulary
size challenges. Luong et al. (2014) describe an
approach that, similar to ours, treats the translation
system as a black box. They eliminate unknown
symbols by training the system to recognize from
where in the source text each unknown word in the
target text came, so that in a postprocessing phase,
the unknown word can be replaced by a dictionary
lookup of the corresponding source word. In con-
trast, our method does not rely on access to a com-
plete dictionary, and instead transforms the data to
allow the system itself to learn translations for even
the rare words.
Some approaches have altered the model to cir-
cumvent the expensive normalization computa-
tion, rather than applying preprocessing and post-
processing on the text. Jean et al. (2014) de-
velop an importance sampling strategy for ap-
proximating the softmax computation. Mnih and
Kavukcuoglu (2013) present a technique for ap-
proximation of the target word probability using
noise-contrastive estimation.
Sequential or hierarchical encodings of large vo-
cabularies have played an important role in recur-
rent neural network language models, primarily to
address the inference time issue of large vocabu-
laries. Mikolov et al. (2011b) describe an architec-
ture in which output word types are grouped into
classes by frequency: the network first predicts a
class, then a word in that class. Mikolov et al.
(2013) describe an encoding of the output vocabu-
lary as a binary tree. To our knowledge, hierarchi-
cal encodings have not been applied to the input
vocabulary of a machine translation system.
Other methods have also been developed to work
around large-vocabulary issues in language model-
ing. Morin and Bengio (2005), Mnih and Hinton
(2009), and Mikolov et al. (2011a) develop hierar-
chical versions of the softmax computation; Huang
et al. (2012) and Collobert and Weston (2008) re-
move the need for normalization, thus avoiding
computation of the summation term over the entire
vocabulary.
</bodyText>
<subsectionHeader confidence="0.998779">
2.3 Huffman Codes
</subsectionHeader>
<bodyText confidence="0.999941375">
An encoding can be used to represent a sequence
of tokens from a large vocabulary V using a small
vocabulary W. In the case of translation, let V be
the original corpus vocabulary, which can number
in the millions of word types in a typical corpus.
Let W be the vocabulary size of a neural transla-
tion model, typically set to a much smaller number
such as 30,000.
A deterministically invertible, variable-length
encoding maps each v E V to a sequence w E W+
such that no other v� E V is mapped to a prefix of
w. Encoding simply replaces each element of V
according to the map, and decoding is unambigu-
ous because of this prefix restriction. An encoding
can be represented as a tree in which each leaf cor-
responds to an element of V, each node contains a
symbol from W, and the encoding of any leaf is its
path from the root.
A Huffman code is an optimal encoding that
uses as few symbols from W as possible to encode
an original sequence of symbols from V. Although
binary codes are typical, W can have any size. An
optimal encoding can be found using a greedy al-
gorithm (Huffman, 1952).
</bodyText>
<page confidence="0.971106">
2089
</page>
<figure confidence="0.712663555555555">
Original Corpus
to be or not to be
take it or leave it
Repeat-All Encoding
to be or not to be
s0 be it or s0 s0 it
be s0
(take) (leave) 13 encoded tokens
Repeat-Symbol Encoding
to be or s0 s0 to be
to be s0 or s1 s1 s0 s0 s1 or s1 s1 s0 s1
s0 s1 s0 s1
(not) (it) (take) (leave) 16 encoded tokens
No-Repeats Encoding
s0 s1 or s2
t0 t1 t0 t1
(to) (not)(it)
(be) (take)(leave) 20 encoded tokens
</figure>
<figureCaption confidence="0.760532">
Figure 1: Our three encoding schemes are applied
</figureCaption>
<bodyText confidence="0.984626444444445">
to a two-sentence toy corpus for which each word
type appears one or two times, and the total vocab-
ulary size V is 7. An optimal encoding tree under
each scheme is shown for an encoded vocabulary
size W of 6. As stricter constraints are imposed on
the encoding, the encoded corpus length increases
and the number of elements of V that can be rep-
resented using a single symbol decreases. Two-
symbol encodings of rare words are underlined.
</bodyText>
<sectionHeader confidence="0.98192" genericHeader="method">
3 Variable-Length Encoding Methods
</sectionHeader>
<bodyText confidence="0.999971444444444">
We consider three different encoding schemes that
are based on Huffman codes. The encoding for a
toy corpus under each scheme is depicted in Fig-
ure 1. While a Huffman code achieves the shortest
possible encoded length using a fixed vocabulary
size W, symbols are often shared between both
common words and rare words. The variants we
consider are designed to prevent specific forms of
symbol sharing across encodings.
</bodyText>
<subsectionHeader confidence="0.998812">
3.1 Encoding Schemes
</subsectionHeader>
<bodyText confidence="0.999590771428571">
Repeat-All. The first scheme is a standard Huff-
man code. In our experiments with V ≈ 2 · 106,
W = 3 · 104, and frequencies drawn from the
WMT corpus, all words in V are encoded as either
a single symbol or two symbols of W. We denote
the single-symbol words (which have the high-
est frequency) as common, and we call the other
words rare. The Repeat-All encoding scheme has
the highest number of common words. In Fig-
ure 1, common words are represented as them-
selves. Rare words are represented by two words,
and the first is always a pseudo-word symbol intro-
duced into W of the form sX for an integer X.
Repeat-Symbol. The Repeat-Symbol encoding
scheme does not allow common-word symbols to
appear in the encoding of rare words. Instead, each
rare word is encoded as a two-symbol sequence
of the form “sX sY,” where X and Y are integers
that may be the same or different. This scheme
decreases the number of common words in order
to encode all rare words using a restricted set of
symbols. In this scheme, a common word in the
encoded vocabulary always corresponds to a com-
mon word in the original vocabulary, reducing am-
biguity of common word symbols at the expense of
increasing ambiguity of pseudo-word symbols.
No-Repeats. Our final encoding scheme, No-
Repeats, uses a different vocabulary for the first
and second symbols in each rare word. That is, rare
words are represented as “sX tY,” where X and Y
are integers that may be the same or different. In
this scheme, common words and rare words do not
share symbols, and each symbol can immediately
be identified as common, the first of a rare encod-
ing pair, or the second of a rare encoding pair.
</bodyText>
<subsectionHeader confidence="0.998365">
3.2 Symbol Counts
</subsectionHeader>
<bodyText confidence="0.99964175">
To maximize performance, it is critical to set the
number of common words (which transform to
themselves) as high as possible while satisfying
the desired total vocabulary size, counting all the
newly introduced symbols. In this section, we al-
gebraically derive this optimal number of common
words for each encoding scheme. We define the
following:
</bodyText>
<listItem confidence="0.919683">
V : Size of the original vocabulary.
W: Size of the encoded vocabulary.
C: Number of common words.
S: Number of pseudo-words of the form sX.
T: Number of pseudo-words of the form tX.
</listItem>
<equation confidence="0.945447">
to be or not it s0
s0 t0 s0 t1 or s1 t0 s0 t0 s0 t1
s2 t0 s1 t1 or s2 t1 s1 t1
t0 t1
</equation>
<page confidence="0.936277">
2090
</page>
<bodyText confidence="0.979090785714286">
We are interested in maximizing C so that total
encoding length is minimized.
Repeat-All. We would like to encode the V − C
rare words, using only W −C new symbols. To do
so, for each new symbol (non-terminal node in our
encoding tree), we have all W symbols under it in
that branch. Therefore, we maximize C satisfying
the constraint that
V − C &lt; (W − C) · W
Repeat-Symbol. Out of the V − C rare words, we
would like to pack them into a complete tree so that
they may be encoded using our remaining W − C
symbols. Therefore, we maximize C satisfying the
constraint that
</bodyText>
<equation confidence="0.505956">
V − C &lt; (W − C)2
</equation>
<bodyText confidence="0.9997745">
No-Repeats. Again, we desire to pack V − C
rare words into a complete tree where we may use
W − C symbols. To maximize C, we let 5 = T.
Because 5 + T + C = W, we have that 25 + C =
W. Therefore, we maximize C satisfying the con-
straint that
</bodyText>
<equation confidence="0.806939">
V − C &lt; (W2 C)2
2
</equation>
<sectionHeader confidence="0.995872" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999946736842105">
We trained a public implementation1 of the sys-
tem described in Bahdanau et al. (2014) on the
English-French parallel corpus from ACL WMT
2014, which contains 348M tokens. We evaluated
on news-test-2014, also from WMT 2014, which
contains 3003 sentences. All experiments used the
same learning parameters and vocabulary size of
30,000.
We constructed each encoding by the following
method. First, we used the formulas derived in the
previous section to calculate the optimal number
of common words C for each encoding scheme,
using V to be the true vocabulary size of the train-
ing corpus and W = 30, 000. We then found the
C most common words in the text and encoded
them as themselves. For the remaining rare words,
we encoded them using a distinct symbol whose
form matched the one prescribed for each encoding
scheme. The encoding was then applied separately
</bodyText>
<footnote confidence="0.354992">
1github.com/lisa-groundhog/GroundHog
</footnote>
<table confidence="0.9991082">
Encoding BLEU # Common Words
None 25.77 30,000
Repeat-All 27.45 29,940
Repeat-Symbol 26.52 28,860
No-Repeats 25.79 27,320
</table>
<tableCaption confidence="0.913587">
Table 1: BLEU scores (%) on detokenized test set
for each encoding scheme after training for 5 days.
</tableCaption>
<bodyText confidence="0.981636357142857">
to both the source text and the target text. Our en-
coding schemes all increased the total number of
tokens in the training corpus by approximately 4%.
To construct the mapping from rare words to
their 2-word encodings, we binned rare words by
frequency into branches. Thus, rare words of sim-
ilar frequency in the training corpus tended to
have encodings with the same first symbol. Simi-
larly, the standard Huffman construction algorithm
groups together rare words with similar frequen-
cies within subtrees. More intelligent heuristics for
constructing trees, such as using translation statis-
tics instead of training corpus frequency, would be
an interesting area of future work.
</bodyText>
<subsectionHeader confidence="0.811493">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999922722222222">
We used the RNNsearch-50 architecture from Bah-
danau et al. (2014) as our machine translation sys-
tem. We report results for this system alone, as well
as for each of our three encoding schemes, using
the BLEU metric (Papineni et al., 2002). Table 1
summarizes our results after training each vari-
ant for 5 days, corresponding to roughly 2 passes
through the 180K-sentence training corpus.
Alternative techniques that leverage bilingual
resources have been shown to provide larger im-
provements. Jean et al. (2014) demonstrate an im-
provement of 3.1 BLEU by using bilingual word
co-occurrence statistics in an aligned corpus to re-
place unknown word tokens. Luong et al. (2014)
demonstrate an improvement of up to 2.8 BLEU
over a series of stronger baselines using an un-
known word model that also makes predictions us-
ing a bilingual dictionary.
</bodyText>
<subsectionHeader confidence="0.982868">
4.2 Analysis
</subsectionHeader>
<bodyText confidence="0.9999255">
Our results indicate that the encoding scheme
that keeps the highest number of common words,
Repeat-All, performs best. Table 2 shows the un-
igram precision of each output. The common
word translation accuracy is higher for all encoding
schemes than for the baseline, although all preci-
</bodyText>
<page confidence="0.970372">
2091
</page>
<table confidence="0.999614">
Encoding Common Rare 1st Symbol
None 62.0 0.0 -
Repeat-All 65.8 28.0 64.8
Repeat-Symbol 65.5 16.5 24.8
No-Repeats 63.6 15.8 25.7
</table>
<tableCaption confidence="0.993891">
Table 2: Test set precision (%) on common words
</tableCaption>
<bodyText confidence="0.999779260869565">
and rare words for each encoding strategy. 1st Sym-
bol denotes the precision of the first pseudo-word
symbol in an encoded rare word.
sions are similar. Larger differences appear in the
precision of rare words. The scheme that encodes
rare words using both pseudo-words and common
words gives substantially higher rare word accu-
racy than any other approach.
The final column of Table 2 shows the unigram
precision of the first pseudo-word in an encoded
rare word. The Repeat-All scheme uses only 60
different first symbols to encode all rare words.
The other schemes require over 1,000. The fact
that Repeat-All has a constrained set of rare word
first symbols may account for its higher rare word
precision.
It is possible for the model to predict an in-
valid encoded sequence that does not correspond
to any word in the original vocabulary. However,
in our experiments, we did not observe any such
sequences in the decoding of the test set. A rea-
sonable way to deal with invalid sequences would
be to drop them from the output during decoding.
</bodyText>
<sectionHeader confidence="0.990526" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999967666666667">
We described a novel approach for encoding the
source and target text based on Huffman cod-
ing schemes, eliminating the use of the unknown
word symbol. An important continuation of our
work would be to develop heuristics for effectively
grouping “similar” words in the source and target
text, so that they tend to have encodings that share
a symbol. Even with our naive grouping by corpus
frequency, our approach offers a simple way to pre-
dict both common and rare words in a neural trans-
lation model. As a result, performance improves
by up to 1.7 BLEU. We expect that the simplic-
ity of our technique will allow for straightforward
combination with other enhancements and neural
models.
</bodyText>
<sectionHeader confidence="0.990112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99901974">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.
Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase represen-
tations using RNN encoder-decoder for statistical
machine translation. CoRR, abs/1406.1078.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the International Conference on Machine
Learning.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the Association for
Computational Linguistics.
David A. Huffman. 1952. A method for the construc-
tion of minimum-redundancy codes. Proceedings of
the IRE.
Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2014. On using very large
target vocabulary for neural machine translation.
CoRR, abs/1412.2007.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
CoRR, abs/1410.8206.
Tomás Mikolov, Anoop Deoras, Daniel Povey, Lukás
Burget, and Jan Oernocký. 2011a. Strategies for
training large scale neural network language models.
In Proceedings of ASRU.
Tomás Mikolov, S. Kombrink, L. Burget, J.H. Cer-
nocky, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Acoustics, Speech and Signal Processing.
Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Andriy Mnih and Geoffrey E. Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems.
</reference>
<page confidence="0.966602">
2092
</page>
<figure confidence="0.756633">
Frederic Morin and Yoshua Bengio. 2005. Hierarchi- uation of machine translation. In Proceedings of the
cal probabilistic neural network language model. In Association for Computational Linguistics.
AI Stats.
</figure>
<reference confidence="0.9944186">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
CoRR, abs/1409.3215.
</reference>
<page confidence="0.980664">
2093
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395265">
<title confidence="0.976019">Variable-Length Word Encodings for Neural Translation Models</title>
<author confidence="0.450973">Chitnis</author>
<affiliation confidence="0.9989585">Computer Science University of California,</affiliation>
<email confidence="0.999882">ronuchit@berkeley.edu</email>
<email confidence="0.999882">denero@berkeley.edu</email>
<abstract confidence="0.993477238095238">Recent work in neural machine translation has shown promising performance, but the most effective architectures do not scale naturally to large vocabulary sizes. We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaffected by our encoding, but rare words are encoded using a sequence of two pseudo-words. Our method is simple and effective: it requires no complete dictionaries, learning procedures, increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare with an word our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2014</date>
<location>CoRR, abs/1409.0473.</location>
<contexts>
<context position="1021" citStr="Bahdanau et al. (2014)" startWordPosition="145" endWordPosition="148">encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaffected by our encoding, but rare words are encoded using a sequence of two pseudo-words. Our method is simple and effective: it requires no complete dictionaries, learning procedures, increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU. 1 Introduction Bahdanau et al. (2014) propose a neural translation model that learns vector representations for individual words as well as word sequences. Their approach jointly predicts a translation and a latent word-level alignment for a sequence of source words. However, the architecture of the network does not scale naturally to large vocabularies (Jean et al., 2014). In this paper, we propose a novel approach to circumvent the large-vocabulary challenge by preprocessing the source and target word sequences, encoding them as a longer token sequence drawn from a small vocabulary that does not discard any information. Common </context>
<context position="2827" citStr="Bahdanau et al. (2014)" startWordPosition="432" endWordPosition="435">or require any additional learning procedures. Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves EnglishFrench news translation by up to 1.7 BLEU. 2 Background 2.1 Neural Machine Translation Neural machine translation describes approaches to machine translation that learn from corpora in a single integrated model that embeds words and sentences into a vector space (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). We focus on one recent approach to neural machine translation, proposed by Bahdanau et al. (2014), that predicts both a translation and its alignment to the source sentence, though our technique is relevant to related approaches as well. The architecture consists of an encoder and a decoder. The encoder receives a source sentence x and encodes each prefix using a recurrent neural network that recursively combines embeddings xj for each word position j: →− h j = f(xj, →−h j−1) (1) where f is a non-linear function. Reverse encod←− ings h j are computed similarly to represent suffixes of the sentence. These vector representations are stacked to form hj, a representation of the 2088 Proceedin</context>
<context position="4163" citStr="Bahdanau et al. (2014)" startWordPosition="650" endWordPosition="653">17-21 September 2015. c�2015 Association for Computational Linguistics. whole sentence focused on position j. The decoder predicts each target word yi sequentially according to the distribution P(yi|yi−1, ..., yi, X) = g(yi−1, si, ci) (2) where si is a hidden decoder state summarizing the prefix of the translation generated so far, ci is a summary of the entire input sequence, and g is another non-linear function. Encoder and decoder parameters are jointly optimized to maximize the log-likelihood of a training corpus. Depending on the approach to neural translation, c can take multiple forms. Bahdanau et al. (2014) propose integrating an attention mechanism in the decoder, which is trained to determine on which portions of the source sentence to focus. The decoder computes ci, the summarizing context vector, as a convex combination of the hj. The coefficients of this combination are proportional (softmax) to an alignment model prediction exp a(hj, si), where a is a non-linear function. The speed of prediction scales with the output vocabulary size, due to the denominator of Equation 2 (Jean et al., 2014). The input vocabulary size is also a challenge for storage and learning. As a result, neural machine</context>
<context position="12687" citStr="Bahdanau et al. (2014)" startWordPosition="2188" endWordPosition="2191">nt that V − C &lt; (W − C) · W Repeat-Symbol. Out of the V − C rare words, we would like to pack them into a complete tree so that they may be encoded using our remaining W − C symbols. Therefore, we maximize C satisfying the constraint that V − C &lt; (W − C)2 No-Repeats. Again, we desire to pack V − C rare words into a complete tree where we may use W − C symbols. To maximize C, we let 5 = T. Because 5 + T + C = W, we have that 25 + C = W. Therefore, we maximize C satisfying the constraint that V − C &lt; (W2 C)2 2 4 Experimental Results We trained a public implementation1 of the system described in Bahdanau et al. (2014) on the English-French parallel corpus from ACL WMT 2014, which contains 348M tokens. We evaluated on news-test-2014, also from WMT 2014, which contains 3003 sentences. All experiments used the same learning parameters and vocabulary size of 30,000. We constructed each encoding by the following method. First, we used the formulas derived in the previous section to calculate the optimal number of common words C for each encoding scheme, using V to be the true vocabulary size of the training corpus and W = 30, 000. We then found the C most common words in the text and encoded them as themselves.</context>
<context position="14478" citStr="Bahdanau et al. (2014)" startWordPosition="2474" endWordPosition="2478">approximately 4%. To construct the mapping from rare words to their 2-word encodings, we binned rare words by frequency into branches. Thus, rare words of similar frequency in the training corpus tended to have encodings with the same first symbol. Similarly, the standard Huffman construction algorithm groups together rare words with similar frequencies within subtrees. More intelligent heuristics for constructing trees, such as using translation statistics instead of training corpus frequency, would be an interesting area of future work. 4.1 Results We used the RNNsearch-50 architecture from Bahdanau et al. (2014) as our machine translation system. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric (Papineni et al., 2002). Table 1 summarizes our results after training each variant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus. Alternative techniques that leverage bilingual resources have been shown to provide larger improvements. Jean et al. (2014) demonstrate an improvement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to replace unknown word tokens. Luong et al.</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<location>CoRR, abs/1406.1078.</location>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6867" citStr="Collobert and Weston (2008)" startWordPosition="1093" endWordPosition="1096">ecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Huang et al. (2012) and Collobert and Weston (2008) remove the need for normalization, thus avoiding computation of the summation term over the entire vocabulary. 2.3 Huffman Codes An encoding can be used to represent a sequence of tokens from a large vocabulary V using a small vocabulary W. In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus. Let W be the vocabulary size of a neural translation model, typically set to a much smaller number such as 30,000. A deterministically invertible, variable-length encoding maps each v E V to a sequence w E W+ such that no</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6835" citStr="Huang et al. (2012)" startWordPosition="1088" endWordPosition="1091">011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Huang et al. (2012) and Collobert and Weston (2008) remove the need for normalization, thus avoiding computation of the summation term over the entire vocabulary. 2.3 Huffman Codes An encoding can be used to represent a sequence of tokens from a large vocabulary V using a small vocabulary W. In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus. Let W be the vocabulary size of a neural translation model, typically set to a much smaller number such as 30,000. A deterministically invertible, variable-length encoding maps each v E V t</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Huffman</author>
</authors>
<title>A method for the construction of minimum-redundancy codes.</title>
<date>1952</date>
<booktitle>Proceedings of the IRE.</booktitle>
<contexts>
<context position="8083" citStr="Huffman, 1952" startWordPosition="1320" endWordPosition="1321">ther v� E V is mapped to a prefix of w. Encoding simply replaces each element of V according to the map, and decoding is unambiguous because of this prefix restriction. An encoding can be represented as a tree in which each leaf corresponds to an element of V, each node contains a symbol from W, and the encoding of any leaf is its path from the root. A Huffman code is an optimal encoding that uses as few symbols from W as possible to encode an original sequence of symbols from V. Although binary codes are typical, W can have any size. An optimal encoding can be found using a greedy algorithm (Huffman, 1952). 2089 Original Corpus to be or not to be take it or leave it Repeat-All Encoding to be or not to be s0 be it or s0 s0 it be s0 (take) (leave) 13 encoded tokens Repeat-Symbol Encoding to be or s0 s0 to be to be s0 or s1 s1 s0 s0 s1 or s1 s1 s0 s1 s0 s1 s0 s1 (not) (it) (take) (leave) 16 encoded tokens No-Repeats Encoding s0 s1 or s2 t0 t1 t0 t1 (to) (not)(it) (be) (take)(leave) 20 encoded tokens Figure 1: Our three encoding schemes are applied to a two-sentence toy corpus for which each word type appears one or two times, and the total vocabulary size V is 7. An optimal encoding tree under eac</context>
</contexts>
<marker>Huffman, 1952</marker>
<rawString>David A. Huffman. 1952. A method for the construction of minimum-redundancy codes. Proceedings of the IRE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sébastien Jean</author>
<author>Kyunghyun Cho</author>
<author>Roland Memisevic</author>
<author>Yoshua Bengio</author>
</authors>
<title>On using very large target vocabulary for neural machine translation.</title>
<date>2014</date>
<location>CoRR, abs/1412.2007.</location>
<contexts>
<context position="1359" citStr="Jean et al., 2014" startWordPosition="199" endWordPosition="202"> time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU. 1 Introduction Bahdanau et al. (2014) propose a neural translation model that learns vector representations for individual words as well as word sequences. Their approach jointly predicts a translation and a latent word-level alignment for a sequence of source words. However, the architecture of the network does not scale naturally to large vocabularies (Jean et al., 2014). In this paper, we propose a novel approach to circumvent the large-vocabulary challenge by preprocessing the source and target word sequences, encoding them as a longer token sequence drawn from a small vocabulary that does not discard any information. Common words are unaffected, but rare words are encoded as a sequence of two pseudo-words. The exact same learning and inference machinery applied to these transformed data yields improved translations. We evaluate a family of 3 different encoding schemes based on Huffman codes. All of them eliminate the need to replace rare words with the unk</context>
<context position="4662" citStr="Jean et al., 2014" startWordPosition="733" endWordPosition="736">f a training corpus. Depending on the approach to neural translation, c can take multiple forms. Bahdanau et al. (2014) propose integrating an attention mechanism in the decoder, which is trained to determine on which portions of the source sentence to focus. The decoder computes ci, the summarizing context vector, as a convex combination of the hj. The coefficients of this combination are proportional (softmax) to an alignment model prediction exp a(hj, si), where a is a non-linear function. The speed of prediction scales with the output vocabulary size, due to the denominator of Equation 2 (Jean et al., 2014). The input vocabulary size is also a challenge for storage and learning. As a result, neural machine translation systems only consider the top 30K to 100K most frequent words in a training corpus, replacing the other words with an unknown word symbol. 2.2 Related Work There has been much recent work in improving translation quality by addressing these vocabulary size challenges. Luong et al. (2014) describe an approach that, similar to ours, treats the translation system as a black box. They eliminate unknown symbols by training the system to recognize from where in the source text each unkno</context>
<context position="14924" citStr="Jean et al. (2014)" startWordPosition="2547" endWordPosition="2550"> statistics instead of training corpus frequency, would be an interesting area of future work. 4.1 Results We used the RNNsearch-50 architecture from Bahdanau et al. (2014) as our machine translation system. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric (Papineni et al., 2002). Table 1 summarizes our results after training each variant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus. Alternative techniques that leverage bilingual resources have been shown to provide larger improvements. Jean et al. (2014) demonstrate an improvement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to replace unknown word tokens. Luong et al. (2014) demonstrate an improvement of up to 2.8 BLEU over a series of stronger baselines using an unknown word model that also makes predictions using a bilingual dictionary. 4.2 Analysis Our results indicate that the encoding scheme that keeps the highest number of common words, Repeat-All, performs best. Table 2 shows the unigram precision of each output. The common word translation accuracy is higher for all encoding schemes than for the b</context>
</contexts>
<marker>Jean, Cho, Memisevic, Bengio, 2014</marker>
<rawString>Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014. On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<contexts>
<context position="2685" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="407" endWordPosition="410">me issue. It does not introduce new parameters into the model, change the model structure, affect inference, require access to a complete dictionary, or require any additional learning procedures. Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves EnglishFrench news translation by up to 1.7 BLEU. 2 Background 2.1 Neural Machine Translation Neural machine translation describes approaches to machine translation that learn from corpora in a single integrated model that embeds words and sentences into a vector space (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). We focus on one recent approach to neural machine translation, proposed by Bahdanau et al. (2014), that predicts both a translation and its alignment to the source sentence, though our technique is relevant to related approaches as well. The architecture consists of an encoder and a decoder. The encoder receives a source sentence x and encodes each prefix using a recurrent neural network that recursively combines embeddings xj for each word position j: →− h j = f(xj, →−h j−1) (1) where f is a non-linear function. Reverse encod←− ings h j are compute</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2014</date>
<location>CoRR, abs/1410.8206.</location>
<contexts>
<context position="5064" citStr="Luong et al. (2014)" startWordPosition="800" endWordPosition="803">onal (softmax) to an alignment model prediction exp a(hj, si), where a is a non-linear function. The speed of prediction scales with the output vocabulary size, due to the denominator of Equation 2 (Jean et al., 2014). The input vocabulary size is also a challenge for storage and learning. As a result, neural machine translation systems only consider the top 30K to 100K most frequent words in a training corpus, replacing the other words with an unknown word symbol. 2.2 Related Work There has been much recent work in improving translation quality by addressing these vocabulary size challenges. Luong et al. (2014) describe an approach that, similar to ours, treats the translation system as a black box. They eliminate unknown symbols by training the system to recognize from where in the source text each unknown word in the target text came, so that in a postprocessing phase, the unknown word can be replaced by a dictionary lookup of the corresponding source word. In contrast, our method does not rely on access to a complete dictionary, and instead transforms the data to allow the system itself to learn translations for even the rare words. Some approaches have altered the model to circumvent the expensi</context>
<context position="15085" citStr="Luong et al. (2014)" startWordPosition="2574" endWordPosition="2577">t al. (2014) as our machine translation system. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric (Papineni et al., 2002). Table 1 summarizes our results after training each variant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus. Alternative techniques that leverage bilingual resources have been shown to provide larger improvements. Jean et al. (2014) demonstrate an improvement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to replace unknown word tokens. Luong et al. (2014) demonstrate an improvement of up to 2.8 BLEU over a series of stronger baselines using an unknown word model that also makes predictions using a bilingual dictionary. 4.2 Analysis Our results indicate that the encoding scheme that keeps the highest number of common words, Repeat-All, performs best. Table 2 shows the unigram precision of each output. The common word translation accuracy is higher for all encoding schemes than for the baseline, although all preci2091 Encoding Common Rare 1st Symbol None 62.0 0.0 - Repeat-All 65.8 28.0 64.8 Repeat-Symbol 65.5 16.5 24.8 No-Repeats 63.6 15.8 25.7 </context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2014</marker>
<rawString>Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2014. Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomás Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukás Burget</author>
<author>Jan Oernocký</author>
</authors>
<title>Strategies for training large scale neural network language models.</title>
<date>2011</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<contexts>
<context position="6219" citStr="Mikolov et al. (2011" startWordPosition="986" endWordPosition="989"> Some approaches have altered the model to circumvent the expensive normalization computation, rather than applying preprocessing and postprocessing on the text. Jean et al. (2014) develop an importance sampling strategy for approximating the softmax computation. Mnih and Kavukcuoglu (2013) present a technique for approximation of the target word probability using noise-contrastive estimation. Sequential or hierarchical encodings of large vocabularies have played an important role in recurrent neural network language models, primarily to address the inference time issue of large vocabularies. Mikolov et al. (2011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Hua</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Oernocký, 2011</marker>
<rawString>Tomás Mikolov, Anoop Deoras, Daniel Povey, Lukás Burget, and Jan Oernocký. 2011a. Strategies for training large scale neural network language models. In Proceedings of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomás Mikolov</author>
<author>S Kombrink</author>
<author>L Burget</author>
<author>J H Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="6219" citStr="Mikolov et al. (2011" startWordPosition="986" endWordPosition="989"> Some approaches have altered the model to circumvent the expensive normalization computation, rather than applying preprocessing and postprocessing on the text. Jean et al. (2014) develop an importance sampling strategy for approximating the softmax computation. Mnih and Kavukcuoglu (2013) present a technique for approximation of the target word probability using noise-contrastive estimation. Sequential or hierarchical encodings of large vocabularies have played an important role in recurrent neural network language models, primarily to address the inference time issue of large vocabularies. Mikolov et al. (2011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Hua</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomás Mikolov, S. Kombrink, L. Burget, J.H. Cernocky, and Sanjeev Khudanpur. 2011b. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomás Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6397" citStr="Mikolov et al. (2013)" startWordPosition="1017" endWordPosition="1020">14) develop an importance sampling strategy for approximating the softmax computation. Mnih and Kavukcuoglu (2013) present a technique for approximation of the target word probability using noise-contrastive estimation. Sequential or hierarchical encodings of large vocabularies have played an important role in recurrent neural network language models, primarily to address the inference time issue of large vocabularies. Mikolov et al. (2011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Huang et al. (2012) and Collobert and Weston (2008) remove the need for normalization, thus avoiding computation of the summation term over the entire vocabulary. 2.3 Huffman Codes </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6729" citStr="Mnih and Hinton (2009)" startWordPosition="1071" endWordPosition="1074">twork language models, primarily to address the inference time issue of large vocabularies. Mikolov et al. (2011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Huang et al. (2012) and Collobert and Weston (2008) remove the need for normalization, thus avoiding computation of the summation term over the entire vocabulary. 2.3 Huffman Codes An encoding can be used to represent a sequence of tokens from a large vocabulary V using a small vocabulary W. In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus. Let W be the vocabulary size of a neural translation model, typically set to a much</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2009. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5890" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="937" endWordPosition="940">n word in the target text came, so that in a postprocessing phase, the unknown word can be replaced by a dictionary lookup of the corresponding source word. In contrast, our method does not rely on access to a complete dictionary, and instead transforms the data to allow the system itself to learn translations for even the rare words. Some approaches have altered the model to circumvent the expensive normalization computation, rather than applying preprocessing and postprocessing on the text. Jean et al. (2014) develop an importance sampling strategy for approximating the softmax computation. Mnih and Kavukcuoglu (2013) present a technique for approximation of the target word probability using noise-contrastive estimation. Sequential or hierarchical encodings of large vocabularies have played an important role in recurrent neural network language models, primarily to address the inference time issue of large vocabularies. Mikolov et al. (2011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchic</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evalIlya Sutskever, Oriol Vinyals, and Quoc</title>
<date>2002</date>
<tech>CoRR, abs/1409.3215.</tech>
<contexts>
<context position="14651" citStr="Papineni et al., 2002" startWordPosition="2506" endWordPosition="2509">in the training corpus tended to have encodings with the same first symbol. Similarly, the standard Huffman construction algorithm groups together rare words with similar frequencies within subtrees. More intelligent heuristics for constructing trees, such as using translation statistics instead of training corpus frequency, would be an interesting area of future work. 4.1 Results We used the RNNsearch-50 architecture from Bahdanau et al. (2014) as our machine translation system. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric (Papineni et al., 2002). Table 1 summarizes our results after training each variant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus. Alternative techniques that leverage bilingual resources have been shown to provide larger improvements. Jean et al. (2014) demonstrate an improvement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to replace unknown word tokens. Luong et al. (2014) demonstrate an improvement of up to 2.8 BLEU over a series of stronger baselines using an unknown word model that also makes predictions using a bilingual dictionary</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evalIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. CoRR, abs/1409.3215.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
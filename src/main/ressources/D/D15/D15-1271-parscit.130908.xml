<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003694">
<title confidence="0.99681">
Adapting Coreference Resolution for Narrative Processing
</title>
<author confidence="0.991779">
Quynh Ngoc Thi Do1, Steven Bethard2, Marie-Francine Moens1
</author>
<affiliation confidence="0.9742305">
1Katholieke Universiteit Leuven, Belgium
2University of Alabama at Birmingham, United States
</affiliation>
<email confidence="0.929049">
quynhngocthi.do@cs.kuleuven.be
bethard@cis.uab.edu
sien.moens@cs.kuleuven.be
</email>
<sectionHeader confidence="0.993393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942375">
Domain adaptation is a challenge for super-
vised NLP systems because of expensive
and time-consuming manual annotated re-
sources. We present a novel method to
adapt a supervised coreference resolution
system trained on newswire to short narra-
tive stories without retraining the system.
The idea is to perform inference via an In-
teger Linear Programming (ILP) formula-
tion with the features of narratives adopted
as soft constraints. When testing on the
UMIREC1 and N22 corpora with the-state-
of-the-art Berkeley coreference resolution
system trained on OntoNotes3, our infer-
ence substantially outperforms the original
inference on the CoNLL 2011 metric.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997275">
Coreference resolution is the task of partitioning
the set of mentions of discourse referents in a text
into classes (or ‘chains’) corresponding to those
referents (Stede, 2011). To solve the problem, con-
textual and grammatical clues, as well as semantic
information and world knowledge are necessary for
either learning-based (Bengtson and Roth, 2008;
Stoyanov et al., 2010; Haghighi and Klein, 2010)
or rule-based (Haghighi and Klein, 2009; Lee et al.,
2011) coreference systems. These systems draw on
diverse information sources and complex heuristics
to resolve pronouns, model discourse, determine
anaphoricity, and identify semantically compati-
ble mentions. However, this leads to systems with
many hetorogenous parts that can be difficult to
interpret or modify.
Durrett and Klein (2013) propose a learning-
based, mention-synchronous coreference system to
</bodyText>
<footnote confidence="0.999591666666667">
1http://dspace.mit.edu/handle/1721.1/57507
2http://dspace.mit.edu/handle/1721.1/85893
3https://catalog.ldc.upenn.edu/LDC2011T03
</footnote>
<bodyText confidence="0.999945926829269">
tackle the various aspects of coreference by using
the simplest possible set of features. Its advantage
is that the system can both implicitly model impor-
tant linguistic effects and capture other patterns in
the data that are not easily teased out by hand. With
a simple set of features including head/first/last
words, preceding/following words, length, exact
string match, head match, sentence/mention dis-
tance, gender, number etc. and an efficient training
using conditional log-likelihood augmented with
a parameterized loss function optimization they
report state-of-the-art results on CoNLL 2011 data.
But while CoNLL 2011 training data
(OntoNotes) includes a few different source
domains (newswire, weblogs, etc.), we witness
significant drops in performance when systems
trained on CoNLL 2011 are applied to new target
domains such as narratives. Some linguistic effects
and patterns that are very important for the target
domain were never seen in the source domain on
which the model was trained. In such cases, when
adapting a coreference system to a new domain,
it is necessary to incorporate these more complex
linguistic features and patterns into the model.
We propose a novel method to adopt the tar-
get domain’s features to a supervised coreference
system without retraining the model. We present
a case of transferring the system of (Durrett and
Klein, 2013), which is trained on OntoNotes, to
short narrative stories. The idea is to perform infer-
ence via a linear programming formulation with the
features of narratives adopted as soft constraints.
Since the new features are incorporated only into
the linear program, there is no need to retrain the
original model. Our formulation models three phe-
nomena that are important for short narrative sto-
ries: local discourse coherence, which we model
via centering theory constraints, speaker-listener
relations, which we model via direct speech act con-
straints, and character-naming, which we model via
definite noun phrase and exact match constraints.
</bodyText>
<page confidence="0.924956">
2262
</page>
<note confidence="0.6476355">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2262–2267,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999755">
We also suggest a method to compute back pointers
(as defined in Durrett and Klein (2013)) globally.
</bodyText>
<sectionHeader confidence="0.472762" genericHeader="introduction">
2 Berkeley coreference system
</sectionHeader>
<bodyText confidence="0.999919434782608">
Given N mentions m1, ..., mN from a document
x, each mi has an associated random variable ai
taking values in the set of {1, ..., i − 1, NEW}.
This variable specifies mi’s selected antecedent or
indicates that it begins a new coreference chain.
We call ai the back pointer of mi. A setting of
all the back pointers, denoted by a = (a1, ..., an),
implies an unique set of coreference chains that
serve as the system output.
A log-linear model of the conditional distribu-
tion P(a|x) a exp Eni=1 f(i, ai, x) is used, where
f(i, ai, x) is a feature function that examines the
coreference decision ai for mi with document con-
text x. If ai = NEW, the features indicate the
suitability of the given mention to be anaphoric or
not; when ai = j for some j, the features express
aspects of the pairwise linkage, and examine rele-
vant attributes of the anaphor i or the antecedent
j. During training, the model is optimized with a
parameterized loss function. The inference is sim-
ple and efficient: because log P(a|x) decomposes
linearly over mentions, ai = argmaxaz P (ai|x)
(Durrett and Klein, 2013).
</bodyText>
<sectionHeader confidence="0.941266" genericHeader="method">
3 Computing back pointers globally
</sectionHeader>
<bodyText confidence="0.999447">
A drawback of computing each ai locally is that the
system does not take into account constraints from
mentions outside of the (mention, antecedent) pairs.
For example, given three mentions m1, m2, m3, if
the system predicts that a2 = 1 and a3 = 2 (i.e.,
that m2’s antecedent is m1 and m3’s antecedent
is m2), then m3 will be automatically inferred as
coreferent with m1. But if there is a clear clue that
m1 and m3 are not coreferent, leveraging this clue
could help avoid the error of linking m3 to m2.
In this work, we perform inference via an ILP
formulation which allows new linguistic features
and patterns over mentions – not only (mention,
antecedent) pairs – that were not part of training
the original model to be adopted as constraints of
the ILP problem.
Let U be the set of binary indicator variables
corresponding to the values assigned to the back
pointers. Specifically, uij = 1 iff ai = j and
uii = 1 iff ai = NEW.
C is the set of K binary constraint indicator vari-
ables indicating if linguistic constraints are violated.
Specifically, ck,i,j = 1 iff the linguistic constraint
Ck is violated for the back pointer uij. Each Ck is
associated with a penalty score ρk.
We aim to maximize the objective function:
</bodyText>
<equation confidence="0.998231142857143">
N i uijP(ai = j|x) − K ρkck,i,j (1)
i=1 j=1 E
k=1
Subject to:
i
`di : uij = 1
j=1
</equation>
<bodyText confidence="0.999915">
To incorporate coreference constraints, we intro-
duce V, a set of binary variables indicating if two
mentions are in the same coreference chain. For
each pair of j &lt; i, a variable vij is added to the
ILP model, where vij = 1 iff mi and mj are in the
same chain. The definition of vij in terms of uij is
encoded as the following ILP constraints:
</bodyText>
<construct confidence="0.918712">
`dj &lt; i : uii + vij &lt; 1
`dj &lt; i : uij − vij &lt; 0
`dk &lt; j &lt; i : uij + vjk − vik &lt; 1
`dk &lt; j &lt; i : uij − vjk + vik &lt; 1
`dj &lt; k &lt; i : uij + vkj − vik &lt; 1
`dj &lt; k &lt; i : uij − vkj + vik &lt; 1
</construct>
<bodyText confidence="0.999904541666667">
For long texts, to reduce the complexity of the ILP
problem, we set a threshold, windowsv, so that vij
is only available if i − windowsv &lt; j.
The framework of V variables allows corefer-
ence constraints to be adopted easily by any coref-
erence resolution system that provides scores for
each possible back pointer value. For example, con-
sider the Stanford exact string match sieve, which
“requires an exact string match between a mention
and its antecedent” (Lee et al., 2011). If we want to
encourage such matches, for each pair j &lt; i where
the two nominal mentions mi and mj have an ex-
act string match, we would introduce a constraint
indicator variable cexact,i,j and add the constraint
vij + cexact,i,j = 1 to the ILP model. The result
would be that when the exact match constraint is
violated and some vij = 0, ILP would force the cor-
responding cexact,i,j = 1 and the objective function
would be reduced by ρexact.
ILP has been used previously to enforce global
consistency in coreference resolution (Finkel and
Manning, 2008; Denis and Baldridge, 2007; Barzi-
lay and Lapata, 2006). These models were de-
signed for an all-pairs classification approach to
</bodyText>
<page confidence="0.771756">
2263
</page>
<bodyText confidence="0.9998759">
coreference resolution, and are not directly appli-
cable to the back pointer approach of (Durrett and
Klein, 2013). But the back pointer approach allows
features to be expressed more naturally using local
context, rather than requiring, for example, judg-
ments of whether two pronouns separated by many
paragraphs are coreferent. Moreover, our ILP for-
mulation is the only one to consider the problem of
adapting to another domain and incorporating new
features without retraining the original model.
</bodyText>
<sectionHeader confidence="0.641486" genericHeader="method">
4 Centering theory constraints
</sectionHeader>
<bodyText confidence="0.999840545454545">
Pronouns, in particular, have a huge effect on in-
formation flow across sentences. Since they are
almost void of meaning (only signal gender and
number of the antecedent), the discourse referent
to be picked up must be particularly salient, so that
it can be readily identified by the reader (Stede,
2011). The discourse center hypothesis (Hudson-
D’Zmura, 1988) states that at any point in discourse
understanding, there is one single entity that is the
most salient discourse referent at that point. This
referent is called the center. Centering theory is
a key element of the discourse center hypothesis
used in anaphora resolution (Grosz et al., 1995).
Beaver (2004) reformulates the centering theory in
terms of Optimality Theory (Prince and Smolensky,
2004). Six ranked constraints – Agree, Disjoint,
ProTop, FamDef, Cohere and Align – are used to
make anaphora decisions. We adopt four of these
constraints in our ILP model as follows:
Disjoint “Co-arguments of a predicate4 are dis-
joint.” For each j &lt; i such that mi and mj are
subject and object arguments of a non-reflexive
predicate, we introduce a constraint indicator vari-
able cdisjoint,i,j, and add the ILP constraint vij −
cdisjoint,i,j = 0.
ProTop “The topic of a sentence which is the en-
tity referred to in both the current and the previous
sentence, is pronominalized.” If a sentence con-
tains pronouns then at least one of its pronouns is
coreferent with a mention in the previous sentence.
For each sentence t containing pronouns, we in-
troduce a constraint indicator variable cprotop,t,t−1,
and add the ILP constraints:
</bodyText>
<equation confidence="0.840110666666667">
bi E Pt, bj E Mt−1 : vij + cprotop,t,t−1 &lt;— 1
�cprotop,t,t−1 + � vij &gt; 1
iEPt jEMt−1
</equation>
<footnote confidence="0.824562">
4A word that evokes a semantic frame (event) in a sentence.
</footnote>
<bodyText confidence="0.999525210526316">
Pt is the set of all pronouns in sentence t. Mt−1 is
the set of all mentions in sentence t − 1 5.
FamDef “No new information about the refer-
ent is provided by the definite.” We consider only
pronouns here, though the original FamDef also
includes definite descriptions and proper names
(Beaver, 2004). For each pronoun mi, we intro-
duce a constraint indicator variable cfamdef,i,i and
add the ILP constraint uii − cfamdef,i,i = 0.
Align “The topic is in subject position.” More
specifically, the topic of a sentence is pronominal-
ized and prefers the subject position over other
positions. For each sentence containing only one
pronoun mi, if the previous sentence has only one
verbal semantic frame and mj is its subject, we
introduce a constraint indicator variable calign,i,j,
and add the ILP constraint vij + calign,i,j = 1.
Note: The ProTop, FamDef and Align constraints
are not applied to sentences containing quotations.
</bodyText>
<sectionHeader confidence="0.95434" genericHeader="method">
5 Direct speech constraints
</sectionHeader>
<bodyText confidence="0.998283260869565">
Direct speech acts (with quotation marks) are de-
tected and attached to the closest verbal commu-
nication semantic frames. For each direct speech
act qt, we call the mentions mst, mot the speaker
and listener of qt if they play the subject and ob-
ject roles respectively in the semantic frame of
qt. We detect the set of subject pronouns6 inside
the quote marks of qt and name it St. The set
of all mentions that refer to the speaker of qt is
SPEAKERt = {mst} U St. For each (mi, mj) E
SPEAKERt X SPEAKERt with i &gt; j, we introduce
a constraint indicator variable csubject,i,j, and add
the ILP constraint vij + csubject,i,j = 1.
Similarly, Ot is the set of object pronouns7 in-
side the quote marks of qt. The set of all men-
tions that refer to the listener of qt is LISTENERt =
{mot}UOt. For each pair of mentions (mi, mj) E
LISTENERtXLISTENERt with i &gt; j, we introduce
a constraint indicator variable cobject,i,j, and add
the constraint vij + cobject,i,j = 1.
If a conversation is detected (a sequence of
“question” and “answer” semantic frames), the sub-
ject of the “question” is coreferent with the object
</bodyText>
<footnote confidence="0.9402398">
5We can relax the constraint by replacing Mt−1 with
Mt−1 ∪ Mt−2 ∪ Mt−3
6(“I”, “me”, “my”, “mine”, “myself”) if mat is singular or
(“we”, “us”, “our”, ”ourself”) if mat is plural
7(“you”, “your”, “yours”, “yourself”)
</footnote>
<page confidence="0.964202">
2264
</page>
<table confidence="0.999319333333333">
Method UMIREC (Tales) N2 (Hadith)
MUC BCUB CEAFE AVG MUC BCUB CEAFE AVG
ILPI with gold mentions 84.16 65.65 50.47 66.76 80.47 65.53 54.06 66.69
BER with gold mentions 80.58 60.96 42.48 61.34 76.28 62.66 45.48 61.47
ILPI with predicted mentions 73.32 59.18 37.54 56.68 66.13 62.55 40.51 56.40
BER with predicted mentions 72.71 58.12 35.76 55.53 64.87 59.60 37.96 54.14
</table>
<tableCaption confidence="0.999813">
Table 1: ILPI and BER inference results on UMIREC (Tales) and N2 (Hadith) data.
</tableCaption>
<bodyText confidence="0.998423571428572">
of the “answer” and vice versa. For each pair of di-
rect speech acts (qt, qt+1) that is a (“question”, “an-
swer”) pair, for each pair of mentions (mi, mj) E
{LISTENERt+1 X SPEAKERt} U {SPEAKERt+1 X
LISTENERt}, we introduce a constraint indicator
variable cconversation,i,j and add the ILP constraint
vij + cconversation,i,j = 1.
</bodyText>
<sectionHeader confidence="0.946881" genericHeader="method">
6 Definite noun phrase and exact match
constraints
</sectionHeader>
<bodyText confidence="0.999991647058823">
In short narrative stories, characters are frequently
named via proper names, pronouns or definite noun
phrases (Toolan, 2009). Character names are re-
peated regularly over the whole stories. A character
is often first presented as an indefinite noun phrase
(such as “a woman”), then later as a definite noun
phrase (such as “the woman”). In this work we
introduce the definite noun phrase constraint: For
each pair j &lt; i, if mj is the indefinite form and mi
is the definite form of the same noun phrase, to en-
force that mi and mj are coreferent, we introduce
a constraint indicator variable cname,i,j, and add
the ILP constraint vij +cname,i,j = 1. To boost the
identification of characters in the stories, the def-
inite noun phrase constraint is used together with
the exact match constraint (See Section 3) applied
to noun phrases and proper nouns.
</bodyText>
<sectionHeader confidence="0.997747" genericHeader="method">
7 Experiment
</sectionHeader>
<bodyText confidence="0.973028680851064">
We test our model on 30 English folktales from the
UCM/MIT Indications, Referring Expressions, and
Coreference (UMIREC) Corpus v1.1 (Finlayson
and Hervs, 2010), and 64 text stories from the Ha-
dith section of the Narrative Networks (N2) Cor-
pus (Finlayson et al., 2014). The texts are prepro-
cessed using the Stanford sentence splitter (Man-
ning et al., 2014)8 and the Berkeley coreference
system’s preprocessor. The Berkeley coreference
system is trained on OntoNotes (newswire, broad-
8If two direct speech acts enclosed in quotation marks are
adjacent and one is placed at the end of a sentence, we separate
them into two different sentences.
cast news/conversation, and web texts). We use
Gurobi9 to solve our ILP problem, and the Lund
semantic role labeler (Bj¨orkelund et al., 2009) to
detect semantic frames. Note that in our imple-
mentation, “subject” and “object” used in Sec-
tion 4 and Section 5 refer to “subject role” and
“object role” of the semantic frames respectively.
We use a separate section of the N2 corpus, the
Inspire story texts, as the held-out validation set
used for parameter tuning, resulting in windowv=
40, Psubject = Pobject = Pconversation = Pdefinite =
Pexact =Pdisjoint =1, Pprotop =0.2, Pfamdef =0.2,
Palign=0.1.
We compare our ILP inference (ILPI) to the
standard Berkeley coreference system (BER) with
both gold and predicted mentions. Table 1 shows
that our inference improves the MUC, BCUB and
CEAFE scores on both datasets, especially when
using gold mentions10. The average ILP running
times are 42.37s per UMIREC document and 22.7s
per N2 document on a Core I7 2.3 GHz quad-core
computer. Table 2 shows the effects of each con-
straint type when used alone. Surprisingly, the sim-
plest constraint type (definite &amp; exact match con-
straints) gives us the best improvement especially
in terms of CEAFE score. This may be because
definite &amp; exact match constraint links mentions
in the whole document, while the centering theory
and direct speech act constraints are more local.
And since short narrative stories often have a small
set of characters (usually represented by definite
noun phrases or proper nouns), when these charac-
ters are linked correctly, the coreference resolution
result is improved considerably.
</bodyText>
<sectionHeader confidence="0.997197" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999941333333333">
Our method provides a promising solution when
retraining a system is impossible or difficult. How-
ever, it may raise a question of the computing cost
</bodyText>
<footnote confidence="0.992020333333333">
9http://www.gurobi.com/
10Using gold mentions, our method also improves the score
on the CoNLL 2011 test set by +1.11% (AVG: 72.46).
</footnote>
<page confidence="0.945042">
2265
</page>
<table confidence="0.99973075">
Constraint MUC BCUB CEAFE AVG
Centering theory 81.15 61.80 43.01 61.99
Direct speech 81.26 62.74 42.93 62.31
Definite &amp; Exact 83.09 62.85 49.60 65.18
</table>
<tableCaption confidence="0.9402865">
Table 2: Effects of different constraints on ILP
inference on UMIREC (Tales) with gold mentions.
</tableCaption>
<bodyText confidence="0.999836864864865">
for tuning penalty scores especially with the large
number of constraints. In such these cases, dividing
the constraints into different groups where all con-
straints in the same group have the same penalty
score may help to limit the number of scores that
need to be tuned. In our case study, the system
is not very sensitive to the values of the penalty
parameters. If we set all the penalty scores to 1,
the final AVG results on UMIREC and N2 corpus
are 66.05 and 66.68 respectively11. Those scores
are a bit less than the scores obtained after tuning
parameters but still higher than the results obtained
without ILP. Regardless, it’s true that the proposed
ILP approach is not necessarily less costly in some
settings, but it can be applied to any coreference
system that provides back pointers, not just the
Berkeley one.
Instead of adopting features of the target domain
as soft constraints as in our method, one may con-
sider to use them as linguistic features and retrain
the model. A simple domain adaptation approach
by augmenting the feature space (Daum´e et al.,
2010) based on a limited set of annotated data in
the target domain might be an alternative solution.
But note that our approach does not use any anno-
tated data of the target domain. Also, an unsuper-
vised system as (Lee et al., 2011) might encode the
target domain features (exact match noun phrases,
direct speech act) as sieves (hard), but with the
soft constraints, our system is more flexible when
making global decisions.
Our approach can be applied to another target do-
main, such as bio-medical domain where we have
entities and a list of acronyms in texts. Constrain-
ing the entities with their acronyms might help to
improve the coreference resolution for bio-medical
texts.
</bodyText>
<sectionHeader confidence="0.997398" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9748505">
We have proposed a novel approach to adapt a su-
pervised coreference resolution system trained on
newswire domain to short narrative stories without
11with gold mentions
retraining the system by modeling the inference
as an ILP problem with the features of narratives
adopted as soft constraints. Three phenomena that
are important for short narrative stories: local dis-
course coherence, speaker-listener relations, and
character-naming are modeled via centering theory,
direct speech act and definite noun phrase &amp; ex-
act match constraints. We obtain promising results
when transferring the Berkeley coreference resolu-
tion trained on OntoNotes to UMIREC (Tales) and
N2 (Hadith). We find that the simplest constraints,
definite noun phrase &amp; exact match constraints, are
the most effective in our case study assuming the
gold mentions. We also suggest an approach to
compute back pointers in coreference resolution
globally.
</bodyText>
<sectionHeader confidence="0.970916" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99996825">
This work acknowledges the financial support
of the Future and Emerging Technologies (FET)
programme within the Seventh Framework Pro-
gramme for Research of the European Com-
mission, under FET-Open grant number 295703
(FET project “Machine Understanding for in-
teractive StorytElling” (MUSE) http://www.
muse-project.eu/).
</bodyText>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978207666666667">
Regina Barzilay and Mirella Lapata. 2006. Aggrega-
tion via set partitioning for natural language genera-
tion. In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ’06, pages 359–
366, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David I. Beaver. 2004. The optimization of discourse
anaphora. Linguistics and Philosophy, 27(1):3–56.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’08,
pages 294–303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
CoNLL ’09, pages 43–48, Stroudsburg, PA, USA.
ACL.
Hal Daum´e, III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
</reference>
<page confidence="0.912018">
2266
</page>
<reference confidence="0.999331036585366">
adaptation. In Proceedings of the 2010 Workshop on
Domain Adaptation for Natural Language Process-
ing, DANLP 2010, pages 53–59, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 236–243, Rochester, New York, April.
Association for Computational Linguistics.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Seattle, Washington, Oc-
tober. Association for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, Short Papers,
pages 45–48.
M.A. Finlayson and R Hervs. 2010. Ucm/mit indica-
tions, referring expressions, and co-reference corpus
v1.1 (umirec corpus). MIT CSAIL Work Product.
Mark A. Finlayson, Jeffry R. Halverson, and Steven R.
Corman. 2014. The n2 corpus: A semantically an-
notated collection of islamist extremist stories. The
9th Language Resources and Evaluation Conference
(LREC), Reykjavik, Iceland.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modelling the local co-
herence of discourse. Computational Linguistics,
21(2):203–226.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ’09, pages 1152–
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, HLT ’10,
pages 385–393, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hudson-D’Zmura. 1988. The structure of discourse
and anaphore resolution: The discourse center and
the roles of nouns and pronouns. Unpublished doc-
toral dissertation.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning:
Shared Task, CONLL Shared Task ’11, pages
28–34, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Alan Prince and Paul Smolensky. 2004. Optimality
theory: Constraint interaction in generative gram-
mar. Wiley-Blackwell.
Manfred Stede. 2011. Discourse processing. Morgan
&amp; Claypool Publishers.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Proceed-
ings of the ACL 2010 Conference Short Papers,
pages 156–161, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael J. Toolan. 2009. Narrative Progression in the
Short Story: A Corpus Stylistic Approach. John Ben-
jamins Publishing.
</reference>
<page confidence="0.992705">
2267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879606">
<title confidence="0.999816">Adapting Coreference Resolution for Narrative Processing</title>
<author confidence="0.994315">Ngoc Thi Steven Marie-Francine</author>
<affiliation confidence="0.997332">Universiteit Leuven,</affiliation>
<address confidence="0.956219">of Alabama at Birmingham, United</address>
<email confidence="0.95932">sien.moens@cs.kuleuven.be</email>
<abstract confidence="0.997391058823529">Domain adaptation is a challenge for supervised NLP systems because of expensive and time-consuming manual annotated resources. We present a novel method to adapt a supervised coreference resolution system trained on newswire to short narrative stories without retraining the system. The idea is to perform inference via an Integer Linear Programming (ILP) formulation with the features of narratives adopted as soft constraints. When testing on the and corpora with the-stateof-the-art Berkeley coreference resolution trained on our inference substantially outperforms the original inference on the CoNLL 2011 metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Aggregation via set partitioning for natural language generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>359--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8342" citStr="Barzilay and Lapata, 2006" startWordPosition="1380" endWordPosition="1384">Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact. ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original model. 4 Centering theory const</context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2006. Aggregation via set partitioning for natural language generation. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 359– 366, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David I Beaver</author>
</authors>
<title>The optimization of discourse anaphora.</title>
<date>2004</date>
<journal>Linguistics and Philosophy,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="9616" citStr="Beaver (2004)" startWordPosition="1584" endWordPosition="1585">tion flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these constraints in our ILP model as follows: Disjoint “Co-arguments of a predicate4 are disjoint.” For each j &lt; i such that mi and mj are subject and object arguments of a non-reflexive predicate, we introduce a constraint indicator variable cdisjoint,i,j, and add the ILP constraint vij − cdisjoint,i,j = 0. ProTop “The topic of a sentence which is the entity referred to</context>
<context position="10980" citStr="Beaver, 2004" startWordPosition="1821" endWordPosition="1822">h a mention in the previous sentence. For each sentence t containing pronouns, we introduce a constraint indicator variable cprotop,t,t−1, and add the ILP constraints: bi E Pt, bj E Mt−1 : vij + cprotop,t,t−1 &lt;— 1 �cprotop,t,t−1 + � vij &gt; 1 iEPt jEMt−1 4A word that evokes a semantic frame (event) in a sentence. Pt is the set of all pronouns in sentence t. Mt−1 is the set of all mentions in sentence t − 1 5. FamDef “No new information about the referent is provided by the definite.” We consider only pronouns here, though the original FamDef also includes definite descriptions and proper names (Beaver, 2004). For each pronoun mi, we introduce a constraint indicator variable cfamdef,i,i and add the ILP constraint uii − cfamdef,i,i = 0. Align “The topic is in subject position.” More specifically, the topic of a sentence is pronominalized and prefers the subject position over other positions. For each sentence containing only one pronoun mi, if the previous sentence has only one verbal semantic frame and mj is its subject, we introduce a constraint indicator variable calign,i,j, and add the ILP constraint vij + calign,i,j = 1. Note: The ProTop, FamDef and Align constraints are not applied to sentenc</context>
</contexts>
<marker>Beaver, 2004</marker>
<rawString>David I. Beaver. 2004. The optimization of discourse anaphora. Linguistics and Philosophy, 27(1):3–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>294--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1311" citStr="Bengtson and Roth, 2008" startWordPosition="178" endWordPosition="181">ratives adopted as soft constraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3ht</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 294–303, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>43--48</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, pages 43–48, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010,</booktitle>
<pages>53--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Daum´e, Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e, III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010, pages 53–59, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>236--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="8314" citStr="Denis and Baldridge, 2007" startWordPosition="1376" endWordPosition="1379">ntion and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact. ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original mod</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236–243, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, Washington,</location>
<contexts>
<context position="1754" citStr="Durrett and Klein (2013)" startWordPosition="242" endWordPosition="245">, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3https://catalog.ldc.upenn.edu/LDC2011T03 tackle the various aspects of coreference by using the simplest possible set of features. Its advantage is that the system can both implicitly model important linguistic effects and capture other patterns in the data that are not easily teased out by hand. With a simple set of features including head/first/last words, preceding/following words, length, exact string match, head match, sentence/mention </context>
<context position="3325" citStr="Durrett and Klein, 2013" startWordPosition="465" endWordPosition="468">in performance when systems trained on CoNLL 2011 are applied to new target domains such as narratives. Some linguistic effects and patterns that are very important for the target domain were never seen in the source domain on which the model was trained. In such cases, when adapting a coreference system to a new domain, it is necessary to incorporate these more complex linguistic features and patterns into the model. We propose a novel method to adopt the target domain’s features to a supervised coreference system without retraining the model. We present a case of transferring the system of (Durrett and Klein, 2013), which is trained on OntoNotes, to short narrative stories. The idea is to perform inference via a linear programming formulation with the features of narratives adopted as soft constraints. Since the new features are incorporated only into the linear program, there is no need to retrain the original model. Our formulation models three phenomena that are important for short narrative stories: local discourse coherence, which we model via centering theory constraints, speaker-listener relations, which we model via direct speech act constraints, and character-naming, which we model via definite</context>
<context position="5387" citStr="Durrett and Klein, 2013" startWordPosition="807" endWordPosition="810">ribution P(a|x) a exp Eni=1 f(i, ai, x) is used, where f(i, ai, x) is a feature function that examines the coreference decision ai for mi with document context x. If ai = NEW, the features indicate the suitability of the given mention to be anaphoric or not; when ai = j for some j, the features express aspects of the pairwise linkage, and examine relevant attributes of the anaphor i or the antecedent j. During training, the model is optimized with a parameterized loss function. The inference is simple and efficient: because log P(a|x) decomposes linearly over mentions, ai = argmaxaz P (ai|x) (Durrett and Klein, 2013). 3 Computing back pointers globally A drawback of computing each ai locally is that the system does not take into account constraints from mentions outside of the (mention, antecedent) pairs. For example, given three mentions m1, m2, m3, if the system predicts that a2 = 1 and a3 = 2 (i.e., that m2’s antecedent is m1 and m3’s antecedent is m2), then m3 will be automatically inferred as coreferent with m1. But if there is a clear clue that m1 and m3 are not coreferent, leveraging this clue could help avoid the error of linking m3 to m2. In this work, we perform inference via an ILP formulation </context>
<context position="8533" citStr="Durrett and Klein, 2013" startWordPosition="1411" endWordPosition="1414">ble cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact. ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original model. 4 Centering theory constraints Pronouns, in particular, have a huge effect on information flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, Short Papers,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="8287" citStr="Finkel and Manning, 2008" startWordPosition="1372" endWordPosition="1375"> string match between a mention and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact. ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without </context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, Short Papers, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Finlayson</author>
<author>R Hervs</author>
</authors>
<title>Ucm/mit indications, referring expressions, and co-reference corpus v1.1 (umirec corpus).</title>
<date>2010</date>
<journal>MIT CSAIL Work Product.</journal>
<contexts>
<context position="14805" citStr="Finlayson and Hervs, 2010" startWordPosition="2471" endWordPosition="2474">int: For each pair j &lt; i, if mj is the indefinite form and mi is the definite form of the same noun phrase, to enforce that mi and mj are coreferent, we introduce a constraint indicator variable cname,i,j, and add the ILP constraint vij +cname,i,j = 1. To boost the identification of characters in the stories, the definite noun phrase constraint is used together with the exact match constraint (See Section 3) applied to noun phrases and proper nouns. 7 Experiment We test our model on 30 English folktales from the UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus v1.1 (Finlayson and Hervs, 2010), and 64 text stories from the Hadith section of the Narrative Networks (N2) Corpus (Finlayson et al., 2014). The texts are preprocessed using the Stanford sentence splitter (Manning et al., 2014)8 and the Berkeley coreference system’s preprocessor. The Berkeley coreference system is trained on OntoNotes (newswire, broad8If two direct speech acts enclosed in quotation marks are adjacent and one is placed at the end of a sentence, we separate them into two different sentences. cast news/conversation, and web texts). We use Gurobi9 to solve our ILP problem, and the Lund semantic role labeler (Bj</context>
</contexts>
<marker>Finlayson, Hervs, 2010</marker>
<rawString>M.A. Finlayson and R Hervs. 2010. Ucm/mit indications, referring expressions, and co-reference corpus v1.1 (umirec corpus). MIT CSAIL Work Product.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Finlayson</author>
<author>Jeffry R Halverson</author>
<author>Steven R Corman</author>
</authors>
<title>The n2 corpus: A semantically annotated collection of islamist extremist stories.</title>
<date>2014</date>
<booktitle>The 9th Language Resources and Evaluation Conference (LREC), Reykjavik,</booktitle>
<contexts>
<context position="14913" citStr="Finlayson et al., 2014" startWordPosition="2491" endWordPosition="2494">enforce that mi and mj are coreferent, we introduce a constraint indicator variable cname,i,j, and add the ILP constraint vij +cname,i,j = 1. To boost the identification of characters in the stories, the definite noun phrase constraint is used together with the exact match constraint (See Section 3) applied to noun phrases and proper nouns. 7 Experiment We test our model on 30 English folktales from the UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus v1.1 (Finlayson and Hervs, 2010), and 64 text stories from the Hadith section of the Narrative Networks (N2) Corpus (Finlayson et al., 2014). The texts are preprocessed using the Stanford sentence splitter (Manning et al., 2014)8 and the Berkeley coreference system’s preprocessor. The Berkeley coreference system is trained on OntoNotes (newswire, broad8If two direct speech acts enclosed in quotation marks are adjacent and one is placed at the end of a sentence, we separate them into two different sentences. cast news/conversation, and web texts). We use Gurobi9 to solve our ILP problem, and the Lund semantic role labeler (Bj¨orkelund et al., 2009) to detect semantic frames. Note that in our implementation, “subject” and “object” u</context>
</contexts>
<marker>Finlayson, Halverson, Corman, 2014</marker>
<rawString>Mark A. Finlayson, Jeffry R. Halverson, and Steven R. Corman. 2014. The n2 corpus: A semantically annotated collection of islamist extremist stories. The 9th Language Resources and Evaluation Conference (LREC), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="9601" citStr="Grosz et al., 1995" startWordPosition="1580" endWordPosition="1583">uge effect on information flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these constraints in our ILP model as follows: Disjoint “Co-arguments of a predicate4 are disjoint.” For each j &lt; i such that mi and mj are subject and object arguments of a non-reflexive predicate, we introduce a constraint indicator variable cdisjoint,i,j, and add the ILP constraint vij − cdisjoint,i,j = 0. ProTop “The topic of a sentence which is the ent</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09,</booktitle>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1401" citStr="Haghighi and Klein, 2009" startWordPosition="192" endWordPosition="195">-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3https://catalog.ldc.upenn.edu/LDC2011T03 tackle the various aspects of coreference by using </context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1152– 1161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1361" citStr="Haghighi and Klein, 2010" startWordPosition="186" endWordPosition="189">g on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3https://catalog.ldc.upenn.edu/LDC2011T03 tackle the </context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 385–393, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hudson-D’Zmura</author>
</authors>
<title>The structure of discourse and anaphore resolution: The discourse center and the roles of nouns and pronouns. Unpublished doctoral dissertation.</title>
<date>1988</date>
<marker>Hudson-D’Zmura, 1988</marker>
<rawString>Hudson-D’Zmura. 1988. The structure of discourse and anaphore resolution: The discourse center and the roles of nouns and pronouns. Unpublished doctoral dissertation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, CONLL Shared Task ’11,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1420" citStr="Lee et al., 2011" startWordPosition="196" endWordPosition="199">coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3https://catalog.ldc.upenn.edu/LDC2011T03 tackle the various aspects of coreference by using the simplest possib</context>
<context position="7733" citStr="Lee et al., 2011" startWordPosition="1273" endWordPosition="1276">ij &lt; 0 `dk &lt; j &lt; i : uij + vjk − vik &lt; 1 `dk &lt; j &lt; i : uij − vjk + vik &lt; 1 `dj &lt; k &lt; i : uij + vkj − vik &lt; 1 `dj &lt; k &lt; i : uij − vkj + vik &lt; 1 For long texts, to reduce the complexity of the ILP problem, we set a threshold, windowsv, so that vij is only available if i − windowsv &lt; j. The framework of V variables allows coreference constraints to be adopted easily by any coreference resolution system that provides scores for each possible back pointer value. For example, consider the Stanford exact string match sieve, which “requires an exact string match between a mention and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact. ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapa</context>
<context position="18744" citStr="Lee et al., 2011" startWordPosition="3125" endWordPosition="3128">ly less costly in some settings, but it can be applied to any coreference system that provides back pointers, not just the Berkeley one. Instead of adopting features of the target domain as soft constraints as in our method, one may consider to use them as linguistic features and retrain the model. A simple domain adaptation approach by augmenting the feature space (Daum´e et al., 2010) based on a limited set of annotated data in the target domain might be an alternative solution. But note that our approach does not use any annotated data of the target domain. Also, an unsupervised system as (Lee et al., 2011) might encode the target domain features (exact match noun phrases, direct speech act) as sieves (hard), but with the soft constraints, our system is more flexible when making global decisions. Our approach can be applied to another target domain, such as bio-medical domain where we have entities and a list of acronyms in texts. Constraining the entities with their acronyms might help to improve the coreference resolution for bio-medical texts. 9 Conclusion We have proposed a novel approach to adapt a supervised coreference resolution system trained on newswire domain to short narrative storie</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, CONLL Shared Task ’11, pages 28–34, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="15001" citStr="Manning et al., 2014" startWordPosition="2505" endWordPosition="2509">i,j, and add the ILP constraint vij +cname,i,j = 1. To boost the identification of characters in the stories, the definite noun phrase constraint is used together with the exact match constraint (See Section 3) applied to noun phrases and proper nouns. 7 Experiment We test our model on 30 English folktales from the UCM/MIT Indications, Referring Expressions, and Coreference (UMIREC) Corpus v1.1 (Finlayson and Hervs, 2010), and 64 text stories from the Hadith section of the Narrative Networks (N2) Corpus (Finlayson et al., 2014). The texts are preprocessed using the Stanford sentence splitter (Manning et al., 2014)8 and the Berkeley coreference system’s preprocessor. The Berkeley coreference system is trained on OntoNotes (newswire, broad8If two direct speech acts enclosed in quotation marks are adjacent and one is placed at the end of a sentence, we separate them into two different sentences. cast news/conversation, and web texts). We use Gurobi9 to solve our ILP problem, and the Lund semantic role labeler (Bj¨orkelund et al., 2009) to detect semantic frames. Note that in our implementation, “subject” and “object” used in Section 4 and Section 5 refer to “subject role” and “object role” of the semantic</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality theory: Constraint interaction in generative grammar.</title>
<date>2004</date>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="9709" citStr="Prince and Smolensky, 2004" startWordPosition="1595" endWordPosition="1598">gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these constraints in our ILP model as follows: Disjoint “Co-arguments of a predicate4 are disjoint.” For each j &lt; i such that mi and mj are subject and object arguments of a non-reflexive predicate, we introduce a constraint indicator variable cdisjoint,i,j, and add the ILP constraint vij − cdisjoint,i,j = 0. ProTop “The topic of a sentence which is the entity referred to in both the current and the previous sentence, is pronominalized.” If a sentence contains pr</context>
</contexts>
<marker>Prince, Smolensky, 2004</marker>
<rawString>Alan Prince and Paul Smolensky. 2004. Optimality theory: Constraint interaction in generative grammar. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Discourse processing.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1137" citStr="Stede, 2011" startWordPosition="154" endWordPosition="155">hort narrative stories without retraining the system. The idea is to perform inference via an Integer Linear Programming (ILP) formulation with the features of narratives adopted as soft constraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett</context>
<context position="9253" citStr="Stede, 2011" startWordPosition="1527" endWordPosition="1528">han requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original model. 4 Centering theory constraints Pronouns, in particular, have a huge effect on information flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these const</context>
</contexts>
<marker>Stede, 2011</marker>
<rawString>Manfred Stede. 2011. Discourse processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Coreference resolution with reconcile.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>156--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1334" citStr="Stoyanov et al., 2010" startWordPosition="182" endWordPosition="185">onstraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1http://dspace.mit.edu/handle/1721.1/57507 2http://dspace.mit.edu/handle/1721.1/85893 3https://catalog.ldc.upenn</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coreference resolution with reconcile. In Proceedings of the ACL 2010 Conference Short Papers, pages 156–161, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Toolan</author>
</authors>
<title>Narrative Progression in the Short Story: A Corpus Stylistic Approach.</title>
<date>2009</date>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="13909" citStr="Toolan, 2009" startWordPosition="2321" endWordPosition="2322">5.53 64.87 59.60 37.96 54.14 Table 1: ILPI and BER inference results on UMIREC (Tales) and N2 (Hadith) data. of the “answer” and vice versa. For each pair of direct speech acts (qt, qt+1) that is a (“question”, “answer”) pair, for each pair of mentions (mi, mj) E {LISTENERt+1 X SPEAKERt} U {SPEAKERt+1 X LISTENERt}, we introduce a constraint indicator variable cconversation,i,j and add the ILP constraint vij + cconversation,i,j = 1. 6 Definite noun phrase and exact match constraints In short narrative stories, characters are frequently named via proper names, pronouns or definite noun phrases (Toolan, 2009). Character names are repeated regularly over the whole stories. A character is often first presented as an indefinite noun phrase (such as “a woman”), then later as a definite noun phrase (such as “the woman”). In this work we introduce the definite noun phrase constraint: For each pair j &lt; i, if mj is the indefinite form and mi is the definite form of the same noun phrase, to enforce that mi and mj are coreferent, we introduce a constraint indicator variable cname,i,j, and add the ILP constraint vij +cname,i,j = 1. To boost the identification of characters in the stories, the definite noun p</context>
</contexts>
<marker>Toolan, 2009</marker>
<rawString>Michael J. Toolan. 2009. Narrative Progression in the Short Story: A Corpus Stylistic Approach. John Benjamins Publishing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
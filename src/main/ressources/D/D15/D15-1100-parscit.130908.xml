<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001119">
<title confidence="0.992921">
A Framework for Comparing Groups of Documents
</title>
<author confidence="0.547618">
Arun S. Maiya
</author>
<affiliation confidence="0.513721">
Institute for Defense Analyses — Alexandria, VA, USA
</affiliation>
<email confidence="0.931669">
amaiya@ida.org
</email>
<sectionHeader confidence="0.992639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982333333333">
We present a general framework for com-
paring multiple groups of documents. A
bipartite graph model is proposed where
document groups are represented as one
node set and the comparison criteria are
represented as the other node set. Using
this model, we present basic algorithms to
extract insights into similarities and differ-
ences among the document groups. Fi-
nally, we demonstrate the versatility of
our framework through an analysis of NSF
funding programs for basic research.
</bodyText>
<sectionHeader confidence="0.985966" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999975444444444">
Given multiple sets (or groups) of documents, it is
often necessary to compare the groups to identify
similarities and differences along different dimen-
sions. In this work, we present a general frame-
work to perform such comparisons for extraction
of important insights. Indeed, many real-world
tasks can be framed as a problem of comparing
two or more groups of documents. Here, we pro-
vide two motivating examples.
</bodyText>
<listItem confidence="0.701421">
1. Program Reviews. To better direct research
</listItem>
<bodyText confidence="0.995458960784314">
efforts, funding organizations such as the National
Science Foundation (NSF), the National Institutes
of Health (NIH), and the Department of Defense
(DoD), are often in the position of reviewing re-
search programs via their artifacts (e.g., grant ab-
stracts, published papers, and other research de-
scriptions). Such reviews might involve identify-
ing overlaps across different programs, which may
indicate a duplication of effort. It may also involve
the identification of unique, emerging, or dimin-
ishing topics. A “document group” here could be
defined either as a particular research program that
funds many organizations, the totality of funded
research conducted by a specific organization, or
all research associated with a particular time pe-
riod (e.g., fiscal year). In all cases, the objective is
to draw comparisons between groups by compar-
ing the document sets associated with them.
2. Intelligence. In the areas of defense and in-
telligence, document sets are sometimes obtained
from different sources or entities. For instance, the
U.S. Armed Forces sometimes seize documents
during raids of terrorist strongholds.1 Similarities
between two document sets (each captured from a
different source) can potentially be used to infer a
non-obvious association between the sources.
Of course, there are numerous additional examples
across many domains (e.g., comparing different
news sources, comparing the reviews for several
products, etc.). Given the abundance of real-world
applications as illustrated above, it is surprising,
then, that there are no existing general-purpose ap-
proaches for drawing such comparisons. While
there is some previous work on the comparison
of document sets (referred to as comparative text
mining), these existing approaches lack the gener-
ality to be widely applicable across different use
case scenarios with different comparison criteria.
Moreover, much of the work in the area focuses
largely on the summarization of shared or un-
shared topics among document groups (e.g., Wan
et al. (2011), Huang et al. (2011), Campr and
Jeˇzek (2013), Wang et al. (2012), Zhai et al.
(2004)). That is, the problem of drawing multi-
faceted comparisons among the groups themselves
is not typically addressed. This, then, motivates
our development of a general-purpose model for
comparisons of document sets along arbitrary di-
mensions. We use this model for the identification
of similarities, differences, trends, and anomalies
among large groups of documents. We begin by
</bodyText>
<footnote confidence="0.960502">
1http://en.wikipedia.org/wiki/
Document_Exploitation_(DOCEX)
</footnote>
<page confidence="0.904375">
840
</page>
<note confidence="0.6779325">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 840–845,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.984335">
formally describing our model.
</bodyText>
<sectionHeader confidence="0.994274" genericHeader="method">
2 Our Formal Model for
</sectionHeader>
<subsectionHeader confidence="0.922112">
Comparing Document Groups
</subsectionHeader>
<bodyText confidence="0.948553352941176">
As input, we are given several groups of doc-
uments, and our task is to compare them. We
now formally define these document groups and
the criteria used to compare them. Let D =
{d1, d2, ... , dN} be a document collection com-
prising the totality of documents under considera-
tion, where N is the size. Let DP be a partition of
D representing the document groups.
Definition 1 A document group is a subset DPi E
DP (where index i E {1 ... |DP |}).
Each document group in DP, for instance,
might represent articles associated with either
a particular organization (e.g., university), a re-
search funding source (e.g., NSF or DARPA pro-
gram), or a time period (e.g., a fiscal year). Docu-
ment groups are compared using comparison cri-
teria, DC, a family of subsets of D.
</bodyText>
<construct confidence="0.7615005">
Definition 2 A comparison criterion is a subset
DCi E DC (where index i E {1... |DC|}).
</construct>
<bodyText confidence="0.9956446">
Intuitively, each subset of DC represents a set
of documents sharing some attribute. Our model
allows great flexibility in how DC is defined. For
instance, DC might be defined by the named en-
tities mentioned within documents (e.g., each sub-
set contains documents that mention a particular
person or organization of interest). For the present
work, we define DC by topics discovered using la-
tent Dirichlet allocation or LDA (Blei et al., 2003).
LDA Topics as Comparison Criteria. Proba-
bilistic topic modeling algorithms like LDA dis-
cover latent themes (i.e., topics) in document col-
lections. By using these discovered topics as
the comparison criteria, we can compare arbitrary
groups of documents by the themes and subject
areas comprising them. Let K be the number
of topics or themes in D. Each document in
D is composed of a sequence of words: di =
(si1, si2, ... , siNi), where Ni is the number of
words in di and i E {1... N}. V = UN i=1 f(di) is
the vocabulary of D, where f(�) takes a sequence
of elements and returns a set. LDA takes K and
D (including its components such as V ) as input
and produces two matrices as output, one of which
is θ. The matrix θ E RN×K is the document-
topic distribution matrix and shows the distribu-
tion of topics within each document. Each row
of the matrix represents a probability distribution.
DC is constructed using K subsets of documents,
each of which represent a set of documents per-
taining largely to the same topic. That is, for
t E {1... K} and i E {1... N}, each subset
DC t E DC is comprised of all documents di where
t = argmaxx θix.2 Having defined the document
groups DP and the comparison criteria DC, we
now construct a bipartite graph model used to per-
form comparisons.
A Bipartite Graph Model. Our objective is to
compare the document groups in DP based on
DC. We do so by representing DP and DC as
a weighted bipartite graph, G = (P, C, E, w),
where P and C are disjoint sets of nodes, E is the
edge set, and w : E —* Z+ are the edge weights.
Each subset of DP is represented as a node in P,
and each subset of DC is represented as a node
in C. Let α : P —* DP and β : C —* DC
be functions that map nodes to the document sub-
sets that they represent. Then, the edge set E is
{(u, v)  |u E P, v E C, α(u)nβ(v) =� 0}, and the
edge weight for any two nodes u E P and v E C
is w((u,v)) = |α(u) n β(v)|. Concisely, each
weighted edge in G between a document group
(in P) and a topic (in C) represents the number
of documents shared among the two sets. Fig-
ure 1 shows a toy illustration of the model. Each
node in P is shown in black and represents a sub-
set of DP (i.e., a document group). Each node in
C is shown in gray and represents a subset of DC
(i.e., a document cluster pertaining primarily to the
same topic). Each edge represents the intersection
of the two subsets it connects. In the next section,
we will describe basic algorithms on such bipartite
graphs capable of yielding important insights into
the similarities and differences among document
groups.
</bodyText>
<sectionHeader confidence="0.95108" genericHeader="method">
3 Basic Algorithms Using the Model
</sectionHeader>
<bodyText confidence="0.999200125">
We focus on three basic operations in this work.
Node Entropy. Let w� be a vector of weights for
all edges incident to some node v E E. The en-
tropy H of v is: H(v) = − Ei pi log|~w|(pi), where
pi = Ewwj and i, j E {1... |w|}. A similar for-
mulation was employed in Eagle et al. (2010). In-
tuitively, if v E P, H(v) measures the extent to
which the document group is concentrated around
</bodyText>
<footnote confidence="0.606669">
2 DC is also a partition of D, when defined in this way.
</footnote>
<page confidence="0.994209">
841
</page>
<figureCaption confidence="0.96894525">
Figure 1: [Toy Illustration of Bipartite Graph Model.]
Each black node (i.e., node ∈ P) represents a document
group. Each gray node (i.e., node ∈ C) represents a clus-
ter of documents pertaining primarily to the same topic.
</figureCaption>
<bodyText confidence="0.989710647058824">
a small number of topics (lower values of H(v)
mean more concentrated). Similarly, if v E C, it is
the extent to which a topic is concentrated around
a small number of document groups.
Node Similarity. Given a graph G, there are many
ways to measure the similarity of two nodes based
on their connections. Such measures can be used
to infer similarity (and dissimilarity) among doc-
ument groups. However, existing methods are not
well-suited for the task of document group com-
parison. The well-known SimRank algorithm (Jeh
and Widom, 2002) ignores edge weights, and nei-
ther SimRank nor its extension, SimRank++ (An-
tonellis et al., 2008), scale to larger graphs. Sim-
Rank++ and ASCOS (Chen and Giles, 2013) do
incorporate edge weights but in ways that are
not appropriate for document group comparisons.
For instance, both SimRank++ and ASCOS in-
corporate magnitude in the similarity computa-
tion. Consider the case where document groups
are defined as research labs. ASCOS and Sim-
Rank++ will measure large research labs and small
research labs as less similar when in fact they may
publish nearly identical lines of research. Finally,
under these existing methods, document groups
sharing zero topics in common could still be con-
sidered similar, which is undesirable here. For
these reasons, we formulate similarity as follows.
Let NG(·) be a function that returns the neighbors
of a given node in G. Given two nodes u, v E P,
let Lu,v = NG(u) U NG(v) and let x : I -* Lu,v
be the indexing function for Lu,v.3 We construct
two vectors, a� and b, where ak = w(u, x(k)),
bk = w(v, x(k)), and k E I. Each vector is es-
</bodyText>
<footnote confidence="0.779661">
3I is the index set of Lu,v.
</footnote>
<bodyText confidence="0.981596894736842">
sentially a sequence of weights for edges between
u, v E P and each node in Lu,v. Similarity of two
nodes is measured using the cosine similarity of
~a·~b which we
b11,
compute using a function sim(·, ·). Thus, doc-
ument groups are considered more similar when
they have similar sets of topics in similar propor-
tions. As we will show later, this simple solution,
based on item-based collaborative filtering (Sar-
war et al., 2001), is surprisingly effective at infer-
ring similarity among document groups in G.
Node Clusters. Identifying clusters of related
nodes in the bipartite graph G can show how doc-
ument groups form larger classes. However, we
find that G is typically fairly dense. For these
reasons, partitioning of the one-mode projection
of G and other standard bipartite graph cluster-
ing techniques (e.g., Dhillion (2001) and Sun et
al. (2009)) are rendered less effective. We instead
employ a different tack and exploit the node sim-
ilarities computed earlier. We transform G into a
new weighted graph GP = (P, EP, wsim) where
EP = {(u, v)  |u, v E P, sim(u, v) &gt; ξJ, ξ
is a pre-defined threshold, and wsim is the edge
weight function (i.e., wsim = sim). Thus, GP is
the similarity graph of document groups. ξ = 0.5
was used as the threshold for our analyses. To
find clusters in GP, we employ the Louvain al-
gorithm, a heuristic method based on modularity
optimization (Blondel et al., 2008). Modularity
measures the fraction of edges falling within clus-
ters as compared to the expected fraction if edges
were distributed evenly in the graph (Newman,
2006). The algorithm initially assigns each node
to its own cluster. At each iteration, in a local and
greedy fashion, nodes are re-assigned to clusters
with which they achieve the highest modularity.
</bodyText>
<sectionHeader confidence="0.995727" genericHeader="method">
4 Example Analysis: NSF Grants
</sectionHeader>
<bodyText confidence="0.999757111111111">
As a realistic and informative case study, we uti-
lize our model to characterize funding programs
of the National Science Foundation (NSF). This
corpus consists of 132,372 grant abstracts describ-
ing awards for basic research and other support
funded by the NSF between the years 1990 and
2002 (Bache and Lichman, 2013).4 Each award is
associated with both a program element (i.e., fund-
ing source) and a date. We define document
</bodyText>
<footnote confidence="0.754763">
4Data for years 1989 and 2003 in this publicly available
corpus were partially missing and omitted in some analyses.
</footnote>
<figure confidence="0.978793588235294">
Document Group IDPI
Document Group 1
Document Group 2
Document Group 3
Document Group 4
Document Groups (DP) Topics (Dc)
�
�
TopicIDcI
Topic 1
Topic 2
Topic 3
Topic 4
Topic 5
Topic 6
their corresponding sequences,
11~a1111
</figure>
<page confidence="0.984747">
842
</page>
<bodyText confidence="0.999603448275862">
groups in two ways: by program element and by
calendar year. For comparison criteria, we used
topics discovered with the MALLET implementa-
tion of LDA (McCallum, 2002) using K = 400 as
the number of topics and 200 as the number of iter-
ations. All other parameters were left as defaults.
The NSF corpus possesses unique properties that
lend themselves to experimental evaluation. For
instance, program elements are not only associ-
ated with specific sets of research topics but are
named based on the content of the program. This
provides a measure of ground truth against which
we can validate our model. We structure our anal-
yses around specific questions, which now follow.
Which ISF programs are focused on specific
areas and which are not? When defining doc-
ument groups as program elements (i.e., each NSF
program is a node in P), node entropy can be
used to answer this question. Table 1 shows ex-
amples of program elements most and least as-
sociated with specific topics, as measured by en-
tropy. For example, the program 1311 Linguistics
(low entropy) is largely focused on a single lin-
guistics topic (labeled by LDA with words such
as “language,” “languages,” and “linguistic”). By
contrast, the Australia program (high entropy) was
designed to support US-Australia cooperative re-
search across many fields, as correctly inferred by
our model.
</bodyText>
<table confidence="0.999398625">
Low Entropy Program Elements
Program Primary LDA Topic
1311 Linguistics language languages linguistic
4091 Network Infrastructure network connection internet
High Entropy Program Elements
Program Primary LDA Topic
5912 Australia (many topics &amp; disciplines)
9130 Research in Minority Instit. (many topics &amp; disciplines)
</table>
<tableCaption confidence="0.999811">
Table 1: [Examples of High/Low Entropy Programs.]
</tableCaption>
<bodyText confidence="0.993379769230769">
Which research areas are growing/emerging?
When defining document groups as calendar years
(instead of program elements), low entropy nodes
in C are topics concentrated around certain years.
Concentrations in later years indicate growth. The
LDA-discovered topic nanotechnology is among
the lowest entropy topics (i.e., an outlier topic with
respect to entropy). As shown in Figure 2, the
number of nanotechnology grants drastically in-
creased in proportion through 2002. This result is
consistent with history, as the National Nanotech-
nology Initiative was proposed in the late 1990s to
promote nanotechnology R&amp;D.5 One could also
</bodyText>
<footnote confidence="0.841875">
5http://en.wikipedia.org/wiki/
</footnote>
<bodyText confidence="0.952819666666667">
measure such trends using budget allocations by
incorporating the award amounts into the edge
weights of G.
</bodyText>
<figureCaption confidence="0.991093666666667">
Figure 2: [Uptrend in Ianotechnology.] Our model cor-
rectly identifies the surge in nanotechnology R&amp;D beginning
in the late 1990s.
</figureCaption>
<bodyText confidence="0.9993113125">
Given an ISF program, to which other pro-
grams is it most similar? As described in Section
3, when each node in P represents an NSF pro-
gram, our model can easily identify the programs
most similar to a given program. For instance, Ta-
ble 2 shows the top three most similar programs
to both the Theoretical Physics and Ecology pro-
grams. Results agree with intuition. For each
NSF program, we identified the top n most sim-
ilar programs ranked by our sim(·, ·) function,
where n E 13, 6, 9}. These programs were man-
ually judged for relatedness, and the Mean Av-
erage Precision (MAP), a standard performance
metric for ranking tasks in information retrieval,
was computed. We were unsuccessful in evaluat-
ing alternative weighted similarity measures men-
tioned in Section 3 due to their aforementioned
issues with scalability and the size of the NSF
dataset. (For instance, the implementations of AS-
COS (Antonellis et al., 2008) and SimRank (Jeh
and Widom, 2002) that we considered are avail-
able here.6) Recall that our sim(·, ·) function is
based on measuring the cosine similarity between
two weight vectors, a� and b, generated from our
bipartite graph model. As a baseline for compar-
ison, we evaluated two additional similarity im-
plementations using these weight vectors. The
first measures the similarity between weight vec-
tors using weighted Jaccard similarity, which is
Ek min(ak,bk) (denoted as Wtd. Jaccard). The sec-
ond measure is implemented by taking the Spear-
man’s rank correlation coefficient of a� and b� (de-
</bodyText>
<footnote confidence="0.922008666666667">
National_Nanotechnology_Initiative
6https://github.com/hhchen1105/
networkx_addon
</footnote>
<figure confidence="0.9935418">
Percentage of Total Grants 1.0 Topic: Nanotechnology
0.8
0.6
0.4
0.2
0.0
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
Ek max(ak,bk)
</figure>
<page confidence="0.998184">
843
</page>
<bodyText confidence="0.992971375">
noted as Rank). Figure 3 shows the Mean Average
Precision (MAP) for each method and each value
of n. With the exception of the difference between
Cosine and Wtd. Jaccard for MAP@3, all other
performance differentials were statistically signif-
icant, based on a one-way ANOVA and post-hoc
Tukey HSD at a 5% significance level. This, then,
provides some validation for our choice.
</bodyText>
<table confidence="0.96728075">
1245 Theoretical Physics 1182 Ecology
1286 Elementary Particle Theory 1128 Ecological Studies
1287 Mathematical Physics 1196 Environmental Biology
1284 Atomic Theory 1195 Ecological Research
</table>
<tableCaption confidence="0.7959265">
Table 2: [Similarity Queries.] Three most similar pro-
grams to the Theoretical Physics and Ecology programs.
</tableCaption>
<figureCaption confidence="0.9934975">
Figure 3: [Mean Average Precision (MAP).] Cosine sim-
ilarity outperforms alternative approaches.
</figureCaption>
<bodyText confidence="0.99614625">
How do NSF programs join together to form
larger program categories? As mentioned, by
using the similarity graph GP constructed from G,
clusters of related NSF programs can be discov-
ered. Figure 4, for instance, shows a discovered
cluster of NSF programs all related to the field of
neuroscience. Each NSF program (i.e., node) is
composed of many documents.
</bodyText>
<figureCaption confidence="0.9775815">
Figure 4: [Neuroscience Programs.] A discovered cluster
of program elements all related to neuroscience.
</figureCaption>
<bodyText confidence="0.998577647058823">
Which pairs of grants are the most similar in
the research they describe? Although the focus
of this paper is on drawing comparisons among
groups of documents, it is often necessary to
draw comparisons among individual documents,
as well. For instance, one may wish to identify
pairs of grants from different programs describing
highly similar lines of research. One common ap-
proach to this is to measure the similarity among
low-dimensional representations of documents re-
turned by LDA (Blei et al., 2003). We employ
the Hellinger distance metric for this. Unfortu-
nately, identifying the set of most similar docu-
ment pairs in this way can be computationally ex-
pensive, as the number of pairwise comparisons
scales quadratically with the size of the corpus. To
address this, our bipartite graph model can be ex-
ploited as a blocking heuristic using either the doc-
ument groups or the comparison criteria. In the
latter case, one can limit the pairwise comparisons
to only those documents that reside in the same
subset of DC. For the former case, node similar-
ity can be used. Instead of comparing each docu-
ment with every other document, we can limit the
comparisons to only those document groups of in-
terest that are deemed similar by our model. As
an illustrative example, the program 1271 Compu-
tational Mathematics and the program 2865 Nu-
meric, Symbolic, and Geometric Computation are
inferred as being highly similar. Between these
groups, the following two grants are easily iden-
tified as being the most similar with a Hellinger
similarity score of 0.73 (only titles are shown due
to space constraints):
</bodyText>
<listItem confidence="0.8317842">
• Grant #1: Analyses of Structured Computational
Problems and Parallel Iterative Algorithms
(Discusses parallel iterative methods for solutions to
large sparse/dense systems of linear equations.)
• Grant #2: Sparse Matrix Algorithms on Distributed
</listItem>
<subsectionHeader confidence="0.845614">
Memory Multiprocessors
</subsectionHeader>
<bodyText confidence="0.9953918">
As can be seen, despite some differences in ter-
minology, the two lines of research are related, as
matrices (studied in Grant #2) are used to com-
pactly represent and work with systems of linear
equations (studied in Grant #1).
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948545454545">
We have presented a bipartite graph model for
drawing comparisons among large groups of docu-
ments. We showed how basic algorithms using the
model can identify trends and anomalies among
the document groups. As an example analysis, we
demonstrated how our model can be used to better
characterize and evaluate NSF research programs.
For future work, we plan on employing alterna-
tive comparison criteria in our model such as those
derived from named entity recognition and para-
phrase detection.
</bodyText>
<figure confidence="0.999058842105263">
Mean Average Pre�lslon
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
MAP@3 MAP@6 MAP@9
MAP for
Similarity
Measures
Cosine
Wtd. Jaccard
Rank
</figure>
<page confidence="0.99371">
844
</page>
<sectionHeader confidence="0.989221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998914">
Ioannis Antonellis, Hector G. Molina, and Chi C.
Chang. 2008. Simrank++: Query Rewriting
Through Link Analysis of the Click Graph. Proc.
VLDB Endow., 1(1):408–421, August.
K. Bache and M. Lichman. 2013. UCI machine learn-
ing repository.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3(4-5):993–1022, March.
Vincent D. Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008+, July.
Michal Campr and Karel Jeˇzek. 2013. Topic Mod-
els for Comparative Summarization. In Ivan Haber-
nal and V´aclav Matouˇsek, editors, Text, Speech, and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 568–574. Springer Berlin Hei-
delberg.
Hung H. Chen and C. Lee Giles. 2013. ASCOS: An
Asymmetric Network Structure COntext Similarity
Measure. In Proceedings of the 2013 IEEE/ACM In-
ternational Conference on Advances in Social Net-
works Analysis and Mining, ASONAM ’13, pages
442–449, New York, NY, USA. ACM.
Inderjit S. Dhillion. 2001. Co-clustering Documents
and Words Using Bipartite Spectral GraphPartition-
ing. Technical report, Austin, TX, USA.
Nathan Eagle, Michael Macy, and Rob Claxton. 2010.
Network diversity and economic development. Sci-
ence, 328(5981):1029–1031, May.
Xiaojiang Huang, Xiaojun Wan, and Jianguo Xiao.
2011. Comparative News Summarization Using
Linear Programming. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ’11, pages 648–653,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Glen Jeh and Jennifer Widom. 2002. SimRank: a
measure of structural-context similarity. In Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ’02, pages 538–543, New York, NY, USA.
ACM.
Andrew K. McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit.
M. E. J. Newman. 2006. Modularity and community
structure in networks. Proceedings of the National
Academy of Sciences, 103(23):8577–8582, June.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based Collaborative Filter-
ing Recommendation Algorithms. In Proceedings
of the 10th International Conference on World Wide
Web, WWW ’01, pages 285–295, New York, NY,
USA. ACM.
Yizhou Sun, Yintao Yu, and Jiawei Han. 2009.
Ranking-based Clustering of Heterogeneous Infor-
mation Networks with Star Network Schema. In
Proceedings of the 15th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, KDD ’09, pages 797–806, New York,
NY, USA. ACM.
Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-
guo Xiao. 2011. Summarizing the Differences
in Multilingual News. In Proceedings of the 34th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’11, pages 735–744, New York, NY, USA. ACM.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2012. Comparative Document Summariza-
tion via Discriminative Sentence Selection. ACM
Trans. Knowl. Discov. Data, 6(3), October.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A Cross-collection Mixture Model for Comparative
Text Mining. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’04, pages 743–
748, New York, NY, USA. ACM.
</reference>
<page confidence="0.998892">
845
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920560">
<title confidence="0.999098">A Framework for Comparing Groups of Documents</title>
<author confidence="0.997876">Arun S Maiya</author>
<affiliation confidence="0.965707">Institute for Defense Analyses — Alexandria, VA,</affiliation>
<email confidence="0.976361">amaiya@ida.org</email>
<abstract confidence="0.998241076923077">We present a general framework for comparing multiple groups of documents. A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set. Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups. Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ioannis Antonellis</author>
<author>Hector G Molina</author>
<author>Chi C Chang</author>
</authors>
<title>Simrank++: Query Rewriting Through Link Analysis of the Click Graph.</title>
<date>2008</date>
<booktitle>Proc. VLDB Endow.,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="9146" citStr="Antonellis et al., 2008" startWordPosition="1563" endWordPosition="1567">ber of topics (lower values of H(v) mean more concentrated). Similarly, if v E C, it is the extent to which a topic is concentrated around a small number of document groups. Node Similarity. Given a graph G, there are many ways to measure the similarity of two nodes based on their connections. Such measures can be used to infer similarity (and dissimilarity) among document groups. However, existing methods are not well-suited for the task of document group comparison. The well-known SimRank algorithm (Jeh and Widom, 2002) ignores edge weights, and neither SimRank nor its extension, SimRank++ (Antonellis et al., 2008), scale to larger graphs. SimRank++ and ASCOS (Chen and Giles, 2013) do incorporate edge weights but in ways that are not appropriate for document group comparisons. For instance, both SimRank++ and ASCOS incorporate magnitude in the similarity computation. Consider the case where document groups are defined as research labs. ASCOS and SimRank++ will measure large research labs and small research labs as less similar when in fact they may publish nearly identical lines of research. Finally, under these existing methods, document groups sharing zero topics in common could still be considered si</context>
<context position="16246" citStr="Antonellis et al., 2008" startWordPosition="2744" endWordPosition="2747">oretical Physics and Ecology programs. Results agree with intuition. For each NSF program, we identified the top n most similar programs ranked by our sim(·, ·) function, where n E 13, 6, 9}. These programs were manually judged for relatedness, and the Mean Average Precision (MAP), a standard performance metric for ranking tasks in information retrieval, was computed. We were unsuccessful in evaluating alternative weighted similarity measures mentioned in Section 3 due to their aforementioned issues with scalability and the size of the NSF dataset. (For instance, the implementations of ASCOS (Antonellis et al., 2008) and SimRank (Jeh and Widom, 2002) that we considered are available here.6) Recall that our sim(·, ·) function is based on measuring the cosine similarity between two weight vectors, a� and b, generated from our bipartite graph model. As a baseline for comparison, we evaluated two additional similarity implementations using these weight vectors. The first measures the similarity between weight vectors using weighted Jaccard similarity, which is Ek min(ak,bk) (denoted as Wtd. Jaccard). The second measure is implemented by taking the Spearman’s rank correlation coefficient of a� and b� (deNation</context>
</contexts>
<marker>Antonellis, Molina, Chang, 2008</marker>
<rawString>Ioannis Antonellis, Hector G. Molina, and Chi C. Chang. 2008. Simrank++: Query Rewriting Through Link Analysis of the Click Graph. Proc. VLDB Endow., 1(1):408–421, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bache</author>
<author>M Lichman</author>
</authors>
<title>UCI machine learning repository.</title>
<date>2013</date>
<contexts>
<context position="12257" citStr="Bache and Lichman, 2013" startWordPosition="2104" endWordPosition="2107">e expected fraction if edges were distributed evenly in the graph (Newman, 2006). The algorithm initially assigns each node to its own cluster. At each iteration, in a local and greedy fashion, nodes are re-assigned to clusters with which they achieve the highest modularity. 4 Example Analysis: NSF Grants As a realistic and informative case study, we utilize our model to characterize funding programs of the National Science Foundation (NSF). This corpus consists of 132,372 grant abstracts describing awards for basic research and other support funded by the NSF between the years 1990 and 2002 (Bache and Lichman, 2013).4 Each award is associated with both a program element (i.e., funding source) and a date. We define document 4Data for years 1989 and 2003 in this publicly available corpus were partially missing and omitted in some analyses. Document Group IDPI Document Group 1 Document Group 2 Document Group 3 Document Group 4 Document Groups (DP) Topics (Dc) � � TopicIDcI Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 their corresponding sequences, 11~a1111 842 groups in two ways: by program element and by calendar year. For comparison criteria, we used topics discovered with the MALLET implementation of </context>
</contexts>
<marker>Bache, Lichman, 2013</marker>
<rawString>K. Bache and M. Lichman. 2013. UCI machine learning repository.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--4</pages>
<contexts>
<context position="5230" citStr="Blei et al., 2003" startWordPosition="816" endWordPosition="819">. Document groups are compared using comparison criteria, DC, a family of subsets of D. Definition 2 A comparison criterion is a subset DCi E DC (where index i E {1... |DC|}). Intuitively, each subset of DC represents a set of documents sharing some attribute. Our model allows great flexibility in how DC is defined. For instance, DC might be defined by the named entities mentioned within documents (e.g., each subset contains documents that mention a particular person or organization of interest). For the present work, we define DC by topics discovered using latent Dirichlet allocation or LDA (Blei et al., 2003). LDA Topics as Comparison Criteria. Probabilistic topic modeling algorithms like LDA discover latent themes (i.e., topics) in document collections. By using these discovered topics as the comparison criteria, we can compare arbitrary groups of documents by the themes and subject areas comprising them. Let K be the number of topics or themes in D. Each document in D is composed of a sequence of words: di = (si1, si2, ... , siNi), where Ni is the number of words in di and i E {1... N}. V = UN i=1 f(di) is the vocabulary of D, where f(�) takes a sequence of elements and returns a set. LDA takes </context>
<context position="18820" citStr="Blei et al., 2003" startWordPosition="3140" endWordPosition="3143">ments. Figure 4: [Neuroscience Programs.] A discovered cluster of program elements all related to neuroscience. Which pairs of grants are the most similar in the research they describe? Although the focus of this paper is on drawing comparisons among groups of documents, it is often necessary to draw comparisons among individual documents, as well. For instance, one may wish to identify pairs of grants from different programs describing highly similar lines of research. One common approach to this is to measure the similarity among low-dimensional representations of documents returned by LDA (Blei et al., 2003). We employ the Hellinger distance metric for this. Unfortunately, identifying the set of most similar document pairs in this way can be computationally expensive, as the number of pairwise comparisons scales quadratically with the size of the corpus. To address this, our bipartite graph model can be exploited as a blocking heuristic using either the document groups or the comparison criteria. In the latter case, one can limit the pairwise comparisons to only those documents that reside in the same subset of DC. For the former case, node similarity can be used. Instead of comparing each docume</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3(4-5):993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent D Blondel</author>
<author>Jean-Loup Guillaume</author>
<author>Renaud Lambiotte</author>
<author>Etienne Lefebvre</author>
</authors>
<title>Fast unfolding of communities in large networks.</title>
<date>2008</date>
<journal>Journal of Statistical Mechanics: Theory and Experiment,</journal>
<volume>2008</volume>
<issue>10</issue>
<contexts>
<context position="11548" citStr="Blondel et al., 2008" startWordPosition="1990" endWordPosition="1993">e graph clustering techniques (e.g., Dhillion (2001) and Sun et al. (2009)) are rendered less effective. We instead employ a different tack and exploit the node similarities computed earlier. We transform G into a new weighted graph GP = (P, EP, wsim) where EP = {(u, v) |u, v E P, sim(u, v) &gt; ξJ, ξ is a pre-defined threshold, and wsim is the edge weight function (i.e., wsim = sim). Thus, GP is the similarity graph of document groups. ξ = 0.5 was used as the threshold for our analyses. To find clusters in GP, we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008). Modularity measures the fraction of edges falling within clusters as compared to the expected fraction if edges were distributed evenly in the graph (Newman, 2006). The algorithm initially assigns each node to its own cluster. At each iteration, in a local and greedy fashion, nodes are re-assigned to clusters with which they achieve the highest modularity. 4 Example Analysis: NSF Grants As a realistic and informative case study, we utilize our model to characterize funding programs of the National Science Foundation (NSF). This corpus consists of 132,372 grant abstracts describing awards for</context>
</contexts>
<marker>Blondel, Guillaume, Lambiotte, Lefebvre, 2008</marker>
<rawString>Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008+, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Campr</author>
<author>Karel Jeˇzek</author>
</authors>
<title>Topic Models for Comparative Summarization.</title>
<date>2013</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>8082</volume>
<pages>568--574</pages>
<editor>In Ivan Habernal and V´aclav Matouˇsek, editors, Text, Speech, and Dialogue,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Campr, Jeˇzek, 2013</marker>
<rawString>Michal Campr and Karel Jeˇzek. 2013. Topic Models for Comparative Summarization. In Ivan Habernal and V´aclav Matouˇsek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture Notes in Computer Science, pages 568–574. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hung H Chen</author>
<author>C Lee Giles</author>
</authors>
<title>ASCOS: An Asymmetric Network Structure COntext Similarity Measure.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM ’13,</booktitle>
<pages>442--449</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9214" citStr="Chen and Giles, 2013" startWordPosition="1576" endWordPosition="1579"> if v E C, it is the extent to which a topic is concentrated around a small number of document groups. Node Similarity. Given a graph G, there are many ways to measure the similarity of two nodes based on their connections. Such measures can be used to infer similarity (and dissimilarity) among document groups. However, existing methods are not well-suited for the task of document group comparison. The well-known SimRank algorithm (Jeh and Widom, 2002) ignores edge weights, and neither SimRank nor its extension, SimRank++ (Antonellis et al., 2008), scale to larger graphs. SimRank++ and ASCOS (Chen and Giles, 2013) do incorporate edge weights but in ways that are not appropriate for document group comparisons. For instance, both SimRank++ and ASCOS incorporate magnitude in the similarity computation. Consider the case where document groups are defined as research labs. ASCOS and SimRank++ will measure large research labs and small research labs as less similar when in fact they may publish nearly identical lines of research. Finally, under these existing methods, document groups sharing zero topics in common could still be considered similar, which is undesirable here. For these reasons, we formulate si</context>
</contexts>
<marker>Chen, Giles, 2013</marker>
<rawString>Hung H. Chen and C. Lee Giles. 2013. ASCOS: An Asymmetric Network Structure COntext Similarity Measure. In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM ’13, pages 442–449, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillion</author>
</authors>
<title>Co-clustering Documents and Words Using Bipartite Spectral GraphPartitioning.</title>
<date>2001</date>
<tech>Technical report,</tech>
<location>Austin, TX, USA.</location>
<contexts>
<context position="10979" citStr="Dhillion (2001)" startWordPosition="1885" endWordPosition="1886">ument groups are considered more similar when they have similar sets of topics in similar proportions. As we will show later, this simple solution, based on item-based collaborative filtering (Sarwar et al., 2001), is surprisingly effective at inferring similarity among document groups in G. Node Clusters. Identifying clusters of related nodes in the bipartite graph G can show how document groups form larger classes. However, we find that G is typically fairly dense. For these reasons, partitioning of the one-mode projection of G and other standard bipartite graph clustering techniques (e.g., Dhillion (2001) and Sun et al. (2009)) are rendered less effective. We instead employ a different tack and exploit the node similarities computed earlier. We transform G into a new weighted graph GP = (P, EP, wsim) where EP = {(u, v) |u, v E P, sim(u, v) &gt; ξJ, ξ is a pre-defined threshold, and wsim is the edge weight function (i.e., wsim = sim). Thus, GP is the similarity graph of document groups. ξ = 0.5 was used as the threshold for our analyses. To find clusters in GP, we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008). Modularity measures the fract</context>
</contexts>
<marker>Dhillion, 2001</marker>
<rawString>Inderjit S. Dhillion. 2001. Co-clustering Documents and Words Using Bipartite Spectral GraphPartitioning. Technical report, Austin, TX, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Eagle</author>
<author>Michael Macy</author>
<author>Rob Claxton</author>
</authors>
<title>Network diversity and economic development.</title>
<date>2010</date>
<journal>Science,</journal>
<volume>328</volume>
<issue>5981</issue>
<contexts>
<context position="8126" citStr="Eagle et al. (2010)" startWordPosition="1387" endWordPosition="1390">t cluster pertaining primarily to the same topic). Each edge represents the intersection of the two subsets it connects. In the next section, we will describe basic algorithms on such bipartite graphs capable of yielding important insights into the similarities and differences among document groups. 3 Basic Algorithms Using the Model We focus on three basic operations in this work. Node Entropy. Let w� be a vector of weights for all edges incident to some node v E E. The entropy H of v is: H(v) = − Ei pi log|~w|(pi), where pi = Ewwj and i, j E {1... |w|}. A similar formulation was employed in Eagle et al. (2010). Intuitively, if v E P, H(v) measures the extent to which the document group is concentrated around 2 DC is also a partition of D, when defined in this way. 841 Figure 1: [Toy Illustration of Bipartite Graph Model.] Each black node (i.e., node ∈ P) represents a document group. Each gray node (i.e., node ∈ C) represents a cluster of documents pertaining primarily to the same topic. a small number of topics (lower values of H(v) mean more concentrated). Similarly, if v E C, it is the extent to which a topic is concentrated around a small number of document groups. Node Similarity. Given a graph</context>
</contexts>
<marker>Eagle, Macy, Claxton, 2010</marker>
<rawString>Nathan Eagle, Michael Macy, and Rob Claxton. 2010. Network diversity and economic development. Science, 328(5981):1029–1031, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojiang Huang</author>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Comparative News Summarization Using Linear Programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>648--653</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3155" citStr="Huang et al. (2011)" startWordPosition="479" endWordPosition="482">products, etc.). Given the abundance of real-world applications as illustrated above, it is surprising, then, that there are no existing general-purpose approaches for drawing such comparisons. While there is some previous work on the comparison of document sets (referred to as comparative text mining), these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria. Moreover, much of the work in the area focuses largely on the summarization of shared or unshared topics among document groups (e.g., Wan et al. (2011), Huang et al. (2011), Campr and Jeˇzek (2013), Wang et al. (2012), Zhai et al. (2004)). That is, the problem of drawing multifaceted comparisons among the groups themselves is not typically addressed. This, then, motivates our development of a general-purpose model for comparisons of document sets along arbitrary dimensions. We use this model for the identification of similarities, differences, trends, and anomalies among large groups of documents. We begin by 1http://en.wikipedia.org/wiki/ Document_Exploitation_(DOCEX) 840 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pa</context>
</contexts>
<marker>Huang, Wan, Xiao, 2011</marker>
<rawString>Xiaojiang Huang, Xiaojun Wan, and Jianguo Xiao. 2011. Comparative News Summarization Using Linear Programming. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 648–653, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Jeh</author>
<author>Jennifer Widom</author>
</authors>
<title>SimRank: a measure of structural-context similarity.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>538--543</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9049" citStr="Jeh and Widom, 2002" startWordPosition="1548" endWordPosition="1551">de ∈ C) represents a cluster of documents pertaining primarily to the same topic. a small number of topics (lower values of H(v) mean more concentrated). Similarly, if v E C, it is the extent to which a topic is concentrated around a small number of document groups. Node Similarity. Given a graph G, there are many ways to measure the similarity of two nodes based on their connections. Such measures can be used to infer similarity (and dissimilarity) among document groups. However, existing methods are not well-suited for the task of document group comparison. The well-known SimRank algorithm (Jeh and Widom, 2002) ignores edge weights, and neither SimRank nor its extension, SimRank++ (Antonellis et al., 2008), scale to larger graphs. SimRank++ and ASCOS (Chen and Giles, 2013) do incorporate edge weights but in ways that are not appropriate for document group comparisons. For instance, both SimRank++ and ASCOS incorporate magnitude in the similarity computation. Consider the case where document groups are defined as research labs. ASCOS and SimRank++ will measure large research labs and small research labs as less similar when in fact they may publish nearly identical lines of research. Finally, under t</context>
<context position="16280" citStr="Jeh and Widom, 2002" startWordPosition="2750" endWordPosition="2753"> Results agree with intuition. For each NSF program, we identified the top n most similar programs ranked by our sim(·, ·) function, where n E 13, 6, 9}. These programs were manually judged for relatedness, and the Mean Average Precision (MAP), a standard performance metric for ranking tasks in information retrieval, was computed. We were unsuccessful in evaluating alternative weighted similarity measures mentioned in Section 3 due to their aforementioned issues with scalability and the size of the NSF dataset. (For instance, the implementations of ASCOS (Antonellis et al., 2008) and SimRank (Jeh and Widom, 2002) that we considered are available here.6) Recall that our sim(·, ·) function is based on measuring the cosine similarity between two weight vectors, a� and b, generated from our bipartite graph model. As a baseline for comparison, we evaluated two additional similarity implementations using these weight vectors. The first measures the similarity between weight vectors using weighted Jaccard similarity, which is Ek min(ak,bk) (denoted as Wtd. Jaccard). The second measure is implemented by taking the Spearman’s rank correlation coefficient of a� and b� (deNational_Nanotechnology_Initiative 6http</context>
</contexts>
<marker>Jeh, Widom, 2002</marker>
<rawString>Glen Jeh and Jennifer Widom. 2002. SimRank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 538–543, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="12877" citStr="McCallum, 2002" startWordPosition="2211" endWordPosition="2212">ch award is associated with both a program element (i.e., funding source) and a date. We define document 4Data for years 1989 and 2003 in this publicly available corpus were partially missing and omitted in some analyses. Document Group IDPI Document Group 1 Document Group 2 Document Group 3 Document Group 4 Document Groups (DP) Topics (Dc) � � TopicIDcI Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 their corresponding sequences, 11~a1111 842 groups in two ways: by program element and by calendar year. For comparison criteria, we used topics discovered with the MALLET implementation of LDA (McCallum, 2002) using K = 400 as the number of topics and 200 as the number of iterations. All other parameters were left as defaults. The NSF corpus possesses unique properties that lend themselves to experimental evaluation. For instance, program elements are not only associated with specific sets of research topics but are named based on the content of the program. This provides a measure of ground truth against which we can validate our model. We structure our analyses around specific questions, which now follow. Which ISF programs are focused on specific areas and which are not? When defining document g</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Modularity and community structure in networks.</title>
<date>2006</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>103</volume>
<issue>23</issue>
<contexts>
<context position="11713" citStr="Newman, 2006" startWordPosition="2018" endWordPosition="2019"> computed earlier. We transform G into a new weighted graph GP = (P, EP, wsim) where EP = {(u, v) |u, v E P, sim(u, v) &gt; ξJ, ξ is a pre-defined threshold, and wsim is the edge weight function (i.e., wsim = sim). Thus, GP is the similarity graph of document groups. ξ = 0.5 was used as the threshold for our analyses. To find clusters in GP, we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008). Modularity measures the fraction of edges falling within clusters as compared to the expected fraction if edges were distributed evenly in the graph (Newman, 2006). The algorithm initially assigns each node to its own cluster. At each iteration, in a local and greedy fashion, nodes are re-assigned to clusters with which they achieve the highest modularity. 4 Example Analysis: NSF Grants As a realistic and informative case study, we utilize our model to characterize funding programs of the National Science Foundation (NSF). This corpus consists of 132,372 grant abstracts describing awards for basic research and other support funded by the NSF between the years 1990 and 2002 (Bache and Lichman, 2013).4 Each award is associated with both a program element </context>
</contexts>
<marker>Newman, 2006</marker>
<rawString>M. E. J. Newman. 2006. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577–8582, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badrul Sarwar</author>
<author>George Karypis</author>
<author>Joseph Konstan</author>
<author>John Riedl</author>
</authors>
<title>Item-based Collaborative Filtering Recommendation Algorithms.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th International Conference on World Wide Web, WWW ’01,</booktitle>
<pages>285--295</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10577" citStr="Sarwar et al., 2001" startWordPosition="1819" endWordPosition="1823">) and let x : I -* Lu,v be the indexing function for Lu,v.3 We construct two vectors, a� and b, where ak = w(u, x(k)), bk = w(v, x(k)), and k E I. Each vector is es3I is the index set of Lu,v. sentially a sequence of weights for edges between u, v E P and each node in Lu,v. Similarity of two nodes is measured using the cosine similarity of ~a·~b which we b11, compute using a function sim(·, ·). Thus, document groups are considered more similar when they have similar sets of topics in similar proportions. As we will show later, this simple solution, based on item-based collaborative filtering (Sarwar et al., 2001), is surprisingly effective at inferring similarity among document groups in G. Node Clusters. Identifying clusters of related nodes in the bipartite graph G can show how document groups form larger classes. However, we find that G is typically fairly dense. For these reasons, partitioning of the one-mode projection of G and other standard bipartite graph clustering techniques (e.g., Dhillion (2001) and Sun et al. (2009)) are rendered less effective. We instead employ a different tack and exploit the node similarities computed earlier. We transform G into a new weighted graph GP = (P, EP, wsim</context>
</contexts>
<marker>Sarwar, Karypis, Konstan, Riedl, 2001</marker>
<rawString>Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based Collaborative Filtering Recommendation Algorithms. In Proceedings of the 10th International Conference on World Wide Web, WWW ’01, pages 285–295, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yizhou Sun</author>
<author>Yintao Yu</author>
<author>Jiawei Han</author>
</authors>
<title>Ranking-based Clustering of Heterogeneous Information Networks with Star Network Schema.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09,</booktitle>
<pages>797--806</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11001" citStr="Sun et al. (2009)" startWordPosition="1888" endWordPosition="1891">sidered more similar when they have similar sets of topics in similar proportions. As we will show later, this simple solution, based on item-based collaborative filtering (Sarwar et al., 2001), is surprisingly effective at inferring similarity among document groups in G. Node Clusters. Identifying clusters of related nodes in the bipartite graph G can show how document groups form larger classes. However, we find that G is typically fairly dense. For these reasons, partitioning of the one-mode projection of G and other standard bipartite graph clustering techniques (e.g., Dhillion (2001) and Sun et al. (2009)) are rendered less effective. We instead employ a different tack and exploit the node similarities computed earlier. We transform G into a new weighted graph GP = (P, EP, wsim) where EP = {(u, v) |u, v E P, sim(u, v) &gt; ξJ, ξ is a pre-defined threshold, and wsim is the edge weight function (i.e., wsim = sim). Thus, GP is the similarity graph of document groups. ξ = 0.5 was used as the threshold for our analyses. To find clusters in GP, we employ the Louvain algorithm, a heuristic method based on modularity optimization (Blondel et al., 2008). Modularity measures the fraction of edges falling w</context>
</contexts>
<marker>Sun, Yu, Han, 2009</marker>
<rawString>Yizhou Sun, Yintao Yu, and Jiawei Han. 2009. Ranking-based Clustering of Heterogeneous Information Networks with Star Network Schema. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 797–806, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Houping Jia</author>
<author>Shanshan Huang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Summarizing the Differences in Multilingual News.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>735--744</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3134" citStr="Wan et al. (2011)" startWordPosition="475" endWordPosition="478">eviews for several products, etc.). Given the abundance of real-world applications as illustrated above, it is surprising, then, that there are no existing general-purpose approaches for drawing such comparisons. While there is some previous work on the comparison of document sets (referred to as comparative text mining), these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria. Moreover, much of the work in the area focuses largely on the summarization of shared or unshared topics among document groups (e.g., Wan et al. (2011), Huang et al. (2011), Campr and Jeˇzek (2013), Wang et al. (2012), Zhai et al. (2004)). That is, the problem of drawing multifaceted comparisons among the groups themselves is not typically addressed. This, then, motivates our development of a general-purpose model for comparisons of document sets along arbitrary dimensions. We use this model for the identification of similarities, differences, trends, and anomalies among large groups of documents. We begin by 1http://en.wikipedia.org/wiki/ Document_Exploitation_(DOCEX) 840 Proceedings of the 2015 Conference on Empirical Methods in Natural La</context>
</contexts>
<marker>Wan, Jia, Huang, Xiao, 2011</marker>
<rawString>Xiaojun Wan, Houping Jia, Shanshan Huang, and Jianguo Xiao. 2011. Summarizing the Differences in Multilingual News. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11, pages 735–744, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yihong Gong</author>
</authors>
<title>Comparative Document Summarization via Discriminative Sentence Selection.</title>
<date>2012</date>
<journal>ACM Trans. Knowl. Discov. Data,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="3200" citStr="Wang et al. (2012)" startWordPosition="487" endWordPosition="490">orld applications as illustrated above, it is surprising, then, that there are no existing general-purpose approaches for drawing such comparisons. While there is some previous work on the comparison of document sets (referred to as comparative text mining), these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria. Moreover, much of the work in the area focuses largely on the summarization of shared or unshared topics among document groups (e.g., Wan et al. (2011), Huang et al. (2011), Campr and Jeˇzek (2013), Wang et al. (2012), Zhai et al. (2004)). That is, the problem of drawing multifaceted comparisons among the groups themselves is not typically addressed. This, then, motivates our development of a general-purpose model for comparisons of document sets along arbitrary dimensions. We use this model for the identification of similarities, differences, trends, and anomalies among large groups of documents. We begin by 1http://en.wikipedia.org/wiki/ Document_Exploitation_(DOCEX) 840 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 840–845, Lisbon, Portugal, 17-21 Septembe</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2012</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2012. Comparative Document Summarization via Discriminative Sentence Selection. ACM Trans. Knowl. Discov. Data, 6(3), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>Atulya Velivelli</author>
<author>Bei Yu</author>
</authors>
<title>A Cross-collection Mixture Model for Comparative Text Mining.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>743--748</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3220" citStr="Zhai et al. (2004)" startWordPosition="491" endWordPosition="494"> illustrated above, it is surprising, then, that there are no existing general-purpose approaches for drawing such comparisons. While there is some previous work on the comparison of document sets (referred to as comparative text mining), these existing approaches lack the generality to be widely applicable across different use case scenarios with different comparison criteria. Moreover, much of the work in the area focuses largely on the summarization of shared or unshared topics among document groups (e.g., Wan et al. (2011), Huang et al. (2011), Campr and Jeˇzek (2013), Wang et al. (2012), Zhai et al. (2004)). That is, the problem of drawing multifaceted comparisons among the groups themselves is not typically addressed. This, then, motivates our development of a general-purpose model for comparisons of document sets along arbitrary dimensions. We use this model for the identification of similarities, differences, trends, and anomalies among large groups of documents. We begin by 1http://en.wikipedia.org/wiki/ Document_Exploitation_(DOCEX) 840 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 840–845, Lisbon, Portugal, 17-21 September 2015. c�2015 Assoc</context>
</contexts>
<marker>Zhai, Velivelli, Yu, 2004</marker>
<rawString>ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A Cross-collection Mixture Model for Comparative Text Mining. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 743– 748, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
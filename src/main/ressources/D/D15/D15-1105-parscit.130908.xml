<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.960691">
How Much Information Does a Human Translator Add to the Original?
</title>
<author confidence="0.996163">
Barret Zoph, Marjan Ghazvininejad and Kevin Knight
</author>
<affiliation confidence="0.998055333333333">
Information Sciences Institute
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.993779">
{zoph,ghazvini,knight}@isi.edu
</email>
<bodyText confidence="0.970896272727273">
Abstract 上个星期的战斗至少夺取12个人的生 命 。
We ask how much information a human
translator adds to an original text, and we
provide a bound. We address this ques-
tion in the context of bilingual text com-
pression: given a source text, how many
bits of additional information are required
to specify the target text produced by a hu-
man translator? We develop new compres-
sion algorithms and establish a benchmark
task.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99961">
Text compression exploits redundancy in human
language to store documents compactly, and trans-
mit them quickly. It is natural to think about com-
pressing bilingual texts, which have even more re-
dundancy:
“From an information theoretic point of
view, accurately translated copies of the
original text would be expected to con-
tain almost no extra information if the
original text is available, so in princi-
ple it should be possible to store and
transmit these texts with very little ex-
tra cost.” (Nevill and Bell, 1992)
Of course, if we look at actual translation data
(Figure 1), we see that there is quite a bit of unpre-
dictability. But the intuition is sound. If there were
a million equally-likely translations of a short sen-
tence, it would only take us log2(1m) = 20 bits to
specify which one.
By finding and exploiting patterns in bilingual
data, we want to provide an upper bound for this
question: How much information does a human
translator add to the original? We do this in
the context of building a practical compressor for
bilingual text.
At least 12 people were killed in the battle last week.
Last week’s fight took at least 12 lives.
The fighting last week killed at least 12.
The battle of last week killed at least 12 persons.
At least 12 people lost their lives in last week’s fighting.
At least 12 persons died in the fighting last week.
At least 12 died in the battle last week.
At least 12 people were killed in the fighting last week.
During last week’s fighting, at least 12 people died.
Last week at least twelve people died in the fighting.
Last week’s fighting took the lives of twelve people.
</bodyText>
<figureCaption confidence="0.94719">
Figure 1: Eleven human translations of the same
source sentence (LDC2002T01).
</figureCaption>
<bodyText confidence="0.997358964285714">
We adopt the same scheme used in mono-
lingual text compression benchmark evaluations,
such as the Hutter Prize (Hutter, 2006), a com-
petition to compress a 100m-word extract of En-
glish Wikipedia. A valid entry is an executable, or
self-extracting archive, that prints out Wikipedia,
byte-for-byte. Decompression code, dictionaries,
and/or other resources must be embedded in the
executable—we cannot assume that the recipient
of the compressed file has access to those re-
sources. This view of compression goes by the
name of algorithmic information theory (or Kol-
mogorov complexity).
Any executable is permitted. For example, if
our job were to compress the first million digits
of π, then we might submit a very short piece of
code that prints those digits. The brevity of that
compression would demonstrate our understand-
ing of the sequence. Of course, in our application,
we will find it useful to develop generic algorithms
that can compress any text.
Our approach will be as follows. Given a bilin-
gual text (file1 and file2), we develop this com-
pression interface:
% compress file1 &gt; file1.exe
% bicompress file2 file1 &gt; file2.exe
The second command compresses file2 while
looking at file1. We take the size of file1.exe
</bodyText>
<page confidence="0.98248">
889
</page>
<note confidence="0.99192">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 889–898,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<table confidence="0.987718904761905">
Spanish English
Uncompressed size 324.9 Mb 294.5 Mb
Word tokens 57,068,133 54,364,566
Vocabulary size 195,314 140,340
Distinct word cooc 93,184,127
Segment pairs 1,965,734
Ave. segment length 29.0 27.7
(word tokens)
Longest segment 809 880
(word tokens)
Figure 2: Large Europarl Spanish/English corpus.
Spanish English
Uncompressed size 32.3 Mb 29.3 Mb
Word tokens 5,682,667 5,426,131
Vocabulary size 73,726 45,423
Distinct word cooc 21,231,874
Segment pairs 196,573
Ave. segment length 28.9 27.6
(word tokens)
Longest segment 733 682
(word tokens)
</table>
<figureCaption confidence="0.993134">
Figure 3: Small Europarl Spanish/English corpus.
</figureCaption>
<bodyText confidence="0.999873666666667">
as the amount of information in the original text.
We bound how much information the translator
adds to the original by:
</bodyText>
<equation confidence="0.797362375">
|file2.exe |/ |file1.exe|
We can say that bilingual compression is more ef-
fective that monolingual compression if:
|file2.exe |&lt; |file3.exe|, where
% compress file2 &gt; file3.exe
Our decompression interface is:
% file1.exe &gt; file1
% file2.exe file1 &gt; file2
</equation>
<bodyText confidence="0.989804333333333">
The second command decompresses file2 while
looking at (uncompressed) file1.
The contributions of this paper are:
</bodyText>
<listItem confidence="0.984060333333333">
1. We provide a new quantitative bound for how
much information a translator adds to an orig-
inal text.
2. We present practical software to compress
bilingual text with compression rates that ex-
ceed the previous state-of-the-art.
3. We set up a public benchmark bilingual text
compression challenge to stimulate new re-
searchers to find and exploit patterns in bilin-
gual text. Ultimately, we want to feed those
ideas into practical machine translation sys-
tems.
</listItem>
<sectionHeader confidence="0.968623" genericHeader="keywords">
2 Data
</sectionHeader>
<bodyText confidence="0.999761058823529">
We propose the widely accessible Spanish/English
Europarl corpus v7 (Koehn, 2005) as a benchmark
for bilingual text compression (Figure 2). Por-
tions of this large corpus have been used in pre-
vious compression work (S´anchez-Martinez et al.,
2012). The Spanish side is in UTF-8. For En-
glish, we have removed accent marks and further
eliminated all but the 95 printable ASCII charac-
ters (Brown et al., 1992), plus newline.
Our task is to compress the data “as is”: un-
tokenized, but already segment aligned. We also
include a tokenized version with 334 manually
word-aligned segment pairs (Lambert et al., 2005)
distributed throughout the corpus.
For rapid development and testing, we have ar-
ranged a smaller corpus that is 10% the size of the
full corpus (Figure 3).
</bodyText>
<sectionHeader confidence="0.989883" genericHeader="introduction">
3 Monolingual compression
</sectionHeader>
<bodyText confidence="0.99944056">
Compression captures patterns in data. Language
modeling also captures patterns, but at first blush,
these two areas seem distinct. In compression, we
seek a small executable that prints out a text, while
in language modeling, we seek an executable that
assigns low perplexity to held-out test data.1 Ac-
tually, the two areas have much more in common,
as a review of compression algorithms reveals.
Huffman coding. A well-known compression
technique is to create a binary Huffman tree whose
leaves are characters in the text,2 and whose edges
are labeled 0 or 1 (Huffman and others, 1952). The
tree is arranged so that frequent characters have
short binary codes (edge sequences). It is very im-
portant that the Huffman tree for a particular text
be included at the beginning of the compressed
file, so that decompression knows how to process
the compressed bit string.
Adaptive Huffman. Actually, we can avoid
shipping the Huffman tree inside the compressed
file, by building the tree adaptively, as the com-
pressor processes the input text. If we start with a
uniform distribution, the first few characters may
not compress very well, but soon we will converge
onto a good tree and good compression. It is very
</bodyText>
<footnote confidence="0.999318">
1File size has advantages, as perplexity computations are
often buggy, and they usually gloss over how probability is
apportioned to out-of-vocabulary items.
2Or other symbols, such as words, bytes, or unicode se-
quences.
</footnote>
<page confidence="0.995512">
890
</page>
<figureCaption confidence="0.999871">
Figure 4: Arithmetic coding.
</figureCaption>
<bodyText confidence="0.999968033333333">
important that the decompressor exactly recapitu-
late the same sequence of Huffman trees that the
compressor made. It can do this by counting char-
acters as it outputs them, just as the compressor
counted characters as it consumed them.
Adaptive compression can also nicely accom-
modate shifting topics in text, if we give higher
counts to recent events. By its single-pass nature,
it is also good for streaming data.
Arithmetic coding. Huffman coding exploits
a predictive unigram distribution over the next
character. If we use more context, we can make
sharper distributions. An n-gram table is one way
to map contexts onto predictions.
How do we convert good predictions into good
compression? The solution is called arithmetic
coding (Rissanen and Langdon Jr., 1981; Witten
et al., 1987). Figure 4 sketches the technique.
We produce context-dependent probability inter-
vals, and each time we observe a character, we
move to its interval. Our working interval be-
comes smaller and smaller, but the better our pre-
dictions, the wider it stays. A document’s com-
pression is the shortest bit string that fits inside the
final interval. In practice, we do the bit-coding as
we navigate probability intervals.
Arithmetic coding separates modeling and com-
pression, making our job similar to language mod-
eling, where we use try to use context to predict
the next symbol.
</bodyText>
<subsectionHeader confidence="0.99312">
3.1 PPM
</subsectionHeader>
<bodyText confidence="0.99998675">
PPM is the most well-known adaptive, predic-
tive compression technique (Cleary and Witten,
1984). PPM updates character n-gram tables (usu-
ally n=1..5) as it compresses. In a given context,
an n-gram table may predict only a subset of char-
acters, so PPM reserves some probability mass for
an escape (ESC), after which it executes a hard
backoff to the (n-1)-gram table. In PPMA, P(ESC)
is 1/(1+D), where D is the number of times the
context has been seen. PPMB uses q/D, where q
is the number of distinct character types seen in
the context. PPMC uses q/(q+D), aka Witten-Bell.
PPMD uses q/2D.
PPM* uses the shortest previously-seen deter-
ministic context, which may be quite long. If
there is no deterministic context, PPM* goes to
the longest matching context and starts PPMD. In-
stead of the longest context, PPMZ rates all con-
texts between lengths 0 and 12 according to each
context’s most probable character. PPMZ also im-
plements an adaptive P(ESC) that combines con-
text length, number of previous ESC in the con-
text, etc.
We use our own C++ implementation of PPMC
for monolingual compression experiments in this
paper. When we pass over a set of characters in
favor of ESC, we remove those characters from
the hard backoff.
</bodyText>
<subsectionHeader confidence="0.996938">
3.2 PAQ
</subsectionHeader>
<bodyText confidence="0.999988285714286">
PAQ (Mahoney, 2005) is a family of state-of-the-
art compression algorithms and a perennial Hutter
Prize winner. PAQ combines hundreds of mod-
els with a logistic unit when making a prediction.
This is most efficient when predictions are at the
bit-level instead of the character-level. The unit’s
model weights are adaptively updated by:
</bodyText>
<equation confidence="0.98683325">
wi ← wi + ηxi(correct − P(1)), where
xi = ln(Pi(1)/(1 − Pi(1))
η = fixed learning rate
Pi(1) = ith model’s prediction
</equation>
<bodyText confidence="0.9954522">
PAQ models include a character n-gram model
that adapts to recent text, a unigram word model
(where word is defined as a subsequence of char-
acters with ASCII &gt; 32), a bigram model, and a
skip-bigram model.
</bodyText>
<sectionHeader confidence="0.978344" genericHeader="method">
4 Bilingual Compression: Prior Work
</sectionHeader>
<bodyText confidence="0.998941">
Nevill and Bell (1992) introduce the concept but
actually carry out experiments on paraphrase cor-
pora, such as different English versions of the
Bible.
</bodyText>
<listItem confidence="0.9364176">
Conley and Klein (2008) and Conley and Klein
(2013) compress a target text that has been word-
aligned to a source text, to which they add a lem-
matizer and bilingual glossary. They obtain a 1%-
6% improvement over monolingual compression,
</listItem>
<page confidence="0.994049">
891
</page>
<bodyText confidence="0.999949483870968">
without counting the cost of auxiliary files needed
for decompression.
Martinez-Prieto et al. (2009), Adiego et al.
(2009), Adiego et al. (2010) rewrite bilingual text
by first interleaving source words with their trans-
lations, then compressing this sequence of bi-
words. S´anchez-Martinez et al. (2012) improve
the interleaving scheme and include offsets to
enable decompression to reconstruct the original
word order. They also compare several character-
based and word-based compression schemes for
biword sequences. On Spanish-English Europarl
data, they reach an 18.7% compression rate on
word-interleaved text, compared to 20.1% for con-
catenated texts, a 7.2% improvement.
Al-Onaizan et al. (1999) study the perplexity
of learned translation models, i.e., the probabil-
ity assigned to the target corpus given the source
corpus. They observed iterative training to im-
prove training-set perplexity (as guaranteed) but
degrade test-set perplexity. They hypothesized
that an increasingly tight, unsmoothed translation
dictionary might exclude word translations needed
to explain test-set data. Subsequently, research
moved to extrinsic evaluation of translation mod-
els, in the context of end-to-end machine transla-
tion.
Foster et al. (2002) and others have used predic-
tion to propose auto-completions to speed up hu-
man translation. As we have seen, prediction and
compression are highly related.
</bodyText>
<sectionHeader confidence="0.987822" genericHeader="method">
5 Predictive Bilingual Compression
</sectionHeader>
<bodyText confidence="0.9999635">
Our algorithm compresses target-language file2
while looking at source-language file1:
% bicompress file2 file1 &gt; file2.exe
To make use of arithmetic coding, we consider
the task of predicting the next target character,
given the source sentence and target string so far:3
</bodyText>
<equation confidence="0.928305">
P(ej|f1 ... fl, e1 ... ej−1)
</equation>
<bodyText confidence="0.958464857142857">
If we are able to accurately predict what a human
translator will type next, then we should be able to
build a good machine translator. Here is an exam-
ple of the task:
Spanish: Pido que hagamos un
minuto de silencio.
English so far: I should like to ob
</bodyText>
<footnote confidence="0.3880655">
3We predict e from f in this paper, reversed from Brown
et al. (1993), who predict f from e.
</footnote>
<table confidence="0.8973264">
Absolute Relative
offsets offsets
Uncompressed 105.3 Mb 88.9 Mb
Huffman coding 36.6 Mb 24.4 Mb
PPMC 12.4 Mb 13.2 Mb
</table>
<bodyText confidence="0.8376885">
Figure 5: Compressing a file of (unidirectional)
automatic Viterbi word alignments computed from
our large Spanish/English corpus (sentences less
than 50 words).
</bodyText>
<subsectionHeader confidence="0.997631">
5.1 Word alignment
</subsectionHeader>
<bodyText confidence="0.991492730769231">
Let us first work at the word level instead of
the character level. If we are predicting the jth
English word, and we know that it translates fi
(“aligns to fi”), and if fi has only a handful of
translations, then we may be able to specify ej
with just a few bits. We may therefore suppose that
a set of Viterbi word alignments may be useful for
compression (Conley and Klein, 2008; S´anchez-
Martinez et al., 2012).
We consider unidirectional alignments that link
each target position j to a single source position
i (including the null word at i = 0). Such align-
ments can be computed automatically using EM
(Brown et al., 1993), and stored in one of two for-
mats:
Absolute: 1 2 5 5 7 0 3 6 .. .
Relative: +1 +1 +3 0 +2 null -4 +3 ...
In order to interpret the bits produced by the
compressor, our decompressor must also have ac-
cess to the same Viterbi alignments. Therefore, we
must include those alignments at the beginning of
the compressed file. So let’s compress them too.
How compressible are alignment sequences?
Figure 5 gives results for Viterbi alignments de-
rived from our large parallel Spanish/English cor-
pus. First, some interesting facts:
</bodyText>
<listItem confidence="0.977853111111111">
• Huffman works better on relative offsets, be-
cause the common “+1” gets a short bit code.
• PPMC’s use of context makes it impressively
insensitive to alignment format.
• PPMC beats Huffman on relative offsets.
This would not happen if relative offset inte-
gers were independent of one another, as as-
sumed by (Brown et al., 1993) and (Vogel et
al., 1996). Bigram statistics bear this out:
</listItem>
<equation confidence="0.999982333333333">
P(+1  |-2) = 0.20 P(+1  |+1) = 0.59
P(+1  |-1) = 0.20 P(+1  |+2) = 0.49
P(+1  |0) = 0.52
</equation>
<bodyText confidence="0.931248">
So this small compression experiment already
</bodyText>
<page confidence="0.990883">
892
</page>
<bodyText confidence="0.9998975">
suggests that translation aligners might want to
model more context than just P(offset).
However, the main point of Figure 5 is that the
compressed alignment file requires 12.4 Mb! This
is too large for us to prepend to our compressed
file, for the sake of enabling decompression.
</bodyText>
<equation confidence="0.843613142857143">
E(m|l)
2. For j = 1..m, choose alignment aj a(aj|j, l)
3. For j = 1..m, choose translation ej t(ej|faj)
which, via the “IBM trick” implies:
P(e1 7
... e7m|f1 ... fl) _(
E(m|l) 11 1 Eli=0 a(Z |j, l)t(ej  |fi)
</equation>
<bodyText confidence="0.9996512">
In compression, we must predict English words in-
crementally, before seeing the whole string. Fur-
thermore, we must predict P(STOP) to end the
English sentence. We can adapt IBM Model 2 to
make incremental predictions:
</bodyText>
<equation confidence="0.992840714285714">
P(STOP|f1 ... fl, e1 ... ej−1) ∼
P(STOP|j, l) _
E(j − 1|l)/ Emax
k=j−1 e(k|l)
P(ej|f1 ... fl, e1 ... ej−1) ∼
P(ej|f1 ... fl) _
[1 − P(STOP|j, l)] Eli=0 a(i|j, l)t(ej|fi)
</equation>
<bodyText confidence="0.998439">
We can train t, a, and c on our bilingual text us-
ing EM (Brown et al., 1993). However, the t-table
is still too large to prepend to the compressed En-
glish file.
</bodyText>
<subsectionHeader confidence="0.990595">
5.3 Adaptive translation modeling
</subsectionHeader>
<bodyText confidence="0.999983518518518">
Instead, inspired by PPM, we build up transla-
tion tables in RAM, during a single pass of our
compressor. Our decompressor then rebuilds these
same tables, in the same way, in order to interpret
the compressed bit string.
Neal and Hinton (1998) describe online EM,
which updates probability tables after each train-
ing example. Liang and Klein (2009) and Leven-
berg et al. (2010) apply online EM to a number of
language tasks, including word alignment. Here
we concentrate on the single-pass case.
We initialize a uniform translation model, use it
to collect fractional counts from the first segment
pair, normalize those counts to probabilities, use
those new probabilities to collect fractional counts
from the second segment pair, and so on. Because
we pass through the data only once, we hope to
converge quickly to high-quality tables for com-
pressing the bulk of the text.
Unlike in batch EM, we need not keep sepa-
rate count and probability tables. We only need
count tables, including summary counts for nor-
malization groups, so memory savings are signif-
icant. Whenever we need a probability, we com-
pute it on the fly. To avoid zeroes being immedi-
ately locked in, we invoke add-λ smoothing every
time we compute a probability from counts:4
</bodyText>
<equation confidence="0.999324">
t(e|f) = count(e,f)+λt
count(f)+λt|VE|
a(i|j,l) = count(i,j,l)+λa
count(j,l)+λa(l+1)
</equation>
<bodyText confidence="0.987765368421053">
where |VE |is the size of the English vocabulary.
We determine |VE |via a quick initial pass through
the data, then include it at the top of our com-
pressed file.
In batch EM, we usually run IBM Model 1 for
a few iterations before Model 2, gripped by an
atavistic fear that the a probabilities will enforce
rigid alignments before word co-occurrences have
a chance to settle in. It turns out this fear is jus-
tified in online EM! Because the a table initially
learns to align most words to null, we smooth it
more heavily (λa = 102, λt = 10−4).
We also implement a single-pass HMM align-
ment model (Vogel et al., 1996). In the IBM mod-
els, we can either collect fractional counts after we
have compressed a whole sentence, or we can do
it word-by-word. In the HMM model, alignment
choices are no longer independent of one another:
Given f1 ... fl:
</bodyText>
<listItem confidence="0.9266382">
1. Choose English length m w/prob e(m|l)
2. For j = 1..m:
2a. set aj to null w/prob pi, or
2b. choose non-null aj w/prob (1 − pl)o(aj − ak)
3. For j = 1..m, choose translation ej w/prob t(ej|fa7 )
</listItem>
<bodyText confidence="0.995388125">
In the expression o(aj − ak), k is the maximum
English index (k &lt; j) such that ak =6 0. The
relative offset o-table learns to encourage adjacent
English words to align to adjacent Spanish words.
Batch HMM performs poorly under uniform
initialization, with two causes of failure. First,
EM training sets o(0) too high, leading to absolute
alignments like “1 2 2 2 5 5 5 5 ... ”. We avoid
</bodyText>
<footnote confidence="0.956600333333333">
4In their online EM Model 1 aligner, Liang and Klein
(p.c.) skirt the smoothing issue by running an epoch of batch
EM to initialize a full set of probabilities before starting.
</footnote>
<subsectionHeader confidence="0.798502">
5.2 Translation dictionary
</subsectionHeader>
<bodyText confidence="0.927534428571429">
Another approach is to forget Viterbi alignments
and instead exploit a probabilistic translation dic-
tionary table t(e|f). To predict the next target
word ej, we admit the possibility that ej might be
translating any of the source tokens. IBM Model 2
(Brown et al., 1993) tells us how to do this:
Given f1 ... fl:
</bodyText>
<listItem confidence="0.35922">
1. Choose English length m
</listItem>
<page confidence="0.994699">
893
</page>
<table confidence="0.998572142857143">
Against silver standard (Batch EM) Against gold standard (Human)
IBM1 IBM2 HMM IBM1 IBM2 HMM
Batch EM 100.0 100.0 100.0 55.4 66.3 70.2
Online EM Full data 82.2 81.7 89.5 54.9 63.4 70.0
First 50% 81.4 79.5 88.9 54.9 62.3 70.6
Last 50% 83.0 83.9 90.0 54.9 64.5 69.4
Reordered 83.7 83.3 88.1 56.8 65.4 69.5
</table>
<figureCaption confidence="0.831105333333333">
Figure 6: Word alignment f-scores. Batch EM for IBM 1 is run for 5 iterations; Batch IBM2 adds
5 further iterations of IBM2; Batch HMM adds a further 5 iterations of HMM. Online EM is single-
pass. Against the silver standard, alignments are unidirectional; against gold, they are bidirectional and
symmetrized with grow-dial-final (Koehn et al., 2003). First and last 50% report on different portions of
the corpus. Reordered is on segment pairs ordered short to long. All runs exclude segment pairs with
segments longer than 50 words.
</figureCaption>
<bodyText confidence="0.9986509">
this with a standard schedule of 5 IBM1 iterations,
5 IBM2 iterations, then 5 HMM iterations. How-
ever, HMM still learns a very high value for p1,
aligning most tokens to null, so we fix p1 = 0.1
for the duration of training.
Single-pass, online HMM suffers the same two
problems, both solved when we smooth differen-
tially (Ao = 102, At = 10−4) and fix p1 = 0.1.
Two quick asides before we examine the effec-
tiveness of our online methods:
</bodyText>
<listItem confidence="0.787589666666667">
• Translation researchers often drop long seg-
ment pairs that slow down HMM model pro-
cessing. In compression, we cannot drop any
</listItem>
<bodyText confidence="0.9117106">
of the text. Therefore, if the source segment
contains more than 50 words, we use only
monolingual PPMC to compress the target.
This affects 26.5% of our word tokens.
• We might assist an online aligner by permut-
ing our n segment pairs to place shorter, less
ambiguous ones at the top. However, we
would have to communicate the permutation
to the decompressor, at a prohibitive cost of
log2(n!)/(8 · 106) = 4.8 Mb.
We next look at alignment accuracy (f-score) on
our large Spanish/English corpus (Figure 6). We
evaluate against both a silver standard (Batch EM
Viterbi alignments5) and a gold standard of 334
human-aligned segment pairs distributed through-
out the corpus. We see that online methods gener-
ate competitive translation dictionaries. Because
single-pass alignment is significantly faster than
traditional multi-pass, we also investigate its im-
pact on an overall Moses pipeline for phrase-based
</bodyText>
<footnote confidence="0.987663333333333">
5We confirm that our Batch HMM implementation gives
f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2,
p=85.5, r=61.0), and its differently parameterized HMM.
</footnote>
<table confidence="0.96131">
Alignment Test Set Bleu
speed Europarl News
Batch HMM 1230.78 min 30.2 26.2
Online HMM 711.87 min 30.0 25.3
</table>
<figureCaption confidence="0.992979375">
Figure 7: Fast, single-pass HMM alignment
yields competitive Spanish-English Moses phrase-
based translation accuracy, as measured by Bleu
(Papineni et al., 2002). In-domain (Europarl)
and out-of-domain (SMAT-07 News Commen-
tary) tune/test sets each consist of approximately
1000 sentences, all longer than 50 words to avoid
overlap with training.
</figureCaption>
<bodyText confidence="0.999694285714286">
machine translation (Koehn et al., 2007). Figure 7
shows that we can achieve competitive translation
accuracy using fast, single-pass alignment, speed-
ing up the system development cycle. For this use
case, we can get an additional +0.3 alignment f-
score (just as fast) if we print Viterbi alignments
in a second pass instead of during training.
</bodyText>
<subsectionHeader confidence="0.994852">
5.4 Word tokenization
</subsectionHeader>
<bodyText confidence="0.999995615384616">
We now want our continuously-improving trans-
lation model (TM) to predict target text, and to
combine its predictions with PPM’s. For that to
happen, our TM will need to predict the exact text,
including spurious double-spaces, how parenthe-
ses combine with quotation marks, and so on.
We devise a tokenization scheme that records
spacing information in the word tokens, which al-
lows us to recover the original text uniquely. First,
we identify word tokens as subsequences of [a-zA-
Z]*, [0-9]*, and [other]*, appending to each token
the number of spaces following it (e.g., “...@2”).
Next, we remove all “@1”, which leaves unique
</bodyText>
<page confidence="0.997176">
894
</page>
<bodyText confidence="0.99129275">
recoverability intact. Finally, we move any suffix
on an alpha-numeric word i to become a prefix on
a non-alpha-numeric word i + 1. This reduces the
vocabulary size for TM learning. An example:
</bodyText>
<table confidence="0.9987636">
&amp;quot;String-theory?&amp;quot; he asked.
&lt;=&gt;
S@0 &amp;quot;@0 String@0 -@0 theory@0 ?@0 &amp;quot;@1
he@2 asked@0 .@0
&lt;=&gt;
S@0 &amp;quot;@0 String@0 -@0 theory@0 ?@0 &amp;quot;
he@2 asked@0 .@0
&lt;=&gt;
S@0 &amp;quot;@0 String @0 -@0 theory @0 ?@0 &amp;quot;
he@2 asked @0.@0
</table>
<subsectionHeader confidence="0.980216">
5.5 Predicting target words
</subsectionHeader>
<bodyText confidence="0.999968">
Under this tokenization scheme, we now ask our
TM to give us a probability distribution over pos-
sible next words. The TM knows the entire
source word sequence f1...fl and the target words
e1...ej−1 seen so far. As candidates, we consider
target words that can be produced, via the current
t-table, from any (non-NULL) source words with
probability greater than 10−4.
For HMM, we compute a prediction lattice that
gives a distribution over possible source alignment
positions for the current word we are predicting.
Intuitively, the prediction lattice tells us “where we
currently are” in translating the source string, and
it prefers translations of source words in that vicin-
ity. We efficiently reuse the lattice as we make
predictions for each subsequent target word.
To make the TM’s prediction more accurate,
we weight its prediction for each word with a
smoothed, adapted English bigram word language
model (LM). This discourages the TM from trying
to predict the first character of a word by simply
using the most frequent source words. We found
that exponentiating the LM’s score by 0.2 before
weighting keeps it from overpowering the HMM
predictions.
</bodyText>
<subsectionHeader confidence="0.999468">
5.6 Predicting target characters
</subsectionHeader>
<bodyText confidence="0.999626363636364">
To convert word predictions into character predic-
tions, we combine scores for words that share the
next character. For example, if the TM predicts
”monkey 0.4, car 0.3, cat 0.2, dog 0.1”, then we
have ”P(c) 0.5, P(m) 0.4, P(d) 0.1”. Addition-
ally, we restrict ourselves to words prefixed by the
portion of ej already observed. The TM predicts
the space character when a predicted word fully
matches the observed prefix.
We also adjust PPM to produce a full distribu-
tion over the 96 possible next characters. PPM
</bodyText>
<table confidence="0.9785192">
File Compression bpb
size rate
Uncompressed 324.9 Mb 100.0% 8.00
Huffman coding 172.8 Mb 53.2% 4.26
PPMC 51.4 Mb 15.8% 1.26
</table>
<figureCaption confidence="0.9568855">
Figure 8: Compression of the Spanish side of the
bilingual corpus. bpb = bits per byte.
</figureCaption>
<table confidence="0.998334375">
File Compression bpb
size rate
Uncompressed 294.5 Mb 100.0% 8.00
Huffman coding 160.7 Mb 54.6% 4.37
PPMC 48.5 Mb 16.5% 1.32
Bilingual (this paper) 35.0 Mb 11.9 % 0.95
Shannon monolingual 1.61
Shannon bilingual 0.51
</table>
<figureCaption confidence="0.7496236">
Figure 9: Main results. Compression of the En-
glish side of the bilingual corpus. Bilingual com-
pression improves results. For Shannon game
studies, bpb are estimated as cross-entropies of n-
gram models fitted to human guess sequences.
</figureCaption>
<bodyText confidence="0.9791592">
normally computes a distribution over only char-
acters previously seen in the current context (plus
ESC). We now back off to the lowest context for
every prediction.
We interpolate PPM and TM probabilities:
</bodyText>
<equation confidence="0.994428666666667">
P(ek|f1 ... fl, e1 ... ek−1) =
µ PPPM(ek|e1 ... ek−1)+
(1 − µ) PTM(ek|f1 ... fl, e1 ... ek−1)
</equation>
<bodyText confidence="0.995668">
We adjust µ dynamically based on the relative con-
fidence of the models:
</bodyText>
<equation confidence="0.7906635">
max(PPM)2.5
max(PPM)2.5+max(HMM)2.5
</equation>
<bodyText confidence="0.9999506">
Here, max(model) refers to the highest probabil-
ity assigned to any character in the current context
by the model. This yields better compression rates
than simply setting µ to a constant. When the TM
is unable to extend a word, we set µ = 1.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.953152111111111">
Figure 8 shows that monolingual PPM compresses
the Spanish side of our corpus to 15.8% of the
original. Figure 9 (Main results) shows results for
the English side of the corpus. Monolingual PPM
compresses to 16.5%, while our HMM-based
bilingual compression compresses to 11.9%.6
We can say that a human translation is charac-
terized by an additional 0.95 bits per byte on top of
the original, rather than the 1.32 bits per byte we
</bodyText>
<footnote confidence="0.99833525">
6For this result, we divide the English corpus into two
pieces and compress them in parallel, and we further increase
the sentence length threshold from 50 to 60, incurring a speed
penalty. Our fictional Weissman score is 0.676.
</footnote>
<equation confidence="0.60822">
µ =
</equation>
<page confidence="0.989383">
895
</page>
<table confidence="0.9982469">
File Compression bpb
size rate
Uncompressed 619.4 Mb 100.0% 8.00
Huffman coding 336.8 Mb 54.4% 4.35
PPMC 101.6 Mb 16.4% 1.31
Bilingual (this paper) 86.4 Mb 13.9% 1.12
(S´anchez-Martinez different 20.1% 1.61
et al., 2012) PPMDI corpus
(S´anchez-Martinez different 18.7% 1.50
et al., 2012) bilingual corpus
</table>
<figureCaption confidence="0.587861">
Figure 10: Compression of Spanish plus English.
All methods are run on a single file of Spanish
concatenated with English, except for “Bilingual
(this paper),” which records the sum of (1) Span-
ish compression and (2) English-given-Spanish
compression. Comparative numbers copied from
S´anchez-Martinez et al (2012) are for a different
subset of Europarl data.
</figureCaption>
<bodyText confidence="0.999301838709678">
would need if the English were independent text.
Assuming our Spanish compression is good, we
can also say that the human translator produces at
most 68.1% (35.0/51.4) of the information that the
original Spanish author produced. Intuitively, we
feel this bound is high and should be reduced with
better translation modeling.
Figure 9 also reports our Shannon game exper-
iments in which bilingual humans guessed subse-
quent characters of the English text. As suggested
by Shannon, we upper-bound bpb as the cross-
entropy of a unigram model over a human guess
sequence (e.g., 1 1 2 5 17 1 1 ... ), which records
how many guesses it took to identify each subse-
quent English character, given context. For a 502-
character English sequence, a team of four bilin-
guals working together gave us an upper-bound
bpb of 0.51. This team had access to the original
Spanish, plus a Google translation. Monolinguals
guessing on the same data (minus the Spanish and
Google translation) yielded an upper-bound bpb of
1.61. These human-level models indicate that hu-
man translators are actually only adding ∼ 32%
more information on top of the original, and that
our current translation models are only capturing
some fraction of this redundancy.7
Figure 10 shows compression of the entire
bilingual corpus, allowing us to compare with
the previous state-of-the-art (S´anchez-Mart´ınez et
al., 2012), which compresses a single, word-
interleaved bilingual corpus. It shows how PPMC
</bodyText>
<footnote confidence="0.997607666666667">
7Machine models can also generate guess sequences, and
we see that entropy of a 30m-character PPMC guess sequence
(1.43) upper-bounds actual PPMC bpb (1.28).
</footnote>
<bodyText confidence="0.971231888888889">
does on a concatenated Spanish/English file.
Uncompressed English (294.5 Mb) is 90.6% the
size of uncompressed Spanish (324.9 Mb). Huff-
man narrows this gap to 93.0%, and PPM nar-
rows it further to 94.4%, consistent with Behr et
al. (2003) and Liberman (2008). Spanish redun-
dancies like adjective-noun agreement and bal-
anced question marks (“¿ ... ?”) may remain un-
exploited.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978897435898">
We have created a bilingual text compression chal-
lenge web site.8 This web site contains standard
bilingual data, specifies what a valid compression
is, and maintains benchmark results.
There are many future directions to pursue.
First, we would like to develop and exploit better
predictive translation modeling. We have so far
adapted machine translation technology circa only
1996. For example, the HMM alignment model
cannot “cross off” a source word and stop trying to
translate it. Also possible are phrase-based trans-
lation, neural nets, or as-yet-unanticipated pattern-
finding algorithms. We only require an executable
that prints the bilingual text.
Our current method requires segment-aligned
input. To work with real-life bilingual corpora,
the compressor should take care of segment align-
ment, in a way that allows decompression back to
the original text. Similarly, we are currently re-
stricted to texts written in the Latin alphabet, per
our definition of “word.”
More broadly, we would also like to import
more compression ideas into NLP. Compression
has so far appeared sporadically in NLP tasks
like native language ID (Bobicev, 2013), text in-
put methods (Powers and Huang, 2004), word
segmentation (Teahan et al., 2000; Sornil and
Chaiwanarom, 2004; Hutchens and Alder, 1998),
alignment (Liu et al., 2014), and text categoriza-
tion (Caruana &amp; Lang, unpub. 1995).
Translation researchers may also view bilingual
compression as an alternate, reference-free evalu-
ation metric for translation models. We anticipate
that future ideas from bilingual compression can
be brought back into translation. Like Brown et
al. (1992), with their gauntlet thrown down and
fury of competitive energy, we hope that cross-
fertilizing compression and translation will bring
fresh ideas to both areas.
</bodyText>
<footnote confidence="0.958708">
8www.isi.edu/natural-language/compression
</footnote>
<page confidence="0.997655">
896
</page>
<sectionHeader confidence="0.998432" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.546851">
This work was supported by a USC Provost Fel-
lowship and ARO grant W911NF-10-1-0533.
</bodyText>
<sectionHeader confidence="0.995317" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876435643565">
Joaqufn Adiego, Nieves R. Brisaboa, Miguel A.
Martfnez-Prieto, and Felipe S´anchez-Martfnez.
2009. A two-level structure for compressing aligned
bitexts. In String Processing and Information Re-
trieval. Springer-Verlag.
Joaqufn Adiego, Miguel A. Martfnez-Prieto, Javier E.
Hoyos-Torfo, and Felipe S´anchez-Martfnez. 2010.
Modelling parallel texts for boosting compression.
In Proc. Data Compression Conference (DCC).
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical Report http://bit.ly/1u9jJsx, Johns Hop-
kins University.
F. Behr, Victoria Fossum, Michael Mitzenmacher, and
David Xiao. 2003. Estimating and comparing en-
tropies across written natural languages using PPM
compression. In Proc. Data Compression Confer-
ence (DCC).
Victoria Bobicev. 2013. Native language identification
with PPM. In Proc. NAACL.
Peter F. Brown, Vincent J. Della Pietra, Robert L. Mer-
cer, Stephen A Della Pietra, and Jennifer C. Lai.
1992. An estimate of an upper bound for the entropy
of English. Computational Linguistics, 18(1):31–
40.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263–311.
John G. Cleary and Ian Witten. 1984. Data com-
pression using adaptive coding and partial string
matching. IEEE Transactions on Communications,
32(4):396–402.
Ehud S. Conley and Shmuel T. Klein. 2008. Using
alignment for multilingual text compression. Inter-
national Journal of Foundations of Computer Sci-
ence, 19(01):89–101.
Ehud S. Conley and Shmuel T. Klein. 2013. Im-
proved alignment-based algorithm for multilingual
text compression. Mathematics in Computer Sci-
ence, 7(2):137–153.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proc. EMNLP.
David A. Huffman et al. 1952. A method for the con-
struction of minimum redundancy codes. Proc. IRE,
40(9):1098–1101.
Jason L. Hutchens and Michael D Alder. 1998. Find-
ing structure via compression. In Proc. Joint Con-
ferences on New Methods in Language Processing
and Computational Natural Language Learning.
Marcus Hutter. 2006. 50,000 Euro prize for com-
pressing human knowledge. http://prize.
hutter1.net. Accessed: 2015-02-04.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Poster and Demo Session.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT Summit
X.
Patrik Lambert, Adri`a De Gispert, Rafael Banchs, and
Jos´e B Mari˜no. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Proc. NAACL,
pages 394–402. Association for Computational Lin-
guistics.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proc. HLT-NAACL.
Mark Liberman. 2008. Is English more efficient than
Chinese after all? http://languagelog.
ldc.upenn.edu/nll/?p=93. Accessed:
2015-02-04.
Wei Liu, Zhipeng Chang, and William J. Teahan.
2014. Experiments with a PPM compression-
based method for English-Chinese bilingual sen-
tence alignment. In Statistical Language and Speech
Processing, pages 70–81. Springer.
M. V. Mahoney. 2005. Adaptive weighting of context
models for lossless data compression. Technical Re-
port CS-2005-16, Florida Institute of Technology.
M. A. Martfnez-Prieto, J. Adiego, F. S´anchez-
Martfnez, P. de la Fuente, and R. C. Carrasco. 2009.
On the use of word alignments to enhance bitext
compression. In Proc. Data Compression Confer-
ence (DCC).
Radford M. Neal and Geoffrey E Hinton. 1998. A
view of the EM algorithm that justifies incremental,
sparse, and other variants. In Learning in Graphical
Models, pages 355–368. Springer.
</reference>
<page confidence="0.980615">
897
</page>
<reference confidence="0.999837848484849">
Craig Nevill and Timothy Bell. 1992. Compression of
parallel texts. Information Processing &amp; Manage-
ment, 28(6):781–793.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. ACL.
David Martin Powers and Jin Hu Huang. 2004. Adap-
tive compression-based approach for chinese pinyin
input. In Proc. Third SIGHAN Workshop on Chinese
Language Learning.
Jorma Rissanen and Glen G Langdon Jr. 1981. Univer-
sal modeling and coding. Information Theory, IEEE
Transactions on, 27(1):12–23.
Felipe S´anchez-Martinez, Rafael C. Carrasco,
Miguel A. Martinez-Prieto, and Joaquin Adiego.
2012. Generalized biwords for bitext compression
and translation spotting. Journal of Artificial
Intelligence Research, 43:389–418.
Ohm Sornil and Paweena Chaiwanarom. 2004. Com-
bining prediction by partial matching and logistic re-
gression for thai word segmentation. In Proc. COL-
ING, page 1208. Association for Computational Lin-
guistics.
William John Teahan, Yingying Wen, Rodger McNab,
and Ian H. Witten. 2000. A compression-based al-
gorithm for Chinese word segmentation. Computa-
tional Linguistics, 26(3):375–393.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. COLING.
Ian H. Witten, Radford M. Neal, and John G. Cleary.
1987. Arithmetic coding for data compression.
Communications of the ACM, 30(6):520–540.
</reference>
<page confidence="0.997915">
898
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864664">
<title confidence="0.999779">How Much Information Does a Human Translator Add to the Original?</title>
<author confidence="0.994664">Barret Zoph</author>
<author confidence="0.994664">Marjan Ghazvininejad</author>
<author confidence="0.994664">Kevin</author>
<affiliation confidence="0.996569333333333">Information Sciences Department of Computer University of Southern</affiliation>
<abstract confidence="0.9870892">We ask how much information a human translator adds to an original text, and we provide a bound. We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator? We develop new compression algorithms and establish a benchmark task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joaqufn Adiego</author>
<author>Nieves R Brisaboa</author>
<author>Miguel A Martfnez-Prieto</author>
<author>Felipe S´anchez-Martfnez</author>
</authors>
<title>A two-level structure for compressing aligned bitexts.</title>
<date>2009</date>
<booktitle>In String Processing and Information Retrieval.</booktitle>
<publisher>Springer-Verlag.</publisher>
<marker>Adiego, Brisaboa, Martfnez-Prieto, S´anchez-Martfnez, 2009</marker>
<rawString>Joaqufn Adiego, Nieves R. Brisaboa, Miguel A. Martfnez-Prieto, and Felipe S´anchez-Martfnez. 2009. A two-level structure for compressing aligned bitexts. In String Processing and Information Retrieval. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaqufn Adiego</author>
<author>Miguel A Martfnez-Prieto</author>
<author>Javier E Hoyos-Torfo</author>
<author>Felipe S´anchez-Martfnez</author>
</authors>
<title>Modelling parallel texts for boosting compression.</title>
<date>2010</date>
<booktitle>In Proc. Data Compression Conference (DCC).</booktitle>
<marker>Adiego, Martfnez-Prieto, Hoyos-Torfo, S´anchez-Martfnez, 2010</marker>
<rawString>Joaqufn Adiego, Miguel A. Martfnez-Prieto, Javier E. Hoyos-Torfo, and Felipe S´anchez-Martfnez. 2010. Modelling parallel texts for boosting compression. In Proc. Data Compression Conference (DCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical Report http://bit.ly/1u9jJsx,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="11979" citStr="Al-Onaizan et al. (1999)" startWordPosition="1945" endWordPosition="1948">nez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, then compressing this sequence of biwords. S´anchez-Martinez et al. (2012) improve the interleaving scheme and include offsets to enable decompression to reconstruct the original word order. They also compare several characterbased and word-based compression schemes for biword sequences. On Spanish-English Europarl data, they reach an 18.7% compression rate on word-interleaved text, compared to 20.1% for concatenated texts, a 7.2% improvement. Al-Onaizan et al. (1999) study the perplexity of learned translation models, i.e., the probability assigned to the target corpus given the source corpus. They observed iterative training to improve training-set perplexity (as guaranteed) but degrade test-set perplexity. They hypothesized that an increasingly tight, unsmoothed translation dictionary might exclude word translations needed to explain test-set data. Subsequently, research moved to extrinsic evaluation of translation models, in the context of end-to-end machine translation. Foster et al. (2002) and others have used prediction to propose auto-completions t</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Technical Report http://bit.ly/1u9jJsx, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Behr</author>
<author>Victoria Fossum</author>
<author>Michael Mitzenmacher</author>
<author>David Xiao</author>
</authors>
<title>Estimating and comparing entropies across written natural languages using PPM compression.</title>
<date>2003</date>
<booktitle>In Proc. Data Compression Conference (DCC).</booktitle>
<contexts>
<context position="30288" citStr="Behr et al. (2003)" startWordPosition="5029" endWordPosition="5032"> shows compression of the entire bilingual corpus, allowing us to compare with the previous state-of-the-art (S´anchez-Mart´ınez et al., 2012), which compresses a single, wordinterleaved bilingual corpus. It shows how PPMC 7Machine models can also generate guess sequences, and we see that entropy of a 30m-character PPMC guess sequence (1.43) upper-bounds actual PPMC bpb (1.28). does on a concatenated Spanish/English file. Uncompressed English (294.5 Mb) is 90.6% the size of uncompressed Spanish (324.9 Mb). Huffman narrows this gap to 93.0%, and PPM narrows it further to 94.4%, consistent with Behr et al. (2003) and Liberman (2008). Spanish redundancies like adjective-noun agreement and balanced question marks (“¿ ... ?”) may remain unexploited. 7 Conclusion We have created a bilingual text compression challenge web site.8 This web site contains standard bilingual data, specifies what a valid compression is, and maintains benchmark results. There are many future directions to pursue. First, we would like to develop and exploit better predictive translation modeling. We have so far adapted machine translation technology circa only 1996. For example, the HMM alignment model cannot “cross off” a source </context>
</contexts>
<marker>Behr, Fossum, Mitzenmacher, Xiao, 2003</marker>
<rawString>F. Behr, Victoria Fossum, Michael Mitzenmacher, and David Xiao. 2003. Estimating and comparing entropies across written natural languages using PPM compression. In Proc. Data Compression Conference (DCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Bobicev</author>
</authors>
<title>Native language identification with PPM.</title>
<date>2013</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="31587" citStr="Bobicev, 2013" startWordPosition="5231" endWordPosition="5232">ral nets, or as-yet-unanticipated patternfinding algorithms. We only require an executable that prints the bilingual text. Our current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and tran</context>
</contexts>
<marker>Bobicev, 2013</marker>
<rawString>Victoria Bobicev. 2013. Native language identification with PPM. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
<author>Stephen A Della Pietra</author>
<author>Jennifer C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>40</pages>
<contexts>
<context position="5738" citStr="Brown et al., 1992" startWordPosition="923" endWordPosition="926">enchmark bilingual text compression challenge to stimulate new researchers to find and exploit patterns in bilingual text. Ultimately, we want to feed those ideas into practical machine translation systems. 2 Data We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2). Portions of this large corpus have been used in previous compression work (S´anchez-Martinez et al., 2012). The Spanish side is in UTF-8. For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline. Our task is to compress the data “as is”: untokenized, but already segment aligned. We also include a tokenized version with 334 manually word-aligned segment pairs (Lambert et al., 2005) distributed throughout the corpus. For rapid development and testing, we have arranged a smaller corpus that is 10% the size of the full corpus (Figure 3). 3 Monolingual compression Compression captures patterns in data. Language modeling also captures patterns, but at first blush, these two areas seem distinct. In compression, we seek a small executable that prints out a text, while in langua</context>
</contexts>
<marker>Brown, Pietra, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer, Stephen A Della Pietra, and Jennifer C. Lai. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics, 18(1):31– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13332" citStr="Brown et al. (1993)" startWordPosition="2159" endWordPosition="2162">lgorithm compresses target-language file2 while looking at source-language file1: % bicompress file2 file1 &gt; file2.exe To make use of arithmetic coding, we consider the task of predicting the next target character, given the source sentence and target string so far:3 P(ej|f1 ... fl, e1 ... ej−1) If we are able to accurately predict what a human translator will type next, then we should be able to build a good machine translator. Here is an example of the task: Spanish: Pido que hagamos un minuto de silencio. English so far: I should like to ob 3We predict e from f in this paper, reversed from Brown et al. (1993), who predict f from e. Absolute Relative offsets offsets Uncompressed 105.3 Mb 88.9 Mb Huffman coding 36.6 Mb 24.4 Mb PPMC 12.4 Mb 13.2 Mb Figure 5: Compressing a file of (unidirectional) automatic Viterbi word alignments computed from our large Spanish/English corpus (sentences less than 50 words). 5.1 Word alignment Let us first work at the word level instead of the character level. If we are predicting the jth English word, and we know that it translates fi (“aligns to fi”), and if fi has only a handful of translations, then we may be able to specify ej with just a few bits. We may therefo</context>
<context position="15133" citStr="Brown et al., 1993" startWordPosition="2476" endWordPosition="2479">. Therefore, we must include those alignments at the beginning of the compressed file. So let’s compress them too. How compressible are alignment sequences? Figure 5 gives results for Viterbi alignments derived from our large parallel Spanish/English corpus. First, some interesting facts: • Huffman works better on relative offsets, because the common “+1” gets a short bit code. • PPMC’s use of context makes it impressively insensitive to alignment format. • PPMC beats Huffman on relative offsets. This would not happen if relative offset integers were independent of one another, as assumed by (Brown et al., 1993) and (Vogel et al., 1996). Bigram statistics bear this out: P(+1 |-2) = 0.20 P(+1 |+1) = 0.59 P(+1 |-1) = 0.20 P(+1 |+2) = 0.49 P(+1 |0) = 0.52 So this small compression experiment already 892 suggests that translation aligners might want to model more context than just P(offset). However, the main point of Figure 5 is that the compressed alignment file requires 12.4 Mb! This is too large for us to prepend to our compressed file, for the sake of enabling decompression. E(m|l) 2. For j = 1..m, choose alignment aj a(aj|j, l) 3. For j = 1..m, choose translation ej t(ej|faj) which, via the “IBM tr</context>
<context position="19626" citStr="Brown et al., 1993" startWordPosition="3273" endWordPosition="3276">ization, with two causes of failure. First, EM training sets o(0) too high, leading to absolute alignments like “1 2 2 2 5 5 5 5 ... ”. We avoid 4In their online EM Model 1 aligner, Liang and Klein (p.c.) skirt the smoothing issue by running an epoch of batch EM to initialize a full set of probabilities before starting. 5.2 Translation dictionary Another approach is to forget Viterbi alignments and instead exploit a probabilistic translation dictionary table t(e|f). To predict the next target word ej, we admit the possibility that ej might be translating any of the source tokens. IBM Model 2 (Brown et al., 1993) tells us how to do this: Given f1 ... fl: 1. Choose English length m 893 Against silver standard (Batch EM) Against gold standard (Human) IBM1 IBM2 HMM IBM1 IBM2 HMM Batch EM 100.0 100.0 100.0 55.4 66.3 70.2 Online EM Full data 82.2 81.7 89.5 54.9 63.4 70.0 First 50% 81.4 79.5 88.9 54.9 62.3 70.6 Last 50% 83.0 83.9 90.0 54.9 64.5 69.4 Reordered 83.7 83.3 88.1 56.8 65.4 69.5 Figure 6: Word alignment f-scores. Batch EM for IBM 1 is run for 5 iterations; Batch IBM2 adds 5 further iterations of IBM2; Batch HMM adds a further 5 iterations of HMM. Online EM is singlepass. Against the silver standar</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Cleary</author>
<author>Ian Witten</author>
</authors>
<title>Data compression using adaptive coding and partial string matching.</title>
<date>1984</date>
<journal>IEEE Transactions on Communications,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="9052" citStr="Cleary and Witten, 1984" startWordPosition="1462" endWordPosition="1465">xt-dependent probability intervals, and each time we observe a character, we move to its interval. Our working interval becomes smaller and smaller, but the better our predictions, the wider it stays. A document’s compression is the shortest bit string that fits inside the final interval. In practice, we do the bit-coding as we navigate probability intervals. Arithmetic coding separates modeling and compression, making our job similar to language modeling, where we use try to use context to predict the next symbol. 3.1 PPM PPM is the most well-known adaptive, predictive compression technique (Cleary and Witten, 1984). PPM updates character n-gram tables (usually n=1..5) as it compresses. In a given context, an n-gram table may predict only a subset of characters, so PPM reserves some probability mass for an escape (ESC), after which it executes a hard backoff to the (n-1)-gram table. In PPMA, P(ESC) is 1/(1+D), where D is the number of times the context has been seen. PPMB uses q/D, where q is the number of distinct character types seen in the context. PPMC uses q/(q+D), aka Witten-Bell. PPMD uses q/2D. PPM* uses the shortest previously-seen deterministic context, which may be quite long. If there is no d</context>
</contexts>
<marker>Cleary, Witten, 1984</marker>
<rawString>John G. Cleary and Ian Witten. 1984. Data compression using adaptive coding and partial string matching. IEEE Transactions on Communications, 32(4):396–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud S Conley</author>
<author>Shmuel T Klein</author>
</authors>
<title>Using alignment for multilingual text compression.</title>
<date>2008</date>
<journal>International Journal of Foundations of Computer Science,</journal>
<volume>19</volume>
<issue>01</issue>
<contexts>
<context position="11061" citStr="Conley and Klein (2008)" startWordPosition="1806" endWordPosition="1809">vel instead of the character-level. The unit’s model weights are adaptively updated by: wi ← wi + ηxi(correct − P(1)), where xi = ln(Pi(1)/(1 − Pi(1)) η = fixed learning rate Pi(1) = ith model’s prediction PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII &gt; 32), a bigram model, and a skip-bigram model. 4 Bilingual Compression: Prior Work Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase corpora, such as different English versions of the Bible. Conley and Klein (2008) and Conley and Klein (2013) compress a target text that has been wordaligned to a source text, to which they add a lemmatizer and bilingual glossary. They obtain a 1%- 6% improvement over monolingual compression, 891 without counting the cost of auxiliary files needed for decompression. Martinez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, then compressing this sequence of biwords. S´anchez-Martinez et al. (2012) improve the interleaving scheme and include offsets to enable decompression to </context>
<context position="14034" citStr="Conley and Klein, 2008" startWordPosition="2282" endWordPosition="2285">88.9 Mb Huffman coding 36.6 Mb 24.4 Mb PPMC 12.4 Mb 13.2 Mb Figure 5: Compressing a file of (unidirectional) automatic Viterbi word alignments computed from our large Spanish/English corpus (sentences less than 50 words). 5.1 Word alignment Let us first work at the word level instead of the character level. If we are predicting the jth English word, and we know that it translates fi (“aligns to fi”), and if fi has only a handful of translations, then we may be able to specify ej with just a few bits. We may therefore suppose that a set of Viterbi word alignments may be useful for compression (Conley and Klein, 2008; S´anchezMartinez et al., 2012). We consider unidirectional alignments that link each target position j to a single source position i (including the null word at i = 0). Such alignments can be computed automatically using EM (Brown et al., 1993), and stored in one of two formats: Absolute: 1 2 5 5 7 0 3 6 .. . Relative: +1 +1 +3 0 +2 null -4 +3 ... In order to interpret the bits produced by the compressor, our decompressor must also have access to the same Viterbi alignments. Therefore, we must include those alignments at the beginning of the compressed file. So let’s compress them too. How c</context>
</contexts>
<marker>Conley, Klein, 2008</marker>
<rawString>Ehud S. Conley and Shmuel T. Klein. 2008. Using alignment for multilingual text compression. International Journal of Foundations of Computer Science, 19(01):89–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud S Conley</author>
<author>Shmuel T Klein</author>
</authors>
<title>Improved alignment-based algorithm for multilingual text compression.</title>
<date>2013</date>
<journal>Mathematics in Computer Science,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="11089" citStr="Conley and Klein (2013)" startWordPosition="1811" endWordPosition="1814">-level. The unit’s model weights are adaptively updated by: wi ← wi + ηxi(correct − P(1)), where xi = ln(Pi(1)/(1 − Pi(1)) η = fixed learning rate Pi(1) = ith model’s prediction PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII &gt; 32), a bigram model, and a skip-bigram model. 4 Bilingual Compression: Prior Work Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase corpora, such as different English versions of the Bible. Conley and Klein (2008) and Conley and Klein (2013) compress a target text that has been wordaligned to a source text, to which they add a lemmatizer and bilingual glossary. They obtain a 1%- 6% improvement over monolingual compression, 891 without counting the cost of auxiliary files needed for decompression. Martinez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, then compressing this sequence of biwords. S´anchez-Martinez et al. (2012) improve the interleaving scheme and include offsets to enable decompression to reconstruct the original wor</context>
</contexts>
<marker>Conley, Klein, 2013</marker>
<rawString>Ehud S. Conley and Shmuel T. Klein. 2013. Improved alignment-based algorithm for multilingual text compression. Mathematics in Computer Science, 7(2):137–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>User-friendly text prediction for translators.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="12517" citStr="Foster et al. (2002)" startWordPosition="2020" endWordPosition="2023">ared to 20.1% for concatenated texts, a 7.2% improvement. Al-Onaizan et al. (1999) study the perplexity of learned translation models, i.e., the probability assigned to the target corpus given the source corpus. They observed iterative training to improve training-set perplexity (as guaranteed) but degrade test-set perplexity. They hypothesized that an increasingly tight, unsmoothed translation dictionary might exclude word translations needed to explain test-set data. Subsequently, research moved to extrinsic evaluation of translation models, in the context of end-to-end machine translation. Foster et al. (2002) and others have used prediction to propose auto-completions to speed up human translation. As we have seen, prediction and compression are highly related. 5 Predictive Bilingual Compression Our algorithm compresses target-language file2 while looking at source-language file1: % bicompress file2 file1 &gt; file2.exe To make use of arithmetic coding, we consider the task of predicting the next target character, given the source sentence and target string so far:3 P(ej|f1 ... fl, e1 ... ej−1) If we are able to accurately predict what a human translator will type next, then we should be able to buil</context>
</contexts>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>George Foster, Philippe Langlais, and Guy Lapalme. 2002. User-friendly text prediction for translators. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Huffman</author>
</authors>
<title>A method for the construction of minimum redundancy codes.</title>
<date>1952</date>
<booktitle>Proc. IRE,</booktitle>
<volume>40</volume>
<issue>9</issue>
<marker>Huffman, 1952</marker>
<rawString>David A. Huffman et al. 1952. A method for the construction of minimum redundancy codes. Proc. IRE, 40(9):1098–1101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason L Hutchens</author>
<author>Michael D Alder</author>
</authors>
<title>Finding structure via compression. In</title>
<date>1998</date>
<booktitle>Proc. Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="31729" citStr="Hutchens and Alder, 1998" startWordPosition="5251" endWordPosition="5254">ent method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas. 8www.isi.edu/natural-language/compression 896 Acknowledgements This work was supported by a USC </context>
</contexts>
<marker>Hutchens, Alder, 1998</marker>
<rawString>Jason L. Hutchens and Michael D Alder. 1998. Finding structure via compression. In Proc. Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Hutter</author>
</authors>
<title>50,000 Euro prize for compressing human knowledge.</title>
<date>2006</date>
<booktitle>http://prize. hutter1.net. Accessed:</booktitle>
<pages>2015--02</pages>
<contexts>
<context position="2482" citStr="Hutter, 2006" startWordPosition="417" endWordPosition="418">t week killed at least 12 persons. At least 12 people lost their lives in last week’s fighting. At least 12 persons died in the fighting last week. At least 12 died in the battle last week. At least 12 people were killed in the fighting last week. During last week’s fighting, at least 12 people died. Last week at least twelve people died in the fighting. Last week’s fighting took the lives of twelve people. Figure 1: Eleven human translations of the same source sentence (LDC2002T01). We adopt the same scheme used in monolingual text compression benchmark evaluations, such as the Hutter Prize (Hutter, 2006), a competition to compress a 100m-word extract of English Wikipedia. A valid entry is an executable, or self-extracting archive, that prints out Wikipedia, byte-for-byte. Decompression code, dictionaries, and/or other resources must be embedded in the executable—we cannot assume that the recipient of the compressed file has access to those resources. This view of compression goes by the name of algorithmic information theory (or Kolmogorov complexity). Any executable is permitted. For example, if our job were to compress the first million digits of π, then we might submit a very short piece o</context>
</contexts>
<marker>Hutter, 2006</marker>
<rawString>Marcus Hutter. 2006. 50,000 Euro prize for compressing human knowledge. http://prize. hutter1.net. Accessed: 2015-02-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="20354" citStr="Koehn et al., 2003" startWordPosition="3402" endWordPosition="3405">Against gold standard (Human) IBM1 IBM2 HMM IBM1 IBM2 HMM Batch EM 100.0 100.0 100.0 55.4 66.3 70.2 Online EM Full data 82.2 81.7 89.5 54.9 63.4 70.0 First 50% 81.4 79.5 88.9 54.9 62.3 70.6 Last 50% 83.0 83.9 90.0 54.9 64.5 69.4 Reordered 83.7 83.3 88.1 56.8 65.4 69.5 Figure 6: Word alignment f-scores. Batch EM for IBM 1 is run for 5 iterations; Batch IBM2 adds 5 further iterations of IBM2; Batch HMM adds a further 5 iterations of HMM. Online EM is singlepass. Against the silver standard, alignments are unidirectional; against gold, they are bidirectional and symmetrized with grow-dial-final (Koehn et al., 2003). First and last 50% report on different portions of the corpus. Reordered is on segment pairs ordered short to long. All runs exclude segment pairs with segments longer than 50 words. this with a standard schedule of 5 IBM1 iterations, 5 IBM2 iterations, then 5 HMM iterations. However, HMM still learns a very high value for p1, aligning most tokens to null, so we fix p1 = 0.1 for the duration of training. Single-pass, online HMM suffers the same two problems, both solved when we smooth differentially (Ao = 102, At = 10−4) and fix p1 = 0.1. Two quick asides before we examine the effectiveness </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL Poster and Demo Session.</booktitle>
<contexts>
<context position="22673" citStr="Koehn et al., 2007" startWordPosition="3778" endWordPosition="3781">on gives f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2, p=85.5, r=61.0), and its differently parameterized HMM. Alignment Test Set Bleu speed Europarl News Batch HMM 1230.78 min 30.2 26.2 Online HMM 711.87 min 30.0 25.3 Figure 7: Fast, single-pass HMM alignment yields competitive Spanish-English Moses phrasebased translation accuracy, as measured by Bleu (Papineni et al., 2002). In-domain (Europarl) and out-of-domain (SMAT-07 News Commentary) tune/test sets each consist of approximately 1000 sentences, all longer than 50 words to avoid overlap with training. machine translation (Koehn et al., 2007). Figure 7 shows that we can achieve competitive translation accuracy using fast, single-pass alignment, speeding up the system development cycle. For this use case, we can get an additional +0.3 alignment fscore (just as fast) if we print Viterbi alignments in a second pass instead of during training. 5.4 Word tokenization We now want our continuously-improving translation model (TM) to predict target text, and to combine its predictions with PPM’s. For that to happen, our TM will need to predict the exact text, including spurious double-spaces, how parentheses combine with quotation marks, a</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL Poster and Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X.</booktitle>
<contexts>
<context position="5414" citStr="Koehn, 2005" startWordPosition="869" endWordPosition="870">ng at (uncompressed) file1. The contributions of this paper are: 1. We provide a new quantitative bound for how much information a translator adds to an original text. 2. We present practical software to compress bilingual text with compression rates that exceed the previous state-of-the-art. 3. We set up a public benchmark bilingual text compression challenge to stimulate new researchers to find and exploit patterns in bilingual text. Ultimately, we want to feed those ideas into practical machine translation systems. 2 Data We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2). Portions of this large corpus have been used in previous compression work (S´anchez-Martinez et al., 2012). The Spanish side is in UTF-8. For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline. Our task is to compress the data “as is”: untokenized, but already segment aligned. We also include a tokenized version with 334 manually word-aligned segment pairs (Lambert et al., 2005) distributed throughout the corpus. For rapid development and testing, we</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Adri`a De Gispert</author>
<author>Rafael Banchs</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Guidelines for word alignment evaluation and manual alignment.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<marker>Lambert, De Gispert, Banchs, Mari˜no, 2005</marker>
<rawString>Patrik Lambert, Adri`a De Gispert, Rafael Banchs, and Jos´e B Mari˜no. 2005. Guidelines for word alignment evaluation and manual alignment. Language Resources and Evaluation, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16772" citStr="Levenberg et al. (2010)" startWordPosition="2770" endWordPosition="2774">.. fl) _ [1 − P(STOP|j, l)] Eli=0 a(i|j, l)t(ej|fi) We can train t, a, and c on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 Adaptive translation modeling Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string. Neal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case. We initialize a uniform translation model, use it to collect fractional counts from the first segment pair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on. Because we pass through the data only once, we hope to converge quickly to high-quality tables for compressing the bulk of the text. Unlike in batch EM, we need not keep separate count and probability tables. We only need count tables</context>
</contexts>
<marker>Levenberg, Callison-Burch, Osborne, 2010</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Proc. NAACL, pages 394–402. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="16744" citStr="Liang and Klein (2009)" startWordPosition="2765" endWordPosition="2768">l, e1 ... ej−1) ∼ P(ej|f1 ... fl) _ [1 − P(STOP|j, l)] Eli=0 a(i|j, l)t(ej|fi) We can train t, a, and c on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 Adaptive translation modeling Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string. Neal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case. We initialize a uniform translation model, use it to collect fractional counts from the first segment pair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on. Because we pass through the data only once, we hope to converge quickly to high-quality tables for compressing the bulk of the text. Unlike in batch EM, we need not keep separate count and probability table</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>Percy Liang and Dan Klein. 2009. Online EM for unsupervised models. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
</authors>
<title>Is English more efficient than Chinese after all?</title>
<date>2008</date>
<booktitle>http://languagelog. ldc.upenn.edu/nll/?p=93. Accessed:</booktitle>
<pages>2015--02</pages>
<contexts>
<context position="30308" citStr="Liberman (2008)" startWordPosition="5034" endWordPosition="5035">he entire bilingual corpus, allowing us to compare with the previous state-of-the-art (S´anchez-Mart´ınez et al., 2012), which compresses a single, wordinterleaved bilingual corpus. It shows how PPMC 7Machine models can also generate guess sequences, and we see that entropy of a 30m-character PPMC guess sequence (1.43) upper-bounds actual PPMC bpb (1.28). does on a concatenated Spanish/English file. Uncompressed English (294.5 Mb) is 90.6% the size of uncompressed Spanish (324.9 Mb). Huffman narrows this gap to 93.0%, and PPM narrows it further to 94.4%, consistent with Behr et al. (2003) and Liberman (2008). Spanish redundancies like adjective-noun agreement and balanced question marks (“¿ ... ?”) may remain unexploited. 7 Conclusion We have created a bilingual text compression challenge web site.8 This web site contains standard bilingual data, specifies what a valid compression is, and maintains benchmark results. There are many future directions to pursue. First, we would like to develop and exploit better predictive translation modeling. We have so far adapted machine translation technology circa only 1996. For example, the HMM alignment model cannot “cross off” a source word and stop trying</context>
</contexts>
<marker>Liberman, 2008</marker>
<rawString>Mark Liberman. 2008. Is English more efficient than Chinese after all? http://languagelog. ldc.upenn.edu/nll/?p=93. Accessed: 2015-02-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Liu</author>
<author>Zhipeng Chang</author>
<author>William J Teahan</author>
</authors>
<title>Experiments with a PPM compressionbased method for English-Chinese bilingual sentence alignment.</title>
<date>2014</date>
<booktitle>In Statistical Language and Speech Processing,</booktitle>
<pages>70--81</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="31759" citStr="Liu et al., 2014" startWordPosition="5256" endWordPosition="5259">put. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas. 8www.isi.edu/natural-language/compression 896 Acknowledgements This work was supported by a USC Provost Fellowship and ARO gra</context>
</contexts>
<marker>Liu, Chang, Teahan, 2014</marker>
<rawString>Wei Liu, Zhipeng Chang, and William J. Teahan. 2014. Experiments with a PPM compressionbased method for English-Chinese bilingual sentence alignment. In Statistical Language and Speech Processing, pages 70–81. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M V Mahoney</author>
</authors>
<title>Adaptive weighting of context models for lossless data compression.</title>
<date>2005</date>
<tech>Technical Report CS-2005-16,</tech>
<institution>Florida Institute of Technology.</institution>
<contexts>
<context position="10210" citStr="Mahoney, 2005" startWordPosition="1665" endWordPosition="1666">istic context, which may be quite long. If there is no deterministic context, PPM* goes to the longest matching context and starts PPMD. Instead of the longest context, PPMZ rates all contexts between lengths 0 and 12 according to each context’s most probable character. PPMZ also implements an adaptive P(ESC) that combines context length, number of previous ESC in the context, etc. We use our own C++ implementation of PPMC for monolingual compression experiments in this paper. When we pass over a set of characters in favor of ESC, we remove those characters from the hard backoff. 3.2 PAQ PAQ (Mahoney, 2005) is a family of state-of-theart compression algorithms and a perennial Hutter Prize winner. PAQ combines hundreds of models with a logistic unit when making a prediction. This is most efficient when predictions are at the bit-level instead of the character-level. The unit’s model weights are adaptively updated by: wi ← wi + ηxi(correct − P(1)), where xi = ln(Pi(1)/(1 − Pi(1)) η = fixed learning rate Pi(1) = ith model’s prediction PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII &gt; 32), a</context>
</contexts>
<marker>Mahoney, 2005</marker>
<rawString>M. V. Mahoney. 2005. Adaptive weighting of context models for lossless data compression. Technical Report CS-2005-16, Florida Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Martfnez-Prieto</author>
<author>J Adiego</author>
<author>F S´anchezMartfnez</author>
<author>P de la Fuente</author>
<author>R C Carrasco</author>
</authors>
<title>On the use of word alignments to enhance bitext compression.</title>
<date>2009</date>
<booktitle>In Proc. Data Compression Conference (DCC).</booktitle>
<marker>Martfnez-Prieto, Adiego, S´anchezMartfnez, Fuente, Carrasco, 2009</marker>
<rawString>M. A. Martfnez-Prieto, J. Adiego, F. S´anchezMartfnez, P. de la Fuente, and R. C. Carrasco. 2009. On the use of word alignments to enhance bitext compression. In Proc. Data Compression Conference (DCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning in Graphical Models,</booktitle>
<pages>355--368</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="16639" citStr="Neal and Hinton (1998)" startWordPosition="2749" endWordPosition="2752"> predictions: P(STOP|f1 ... fl, e1 ... ej−1) ∼ P(STOP|j, l) _ E(j − 1|l)/ Emax k=j−1 e(k|l) P(ej|f1 ... fl, e1 ... ej−1) ∼ P(ej|f1 ... fl) _ [1 − P(STOP|j, l)] Eli=0 a(i|j, l)t(ej|fi) We can train t, a, and c on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file. 5.3 Adaptive translation modeling Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string. Neal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case. We initialize a uniform translation model, use it to collect fractional counts from the first segment pair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on. Because we pass through the data only once, we hope to converge quickly to high-quality tables for co</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>Radford M. Neal and Geoffrey E Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in Graphical Models, pages 355–368. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Nevill</author>
<author>Timothy Bell</author>
</authors>
<title>Compression of parallel texts.</title>
<date>1992</date>
<journal>Information Processing &amp; Management,</journal>
<volume>28</volume>
<issue>6</issue>
<contexts>
<context position="1177" citStr="Nevill and Bell, 1992" startWordPosition="183" endWordPosition="186"> a human translator? We develop new compression algorithms and establish a benchmark task. 1 Introduction Text compression exploits redundancy in human language to store documents compactly, and transmit them quickly. It is natural to think about compressing bilingual texts, which have even more redundancy: “From an information theoretic point of view, accurately translated copies of the original text would be expected to contain almost no extra information if the original text is available, so in principle it should be possible to store and transmit these texts with very little extra cost.” (Nevill and Bell, 1992) Of course, if we look at actual translation data (Figure 1), we see that there is quite a bit of unpredictability. But the intuition is sound. If there were a million equally-likely translations of a short sentence, it would only take us log2(1m) = 20 bits to specify which one. By finding and exploiting patterns in bilingual data, we want to provide an upper bound for this question: How much information does a human translator add to the original? We do this in the context of building a practical compressor for bilingual text. At least 12 people were killed in the battle last week. Last week’</context>
<context position="10908" citStr="Nevill and Bell (1992)" startWordPosition="1782" endWordPosition="1785">r Prize winner. PAQ combines hundreds of models with a logistic unit when making a prediction. This is most efficient when predictions are at the bit-level instead of the character-level. The unit’s model weights are adaptively updated by: wi ← wi + ηxi(correct − P(1)), where xi = ln(Pi(1)/(1 − Pi(1)) η = fixed learning rate Pi(1) = ith model’s prediction PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII &gt; 32), a bigram model, and a skip-bigram model. 4 Bilingual Compression: Prior Work Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase corpora, such as different English versions of the Bible. Conley and Klein (2008) and Conley and Klein (2013) compress a target text that has been wordaligned to a source text, to which they add a lemmatizer and bilingual glossary. They obtain a 1%- 6% improvement over monolingual compression, 891 without counting the cost of auxiliary files needed for decompression. Martinez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, t</context>
</contexts>
<marker>Nevill, Bell, 1992</marker>
<rawString>Craig Nevill and Timothy Bell. 1992. Compression of parallel texts. Information Processing &amp; Management, 28(6):781–793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="22448" citStr="Papineni et al., 2002" startWordPosition="3746" endWordPosition="3749">ranslation dictionaries. Because single-pass alignment is significantly faster than traditional multi-pass, we also investigate its impact on an overall Moses pipeline for phrase-based 5We confirm that our Batch HMM implementation gives f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2, p=85.5, r=61.0), and its differently parameterized HMM. Alignment Test Set Bleu speed Europarl News Batch HMM 1230.78 min 30.2 26.2 Online HMM 711.87 min 30.0 25.3 Figure 7: Fast, single-pass HMM alignment yields competitive Spanish-English Moses phrasebased translation accuracy, as measured by Bleu (Papineni et al., 2002). In-domain (Europarl) and out-of-domain (SMAT-07 News Commentary) tune/test sets each consist of approximately 1000 sentences, all longer than 50 words to avoid overlap with training. machine translation (Koehn et al., 2007). Figure 7 shows that we can achieve competitive translation accuracy using fast, single-pass alignment, speeding up the system development cycle. For this use case, we can get an additional +0.3 alignment fscore (just as fast) if we print Viterbi alignments in a second pass instead of during training. 5.4 Word tokenization We now want our continuously-improving translatio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martin Powers</author>
<author>Jin Hu Huang</author>
</authors>
<title>Adaptive compression-based approach for chinese pinyin input.</title>
<date>2004</date>
<booktitle>In Proc. Third SIGHAN Workshop on Chinese Language Learning.</booktitle>
<contexts>
<context position="31632" citStr="Powers and Huang, 2004" startWordPosition="5237" endWordPosition="5240">tternfinding algorithms. We only require an executable that prints the bilingual text. Our current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas.</context>
</contexts>
<marker>Powers, Huang, 2004</marker>
<rawString>David Martin Powers and Jin Hu Huang. 2004. Adaptive compression-based approach for chinese pinyin input. In Proc. Third SIGHAN Workshop on Chinese Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
<author>Glen G Langdon Jr</author>
</authors>
<title>Universal modeling and coding. Information Theory,</title>
<date>1981</date>
<journal>IEEE Transactions on,</journal>
<volume>27</volume>
<issue>1</issue>
<marker>Rissanen, Jr, 1981</marker>
<rawString>Jorma Rissanen and Glen G Langdon Jr. 1981. Universal modeling and coding. Information Theory, IEEE Transactions on, 27(1):12–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felipe S´anchez-Martinez</author>
<author>Rafael C Carrasco</author>
<author>Miguel A Martinez-Prieto</author>
<author>Joaquin Adiego</author>
</authors>
<title>Generalized biwords for bitext compression and translation spotting.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>43--389</pages>
<marker>S´anchez-Martinez, Carrasco, Martinez-Prieto, Adiego, 2012</marker>
<rawString>Felipe S´anchez-Martinez, Rafael C. Carrasco, Miguel A. Martinez-Prieto, and Joaquin Adiego. 2012. Generalized biwords for bitext compression and translation spotting. Journal of Artificial Intelligence Research, 43:389–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ohm Sornil</author>
<author>Paweena Chaiwanarom</author>
</authors>
<title>Combining prediction by partial matching and logistic regression for thai word segmentation.</title>
<date>2004</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1208</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="31702" citStr="Sornil and Chaiwanarom, 2004" startWordPosition="5247" endWordPosition="5250">s the bilingual text. Our current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas. 8www.isi.edu/natural-language/compression 896 Acknowledgements This w</context>
</contexts>
<marker>Sornil, Chaiwanarom, 2004</marker>
<rawString>Ohm Sornil and Paweena Chaiwanarom. 2004. Combining prediction by partial matching and logistic regression for thai word segmentation. In Proc. COLING, page 1208. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William John Teahan</author>
<author>Yingying Wen</author>
<author>Rodger McNab</author>
<author>Ian H Witten</author>
</authors>
<title>A compression-based algorithm for Chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="31672" citStr="Teahan et al., 2000" startWordPosition="5243" endWordPosition="5246">executable that prints the bilingual text. Our current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.” More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana &amp; Lang, unpub. 1995). Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas. 8www.isi.edu/natural-language/compressi</context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2000</marker>
<rawString>William John Teahan, Yingying Wen, Rodger McNab, and Ian H. Witten. 2000. A compression-based algorithm for Chinese word segmentation. Computational Linguistics, 26(3):375–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="15158" citStr="Vogel et al., 1996" startWordPosition="2481" endWordPosition="2484">ude those alignments at the beginning of the compressed file. So let’s compress them too. How compressible are alignment sequences? Figure 5 gives results for Viterbi alignments derived from our large parallel Spanish/English corpus. First, some interesting facts: • Huffman works better on relative offsets, because the common “+1” gets a short bit code. • PPMC’s use of context makes it impressively insensitive to alignment format. • PPMC beats Huffman on relative offsets. This would not happen if relative offset integers were independent of one another, as assumed by (Brown et al., 1993) and (Vogel et al., 1996). Bigram statistics bear this out: P(+1 |-2) = 0.20 P(+1 |+1) = 0.59 P(+1 |-1) = 0.20 P(+1 |+2) = 0.49 P(+1 |0) = 0.52 So this small compression experiment already 892 suggests that translation aligners might want to model more context than just P(offset). However, the main point of Figure 5 is that the compressed alignment file requires 12.4 Mb! This is too large for us to prepend to our compressed file, for the sake of enabling decompression. E(m|l) 2. For j = 1..m, choose alignment aj a(aj|j, l) 3. For j = 1..m, choose translation ej t(ej|faj) which, via the “IBM trick” implies: P(e1 7 ... </context>
<context position="18339" citStr="Vogel et al., 1996" startWordPosition="3040" endWordPosition="3043">+1) where |VE |is the size of the English vocabulary. We determine |VE |via a quick initial pass through the data, then include it at the top of our compressed file. In batch EM, we usually run IBM Model 1 for a few iterations before Model 2, gripped by an atavistic fear that the a probabilities will enforce rigid alignments before word co-occurrences have a chance to settle in. It turns out this fear is justified in online EM! Because the a table initially learns to align most words to null, we smooth it more heavily (λa = 102, λt = 10−4). We also implement a single-pass HMM alignment model (Vogel et al., 1996). In the IBM models, we can either collect fractional counts after we have compressed a whole sentence, or we can do it word-by-word. In the HMM model, alignment choices are no longer independent of one another: Given f1 ... fl: 1. Choose English length m w/prob e(m|l) 2. For j = 1..m: 2a. set aj to null w/prob pi, or 2b. choose non-null aj w/prob (1 − pl)o(aj − ak) 3. For j = 1..m, choose translation ej w/prob t(ej|fa7 ) In the expression o(aj − ak), k is the maximum English index (k &lt; j) such that ak =6 0. The relative offset o-table learns to encourage adjacent English words to align to adj</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Radford M Neal</author>
<author>John G Cleary</author>
</authors>
<title>Arithmetic coding for data compression.</title>
<date>1987</date>
<journal>Communications of the ACM,</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="8377" citStr="Witten et al., 1987" startWordPosition="1352" endWordPosition="1355">m, just as the compressor counted characters as it consumed them. Adaptive compression can also nicely accommodate shifting topics in text, if we give higher counts to recent events. By its single-pass nature, it is also good for streaming data. Arithmetic coding. Huffman coding exploits a predictive unigram distribution over the next character. If we use more context, we can make sharper distributions. An n-gram table is one way to map contexts onto predictions. How do we convert good predictions into good compression? The solution is called arithmetic coding (Rissanen and Langdon Jr., 1981; Witten et al., 1987). Figure 4 sketches the technique. We produce context-dependent probability intervals, and each time we observe a character, we move to its interval. Our working interval becomes smaller and smaller, but the better our predictions, the wider it stays. A document’s compression is the shortest bit string that fits inside the final interval. In practice, we do the bit-coding as we navigate probability intervals. Arithmetic coding separates modeling and compression, making our job similar to language modeling, where we use try to use context to predict the next symbol. 3.1 PPM PPM is the most well</context>
</contexts>
<marker>Witten, Neal, Cleary, 1987</marker>
<rawString>Ian H. Witten, Radford M. Neal, and John G. Cleary. 1987. Arithmetic coding for data compression. Communications of the ACM, 30(6):520–540.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
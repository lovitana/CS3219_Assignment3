<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003380">
<title confidence="0.99945">
Improving Distant Supervision for Information Extraction Using Label
Propagation Through Lists
</title>
<author confidence="0.992161">
Lidong Bing§ Sneha Chaudhari§ Richard C. Wangb William W. Cohen§
</author>
<affiliation confidence="0.994655">
§Carnegie Mellon University, Pittsburgh, PA 15213
</affiliation>
<address confidence="0.691563">
bUS Development Center, Baidu USA, Sunnyvale, CA 94089
§{lbing@cs, sschaudh@andrew, wcohen@cs}.cmu.edu
</address>
<email confidence="0.992061">
brichardwang@baidu.com
</email>
<sectionHeader confidence="0.997332" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831">
Because of polysemy, distant labeling for
information extraction leads to noisy train-
ing data. We describe a procedure for re-
ducing this noise by using label propaga-
tion on a graph in which the nodes are
entity mentions, and mentions are cou-
pled when they occur in coordinate list
structures. We show that this labeling ap-
proach leads to good performance even
when off-the-shelf classifiers are used on
the distantly-labeled data.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99831932">
In distantly-supervised information extraction
(IE), a knowledge base (KB) of relation or
concept instances is used to train an IE sys-
tem. For instance, a set of facts like adverse-
EffectOf(meloxicam, stomachBleeding), interacts-
With(meloxicam, ibuprofen), might be used to
train an IE system that extracts these relations
from documents. In distant supervision, instances
are first matched against a corpus, and the match-
ing sentences are then used to generate training
data consisting of labeled entity mentions. For in-
stance, matching the KB above might lead to la-
beling passage 1 from Table 1 as support for the
fact adverseEffectOf(, stomachBleeding).
A weakness of distant supervision is that it pro-
duces noisy training data: for instance, match-
ing the adverse effect weakness might lead to
incorrectly-labeled mention examples. Distant su-
pervision is often coupled with learning methods
that allow for this sort of noise by introducing la-
tent variables for each entity mention (e.g., (Hoff-
mann et al., 2011; Riedel et al., 2010; Surdeanu
et al., 2012)); by carefully selecting the entity
mentions from contexts likely to include specific
KB facts (Wu and Weld, 2010); by careful filter-
</bodyText>
<listItem confidence="0.955046833333333">
1. “Avoid drinking alcohol. It may increase
your risk of stomach bleeding.”
2. “Get emergency medical help if you have
chest pain, weakness, shortness of breath,
slurred speech, or problems with vision or
balance.”
</listItem>
<tableCaption confidence="0.739527">
Table 1: Passages from a page discussing the drug
meloxicam.
</tableCaption>
<bodyText confidence="0.999469">
ing of the KB strings used as seeds (Movshovitz-
Attias and Cohen, 2012); or by making use of
named-entity linking methods and co-reference to
improve the matching phase of distant learning
(Koch et al., 2014).
Here we explore an alternative approach of
Distant IE using coordinate-term Lists (DIEL)
based on detection of lists in text, such as the one
illustrated in passage 2 in Table 1. Since list items
are usually of the same type, the unambiguous
mention chest pain here disambiguates the men-
tion weakness. Label propagation methods (Zhu
et al., 2003; Lin and Cohen, 2010) can be used
to exploit this intuition, by propagating the low-
confidence labels associated with distance super-
vision through an appropriate graph.
Here we describe a pipelined system which (1)
identifies lists of semantically-related items us-
ing lexico-syntactic patterns (2) uses distant su-
pervision, in combination with a label-propagation
method, to find entity mentions that can be confi-
dently labeled and (3) from this data, uses ordi-
nary classifier learners to classify entity mentions
by their semantic type. We show that this approach
outperforms a naive distant-supervision approach.
</bodyText>
<page confidence="0.972646">
524
</page>
<note confidence="0.7297865">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 524–529,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.631307333333333">
watery_eyes
muscle_aches
goose_bumps
</bodyText>
<sectionHeader confidence="0.474559" genericHeader="method">
2 DIEL: Distant IE Using Coordinate Lists
</sectionHeader>
<subsectionHeader confidence="0.797938">
2.1 Corpus and KB
</subsectionHeader>
<bodyText confidence="0.999841380952381">
We consider extending the coverage of Free-
base in the medical domain, which is currently
fairly limited: e.g., a Freebase snapshot from
April 2014 has (after filtering noise with sim-
ple rules such as length greater than 60 char-
acters and containing comma) only 4,605 dis-
ease instances and 4,383 drug instances, whereas
dailymed.nlm.nih.gov contains data on
over 74k drugs, and malacards.org lists
nearly 10k diseases. We use a corpus down-
loaded from dailymed.nlm.nih.gov which
contains 28,590 XML documents, each of which
describes a drug that can be legally prescribed in
the United States. We focus here on extracting in-
stances of four semantic types, without explicitly
extracting relationships between them.
We used the GDep parser (Sagae and Tsujii,
2007), a dependency parser trained on the GENIA
Treebank, to parse this corpus. We used a simple
POS-tag based noun-phrase (NP) chunker, and ex-
tract a list for each coordinating conjunction that
modifies a nominal. For each NP we extract fea-
tures (described below); and for each identified
coordinate-term list, we extract its items, and a
similar feature set describing the list.
The extracted lists and their items, as well as
entity mentions and their corresponding NPs, can
be viewed as a bipartite graph, where one set of
vertices are identifiers for the lists and entity men-
tions, and the other set of vertices are the strings
that occur as items of those lists, or as NPs of those
mentions. Note that list items are also NPs. A
mention can be regarded as a singleton list that
contains only one item, and a list can be regarded
as a complexus mention that contains a few men-
tions. If an item is contained by a list, an edge
between the item vertex and the list vertex is in-
cluded in the graph. An example bipartite graph is
given in Figure 1, in which there are nine symp-
toms from three lists and three mentions. Some
symptoms are common, such as vomiting, while
some others are not, such as epigastric pain.
</bodyText>
<subsectionHeader confidence="0.997986">
2.2 Label Propagation
</subsectionHeader>
<bodyText confidence="0.999911">
It seems intuitive to assume that if two items co-
occur in a coordinate-term list, they are very likely
to have the same type, so it seems plausible to use
label propagation on this graph to propagate types
from NPs with known types (e.g., that match enti-
</bodyText>
<figure confidence="0.4464215">
list2
diarrhea
</figure>
<figureCaption confidence="0.999713">
Figure 1: A bipartite graph example.
</figureCaption>
<bodyText confidence="0.998988913043478">
ties in the KB) to lists, and then from lists to NPs,
across this graph.
This can be viewed as semi-supervised learn-
ing (SSL) of the NPs that may denote a type (e.g.,
diseases or adverse effects). We adopt an exist-
ing multi-class label propagation method, namely,
MultiRankWalk (MRW) (Lin and Cohen, 2010),
to handle our task, which is a graph-based SSL
related to personalized PageRank (PPR) (Haveli-
wala et al., 2003) (aka random walk with restart
(Tong et al., 2006)). MRW can be described
as simply computing one personalized PageRank
vector for each class, where each vector is com-
puted using a personalization vector that is uni-
form over the seeds, and finally assigning to each
node the class associated with its highest-scoring
vector. MRW’s final scores depend on centrality of
nodes, as well as proximity to the seeds, and in this
respect MRW differs from other label propagation
methods (e.g., (Zhu et al., 2003)): in particular, it
will not assign identical scores to all seed exam-
ples. The MRW implementation we use is based
on ProPPR (Wang et al., 2013).
</bodyText>
<subsectionHeader confidence="0.996521">
2.3 Classification
</subsectionHeader>
<bodyText confidence="0.999979416666667">
One could imagine using the output of MRW to
extend a KB directly. However, the process de-
scribed above cannot be used conveniently to la-
bel new documents as they appear. Since this is
also frequently a goal, we use the MRW output to
train a classifier, which can be then used to classify
the entity mentions (singleton lists) and coordinate
lists in any new document.
We use the same feature generator for both en-
tity mentions and lists. Shallow features include:
tokens in the NPs, and character prefixes/suffixes
of these tokens; tokens from the sentence contain-
</bodyText>
<figure confidence="0.9844943">
list3
vomiting
nausea
drowsiness
lethargy
epigastric_pain
mention1
mention2
mention3
list1
</figure>
<page confidence="0.987513">
525
</page>
<table confidence="0.9993326">
Run 1 2 3 4 5 6 7 8 9 10 Avg.
DIEL 0.418 0.390 0.404 0.420 0.400 0.397 0.404 0.403 0.409 0.404 0.405
DS-baseline 0.394 0.342 0.400 0.373 0.342 0.297 0.326 0.332 0.330 0.352 0.349
LP-baseline 0.167 0.167 0.162 0.155 0.150 0.159 0.157 0.165 0.167 0.167 0.162
Upper bound 0.617 0.616 0.615 0.619 0.620 0.614 0.618 0.617 0.615 0.618 0.617
</table>
<tableCaption confidence="0.999458">
Table 2: Recall on the held-out set.
</tableCaption>
<bodyText confidence="0.9999426">
ing the NP; and tokens and bigrams from a win-
dow around the NPs. From the dependence parse,
we also find the verb which is the closest ancestor
of the head of the NP, all modifiers of this verb,
and the path to this verb. For a list, the dependency
features are computed relative to the head of the
list. We used an SVM classifier (Chang and Lin,
2001) and discard singleton features, and also the
frequent 5% of all features (as a stop-wording vari-
ant). We train a binary classifier on the top N lists
(including entity mentions and coordinate lists) of
each type, as scored by MRW. A linear kernel and
defaults for all other parameters are used. If a new
list or mention is not classified as positive by all
binary classifiers, it is predicted as “other”.
</bodyText>
<sectionHeader confidence="0.997107" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.987349">
3.1 Results of Recovering KB
</subsectionHeader>
<bodyText confidence="0.9998806">
In this experiment, we examine the capability of
our approach in recovering KB type instances.
The targeted types are diseases, symptoms treated
and adverse effects (symptom for short), drugs,
and drug ingredients.
</bodyText>
<subsectionHeader confidence="0.824475">
3.1.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999978882352941">
We implemented a distant-supervision-based
baseline (DS-baseline). It attempts to classify
each NP in the input corpus into one of the four
types or “other” with the training seeds as distance
supervision. Each sentence is processed with the
same reprocessing pipeline to detect NPs. Then,
these NPs are labeled with the training seeds.
The features are defined and extracted in the
same way as we did for DIEL, and four binary
classifiers are trained with the same method.
Another baseline is developed with the output of
MRW LP (LP-baseline) that contains labeled lists
and mentions. Specifically, the labeled coordinate
lists are broken into items each of which has
the list class, and evaluation is conducted with
these items together with the labeled mentions as
positive predictions.
</bodyText>
<subsectionHeader confidence="0.807405">
3.1.2 Settings
</subsectionHeader>
<bodyText confidence="0.999985034482758">
We extracted the seeds of these types from Free-
base, and got 4,605, 1,244, 4,383, and 4,066 in-
stances, respectively. The seeds are split into de-
velopment set and held-out evaluation set. The de-
velopment set is further split into a training set and
a validating set in the ratio of 4:1. The validating
set will be used in the next subsection to validate
different parameter settings, and the training set
is used in this experiment as MRW seeds and the
distant supervision of DS-baseline.
For getting the development set, the polyse-
mous instances (i.e., “headache”, belonging to
multiple classes: disease and symptom) are dis-
carded since such instances will bring in ambigu-
ity to the training examples of DS-baseline and
MRW LP. After that, we randomly take half of
the single-type instances as development set, and
the remaining single-type instances together with
the polysemous instances are used as held-out set.
We report the performance of 10 runs, and each
run has its own randomly generated training set
(containing 1,980 diseases, 310 symptoms, 1,066
drugs, and 911 ingredients on average) and held-
out set (containing 2,130 diseases, 857 symptoms,
3,051 drugs, and 2,927 ingredients on average).
For DS-baseline, 35,689 training examples are
collected on average for each run. For evaluation
metric, we calculate recall on the instances in the
held-out set.1
</bodyText>
<sectionHeader confidence="0.868109" genericHeader="method">
3.1.3 Results
</sectionHeader>
<bodyText confidence="0.998688444444444">
DIEL can classify both NPs and coordinate lists,
and the lists are also broken into items for recall
calculation. The training examples of DIEL are
prepared with the top 20,000 lists of each type, as
scored by MRW with the training seeds. After re-
moving the multiple-class ones, we obtained, on
average, 58,614 training examples for each run.
The results are given in Table 2. DIEL out-
performs the baselines in all runs. It shows that
</bodyText>
<footnote confidence="0.9718565">
1Because the outputs contain many correct instances that
are not in Freebase, it is not suitable to calculate precision.
</footnote>
<page confidence="0.992693">
526
</page>
<table confidence="0.999955076923077">
N 2.5% 7.5% 12.5% 25% 50% 75% 100%
P R P R P R P R P R P R P R
1,000 0.581 0.119 0.680 0.150 0.706 0.154 0.754 0.216 0.768 0.277 0.749 0.308 0.769 0.331
2,000 0.587 0.168 0.675 0.183 0.705 0.192 0.746 0.245 0.760 0.292 0.747 0.317 0.777 0.347
4,000 0.551 0.205 0.661 0.225 0.682 0.254 0.726 0.295 0.763 0.334 0.763 0.356 0.766 0.375
DIEL 6,000 0.500 0.205 0.636 0.244 0.650 0.281 0.680 0.314 0.757 0.362 0.758 0.380 0.768 0.392
8,000 0.518 0.219 0.615 0.251 0.641 0.300 0.662 0.331 0.727 0.393 0.764 0.407 0.762 0.409
10,000 0.520 0.219 0.610 0.260 0.643 0.309 0.664 0.349 0.708 0.406 0.753 0.436 0.765 0.429
20,000 0.520 0.216 0.590 0.277 0.634 0.335 0.661 0.375 0.688 0.435 0.723 0.473 0.746 0.494
30,000 0.520 0.216 0.584 0.284 0.624 0.341 0.649 0.392 0.688 0.454 0.717 0.486 0.737 0.496
LP-baseline 0.442 0.024 0.604 0.074 0.673 0.108 0.722 0.134 0.797 0.173 0.838 0.187 0.855 0.212
DS- Training NP# 678 2,442 5,983 9,831 18,230 27,549 35,689
baseline Performance 0.317 0.139 0.384 0.161 0.643 0.244 0.714 0.252 0.747 0.322 0.767 0.347 0.742 0.351
</table>
<tableCaption confidence="0.997575">
Table 4: The classification performance of the three methods with different parameters.
</tableCaption>
<table confidence="0.999859">
Type Disease Drug Ingredient Symptom
DIEL 0.369 0.312 0.489 0.555
DS-baseline 0.375 0.300 0.408 0.224
LP-baseline 0.185 0.101 0.117 0.453
Upper bound 0.445 0.652 0.700 0.697
</table>
<tableCaption confidence="0.99992">
Table 3: Recall on individual types.
</tableCaption>
<bodyText confidence="0.999840137931034">
our result is consistently better. The reason is
twofold. First, DIEL can avoid the effect of noisy
training data by disambiguation with the coordi-
nate relation in the list, so that the training exam-
ples are of high quality. Second, with label prop-
agation, we have a larger number of training ex-
amples, which helps the recall. Compared with
DS-baseline, DIEL’s performance is more stable
in different runs. It is because DS-baseline suffers
from the noisy training data and training seed sets
of different runs may bring in different levels of
noisy data. Thus, its run 3 achieves 0.400, while
run 6 only achieves 0.297. We also examined the
upper bound recall that a system can achieve on
our corpus. The results are given in the last row
of Table 2. On average, the best performance of a
system can achieve is 0.617.
The results for individual types are given in Ta-
ble 3. DIEL and DS-baseline achieve similar re-
sults for disease and drug. Especially, both sys-
tems cover more than 80% of the held-out dis-
ease instances that exist in the corpus. DS-baseline
performs poorly for symptom. The reason is that
symptom instances are more ambiguous than other
types, and they lead to more incorrectly-labeled
mention examples. LP-baseline achieves an en-
couraging recall for symptom, which shows that
coordinate lists are very helpful for disambiguat-
ing those symptom mentions.
</bodyText>
<subsectionHeader confidence="0.999978">
3.2 Classification Results and Parameters
</subsectionHeader>
<bodyText confidence="0.999958">
We present another experiment to examine the pre-
cision of the systems, and investigate the effect of
training size and top N numbers on the results.
</bodyText>
<subsectionHeader confidence="0.938791">
3.2.1 Setting
</subsectionHeader>
<bodyText confidence="0.999995166666666">
The evaluation data is generated with the validat-
ing set of each run. Specifically, for DIEL and
LP-baseline, the evaluation data is prepared with
the top 500 lists (singleton and coordinate lists)
of each type, as scored by MRW with the validat-
ing instances as seeds. 2,000 testing examples are
collected in total since no multiple-class ones are
found. Manual checking on 200 random examples
shows that 99% of them are correct. The systems
are evaluated by their performance for classifying
these example lists. DIEL predicts the class of a
list with its feature vector, while LP-baseline de-
termines the class of a list by checking its pre-
dicted class in the result of MRW with the cor-
responding training instances as seeds. For DS-
baseline, the testing NP examples are obtained by
distant labeling with validating instances, and on
average 8,270 examples are collected for each run.
</bodyText>
<sectionHeader confidence="0.580635" genericHeader="evaluation">
3.2.2 Results
</sectionHeader>
<bodyText confidence="0.999670076923077">
The precision and recall are given in Table 4. For
each system, different portions of the training set
are used to train the system, as shown in the first
row. For DIEL, the top N number varies for gen-
erating training examples, as shown in the second
column. F1 values of DIEL with different settings
are given in Figure 2, and F1 comparison of three
systems is given in Figure 3. All results are the
average of 10 runs. Each run has its own ran-
domly generated development set, which is split
into training set and validating set.
In general, for all systems, larger number of
training seeds leads to better performance. For
</bodyText>
<page confidence="0.995179">
527
</page>
<figureCaption confidence="0.990774">
Figure 2: F1 values of DIEL under different settings.
</figureCaption>
<figure confidence="0.99566196875">
0.6
0.5
0.5
0.45
0.2
0.25
0.1
0.2
30000
20000
100%
10000
0.15
75%
8000
50%
6000
4000
2000
25%
12.5%
7.5%
Top N
1000
Percentage of training seeds
2.5%
F1 values
0.4
0.3
0.4
0.35
0.3
</figure>
<bodyText confidence="0.996207774193548">
DIEL, smaller N values achieve higher precision,
but lower recall. For smaller seed numbers, the
precision value is more sensitive to N. This is be-
cause the quality of training examples drops faster
compared with that from larger seed numbers. For
larger seeds numbers, the recall values are im-
proved more significantly when the N value is
larger. The reported results of DIEL in the pre-
vious experiment are obtained with top 20,000 ex-
amples from 100% seeds as training data, since
this setting achieves the highest F1 value as shown
in Figure 2.
For the DS-baseline, the number of training NPs
obtained with different portions of the training set
is given in the penultimate row. The recall val-
ues of this baseline are low. The reason is that it
only uses the training examples that are distantly
labeled with training seeds, thus, the trained clas-
sifier may not have good generalization on the test-
ing examples labeled with validating seeds. In ad-
dition, its performance is more sensitive on the
amount of training data. When the percentage
is lower than 25%, its precision and recall drop
significantly. Its F1 values are 0.381, 0.399 and
0.402 for 50%, 75%, and 100%, respectively. LP-
baseline achieves the highest precision when using
all training instances. It shows that MRW does
label the testing lists very accurately in condition
that the lists are traversed in the propagation with
the training instances as seeds. However, its recall
is much lower than DIEL. It is because, with the
</bodyText>
<figureCaption confidence="0.9171135">
Figure 3: F1 comparison under different portions
of the training seeds. For DIEL, top N = 20,000.
</figureCaption>
<bodyText confidence="0.969842333333333">
training seeds, MRW cannot effectively walk to
testing lists that are generated with the validating
set, having no intersection with the training set.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9998925">
We explored an alternative approach to distant su-
pervision, based on detection of lists in text, to
overcome the weakness of distant supervision re-
sulted by noisy training data. It uses distant su-
pervision and label propagation to find mentions
that can be confidently labeled, and uses them
to train classifiers to label more entity mentions.
The experimental results show that this approach
consistently and significantly outperforms a naive
distant-supervision approach.
</bodyText>
<sectionHeader confidence="0.998808" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9977775">
This work was funded by a grant from Baidu USA
and by the NSF under research grant IIS-1250956.
</bodyText>
<figure confidence="0.995603222222222">
0.6
0.5
0.4
0.3
0.2
0.1
0
2.5% 7.5% 12.5% 25% 50%
DIEL LP-baseline DS-baseline
</figure>
<page confidence="0.984887">
528
</page>
<sectionHeader confidence="0.990435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997365855263158">
Chih-Chung Chang and Chih-Jen Lin,
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/˜cjlin/libsvm.
Taher Haveliwala, Sepandar Kamvar, Ar Kamvar, and
Glen Jeh. 2003. An analytical comparison of ap-
proaches to personalizing pagerank. Technical re-
port, Stanford University.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541–550. Association for Compu-
tational Linguistics.
Mitchell Koch, John Gilmer, Stephen Soderland, and
Daniel S. Weld. 2014. Type-aware distantly super-
vised relation extraction with linked arguments. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2014, October 25-29, 2014, Doha, Qatar, A meet-
ing of SIGDAT, a Special Interest Group of the ACL,
pages 1891–1901.
Frank Lin and William W. Cohen. 2010. Semi-
supervised classification of network data using very
few labels. In Nasrullah Memon and Reda Alhajj,
editors, ASONAM, pages 192–199. IEEE Computer
Society.
Dana Movshovitz-Attias and William W. Cohen. 2012.
Bootstrapping biomedical ontologies for scientific
text using nell. In Proceedings of the 2012 Work-
shop on Biomedical Natural Language Processing,
BioNLP ’12, pages 11–19, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148–163.
Springer.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser
ensembles. In Proceedings of the CoNLL 2007
Shared Task in the Joint Conferences on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL’07 shared task), pages 1044–1050.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 455–465, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its appli-
cations. In ICDM, pages 613–622. IEEE Computer
Society.
William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2013. Programming with personalized
pagerank: a locally groundable first-order proba-
bilistic logic. In Proceedings of the 22nd ACM in-
ternational conference on Conference on informa-
tion &amp; knowledge management, pages 2129–2138.
ACM.
Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118–127. Association for
Computational Linguistics.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of ICML-03, the
20th International Conference on Machine Learn-
ing.
</reference>
<page confidence="0.998574">
529
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.276138">
<title confidence="0.999026">Improving Distant Supervision for Information Extraction Using Propagation Through Lists</title>
<author confidence="0.892983">C W</author>
<affiliation confidence="0.803131">Mellon University, Pittsburgh, PA</affiliation>
<address confidence="0.410898">bUS Development Center, Baidu USA, Sunnyvale, CA</address>
<email confidence="0.871742">sschaudh@andrew,brichardwang@baidu.com</email>
<abstract confidence="0.997737833333333">Because of polysemy, distant labeling for information extraction leads to noisy training data. We describe a procedure for reducing this noise by using label propagation on a graph in which the nodes are entity mentions, and mentions are coupled when they occur in coordinate list structures. We show that this labeling approach leads to good performance even when off-the-shelf classifiers are used on the distantly-labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="8490" citStr="Chang and Lin, 2001" startWordPosition="1389" endWordPosition="1392">baseline 0.394 0.342 0.400 0.373 0.342 0.297 0.326 0.332 0.330 0.352 0.349 LP-baseline 0.167 0.167 0.162 0.155 0.150 0.159 0.157 0.165 0.167 0.167 0.162 Upper bound 0.617 0.616 0.615 0.619 0.620 0.614 0.618 0.617 0.615 0.618 0.617 Table 2: Recall on the held-out set. ing the NP; and tokens and bigrams from a window around the NPs. From the dependence parse, we also find the verb which is the closest ancestor of the head of the NP, all modifiers of this verb, and the path to this verb. For a list, the dependency features are computed relative to the head of the list. We used an SVM classifier (Chang and Lin, 2001) and discard singleton features, and also the frequent 5% of all features (as a stop-wording variant). We train a binary classifier on the top N lists (including entity mentions and coordinate lists) of each type, as scored by MRW. A linear kernel and defaults for all other parameters are used. If a new list or mention is not classified as positive by all binary classifiers, it is predicted as “other”. 3 Experimental Results 3.1 Results of Recovering KB In this experiment, we examine the capability of our approach in recovering KB type instances. The targeted types are diseases, symptoms treat</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher Haveliwala</author>
<author>Sepandar Kamvar</author>
<author>Ar Kamvar</author>
<author>Glen Jeh</author>
</authors>
<title>An analytical comparison of approaches to personalizing pagerank.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="6437" citStr="Haveliwala et al., 2003" startWordPosition="1031" endWordPosition="1035">ly to have the same type, so it seems plausible to use label propagation on this graph to propagate types from NPs with known types (e.g., that match entilist2 diarrhea Figure 1: A bipartite graph example. ties in the KB) to lists, and then from lists to NPs, across this graph. This can be viewed as semi-supervised learning (SSL) of the NPs that may denote a type (e.g., diseases or adverse effects). We adopt an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), to handle our task, which is a graph-based SSL related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be described as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector. MRW’s final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW differs from other label propagation methods (e.g., (Zhu et al., 2003)): in particular, it will not assign identical scores to all seed examples. The MRW implementati</context>
</contexts>
<marker>Haveliwala, Kamvar, Kamvar, Jeh, 2003</marker>
<rawString>Taher Haveliwala, Sepandar Kamvar, Ar Kamvar, and Glen Jeh. 2003. An analytical comparison of approaches to personalizing pagerank. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>541--550</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1802" citStr="Hoffmann et al., 2011" startWordPosition="266" endWordPosition="270">gainst a corpus, and the matching sentences are then used to generate training data consisting of labeled entity mentions. For instance, matching the KB above might lead to labeling passage 1 from Table 1 as support for the fact adverseEffectOf(, stomachBleeding). A weakness of distant supervision is that it produces noisy training data: for instance, matching the adverse effect weakness might lead to incorrectly-labeled mention examples. Distant supervision is often coupled with learning methods that allow for this sort of noise by introducing latent variables for each entity mention (e.g., (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012)); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); by careful filter1. “Avoid drinking alcohol. It may increase your risk of stomach bleeding.” 2. “Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.” Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to impr</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 541–550. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Koch</author>
<author>John Gilmer</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Type-aware distantly supervised relation extraction with linked arguments.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>1891--1901</pages>
<location>Doha,</location>
<contexts>
<context position="2464" citStr="Koch et al., 2014" startWordPosition="375" endWordPosition="378">)); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); by careful filter1. “Avoid drinking alcohol. It may increase your risk of stomach bleeding.” 2. “Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.” Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phase of distant learning (Koch et al., 2014). Here we explore an alternative approach of Distant IE using coordinate-term Lists (DIEL) based on detection of lists in text, such as the one illustrated in passage 2 in Table 1. Since list items are usually of the same type, the unambiguous mention chest pain here disambiguates the mention weakness. Label propagation methods (Zhu et al., 2003; Lin and Cohen, 2010) can be used to exploit this intuition, by propagating the lowconfidence labels associated with distance supervision through an appropriate graph. Here we describe a pipelined system which (1) identifies lists of semantically-relat</context>
</contexts>
<marker>Koch, Gilmer, Soderland, Weld, 2014</marker>
<rawString>Mitchell Koch, John Gilmer, Stephen Soderland, and Daniel S. Weld. 2014. Type-aware distantly supervised relation extraction with linked arguments. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1891–1901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Lin</author>
<author>William W Cohen</author>
</authors>
<title>Semisupervised classification of network data using very few labels.</title>
<date>2010</date>
<booktitle>In Nasrullah Memon and Reda Alhajj,</booktitle>
<pages>192--199</pages>
<editor>editors, ASONAM,</editor>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="2833" citStr="Lin and Cohen, 2010" startWordPosition="437" endWordPosition="440"> from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phase of distant learning (Koch et al., 2014). Here we explore an alternative approach of Distant IE using coordinate-term Lists (DIEL) based on detection of lists in text, such as the one illustrated in passage 2 in Table 1. Since list items are usually of the same type, the unambiguous mention chest pain here disambiguates the mention weakness. Label propagation methods (Zhu et al., 2003; Lin and Cohen, 2010) can be used to exploit this intuition, by propagating the lowconfidence labels associated with distance supervision through an appropriate graph. Here we describe a pipelined system which (1) identifies lists of semantically-related items using lexico-syntactic patterns (2) uses distant supervision, in combination with a label-propagation method, to find entity mentions that can be confidently labeled and (3) from this data, uses ordinary classifier learners to classify entity mentions by their semantic type. We show that this approach outperforms a naive distant-supervision approach. 524 Pro</context>
<context position="6324" citStr="Lin and Cohen, 2010" startWordPosition="1013" endWordPosition="1016">pagation It seems intuitive to assume that if two items cooccur in a coordinate-term list, they are very likely to have the same type, so it seems plausible to use label propagation on this graph to propagate types from NPs with known types (e.g., that match entilist2 diarrhea Figure 1: A bipartite graph example. ties in the KB) to lists, and then from lists to NPs, across this graph. This can be viewed as semi-supervised learning (SSL) of the NPs that may denote a type (e.g., diseases or adverse effects). We adopt an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), to handle our task, which is a graph-based SSL related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be described as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector. MRW’s final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW differs from other label propagation methods (e.g., (</context>
</contexts>
<marker>Lin, Cohen, 2010</marker>
<rawString>Frank Lin and William W. Cohen. 2010. Semisupervised classification of network data using very few labels. In Nasrullah Memon and Reda Alhajj, editors, ASONAM, pages 192–199. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Movshovitz-Attias</author>
<author>William W Cohen</author>
</authors>
<title>Bootstrapping biomedical ontologies for scientific text using nell.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP ’12,</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Movshovitz-Attias, Cohen, 2012</marker>
<rawString>Dana Movshovitz-Attias and William W. Cohen. 2012. Bootstrapping biomedical ontologies for scientific text using nell. In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP ’12, pages 11–19, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>148--163</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1823" citStr="Riedel et al., 2010" startWordPosition="271" endWordPosition="274">e matching sentences are then used to generate training data consisting of labeled entity mentions. For instance, matching the KB above might lead to labeling passage 1 from Table 1 as support for the fact adverseEffectOf(, stomachBleeding). A weakness of distant supervision is that it produces noisy training data: for instance, matching the adverse effect weakness might lead to incorrectly-labeled mention examples. Distant supervision is often coupled with learning methods that allow for this sort of noise by introducing latent variables for each entity mention (e.g., (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012)); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); by careful filter1. “Avoid drinking alcohol. It may increase your risk of stomach bleeding.” 2. “Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.” Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phas</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL 2007 Shared Task in the Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL’07 shared task),</booktitle>
<pages>1044--1050</pages>
<contexts>
<context position="4480" citStr="Sagae and Tsujii, 2007" startWordPosition="684" endWordPosition="687">pril 2014 has (after filtering noise with simple rules such as length greater than 60 characters and containing comma) only 4,605 disease instances and 4,383 drug instances, whereas dailymed.nlm.nih.gov contains data on over 74k drugs, and malacards.org lists nearly 10k diseases. We use a corpus downloaded from dailymed.nlm.nih.gov which contains 28,590 XML documents, each of which describes a drug that can be legally prescribed in the United States. We focus here on extracting instances of four semantic types, without explicitly extracting relationships between them. We used the GDep parser (Sagae and Tsujii, 2007), a dependency parser trained on the GENIA Treebank, to parse this corpus. We used a simple POS-tag based noun-phrase (NP) chunker, and extract a list for each coordinating conjunction that modifies a nominal. For each NP we extract features (described below); and for each identified coordinate-term list, we extract its items, and a similar feature set describing the list. The extracted lists and their items, as well as entity mentions and their corresponding NPs, can be viewed as a bipartite graph, where one set of vertices are identifiers for the lists and entity mentions, and the other set </context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Proceedings of the CoNLL 2007 Shared Task in the Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL’07 shared task), pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1847" citStr="Surdeanu et al., 2012" startWordPosition="275" endWordPosition="278">are then used to generate training data consisting of labeled entity mentions. For instance, matching the KB above might lead to labeling passage 1 from Table 1 as support for the fact adverseEffectOf(, stomachBleeding). A weakness of distant supervision is that it produces noisy training data: for instance, matching the adverse effect weakness might lead to incorrectly-labeled mention examples. Distant supervision is often coupled with learning methods that allow for this sort of noise by introducing latent variables for each entity mention (e.g., (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012)); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); by careful filter1. “Avoid drinking alcohol. It may increase your risk of stomach bleeding.” 2. “Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.” Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phase of distant learning (K</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 455–465, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanghang Tong</author>
<author>Christos Faloutsos</author>
<author>Jia-Yu Pan</author>
</authors>
<title>Fast random walk with restart and its applications. In</title>
<date>2006</date>
<booktitle>ICDM,</booktitle>
<pages>613--622</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="6487" citStr="Tong et al., 2006" startWordPosition="1041" endWordPosition="1044">abel propagation on this graph to propagate types from NPs with known types (e.g., that match entilist2 diarrhea Figure 1: A bipartite graph example. ties in the KB) to lists, and then from lists to NPs, across this graph. This can be viewed as semi-supervised learning (SSL) of the NPs that may denote a type (e.g., diseases or adverse effects). We adopt an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), to handle our task, which is a graph-based SSL related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be described as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector. MRW’s final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW differs from other label propagation methods (e.g., (Zhu et al., 2003)): in particular, it will not assign identical scores to all seed examples. The MRW implementation we use is based on ProPPR (Wang et al., 2013). </context>
</contexts>
<marker>Tong, Faloutsos, Pan, 2006</marker>
<rawString>Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk with restart and its applications. In ICDM, pages 613–622. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kathryn Mazaitis</author>
<author>William W Cohen</author>
</authors>
<title>Programming with personalized pagerank: a locally groundable first-order probabilistic logic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>2129--2138</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7085" citStr="Wang et al., 2013" startWordPosition="1142" endWordPosition="1145"> (Tong et al., 2006)). MRW can be described as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector. MRW’s final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW differs from other label propagation methods (e.g., (Zhu et al., 2003)): in particular, it will not assign identical scores to all seed examples. The MRW implementation we use is based on ProPPR (Wang et al., 2013). 2.3 Classification One could imagine using the output of MRW to extend a KB directly. However, the process described above cannot be used conveniently to label new documents as they appear. Since this is also frequently a goal, we use the MRW output to train a classifier, which can be then used to classify the entity mentions (singleton lists) and coordinate lists in any new document. We use the same feature generator for both entity mentions and lists. Shallow features include: tokens in the NPs, and character prefixes/suffixes of these tokens; tokens from the sentence containlist3 vomiting</context>
</contexts>
<marker>Wang, Mazaitis, Cohen, 2013</marker>
<rawString>William Yang Wang, Kathryn Mazaitis, and William W Cohen. 2013. Programming with personalized pagerank: a locally groundable first-order probabilistic logic. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 2129–2138. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1962" citStr="Wu and Weld, 2010" startWordPosition="293" endWordPosition="296">t lead to labeling passage 1 from Table 1 as support for the fact adverseEffectOf(, stomachBleeding). A weakness of distant supervision is that it produces noisy training data: for instance, matching the adverse effect weakness might lead to incorrectly-labeled mention examples. Distant supervision is often coupled with learning methods that allow for this sort of noise by introducing latent variables for each entity mention (e.g., (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012)); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); by careful filter1. “Avoid drinking alcohol. It may increase your risk of stomach bleeding.” 2. “Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.” Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phase of distant learning (Koch et al., 2014). Here we explore an alternative approach of Distant IE using coordinate-term Lists (DIEL) based o</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML-03, the 20th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2811" citStr="Zhu et al., 2003" startWordPosition="433" endWordPosition="436"> Table 1: Passages from a page discussing the drug meloxicam. ing of the KB strings used as seeds (MovshovitzAttias and Cohen, 2012); or by making use of named-entity linking methods and co-reference to improve the matching phase of distant learning (Koch et al., 2014). Here we explore an alternative approach of Distant IE using coordinate-term Lists (DIEL) based on detection of lists in text, such as the one illustrated in passage 2 in Table 1. Since list items are usually of the same type, the unambiguous mention chest pain here disambiguates the mention weakness. Label propagation methods (Zhu et al., 2003; Lin and Cohen, 2010) can be used to exploit this intuition, by propagating the lowconfidence labels associated with distance supervision through an appropriate graph. Here we describe a pipelined system which (1) identifies lists of semantically-related items using lexico-syntactic patterns (2) uses distant supervision, in combination with a label-propagation method, to find entity mentions that can be confidently labeled and (3) from this data, uses ordinary classifier learners to classify entity mentions by their semantic type. We show that this approach outperforms a naive distant-supervi</context>
<context position="6941" citStr="Zhu et al., 2003" startWordPosition="1116" endWordPosition="1119">, to handle our task, which is a graph-based SSL related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be described as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector. MRW’s final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW differs from other label propagation methods (e.g., (Zhu et al., 2003)): in particular, it will not assign identical scores to all seed examples. The MRW implementation we use is based on ProPPR (Wang et al., 2013). 2.3 Classification One could imagine using the output of MRW to extend a KB directly. However, the process described above cannot be used conveniently to label new documents as they appear. Since this is also frequently a goal, we use the MRW output to train a classifier, which can be then used to classify the entity mentions (singleton lists) and coordinate lists in any new document. We use the same feature generator for both entity mentions and lis</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In Proceedings of ICML-03, the 20th International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
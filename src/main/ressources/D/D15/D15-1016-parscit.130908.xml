<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.985135">
Cross-Lingual Sentiment Analysis using modified BRAE
</title>
<author confidence="0.997358">
Sarthak Jain
</author>
<affiliation confidence="0.994359">
Department of Computer Engineering
Delhi Technological University
</affiliation>
<address confidence="0.498321">
DL, India
</address>
<email confidence="0.997266">
successar@gmail.com
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999634958333334">
Cross-Lingual Learning provides a mech-
anism to adapt NLP tools available for la-
bel rich languages to achieve similar tasks
for label-scarce languages. An efficient
cross-lingual tool significantly reduces the
cost and effort required to manually an-
notate data. In this paper, we use the
Recursive Autoencoder architecture to de-
velop a Cross Lingual Sentiment Analysis
(CLSA) tool using sentence aligned cor-
pora between a pair of resource rich (En-
glish) and resource poor (Hindi) language.
The system is based on the assumption
that semantic similarity between different
phrases also implies sentiment similarity in
majority of sentences. The resulting sys-
tem is then analyzed on a newly developed
Movie Reviews Dataset in Hindi with la-
bels given on a rating scale and compare
performance of our system against exist-
ing systems. It is shown that our approach
significantly outperforms state of the art
systems for Sentiment Analysis, especially
when labeled data is scarce.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995824307692308">
Sentiment Analysis is a NLP task that deals with
extraction of opinion from a piece of text on a
topic. This is used by a large number of advertising
and media companies to get a sense of public opin-
ion from their reviews. The ever increasing user
generated content has always been motivation for
sentiment analysis research, but majority of work
has been done for English Language. However, in
recent years, there has been emergence of increas-
ing amount of text in Hindi on electronic sources
but NLP Frameworks to process this data is sadly
miniscule. A major cause for this is the lack of
annotated datasets in Indian Languages.
</bodyText>
<author confidence="0.806512">
Shashank Batra
</author>
<affiliation confidence="0.844139">
Department of Computer Engineering
Indian Institute of technology, Delhi
DL, India
</affiliation>
<email confidence="0.972709">
shashankg@gmail.com
</email>
<bodyText confidence="0.999967931034483">
One solution is to create cross lingual tools be-
tween a resource rich and resource poor language
that exploit large amounts of unlabeled data and
sentence aligned corpora that are widely available
on web through bilingual newspapers, magazines,
etc. Many different approaches have been identi-
fied to perform Cross Lingual Tasks but they de-
pend on the presence of MT-System or Bilingual
Dictionaries between the source and target lan-
guage.
In this paper, we use Bilingually Constrained
Recursive Auto-encoder (BRAE) given by (Zhang
et al., 2014) to perform Cross Lingual sentiment
analysis. Major Contributions of this paper are
as follows: First, We develop a new Rating scale
based Movie Review Dataset for Hindi. Second,
a general framework to perform Cross Lingual
Classification tasks is developed by modifying
the architecture and training procedure for BRAE
model. This model exploits the fact that phrases in
two languages, that share same semantic meaning,
can be used to learn language independent seman-
tic vector representations. These embeddings can
further be fine-tuned using labeled dataset in En-
glish to capture enough class information regard-
ing Resource poor language. We train the resultant
framework on English-Hindi Language pair and
evaluate it against state of the art SA systems on
existing and newly developed dataset.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99973">
2.1 Sentiment Analysis in Hindi
</subsectionHeader>
<bodyText confidence="0.999267875">
In recent years, there have been emergence of
works on Sentiment Analysis (both monolingual
and cross-lingual) for Hindi. (Joshi et al., 2010)
provided a comparative analysis of Unigram based
In-language, MT based Cross Lingual and Word-
Net based Sentiment classifier, achieving highest
accuracy of 78.14%. (Mittal et al., 2013) described
a system based on Hindi SentiWordNet for assign-
</bodyText>
<page confidence="0.983628">
159
</page>
<note confidence="0.9850475">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 159–168,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999950945945946">
ing positive/negative polarity to movie reviews. In
this approach, overall semantic orientation of the
review document was determined by aggregating
the polarity values of the words in the document
assigned using the WordNet. They also included
explicit rules for handling Negation and Discourse
relations during preprocessing in their model to
achieve better accuracies.
For Languages where labeled data is not present,
approaches based on cross-lingual sentiment anal-
ysis are used. Usually, such methods need inter-
mediary machine translation system (Wan et al.,
2011; Brooke et al., 2009) or abilingual dictionary
(Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge
the language gap. Given the subtle and different
ways in which sentiments can be expressed and the
cultural diversity amongst different languages, an
MT system has to be of a superior quality to per-
form well(Balamurali et al., 2012).
(Balamurali et al., 2012) present an alterna-
tive approach to Cross Lingual Sentiment Analy-
sis (CLSA) using WordNet senses as features for
supervised sentiment classification. A document
in Resource Poor Language was tested for polarity
through a classifier trained on sense marked and
polarity labeled corpora in Resource rich language.
The crux of the idea was to use the linked Word-
Nets of two languages to bridge the language gap.
Recently, (Popat et al., 2013) describes a Cross
Lingual Clustering based SA System. In this ap-
proach, features were generated using syntagmatic
property based word clusters created from unla-
beled monolingual corpora, thereby eliminating
the need for Bilingual Dictionaries. These features
were then used to train a linear SVM to predict
positive or negative polarity on a tourism review
dataset.
</bodyText>
<subsectionHeader confidence="0.999578">
2.2 Autoencoders in NLP Tasks
</subsectionHeader>
<bodyText confidence="0.999980210526316">
Autoencoders are neural networks that learn a low
dimensional vector representation of fixed-size in-
puts such as image segments or bag-of-word rep-
resentations of documents. They can be used to
efficiently learn feature encodings that are useful
for classification. The Autoencoders were first
applied in a recursive setting by Pollack (1990)
in recursive auto-associative memories (RAAMs).
However, RAAMs needed fixed recursive data
structures to learn vector representations, whereas
RAE given by (Socher et al., 2011) builds recur-
sive data structure using a greedy algorithm. The
RAE can be pre-trained with an unsupervised algo-
rithm and then fine-tuned according to the label of
the phrase, such as the syntactic category in pars-
ing(Socher et al., 2013), the polarity in sentiment
analysis, etc. The learned structures are not neces-
sarily syntactically accurate but can capture more
of the semantic information in the word vectors.
</bodyText>
<sectionHeader confidence="0.997852" genericHeader="method">
3 BRAE Framework
</sectionHeader>
<bodyText confidence="0.999890541666667">
(Zhang et al., 2014) used the RAE along with a
Bilingually Constrained Model to simultaneously
learn phrase embeddings for two languages in se-
mantic vector space. The core idea behind BRAE
is that a phrase and its correct translation should
share the same semantic meaning. Thus, they
can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings. In this
method, a standard recursive autoencoder (RAE)
pre-trains the phrase embedding with an unsuper-
vised algorithm by greedily minimizing the re-
construction error (Socher et al., 2011), while the
bilingually-constrained model learns to finetune
the phrase embedding by minimizing the seman-
tic distance between translation equivalents and
maximizing the semantic distance between non-
translation pairs.
In this section, We will briefly present the struc-
ture and training algorithm for BRAE model. Af-
ter that, we show how this model can be adapted
to perform CLSA.
</bodyText>
<subsectionHeader confidence="0.999719">
3.1 Recursive Auto-encoder Framework
</subsectionHeader>
<bodyText confidence="0.999848181818182">
In this model, each word wk in the vocabulary V
of given language corresponds to a vector xk E Rn
and stacked into a single word embedding matrix
L E Rnx|V |. This matrix is learned using DNN
(Collobert and Weston, 2008; Mikolov et al., 2013)
and serves as input to further stages of RAE.
Using this matrix, a phrase (w1w2 ... wm) is
first projected into a list of vectors (x1, x2, ... xm).
The RAE learns the vector representation of the
phrase by combining two children vectors recur-
sively in a bottom-up manner. For two children
</bodyText>
<equation confidence="0.969080333333333">
c1 = x1, c2 = x2, the auto-encoder computes the
parent vector y1:
y1 = f(W(1)[c1; c21 + b(1)); y1 E Rn (1)
</equation>
<bodyText confidence="0.993042">
To assess how well the parent vector represents
its children, the auto-encoder reconstructs the chil-
</bodyText>
<page confidence="0.997197">
160
</page>
<figureCaption confidence="0.997919">
Figure 1: An illustration of BRAE structure
</figureCaption>
<equation confidence="0.985465">
dren :
[c′1; c′2] = W (2)p + b(2) (2)
</equation>
<bodyText confidence="0.997148222222222">
and tries to minimize the reconstruction error (Eu-
clideanDistance) Erec([c1; c2]) between the inputs
[c1; c2] and their reconstructions [c′1; c′2].
Given y1, Eq.1 is used again to compute y2 by
setting the children to be [c1; c2] = [y1; x3]. The
same auto-encoder is re-used until the vector of
the whole phrase is generated. For unsupervised
phrase embedding, the sum of reconstruction er-
rors at each node in binary tree y is minimized:
</bodyText>
<equation confidence="0.896762333333333">
∑Erec(x; B) = arg miny∈A(x) Erec([c1; c2]k)
k∈y
(3)
</equation>
<bodyText confidence="0.992721333333333">
Where A(x) denotes all the possible binary trees
that can be built from inputs x. A greedy algorithm
is used to generate the optimal binary tree y∗. The
parameters Brec = (B(1), B(2)) are optimized over
all the phrases in the training data. For further de-
tails, please refer (Socher et al., 2011)
</bodyText>
<subsectionHeader confidence="0.999797">
3.2 Semantic Error
</subsectionHeader>
<bodyText confidence="0.999122142857143">
The BRAE model jointly learns two RAEs for
source language LS and target language LT. Each
RAE learn semantic vector representation ps and
pt of phrases s and t respectively in translation-
equivalent phrase pair (s, t) in bilingual corpora
(shown in Fig.1). The transformation between the
two is defined by:
</bodyText>
<equation confidence="0.987012">
p′t = f(Wtsps + bts), p′s = f(Wts pt + bst) (4)
</equation>
<bodyText confidence="0.99971825">
where Bts = (Wts, bts), Bst = (Wts, bst) are new pa-
rameters introduced.
The semantic error between learned vector rep-
resentations ps and pt is calculated as :
</bodyText>
<equation confidence="0.7303895">
Esem(s, t; B) = E∗sem(t|s; Bst) + E∗sem(s|t; Bts)
(5)
</equation>
<bodyText confidence="0.728879714285714">
where E∗sem(s|t; Bst) is the semantic distance of
ps given pt and vice versa. To calculate it, we
first calculate Euclidean distance between origi-
nal pt and transformation p′t as Dsem (s  |t, Bts) =
211pt − p′
1 t112. The max-semantic-margin distance
between them is then defined as
</bodyText>
<equation confidence="0.9894865">
E∗sem(s|t, Bts) = max{0, Dsem(s|t, Bts)
−Dsem(s|t′, Bts) + 1} (6)
</equation>
<bodyText confidence="0.999206">
where we simultaneously minimize the distance
between translation pairs and maximized between
non-translation pairs. Here t′ in non-translation
pair (s, t′) is obtained by replacing the words in t
with randomly chosen target language words. We
calculate the E∗sem(t|s; Bst) in similar manner.
</bodyText>
<subsectionHeader confidence="0.985918">
3.3 BRAE Objective Function
</subsectionHeader>
<bodyText confidence="0.998244">
Thus, for the phrase pair (s, t), the joint error be-
comes:
</bodyText>
<equation confidence="0.9980704">
E(s, t, B) = E(s|t, B) + E(t|s, B)
E(s|t, B) = aErec(s; Brec
s ) + (1 − a)E∗sem(s|t, Bts))
E(t|s, B) = aErec(t; Brec
t ) + (1 − a)E∗sem(t|s, Bst ))
</equation>
<bodyText confidence="0.915762818181818">
The hyper-parameter a weighs the reconstruction
and semantic errors. The above equation indi-
cates that the Parameter sets Bt = (Bst , Brec
t ) and
Bs = (Bt s,Brec
s )
optimized independently as long as the phrase rep-
resentation of other side is given to compute se-
mantic error.
The final BRAE objective over the phrase pairs
training set (5, T) becomes:
</bodyText>
<equation confidence="0.936259666666667">
1 ∑ JBRAE = N E(s, t; B) + 2 11B112
�BRAE
(s,t)∈(S,T )
</equation>
<subsectionHeader confidence="0.984797">
3.4 Unsupervised Training of BRAE
</subsectionHeader>
<bodyText confidence="0.998385142857143">
The word embedding matrices Ls and Lt are pre-
trained using unlabeled monolingual data with
Word2Vec toolkit (Mikolov et al., 2013). All other
parameters are initialized randomly. We use SGD
algorithm for parameter optimization. For full gra-
dient calculations for each parameter set, please
see (Zhang et al., 2014).
</bodyText>
<listItem confidence="0.987954">
1. RAE Training Phase: Apply RAE Frame-
work (Sec. 3.1) to pre-train the source and target
phrase representations ps and pt respectively by
optimizing Brec sand Brec
t using unlabeled monolin-
gual datasets.
2. Cross-Training Phase: Use target-side
phrase representation pt to update the source-side
</listItem>
<figure confidence="0.744000666666667">
Zff 3TUW to It was good
PS Pt
transformations
on each side respectively can be
161
Reconstruction Cross-Entropy Reconstruction
</figure>
<bodyText confidence="0.999333166666667">
parameters θs and obtain source-side phrase repre-
sentation p′s, and vice-versa for ps. Calculate the
joint error over the bilingual training corpus. On
reaching a local minima or predefined no. of iter-
ations (30 in our case), terminate this phase, other-
wise set ps = p′s, pt = p′t, and repeat.
</bodyText>
<sectionHeader confidence="0.9595215" genericHeader="method">
4 Adapting Model for Classifying
Sentiments
</sectionHeader>
<bodyText confidence="0.9999684">
At the end of previous Training procedure, we ob-
tain high quality phrase embeddings in both source
and target language and transformation function
between them. We now extend that model to per-
form cross lingual supervised tasks, specifically
CLSA.
To achieve this, we need to modify the learned
semantic phrase embeddings such that they can
capture information about sentiment. Since we
only use monolingual labeled datasets from this
point onwards, the supervised learning phases will
occur independently for each RAE as we do not
have any &amp;quot;phrase pairs&amp;quot;now. Thus, the new se-
mantic vector space generated for word and phrase
embeddings may no longer be in sync with their
corresponding transformations.
We propose following modifications to the sys-
tem to deal with this problem. Let LS and LT rep-
resent Resource rich and Resource poor language
respectively in above model.
Modifications in architecture: We first in-
clude a softmax (σ) layer on top of each parent
node in RAE for LS to predict a K-dimensional
multinomial distribution over the set of output
classes defined by the task (e.g : polarity, Ratings).
</bodyText>
<equation confidence="0.949665">
d(p; θce) = σ(Wcep) (9)
</equation>
<bodyText confidence="0.998457153846154">
Given this layer, we calculate cross entropy er-
ror Ece(pk, t, Wce) generated for node pk in binary
tree, where t is target multinomial distribution or
one-hot binary vector for target label. We use this
layer to capture and predict actual sentiment in-
formation about the data in both LS and LT (de-
scribed in next section). We show a node in modi-
fied architecture in Fig.2.
Penalty for Movement in Semantic Vector
space: During subsequent training phases, we in-
clude the euclidean norm of the difference between
the original and new phrase embeddings as penalty
in reconstruction error at each node of the tree.
</bodyText>
<equation confidence="0.864157333333333">
λp
E∗ rec([c1; c2]; θ) = Erec([c1; c2]; θ) + 2 ∥p − p∗∥2
(10)
</equation>
<note confidence="0.267557">
Resource Rich Language Resource Poor Language
</note>
<figureCaption confidence="0.994604">
Figure 2: An illustration of BRAE segment with
Cross Entropy layer
</figureCaption>
<bodyText confidence="0.999745875">
Here p is the phrase representation we get during
forward propagation of current training iteration
and p∗ is the representation we get if we apply the
parameters obtained at the end of the Cross training
phase to children [c1; c2] of that node. The reason
to do this is twofold.
First, during supervised training, the error will
back propagate through RAEs for both languages
affecting their respective weights matrices and
word embeddings. This will modify the semantic
representation of phrases captured during previous
phases of training procedure and adversely affect
the transformations derived from them. Therefore
we need to include some procedure such that the
transformation information learned during Cross-
training phase is not lost.
Secondly, we observe that the information about
the semantic similarity of a word or phrase also im-
plies sentiment similarity between the two. That is
when dealing with bilingual data, words or phrases
that appear near each other in semantic space typi-
cally represent common sentiment information and
we want our model to create a decision boundary
around these vectors instead of modifying them too
much.
Disconnecting the RAEs: We fix the trans-
formation weights between the two RAEs, i.e.
in subsequent training steps the transformation
weights(θts, θst) are not modified but rather pass
the back propagated error as it is to previous lay-
ers. We observed that on optimizing the objec-
tive along with the penalty term, the transforma-
tion weights are preserved between new seman-
tic/sentiment vector spaces, resulting in slightly
degraded performance, but were still able to
preserve enough information about the semantic
structure of two languages.Also, it reinforced the
penalty imposed on the movement of phrase em-
beddings in semantic vector space.On the other
hand, if the weights were allowed to be updated,
</bodyText>
<page confidence="0.990987">
162
</page>
<bodyText confidence="0.999818">
the accuracies were affected severely as infor-
mation learned during previous phases was lost
and the weights were not been able to capture
enough information about the modified phrase em-
beddings and generalize well on test phrases not
encountered in labeled training set of Resource
Scarce Language.
</bodyText>
<subsectionHeader confidence="0.998715">
4.1 Supervised Training Phases
</subsectionHeader>
<bodyText confidence="0.999918666666667">
We now explain supervised training procedure us-
ing only monolingual labeled data for each lan-
guage. These training phases occur at the end of
BRAE training. In each training phase, we use
SGD algorithm to perform parameter optimiza-
tion.
</bodyText>
<subsubsectionHeader confidence="0.777704">
4.1.1 Phase I : Resource Rich language
</subsubsectionHeader>
<bodyText confidence="0.9498504">
In this phase, we only modify the parameters of
RAELS, i.e. θrec
s and θce by optimizing following
objective over (sentence, label) pairs (x, t) in its
labeled corpus.
</bodyText>
<equation confidence="0.999314">
1 ∑ JS = N λS
(x,t) E(x, t; θ) + 2 11θ112 (11)
</equation>
<bodyText confidence="0.605393666666667">
where E(x, t; θ) is the sum over the errors obtained
at each node of the tree that is constructed by the
greedy RAE:
</bodyText>
<equation confidence="0.9945">
E(x, t; θ) = ∑ κE�rec([c1; c2]k; θs)
kERAELS (x)
+ (1 − κ)Ece(pk, t; θce)
(12)
</equation>
<bodyText confidence="0.985465090909091">
To compute this gradient, we first greedily con-
struct all trees and then derivatives for these trees
are computed efficiently via back-propagation
through structure (Goller and Kuchler, 1996). The
gradient for our new reconstruction function (Eq.
10) w.r.t to p at a given node is calculated as
The first term ∂E��c
∂p is calculated as in standard
RAE model. The partial derivative in above equa-
tion is used to compute parameter gradients in stan-
dard back-propagation algorithm.
</bodyText>
<subsubsectionHeader confidence="0.706838">
4.1.2 Phase II : Resource Poor Language
</subsubsectionHeader>
<bodyText confidence="0.99996875">
In this phase, we modify the parameters of
RAEL, and θce by optimizing Objective JT over
(sentence, label) pairs (x, t) in labeled corpus for
LT (much smaller than that for LS). The equation
for JT is similar to Eq.11 and Eq.12 but with θt and
η as parameters instead of θs and κ respectively.
Since cross-entropy layer is only associated with
LS, we need to traverse the transformation param-
eters to obtain sentiment distribution for each node
(green path in Fig.2). That is, we first transform pt
to source side phrase p′s and then apply the cross
entropy weights to it.
</bodyText>
<equation confidence="0.90417">
d(pt, θce) = σ(Wce.f(Wtspt + bts)) (14)
</equation>
<bodyText confidence="0.98800325">
We use the similar back-propagation through
structure approach for gradient calculation in
Phase I. During back propagation, 1) we do not
update the transformation weights, 2) we transfer
error signals during back-propagation from Cross-
entropy layer to θ(1)
t as if the transformation was
an additional layer in the network.
</bodyText>
<subsectionHeader confidence="0.754351">
4.1.3 Predicting overall sentiment
</subsectionHeader>
<bodyText confidence="0.999938">
To predict overall sentiment associated with the
sentence in LT, we use the phrase embeddings pt
of the top layer of the RAEL, and it transforma-
tion p′s. Together, we train a softmax regression
classifier on concatenation of these two vector us-
ing weight matrix W E RK�2n
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="evaluation">
5 Experimental Work
</sectionHeader>
<bodyText confidence="0.99866325">
We perform experiments on two kind of sentiment
analysis systems : (1) that gives +ve/-ve polarity
to each review and (2) assigns ratings in range 1 -
4 to each review.
</bodyText>
<subsectionHeader confidence="0.957024">
5.1 External Datasets Used
</subsectionHeader>
<bodyText confidence="0.9848645">
For pre-training the word embeddings and RAE
Training, we used HindMonoCorp 0.5(Bojar et al.,
2014) with 44.49M sentences (787M Tokens) and
English Gigaword Corpus.
For Cross Training, we used the bilingual
sentence-aligned data from HindEnCorp1 (Bojar et
al., 2014) with 273.9k sentence pairs (3.76M En-
glish, 3.88M Hindi Tokens). This dataset contains
sentence pair obtained from Bilingual New Arti-
cles, Wikipedia entries, Automated Translations,
etc. Training and Validation division is 70% and
30% for all above datasets.
In Supervised Phase I, we
used IMDB11 dataset available at
http://ai.stanford.edu/~amaas/data/sentiment/
and first used by (Maas et al., 2011) for +ve/-ve
</bodyText>
<footnote confidence="0.730787">
1http://ufal.mff.cuni.cz/hindencorp
</footnote>
<equation confidence="0.9419446">
∂E�rec
∂p
∂ + λp(p − p*) (13)
p
∂Erec
</equation>
<page confidence="0.994143">
163
</page>
<bodyText confidence="0.999423666666667">
system containing 25000 +ve and 25000 -ve
movie reviews.
For 4-ratings system, we use Rotten Toma-
toes Review dataset (scale dataset v1.0) found
at http://www.cs.cornell.edu/People/pabo/movie-
review-data. The dataset is divided into four
author-specific corpora, containing 1770, 902,
1307, and 1027 documents and each document has
accompanying 4-Ratings ({0, 1, 2, 3}) label.
</bodyText>
<subsectionHeader confidence="0.847807">
5.2 Rating Based Hindi Movie Review
(RHMR) Dataset
</subsectionHeader>
<bodyText confidence="0.999980538461538">
We crawled the Hindi Movie Reviews Website2 to
obtain 2945 movie reviews. Each Movie Review
on this site is assigned rating in range 1 to 4 by
at least three reviewers. We first discard reviews
that whose sum of pairwise difference of ratings is
greater than two. The final rating for each review is
calculated by taking the average of the ratings and
rounding up to nearest integer. The fraction of Re-
views obtained in ratings 1-4 are [0.20, 0.25, 0.35,
0.20] respectively. Average length of reviews is
84 words. For +ve/-ve polarity based system, we
group the reviews with ratings {1, 2} as negative
and {3, 4} as positive.
</bodyText>
<subsectionHeader confidence="0.991345">
5.3 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.987115217391304">
We used following Baselines for Sentiment Anal-
ysis in Hindi:
Majority class: Assign the most frequent class
in the training set (Rating:3 / Polarity:+ve)
Bag-of-words: Softmax regression on Binary
Bag-of-words
We also compare our system with state of the art
Monolingual and Cross Lingual System for Senti-
ment Analysis in Hindi as described by (Popat et
al., 2013) using the same experimental setup. The
best systems in each category given by them are as
below:
WordNet Based: Using Hindi-SentiWordNet3,
each word in a review was mapped to correspond-
ing synset identifiers. These identifiers were used
as features for creating sentiment classifiers based
on Binary/Multiclass SVM trained on bag ofwords
representation using libSVM library.
Cross Lingual (XL) Clustering Based: Here,
joint clustering was performed on unlabeled bilin-
gual corpora which maximizes the joint likelihood
of monolingual and cross-lingual factors.. For de-
tails, please refer the work of (Popat et al., 2013).
</bodyText>
<footnote confidence="0.999555">
2http://hindi.webdunia.com/bollywood-movie-review/
3http://www.cfilt.iitb.ac.in/
</footnote>
<bodyText confidence="0.998648578947368">
Each word in a review was then mapped to its clus-
ter identifier and used as features in an SVM.
Our approaches
Basic RAE: We use the Semi-Supervised RAE
based classification where we first trained a stan-
dard RAE using Hindi monolingual corpora, then
applied supervised training procedure as described
in (Socher et al., 2011). This approach doesn&apos;t use
bilingual corpora, but is dependent on amount of
labeled data in Hindi.
BRAE-U: We neither include penalty term, nor
fix the transformations weights in our proposed
system.
BRAE-P: We only include the penalty term but
allow the transformation weights to be modified in
proposed system.
BRAE-F: We add the penalty term and fix the
transformation weights during back propagation in
proposed system.
</bodyText>
<subsectionHeader confidence="0.91327">
5.4 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999472">
We combined the text data from all English
Datasets (English Gigaword + HindEnCorp En-
glish Portion + IBMD11 + Scale Dataset) de-
scribed above to train the word embeddings us-
ing Word2Vec toolkit and RAE. Similarly, we
combined text data from all Hindi Datasets
(HindMonoCorp + HindiEnCorp Hindi Portion +
RHMR) to train word embeddings and RAE for
Hindi.
We used MOSES Toolkit (Koehn et al., 2007)
to obtain high quality bilingual phrase pairs from
HindEnCorp to train our BRAE model. After
removing the duplicates, 364.3k bilingual phrase
pairs were obtained with lengths ranging from 1-
6, since bigger phrases reduced the performance of
the system in terms of Joint Error of BRAE model.
We randomly split our RHMR dataset into 10
segments and report the average of 10-fold cross
validation accuracies for each setting for both Rat-
ings and Polarity classifiers.
We also report 5-fold cross validation accuracy
on Standard Movie Reviews Dataset (hereby re-
ferred as SMRD) given by (Joshi et al., 2010)
which contains 125 +ve and 125 -ve reviews
in Hindi. The dataset can be obtained at
http://www.cfilt.iitb.ac.in/Resources.html.
Since this project is about reducing depen-
dence on annotated datasets, we experiment on
how accuracy varies with labeled training dataset
(RHMR) size. To perform this, we train our model
</bodyText>
<page confidence="0.996844">
164
</page>
<bodyText confidence="0.999963166666667">
in 10% increments (150 examples) of training set
size (each class sampled in proportion of original
set). For each size, we sample the data 10 times
with replacement and trained the model. For each
sample, we calculated 10-fold cross validation ac-
curacy as described above. Final accuracy for each
size was calculated by averaging the accuracies ob-
tained on all 10 samples. Similar kind of evalua-
tion is done for all other Baselines explored.
In subsequent section, the word &apos;significant&apos; im-
plies that the results were statistically significant
(p &lt; 0.05) with paired T-test
</bodyText>
<subsectionHeader confidence="0.998756">
5.5 BRAE Hyper Parameters
</subsectionHeader>
<bodyText confidence="0.999788846153846">
We empirically set the learning rate as 0.05. The
word vector dimension was selected as 80 from set
[40, 60, 80,100,120] using Cross Validation. We
used joint error of BRAE model to select α as 0.2
from range [0.05, 0.5] in steps of 0.05. Also, AL
was set as 0.001 for DNN trained for word embed-
ding and ABRAE as 0.0001.
For semi-supervised phases , we used 5-fold
cross validation on training set to select r. and q in
range [0.0, 1.0] in steps of 0.05 with optimal value
obtained at r. = 0.2 and q = 0.35. Parameter Ap
was selected as 0.01 , AS as 0.1 and AT as 0.04
after selection in range [0.0, 1.0] in steps of 0.01.
</bodyText>
<table confidence="0.998829727272727">
5.6 Results RHMR SMRD
Dataset
Classifier Ratings Polarity Polarity
Majority class 35.19 51.83 52.34
Bag-of-Words 51.98 62.52 68.47
WordNet based 55.47 67.29 75.5
XL Clustering 72.34 84.46 84.71
Basic RAE 75.53 79.31 81.06
BRAE-U 76.01 82.66 84.83
BRAE-P 79.70 84.85 87.00
BRAE-F 81.22 90.50 90.21
</table>
<tableCaption confidence="0.976129333333333">
Table 1: Accuracies obtained for various Exper-
imental Settings. Model are trained on complete
labeled training datasets
</tableCaption>
<table confidence="0.9998028">
A l /P -* P-1 P-2 P-3 P-4
A-1 83.19 15.28 1.53 0.00
A-2 0.00
A-3
A-4
12.23 82.20 5.57
0.00 9.03 81.26 9.71
0.00
1.87 19.69 78.44
F1-score 0.83 0.78 0.82 0.80
</table>
<tableCaption confidence="0.935930666666667">
Table 2: Confusion Matrix for Ratings by BRAE-
F, Across: Predicted Rating, Downward: Actual
Rating
</tableCaption>
<bodyText confidence="0.998766315789474">
+ve/-ve polarity classifier, the accuracy showed an
improvement of 6% over next highest baseline.
In Table 2, we calculate the confusion matrix for
our model(BRAE-F) for the 4-Ratings case. Value
in a cell (AZ, Pj) represents the percentage of ex-
amples in actual rating class i that are predicted
as rating j. We also show the F1 score calcu-
lated for each individual rating class. It clearly
shows that our model has low variation in F1-
scores and thereby its performance among various
rating classes.
In Fig. 3, we show the variation in accuracy
of the classifiers with amount of sentiment labeled
Training data used. We note that our approach con-
sistently outperforms the explored baselines at all
dataset sizes. Also, our model was able to attain
accuracy comparable to other baselines at about
50% less labeled data showing its strength in ex-
ploiting the unlabeled resources.
</bodyText>
<figure confidence="0.9776187">
90
80
60
70
50
Majority BOW
WordNet XL
RAE BRAE-U
BRAE-P BRAE-F
0 0.2 0.4 0.6 0.8 1
</figure>
<bodyText confidence="0.994602857142857">
Table 1 present the results obtained for both rat-
ings based and polarity classifier on RHMR and
MRD Dataset. Our model gives significantly bet-
ter performance for ratings based classification
than any other baseline system currently used for
SA in Hindi. The margin of accuracy obtained
against next best classifier is about 8%. Also, for
</bodyText>
<figureCaption confidence="0.78734475">
Figure 3: Variation of Accuracy (+ve/-ve Polarity)
with Size of labeled Dataset(Hindi), x-axis: Frac-
tion of Dataset Used, y-axis: %age Accuracy Ob-
tained
</figureCaption>
<bodyText confidence="0.996383">
We also experiment with variation of accuracies
</bodyText>
<page confidence="0.993625">
165
</page>
<table confidence="0.999894111111111">
New Word/Phrase Similar Words/Phrases Sentiment label
depressing gloomy उदास Rating : 1
Ǔनराशाजनक discouraging Ǔनराशा×मक Polarity : -ve
was painful was difficult कǑठन था Rating : 2
दद[नाक था was bad खराब था Polarity : -ve
should be awarded was appreciated सराहना कȧ गई Rating : 4
सàमाǓनत ͩकया जाना चाǑहए will get accolades वाहवाहȣ ͧमलना चाǑहए Polarity : +ve
public won&apos;t come no one will come कोई नहȣं आएगा Rating : 1
लोग नहȣं आएगा viewers won&apos;t come दश[क नहȣं आएगा Polarity : -ve
</table>
<tableCaption confidence="0.999873">
Table 3: Semantically similar phrases obtained for new phrases and their assigned label
</tableCaption>
<bodyText confidence="0.998083444444444">
with amount of Unlabeled Bilingual Training Data
used for Cross Lingual models explored. Again
we increase size of bilingual dataset in 10% incre-
ments and calculate the accuracy as described pre-
viously. In Fig. 4, we observed that performance
of the proposed approach steadily increases with
amount of data added, yet even at about 50000
(20%) phrase pairs, our model produces remark-
able gains in accuracy.
</bodyText>
<figure confidence="0.708817">
0 0.2 0.4 0.6 0.8 1
</figure>
<figureCaption confidence="0.83100975">
Figure 4: Variation of Accuracy (+ve/-ve polarity)
with Size of Unlabeled Bilingual Corpora, x-axis:
Fraction of Training Data Used, y-axis: %age Ac-
curacy Obtained
</figureCaption>
<bodyText confidence="0.99978325">
We also observed that the model which restricts
modification to transformation weights during su-
pervised phase II does better than the one which
allows the modification at all dataset sizes. This
result appears to be counterintuitive to normal op-
eration of neural network based models, but sup-
ports our hypothesis as explained in previous sec-
tions.
</bodyText>
<subsectionHeader confidence="0.965665">
5.7 Performance and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999686421052632">
Analysis on the test results showed that the major
advantage given by our model occurs due to pres-
ence of unknown words (i.e.words not present in
labeled dataset) in test data. Since we restricted
the movement in semantic vector space, our model
was able to infer the sentiment for a unknown
word/phrase by comparing it with semantically
similar words/phrases. In Table 3, we extracted
the Top-2 semantically similar phrases in training
set for small new phrases and sentiment labeled
assigned to them by our model (the phrases are
manually translated from Hindi for reader&apos;s under-
standing). As we can see, our model was able to
extract grammatically correct phrases with similar
semantic nature as given phrase and assign correct
sentiment label to it.
Secondly, We found that our model was able to
correctly infer word sense for polysemous words
that adversely affected the quality of sentiment
classifiers in our baselines. This eliminates the
need for manually constructed fine grained lexi-
cal resource like WordNets and development of
automated annotation resources. For example,
to a phrase like &amp;quot;Her acting of a schizophrenic
mother made our hearts weep&amp;quot;, the baselines clas-
sifiers assigned negative polarity due to presence
of words like &apos;weep&apos;, yet our model was correctly
able to predict positive polarity and assigned it a
rating of 3.
Error Analysis of test results showed that errors
made by our model can be classified in two major
categories :
1) A review may only give description of the
object in question (in our case , the description of
the film) without actually presenting any individ-
ual sentiments about it or it may express conflict-
ing sentiments about two different aspects about
the same object. This presents difficulty in assign-
</bodyText>
<figure confidence="0.980468333333333">
90
80
60
70
XL BRAE-U
BRAE-P BRAE-F
</figure>
<page confidence="0.979716">
166
</page>
<bodyText confidence="0.992018928571429">
ing a single polarity/rating to the review.
2) Presence of subtle contextual references af-
fected the quality of predictions made by our clas-
sifier. For example, sentence like &apos;&apos;His poor acting
generally destroys a movie, but this time it didn&apos;t&amp;quot;got a rating of 2 due to presence of phrase with
negative sense (here the phrase doesn&apos;t have am-
biguous sense), yet the actual sentiment expressed
is positive due to temporal dependence and gen-
eralization. Also, &amp;quot;This movie made his last one
looked good&amp;quot; makes a reference to entities exter-
nal to the review, which again forces our model to
make wrong prediction of rating 3.
Analyzing these aspects and making correct pre-
dictions on such examples needs further work.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999828407407407">
This study focused on developing a Cross Lin-
gual Supervised Classifier based on Bilingually
Constrained Recursive Autoencoder. To achieve
this, our model first learns phrase embeddings for
two languages using Standard RAE, then fine tune
these embeddings using Cross Training procedure.
After imposing certain restrictions on these em-
beddings, we perform supervised training using
labeled sentiment corpora in English and a much
smaller one in Hindi to get the final classifier.
The experimental work showed that our model
was remarkably effective for classification of
Movie Reviews in Hindi on a rating scale and
predicting polarity using least amount of data to
achieve same accuracy as other systems explored.
Moreover it reduces the need for MT System or
lexical resources like Linked WordNets since the
performance is not degraded too much even when
we lack large quantity of labeled data.
In Future, we hope to 1) extend this system to
learn phrase representations among multiple lan-
guages simultaneously, 2) apply this framework to
other cross Lingual Tasks such as Paraphrase de-
tection, Question Answering, Aspect Based Opin-
ion Mining etc and 3) Learning different weight
matrices at different nodes to capture complex re-
lations between words and phrases.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985375409836">
Balamurali, Aditya Joshi, and Pushpak Bhattacharyya.
2012. Cross-lingual sentiment analysis for Indian
languages using linked wordnets. In Proceedings of
COLING 2012: Posters, pages 73--82. The COL-
ING 2012 Organizing Committee.
Ondřej Bojar, Vojtěch Diatka, Pavel Rychlý, Pavel
Straiiák, Vít Suchomel, Aleš Tamchyna, and Daniel
Zeman. 2014. HindEnCorp - Hindi-English and
Hindi-only Corpus for Machine Translation. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC&apos;14). Eu-
ropean Language Resources Association (ELRA).
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In RANLP, pages 50--54.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on Ma-
chine learning, pages 160--167. ACM.
Hatem Ghorbel and David Jacot. 2011. Further experi-
ments in sentiment analysis of french movie reviews.
In Advances in Intelligent Web Mastering--3, pages
19--28. Springer.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347--352. IEEE.
Aditya Joshi, AR Balamurali, and Pushpak Bhat-
tacharyya. 2010. A fall-back strategy for sentiment
analysis in hindi: a case study. Proceedings of the
8th ICON.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177--180. Association for Computational Linguis-
tics.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 320--330. Associa-
tion for Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142--150. Asso-
ciation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111--3119.
</reference>
<page confidence="0.975084">
167
</page>
<reference confidence="0.999359432432433">
Namita Mittal, Basant Agarwal, Garvit Chouhan, Nitin
Bania, and Prateek Pareek. 2013. Sentiment analy-
sis of hindi review based on negation and discourse
relation. In proceedings of International Joint Con-
ference on Natural Language Processing, pages 45-
-50.
Kashyap Popat, Balamurali A.R, Pushpak Bhat-
tacharyya, and Gholamreza Haffari. 2013. The
haves and the have-nots: Leveraging unlabelled cor-
pora for sentiment analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
412--422. Association for Computational Linguis-
tics.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151--161. Association for
Computational Linguistics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Chang Wan, Rong Pan, and Jiefei Li. 2011. Bi-
weighting domain adaptation for cross-language text
classification. In IJCAI Proceedings-International
Joint Conference on Artificial Intelligence, vol-
ume 22, page 1535.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 111--121. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.997297">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856754">
<title confidence="0.987362">Cross-Lingual Sentiment Analysis using modified BRAE</title>
<author confidence="0.972989">Sarthak</author>
<affiliation confidence="0.959562">Department of Computer Delhi Technological DL,</affiliation>
<email confidence="0.99923">successar@gmail.com</email>
<abstract confidence="0.99938372">Cross-Lingual Learning provides a mechanism to adapt NLP tools available for label rich languages to achieve similar tasks for label-scarce languages. An efficient cross-lingual tool significantly reduces the cost and effort required to manually annotate data. In this paper, we use the Recursive Autoencoder architecture to develop a Cross Lingual Sentiment Analysis (CLSA) tool using sentence aligned corpora between a pair of resource rich (English) and resource poor (Hindi) language. The system is based on the assumption that semantic similarity between different phrases also implies sentiment similarity in majority of sentences. The resulting system is then analyzed on a newly developed Movie Reviews Dataset in Hindi with labels given on a rating scale and compare performance of our system against existing systems. It is shown that our approach significantly outperforms state of the art systems for Sentiment Analysis, especially when labeled data is scarce.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aditya Joshi Balamurali</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Cross-lingual sentiment analysis for Indian languages using linked wordnets.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>73--82</pages>
<marker>Balamurali, Bhattacharyya, 2012</marker>
<rawString>Balamurali, Aditya Joshi, and Pushpak Bhattacharyya. 2012. Cross-lingual sentiment analysis for Indian languages using linked wordnets. In Proceedings of COLING 2012: Posters, pages 73--82. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondřej Bojar</author>
<author>Vojtěch Diatka</author>
<author>Pavel Rychlý</author>
<author>Pavel Straiiák</author>
<author>Vít Suchomel</author>
<author>Aleš Tamchyna</author>
<author>Daniel Zeman</author>
</authors>
<title>HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="19185" citStr="Bojar et al., 2014" startWordPosition="3127" endWordPosition="3130">.3 Predicting overall sentiment To predict overall sentiment associated with the sentence in LT, we use the phrase embeddings pt of the top layer of the RAEL, and it transformation p′s. Together, we train a softmax regression classifier on concatenation of these two vector using weight matrix W E RK�2n 5 Experimental Work We perform experiments on two kind of sentiment analysis systems : (1) that gives +ve/-ve polarity to each review and (2) assigns ratings in range 1 - 4 to each review. 5.1 External Datasets Used For pre-training the word embeddings and RAE Training, we used HindMonoCorp 0.5(Bojar et al., 2014) with 44.49M sentences (787M Tokens) and English Gigaword Corpus. For Cross Training, we used the bilingual sentence-aligned data from HindEnCorp1 (Bojar et al., 2014) with 273.9k sentence pairs (3.76M English, 3.88M Hindi Tokens). This dataset contains sentence pair obtained from Bilingual New Articles, Wikipedia entries, Automated Translations, etc. Training and Validation division is 70% and 30% for all above datasets. In Supervised Phase I, we used IMDB11 dataset available at http://ai.stanford.edu/~amaas/data/sentiment/ and first used by (Maas et al., 2011) for +ve/-ve 1http://ufal.mff.cu</context>
</contexts>
<marker>Bojar, Diatka, Rychlý, Straiiák, Suchomel, Tamchyna, Zeman, 2014</marker>
<rawString>Ondřej Bojar, Vojtěch Diatka, Pavel Rychlý, Pavel Straiiák, Vít Suchomel, Aleš Tamchyna, and Daniel Zeman. 2014. HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Maite Taboada</author>
</authors>
<title>Cross-linguistic sentiment analysis: From english to spanish. In</title>
<date>2009</date>
<booktitle>RANLP,</booktitle>
<pages>50--54</pages>
<contexts>
<context position="4471" citStr="Brooke et al., 2009" startWordPosition="680" endWordPosition="683">or Computational Linguistics. ing positive/negative polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to achieve better accuracies. For Languages where labeled data is not present, approaches based on cross-lingual sentiment analysis are used. Usually, such methods need intermediary machine translation system (Wan et al., 2011; Brooke et al., 2009) or abilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways in which sentiments can be expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to perform well(Balamurali et al., 2012). (Balamurali et al., 2012) present an alternative approach to Cross Lingual Sentiment Analysis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained on sense marked and po</context>
</contexts>
<marker>Brooke, Tofiloski, Taboada, 2009</marker>
<rawString>Julian Brooke, Milan Tofiloski, and Maite Taboada. 2009. Cross-linguistic sentiment analysis: From english to spanish. In RANLP, pages 50--54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7929" citStr="Collobert and Weston, 2008" startWordPosition="1223" endWordPosition="1226">the bilingually-constrained model learns to finetune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between nontranslation pairs. In this section, We will briefly present the structure and training algorithm for BRAE model. After that, we show how this model can be adapted to perform CLSA. 3.1 Recursive Auto-encoder Framework In this model, each word wk in the vocabulary V of given language corresponds to a vector xk E Rn and stacked into a single word embedding matrix L E Rnx|V |. This matrix is learned using DNN (Collobert and Weston, 2008; Mikolov et al., 2013) and serves as input to further stages of RAE. Using this matrix, a phrase (w1w2 ... wm) is first projected into a list of vectors (x1, x2, ... xm). The RAE learns the vector representation of the phrase by combining two children vectors recursively in a bottom-up manner. For two children c1 = x1, c2 = x2, the auto-encoder computes the parent vector y1: y1 = f(W(1)[c1; c21 + b(1)); y1 E Rn (1) To assess how well the parent vector represents its children, the auto-encoder reconstructs the chil160 Figure 1: An illustration of BRAE structure dren : [c′1; c′2] = W (2)p + b(2</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160--167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hatem Ghorbel</author>
<author>David Jacot</author>
</authors>
<title>Further experiments in sentiment analysis of french movie reviews.</title>
<date>2011</date>
<booktitle>In Advances in Intelligent Web Mastering--3,</booktitle>
<pages>19--28</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4521" citStr="Ghorbel and Jacot, 2011" startWordPosition="687" endWordPosition="690">tive polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to achieve better accuracies. For Languages where labeled data is not present, approaches based on cross-lingual sentiment analysis are used. Usually, such methods need intermediary machine translation system (Wan et al., 2011; Brooke et al., 2009) or abilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways in which sentiments can be expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to perform well(Balamurali et al., 2012). (Balamurali et al., 2012) present an alternative approach to Cross Lingual Sentiment Analysis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained on sense marked and polarity labeled corpora in Resource rich language. </context>
</contexts>
<marker>Ghorbel, Jacot, 2011</marker>
<rawString>Hatem Ghorbel and David Jacot. 2011. Further experiments in sentiment analysis of french movie reviews. In Advances in Intelligent Web Mastering--3, pages 19--28. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Neural Networks, 1996., IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="17297" citStr="Goller and Kuchler, 1996" startWordPosition="2807" endWordPosition="2810">Resource Rich language In this phase, we only modify the parameters of RAELS, i.e. θrec s and θce by optimizing following objective over (sentence, label) pairs (x, t) in its labeled corpus. 1 ∑ JS = N λS (x,t) E(x, t; θ) + 2 11θ112 (11) where E(x, t; θ) is the sum over the errors obtained at each node of the tree that is constructed by the greedy RAE: E(x, t; θ) = ∑ κE�rec([c1; c2]k; θs) kERAELS (x) + (1 − κ)Ece(pk, t; θce) (12) To compute this gradient, we first greedily construct all trees and then derivatives for these trees are computed efficiently via back-propagation through structure (Goller and Kuchler, 1996). The gradient for our new reconstruction function (Eq. 10) w.r.t to p at a given node is calculated as The first term ∂E��c ∂p is calculated as in standard RAE model. The partial derivative in above equation is used to compute parameter gradients in standard back-propagation algorithm. 4.1.2 Phase II : Resource Poor Language In this phase, we modify the parameters of RAEL, and θce by optimizing Objective JT over (sentence, label) pairs (x, t) in labeled corpus for LT (much smaller than that for LS). The equation for JT is similar to Eq.11 and Eq.12 but with θt and η as parameters instead of θ</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347--352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Joshi</author>
<author>AR Balamurali</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>A fall-back strategy for sentiment analysis in hindi: a case study.</title>
<date>2010</date>
<booktitle>Proceedings of the 8th ICON.</booktitle>
<contexts>
<context position="3441" citStr="Joshi et al., 2010" startWordPosition="532" endWordPosition="535">in two languages, that share same semantic meaning, can be used to learn language independent semantic vector representations. These embeddings can further be fine-tuned using labeled dataset in English to capture enough class information regarding Resource poor language. We train the resultant framework on English-Hindi Language pair and evaluate it against state of the art SA systems on existing and newly developed dataset. 2 Related Work 2.1 Sentiment Analysis in Hindi In recent years, there have been emergence of works on Sentiment Analysis (both monolingual and cross-lingual) for Hindi. (Joshi et al., 2010) provided a comparative analysis of Unigram based In-language, MT based Cross Lingual and WordNet based Sentiment classifier, achieving highest accuracy of 78.14%. (Mittal et al., 2013) described a system based on Hindi SentiWordNet for assign159 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 159–168, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ing positive/negative polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarit</context>
<context position="23760" citStr="Joshi et al., 2010" startWordPosition="3836" endWordPosition="3839">n et al., 2007) to obtain high quality bilingual phrase pairs from HindEnCorp to train our BRAE model. After removing the duplicates, 364.3k bilingual phrase pairs were obtained with lengths ranging from 1- 6, since bigger phrases reduced the performance of the system in terms of Joint Error of BRAE model. We randomly split our RHMR dataset into 10 segments and report the average of 10-fold cross validation accuracies for each setting for both Ratings and Polarity classifiers. We also report 5-fold cross validation accuracy on Standard Movie Reviews Dataset (hereby referred as SMRD) given by (Joshi et al., 2010) which contains 125 +ve and 125 -ve reviews in Hindi. The dataset can be obtained at http://www.cfilt.iitb.ac.in/Resources.html. Since this project is about reducing dependence on annotated datasets, we experiment on how accuracy varies with labeled training dataset (RHMR) size. To perform this, we train our model 164 in 10% increments (150 examples) of training set size (each class sampled in proportion of original set). For each size, we sample the data 10 times with replacement and trained the model. For each sample, we calculated 10-fold cross validation accuracy as described above. Final </context>
</contexts>
<marker>Joshi, Balamurali, Bhattacharyya, 2010</marker>
<rawString>Aditya Joshi, AR Balamurali, and Pushpak Bhattacharyya. 2010. A fall-back strategy for sentiment analysis in hindi: a case study. Proceedings of the 8th ICON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23156" citStr="Koehn et al., 2007" startWordPosition="3738" endWordPosition="3741"> the penalty term but allow the transformation weights to be modified in proposed system. BRAE-F: We add the penalty term and fix the transformation weights during back propagation in proposed system. 5.4 Experimental Setup We combined the text data from all English Datasets (English Gigaword + HindEnCorp English Portion + IBMD11 + Scale Dataset) described above to train the word embeddings using Word2Vec toolkit and RAE. Similarly, we combined text data from all Hindi Datasets (HindMonoCorp + HindiEnCorp Hindi Portion + RHMR) to train word embeddings and RAE for Hindi. We used MOSES Toolkit (Koehn et al., 2007) to obtain high quality bilingual phrase pairs from HindEnCorp to train our BRAE model. After removing the duplicates, 364.3k bilingual phrase pairs were obtained with lengths ranging from 1- 6, since bigger phrases reduced the performance of the system in terms of Joint Error of BRAE model. We randomly split our RHMR dataset into 10 segments and report the average of 10-fold cross validation accuracies for each setting for both Ratings and Polarity classifiers. We also report 5-fold cross validation accuracy on Standard Movie Reviews Dataset (hereby referred as SMRD) given by (Joshi et al., 2</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177--180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>320--330</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4539" citStr="Lu et al., 2011" startWordPosition="691" endWordPosition="694">views. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to achieve better accuracies. For Languages where labeled data is not present, approaches based on cross-lingual sentiment analysis are used. Usually, such methods need intermediary machine translation system (Wan et al., 2011; Brooke et al., 2009) or abilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways in which sentiments can be expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to perform well(Balamurali et al., 2012). (Balamurali et al., 2012) present an alternative approach to Cross Lingual Sentiment Analysis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained on sense marked and polarity labeled corpora in Resource rich language. The crux of the id</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 320--330. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19753" citStr="Maas et al., 2011" startWordPosition="3208" endWordPosition="3211">ning, we used HindMonoCorp 0.5(Bojar et al., 2014) with 44.49M sentences (787M Tokens) and English Gigaword Corpus. For Cross Training, we used the bilingual sentence-aligned data from HindEnCorp1 (Bojar et al., 2014) with 273.9k sentence pairs (3.76M English, 3.88M Hindi Tokens). This dataset contains sentence pair obtained from Bilingual New Articles, Wikipedia entries, Automated Translations, etc. Training and Validation division is 70% and 30% for all above datasets. In Supervised Phase I, we used IMDB11 dataset available at http://ai.stanford.edu/~amaas/data/sentiment/ and first used by (Maas et al., 2011) for +ve/-ve 1http://ufal.mff.cuni.cz/hindencorp ∂E�rec ∂p ∂ + λp(p − p*) (13) p ∂Erec 163 system containing 25000 +ve and 25000 -ve movie reviews. For 4-ratings system, we use Rotten Tomatoes Review dataset (scale dataset v1.0) found at http://www.cs.cornell.edu/People/pabo/moviereview-data. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents and each document has accompanying 4-Ratings ({0, 1, 2, 3}) label. 5.2 Rating Based Hindi Movie Review (RHMR) Dataset We crawled the Hindi Movie Reviews Website2 to obtain 2945 movie reviews. Each Movi</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142--150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="7952" citStr="Mikolov et al., 2013" startWordPosition="1227" endWordPosition="1230">model learns to finetune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between nontranslation pairs. In this section, We will briefly present the structure and training algorithm for BRAE model. After that, we show how this model can be adapted to perform CLSA. 3.1 Recursive Auto-encoder Framework In this model, each word wk in the vocabulary V of given language corresponds to a vector xk E Rn and stacked into a single word embedding matrix L E Rnx|V |. This matrix is learned using DNN (Collobert and Weston, 2008; Mikolov et al., 2013) and serves as input to further stages of RAE. Using this matrix, a phrase (w1w2 ... wm) is first projected into a list of vectors (x1, x2, ... xm). The RAE learns the vector representation of the phrase by combining two children vectors recursively in a bottom-up manner. For two children c1 = x1, c2 = x2, the auto-encoder computes the parent vector y1: y1 = f(W(1)[c1; c21 + b(1)); y1 E Rn (1) To assess how well the parent vector represents its children, the auto-encoder reconstructs the chil160 Figure 1: An illustration of BRAE structure dren : [c′1; c′2] = W (2)p + b(2) (2) and tries to mini</context>
<context position="11358" citStr="Mikolov et al., 2013" startWordPosition="1833" endWordPosition="1836"> = aErec(t; Brec t ) + (1 − a)E∗sem(t|s, Bst )) The hyper-parameter a weighs the reconstruction and semantic errors. The above equation indicates that the Parameter sets Bt = (Bst , Brec t ) and Bs = (Bt s,Brec s ) optimized independently as long as the phrase representation of other side is given to compute semantic error. The final BRAE objective over the phrase pairs training set (5, T) becomes: 1 ∑ JBRAE = N E(s, t; B) + 2 11B112 �BRAE (s,t)∈(S,T ) 3.4 Unsupervised Training of BRAE The word embedding matrices Ls and Lt are pretrained using unlabeled monolingual data with Word2Vec toolkit (Mikolov et al., 2013). All other parameters are initialized randomly. We use SGD algorithm for parameter optimization. For full gradient calculations for each parameter set, please see (Zhang et al., 2014). 1. RAE Training Phase: Apply RAE Framework (Sec. 3.1) to pre-train the source and target phrase representations ps and pt respectively by optimizing Brec sand Brec t using unlabeled monolingual datasets. 2. Cross-Training Phase: Use target-side phrase representation pt to update the source-side Zff 3TUW to It was good PS Pt transformations on each side respectively can be 161 Reconstruction Cross-Entropy Recons</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111--3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Namita Mittal</author>
<author>Basant Agarwal</author>
<author>Garvit Chouhan</author>
<author>Nitin Bania</author>
<author>Prateek Pareek</author>
</authors>
<title>Sentiment analysis of hindi review based on negation and discourse relation.</title>
<date>2013</date>
<booktitle>In proceedings of International Joint Conference on Natural Language Processing,</booktitle>
<pages>45--50</pages>
<contexts>
<context position="3626" citStr="Mittal et al., 2013" startWordPosition="559" endWordPosition="562">dataset in English to capture enough class information regarding Resource poor language. We train the resultant framework on English-Hindi Language pair and evaluate it against state of the art SA systems on existing and newly developed dataset. 2 Related Work 2.1 Sentiment Analysis in Hindi In recent years, there have been emergence of works on Sentiment Analysis (both monolingual and cross-lingual) for Hindi. (Joshi et al., 2010) provided a comparative analysis of Unigram based In-language, MT based Cross Lingual and WordNet based Sentiment classifier, achieving highest accuracy of 78.14%. (Mittal et al., 2013) described a system based on Hindi SentiWordNet for assign159 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 159–168, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ing positive/negative polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to </context>
</contexts>
<marker>Mittal, Agarwal, Chouhan, Bania, Pareek, 2013</marker>
<rawString>Namita Mittal, Basant Agarwal, Garvit Chouhan, Nitin Bania, and Prateek Pareek. 2013. Sentiment analysis of hindi review based on negation and discourse relation. In proceedings of International Joint Conference on Natural Language Processing, pages 45--50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashyap Popat</author>
<author>A R Balamurali</author>
<author>Pushpak Bhattacharyya</author>
<author>Gholamreza Haffari</author>
</authors>
<title>The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>412--422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5248" citStr="Popat et al., 2013" startWordPosition="807" endWordPosition="810">e expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to perform well(Balamurali et al., 2012). (Balamurali et al., 2012) present an alternative approach to Cross Lingual Sentiment Analysis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained on sense marked and polarity labeled corpora in Resource rich language. The crux of the idea was to use the linked WordNets of two languages to bridge the language gap. Recently, (Popat et al., 2013) describes a Cross Lingual Clustering based SA System. In this approach, features were generated using syntagmatic property based word clusters created from unlabeled monolingual corpora, thereby eliminating the need for Bilingual Dictionaries. These features were then used to train a linear SVM to predict positive or negative polarity on a tourism review dataset. 2.2 Autoencoders in NLP Tasks Autoencoders are neural networks that learn a low dimensional vector representation of fixed-size inputs such as image segments or bag-of-word representations of documents. They can be used to efficientl</context>
<context position="21286" citStr="Popat et al., 2013" startWordPosition="3453" endWordPosition="3456">of Reviews obtained in ratings 1-4 are [0.20, 0.25, 0.35, 0.20] respectively. Average length of reviews is 84 words. For +ve/-ve polarity based system, we group the reviews with ratings {1, 2} as negative and {3, 4} as positive. 5.3 Experimental Settings We used following Baselines for Sentiment Analysis in Hindi: Majority class: Assign the most frequent class in the training set (Rating:3 / Polarity:+ve) Bag-of-words: Softmax regression on Binary Bag-of-words We also compare our system with state of the art Monolingual and Cross Lingual System for Sentiment Analysis in Hindi as described by (Popat et al., 2013) using the same experimental setup. The best systems in each category given by them are as below: WordNet Based: Using Hindi-SentiWordNet3, each word in a review was mapped to corresponding synset identifiers. These identifiers were used as features for creating sentiment classifiers based on Binary/Multiclass SVM trained on bag ofwords representation using libSVM library. Cross Lingual (XL) Clustering Based: Here, joint clustering was performed on unlabeled bilingual corpora which maximizes the joint likelihood of monolingual and cross-lingual factors.. For details, please refer the work of (</context>
</contexts>
<marker>Popat, Balamurali, Bhattacharyya, Haffari, 2013</marker>
<rawString>Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, and Gholamreza Haffari. 2013. The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 412--422. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6164" citStr="Socher et al., 2011" startWordPosition="942" endWordPosition="945"> to predict positive or negative polarity on a tourism review dataset. 2.2 Autoencoders in NLP Tasks Autoencoders are neural networks that learn a low dimensional vector representation of fixed-size inputs such as image segments or bag-of-word representations of documents. They can be used to efficiently learn feature encodings that are useful for classification. The Autoencoders were first applied in a recursive setting by Pollack (1990) in recursive auto-associative memories (RAAMs). However, RAAMs needed fixed recursive data structures to learn vector representations, whereas RAE given by (Socher et al., 2011) builds recursive data structure using a greedy algorithm. The RAE can be pre-trained with an unsupervised algorithm and then fine-tuned according to the label of the phrase, such as the syntactic category in parsing(Socher et al., 2013), the polarity in sentiment analysis, etc. The learned structures are not necessarily syntactically accurate but can capture more of the semantic information in the word vectors. 3 BRAE Framework (Zhang et al., 2014) used the RAE along with a Bilingually Constrained Model to simultaneously learn phrase embeddings for two languages in semantic vector space. The </context>
<context position="9321" citStr="Socher et al., 2011" startWordPosition="1471" endWordPosition="1474">used again to compute y2 by setting the children to be [c1; c2] = [y1; x3]. The same auto-encoder is re-used until the vector of the whole phrase is generated. For unsupervised phrase embedding, the sum of reconstruction errors at each node in binary tree y is minimized: ∑Erec(x; B) = arg miny∈A(x) Erec([c1; c2]k) k∈y (3) Where A(x) denotes all the possible binary trees that can be built from inputs x. A greedy algorithm is used to generate the optimal binary tree y∗. The parameters Brec = (B(1), B(2)) are optimized over all the phrases in the training data. For further details, please refer (Socher et al., 2011) 3.2 Semantic Error The BRAE model jointly learns two RAEs for source language LS and target language LT. Each RAE learn semantic vector representation ps and pt of phrases s and t respectively in translationequivalent phrase pair (s, t) in bilingual corpora (shown in Fig.1). The transformation between the two is defined by: p′t = f(Wtsps + bts), p′s = f(Wts pt + bst) (4) where Bts = (Wts, bts), Bst = (Wts, bst) are new parameters introduced. The semantic error between learned vector representations ps and pt is calculated as : Esem(s, t; B) = E∗sem(t|s; Bst) + E∗sem(s|t; Bts) (5) where E∗sem(</context>
<context position="22313" citStr="Socher et al., 2011" startWordPosition="3602" endWordPosition="3605">ased: Here, joint clustering was performed on unlabeled bilingual corpora which maximizes the joint likelihood of monolingual and cross-lingual factors.. For details, please refer the work of (Popat et al., 2013). 2http://hindi.webdunia.com/bollywood-movie-review/ 3http://www.cfilt.iitb.ac.in/ Each word in a review was then mapped to its cluster identifier and used as features in an SVM. Our approaches Basic RAE: We use the Semi-Supervised RAE based classification where we first trained a standard RAE using Hindi monolingual corpora, then applied supervised training procedure as described in (Socher et al., 2011). This approach doesn&apos;t use bilingual corpora, but is dependent on amount of labeled data in Hindi. BRAE-U: We neither include penalty term, nor fix the transformations weights in our proposed system. BRAE-P: We only include the penalty term but allow the transformation weights to be modified in proposed system. BRAE-F: We add the penalty term and fix the transformation weights during back propagation in proposed system. 5.4 Experimental Setup We combined the text data from all English Datasets (English Gigaword + HindEnCorp English Portion + IBMD11 + Scale Dataset) described above to train th</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151--161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="6401" citStr="Socher et al., 2013" startWordPosition="981" endWordPosition="985">f-word representations of documents. They can be used to efficiently learn feature encodings that are useful for classification. The Autoencoders were first applied in a recursive setting by Pollack (1990) in recursive auto-associative memories (RAAMs). However, RAAMs needed fixed recursive data structures to learn vector representations, whereas RAE given by (Socher et al., 2011) builds recursive data structure using a greedy algorithm. The RAE can be pre-trained with an unsupervised algorithm and then fine-tuned according to the label of the phrase, such as the syntactic category in parsing(Socher et al., 2013), the polarity in sentiment analysis, etc. The learned structures are not necessarily syntactically accurate but can capture more of the semantic information in the word vectors. 3 BRAE Framework (Zhang et al., 2014) used the RAE along with a Bilingually Constrained Model to simultaneously learn phrase embeddings for two languages in semantic vector space. The core idea behind BRAE is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, non-translation pairs should have differ</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wan</author>
<author>Rong Pan</author>
<author>Jiefei Li</author>
</authors>
<title>Biweighting domain adaptation for cross-language text classification.</title>
<date>2011</date>
<booktitle>In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,</booktitle>
<volume>22</volume>
<pages>1535</pages>
<contexts>
<context position="4449" citStr="Wan et al., 2011" startWordPosition="676" endWordPosition="679">2015 Association for Computational Linguistics. ing positive/negative polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to achieve better accuracies. For Languages where labeled data is not present, approaches based on cross-lingual sentiment analysis are used. Usually, such methods need intermediary machine translation system (Wan et al., 2011; Brooke et al., 2009) or abilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways in which sentiments can be expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to perform well(Balamurali et al., 2012). (Balamurali et al., 2012) present an alternative approach to Cross Lingual Sentiment Analysis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained </context>
</contexts>
<marker>Wan, Pan, Li, 2011</marker>
<rawString>Chang Wan, Rong Pan, and Jiefei Li. 2011. Biweighting domain adaptation for cross-language text classification. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 1535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>111--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2453" citStr="Zhang et al., 2014" startWordPosition="380" endWordPosition="383">eering Indian Institute of technology, Delhi DL, India shashankg@gmail.com One solution is to create cross lingual tools between a resource rich and resource poor language that exploit large amounts of unlabeled data and sentence aligned corpora that are widely available on web through bilingual newspapers, magazines, etc. Many different approaches have been identified to perform Cross Lingual Tasks but they depend on the presence of MT-System or Bilingual Dictionaries between the source and target language. In this paper, we use Bilingually Constrained Recursive Auto-encoder (BRAE) given by (Zhang et al., 2014) to perform Cross Lingual sentiment analysis. Major Contributions of this paper are as follows: First, We develop a new Rating scale based Movie Review Dataset for Hindi. Second, a general framework to perform Cross Lingual Classification tasks is developed by modifying the architecture and training procedure for BRAE model. This model exploits the fact that phrases in two languages, that share same semantic meaning, can be used to learn language independent semantic vector representations. These embeddings can further be fine-tuned using labeled dataset in English to capture enough class info</context>
<context position="6617" citStr="Zhang et al., 2014" startWordPosition="1016" endWordPosition="1019">e auto-associative memories (RAAMs). However, RAAMs needed fixed recursive data structures to learn vector representations, whereas RAE given by (Socher et al., 2011) builds recursive data structure using a greedy algorithm. The RAE can be pre-trained with an unsupervised algorithm and then fine-tuned according to the label of the phrase, such as the syntactic category in parsing(Socher et al., 2013), the polarity in sentiment analysis, etc. The learned structures are not necessarily syntactically accurate but can capture more of the semantic information in the word vectors. 3 BRAE Framework (Zhang et al., 2014) used the RAE along with a Bilingually Constrained Model to simultaneously learn phrase embeddings for two languages in semantic vector space. The core idea behind BRAE is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, non-translation pairs should have different semantic meanings, and this information can also be used to guide learning semantic phrase embeddings. In this method, a standard recursive autoencoder (RAE) pre-trains the phrase embedding with an unsupervised a</context>
<context position="11542" citStr="Zhang et al., 2014" startWordPosition="1861" endWordPosition="1864"> t ) and Bs = (Bt s,Brec s ) optimized independently as long as the phrase representation of other side is given to compute semantic error. The final BRAE objective over the phrase pairs training set (5, T) becomes: 1 ∑ JBRAE = N E(s, t; B) + 2 11B112 �BRAE (s,t)∈(S,T ) 3.4 Unsupervised Training of BRAE The word embedding matrices Ls and Lt are pretrained using unlabeled monolingual data with Word2Vec toolkit (Mikolov et al., 2013). All other parameters are initialized randomly. We use SGD algorithm for parameter optimization. For full gradient calculations for each parameter set, please see (Zhang et al., 2014). 1. RAE Training Phase: Apply RAE Framework (Sec. 3.1) to pre-train the source and target phrase representations ps and pt respectively by optimizing Brec sand Brec t using unlabeled monolingual datasets. 2. Cross-Training Phase: Use target-side phrase representation pt to update the source-side Zff 3TUW to It was good PS Pt transformations on each side respectively can be 161 Reconstruction Cross-Entropy Reconstruction parameters θs and obtain source-side phrase representation p′s, and vice-versa for ps. Calculate the joint error over the bilingual training corpus. On reaching a local minima</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 111--121. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
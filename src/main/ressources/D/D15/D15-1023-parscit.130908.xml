<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98118">
On a Strictly Convex IBM Model 1
</title>
<author confidence="0.993492">
Andrei Simion
</author>
<affiliation confidence="0.99674">
Columbia University
</affiliation>
<address confidence="0.987455">
New York, NY, 10027
</address>
<email confidence="0.997592">
aas2148@columbia.edu
</email>
<author confidence="0.934372">
Michael Collins∗
</author>
<affiliation confidence="0.971226">
Columbia University
Computer Science
</affiliation>
<address confidence="0.991465">
New York, NY, 10027
</address>
<email confidence="0.998903">
mc3354@columbia.edu
</email>
<author confidence="0.991212">
Clifford Stein
</author>
<affiliation confidence="0.996287">
Columbia University
</affiliation>
<address confidence="0.848284">
IEOR Department
New York, NY, 10027
</address>
<email confidence="0.999181">
cs2035@columbia.edu
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932227272727">
IBM Model 1 is a classical alignment
model. Of the first generation word-based
SMT models, it was the only such model
with a concave objective function. For
concave optimization problems like IBM
Model 1, we have guarantees on the con-
vergence of optimization algorithms such
as Expectation Maximization (EM). How-
ever, as was pointed out recently, the ob-
jective of IBM Model 1 is not strictly con-
cave and there is quite a bit of alignment
quality variance within the optimal solu-
tion set. In this work we detail a strictly
concave version of IBM Model 1 whose
EM algorithm is a simple modification of
the original EM algorithm of Model 1 and
does not require the tuning of a learning
rate or the insertion of an 12 penalty. More-
over, by addressing Model 1’s shortcom-
ings, we achieve AER and F-Measure im-
provements over the classical Model 1 by
over 30%.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.942044709677419">
The IBM translation models were introduced in
(Brown et al., 1993) and were the first-generation
Statistical Machine Translation (SMT) systems.
In the current pipeline, these word-based models
are the seeds for more sophisticated models which
need alignment tableaus to start their optimization
procedure. Among the original IBM Models, only
IBM Model 1 can be formulated as a concave opti-
mization problem. Recently, there has been some
research on IBM Model 2 which addresses either
the model’s non-concavity (Simion et al., 2015)
∗Currently on leave at Google Inc. New York.
or over parametrization (Dyer et al., 2013). We
make the following contributions in this paper:
• We utilize and expand the mechanism intro-
duced in (Simion et al., 2015) to construct
strictly concave versions of IBM Model 11.
As was shown in (Toutanova and Galley,
2011), IBM Model 1 is not a strictly con-
cave optimization problem. What this means
in practice is that although we can initialize
the model with random parameters and get to
the same objective cost via the EM algorithm,
there is quite a bit of alignment quality vari-
ance within the model’s optimal solution set
and ambiguity persists on which optimal so-
lution truly is the best. Typically, the easiest
way to make a concave model strictly con-
cave is to append an 12 regularizer. However,
this method does not allow for seamless EM
training: we have to either use a learning-rate
</bodyText>
<listItem confidence="0.938683071428571">
dependent gradient based algorithm directly
or use a gradient method within the M step of
EM training. In this paper we show how to
get via a simple technique an infinite supply
of models that still allows a straightforward
application of the EM algorithm.
• As a concrete application of the above, we
detail a very simple strictly concave version
of IBM Model 1 and study the performance
of different members within this class. Our
strictly concave models combine some of the
elements of word association and positional
dependance as in IBM Model 2 to yield a sig-
nificant model improvement. Furthermore,
</listItem>
<footnote confidence="0.997075666666667">
1Please refer as needed to the Appendix for examples
and definitions of convexity/concavity and strict convex-
ity/concavity.
</footnote>
<page confidence="0.935905">
221
</page>
<note confidence="0.6859675">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 221–226,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.7777095">
we now have guarantees that the solution we
find is unique.
</bodyText>
<listItem confidence="0.862084">
• We detail an EM algorithm for a subclass of
</listItem>
<bodyText confidence="0.989734777777778">
strictly concave IBM Model 1 variants. The
EM algorithm is a small change to the orig-
inal EM algorithm introduced in (Brown et
al., 1993).
Notation. Throughout this paper, for any posi-
tive integer N, we use [N] to denote 11 ... N} and
[N]0 to denote 10 ... N}. We denote by Rn+ the set
of nonnegative n dimensional vectors. We denote
by [0, 1]n the n−dimensional unit cube.
</bodyText>
<sectionHeader confidence="0.969538" genericHeader="method">
2 IBM Model 1
</sectionHeader>
<bodyText confidence="0.999850307692308">
We begin by reviewing IBM Model 1 and in-
troducing the necessary notation. To this end,
throughout this section and the remainder of the
paper we assume that our set of training exam-
ples is (e(k), f(k)) for k = 1... n, where e(k)
is the k’th English sentence and f(k) is the k’th
French sentence. Following standard convention,
we assume the task is to translate from French (the
“source” language) into English (the “target” lan-
guage). We use E to denote the English vocabu-
lary (set of possible English words), and F to de-
note the French vocabulary. The k’th English sen-
tence is a sequence of words e(k)
</bodyText>
<equation confidence="0.784701285714286">
1 ... e(k)
lk where lk
is the length of the k’th English sentence, and each
ei E E; similarly the k’th French sentence is a
(k)
sequence f(k) 1 . . .f(k) where each f�k) E F. We
mk
</equation>
<bodyText confidence="0.954013789473684">
define e(k)
0 for k = 1... n to be a special NULL
word (note that E contains the NULL word).
For each English word e E E, we will assume
that D(e) is a dictionary specifying the set of pos-
sible French words that can be translations of e.
The set D(e) is a subset of F. In practice, D(e)
can be derived in various ways; in our experiments
we simply define D(e) to include all French words
f such that e and f are seen in a translation pair.
Given these definitions, the IBM Model 1 opti-
mization problem is given in Fig. 1 and, for exam-
ple, (Koehn, 2008). The parameters in this prob-
lem are t(f|e). The t(f|e) parameters are transla-
tion parameters specifying the probability of En-
glish word e being translated as French word f.
The objective function is then the log-likelihood
of the training data (see Eq. 3):
where log p(f(k)
</bodyText>
<equation confidence="0.9733258">
j |e(k)) is
�lk t(f(k)|e(k))
j i
1 + lk
and C is a constant that can be ignored.
</equation>
<figureCaption confidence="0.973628">
Figure 1: The IBM Model 1 Optimization Prob-
lem.
</figureCaption>
<bodyText confidence="0.984234538461539">
While IBM Model 1 is concave optimization
problem, it is not strictly concave (Toutanova and
Galley, 2011). Therefore, optimization methods
for IBM Model 1 (specifically, the EM algorithm)
are typically only guaranteed to reach a global
maximum of the objective function (see the Ap-
pendix for a simple example contrasting convex
and strictly convex functions). In particular, al-
though the objective cost is the same for any op-
timal solution, the translation quality of the so-
lutions is not fixed and will still depend on the
initialization of the model (Toutanova and Galley,
2011).
</bodyText>
<sectionHeader confidence="0.948676" genericHeader="method">
3 A Strictly Concave IBM Model 1
</sectionHeader>
<bodyText confidence="0.999602666666667">
We now detail a very simple method to make IBM
Model 1 strictly concave with a unique optimal so-
lution without the need for appending an l2 loss.
</bodyText>
<construct confidence="0.9803415">
Theorem 1. Consider IBM Model 1 and modify
its objective to be
</construct>
<bodyText confidence="0.560422">
Input: Define E, F, L, M, (e(k), f(k), lk, mk) for
k = 1... n, D(e) for e E E as in Section 2.
</bodyText>
<figure confidence="0.592155733333333">
Parameters:
• A parameter t(f|e) for each e E E, f E D(e).
Constraints:
beEE,f E D(e), t(f|e) &gt; 0 (1)
be E E, � t(f|e) = 1 (2)
f∈D(e)
Objective: Maximize
t(f(k)
j |e(k)
i ) (3)
with respect to the t(f|e) parameters.
lk
1
n
log
�n
k=1
�
i=0
�mk
j=1
log
i=0
lk
= C + log � t(f(k)
i=0 j |e(k)
i ) ,
1 �n �mk log p(f(k) j|e(k)) , 1 �n �mk log l k hi,j,k(t(fjk)|e(k)i)) (4)
n k=1 j=1 n k=1 j=1 �
i=0
</figure>
<page confidence="0.548882">
222
</page>
<equation confidence="0.98692375">
θhi,j,k(t(f(k)
j |e(k)
i )) + (1 − θ)hi,j,k(t�(f(k)
j |e(k)
i ))
l k
log
�
i=0
m k
�
j=1
</equation>
<bodyText confidence="0.997131833333333">
where hi,j,k : R+ —* R+ is strictly concave. With
the new objective and the same constraints as IBM
Model 1, this new optimization problem is strictly
concave.
Proof. To prove concavity, we now show that the
new likelihood function
</bodyText>
<equation confidence="0.995865666666667">
1 n mk log l k hi,j,k(t(f(k)
L(t) = � � � j |e(k)
n k=1 j=1 i=0 i )) ,
</equation>
<bodyText confidence="0.990956166666667">
is strictly concave (concavity follows in the same
way trivially). Suppose by way of contradiction
that there is (t) =� (t&apos;) and 0 E (0, 1) such
that equality hold for Jensen’s inequality. Since
hi,j,k is strictly concave and (t) =� (t&apos;) we must
have that there must be some (k, j, i) such that
</bodyText>
<equation confidence="0.996307777777778">
t(f(k)j|e(k)) � t&apos;(f(k)|e(k)) so that Jensen’s in
i j i
equality is strict for hi,j,k and we have
hi,j,k(et (fj(k)|e (k)i) + (1 − e)t�(fjk)|e (k)i))
&gt; ehi,j,k(t(f(k)
j |e(k)
i )) + (1 − e)hi,j,k(t�(f(k)
j |e(k)
i ))
</equation>
<bodyText confidence="0.822088">
Using Jensen’s inequality, the monotonicity of the
log, and the above strict inequality we have
</bodyText>
<equation confidence="0.998697">
L(θt + (1 − θ)t�)
�n
&gt;
k=1
hi,j,k(t(f(k)
j |e(k)
i ))
</equation>
<bodyText confidence="0.934132428571429">
The IBM Model 1 strictly concave optimiza-
tion problem is presented in Fig. 2. In (7) it is
crucial that each hi,j,k be strictly concave within
�lk i=0 hi,j,k(t(f(k)
j |e(k)
i )). For example, we have
that ,/x1 + x2 is concave but not strictly concave
and the proof of Theorem 1 would break down. To
see this, we can consider (x1, x2) =� (x1, x3) and
note that equality holds in Jensen’s inequality. We
should be clear: the main reason why Theorem 1
works is that we have hi,j,k are strictly concave (on
R+) and all the lexical probabilities that are argu-
ments to L are present within the log-likelihood.
</bodyText>
<figureCaption confidence="0.9810265">
Figure 2: The IBM Model 1 strictly concave opti-
mization problem.
</figureCaption>
<sectionHeader confidence="0.956381" genericHeader="method">
4 Parameter Estimation via EM
</sectionHeader>
<bodyText confidence="0.999864666666667">
For the IBM Model 1 strictly concave optimization
problem, we can derive a clean EM Algorithm if
we base our relaxation of
</bodyText>
<equation confidence="0.97715175">
hi,j,k(t(f(k)j|e(k)
i = α(e�k), fjk))(t(fjk)|e(k)i))β(e.k),fjk))
with Q(e(k)
i , f(k)
</equation>
<bodyText confidence="0.9053465">
j ) &lt; 1. To justify this, we first
need the following:
</bodyText>
<equation confidence="0.823401666666667">
Lemma 1. Consider h : R+ —* R+ given by
h(x) = xβ where Q E (0, 1). Then h is strictly
concave.
</equation>
<bodyText confidence="0.9568725">
Proof. The proof of this lemma is elementary
and follows since the second derivative given by
</bodyText>
<equation confidence="0.704758">
h��(x) = Q(Q − 1)xβ−2 is strictly negative.
</equation>
<bodyText confidence="0.956658666666667">
For our concrete experiments, we picked a
model based on Lemma 1 and used h(x) = αxβ
with α, Q E (0, 1) so that
</bodyText>
<equation confidence="0.995546">
hi,j,k(t(fjk)  |e{k))) = α(fjk), e{k))(t(fjk)|e{k)))β(4(k),e{k)) .
</equation>
<bodyText confidence="0.999881857142857">
Using this setup, parameter estimation for the new
model can be accomplished via a slight modifica-
tion of the EM algorithm for IBM Model 1. In
particular, we have that the posterior probabilities
of this model factor just as those of the standard
Model 1 and we have an M step that requires opti-
mizing
</bodyText>
<equation confidence="0.972737764705882">
� q(a(k)|e(k), f(k)) log p(f(k), a(k)|e(k))
a(k)
hi,j,k(θt(f(k)
j |e(k)
i ) + (1 − θ)t�(f(k)
j|e(k)
i ))
l k
=
log
�
i=0
mk�
j=1
n
�
k=1
</equation>
<bodyText confidence="0.924806">
Input: Define E, F, L, M, (e(k), f(k), lk, mk) for
k = 1... n, D(e) for e E E as in Section 2. A set
of strictly concave functions hi,j,k : R+ —* R+.
</bodyText>
<figure confidence="0.9674793">
Parameters:
• A parameter t(f|e) for each e E E, f E D(e).
Constraints:
be E E, f E D(e), t(f|e) &gt; 0 (5)
be E E, � t(f|e) = 1 (6)
fED(e)
Objective: Maximize
with respect to the t(f|e) parameters.
hi,j,k(t(f(k)
j |e(k)
</figure>
<equation confidence="0.70827024">
i )) (7)
l k
1
n
log
�
i=0
�mk
j=1
�n
k=1
l k
log
&gt; θ
�
i=0
m k
�
j=1
�n
k=1
+ (1 − θ) �n �m k log l k hi,j,k(t�(f(k)
k=1 j=1 � j |e(k)
i=0 i ))
= θL(t) + (1 − θ)L(t�)
</equation>
<page confidence="0.996651">
223
</page>
<listItem confidence="0.996441133333333">
1: Input: Define E, F, L, M, (e(k), f(k), lk, mk) for k = 1 ... n,
D(e) for e ∈ E as in Section 2. An integer T specifying the number of
passes over the data. A set of weighting parameter α(e, f), 0(e, f) ∈
(0, 1) for each e ∈ E, f ∈ D(e). A tuning parameter A &gt; 0.
2: Parameters:
• A parameter t(f|e) for each e ∈ E, f ∈ D(e).
3: Initialization:
• ∀e ∈ E, f ∈ D(e), set t(f|e) = 1/|D(e)|.
4: EM Algorithm:
5: for all t = 1 ... T do
6: ∀e ∈ E, f ∈ D(e), count(f, e) = 0
7: ∀e ∈ E, count(e) = 0
8: EM Algorithm: Expectation
9: for all k = 1 ... n do
10: for all j = 1 ... mk do
</listItem>
<equation confidence="0.985317142857143">
11: b1[i] = 0 ∀i ∈ [lk]0
12: Δ1 = 0
13: for all i = 0 ... lk do
k k
14: b1[i] = α(f(k),e(k))(t(f(k)e(k)i))β(fj ei )
j • j
15: Δ1 += b1[i]
16: for all i = 0 ... lk do
17: b1[i] = δ1[i]
p Δ1
18: count(f(k),e(k)) += 0(f(k), e(k)i)b1[i]
j * j
19: count(e(k)
i ) += 0(fjk), e(k))b1[i]
</equation>
<listItem confidence="0.925985666666667">
20: EM Algorithm: Maximization
21: for all e ∈ Edo
22: for all f ∈ D(e) do
23: t(f|e) = count(e,f)
count(e)
24: Output: t parameters
</listItem>
<figureCaption confidence="0.932099333333333">
Figure 3: Pseudocode for T iterations of the EM
Algorithm for the IBM Model 1 strictly concave
optimization problem.
</figureCaption>
<bodyText confidence="0.993975">
where
</bodyText>
<equation confidence="0.982651666666667">
h))
a(k)
j ,j,k(t(f(k)
j |e(k)
a(k)
j
</equation>
<bodyText confidence="0.964618625">
are constants gotten in the E step. This optimiza-
tion step is very similar to the regular Model 1 M
step since the Q drops down using log tβ = Q log t;
the exact same count-based method can be ap-
plied. The details of this algorithm are in Fig. 3.
5 Choosing α and β
The performance of our new model will rely heav-
ily on the choice of α(e(k)
</bodyText>
<equation confidence="0.9515285">
i , f(k)
j ), Q(e(k)
</equation>
<bodyText confidence="0.857719142857143">
i , f(k)
j ) E
(0, 1) we use. In particular, we could make Q de-
pend on the association between the words, or the
words’ positions, or both. One classical measure
of word association is the dice coefficient (Och
and Ney, 2003) given by
</bodyText>
<equation confidence="0.879674">
c(e) + c(f) .
</equation>
<bodyText confidence="0.988105333333333">
In the above, the count terms c are the number
of training sentences that have either a particular
word or a pair of of words (e, f). As with the other
choices we explore, the dice coefficient is a frac-
tion between 0 and 1, with 0 and 1 implying less
and more association, respectively. Additionally,
we make use of positional constants like those of
the IBM Model 2 distortions given by
In the above, Z(j,l, m) is the partition func-
tion discussed in (Dyer et al., 2013). The previ-
ous measures all lead to potential candidates for
Q(e, f), we have t(f|e) E (0, 1), and we want to
enlarge competing values when decoding (we use
αtβ instead of t when getting the Viterbi align-
ment). The above then implies that we will have
the word association measures inversely propor-
tional to Q, and so we set Q(e, f) = 1− dice(e, f)
or Q(e, f) = 1 − d(i|j,l, m). In our experiments
we picked α(f(k)
j , e(k)
i ) = d(i|j, lk, mk) or 1; we
hold A to a constant of either 16 or 0 and do not
estimate this variable (A = 16 can be chosen by
cross validation on a small trial data set).
</bodyText>
<sectionHeader confidence="0.998015" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998904">
6.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9999728">
For our alignment experiments, we used a subset
of the Canadian Hansards bilingual corpus with
247,878 English-French sentence pairs as training
data, 37 sentences of development data, and 447
sentences of test data (Michalcea and Pederson,
2003). As a second validation corpus, we con-
sidered a training set of 48,706 Romanian-English
sentence-pairs, a development set of 17 sentence
pairs, and a test set of 248 sentence pairs (Michal-
cea and Pederson, 2003).
</bodyText>
<subsectionHeader confidence="0.998713">
6.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999750428571429">
Below we report results in both AER (lower is
better) and F-Measure (higher is better) (Och and
Ney, 2003) for the English —* French translation
direction. To declare a better model we have to
settle on an alignment measure. Although the
relationship between AER/F-Measure and trans-
lation quality varies (Dyer et al., 2013), there
are some positive experiments (Fraser and Marcu,
2004) showing that F-Measure may be more use-
ful, so perhaps a comparison based on F-Measure
is ideal.
Table 1 contains our results for the Hansards
data. For the smaller Romanian data, we obtained
similar behavior, but we leave out these results due
</bodyText>
<equation confidence="0.996818">
mk
q(a(k)|e(k), f(k)) a H
j=1
2c(e, f)
dice(e, f) =
{ (l+1)Z(j,l,m)
le−λ |l i − jm I : i � 0
(l+1)Z(j,l,m)
d(i|j,l,m) =
1 : i = 0
</equation>
<page confidence="0.997094">
224
</page>
<table confidence="0.99989204">
(α, Q) (1, 1) (d, 1) (1,1 − dice) (1,1 − d) (d,1 − d)
Iteration AER
0 0.8716 0.6750 0.6240 0.6597 0.5570
1 0.4426 0.2917 0.4533 0.2738 0.3695
2 0.3383 0.2323 0.4028 0.2318 0.3085
3 0.3241 0.2190 0.3845 0.2252 0.2881
4 0.3191 0.2141 0.3751 0.2228 0.2833
5 0.3175 0.2118 0.3590 0.2229 0.2812
6 0.3160 0.2093 0.3566 0.2231 0.2793
7 0.3203 0.2090 0.3555 0.2236 0.2783
8 0.3198 0.2075 0.3546 0.2276 0.2777
9 0.3198 0.2066 0.3535 0.2323 0.2769
10 0.3177 0.2065 0.3531 0.2352 0.2769
Iteration F-Measure
0 0.0427 0.1451 0.2916 0.1897 0.2561
1 0.4213 0.5129 0.4401 0.5453 0.4427
2 0.5263 0.5726 0.4851 0.5940 0.5014
3 0.5413 0.5852 0.5022 0.6047 0.5199
4 0.5480 0.5909 0.5111 0.6085 0.5255
5 0.5500 0.5939 0.5264 0.6101 0.5273
6 0.5505 0.5959 0.5282 0.6101 0.5286
7 0.5449 0.5965 0.5298 0.6096 0.5296
8 0.5456 0.5977 0.5307 0.6068 0.5300
9 0.5451 0.5985 0.5318 0.6040 0.5309
10 0.5468 0.5984 0.5322 0.6024 0.5311
</table>
<tableCaption confidence="0.783499">
Table 1: Results on the English-French data for
various (α, Q) settings as discussed in Section 5.
</tableCaption>
<bodyText confidence="0.999412">
For the d parameters, we use A = 16 throughout.
The standard IBM Model 1 is column 1 and cor-
responds to a setting of (1,1). The not necessarily
strictly concave model with (d,1) setting gives the
best AER, while the strictly concave model given
by the (1, 1−d) setting has the highest F-Measure.
to space limitations. Our experiments show that
using
</bodyText>
<equation confidence="0.95934625">
hi,j,k(t(f(k)
j |e(k)
i )) = (t(f(k)
j |e(k)
</equation>
<bodyText confidence="0.969288923076923">
i ))1−d(i|j,lk,mk)
yields the best F-Measure performance and is not
far off in AER from the “fake”2 IBM Model 2
(gotten by setting (α,Q) = (d, 1)) whose results
are in column 2 (the reason why we use this model
at all is since it should be better than IBM 1: we
want to know how far off we are from this obvi-
ous improvement). Moreover, we note that dice
does not lead to quality Q exponents and that, un-
fortunately, combining methods as in column 5
((α, Q) = (d,1 − d)) does not necessarily lead
to additive gains in AER and F-Measure perfor-
mance.
</bodyText>
<footnote confidence="0.863848">
2Generally speaking, when using
</footnote>
<equation confidence="0.949078">
hi,j,k(t(f(k)
j |e(k)
i )) = d(i|j, lk, mk)t(f(k)
j |ei
k))
</equation>
<bodyText confidence="0.972783">
with d constant we cannot use Theorem 3 since h is linear.
Most likely, the strict concavity of the model will hold be-
cause of the asymmetry introduced by the d term; however,
there will be a necessary dependency on the data set.
</bodyText>
<sectionHeader confidence="0.907535" genericHeader="method">
7 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.999987103448276">
In this section we take a moment to also compare
our work with the classical IBM 1 work of (Moore,
2004). Summarizing (Moore, 2004), we note that
this work improves substancially upon the classi-
cal IBM Model 1 by introducing a set of heuris-
tics, among which are to (1) modify the lexical
parameter dictionaries (2) introduce an initializa-
tion heuristic (3) modify the standard IBM 1 EM
algorithm by introducing smoothing (4) tune ad-
ditional parameters. However, we stress that the
main concern of this work is not just heuristic-
based empirical improvement, but also structured
learning. In particular, although using an regular-
izer l2 and the methods of (Moore, 2004) would
yield a strictly concave version of IBM 1 as well
(with improvements), it is not at all obvious how
to choose the learning rate or set the penalty on
the lexical parameters. The goal of our work was
to offer a new, alternate form of regularization.
Moreover, since we are changing the original log-
likelihood, our method can be thought of as way
of bringing the l2 regularizer inside the log like-
lihood. Like (Moore, 2004), we also achieve ap-
preciable gains but have just one tuning parame-
ter (when Q = 1 − d we just have the centering
A parameter) and do not break the probabilistic in-
terpretation any more than appending a regularizer
would (our method modifies the log-likelihood but
the simplex constrains remain).
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999748833333333">
In this paper we showed how IBM Model 1 can
be made into a strictly convex optimization prob-
lem via functional composition. We looked at a
specific member within the studied optimization
family that allows for an easy EM algorithm. Fi-
nally, we conducted experiments showing how the
model performs on some standard data sets and
empirically showed 30% important over the stan-
dard IBM Model 1 algorithm. For further re-
search, we note that picking the optimal hi,j,k is
an open question, so provably finding and justify-
ing the choice is one topic of interest.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.845833333333333">
Andrei Simion was supported by a Google re-
search award. Cliff Stein is supported in part by
NSF grants CCF-1349602 and CCF-1421161.
</bodyText>
<page confidence="0.997916">
225
</page>
<sectionHeader confidence="0.995887" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895468085107">
Steven Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert. L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19:263-311.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood From Incomplete Data via the
EM Algorithm. Journal of the royal statistical soci-
ety, series B, 39(1):1-38.
Chris Dyer, Victor Chahuneau, Noah A. Smith. 2013.
A Simple, Fast, and Effective Reparameterization of
IBM Model 2. In Proceedings of NAACL.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing Word Alignment Quality for Statistical Ma-
chine Translation. Journal Computational Linguis-
tics, 33(3): 293-303.
Phillip Koehn. 2008. Statistical Machine Translation.
Cambridge University Press.
Rada Michalcea and Ted Pederson. 2003. An Eval-
uation Exercise in Word Alignment. HLT-NAACL
2003: Workshop in building and using Parallel
Texts: Data Driven Machine Translation and Be-
yond.
Robert C. Moore. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of the ACL.
Stephan Vogel, Hermann Ney and Christoph Tillman.
1996. HMM-Based Word Alignment in Statistical
Translation. In Proceedings of COLING.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational-Linguistics, 29(1): 19-52.
Andrei Simion, Michael Collins and Cliff Stein. 2013.
A Convex Alternative to IBM Model 2. In Proceed-
ings of EMNLP.
Andrei Simion, Michael Collins and Cliff Stein. 2015.
A Family of Latent Variable Convex Relaxations for
IBM Model 2. In Proceedings of the AAAI.
Kristina Toutanova and Michel Galley. 2011. Why Ini-
tialization Matters for IBM Model 1: Multiple Op-
tima and Non-Strict Convexity. In Proceedings of
the ACL.
Ashish Vaswani, Liang Huang and David Chiang.
2012. Smaller Alignment Models for Better Trans-
lations: Unsupervised Word Alignment with the LO-
norm. In Proceedings of the ACL.
</reference>
<page confidence="0.998841">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.006226">
<title confidence="0.994833">On a Strictly Convex IBM Model 1</title>
<author confidence="0.689202">Andrei</author>
<address confidence="0.491074">Columbia</address>
<author confidence="0.663217">New York</author>
<author confidence="0.663217">NY</author>
<email confidence="0.982148">aas2148@columbia.edu</email>
<affiliation confidence="0.807684">Columbia</affiliation>
<title confidence="0.226379">Computer</title>
<author confidence="0.642682">New York</author>
<author confidence="0.642682">NY</author>
<email confidence="0.976981">mc3354@columbia.edu</email>
<affiliation confidence="0.718409">Clifford</affiliation>
<address confidence="0.377796">Columbia</address>
<email confidence="0.391505">IEOR</email>
<author confidence="0.881589">New York</author>
<author confidence="0.881589">NY</author>
<email confidence="0.988801">cs2035@columbia.edu</email>
<abstract confidence="0.976315217391304">IBM Model 1 is a classical alignment Of the generation word-based SMT models, it was the only such model with a concave objective function. For concave optimization problems like IBM Model 1, we have guarantees on the convergence of optimization algorithms such as Expectation Maximization (EM). However, as was pointed out recently, the objective of IBM Model 1 is not strictly concave and there is quite a bit of alignment quality variance within the optimal solution set. In this work we detail a strictly concave version of IBM Model 1 whose algorithm is a simple of the original EM algorithm of Model 1 and does not require the tuning of a learning or the insertion of an Moreover, by addressing Model 1’s shortcomings, we achieve AER and F-Measure improvements over the classical Model 1 by over 30%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Steven Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--263</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19:263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood From Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the royal statistical society, series B,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood From Incomplete Data via the EM Algorithm. Journal of the royal statistical society, series B, 39(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A Simple, Fast, and Effective Reparameterization of IBM Model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1790" citStr="Dyer et al., 2013" startWordPosition="285" endWordPosition="288">e IBM translation models were introduced in (Brown et al., 1993) and were the first-generation Statistical Machine Translation (SMT) systems. In the current pipeline, these word-based models are the seeds for more sophisticated models which need alignment tableaus to start their optimization procedure. Among the original IBM Models, only IBM Model 1 can be formulated as a concave optimization problem. Recently, there has been some research on IBM Model 2 which addresses either the model’s non-concavity (Simion et al., 2015) ∗Currently on leave at Google Inc. New York. or over parametrization (Dyer et al., 2013). We make the following contributions in this paper: • We utilize and expand the mechanism introduced in (Simion et al., 2015) to construct strictly concave versions of IBM Model 11. As was shown in (Toutanova and Galley, 2011), IBM Model 1 is not a strictly concave optimization problem. What this means in practice is that although we can initialize the model with random parameters and get to the same objective cost via the EM algorithm, there is quite a bit of alignment quality variance within the model’s optimal solution set and ambiguity persists on which optimal solution truly is the best.</context>
<context position="12710" citStr="Dyer et al., 2013" startWordPosition="2407" endWordPosition="2410">rds, or the words’ positions, or both. One classical measure of word association is the dice coefficient (Och and Ney, 2003) given by c(e) + c(f) . In the above, the count terms c are the number of training sentences that have either a particular word or a pair of of words (e, f). As with the other choices we explore, the dice coefficient is a fraction between 0 and 1, with 0 and 1 implying less and more association, respectively. Additionally, we make use of positional constants like those of the IBM Model 2 distortions given by In the above, Z(j,l, m) is the partition function discussed in (Dyer et al., 2013). The previous measures all lead to potential candidates for Q(e, f), we have t(f|e) E (0, 1), and we want to enlarge competing values when decoding (we use αtβ instead of t when getting the Viterbi alignment). The above then implies that we will have the word association measures inversely proportional to Q, and so we set Q(e, f) = 1− dice(e, f) or Q(e, f) = 1 − d(i|j,l, m). In our experiments we picked α(f(k) j , e(k) i ) = d(i|j, lk, mk) or 1; we hold A to a constant of either 16 or 0 and do not estimate this variable (A = 16 can be chosen by cross validation on a small trial data set). 6 E</context>
<context position="14134" citStr="Dyer et al., 2013" startWordPosition="2662" endWordPosition="2665">ta, and 447 sentences of test data (Michalcea and Pederson, 2003). As a second validation corpus, we considered a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sentence pairs, and a test set of 248 sentence pairs (Michalcea and Pederson, 2003). 6.2 Methodology Below we report results in both AER (lower is better) and F-Measure (higher is better) (Och and Ney, 2003) for the English —* French translation direction. To declare a better model we have to settle on an alignment measure. Although the relationship between AER/F-Measure and translation quality varies (Dyer et al., 2013), there are some positive experiments (Fraser and Marcu, 2004) showing that F-Measure may be more useful, so perhaps a comparison based on F-Measure is ideal. Table 1 contains our results for the Hansards data. For the smaller Romanian data, we obtained similar behavior, but we leave out these results due mk q(a(k)|e(k), f(k)) a H j=1 2c(e, f) dice(e, f) = { (l+1)Z(j,l,m) le−λ |l i − jm I : i � 0 (l+1)Z(j,l,m) d(i|j,l,m) = 1 : i = 0 224 (α, Q) (1, 1) (d, 1) (1,1 − dice) (1,1 − d) (d,1 − d) Iteration AER 0 0.8716 0.6750 0.6240 0.6597 0.5570 1 0.4426 0.2917 0.4533 0.2738 0.3695 2 0.3383 0.2323 0</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, Noah A. Smith. 2013. A Simple, Fast, and Effective Reparameterization of IBM Model 2. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring Word Alignment Quality for Statistical Machine Translation.</title>
<date>2007</date>
<journal>Journal Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<pages>293--303</pages>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring Word Alignment Quality for Statistical Machine Translation. Journal Computational Linguistics, 33(3): 293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5348" citStr="Koehn, 2008" startWordPosition="928" endWordPosition="929"> a (k) sequence f(k) 1 . . .f(k) where each f�k) E F. We mk define e(k) 0 for k = 1... n to be a special NULL word (note that E contains the NULL word). For each English word e E E, we will assume that D(e) is a dictionary specifying the set of possible French words that can be translations of e. The set D(e) is a subset of F. In practice, D(e) can be derived in various ways; in our experiments we simply define D(e) to include all French words f such that e and f are seen in a translation pair. Given these definitions, the IBM Model 1 optimization problem is given in Fig. 1 and, for example, (Koehn, 2008). The parameters in this problem are t(f|e). The t(f|e) parameters are translation parameters specifying the probability of English word e being translated as French word f. The objective function is then the log-likelihood of the training data (see Eq. 3): where log p(f(k) j |e(k)) is �lk t(f(k)|e(k)) j i 1 + lk and C is a constant that can be ignored. Figure 1: The IBM Model 1 Optimization Problem. While IBM Model 1 is concave optimization problem, it is not strictly concave (Toutanova and Galley, 2011). Therefore, optimization methods for IBM Model 1 (specifically, the EM algorithm) are typ</context>
</contexts>
<marker>Koehn, 2008</marker>
<rawString>Phillip Koehn. 2008. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Michalcea</author>
<author>Ted Pederson</author>
</authors>
<title>An Evaluation Exercise in Word Alignment. HLT-NAACL 2003: Workshop in building and using Parallel Texts: Data Driven Machine Translation and Beyond.</title>
<date>2003</date>
<contexts>
<context position="13581" citStr="Michalcea and Pederson, 2003" startWordPosition="2572" endWordPosition="2575">will have the word association measures inversely proportional to Q, and so we set Q(e, f) = 1− dice(e, f) or Q(e, f) = 1 − d(i|j,l, m). In our experiments we picked α(f(k) j , e(k) i ) = d(i|j, lk, mk) or 1; we hold A to a constant of either 16 or 0 and do not estimate this variable (A = 16 can be chosen by cross validation on a small trial data set). 6 Experiments 6.1 Data Sets For our alignment experiments, we used a subset of the Canadian Hansards bilingual corpus with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (Michalcea and Pederson, 2003). As a second validation corpus, we considered a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sentence pairs, and a test set of 248 sentence pairs (Michalcea and Pederson, 2003). 6.2 Methodology Below we report results in both AER (lower is better) and F-Measure (higher is better) (Och and Ney, 2003) for the English —* French translation direction. To declare a better model we have to settle on an alignment measure. Although the relationship between AER/F-Measure and translation quality varies (Dyer et al., 2013), there are some positive experiments (Fraser a</context>
</contexts>
<marker>Michalcea, Pederson, 2003</marker>
<rawString>Rada Michalcea and Ted Pederson. 2003. An Evaluation Exercise in Word Alignment. HLT-NAACL 2003: Workshop in building and using Parallel Texts: Data Driven Machine Translation and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM WordAlignment Model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="16979" citStr="Moore, 2004" startWordPosition="3177" endWordPosition="3178"> that, unfortunately, combining methods as in column 5 ((α, Q) = (d,1 − d)) does not necessarily lead to additive gains in AER and F-Measure performance. 2Generally speaking, when using hi,j,k(t(f(k) j |e(k) i )) = d(i|j, lk, mk)t(f(k) j |ei k)) with d constant we cannot use Theorem 3 since h is linear. Most likely, the strict concavity of the model will hold because of the asymmetry introduced by the d term; however, there will be a necessary dependency on the data set. 7 Comparison with Previous Work In this section we take a moment to also compare our work with the classical IBM 1 work of (Moore, 2004). Summarizing (Moore, 2004), we note that this work improves substancially upon the classical IBM Model 1 by introducing a set of heuristics, among which are to (1) modify the lexical parameter dictionaries (2) introduce an initialization heuristic (3) modify the standard IBM 1 EM algorithm by introducing smoothing (4) tune additional parameters. However, we stress that the main concern of this work is not just heuristicbased empirical improvement, but also structured learning. In particular, although using an regularizer l2 and the methods of (Moore, 2004) would yield a strictly concave versi</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM WordAlignment Model 1. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillman</author>
</authors>
<title>HMM-Based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Vogel, Ney, Tillman, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney and Christoph Tillman. 1996. HMM-Based Word Alignment in Statistical Translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational-Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12216" citStr="Och and Ney, 2003" startWordPosition="2313" endWordPosition="2316">) j ,j,k(t(f(k) j |e(k) a(k) j are constants gotten in the E step. This optimization step is very similar to the regular Model 1 M step since the Q drops down using log tβ = Q log t; the exact same count-based method can be applied. The details of this algorithm are in Fig. 3. 5 Choosing α and β The performance of our new model will rely heavily on the choice of α(e(k) i , f(k) j ), Q(e(k) i , f(k) j ) E (0, 1) we use. In particular, we could make Q depend on the association between the words, or the words’ positions, or both. One classical measure of word association is the dice coefficient (Och and Ney, 2003) given by c(e) + c(f) . In the above, the count terms c are the number of training sentences that have either a particular word or a pair of of words (e, f). As with the other choices we explore, the dice coefficient is a fraction between 0 and 1, with 0 and 1 implying less and more association, respectively. Additionally, we make use of positional constants like those of the IBM Model 2 distortions given by In the above, Z(j,l, m) is the partition function discussed in (Dyer et al., 2013). The previous measures all lead to potential candidates for Q(e, f), we have t(f|e) E (0, 1), and we want</context>
<context position="13917" citStr="Och and Ney, 2003" startWordPosition="2628" endWordPosition="2631">a set). 6 Experiments 6.1 Data Sets For our alignment experiments, we used a subset of the Canadian Hansards bilingual corpus with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data (Michalcea and Pederson, 2003). As a second validation corpus, we considered a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sentence pairs, and a test set of 248 sentence pairs (Michalcea and Pederson, 2003). 6.2 Methodology Below we report results in both AER (lower is better) and F-Measure (higher is better) (Och and Ney, 2003) for the English —* French translation direction. To declare a better model we have to settle on an alignment measure. Although the relationship between AER/F-Measure and translation quality varies (Dyer et al., 2013), there are some positive experiments (Fraser and Marcu, 2004) showing that F-Measure may be more useful, so perhaps a comparison based on F-Measure is ideal. Table 1 contains our results for the Hansards data. For the smaller Romanian data, we obtained similar behavior, but we leave out these results due mk q(a(k)|e(k), f(k)) a H j=1 2c(e, f) dice(e, f) = { (l+1)Z(j,l,m) le−λ |l </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational-Linguistics, 29(1): 19-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Simion</author>
<author>Michael Collins</author>
<author>Cliff Stein</author>
</authors>
<title>A Convex Alternative to IBM Model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Simion, Collins, Stein, 2013</marker>
<rawString>Andrei Simion, Michael Collins and Cliff Stein. 2013. A Convex Alternative to IBM Model 2. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Simion</author>
<author>Michael Collins</author>
<author>Cliff Stein</author>
</authors>
<title>A Family of Latent Variable Convex Relaxations for IBM Model 2.</title>
<date>2015</date>
<booktitle>In Proceedings of the AAAI.</booktitle>
<contexts>
<context position="1701" citStr="Simion et al., 2015" startWordPosition="270" endWordPosition="273">ve AER and F-Measure improvements over the classical Model 1 by over 30%. 1 Introduction The IBM translation models were introduced in (Brown et al., 1993) and were the first-generation Statistical Machine Translation (SMT) systems. In the current pipeline, these word-based models are the seeds for more sophisticated models which need alignment tableaus to start their optimization procedure. Among the original IBM Models, only IBM Model 1 can be formulated as a concave optimization problem. Recently, there has been some research on IBM Model 2 which addresses either the model’s non-concavity (Simion et al., 2015) ∗Currently on leave at Google Inc. New York. or over parametrization (Dyer et al., 2013). We make the following contributions in this paper: • We utilize and expand the mechanism introduced in (Simion et al., 2015) to construct strictly concave versions of IBM Model 11. As was shown in (Toutanova and Galley, 2011), IBM Model 1 is not a strictly concave optimization problem. What this means in practice is that although we can initialize the model with random parameters and get to the same objective cost via the EM algorithm, there is quite a bit of alignment quality variance within the model’s</context>
</contexts>
<marker>Simion, Collins, Stein, 2015</marker>
<rawString>Andrei Simion, Michael Collins and Cliff Stein. 2015. A Family of Latent Variable Convex Relaxations for IBM Model 2. In Proceedings of the AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Michel Galley</author>
</authors>
<title>Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2017" citStr="Toutanova and Galley, 2011" startWordPosition="324" endWordPosition="327">isticated models which need alignment tableaus to start their optimization procedure. Among the original IBM Models, only IBM Model 1 can be formulated as a concave optimization problem. Recently, there has been some research on IBM Model 2 which addresses either the model’s non-concavity (Simion et al., 2015) ∗Currently on leave at Google Inc. New York. or over parametrization (Dyer et al., 2013). We make the following contributions in this paper: • We utilize and expand the mechanism introduced in (Simion et al., 2015) to construct strictly concave versions of IBM Model 11. As was shown in (Toutanova and Galley, 2011), IBM Model 1 is not a strictly concave optimization problem. What this means in practice is that although we can initialize the model with random parameters and get to the same objective cost via the EM algorithm, there is quite a bit of alignment quality variance within the model’s optimal solution set and ambiguity persists on which optimal solution truly is the best. Typically, the easiest way to make a concave model strictly concave is to append an 12 regularizer. However, this method does not allow for seamless EM training: we have to either use a learning-rate dependent gradient based a</context>
<context position="5858" citStr="Toutanova and Galley, 2011" startWordPosition="1017" endWordPosition="1020">air. Given these definitions, the IBM Model 1 optimization problem is given in Fig. 1 and, for example, (Koehn, 2008). The parameters in this problem are t(f|e). The t(f|e) parameters are translation parameters specifying the probability of English word e being translated as French word f. The objective function is then the log-likelihood of the training data (see Eq. 3): where log p(f(k) j |e(k)) is �lk t(f(k)|e(k)) j i 1 + lk and C is a constant that can be ignored. Figure 1: The IBM Model 1 Optimization Problem. While IBM Model 1 is concave optimization problem, it is not strictly concave (Toutanova and Galley, 2011). Therefore, optimization methods for IBM Model 1 (specifically, the EM algorithm) are typically only guaranteed to reach a global maximum of the objective function (see the Appendix for a simple example contrasting convex and strictly convex functions). In particular, although the objective cost is the same for any optimal solution, the translation quality of the solutions is not fixed and will still depend on the initialization of the model (Toutanova and Galley, 2011). 3 A Strictly Concave IBM Model 1 We now detail a very simple method to make IBM Model 1 strictly concave with a unique opti</context>
</contexts>
<marker>Toutanova, Galley, 2011</marker>
<rawString>Kristina Toutanova and Michel Galley. 2011. Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the LOnorm.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Vaswani, Huang, Chiang, 2012</marker>
<rawString>Ashish Vaswani, Liang Huang and David Chiang. 2012. Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the LOnorm. In Proceedings of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9986485">
Incorporating Trustiness and Collective Synonym/Contrastive Evidence
into Taxonomy Construction
</title>
<author confidence="0.986226">
Luu Anh Tuan 41, Jung-jae Kim 42, Ng See Kiong ∗3
</author>
<affiliation confidence="0.9971">
#School of Computer Engineering, Nanyang Technological University, Singapore
</affiliation>
<email confidence="0.858469">
1anhtuan001@e.ntu.edu.sg, 2jungjae.kim@ntu.edu.sg
</email>
<affiliation confidence="0.788603">
∗Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore
</affiliation>
<email confidence="0.986286">
3skng@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993464">
Taxonomy plays an important role in many
applications by organizing domain knowl-
edge into a hierarchy of is-a relations be-
tween terms. Previous works on the taxo-
nomic relation identification from text cor-
pora lack in two aspects: 1) They do not
consider the trustiness of individual source
texts, which is important to filter out in-
correct relations from unreliable sources.
2) They also do not consider collective
evidence from synonyms and contrastive
terms, where synonyms may provide ad-
ditional supports to taxonomic relations,
while contrastive terms may contradict
them. In this paper, we present a method
of taxonomic relation identification that
incorporates the trustiness of source texts
measured with such techniques as PageR-
ank and knowledge-based trust, and the
collective evidence of synonyms and con-
trastive terms identified by linguistic pat-
tern matching and machine learning. The
experimental results show that the pro-
posed features can consistently improve
performance up to 4%-10% of F-measure.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976212195122">
Taxonomies which serve as backbone of struc-
tured knowledge are useful for many applica-
tions such as question answering (Harabagiu et
al., 2003) and document clustering (Fodeh et al.,
2011). Even though there are many hand-crafted,
well-structured taxonomies publicly available, in-
cluding WordNet (Miller, 1995), OpenCyc (Ma-
tuszek et al., 2006), and Freebase (Bollacker et
al., 2008), they are incomplete in specific domains,
and it is time-consuming to manually extend them
or create new ones. There is thus a need for auto-
matically extracting taxonomic relations from text
corpora to construct/extend taxonomies.
Previous works on the task of taxonomy con-
struction capture information about potential tax-
onomic relations between concepts, rank the can-
didate relations based on the captured information,
and integrate the highly ranked relations into a tax-
onomic structure. They utilize such information
as hypernym patterns (e.g. A is a B, A such as
B) (Kozareva et al., 2008), syntactic dependency
(Drumond and Girardi, 2010), definition sentences
(Navigli et al., 2011), co-occurrence (Zhu et al.,
2013), syntactic contextual similarity (Tuan et al.,
2014), and sibling relations (Bansal et al., 2014).
They, however, lack in the three following as-
pects: 1) Trustiness: Not all sources are trust-
worthy (e.g. gossip, forum posts written by
non-experts) (Dong et al., 2015). The trustiness
of source texts is important in taxonomic rela-
tion identification because evidence from unre-
liable sources can be incorrect. For example,
the invalid taxonomic relation between “American
chameleon” and “chameleon” is mistakenly more
popular in the Web than the valid taxonomic rela-
tion between “American chameleon” and “lizard”,
and statistical methods without considering the
trustiness may incorrectly extract the invalid rela-
tion instead of the latter. However, to the best of
our knowledge, no previous work considered this
aspect.
</bodyText>
<listItem confidence="0.990072076923077">
2) Synonyms: A concept may be expressed in
multiple ways, for example with synonyms. The
previous works mostly assumed that a term repre-
sents an independent concept, and did not combine
information about a concept, which is expressed
with multiple synonyms. The lack of evidence
from synonyms may hamper the ranking of can-
didate taxonomic relations. Navigli and Velardi
(2004) combined synonyms into a concept, but
only for those from WordNet, called synsets.
3) Contrastive terms: We observe that if two
terms are often contrasted (e.g. A but not B, A
is different from B) (Kim et al., 2006), they may
</listItem>
<page confidence="0.892848">
1013
</page>
<note confidence="0.9850065">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1013–1022,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999563941176471">
not have a taxonomic relation.
In this paper, we present a method based on the
state-of-the-art method (Tuan et al., 2014), which
incorporates the trustiness of source texts and
the collective evidence from synonyms/contrastive
terms. Tuan et al. (2014) rank candidate tax-
onomic relations based on such evidence as hy-
pernym patterns, WordNet, and syntactic contex-
tual similarity, where the pattern matches and the
syntactic contexts are found from the Web by us-
ing a Web search engine. First, we calculate the
trustiness score of each data source with the four
following weights: importance (if it is linked by
many pages), popularity (if it is visited by many
users), authority (if it is from a creditable Web site)
and accuracy (if it has many facts). We integrate
this score to the work of (Tuan et al., 2014) so that
evidence for taxonomic relations from unreliable
sources is discarded.
Second, we identify synonyms of two terms (t1,
t2), whose taxonomic relation is being scrutinized,
by matching such queries as “t1 also known as”
against the Web to find t1’s synonyms next to the
query matches (e.g. t in “t1 also known as t”). We
then collect the evidence for all taxonomic rela-
tions between ti and t2, where tz is either ti or its
synonym (i E {1, 2}), and combine them to calcu-
late the evidence score of the candidate taxonomic
relation between t1 and t2. Similarly, for each pair
of two terms (t1, t2), we collect their contrastive
evidence by matching such queries as “t1 is not a
type of t2” against the Web, and use them to pro-
portionally decrease the evidence score for taxo-
nomic relation between contrasting terms.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999953981132076">
The previous methods for identifying taxonomic
relations from text can be generally classified
into two categories: linguistic and statistical ap-
proaches. The former approach mostly exploits
lexical-syntactic patterns (e.g. A is a B, A such
as B) (Hearst, 1992). Those patterns can be man-
ually created (Kozareva et al., 2008; Wentao et al.,
2012) or automatically identified (Navigli et al.,
2011; Bansal et al., 2014). The pattern matching
methods show high precision when the patterns
are carefully defined, but low coverage due to the
lack of contextual analysis across sentences.
The latter approach, on the other hand, includes
asymmetrical term co-occurrence (Fotzo and Gal-
linari, 2004), clustering (Wong et al., 2007), syn-
tactic contextual similarity (Tuan et al., 2014), and
word embedding (Fu et al., 2014). The main idea
behind these techniques is that the terms that are
asymmetrically similar to each other with regard
to, for example, co-occurrences, syntactic con-
texts, and latent vector representation may have
taxonomic relationships. Such methods, however,
usually suffer from low accuracy, though show-
ing relatively high coverage. To get the balance
between the two approaches, Yang and Callan
(2009), Zhu et al. (2013) and Tuan et al. (2014)
combine both statistical and linguistic features in
the process of finding taxonomic relations.
Most of these previous methods do not consider
if the source text of evidence (e.g. co-occurrences,
pattern matches) is trustworthy or not and do not
combine evidence from synonyms and contrastive
terms as discussed earlier. Related to synonyms, a
few previous works utilize siblings for taxonomy
construction. Yang and Callan (2009) use siblings
as one of the features in the metric-based frame-
work which incrementally clusters terms to form
taxonomies. Wentao et al. (2012) also utilize such
sibling feature that, for example of the linguistic
pattern “A such as B1, B2, · · · and Bn”, if the
concept at the k-th position (e.g. Bk) from pat-
tern keywords (e.g. such as) is a valid sub-concept
(e.g. of A), then most likely its siblings from po-
sition 1 to position k-1 (e.g. B1, · · · , Bk−1) are
also valid sub-concepts. Bansal et al. (2014) in-
clude the sibling factors to a structured probabilis-
tic model over the full space of taxonomy trees,
thus helping to add more evidence to taxonomic
relations. Navigli and Velardi (2004) utilize the
synonym feature (i.e. WordNet synsets) for the
process of semantic disambiguation and concept
clustering as mentioned above, but not for the pro-
cess of inducing novel taxonomic relations.
</bodyText>
<sectionHeader confidence="0.997884" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9999396">
We briefly introduce (Tuan et al., 2014) in Section
3.1. We then explain how to incorporate trustiness
(Section 3.2) and collective evidence from syn-
onyms (Section 3.3) and from contrastive terms
(Section 3.4) into the work of (Tuan et al., 2014).
</bodyText>
<subsectionHeader confidence="0.999901">
3.1 Overview of baseline (Tuan et al., 2014)
</subsectionHeader>
<bodyText confidence="0.99772975">
Tuan et al. (2014) follow three steps to construct a
taxonomy: term extraction/filtering, taxonomic re-
lation identification and taxonomy induction. Be-
cause the focus of this paper is on the second step,
</bodyText>
<page confidence="0.993109">
1014
</page>
<bodyText confidence="0.994467">
taxonomic relation identification, we use the same
methods for the first and third steps as in (Tuan et
al., 2014) and will not discuss them here.
Given each ordered pair of two terms t1 and t2
from the term extraction/filtering, the taxonomic
relation identification of (Tuan et al., 2014) calcu-
lates the evidence score that t1 is a hypernym of t2
(denoted as t1 » t2) based on the following three
measures:
String inclusion with WordNet (SIWN): This
measure is to check if t1 is a substring of t2, con-
sidering synonymy between words using WordNet
synsets. ScoreSIWN(t1, t2) is set to 1 if there is
such evidence; otherwise, it is set to 0.
Lexical-syntactic pattern (LSP): This measure
is to find how much more evidence for t1 » t2
is found in the Web than for t2 » t1. Specif-
ically, a list of manually constructed hypernym
patterns (e.g. “t2 is a t1”) is queried with a Web
search engine to estimate the number of evidence
for t1 » t2 from the Web. The LSP measure is
calculated as follows, where CWeb(t1, t2) denotes
the collection of search results:
</bodyText>
<equation confidence="0.998916">
ScoreLSP(t1,t2) = log(|CWeb(t1,t2)|)
1+log(|CWeb(t2,t1)|)
</equation>
<bodyText confidence="0.994771588235294">
Syntactic contextual subsumption (SCS): The
idea of this measure is to derive the hypernymy
evidence for two terms t1 and t2 from their syntac-
tic contexts, particularly from the triples of (sub-
ject,verb,object). They observe that if the context
set of t1 mostly contains that of t2 but not vice
versa, then t1 is likely to be a hypernym of t2. To
implement this idea, they first find the most com-
mon relation (or verb) r between t1 and t2, and
use the queries “t1 r” and “t2 r” to construct two
corpora CorpusΓt1 and CorpusΓt2 for t1 and t2, re-
spectively. Then the syntactic context sets are cre-
ated from these contextual corpora using a non-
taxonomic relation identification method. The de-
tails of calculating ScoreSCS(t1, t2) can be found
in (Tuan et al., 2014).
They linearly combine the three scores as follows:
</bodyText>
<equation confidence="0.999457">
Score(t1, t2) = α x ScoreSIWN(t1, t2)
+ β x ScoreLSP(t1, t2) + γ x ScoreSCS(t1, t2)
(1)
</equation>
<bodyText confidence="0.92939725">
If Score(t1, t2) is greater than a threshold value,
then t1 is regarded as a hypernym of t2. We use
the same values of α, Q and γ as in (Tuan et al.,
2014).
</bodyText>
<subsectionHeader confidence="0.999494">
3.2 Trustiness of the evidence data
</subsectionHeader>
<bodyText confidence="0.999967">
We introduce our method of estimating the trusti-
ness of a given source text in Section 3.2.1 and ex-
plain how to incorporate it into the work of (Tuan
et al., 2014) in Section 3.2.2.
</bodyText>
<subsectionHeader confidence="0.7986835">
3.2.1 Collecting trustiness score of the
evidence data
</subsectionHeader>
<bodyText confidence="0.9707115">
Given a data source (e.g. Web page), we consider
four aspects of trustiness as follows:
</bodyText>
<listItem confidence="0.996049">
• Importance: A data source may be important
if it is referenced by many other sources.
• Popularity: If a data source is accessed by
many people, it is considered popular.
• Authority: If the data is created by a trusted
agency, such as government and education
institute, it may be more trustful than others
from less trusted sources such as forums and
social media.
• Accuracy: If the data contains many pieces
of accurate information, it seems trustful.
</listItem>
<sectionHeader confidence="0.362698" genericHeader="method">
Importance
</sectionHeader>
<bodyText confidence="0.999957375">
To measure the importance of a Web page (d) as
data source, we use the Google PageRank score1
(ScorePageRank(d)) that is calculated based on
the number and quality of links to the page. The
PageRank scores have the scale from 0 to 9, where
the bigger score means more importance than the
lower one. Using this score, the importance of a
page is calculated as follows:
</bodyText>
<equation confidence="0.997869">
1
TrustImp(d) = 10 − ScorePageRank(d) (2)
</equation>
<bodyText confidence="0.999971142857143">
Note that we use the non-linearity for PageR-
ank score rather than just normalizing PageRank
to 0-1. The reason is to vary the gaps between the
important sites (which usually have the PageRank
score value from 7-10) and majority unimportant
site (which usually have the PageRank score value
less than 5).
</bodyText>
<subsectionHeader confidence="0.531081">
Popularity
</subsectionHeader>
<bodyText confidence="0.9997562">
We use Alexa’s Traffic Rank2 as the measure of
popularity, (ScoreAlexa(d)) which is based on the
traffic data provided by users in Alexa’s global
data panel over a rolling 3 month period. The
Traffic Ranks are updated daily. A site’s rank is
</bodyText>
<footnote confidence="0.999641333333333">
1http://searchengineland.com/what-is-google-pagerank-
a-guide-for-searchers-webmasters-11068
2http://www.alexa.com/
</footnote>
<page confidence="0.994484">
1015
</page>
<bodyText confidence="0.999598666666667">
based on a combined measure of unique visitors
and page views. Using this rank, the popularity of
a data source is calculated as follows:
</bodyText>
<equation confidence="0.9983">
1
TrustPop(d) = log(ScoreAlexa(d) + 1) (3)
</equation>
<bodyText confidence="0.99985">
We use log transform in the popularity score in-
stead of, for example, linear scoring because we
want to avoid the bias of the much large gap be-
tween the Alexa scores of different sites (e.g. one
site has Alexa score 1000, but the other may have
score 100,000)
</bodyText>
<sectionHeader confidence="0.670567" genericHeader="method">
Authority
</sectionHeader>
<bodyText confidence="0.999881428571429">
We rank the authority of a data source based on the
internet top-level domain (TLD). We observe that
the pages with limited and registered TLD (e.g.
.gov, .mil, .edu) are often more credible than those
with open domain (e.g. .com, .net). Therefore,
the authority score of a data source is calculated
as follows:
</bodyText>
<equation confidence="0.906565">
� 1 if TLD of d is .gov, .mil or .edu
TrustAuth(d) =
0 otherwise
(4)
</equation>
<bodyText confidence="0.999973">
Note that there are some reasons we choose
such implementation of Authority in an elemen-
tary way. First, we tried finer categorization of
various domains, e.g. .int has score 1/3, .com has
score 1/4, etc. However, the experimental results
did not show much change of performance. In ad-
dition, there is controversy on which open TLD
domains are more trustful than the others, e.g. it is
difficult to judge whether a .net site is more trust-
ful than .org or not. Thus, we let all open TLD
domains have the same score.
</bodyText>
<sectionHeader confidence="0.582694" genericHeader="method">
Accuracy
</sectionHeader>
<bodyText confidence="0.999969947368421">
If the data source contains many pieces of accu-
rate information, it is trustful. Inspired by the idea
of Dong et al. (2015), we estimate the accuracy of
a data source by identifying correct and incorrect
information in form of the triples (Subject, Predi-
cate, Object) in the source, where Subject, Predi-
cate and Object are normalized with regard to the
knowledge base Freebase. The extraction of the
triples includes six tasks: named entity recogni-
tion, part of speech tagging, dependency parsing,
triple extraction, entity linkage (which maps men-
tions of proper nouns and their co-references to
the corresponding entities in Freebase) and rela-
tion linkage. We use three information extraction
(IE) tools (Angeli et al. (2014), Manning et al.
(2014), MITIE3) for the first four tasks, and de-
velop a method similar to Hachey et al. (2013)
for the last two tasks of entity linkage and relation
linkage.
Since the IE tools may produce noisy or unre-
liable triples, we use a voting scheme for triple
extraction as follows: A triple is only considered
to be true if it is extracted by at least two extrac-
tors. After obtaining all triples in the data source,
we use the closed world assumption as follows:
Given subject s and predicate p, O(s, p) denotes
the set of such objects that a triple (s,p,o) is found
in Freebase. Now given a triple (s, p, o) found
in the data source, if o E O(s, p), we conclude
that the triple is correct; but if o E� O(s, p) and
O(s, p) &gt; 0, we conclude that the triple is incor-
rect. If O(s, p) = 0, we do not conclude any-
thing about the triple, and the triple is removed
from the set of facts found in the data source.
Given a data source d, we define cf(d) as the
number of correct facts, and icf(d) as the number
of incorrect facts found in d. The accuracy of d is
calculated as follows:
</bodyText>
<equation confidence="0.9976885">
1 1
TrustAccu(d) = 1 + icf(d)2 −1 + cf(d)2 (5)
</equation>
<bodyText confidence="0.862539">
Combining trustiness scores
The final trustiness score of a data source is the
linear combination of the four scores as follows:
</bodyText>
<equation confidence="0.9978725">
Trust(d) = α × TrustImp(d) + β × TrustPop(d)
+ γ × TrustAuth(d) + δ × TrustAccu(d)
</equation>
<bodyText confidence="0.9999577">
To estimate the optimal combination for parame-
ters α, β, γ and δ, we apply linear regression algo-
rithm (Hastie et al., 2009). For parameter learning,
we manually list 50 websites as trusted sources
(e.g. stanford.edu, bbc.com, nasa.gov), and the
top 15 gossip websites listed in a site4 as untrusted
sources. Then we use the scores of their individual
pages by the four methods to learn the parameters
in Formula 6. The learning results are as follows:
α=0.46, β=0.46, γ=2.03, δ=0.61.
</bodyText>
<sectionHeader confidence="0.989443" genericHeader="method">
3.2.2 Integrating trustiness into taxonomic
relation identification methods
</sectionHeader>
<bodyText confidence="0.973959">
Given a data collection C, we define:
</bodyText>
<equation confidence="0.911994">
AvgTrust(C) = |C|
</equation>
<footnote confidence="0.6550235">
3https://github.com/mit-nlp/MITIE
4http://www.ebizmba.com/articles/gossip-websites
</footnote>
<figure confidence="0.518773666666667">
(6)
E
d∈C Trust(d)
</figure>
<page confidence="0.954817">
1016
</page>
<bodyText confidence="0.816661714285714">
as the average trustiness score of all data in C.
We integrate the trustiness score to the three tax-
onomic relation identification methods described
in Section 3.1 as follows:
LSP method:
The LSP evidence score of the taxonomic relation
between t1 and t2 is recalculated as follows:
</bodyText>
<equation confidence="0.966671666666667">
ScoreT rust
LSP (t1, t2) = ScoreLSP(t1, t2)×
(AvgTrust(CWeb(t1, t2)) + AvgTrust(CW eb(t2, t1)))
</equation>
<bodyText confidence="0.999863142857143">
The intuition of Formula 7 is that the original
LSP evidence score is multiplied with the average
trustiness score of all evidence documents for the
taxonomic relation from the Web. If the number
of Web search results is too large, we use only the
first 1,000 results to estimate the average trustiness
score.
</bodyText>
<sectionHeader confidence="0.449304" genericHeader="method">
SCS method:
</sectionHeader>
<bodyText confidence="0.948964">
Similarly, the SCS evidence score is recalculated
as follows:
</bodyText>
<equation confidence="0.890664">
ScoreT rust
SCS (t1, t2) = ScoreSCS(t1, t2)×
(AvgTrust(CorpusΓ t1) + AvgTrust(CorpusΓ t2))
SIWN method:
</equation>
<bodyText confidence="0.998982">
This method does not use any evidence from the
Web, and so its measure does not change as fol-
lows:
</bodyText>
<equation confidence="0.871354">
ScoreT rust
SIW N(t1, t2) = ScoreSIWN(t1, t2) (9)
</equation>
<bodyText confidence="0.9611025">
The three measures of trustiness are also linearly
combined as follows:
</bodyText>
<equation confidence="0.999525833333333">
ScoreTrust(t1, t2) = α × ScoreT rust
SIW N(t1, t2)+
β × ScoreT rust
LSP (t1, t2) + γ × ScoreT rust
SCS (t1, t2)
(10)
</equation>
<bodyText confidence="0.999423">
The values of α, Q, and γ in Formula 10 are
identical to those for Formula 1.
</bodyText>
<subsectionHeader confidence="0.9882165">
3.3 Collective synonym evidence
3.3.1 Synonymy identification
</subsectionHeader>
<bodyText confidence="0.989362">
We use the three following methods to collect syn-
onyms: dictionaries, pattern matching, and super-
vised learning.
Dictionaries: Synonyms can be found in dic-
tionaries like a general-purpose dictionary Word-
Net and also domain-specific ones. Since our do-
mains of interest include virus, animals, and plants
(see the next section for details), we also utilize
MeSH5, a well-known vocabulary in biomedicine.
Pattern matching: Given two terms t1 and t2, we
use the following patterns to find their synonymy
evidence from the Web:
</bodyText>
<listItem confidence="0.99821325">
• t1 also [known|called|named|abbreviated] as t2
• Other common name[s] of t1 [is|are|include] t2
• t1, or t2, is a
• t1 (short for t2)
</listItem>
<bodyText confidence="0.795291769230769">
, where [a|b] denotes a choice between a and b. If
the number of Web search results is greater than a
threshold Ψ, t1 is considered as a synonym of t2.
Supervised learning: We randomly pick 100
pairs of synonyms in WordNet, and for each pair,
we use the Web search engine to collect sample
sentences in which both terms of the pair are men-
tioned. If the number of collected sentences is
greater than 2000, we use only the first 2000 sen-
tences for training. After that, we extract the fol-
lowing features from the sentences to train a logis-
tic regression model (Hastie et al., 2009) for the
synonymy identification:
</bodyText>
<listItem confidence="0.999959285714286">
• Headwords of the two terms
• Average distance between the terms
• Sequence of words between the terms
• Bag of words between the terms
• Dependency path between the terms (using
Stanford parser (Klein and Manning, 2003))
• Bag of words on the dependency path
</listItem>
<bodyText confidence="0.99987375">
The average F-measure of the obtained model
with 10-fold cross-validation is 81%. We use the
learned model to identify more synonym pairs in
the next step.
</bodyText>
<subsectionHeader confidence="0.960666">
3.3.2 Embedding synonym information
</subsectionHeader>
<bodyText confidence="0.999803">
Given a term t, we denote Syn(t) as the set of
synonyms of t (including t itself). The evidence
scores of the SCS and LSP methods are recalcu-
lated with synonyms as follows:
</bodyText>
<equation confidence="0.8976982">
� ScoreX(t�1, t2) (11)
ScoreSynonym
X (t1, t2) =
tiESyn(t1)
t2ESyn(t2)
</equation>
<bodyText confidence="0.8288415">
, where the variable X can be replaced with SCS
and LSP.
</bodyText>
<footnote confidence="0.978871">
5http://www.ncbi.nlm.nih.gov/mesh
</footnote>
<page confidence="0.99366">
1017
</page>
<bodyText confidence="0.999026285714286">
The intuition of Formula (11) is that the evi-
dence score of the taxonomic relation between two
terms t1 and t2 can be boosted by adding all the
evidence scores of taxonomic relations between
them and their synonyms.
Again, as for the SIWN method, we do not
change the evidence score as follows:
</bodyText>
<subsectionHeader confidence="0.359957">
ScoreSynonym
</subsectionHeader>
<bodyText confidence="0.495427">
SIW N (t1, t2) = ScoreSIWN(t1, t2)
</bodyText>
<subsectionHeader confidence="0.98947">
3.4 Contrastive evidence
</subsectionHeader>
<bodyText confidence="0.998689666666667">
Given two terms t1 and t2, we use the following
patterns to find their contrastive (thus negative) ev-
idence from the Web:
</bodyText>
<listItem confidence="0.988922333333333">
• t1 is not a t2 • t1 is not a [type|kind] of t2
• t1, unlike the t2 • t1 is different [from|with] t2
• t1 but not t2 • t1, not t2
</listItem>
<bodyText confidence="0.868014666666667">
WH(t1, t2) denotes the total number of Web
search results, and the contrastive evidence score
between t1 and t2 is computed as follows:
</bodyText>
<equation confidence="0.992341">
Contrast(t1, t2) = log(WH(t1, t2) + 1) (12)
</equation>
<bodyText confidence="0.9976288">
Similar to the collective synonym evidence, the
contrastive evidence score of taxonomic relation
between t1 and t2 is boosted with the contrastive
evidence scores of taxonomic relations between
the two terms and their synonyms as follows:
</bodyText>
<equation confidence="0.999072">
E tiESyn(t1) Contrast(t�1, t2)
t2ESyn(t2)
|Syn(t1) |∗ |Syn(t2)|
</equation>
<subsectionHeader confidence="0.952815">
3.5 Combining trustiness, synonym and
contrastive evidence
</subsectionHeader>
<bodyText confidence="0.9998435">
We combine all the three features into the system
of (Tuan et al., 2014) as follows:
</bodyText>
<equation confidence="0.9513935">
Final
X(t1,t2)=ScoreTXrust(t1, t2)
+ ScoreSynonym
X (t1, t2)
</equation>
<bodyText confidence="0.99944075">
, where the variable X can be replaced with each
of the three taxonomic relation evidence measures
(i.e. SCS, LSP, SIWN). The final combined score
is calculated as follows:
</bodyText>
<equation confidence="0.975665">
ScoreFinal
Combined(t1, t2) = α × ScoreFinal
SIW N(t1, t2)
+ β × ScoreF inal
LSP (t1, t2) + γ × ScoreF inal
SCS (t1, t2)
− δ × ScoreContrast(t1, t2)
For each ordered pair of terms t1 and t2, if
ScoreF inal
Combined(t1, t2) is greater than a threshold
</equation>
<bodyText confidence="0.997753117647059">
value, then t1 is considered as a hypernym of t2.
We estimate the optimal values of parameters
α, β, -y and 6 in Formula 15 with ridge regres-
sion technique (Hastie et al., 2009) as follows:
First, we randomly select 100 taxonomic relations
in Animal domain as the training set. For each
taxonomic relation t1 » t2, its evidence score is
estimated as T + Dist(t1,t2), where T is the thresh-
old value for ScoreFinal Combined, and Dist(t1, t2) is
the length of the shortest path between t1 and t2
found in WordNet. Then we use our system to
find evidence scores with taxonomic relation iden-
tification methods in Formulas 13 and 14. Finally,
we build the training set using Formula 15, and
use the ridge regression algorithm to learn that the
best value for α is 1.31, β 1.57, -y 1.24 and 6 0.79,
where T=2.3.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.903584">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.997562666666667">
We evaluate our method for taxonomy construc-
tion against the following collections of six do-
mains:
</bodyText>
<listItem confidence="0.975715352941177">
• Artificial Intelligence (AI) domain: The cor-
pus consists of 4,976 papers extracted from
the IJCAI proceedings from 1969 to 2014 and
the ACL archives from year 1979 to 2014.
• Finance domain: The corpus consists of
1,253 papers extracted from the freely avail-
able collection of “Journal of Financial Eco-
nomics” from 1995 to 2012 and from “Re-
view Of Finance” from 1997 to 2012.
• Virus domain: We submit the query “virus”
to PUBMED search engine 6 and retrieve the
first 20,000 abstracts as the corpus of the
virus domain.
• Animals, Plants and Vehicles domains: Col-
lections of Web pages crawled by using
the bootstrapping algorithm described by
Kozareva et al. (2008).
</listItem>
<bodyText confidence="0.9943085">
We report the results of two experiments in
this section: (1) Evaluating the construction of
new taxonomies for Finance and AI domains (Sec-
tion 4.2), and (2) comparing with the curated
</bodyText>
<footnote confidence="0.640265">
6http://www.ncbi.nlm.nih.gov/pubmed
</footnote>
<equation confidence="0.897497">
ScoreContrast(t1, t2) =
</equation>
<page confidence="0.969239">
1018
</page>
<bodyText confidence="0.999874125">
databases’ sub-hierarchies. We compare our ap-
proach with other three state-of-the-art methods
in the literature, i.e. (Kozareva and Hovy, 2010),
(Navigli et al., 2011) and (Tuan et al., 2014) (Sec-
tion 4.3). In addition, for Animal domain, we also
compare with the reported performance of Bansal
et al. (2014), a recent work to construct taxonomy
using belief propagation.
</bodyText>
<subsectionHeader confidence="0.9582055">
4.2 Constructing new taxonomies for
Finance and AI domains
</subsectionHeader>
<bodyText confidence="0.999835857142857">
Referential taxonomy structures such as WordNet
and OpenCyc are widely used in semantic analyt-
ics applications. However, their coverage is lim-
ited to common, well-known areas, and many spe-
cific domains like Finance and AI are not well cov-
ered in those structures. Therefore, an automatic
method which can induce taxonomies for those
specific domains can greatly contribute to the pro-
cess of knowledge discovery.
To estimate the precision of a given method, we
randomly choose 100 relations among the results
of the method and manually check their correct-
ness. The results summarized in Table 1 show that
our method extracts much more relations, though
with slightly lower precision, than Kozareva et al.
(2008) and Navigli and Velardi (2004). Note that
due to the lack of gold standards in these two do-
mains, we do not compare the methods in terms
of F-score, which we will measure with curated
databases in the next section. Compared to Tuan
et al. (2014), which can be considered as the base-
line of our approach, our method has significant
improvement in both precision and the number
of extracted relations. It indicates that the three
incorporated features of trustiness, and synonym
and contrastive evidence are effective in improv-
ing the performance of existing taxonomy con-
struction methods.
</bodyText>
<table confidence="0.999385">
Finance P AI
P N N
Kozareva 90% 753 94% 950
Navigli 88% 1161 93% 1711
Tuan 85% 1312 90% 1927
Our method 88% 1570 92% 2168
</table>
<tableCaption confidence="0.998679">
Table 1: Experiment result for finance and AI do-
</tableCaption>
<bodyText confidence="0.8794115">
mains. P stands for Precision, and N indicates the
number of extracted relations.
</bodyText>
<subsectionHeader confidence="0.992182">
4.3 Evaluation against curated databases
</subsectionHeader>
<bodyText confidence="0.999421">
We evaluate automatically constructed tax-
onomies for four domains (i.e. Animal, Plant,
Vehicle, Virus) against the corresponding sub-
hierarchies of curated databases. For Animal,
Plant and Vehicle domains, we use WordNet as
the gold standards, whereas for Virus domain, we
use MeSH sub-hierarchy of virus as the reference.
Note that in this comparison, to be fair, we
change our algorithm to avoid using WordNet in
identifying taxonomic relations (i.e. SIWN algo-
rithm), and we only use the exact string-matching
comparison without WordNet. The evaluation
uses the following measures:
</bodyText>
<tableCaption confidence="0.49455325">
Precision — #relations found in database and by the method
#relations found by the method
Recall —_ #relations found in database and by the method
#relations found in database
</tableCaption>
<bodyText confidence="0.999619583333333">
To understand the individual contribution of the
three introduced features (i.e. trustiness, synonym,
contrast), we also evaluate our method only with
one of the three features each time, as well as with
all the three features (denoted as “Combined”).
Tables 2 and 3 summarize the experiment re-
sults. Our combined method achieves significantly
better performance than the previous state-of-the-
art methods in terms of F-measure and Recall (t-
test, p-value &lt; 0.05) for all the four domains.
For Animal domain, it also shows higher perfor-
mance than the reported performance of Bansal et
al. (2014). In addition, the proposed method im-
proves the baseline (i.e. Tuan et al. (2014)) up to
4%-10% of F-measure.
Furthermore, we find that the three features
have different contribution to the performance im-
provement. The trustiness feature contributes to
the improvement on both precision and recall. The
synonym feature has the tendency of improving
the recall further than the trustiness, whereas the
contrastive evidence improves the precision. Note
that we discussed these different contributions of
the features in the Introduction.
</bodyText>
<table confidence="0.9996715">
Animal P Plant F
P R F R
Kozareva 98 38 55 97 39 56
Navigli 97 44 61 97 38 55
Bansal 84 55 67
Tuan 95 56 70 95 53 68
Trustiness 97 61 75 97 56 71
Synonym 92 65 76 93 58 71
Contrast 97 55 70 97 53 69
Combined 97 65 78 96 59 73
</table>
<tableCaption confidence="0.747159">
Table 2: Experiment results for animal and plant
domains. P stands for Precision, R Recall, and F
F-score. The unit is %.
</tableCaption>
<page confidence="0.827436">
1019
</page>
<table confidence="0.999844444444444">
Vehicle P Virus F
P R F R
Kozareva 99 60 75 97 31 47
Navigli 91 49 64 99 37 54
Tuan 93 69 79 93 43 59
Trustiness 96 72 82 97 48 64
Synonym 91 72 80 91 53 67
Contrastive 97 68 80 98 42 59
Combined 95 73 83 96 54 69
</table>
<tableCaption confidence="0.763113">
Table 3: Experiment results for vehicle and virus
domains. P stands for Precision, R Recall, and F
F-score. The unit is %.
</tableCaption>
<subsectionHeader confidence="0.754367">
4.3.1 Evaluation of individual methods for
trustiness and synonymy identification
</subsectionHeader>
<bodyText confidence="0.99982327027027">
We evaluate the individual methods for trusti-
ness measurement and synonymy identification
described in Sections 3.2.1 and 3.3.1. For this pur-
pose, we evaluate our system only with one of the
individual methods at a time (i.e. importance, pop-
ularity, authority and accuracy for trustiness mea-
sure, and dictionary, pattern matching, and ma-
chine learning methods for synonymy identifica-
tion).
As summarized in Table 4, the “Importance”
and “Accuracy” methods for trustiness measure-
ment based on PageRank and IE systems, respec-
tively, have more contribution than the others.
Similarly, the experiment results indicate that the
“Machine learning” method has the most contribu-
tion among the three methods of synonymy iden-
tification.
In addition, we also examine the interdepen-
dence of the four introduced aspects of trustiness
by running the system with the combination of
only two aspects, Importance and Accuracy. The
results in all domains show that when combining
only the Importance and Accuracy, the system al-
most achieves the same performance to that of the
combined system with all four criteria, except for
the Plant domain. It can be explained as the Impor-
tance aspect (which is expressed as the PageRank
score) may subsume the Popularity and Authority
aspects. Another interesting point is that the per-
formance of Accuracy, which is solely based on
the local information from the website, when ap-
plied individually, is almost the same with that of
Importance which is based on the distributed in-
formation. It shows that the method of ranking of
the sites based on the knowledge-based facts can
achieve the effectiveness as good as the traditional
ranking method using PageRank score.
</bodyText>
<table confidence="0.99982">
Animal Plant Vehicle Virus
Trustiness:
Importance 74% 70% 81% 63%
Popularity 72% 69% 81% 61%
Authority 72% 69% 80% 61%
Accuracy 73% 70% 81% 62%
Imp + Accu 75% 70% 82% 64%
Synonym:
Dictionaries 73% 69% 79% 62%
Pattern matching 74% 69% 80% 64%
Machine learning 74% 70% 80% 65%
</table>
<tableCaption confidence="0.98010075">
Table 4: Contribution of individual trustiness mea-
sures and collective synonym evidence in terms of
F-measure. Imp stands for Important and Accu
stands for Accuracy
</tableCaption>
<subsectionHeader confidence="0.8741095">
4.4 Discussion
4.4.1 Case studies
</subsectionHeader>
<bodyText confidence="0.9981741875">
We give two examples to illustrate how the pro-
posed features help to infer correct taxonomic re-
lations and filter out incorrect relations. Our base-
line (Tuan et al. (2014)) extracts an incorrect taxo-
nomic relation between ‘fox’ and ‘flying fox’ due
to the following reasons: (1) ‘flying fox’ includes
‘fox’ (SIWN) and (2) untrusted sources such as
a public forum7 support the relation. Using our
proposed method, this relation is filtered out be-
cause those untrusted sources are discouraged by
the trustiness feature, and also because there are
contrastive evidence8 saying that ‘flying fox’ is
NOT a ‘fox’. Specifically, the average trustiness
score of LSP method of the sources for the invalid
relation (i.e. AvgTrust(CW b(fox, flying fox)) +
AvgTrust(CW b(flying fox, fox))) is 0.63, which
is lower than the average of those scores, 0.90.
Also, the collective contrastive evidence score
(i.e. ScoreC°&amp;quot;t...t(fox, flying fox)) is 1.10, which
is higher than the average collective contrastive
score, 0.32.
On the other hand, the true taxonomic rela-
tion between ‘bat’ and ‘flying fox’ is not identi-
fied by the baseline, mainly due to the rare men-
tion of this relation in the Web. However, our
proposed method can recognize this relation be-
cause of two reasons: (1) ‘flying fox’ has many
synonyms such as ‘fruit bat’, ‘pteropus’, ‘kalong’,
and ‘megabat’, and there are much evidence that
these synonyms are kinds of ‘bat’ (i.e. using
the collective synonym evidence). (2) The ev-
idence for the taxonomic relation between ‘fly-
</bodyText>
<footnote confidence="0.9994165">
7http://redwall.wikia.com/wiki/User:Ferretmaiden/Archive3
8http://en.cc-english.com/index.php?shownews-1397
</footnote>
<page confidence="0.993501">
1020
</page>
<bodyText confidence="0.999866352941177">
ing fox’ and ‘bat’, though rare, is from trusted
sites9 which are maintained by scientists. Thus,
the trustiness feature helps to boost the evidence
score for this relation over the threshold value.
Specifically, the average trustiness score of LSP
method (i.e. (AvgTrust(CW eb(bat, flying fox)) +
AvgTrust(CWeb(flying fox,bat)))), 2.84, is higher
than the average in total, 0.90.
We further investigate on 256 taxonomic rela-
tions that were missed by the baseline but correctly
identified by the proposed method. The average
ScoreLSP and the average ScoreSCS of the re-
lations by the baseline are 0.35 and 0.60, respec-
tively, while those by the proposed method are
1.17 and 0.82, respectively. We thus find that the
proposed method is more effective in correctly im-
proving the LSP method than the SCS method.
</bodyText>
<subsubsectionHeader confidence="0.594311">
4.4.2 Empirical comparison with WordNet
</subsubsectionHeader>
<bodyText confidence="0.999959818181818">
By error analysis, we find that our results may
complement WordNet. For example, in Animal
domain, our method identifies ‘wild sheep’ as a
hyponym of ‘sheep’, but in WordNet, they are sib-
lings. However, many references 10 11 consider
‘wild sheep’ as a species of ‘sheep’. Another such
example is that our system recognizes ‘aquatic
vertebrate’ as a hypernym of ‘aquatic mammal’,
but WordNet places them in different subtrees in-
correctly 12. Therefore, our results may help re-
structure and extend WordNet.
</bodyText>
<subsectionHeader confidence="0.513746">
4.4.3 Threshold tuning
</subsectionHeader>
<bodyText confidence="0.963163352941176">
Our scoring methods utilize several thresholds to
select relations of high ranks. Here we discuss
them in details below.
The threshold value Ψ for the pattern match-
ing method in Section 3.3.1 controls the number
of synonymy relations extracted from text. The
threshold value for Scores
Final of Formula 15
in Section 3.5 controls the number of extracted
taxonomic relations. In general, the larger these
threshold values are, the higher number of syn-
onyms and taxonomic relations we can get. In
our experiments, we found that the threshold val-
ues for Ψ between 100 and 120, and those for
Scores
Final between 2.3 and 2.5 generally help
the system achieve the best performance.
</bodyText>
<footnote confidence="0.9990474">
9http://krjsoutheastasianrainforests.weebly.com/animals-
in-biome-and-habitat-structures.html
10http://en.wikipedia.org/wiki/Ovis
11http://www.bjornefabrikken.no/side/norwegian-sheep/
12http://en.wikipedia.org/wiki/Aquatic mammal
</footnote>
<sectionHeader confidence="0.994362" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999976083333333">
In this paper, we propose the features of trusti-
ness, and synonym and contrastive collective ev-
idence for the task of taxonomy construction, and
show that these features help the system improve
the performance significantly. As future work, we
will investigate into the task of automatically con-
structing patterns for the pattern matching meth-
ods in Sections 3.3 and 3.4, to improve cover-
age. We will also enhance the accuracy measure
of trustiness, based on the observation that some
untrusted sites copy information from other sites
to make them look more trustful.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999551916666667">
Gabor Angeli, Julie Tibshirani, Jean Y. Wu, and
Christopher D. Manning. 2014. Combining Dis-
tant and Partial Supervision for Relation Extraction.
Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing, pages 1556–
1567.
Mohit Bansal, David Burkett, Gerard De Melo, and
Dan Klein. 2014. Structured learning for taxonomy
induction with belief propagation. Proceedings of
the 52nd Annual Meeting of the ACL, pages 1041–
1051.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. Proceedings of the ACM SIG-
MOD International Conference on Management of
Data, pages 1247–1250.
Xin L. Dong, Evgeniy Gabrilovich, Kevin Murphy, Van
Dang, Wilko Horn, Camillo Lugaresi, Shaohua Sun,
and Wei Zhang. 2015. Knowledge-Based Trust: Es-
timating the Trustworthiness of Web Sources. Pro-
ceedings of the VLDB Endowment, 8(9).
Lucas Drumond and Rosario Girardi. 2010. Extract-
ing ontology concept hierarchies from text using
markov logic. Proceedings of the ACM Symposium
on Applied Computing, pages 1354–1358.
Samah Fodeh, Bill Punch, and Pang N. Tan. 2011. On
ontology-driven document clustering using core se-
mantic features. Knowledge and information sys-
tems, 28(2):395–421.
Hermine N. Fotzo and Patrick Gallinari. 2004.
Learning “generalization/specialization” relations
between concepts - application for automatically
building thematic document hierarchies. Pro-
ceedings of the 7th International Conference on
Computer-Assisted Information Retrieval.
</reference>
<page confidence="0.801681">
1021
</page>
<reference confidence="0.999443090909091">
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hi-
erarchies via word embeddings. Proceedings of the
52nd Annual Meeting of the ACL, pages 1199–1209.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2013. Evaluat-
ing entity linking with Wikipedia. Artificial intel-
ligence, 194:130–150.
Sanda M. Harabagiu, Steven J. Maiorano, and Mar-
ius A. Pasca. 2003. Open-domain textual question
answering techniques. Natural Language Engineer-
ing, 9(3):231–267.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The elements of statistical learning.
Springer-Verlag.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. Proceedings of
the 14th Conference on Computational Linguistics,
pages 539–545.
Jung J. Kim, Zhuo Zhang, Jong C. Park, and See K.
Ng. 2006. Biocontrasts: extracting and exploiting
protein-protein contrastive relations from biomedi-
cal literature. Bioinformatics, 22(5):597–605.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Annual Meeting of the ACL, pages 423–430.
Zornitsa Kozareva and Eduard Hovy. 2010. A Semi-
supervised Method to Learn and Construct Tax-
onomies Using the Web. Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1110–1118.
Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.
2008. Semantic Class Learning from the Web with
Hyponym Pattern Linkage Graphs. Proceedings of
the 46th Annual Meeting of the ACL, pages 1048–
1056.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. Proceedings of the 52nd
Annual Meeting of the ACL, pages 55–60.
Cynthia Matuszek, John Cabral, Michael J. Witbrock,
and John DeOliveira. 2006. An introduction to the
syntax and content of cyc. Proceedings of the AAAI
Spring Symposium, pages 44–49.
George A. Miller. 1995. WordNet: a Lexical
Database for English. Communications of the ACM,
38(11):39–41.
Roberto Navigli and Paola Velardi. 2004. Learn-
ing Domain Ontologies from Document Warehouses
and Dedicated Web Sites. Computational Linguis-
tics, 30(2):151–179.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A Graph-based Algorithm for Inducing Lex-
ical Taxonomies from Scratch. Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1872–1877.
Luu A. Tuan, Jung J. Kim, and See K. Ng. 2014.
Taxonomy Construction using Syntactic Contextual
Evidence. Proceedings of the EMNLP conference,
pages 810–819.
Wu Wentao, Li Hongsong, Wang Haixun, and
Kenny. Q. Zhu. 2012. Probase: A probabilistic tax-
onomy for text understanding. Proceedings of the
ACM SIGMOD conference, pages 481–492.
Wilson Wong, Wei Liu, and Mohammed Bennamoun.
2007. Tree-traversing ant algorithm for term cluster-
ing based on featureless similarities. Data Mining
and Knowledge Discovery, 15(3):349–381.
Hui Yang and Jamie Callan. 2009. A Metric-
based Framework for Automatic Taxonomy Induc-
tion. Proceedings of the 47th Annual Meeting of the
ACL, pages 271–279.
Xingwei Zhu, Zhao Y. Ming, and Tat S. Chua. 2013.
Topic hierarchy construction for the organization of
multi-source user generated contents. Proceedings
of the 36th ACM SIGIR conference, pages 233–242.
</reference>
<page confidence="0.99411">
1022
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748663">
<title confidence="0.9982505">Incorporating Trustiness and Collective Synonym/Contrastive into Taxonomy Construction</title>
<author confidence="0.996513">Anh Tuan Jung-jae Kim Ng See Kiong</author>
<affiliation confidence="0.8930515">of Computer Engineering, Nanyang Technological University, for Infocomm Research, Agency for Science, Technology and Research,</affiliation>
<abstract confidence="0.998133807692308">Taxonomy plays an important role in many applications by organizing domain knowlinto a hierarchy of between terms. Previous works on the taxonomic relation identification from text corpora lack in two aspects: 1) They do not consider the trustiness of individual source texts, which is important to filter out incorrect relations from unreliable sources. 2) They also do not consider collective evidence from synonyms and contrastive terms, where synonyms may provide additional supports to taxonomic relations, while contrastive terms may contradict them. In this paper, we present a method of taxonomic relation identification that incorporates the trustiness of source texts measured with such techniques as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Julie Tibshirani</author>
<author>Jean Y Wu</author>
<author>Christopher D Manning</author>
</authors>
<title>Combining Distant and Partial Supervision for Relation Extraction.</title>
<date>2014</date>
<booktitle>Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>1556--1567</pages>
<contexts>
<context position="15075" citStr="Angeli et al. (2014)" startWordPosition="2462" endWordPosition="2465">ong et al. (2015), we estimate the accuracy of a data source by identifying correct and incorrect information in form of the triples (Subject, Predicate, Object) in the source, where Subject, Predicate and Object are normalized with regard to the knowledge base Freebase. The extraction of the triples includes six tasks: named entity recognition, part of speech tagging, dependency parsing, triple extraction, entity linkage (which maps mentions of proper nouns and their co-references to the corresponding entities in Freebase) and relation linkage. We use three information extraction (IE) tools (Angeli et al. (2014), Manning et al. (2014), MITIE3) for the first four tasks, and develop a method similar to Hachey et al. (2013) for the last two tasks of entity linkage and relation linkage. Since the IE tools may produce noisy or unreliable triples, we use a voting scheme for triple extraction as follows: A triple is only considered to be true if it is extracted by at least two extractors. After obtaining all triples in the data source, we use the closed world assumption as follows: Given subject s and predicate p, O(s, p) denotes the set of such objects that a triple (s,p,o) is found in Freebase. Now given </context>
</contexts>
<marker>Angeli, Tibshirani, Wu, Manning, 2014</marker>
<rawString>Gabor Angeli, Julie Tibshirani, Jean Y. Wu, and Christopher D. Manning. 2014. Combining Distant and Partial Supervision for Relation Extraction. Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 1556– 1567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>David Burkett</author>
<author>Gerard De Melo</author>
<author>Dan Klein</author>
</authors>
<title>Structured learning for taxonomy induction with belief propagation.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the ACL,</booktitle>
<pages>1041--1051</pages>
<marker>Bansal, Burkett, De Melo, Klein, 2014</marker>
<rawString>Mohit Bansal, David Burkett, Gerard De Melo, and Dan Klein. 2014. Structured learning for taxonomy induction with belief propagation. Proceedings of the 52nd Annual Meeting of the ACL, pages 1041– 1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>Proceedings of the ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="1806" citStr="Bollacker et al., 2008" startWordPosition="253" endWordPosition="256">idence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 1 Introduction Taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering (Harabagiu et al., 2003) and document clustering (Fodeh et al., 2011). Even though there are many hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syn</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. Proceedings of the ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin L Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Kevin Murphy</author>
<author>Wilko Horn Van Dang</author>
<author>Camillo Lugaresi</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources.</title>
<date>2015</date>
<journal>Proceedings of the VLDB Endowment,</journal>
<volume>8</volume>
<issue>9</issue>
<marker>Dong, Gabrilovich, Murphy, Van Dang, Lugaresi, Sun, Zhang, 2015</marker>
<rawString>Xin L. Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua Sun, and Wei Zhang. 2015. Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources. Proceedings of the VLDB Endowment, 8(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas Drumond</author>
<author>Rosario Girardi</author>
</authors>
<title>Extracting ontology concept hierarchies from text using markov logic.</title>
<date>2010</date>
<booktitle>Proceedings of the ACM Symposium on Applied Computing,</booktitle>
<pages>1354--1358</pages>
<contexts>
<context position="2451" citStr="Drumond and Girardi, 2010" startWordPosition="353" endWordPosition="356">te in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syntactic dependency (Drumond and Girardi, 2010), definition sentences (Navigli et al., 2011), co-occurrence (Zhu et al., 2013), syntactic contextual similarity (Tuan et al., 2014), and sibling relations (Bansal et al., 2014). They, however, lack in the three following aspects: 1) Trustiness: Not all sources are trustworthy (e.g. gossip, forum posts written by non-experts) (Dong et al., 2015). The trustiness of source texts is important in taxonomic relation identification because evidence from unreliable sources can be incorrect. For example, the invalid taxonomic relation between “American chameleon” and “chameleon” is mistakenly more pop</context>
</contexts>
<marker>Drumond, Girardi, 2010</marker>
<rawString>Lucas Drumond and Rosario Girardi. 2010. Extracting ontology concept hierarchies from text using markov logic. Proceedings of the ACM Symposium on Applied Computing, pages 1354–1358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samah Fodeh</author>
<author>Bill Punch</author>
<author>Pang N Tan</author>
</authors>
<title>On ontology-driven document clustering using core semantic features. Knowledge and information systems,</title>
<date>2011</date>
<pages>28--2</pages>
<contexts>
<context position="1612" citStr="Fodeh et al., 2011" startWordPosition="226" endWordPosition="229">sent a method of taxonomic relation identification that incorporates the trustiness of source texts measured with such techniques as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 1 Introduction Taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering (Harabagiu et al., 2003) and document clustering (Fodeh et al., 2011). Even though there are many hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captur</context>
</contexts>
<marker>Fodeh, Punch, Tan, 2011</marker>
<rawString>Samah Fodeh, Bill Punch, and Pang N. Tan. 2011. On ontology-driven document clustering using core semantic features. Knowledge and information systems, 28(2):395–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermine N Fotzo</author>
<author>Patrick Gallinari</author>
</authors>
<title>Learning “generalization/specialization” relations between concepts - application for automatically building thematic document hierarchies.</title>
<date>2004</date>
<booktitle>Proceedings of the 7th International Conference on Computer-Assisted Information Retrieval.</booktitle>
<contexts>
<context position="6493" citStr="Fotzo and Gallinari, 2004" startWordPosition="1003" endWordPosition="1007"> can be generally classified into two categories: linguistic and statistical approaches. The former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and li</context>
</contexts>
<marker>Fotzo, Gallinari, 2004</marker>
<rawString>Hermine N. Fotzo and Patrick Gallinari. 2004. Learning “generalization/specialization” relations between concepts - application for automatically building thematic document hierarchies. Proceedings of the 7th International Conference on Computer-Assisted Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Jiang Guo</author>
<author>Bing Qin</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning semantic hierarchies via word embeddings.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the ACL,</booktitle>
<pages>1199--1209</pages>
<contexts>
<context position="6616" citStr="Fu et al., 2014" startWordPosition="1024" endWordPosition="1027">syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and linguistic features in the process of finding taxonomic relations. Most of these previous methods do not consider if the sour</context>
</contexts>
<marker>Fu, Guo, Qin, Che, Wang, Liu, 2014</marker>
<rawString>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierarchies via word embeddings. Proceedings of the 52nd Annual Meeting of the ACL, pages 1199–1209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with Wikipedia. Artificial intelligence,</title>
<date>2013</date>
<pages>194--130</pages>
<contexts>
<context position="15186" citStr="Hachey et al. (2013)" startWordPosition="2483" endWordPosition="2486">n form of the triples (Subject, Predicate, Object) in the source, where Subject, Predicate and Object are normalized with regard to the knowledge base Freebase. The extraction of the triples includes six tasks: named entity recognition, part of speech tagging, dependency parsing, triple extraction, entity linkage (which maps mentions of proper nouns and their co-references to the corresponding entities in Freebase) and relation linkage. We use three information extraction (IE) tools (Angeli et al. (2014), Manning et al. (2014), MITIE3) for the first four tasks, and develop a method similar to Hachey et al. (2013) for the last two tasks of entity linkage and relation linkage. Since the IE tools may produce noisy or unreliable triples, we use a voting scheme for triple extraction as follows: A triple is only considered to be true if it is extracted by at least two extractors. After obtaining all triples in the data source, we use the closed world assumption as follows: Given subject s and predicate p, O(s, p) denotes the set of such objects that a triple (s,p,o) is found in Freebase. Now given a triple (s, p, o) found in the data source, if o E O(s, p), we conclude that the triple is correct; but if o E</context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with Wikipedia. Artificial intelligence, 194:130–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Steven J Maiorano</author>
<author>Marius A Pasca</author>
</authors>
<title>Open-domain textual question answering techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="1567" citStr="Harabagiu et al., 2003" startWordPosition="219" endWordPosition="222"> terms may contradict them. In this paper, we present a method of taxonomic relation identification that incorporates the trustiness of source texts measured with such techniques as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 1 Introduction Taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering (Harabagiu et al., 2003) and document clustering (Fodeh et al., 2011). Even though there are many hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, ran</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>Sanda M. Harabagiu, Steven J. Maiorano, and Marius A. Pasca. 2003. Open-domain textual question answering techniques. Natural Language Engineering, 9(3):231–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome H Friedman</author>
</authors>
<title>The elements of statistical learning.</title>
<date>2009</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="16545" citStr="Hastie et al., 2009" startWordPosition="2748" endWordPosition="2751">triple is removed from the set of facts found in the data source. Given a data source d, we define cf(d) as the number of correct facts, and icf(d) as the number of incorrect facts found in d. The accuracy of d is calculated as follows: 1 1 TrustAccu(d) = 1 + icf(d)2 −1 + cf(d)2 (5) Combining trustiness scores The final trustiness score of a data source is the linear combination of the four scores as follows: Trust(d) = α × TrustImp(d) + β × TrustPop(d) + γ × TrustAuth(d) + δ × TrustAccu(d) To estimate the optimal combination for parameters α, β, γ and δ, we apply linear regression algorithm (Hastie et al., 2009). For parameter learning, we manually list 50 websites as trusted sources (e.g. stanford.edu, bbc.com, nasa.gov), and the top 15 gossip websites listed in a site4 as untrusted sources. Then we use the scores of their individual pages by the four methods to learn the parameters in Formula 6. The learning results are as follows: α=0.46, β=0.46, γ=2.03, δ=0.61. 3.2.2 Integrating trustiness into taxonomic relation identification methods Given a data collection C, we define: AvgTrust(C) = |C| 3https://github.com/mit-nlp/MITIE 4http://www.ebizmba.com/articles/gossip-websites (6) E d∈C Trust(d) 1016 </context>
<context position="19729" citStr="Hastie et al., 2009" startWordPosition="3276" endWordPosition="3279">2 • t1, or t2, is a • t1 (short for t2) , where [a|b] denotes a choice between a and b. If the number of Web search results is greater than a threshold Ψ, t1 is considered as a synonym of t2. Supervised learning: We randomly pick 100 pairs of synonyms in WordNet, and for each pair, we use the Web search engine to collect sample sentences in which both terms of the pair are mentioned. If the number of collected sentences is greater than 2000, we use only the first 2000 sentences for training. After that, we extract the following features from the sentences to train a logistic regression model (Hastie et al., 2009) for the synonymy identification: • Headwords of the two terms • Average distance between the terms • Sequence of words between the terms • Bag of words between the terms • Dependency path between the terms (using Stanford parser (Klein and Manning, 2003)) • Bag of words on the dependency path The average F-measure of the obtained model with 10-fold cross-validation is 81%. We use the learned model to identify more synonym pairs in the next step. 3.3.2 Embedding synonym information Given a term t, we denote Syn(t) as the set of synonyms of t (including t itself). The evidence scores of the SCS</context>
<context position="22466" citStr="Hastie et al., 2009" startWordPosition="3754" endWordPosition="3757">Synonym X (t1, t2) , where the variable X can be replaced with each of the three taxonomic relation evidence measures (i.e. SCS, LSP, SIWN). The final combined score is calculated as follows: ScoreFinal Combined(t1, t2) = α × ScoreFinal SIW N(t1, t2) + β × ScoreF inal LSP (t1, t2) + γ × ScoreF inal SCS (t1, t2) − δ × ScoreContrast(t1, t2) For each ordered pair of terms t1 and t2, if ScoreF inal Combined(t1, t2) is greater than a threshold value, then t1 is considered as a hypernym of t2. We estimate the optimal values of parameters α, β, -y and 6 in Formula 15 with ridge regression technique (Hastie et al., 2009) as follows: First, we randomly select 100 taxonomic relations in Animal domain as the training set. For each taxonomic relation t1 » t2, its evidence score is estimated as T + Dist(t1,t2), where T is the threshold value for ScoreFinal Combined, and Dist(t1, t2) is the length of the shortest path between t1 and t2 found in WordNet. Then we use our system to find evidence scores with taxonomic relation identification methods in Formulas 13 and 14. Finally, we build the training set using Formula 15, and use the ridge regression algorithm to learn that the best value for α is 1.31, β 1.57, -y 1.</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. 2009. The elements of statistical learning. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="6062" citStr="Hearst, 1992" startWordPosition="939" endWordPosition="940">e evidence score of the candidate taxonomic relation between t1 and t2. Similarly, for each pair of two terms (t1, t2), we collect their contrastive evidence by matching such queries as “t1 is not a type of t2” against the Web, and use them to proportionally decrease the evidence score for taxonomic relation between contrasting terms. 2 Related Work The previous methods for identifying taxonomic relations from text can be generally classified into two categories: linguistic and statistical approaches. The former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is tha</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. Proceedings of the 14th Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung J Kim</author>
<author>Zhuo Zhang</author>
<author>Jong C Park</author>
<author>See K Ng</author>
</authors>
<title>Biocontrasts: extracting and exploiting protein-protein contrastive relations from biomedical literature.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="3943" citStr="Kim et al., 2006" startWordPosition="586" endWordPosition="589"> considered this aspect. 2) Synonyms: A concept may be expressed in multiple ways, for example with synonyms. The previous works mostly assumed that a term represents an independent concept, and did not combine information about a concept, which is expressed with multiple synonyms. The lack of evidence from synonyms may hamper the ranking of candidate taxonomic relations. Navigli and Velardi (2004) combined synonyms into a concept, but only for those from WordNet, called synsets. 3) Contrastive terms: We observe that if two terms are often contrasted (e.g. A but not B, A is different from B) (Kim et al., 2006), they may 1013 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1013–1022, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. not have a taxonomic relation. In this paper, we present a method based on the state-of-the-art method (Tuan et al., 2014), which incorporates the trustiness of source texts and the collective evidence from synonyms/contrastive terms. Tuan et al. (2014) rank candidate taxonomic relations based on such evidence as hypernym patterns, WordNet, and syntactic contextual similarity, where the</context>
</contexts>
<marker>Kim, Zhang, Park, Ng, 2006</marker>
<rawString>Jung J. Kim, Zhuo Zhang, Jong C. Park, and See K. Ng. 2006. Biocontrasts: extracting and exploiting protein-protein contrastive relations from biomedical literature. Bioinformatics, 22(5):597–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="19984" citStr="Klein and Manning, 2003" startWordPosition="3319" endWordPosition="3322"> in WordNet, and for each pair, we use the Web search engine to collect sample sentences in which both terms of the pair are mentioned. If the number of collected sentences is greater than 2000, we use only the first 2000 sentences for training. After that, we extract the following features from the sentences to train a logistic regression model (Hastie et al., 2009) for the synonymy identification: • Headwords of the two terms • Average distance between the terms • Sequence of words between the terms • Bag of words between the terms • Dependency path between the terms (using Stanford parser (Klein and Manning, 2003)) • Bag of words on the dependency path The average F-measure of the obtained model with 10-fold cross-validation is 81%. We use the learned model to identify more synonym pairs in the next step. 3.3.2 Embedding synonym information Given a term t, we denote Syn(t) as the set of synonyms of t (including t itself). The evidence scores of the SCS and LSP methods are recalculated with synonyms as follows: � ScoreX(t�1, t2) (11) ScoreSynonym X (t1, t2) = tiESyn(t1) t2ESyn(t2) , where the variable X can be replaced with SCS and LSP. 5http://www.ncbi.nlm.nih.gov/mesh 1017 The intuition of Formula (11</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Annual Meeting of the ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>A Semisupervised Method to Learn and Construct Taxonomies Using the Web.</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1110--1118</pages>
<contexts>
<context position="24282" citStr="Kozareva and Hovy, 2010" startWordPosition="4058" endWordPosition="4061">engine 6 and retrieve the first 20,000 abstracts as the corpus of the virus domain. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). We report the results of two experiments in this section: (1) Evaluating the construction of new taxonomies for Finance and AI domains (Section 4.2), and (2) comparing with the curated 6http://www.ncbi.nlm.nih.gov/pubmed ScoreContrast(t1, t2) = 1018 databases’ sub-hierarchies. We compare our approach with other three state-of-the-art methods in the literature, i.e. (Kozareva and Hovy, 2010), (Navigli et al., 2011) and (Tuan et al., 2014) (Section 4.3). In addition, for Animal domain, we also compare with the reported performance of Bansal et al. (2014), a recent work to construct taxonomy using belief propagation. 4.2 Constructing new taxonomies for Finance and AI domains Referential taxonomy structures such as WordNet and OpenCyc are widely used in semantic analytics applications. However, their coverage is limited to common, well-known areas, and many specific domains like Finance and AI are not well covered in those structures. Therefore, an automatic method which can induce </context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. A Semisupervised Method to Learn and Construct Taxonomies Using the Web. Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1110–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard H Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the ACL,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="2401" citStr="Kozareva et al., 2008" startWordPosition="347" endWordPosition="350">se (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syntactic dependency (Drumond and Girardi, 2010), definition sentences (Navigli et al., 2011), co-occurrence (Zhu et al., 2013), syntactic contextual similarity (Tuan et al., 2014), and sibling relations (Bansal et al., 2014). They, however, lack in the three following aspects: 1) Trustiness: Not all sources are trustworthy (e.g. gossip, forum posts written by non-experts) (Dong et al., 2015). The trustiness of source texts is important in taxonomic relation identification because evidence from unreliable sources can be incorrect. For example, the invalid taxonomic relation between “American</context>
<context position="6125" citStr="Kozareva et al., 2008" startWordPosition="948" endWordPosition="951">tween t1 and t2. Similarly, for each pair of two terms (t1, t2), we collect their contrastive evidence by matching such queries as “t1 is not a type of t2” against the Web, and use them to proportionally decrease the evidence score for taxonomic relation between contrasting terms. 2 Related Work The previous methods for identifying taxonomic relations from text can be generally classified into two categories: linguistic and statistical approaches. The former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with </context>
<context position="23887" citStr="Kozareva et al. (2008)" startWordPosition="4002" endWordPosition="4005">pus consists of 4,976 papers extracted from the IJCAI proceedings from 1969 to 2014 and the ACL archives from year 1979 to 2014. • Finance domain: The corpus consists of 1,253 papers extracted from the freely available collection of “Journal of Financial Economics” from 1995 to 2012 and from “Review Of Finance” from 1997 to 2012. • Virus domain: We submit the query “virus” to PUBMED search engine 6 and retrieve the first 20,000 abstracts as the corpus of the virus domain. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). We report the results of two experiments in this section: (1) Evaluating the construction of new taxonomies for Finance and AI domains (Section 4.2), and (2) comparing with the curated 6http://www.ncbi.nlm.nih.gov/pubmed ScoreContrast(t1, t2) = 1018 databases’ sub-hierarchies. We compare our approach with other three state-of-the-art methods in the literature, i.e. (Kozareva and Hovy, 2010), (Navigli et al., 2011) and (Tuan et al., 2014) (Section 4.3). In addition, for Animal domain, we also compare with the reported performance of Bansal et al. (2014), a recent work to construct taxonomy us</context>
<context position="25280" citStr="Kozareva et al. (2008)" startWordPosition="4219" endWordPosition="4222">analytics applications. However, their coverage is limited to common, well-known areas, and many specific domains like Finance and AI are not well covered in those structures. Therefore, an automatic method which can induce taxonomies for those specific domains can greatly contribute to the process of knowledge discovery. To estimate the precision of a given method, we randomly choose 100 relations among the results of the method and manually check their correctness. The results summarized in Table 1 show that our method extracts much more relations, though with slightly lower precision, than Kozareva et al. (2008) and Navigli and Velardi (2004). Note that due to the lack of gold standards in these two domains, we do not compare the methods in terms of F-score, which we will measure with curated databases in the next section. Compared to Tuan et al. (2014), which can be considered as the baseline of our approach, our method has significant improvement in both precision and the number of extracted relations. It indicates that the three incorporated features of trustiness, and synonym and contrastive evidence are effective in improving the performance of existing taxonomy construction methods. Finance P A</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. Proceedings of the 46th Annual Meeting of the ACL, pages 1048– 1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the ACL,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="15098" citStr="Manning et al. (2014)" startWordPosition="2466" endWordPosition="2469">estimate the accuracy of a data source by identifying correct and incorrect information in form of the triples (Subject, Predicate, Object) in the source, where Subject, Predicate and Object are normalized with regard to the knowledge base Freebase. The extraction of the triples includes six tasks: named entity recognition, part of speech tagging, dependency parsing, triple extraction, entity linkage (which maps mentions of proper nouns and their co-references to the corresponding entities in Freebase) and relation linkage. We use three information extraction (IE) tools (Angeli et al. (2014), Manning et al. (2014), MITIE3) for the first four tasks, and develop a method similar to Hachey et al. (2013) for the last two tasks of entity linkage and relation linkage. Since the IE tools may produce noisy or unreliable triples, we use a voting scheme for triple extraction as follows: A triple is only considered to be true if it is extracted by at least two extractors. After obtaining all triples in the data source, we use the closed world assumption as follows: Given subject s and predicate p, O(s, p) denotes the set of such objects that a triple (s,p,o) is found in Freebase. Now given a triple (s, p, o) foun</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. Proceedings of the 52nd Annual Meeting of the ACL, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>John Cabral</author>
<author>Michael J Witbrock</author>
<author>John DeOliveira</author>
</authors>
<title>An introduction to the syntax and content of cyc.</title>
<date>2006</date>
<booktitle>Proceedings of the AAAI Spring Symposium,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="1767" citStr="Matuszek et al., 2006" startWordPosition="246" endWordPosition="250">dge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 1 Introduction Taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering (Harabagiu et al., 2003) and document clustering (Fodeh et al., 2011). Even though there are many hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A </context>
</contexts>
<marker>Matuszek, Cabral, Witbrock, DeOliveira, 2006</marker>
<rawString>Cynthia Matuszek, John Cabral, Michael J. Witbrock, and John DeOliveira. 2006. An introduction to the syntax and content of cyc. Proceedings of the AAAI Spring Symposium, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1734" citStr="Miller, 1995" startWordPosition="243" endWordPosition="244">s as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 1 Introduction Taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering (Harabagiu et al., 2003) and document clustering (Fodeh et al., 2011). Even though there are many hand-crafted, well-structured taxonomies publicly available, including WordNet (Miller, 1995), OpenCyc (Matuszek et al., 2006), and Freebase (Bollacker et al., 2008), they are incomplete in specific domains, and it is time-consuming to manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hyp</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a Lexical Database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="3727" citStr="Navigli and Velardi (2004)" startWordPosition="547" endWordPosition="550">between “American chameleon” and “lizard”, and statistical methods without considering the trustiness may incorrectly extract the invalid relation instead of the latter. However, to the best of our knowledge, no previous work considered this aspect. 2) Synonyms: A concept may be expressed in multiple ways, for example with synonyms. The previous works mostly assumed that a term represents an independent concept, and did not combine information about a concept, which is expressed with multiple synonyms. The lack of evidence from synonyms may hamper the ranking of candidate taxonomic relations. Navigli and Velardi (2004) combined synonyms into a concept, but only for those from WordNet, called synsets. 3) Contrastive terms: We observe that if two terms are often contrasted (e.g. A but not B, A is different from B) (Kim et al., 2006), they may 1013 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1013–1022, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. not have a taxonomic relation. In this paper, we present a method based on the state-of-the-art method (Tuan et al., 2014), which incorporates the trustiness of source texts</context>
<context position="8188" citStr="Navigli and Velardi (2004)" startWordPosition="1283" endWordPosition="1286">h incrementally clusters terms to form taxonomies. Wentao et al. (2012) also utilize such sibling feature that, for example of the linguistic pattern “A such as B1, B2, · · · and Bn”, if the concept at the k-th position (e.g. Bk) from pattern keywords (e.g. such as) is a valid sub-concept (e.g. of A), then most likely its siblings from position 1 to position k-1 (e.g. B1, · · · , Bk−1) are also valid sub-concepts. Bansal et al. (2014) include the sibling factors to a structured probabilistic model over the full space of taxonomy trees, thus helping to add more evidence to taxonomic relations. Navigli and Velardi (2004) utilize the synonym feature (i.e. WordNet synsets) for the process of semantic disambiguation and concept clustering as mentioned above, but not for the process of inducing novel taxonomic relations. 3 Methodology We briefly introduce (Tuan et al., 2014) in Section 3.1. We then explain how to incorporate trustiness (Section 3.2) and collective evidence from synonyms (Section 3.3) and from contrastive terms (Section 3.4) into the work of (Tuan et al., 2014). 3.1 Overview of baseline (Tuan et al., 2014) Tuan et al. (2014) follow three steps to construct a taxonomy: term extraction/filtering, ta</context>
<context position="25311" citStr="Navigli and Velardi (2004)" startWordPosition="4224" endWordPosition="4227">ever, their coverage is limited to common, well-known areas, and many specific domains like Finance and AI are not well covered in those structures. Therefore, an automatic method which can induce taxonomies for those specific domains can greatly contribute to the process of knowledge discovery. To estimate the precision of a given method, we randomly choose 100 relations among the results of the method and manually check their correctness. The results summarized in Table 1 show that our method extracts much more relations, though with slightly lower precision, than Kozareva et al. (2008) and Navigli and Velardi (2004). Note that due to the lack of gold standards in these two domains, we do not compare the methods in terms of F-score, which we will measure with curated databases in the next section. Compared to Tuan et al. (2014), which can be considered as the baseline of our approach, our method has significant improvement in both precision and the number of extracted relations. It indicates that the three incorporated features of trustiness, and synonym and contrastive evidence are effective in improving the performance of existing taxonomy construction methods. Finance P AI P N N Kozareva 90% 753 94% 95</context>
</contexts>
<marker>Navigli, Velardi, 2004</marker>
<rawString>Roberto Navigli and Paola Velardi. 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites. Computational Linguistics, 30(2):151–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A Graph-based Algorithm for Inducing Lexical Taxonomies from Scratch.</title>
<date>2011</date>
<booktitle>Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1872--1877</pages>
<contexts>
<context position="2496" citStr="Navigli et al., 2011" startWordPosition="359" endWordPosition="362">o manually extend them or create new ones. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syntactic dependency (Drumond and Girardi, 2010), definition sentences (Navigli et al., 2011), co-occurrence (Zhu et al., 2013), syntactic contextual similarity (Tuan et al., 2014), and sibling relations (Bansal et al., 2014). They, however, lack in the three following aspects: 1) Trustiness: Not all sources are trustworthy (e.g. gossip, forum posts written by non-experts) (Dong et al., 2015). The trustiness of source texts is important in taxonomic relation identification because evidence from unreliable sources can be incorrect. For example, the invalid taxonomic relation between “American chameleon” and “chameleon” is mistakenly more popular in the Web than the valid taxonomic rela</context>
<context position="6197" citStr="Navigli et al., 2011" startWordPosition="959" endWordPosition="962">ct their contrastive evidence by matching such queries as “t1 is not a type of t2” against the Web, and use them to proportionally decrease the evidence score for taxonomic relation between contrasting terms. 2 Related Work The previous methods for identifying taxonomic relations from text can be generally classified into two categories: linguistic and statistical approaches. The former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent v</context>
<context position="24306" citStr="Navigli et al., 2011" startWordPosition="4062" endWordPosition="4065">irst 20,000 abstracts as the corpus of the virus domain. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). We report the results of two experiments in this section: (1) Evaluating the construction of new taxonomies for Finance and AI domains (Section 4.2), and (2) comparing with the curated 6http://www.ncbi.nlm.nih.gov/pubmed ScoreContrast(t1, t2) = 1018 databases’ sub-hierarchies. We compare our approach with other three state-of-the-art methods in the literature, i.e. (Kozareva and Hovy, 2010), (Navigli et al., 2011) and (Tuan et al., 2014) (Section 4.3). In addition, for Animal domain, we also compare with the reported performance of Bansal et al. (2014), a recent work to construct taxonomy using belief propagation. 4.2 Constructing new taxonomies for Finance and AI domains Referential taxonomy structures such as WordNet and OpenCyc are widely used in semantic analytics applications. However, their coverage is limited to common, well-known areas, and many specific domains like Finance and AI are not well covered in those structures. Therefore, an automatic method which can induce taxonomies for those spe</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A Graph-based Algorithm for Inducing Lexical Taxonomies from Scratch. Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1872–1877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luu A Tuan</author>
<author>Jung J Kim</author>
<author>See K Ng</author>
</authors>
<title>Taxonomy Construction using Syntactic Contextual Evidence.</title>
<date>2014</date>
<booktitle>Proceedings of the EMNLP conference,</booktitle>
<pages>810--819</pages>
<contexts>
<context position="2583" citStr="Tuan et al., 2014" startWordPosition="371" endWordPosition="374">ng taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syntactic dependency (Drumond and Girardi, 2010), definition sentences (Navigli et al., 2011), co-occurrence (Zhu et al., 2013), syntactic contextual similarity (Tuan et al., 2014), and sibling relations (Bansal et al., 2014). They, however, lack in the three following aspects: 1) Trustiness: Not all sources are trustworthy (e.g. gossip, forum posts written by non-experts) (Dong et al., 2015). The trustiness of source texts is important in taxonomic relation identification because evidence from unreliable sources can be incorrect. For example, the invalid taxonomic relation between “American chameleon” and “chameleon” is mistakenly more popular in the Web than the valid taxonomic relation between “American chameleon” and “lizard”, and statistical methods without conside</context>
<context position="4276" citStr="Tuan et al., 2014" startWordPosition="634" endWordPosition="637"> ranking of candidate taxonomic relations. Navigli and Velardi (2004) combined synonyms into a concept, but only for those from WordNet, called synsets. 3) Contrastive terms: We observe that if two terms are often contrasted (e.g. A but not B, A is different from B) (Kim et al., 2006), they may 1013 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1013–1022, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. not have a taxonomic relation. In this paper, we present a method based on the state-of-the-art method (Tuan et al., 2014), which incorporates the trustiness of source texts and the collective evidence from synonyms/contrastive terms. Tuan et al. (2014) rank candidate taxonomic relations based on such evidence as hypernym patterns, WordNet, and syntactic contextual similarity, where the pattern matches and the syntactic contexts are found from the Web by using a Web search engine. First, we calculate the trustiness score of each data source with the four following weights: importance (if it is linked by many pages), popularity (if it is visited by many users), authority (if it is from a creditable Web site) and a</context>
<context position="6578" citStr="Tuan et al., 2014" startWordPosition="1017" endWordPosition="1020">former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and linguistic features in the process of finding taxonomic relations. Most of these previo</context>
<context position="8443" citStr="Tuan et al., 2014" startWordPosition="1322" endWordPosition="1325">as) is a valid sub-concept (e.g. of A), then most likely its siblings from position 1 to position k-1 (e.g. B1, · · · , Bk−1) are also valid sub-concepts. Bansal et al. (2014) include the sibling factors to a structured probabilistic model over the full space of taxonomy trees, thus helping to add more evidence to taxonomic relations. Navigli and Velardi (2004) utilize the synonym feature (i.e. WordNet synsets) for the process of semantic disambiguation and concept clustering as mentioned above, but not for the process of inducing novel taxonomic relations. 3 Methodology We briefly introduce (Tuan et al., 2014) in Section 3.1. We then explain how to incorporate trustiness (Section 3.2) and collective evidence from synonyms (Section 3.3) and from contrastive terms (Section 3.4) into the work of (Tuan et al., 2014). 3.1 Overview of baseline (Tuan et al., 2014) Tuan et al. (2014) follow three steps to construct a taxonomy: term extraction/filtering, taxonomic relation identification and taxonomy induction. Because the focus of this paper is on the second step, 1014 taxonomic relation identification, we use the same methods for the first and third steps as in (Tuan et al., 2014) and will not discuss the</context>
<context position="10779" citStr="Tuan et al., 2014" startWordPosition="1727" endWordPosition="1730">c contexts, particularly from the triples of (subject,verb,object). They observe that if the context set of t1 mostly contains that of t2 but not vice versa, then t1 is likely to be a hypernym of t2. To implement this idea, they first find the most common relation (or verb) r between t1 and t2, and use the queries “t1 r” and “t2 r” to construct two corpora CorpusΓt1 and CorpusΓt2 for t1 and t2, respectively. Then the syntactic context sets are created from these contextual corpora using a nontaxonomic relation identification method. The details of calculating ScoreSCS(t1, t2) can be found in (Tuan et al., 2014). They linearly combine the three scores as follows: Score(t1, t2) = α x ScoreSIWN(t1, t2) + β x ScoreLSP(t1, t2) + γ x ScoreSCS(t1, t2) (1) If Score(t1, t2) is greater than a threshold value, then t1 is regarded as a hypernym of t2. We use the same values of α, Q and γ as in (Tuan et al., 2014). 3.2 Trustiness of the evidence data We introduce our method of estimating the trustiness of a given source text in Section 3.2.1 and explain how to incorporate it into the work of (Tuan et al., 2014) in Section 3.2.2. 3.2.1 Collecting trustiness score of the evidence data Given a data source (e.g. Web</context>
<context position="21791" citStr="Tuan et al., 2014" startWordPosition="3631" endWordPosition="3634"> t2 WH(t1, t2) denotes the total number of Web search results, and the contrastive evidence score between t1 and t2 is computed as follows: Contrast(t1, t2) = log(WH(t1, t2) + 1) (12) Similar to the collective synonym evidence, the contrastive evidence score of taxonomic relation between t1 and t2 is boosted with the contrastive evidence scores of taxonomic relations between the two terms and their synonyms as follows: E tiESyn(t1) Contrast(t�1, t2) t2ESyn(t2) |Syn(t1) |∗ |Syn(t2)| 3.5 Combining trustiness, synonym and contrastive evidence We combine all the three features into the system of (Tuan et al., 2014) as follows: Final X(t1,t2)=ScoreTXrust(t1, t2) + ScoreSynonym X (t1, t2) , where the variable X can be replaced with each of the three taxonomic relation evidence measures (i.e. SCS, LSP, SIWN). The final combined score is calculated as follows: ScoreFinal Combined(t1, t2) = α × ScoreFinal SIW N(t1, t2) + β × ScoreF inal LSP (t1, t2) + γ × ScoreF inal SCS (t1, t2) − δ × ScoreContrast(t1, t2) For each ordered pair of terms t1 and t2, if ScoreF inal Combined(t1, t2) is greater than a threshold value, then t1 is considered as a hypernym of t2. We estimate the optimal values of parameters α, β, -</context>
<context position="24330" citStr="Tuan et al., 2014" startWordPosition="4067" endWordPosition="4070">e corpus of the virus domain. • Animals, Plants and Vehicles domains: Collections of Web pages crawled by using the bootstrapping algorithm described by Kozareva et al. (2008). We report the results of two experiments in this section: (1) Evaluating the construction of new taxonomies for Finance and AI domains (Section 4.2), and (2) comparing with the curated 6http://www.ncbi.nlm.nih.gov/pubmed ScoreContrast(t1, t2) = 1018 databases’ sub-hierarchies. We compare our approach with other three state-of-the-art methods in the literature, i.e. (Kozareva and Hovy, 2010), (Navigli et al., 2011) and (Tuan et al., 2014) (Section 4.3). In addition, for Animal domain, we also compare with the reported performance of Bansal et al. (2014), a recent work to construct taxonomy using belief propagation. 4.2 Constructing new taxonomies for Finance and AI domains Referential taxonomy structures such as WordNet and OpenCyc are widely used in semantic analytics applications. However, their coverage is limited to common, well-known areas, and many specific domains like Finance and AI are not well covered in those structures. Therefore, an automatic method which can induce taxonomies for those specific domains can greatl</context>
<context position="27595" citStr="Tuan et al. (2014)" startWordPosition="4597" endWordPosition="4600">ntroduced features (i.e. trustiness, synonym, contrast), we also evaluate our method only with one of the three features each time, as well as with all the three features (denoted as “Combined”). Tables 2 and 3 summarize the experiment results. Our combined method achieves significantly better performance than the previous state-of-theart methods in terms of F-measure and Recall (ttest, p-value &lt; 0.05) for all the four domains. For Animal domain, it also shows higher performance than the reported performance of Bansal et al. (2014). In addition, the proposed method improves the baseline (i.e. Tuan et al. (2014)) up to 4%-10% of F-measure. Furthermore, we find that the three features have different contribution to the performance improvement. The trustiness feature contributes to the improvement on both precision and recall. The synonym feature has the tendency of improving the recall further than the trustiness, whereas the contrastive evidence improves the precision. Note that we discussed these different contributions of the features in the Introduction. Animal P Plant F P R F R Kozareva 98 38 55 97 39 56 Navigli 97 44 61 97 38 55 Bansal 84 55 67 Tuan 95 56 70 95 53 68 Trustiness 97 61 75 97 56 71</context>
<context position="31156" citStr="Tuan et al. (2014)" startWordPosition="5216" endWordPosition="5219">Trustiness: Importance 74% 70% 81% 63% Popularity 72% 69% 81% 61% Authority 72% 69% 80% 61% Accuracy 73% 70% 81% 62% Imp + Accu 75% 70% 82% 64% Synonym: Dictionaries 73% 69% 79% 62% Pattern matching 74% 69% 80% 64% Machine learning 74% 70% 80% 65% Table 4: Contribution of individual trustiness measures and collective synonym evidence in terms of F-measure. Imp stands for Important and Accu stands for Accuracy 4.4 Discussion 4.4.1 Case studies We give two examples to illustrate how the proposed features help to infer correct taxonomic relations and filter out incorrect relations. Our baseline (Tuan et al. (2014)) extracts an incorrect taxonomic relation between ‘fox’ and ‘flying fox’ due to the following reasons: (1) ‘flying fox’ includes ‘fox’ (SIWN) and (2) untrusted sources such as a public forum7 support the relation. Using our proposed method, this relation is filtered out because those untrusted sources are discouraged by the trustiness feature, and also because there are contrastive evidence8 saying that ‘flying fox’ is NOT a ‘fox’. Specifically, the average trustiness score of LSP method of the sources for the invalid relation (i.e. AvgTrust(CW b(fox, flying fox)) + AvgTrust(CW b(flying fox, </context>
</contexts>
<marker>Tuan, Kim, Ng, 2014</marker>
<rawString>Luu A. Tuan, Jung J. Kim, and See K. Ng. 2014. Taxonomy Construction using Syntactic Contextual Evidence. Proceedings of the EMNLP conference, pages 810–819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhu</author>
</authors>
<title>Probase: A probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>Proceedings of the ACM SIGMOD conference,</booktitle>
<pages>481--492</pages>
<marker>Zhu, 2012</marker>
<rawString>Wu Wentao, Li Hongsong, Wang Haixun, and Kenny. Q. Zhu. 2012. Probase: A probabilistic taxonomy for text understanding. Proceedings of the ACM SIGMOD conference, pages 481–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Wong</author>
<author>Wei Liu</author>
<author>Mohammed Bennamoun</author>
</authors>
<title>Tree-traversing ant algorithm for term clustering based on featureless similarities.</title>
<date>2007</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="6525" citStr="Wong et al., 2007" startWordPosition="1009" endWordPosition="1012">tegories: linguistic and statistical approaches. The former approach mostly exploits lexical-syntactic patterns (e.g. A is a B, A such as B) (Hearst, 1992). Those patterns can be manually created (Kozareva et al., 2008; Wentao et al., 2012) or automatically identified (Navigli et al., 2011; Bansal et al., 2014). The pattern matching methods show high precision when the patterns are carefully defined, but low coverage due to the lack of contextual analysis across sentences. The latter approach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and linguistic features in the process</context>
</contexts>
<marker>Wong, Liu, Bennamoun, 2007</marker>
<rawString>Wilson Wong, Wei Liu, and Mohammed Bennamoun. 2007. Tree-traversing ant algorithm for term clustering based on featureless similarities. Data Mining and Knowledge Discovery, 15(3):349–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A Metricbased Framework for Automatic Taxonomy Induction.</title>
<date>2009</date>
<booktitle>Proceedings of the 47th Annual Meeting of the ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="7019" citStr="Yang and Callan (2009)" startWordPosition="1084" endWordPosition="1087">pproach, on the other hand, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and linguistic features in the process of finding taxonomic relations. Most of these previous methods do not consider if the source text of evidence (e.g. co-occurrences, pattern matches) is trustworthy or not and do not combine evidence from synonyms and contrastive terms as discussed earlier. Related to synonyms, a few previous works utilize siblings for taxonomy construction. Yang and Callan (2009) use siblings as one of the features in the metric-based framework which incrementally clusters terms to form taxonomies. Wentao</context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A Metricbased Framework for Automatic Taxonomy Induction. Proceedings of the 47th Annual Meeting of the ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingwei Zhu</author>
<author>Zhao Y Ming</author>
<author>Tat S Chua</author>
</authors>
<title>Topic hierarchy construction for the organization of multi-source user generated contents.</title>
<date>2013</date>
<booktitle>Proceedings of the 36th ACM SIGIR conference,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="2530" citStr="Zhu et al., 2013" startWordPosition="364" endWordPosition="367">nes. There is thus a need for automatically extracting taxonomic relations from text corpora to construct/extend taxonomies. Previous works on the task of taxonomy construction capture information about potential taxonomic relations between concepts, rank the candidate relations based on the captured information, and integrate the highly ranked relations into a taxonomic structure. They utilize such information as hypernym patterns (e.g. A is a B, A such as B) (Kozareva et al., 2008), syntactic dependency (Drumond and Girardi, 2010), definition sentences (Navigli et al., 2011), co-occurrence (Zhu et al., 2013), syntactic contextual similarity (Tuan et al., 2014), and sibling relations (Bansal et al., 2014). They, however, lack in the three following aspects: 1) Trustiness: Not all sources are trustworthy (e.g. gossip, forum posts written by non-experts) (Dong et al., 2015). The trustiness of source texts is important in taxonomic relation identification because evidence from unreliable sources can be incorrect. For example, the invalid taxonomic relation between “American chameleon” and “chameleon” is mistakenly more popular in the Web than the valid taxonomic relation between “American chameleon” </context>
<context position="7038" citStr="Zhu et al. (2013)" startWordPosition="1088" endWordPosition="1091">nd, includes asymmetrical term co-occurrence (Fotzo and Gallinari, 2004), clustering (Wong et al., 2007), syntactic contextual similarity (Tuan et al., 2014), and word embedding (Fu et al., 2014). The main idea behind these techniques is that the terms that are asymmetrically similar to each other with regard to, for example, co-occurrences, syntactic contexts, and latent vector representation may have taxonomic relationships. Such methods, however, usually suffer from low accuracy, though showing relatively high coverage. To get the balance between the two approaches, Yang and Callan (2009), Zhu et al. (2013) and Tuan et al. (2014) combine both statistical and linguistic features in the process of finding taxonomic relations. Most of these previous methods do not consider if the source text of evidence (e.g. co-occurrences, pattern matches) is trustworthy or not and do not combine evidence from synonyms and contrastive terms as discussed earlier. Related to synonyms, a few previous works utilize siblings for taxonomy construction. Yang and Callan (2009) use siblings as one of the features in the metric-based framework which incrementally clusters terms to form taxonomies. Wentao et al. (2012) also</context>
</contexts>
<marker>Zhu, Ming, Chua, 2013</marker>
<rawString>Xingwei Zhu, Zhao Y. Ming, and Tat S. Chua. 2013. Topic hierarchy construction for the organization of multi-source user generated contents. Proceedings of the 36th ACM SIGIR conference, pages 233–242.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007272">
<title confidence="0.991667">
A Discriminative Training Procedure for Continuous Translation Models
</title>
<note confidence="0.531182666666667">
† *Quoc-Khanh Do, † *Alexandre Allauzen and *Franc¸ois Yvon
† Universit´e Paris-Sud, Orsay, France
* LIMSI/CNRS, Orsay, France
</note>
<email confidence="0.923591">
firstname.surname@limsi.fr
</email>
<sectionHeader confidence="0.996519" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999693071428571">
Continuous-space translation models have
recently emerged as extremely powerful
ways to boost the performance of existing
translation systems. A simple, yet effec-
tive way to integrate such models in infer-
ence is to use them in an N-best rescor-
ing step. In this paper, we focus on this
scenario and show that the performance
gains in rescoring can be greatly increased
when the neural network is trained jointly
with all the other model parameters, using
an appropriate objective function. Our ap-
proach is validated on two domains, where
it outperforms strong baselines.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995456327272728">
Over the past few years, research on neural net-
works (NN) architectures for Natural Language
Processing has been rejuvenated. Boosted by early
successes in language modelling for speech recog-
nition (Schwenk, 2007; Le et al., 2011), NNs have
since been successufully applied to many other
tasks (Socher et al., 2013; Huang et al., 2012;
Yang et al., 2013). In particular, these techniques
have been applied to Statistical Machine Trans-
lation (SMT), first to estimate continuous-space
translation models (CTMs) (Schwenk et al., 2007;
Le et al., 2012; Devlin et al., 2014), and more
recently to implement end-to-end translation sys-
tems (Cho et al., 2014; Sutskever et al., 2014).
In most SMT settings, CTMs are used as an ad-
ditional feature function in the log-linear model,
and are conventionally trained by maximizing the
regularized log-likelihood on some parallel train-
ing corpora. Since this objective function requires
to normalize scores, several alternative training
objectives have recently been proposed to speed
up training and inference, a popular and effec-
tive choice being the Noise Contrastive Estimation
(NCE) introduced in (Gutmann and Hyv¨arinen,
2010). In any case, NN training is typically per-
formed (a) in isolation from the other components
of the SMT system and (b) using a criterion that
is unrelated to the actual performance of the SMT
system (as measured for instance by BLEU). It is
therefore likely that the resulting NN parameters
are sub-optimal with respect to their intended use.
In this paper, we study an alternative training
regime aimed at addressing problems (a) and (b).
To this end, we propose a new objective func-
tion used to discriminatively train or adapt CTMs,
along with a training procedure that enables to take
the other components of the system into account.
Our starting point is a non-normalized extension
of the n-gram CTM of (Le et al., 2012) that we
briefly restate in section 2. We then introduce our
objective function and the associated optimization
procedure in section 3. As will be discussed, our
new training criterion is inspired both from max-
margin methods (Watanabe et al., 2007) and from
pair-wise ranking (PRO) (Hopkins and May, 2011;
Simianer et al., 2012). This proposal is evaluated
in an N-best rescoring step, using the framework
of n-gram-based systems, within which they in-
tegrate seamlessly. Note, however that it could
be used with any phrase-based system. Experi-
mental results for two translation tasks (section 4)
clearly demonstrate the benefits of using discrimi-
native training on top of an NCE-trained model, as
it almost doubles the performance improvements
of the rescoring step in all settings.
</bodyText>
<sectionHeader confidence="0.992627" genericHeader="method">
2 n-gram-based CTMs
</sectionHeader>
<bodyText confidence="0.992316">
The n-gram-based approach in Machine Trans-
lation is a variant of the phrase-based ap-
proach (Zens et al., 2002). Introduced in (Casacu-
berta and Vidal, 2004), and extended in (Mari˜no et
al., 2006; Crego and Mari˜no, 2006), this approach
is based on a specific factorization of the joint
probability of parallel sentence pairs, where the
</bodyText>
<page confidence="0.928942">
1046
</page>
<note confidence="0.7326215">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.668877">
source sentence has been reordered beforehand.
</bodyText>
<subsectionHeader confidence="0.948411">
2.1 n-gram-based Machine Translation
</subsectionHeader>
<bodyText confidence="0.9999392">
Let (s, t) denote a sentence pair made of a source
s and target t sides. This sentence pair is decom-
posed into a sequence of L bilingual units called
tuples defining a joint segmentation. In this frame-
work, tuples constitute the basic translation units:
like phrase pairs, they represent a matching be-
tween a source and a target chunk. The joint prob-
ability of a synchronized and segmented sentence
pair can be estimated using the n-gram assump-
tion. During training, the segmentation is obtained
as a by-product of source reordering, (see (Crego
and Mari˜no, 2006) for details). During the infer-
ence step, the SMT decoder will compute and out-
put the best derivation in a small set of pre-defined
reorderings.
Note that the n-gram translation model manipu-
lates bilingual tuples. The underlying set of events
is thus much larger than for word-based models,
while the training data (parallel corpora) are typ-
ically order of magnitude smaller than monolin-
gual resources. As a consequence, data sparsity
issues for such models are particularly severe. Ef-
fective workarounds consist in factorizing the con-
ditional probabitily of tuples into terms involv-
ing smaller units: the resulting model thus splits
bilingual phrases in two sequences of respectively
source and target words, synchronised by the tuple
segmentation. Such bilingual word-based n-gram
models were initially described in (Le et al., 2012).
We assume here a similar decomposition.
</bodyText>
<subsectionHeader confidence="0.996875">
2.2 Neural Architectures
</subsectionHeader>
<bodyText confidence="0.999921741935484">
The estimation of n-gram probabilities can be per-
formed via multi-layer NN structures, as described
in (Bengio et al., 2003; Schwenk, 2007) for a
monolingual language model. The standard feed-
forward structure is used to estimate the trans-
lation models sketched in the previous section.
We give here a brief description, more details are
in (Le et al., 2012): first, each context word is pro-
jected into language dependent continuous spaces,
using two projection matrices for the source and
target languages. The continuous representations
are then concatenated to form the representation
of the context, which is used as input for a feed-
forward NN predicting a target word.
In such architecture, the size of output vocab-
ulary is a bottleneck when normalized distribu-
tions are expected. Various workarounds have
been proposed, relying for instance on a struc-
tured output layer using word-classes (Mnih and
Hinton, 2008; Le et al., 2011). A more effective
alternative, which however only delivers quasi-
normalized scores, is to train the network using
the Noise Contrastive Estimation or NCE (Gut-
mann and Hyv¨arinen, 2010; Mnih and Teh, 2012).
This technique is readily applicable for CTMs and
has been adopted here. We therefore assume that
the NN outputs a positive score be(w, c) for each
word w given its context c; this score is simply
computed as be(w, c) = exp(ae(w, c)), where
ae(w, c) is the activation at the output layer; θ de-
notes all the network free parameters.
</bodyText>
<sectionHeader confidence="0.996412" genericHeader="method">
3 Discriminative Training of CTMs
</sectionHeader>
<bodyText confidence="0.999981722222223">
In SMT, the primary role of CTMs is to help
the system in ranking a set of hypotheses so that
the top scoring hypotheses correspond to the best
translations, where quality is measured using au-
tomatic metrics such as BLEU (Papineni et al.,
2002). Given the computational burden of con-
tinuous models, the prefered use of CTMs is to
rescore a list of N-best hypotheses, a scenario we
favor here; note that their integration in a first pass
search is also possible (Niehues and Waibel, 2012;
Vaswani et al., 2013; Devlin et al., 2014). The im-
portant point is to realize that the CTM score will
in any case be composed with several scores com-
puted by other components: reordering model(s),
monolingual language model(s), etc. In this sec-
tion, we propose a discriminative training frame-
work which implements a tight integration of the
CTM with the rest of the system.
</bodyText>
<subsectionHeader confidence="0.998373">
3.1 A Discriminative Training Framework
</subsectionHeader>
<bodyText confidence="0.99979025">
The decoder generates a list of N hypotheses for
each source sentence s. Each hypothesis h is com-
posed of a target sentence t along with its associ-
ated derivation and is evaluated as follows:
</bodyText>
<equation confidence="0.988384">
M
Ga,e(s, h) = Akfk(s, h) + AM+1fe(s, h),
k=1
</equation>
<bodyText confidence="0.9995698">
where M conventional feature functions1 f1...fM,
estimated during the training phase, are scaled by
coefficients A1...AM. The introduction of a con-
tinuous model during the rescoring step is imple-
mented by adding the feature fe(s, h), which ac-
</bodyText>
<footnote confidence="0.983994">
1The functions used in our experiments are similar to the
ones used in other phrase-based systems (Crego et al., 2011).
</footnote>
<page confidence="0.992241">
1047
</page>
<bodyText confidence="0.871405">
Algorithm 1 Joint optimization of 0 and A
</bodyText>
<listItem confidence="0.977775333333333">
1: Init. of 0 and A
2: for each iteration do
3: for P mini-batch do A is fixed
4: Compute the sub-gradient of L(0) for
each sentence s in the mini-batch
5: Update 0
6: end for
7: Update A on development set &gt; 0 is fixed
8: end for
</listItem>
<bodyText confidence="0.989931210526316">
cumulates, over all contexts c and word w, the
CTM log-score log bθ(w, c).
Gλ,θ depends both on the NN parameters 0
and on the log-linear coefficients A. We pro-
pose to train these two sets of parameters, by al-
ternatively updating 0 through SGD on the train-
ing corpus, and updating A using conventional al-
gorithms on the development data. This proce-
dure, which has also been adopted in recent stud-
ies (e.g. (He and Deng, 2012; Gao and He, 2013))
is sketched in algorithm 1. In practice, the train-
ing data is successively divided into mini-batches
of 128 sentences. Each mini-batch is used to com-
pute the sub-gradient of the training criterion (see
section 3.2) and to update 0. After each training
iteration of the CTM, As are retuned on the de-
velopment set; we use here the K-Best Mira algo-
rithm of Cherry and Foster (2012) as implemented
in MOSES.2
</bodyText>
<subsectionHeader confidence="0.999587">
3.2 Loss function
</subsectionHeader>
<bodyText confidence="0.99999025">
The training criterion considered here draws in-
spiration both from max-margin methods (Watan-
abe et al., 2007) and from the pair-wise ranking
(PRO) (Hopkins and May, 2011; Simianer et al.,
2012). The choice of a ranking loss seems to be
the most appropriate in our setting; as in many
recent studies on discriminative training for MT
(e.g. (Chiang, 2012; Flanigan et al., 2013)), the
integration of the translation metric into the loss
function is critical to obtain parameters that will
yield good translation performance.
Translation hypotheses hi are scored using a
sentence-level approximation of BLEU denoted
5BLEU(hi). Let ri be the rank of hypothesis
hi when hypotheses are sorted according to their
sentence-level BLEU. Critical hypotheses are de-
</bodyText>
<equation confidence="0.79924375">
2http://www.statmt.org/moses/
fined as follows:3
Cαδ (s) = {(i,k) : 1 &lt; k,i &lt; N,rk − ri ≥ S,
Di,kGλ,θ(s, h) &lt; αDi,k5BLEU(h).
</equation>
<bodyText confidence="0.999741285714286">
A pair of hypotheses is thus deemed critical when
a large difference in 5BLEU is not reflected
by the difference of scores, which falls below a
threshold. This threshold is defined by the differ-
ence between their sentence-level BLEU, multi-
plied by α. Our loss function L(0) is defined with
respect to this critical set and can be written as:4
</bodyText>
<equation confidence="0.994416">
� αDi,k5BLEU(h) − Di,kGλ,θ(s, hi)
(i,k)∈Cαδ (s)
</equation>
<bodyText confidence="0.999554363636364">
Initialization is an important issue when opti-
mizing NN. Moreover, our training procedure de-
pends heavily on the log-linear coefficients A. To
initialize 0, preliminary experiments (Do et al.,
2014; Do et al., 2015) show that it is more effi-
cient to start from a NN pre-trained using NCE,
while the discriminative loss is used only in a fine-
tuning phase. Given the pre-trained CTM’s scores,
we initialize A by optimizing it on the develop-
ment set. This strategy forces the training of 0 to
focus on errors made by the system as a whole.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998458">
4.1 Tasks and Corpora
</subsectionHeader>
<bodyText confidence="0.965087263157895">
The discriminative optimization framework is
evaluated both in a training and in an adaptation
scenario. In the training scenario, the CTM is
trained on the same parallel data as the one used
for the baseline system. In the adaptation sce-
nario, large out-of-domain corpora are used to
train the baseline SMT system, while the CTM is
trained on a much smaller, in-domain corpus and
only serves for rescoring. An intermediate situa-
tion (partial training) is when only a fraction of
the training data is re-used to estimate the CTM:
this situation is interesting because it allows us to
train the CTM much faster than in the training sce-
nario.5
Two domains are investigated. For the
TED Talkstask6 the only parallel in-domain data
contains 180K sentence pairs; the out-of-domain
3Δi,k denotes the difference of values (for SBLEU or
Gλ,θ) between hypthoses hi and hk.
</bodyText>
<footnote confidence="0.99904775">
4This is for one single training sample.
5The discriminative training step also uses the develop-
ment data.
6http://workshop2014.iwslt.org/
</footnote>
<page confidence="0.781903">
1048
</page>
<table confidence="0.999517444444445">
dev test train
Training scenario
Baseline Ncode on TED 28.1 32.3 65.6
Baseline + CTM NCE 28.9 33.1 64.1
Baseline + CTM discriminative 29.0 33.5 64.9
Adaptation scenario
Baseline Ncode on WMT 28.5 32.0 33.3
Baseline + CTM NCE 29.2 33.0 34.9
Baseline + CTM discriminative 29.8 33.9 35.8
</table>
<tableCaption confidence="0.98493">
Table 1: BLEU scores for the TED Talkstasks.
</tableCaption>
<table confidence="0.999845222222222">
dev test train
Partial training scenario
Baseline Ncode 40.4 37.4 45.8
Baseline + CTM NCE 40.8 38.1 45.2
Baseline + CTM discriminative 41.8 38.8 46.0
“Adaptation” scenario
Baseline Ncode 39.8 37.2 39.4
Baseline + CTM NCE 41.2 38.2 40.4
Baseline + CTM discriminative 41.8 38.9 41.5
</table>
<tableCaption confidence="0.998468">
Table 2: BLEU scores for the medical tasks.
</tableCaption>
<bodyText confidence="0.99997055">
data is much larger and contains all corpora al-
lowed in the translation shared task of WMT’14
(English-French), amounting to 12M parallel sen-
tences. The second task is the medical transla-
tion task of WMT’147 (English to French) for
which we use all authorized corpora. The Patent-
Abstract corpus, made of 200K parallel sentence
pairs, is used either for adaptation or partial train-
ing for the CTM. Experimental results are re-
ported on official evaluation sets, as well as on the
CTM training set.
All translation systems are based on the open
source implementation8 of the bilingual n-gram
approach to MT. For the NN structure, each vo-
cabulary’s word is projected into a 500-dimension
space followed by two hidden layers of 1000 and
500 units. For the discriminative training and
adaptation tasks, baseline SMT systems are used
to generate respectively 600 and 300 best hypothe-
ses for each sentence of the in-domain corpus. 9
</bodyText>
<subsectionHeader confidence="0.974364">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9999592">
Results in Table 1 measure the impact of discrim-
inative training on top of an NCE-trained model
for the two TED Talks conditions. In the adapta-
tion task, the discriminative training of the CTM
gives a large improvement of 0.9 BLEU score
over the CTM only trained with NCE and 1.9 over
the baseline system. However, for the training sce-
nario, these gains are reduced respectively to 0.4
and 1.2 BLEU points. The BLEU scores (in the
train column) measured on the N-best lists used
to train the CTM provide an explanation for this
difference: in training, the N-best lists contain hy-
potheses with an overoptimistic BLEU score, to
be compared with the ones observed on unseen
data. As a result, adding the CTM significantly
</bodyText>
<footnote confidence="0.99934325">
7www.statmt.org/wmt14/medical-task/
8ncode.limsi.fr/
9The threshold δ is set to 250 for 300-best and to 500 for
600-best lists, while α is set empirically.
</footnote>
<bodyText confidence="0.999398038461539">
worsens the performance on the discriminative
training data, contrarily to what is observed on the
development and test sets. Even if the results of
these two conditions cannot be directly compared
(the baselines are different), it seems that the pro-
posed discriminative training has a greater impact
on performance in the adaptation scenario, even
though the out-of-domain system initially yields
lower BLEU scores.
The medical translation task represents a dif-
ferent situation, in which a large-scale system is
built from multiples but domain-related corpora,
among which, one is used to train the CTM. Nev-
ertheless, results reported in Table 2 exhibit a sim-
ilar trend. For both conditions, the discrimina-
tive training gives a significant improvement, up
to 0.7 BLEU score over the one only trained with
NCE and up to 1.7 over the baseline system. Ar-
guably, the difference between the two conditions
is much smaller than what was observed with the
TED Talks task, due to the fact that the Patent-
Abstract corpus used to discriminatively train the
CTM only corresponds to a small subset of the
parallel data. However, the best strategy seems,
here again, to exclude the data used for the CTM
from the data used to train the baseline system.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999973416666667">
It is important to notice that similar discrimina-
tive methods have been used to train phrase table’s
scores (He and Deng, 2012; Gao and He, 2013;
Gao et al., 2014), or a recurrent NNLM (Auli
and Gao, 2014). In recent studies, the authors
tend to limit the number of iterations to 1 (Gao
et al., 2014; Auli and Gao, 2014), while we still
advocate the general iterative procedure sketched
in Algo. 1. Initialization is also an important is-
sue when optimizing NN. In this work, we ini-
tialize CTM’s parameters by using a pre-training
procedure based on the model’s probabilistic in-
</bodyText>
<page confidence="0.992425">
1049
</page>
<bodyText confidence="0.999975742857143">
terpretation and NCE algorithm to produce quasi-
normalized scores, while similar work in (Auli and
Gao, 2014) only uses un-normalized scores. The
initial values of λ also needs some investigation.
Gao et al. (2014) and Auli and Gao (2014) ini-
tialize AM+1 to 1, and normalize all other coef-
ficients; here we initialize λ by optimizing it on
the development set using the pre-trained CTM’s
scores. This strategy forces the training of θ to
focus on errors made by the system as a whole.
The fundamental difference of this work hence
lays in the use of the ranking loss described in
Section 3.2, whereas previous works use expected
BLEU loss. We plan a systematic comparison be-
tween these two criteria, along with some other
discriminative losses in a future work.
About the CTM’s structure, our used model is
based on the feed-forward CTM described in (Le
et al., 2012) and extended in (Devlin et al., 2014).
This structure, though simple, have been shown
to achieve impressive results, and with which effi-
cient tricks are available to speed up both train-
ing and inference. While models in (Le et al.,
2012) employ a structured output layer to reduce
softmax operation’s cost, we prefer the NCE self-
normalized output which is very efficient both
in training and inference. Another form of self-
normalization is presented in (Devlin et al., 2014)
but does not seem to have fast training. Finally,
although N-best rescoring is used in this work to
facilitate the discriminative training, other CTM’s
integration into SMT systems exist, such as lat-
tice reranking (Auli et al., 2013) or direct decod-
ing with CTM (Niehues and Waibel, 2012; Devlin
et al., 2014; Auli and Gao, 2014).
</bodyText>
<sectionHeader confidence="0.996776" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999941666666667">
In this paper, we have proposed a new discrimina-
tive training procedure for continuous-space trans-
lation models, which correlates better with trans-
lation quality than conventional training meth-
ods. This procedure has been validated using an
n-gram-based CTM, but the general idea could be
applied to other continuous models which com-
pute a score for each translation hypothesis. The
core of the method lays in the definition of a new
objective function inspired both from max-margin
and Pairwise Ranking approach in MT, which en-
ables us to effectively integrate the CTM into the
SMT system through N-best rescoring. A major
difference with most past efforts along these lines
is the joint training of the CTM and the log-linear
parameters. In all our experiments, discriminative
training, when applied on a CTM initially trained
with NCE, yields substantial performance gains.
</bodyText>
<sectionHeader confidence="0.985216" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999656">
This work has been partly funded by the European
Union’s Horizon 2020 research and innovation
programme under grant agreement No. 645452
(QT21).
</bodyText>
<sectionHeader confidence="0.950245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.968789256410257">
Michael Auli and Jianfeng Gao. 2014. Decoder in-
tegration and expected bleu training for recurrent
neural network language models. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 136–142.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1044–1054.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205–
225.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT), pages 427–
436.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 13(1):1159–1187.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1724–1734,
Doha, Qatar.
Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199–215.
</reference>
<page confidence="0.891284">
1050
</page>
<reference confidence="0.99674017699115">
Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49–58.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1370–1380, Baltimore, MD.
Quoc-Khanh Do, Alexandre Allauzen, and Franc¸ois
Yvon. 2014. Discriminative adaptation of con-
tinuous space translation models. In International
Workshop on Spoken Language Translation (IWSLT
2014), Lake Tahoe, USA.
Quoc-Khanh Do, Alexandre Allauzen, and Franc¸ois
Yvon. 2015. Apprentissage discriminant des
mod`eles continus de traduction. In Actes de la
22e conf´erence sur le Traitement Automatique des
Langues Naturelles, pages 267–278, Caen, France,
June. Association pour le Traitement Automatique
des Langues.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 248–258, Atlanta, Georgia.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient as-
cent. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 450–459, Atlanta, Georgia.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, Baltimore, MD.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Yeh Whye
Teh and Mike Titterington, editors, Proceedings
of th International Conference on Artificial Intel-
ligence and Statistics (AISTATS), volume 9, pages
297–304.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 292–301.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882, Jeju Island,
Korea.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc¸ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of the International Conference on Audio,
Speech and Signal Processing, pages 5524–5527.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 39–48, Montr´eal, Canada.
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549.
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou,
editors, Advances in Neural Information Processing
Systems 21, volume 21, pages 1081–1088.
Andriy Mnih and Yeh Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference of Machine Learning (ICML).
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted Boltzmann ma-
chines. In Proceedings of International Workshop
on Spoken Language Translation (IWSLT), pages
164–170, Hong-Kong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311–318.
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram trans-
lation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 430–438, Prague, Czech Republic.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492–518.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
11–21.
</reference>
<page confidence="0.798555">
1051
</page>
<reference confidence="0.999670852941177">
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 455–465, Sofia, Bulgaria.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, NIPS*27, pages 3104–3112, Montr´eal,
Canada.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1387–1392, Seattle, Washington, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764–773, Prague, Czech Republic.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with context
dependent deep neural networks. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 166–175, Sofia, Bul-
garia.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In KI ’02: Proceedings of the 25th Annual Ger-
man Conference on AI, pages 18–32, London, UK.
Springer-Verlag.
</reference>
<page confidence="0.994646">
1052
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678301">
<title confidence="0.996315">A Discriminative Training Procedure for Continuous Translation Models</title>
<author confidence="0.925406">Allauzen Do</author>
<affiliation confidence="0.761043">Paris-Sud, Orsay,</affiliation>
<address confidence="0.929198">Orsay,</address>
<email confidence="0.9994">firstname.surname@limsi.fr</email>
<abstract confidence="0.9998042">Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inferis to use them in an rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Jianfeng Gao</author>
</authors>
<title>Decoder integration and expected bleu training for recurrent neural network language models.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>136--142</pages>
<contexts>
<context position="16544" citStr="Auli and Gao, 2014" startWordPosition="2725" endWordPosition="2728">system. Arguably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the PatentAbstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data. However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system. 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some investigation. Gao et al. (2014) and Auli </context>
<context position="18592" citStr="Auli and Gao, 2014" startWordPosition="3073" endWordPosition="3076">ing and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N-best rescoring is used in this work to facilitate the discriminative training, other CTM’s integration into SMT systems exist, such as lattice reranking (Auli et al., 2013) or direct decoding with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). 6 Conclusions In this paper, we have proposed a new discriminative training procedure for continuous-space translation models, which correlates better with translation quality than conventional training methods. This procedure has been validated using an n-gram-based CTM, but the general idea could be applied to other continuous models which compute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and Pairwise Ranking approach in MT, which enables us to effectively integrate the CTM into the SMT s</context>
</contexts>
<marker>Auli, Gao, 2014</marker>
<rawString>Michael Auli and Jianfeng Gao. 2014. Decoder integration and expected bleu training for recurrent neural network language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 136–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="18496" citStr="Auli et al., 2013" startWordPosition="3055" endWordPosition="3058">chieve impressive results, and with which efficient tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N-best rescoring is used in this work to facilitate the discriminative training, other CTM’s integration into SMT systems exist, such as lattice reranking (Auli et al., 2013) or direct decoding with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). 6 Conclusions In this paper, we have proposed a new discriminative training procedure for continuous-space translation models, which correlates better with translation quality than conventional training methods. This procedure has been validated using an n-gram-based CTM, but the general idea could be applied to other continuous models which compute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and P</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="5721" citStr="Bengio et al., 2003" startWordPosition="892" endWordPosition="895">a consequence, data sparsity issues for such models are particularly severe. Effective workarounds consist in factorizing the conditional probabitily of tuples into terms involving smaller units: the resulting model thus splits bilingual phrases in two sequences of respectively source and target words, synchronised by the tuple segmentation. Such bilingual word-based n-gram models were initially described in (Le et al., 2012). We assume here a similar decomposition. 2.2 Neural Architectures The estimation of n-gram probabilities can be performed via multi-layer NN structures, as described in (Bengio et al., 2003; Schwenk, 2007) for a monolingual language model. The standard feedforward structure is used to estimate the translation models sketched in the previous section. We give here a brief description, more details are in (Le et al., 2012): first, each context word is projected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representations are then concatenated to form the representation of the context, which is used as input for a feedforward NN predicting a target word. In such architecture, the size of output vocabular</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Machine translation with inferred stochastic finite-state transducers.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<pages>225</pages>
<contexts>
<context position="3667" citStr="Casacuberta and Vidal, 2004" startWordPosition="573" endWordPosition="577">This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingua</context>
</contexts>
<marker>Casacuberta, Vidal, 2004</marker>
<rawString>Francesco Casacuberta and Enrique Vidal. 2004. Machine translation with inferred stochastic finite-state transducers. Computational Linguistics, 30(3):205– 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>427--436</pages>
<contexts>
<context position="9707" citStr="Cherry and Foster (2012)" startWordPosition="1587" endWordPosition="1590">ers, by alternatively updating 0 through SGD on the training corpus, and updating A using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are sc</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 427– 436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="10104" citStr="Chiang, 2012" startWordPosition="1655" endWordPosition="1656">radient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted 5BLEU(hi). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2http://www.statmt.org/moses/ fined as follows:3 Cαδ (s) = {(i,k) : 1 &lt; k,i &lt; N,rk − ri ≥ S, Di,kGλ,θ(s, h) &lt; αDi,k5BLEU(h). A pair of hypotheses is thus deemed critical when a large difference in 5B</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, 13(1):1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1724--1734</pages>
<location>Doha, Qatar.</location>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Improving statistical MT by coupling reordering and decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving statistical MT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2011</date>
<booktitle>N-code: an open-source bilingual N-gram SMT toolkit. Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. N-code: an open-source bilingual N-gram SMT toolkit. Prague Bulletin of Mathematical Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1370--1380</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="1387" citStr="Devlin et al., 2014" startWordPosition="208" endWordPosition="211">re it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann and Hyv¨arinen, 2010). </context>
<context position="7634" citStr="Devlin et al., 2014" startWordPosition="1214" endWordPosition="1217">utput layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as follows: M Ga,e(s, h) = Akfk(s, h) + AM+1fe(s</context>
<context position="17825" citStr="Devlin et al., 2014" startWordPosition="2945" endWordPosition="2948">coefficients; here we initialize λ by optimizing it on the development set using the pre-trained CTM’s scores. This strategy forces the training of θ to focus on errors made by the system as a whole. The fundamental difference of this work hence lays in the use of the ranking loss described in Section 3.2, whereas previous works use expected BLEU loss. We plan a systematic comparison between these two criteria, along with some other discriminative losses in a future work. About the CTM’s structure, our used model is based on the feed-forward CTM described in (Le et al., 2012) and extended in (Devlin et al., 2014). This structure, though simple, have been shown to achieve impressive results, and with which efficient tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N-best rescoring is used in this work to facilitate the discriminative training, other CTM’s integratio</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1370–1380, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc-Khanh Do</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Discriminative adaptation of continuous space translation models.</title>
<date>2014</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT 2014),</booktitle>
<location>Lake Tahoe, USA.</location>
<contexts>
<context position="11218" citStr="Do et al., 2014" startWordPosition="1831" endWordPosition="1834">λ,θ(s, h) &lt; αDi,k5BLEU(h). A pair of hypotheses is thus deemed critical when a large difference in 5BLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(0) is defined with respect to this critical set and can be written as:4 � αDi,k5BLEU(h) − Di,kGλ,θ(s, hi) (i,k)∈Cαδ (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients A. To initialize 0, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize A by optimizing it on the development set. This strategy forces the training of 0 to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the </context>
</contexts>
<marker>Do, Allauzen, Yvon, 2014</marker>
<rawString>Quoc-Khanh Do, Alexandre Allauzen, and Franc¸ois Yvon. 2014. Discriminative adaptation of continuous space translation models. In International Workshop on Spoken Language Translation (IWSLT 2014), Lake Tahoe, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc-Khanh Do</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Apprentissage discriminant des mod`eles continus de traduction.</title>
<date>2015</date>
<booktitle>In Actes de la 22e conf´erence sur le Traitement Automatique des Langues Naturelles,</booktitle>
<pages>267--278</pages>
<location>Caen, France,</location>
<contexts>
<context position="11236" citStr="Do et al., 2015" startWordPosition="1835" endWordPosition="1838">5BLEU(h). A pair of hypotheses is thus deemed critical when a large difference in 5BLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the difference between their sentence-level BLEU, multiplied by α. Our loss function L(0) is defined with respect to this critical set and can be written as:4 � αDi,k5BLEU(h) − Di,kGλ,θ(s, hi) (i,k)∈Cαδ (s) Initialization is an important issue when optimizing NN. Moreover, our training procedure depends heavily on the log-linear coefficients A. To initialize 0, preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more efficient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a finetuning phase. Given the pre-trained CTM’s scores, we initialize A by optimizing it on the development set. This strategy forces the training of 0 to focus on errors made by the system as a whole. 4 Experiments 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the adaptation scenari</context>
</contexts>
<marker>Do, Allauzen, Yvon, 2015</marker>
<rawString>Quoc-Khanh Do, Alexandre Allauzen, and Franc¸ois Yvon. 2015. Apprentissage discriminant des mod`eles continus de traduction. In Actes de la 22e conf´erence sur le Traitement Automatique des Langues Naturelles, pages 267–278, Caen, France, June. Association pour le Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Chris Dyer</author>
<author>Jaime Carbonell</author>
</authors>
<title>Large-scale discriminative training for statistical machine translation using held-out line search.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>248--258</pages>
<location>Atlanta,</location>
<contexts>
<context position="10128" citStr="Flanigan et al., 2013" startWordPosition="1657" endWordPosition="1660"> training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted 5BLEU(hi). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2http://www.statmt.org/moses/ fined as follows:3 Cαδ (s) = {(i,k) : 1 &lt; k,i &lt; N,rk − ri ≥ S, Di,kGλ,θ(s, h) &lt; αDi,k5BLEU(h). A pair of hypotheses is thus deemed critical when a large difference in 5BLEU is not reflected by </context>
</contexts>
<marker>Flanigan, Dyer, Carbonell, 2013</marker>
<rawString>Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013. Large-scale discriminative training for statistical machine translation using held-out line search. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 248–258, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
</authors>
<title>Training mrfbased phrase translation models using gradient ascent.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>450--459</pages>
<location>Atlanta,</location>
<contexts>
<context position="9327" citStr="Gao and He, 2013" startWordPosition="1520" endWordPosition="1523">i-batch do A is fixed 4: Compute the sub-gradient of L(0) for each sentence s in the mini-batch 5: Update 0 6: end for 7: Update A on development set &gt; 0 is fixed 8: end for cumulates, over all contexts c and word w, the CTM log-score log bθ(w, c). Gλ,θ depends both on the NN parameters 0 and on the log-linear coefficients A. We propose to train these two sets of parameters, by alternatively updating 0 through SGD on the training corpus, and updating A using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simi</context>
<context position="16483" citStr="Gao and He, 2013" startWordPosition="2713" endWordPosition="2716">one only trained with NCE and up to 1.7 over the baseline system. Arguably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the PatentAbstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data. However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system. 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of</context>
</contexts>
<marker>Gao, He, 2013</marker>
<rawString>Jianfeng Gao and Xiaodong He. 2013. Training mrfbased phrase translation models using gradient ascent. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 450–459, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="16502" citStr="Gao et al., 2014" startWordPosition="2717" endWordPosition="2720">ith NCE and up to 1.7 over the baseline system. Arguably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the PatentAbstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data. However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system. 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The initial values of λ also needs some </context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>In Yeh Whye Teh and</booktitle>
<volume>9</volume>
<pages>297--304</pages>
<editor>Mike Titterington, editors,</editor>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In Yeh Whye Teh and Mike Titterington, editors, Proceedings of th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 9, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum expected bleu training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>292--301</pages>
<contexts>
<context position="9308" citStr="He and Deng, 2012" startWordPosition="1516" endWordPosition="1519">ion do 3: for P mini-batch do A is fixed 4: Compute the sub-gradient of L(0) for each sentence s in the mini-batch 5: Update 0 6: end for 7: Update A on development set &gt; 0 is fixed 8: end for cumulates, over all contexts c and word w, the CTM log-score log bθ(w, c). Gλ,θ depends both on the NN parameters 0 and on the log-linear coefficients A. We propose to train these two sets of parameters, by alternatively updating 0 through SGD on the training corpus, and updating A using conventional algorithms on the development data. This procedure, which has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins </context>
<context position="16465" citStr="He and Deng, 2012" startWordPosition="2709" endWordPosition="2712">LEU score over the one only trained with NCE and up to 1.7 over the baseline system. Arguably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the PatentAbstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data. However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system. 5 Related work It is important to notice that similar discriminative methods have been used to train phrase table’s scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important issue when optimizing NN. In this work, we initialize CTM’s parameters by using a pre-training procedure based on the model’s probabilistic in1049 terpretation and NCE algorithm to produce quasinormalized scores, while similar work in (Auli and Gao, 2014) only uses un-normalized scores. The</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum expected bleu training of phrase and lexicon translation models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 292–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="3013" citStr="Hopkins and May, 2011" startWordPosition="471" endWordPosition="474">addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et a</context>
<context position="9921" citStr="Hopkins and May, 2011" startWordPosition="1621" endWordPosition="1624">ng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted 5BLEU(hi). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2http://www.stat</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1152" citStr="Huang et al., 2012" startWordPosition="172" endWordPosition="175">w that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>JeanLuc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Audio, Speech and Signal Processing,</booktitle>
<pages>5524--5527</pages>
<contexts>
<context position="1048" citStr="Le et al., 2011" startWordPosition="154" endWordPosition="157">inference is to use them in an N-best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regulariz</context>
<context position="6532" citStr="Le et al., 2011" startWordPosition="1022" endWordPosition="1025">, more details are in (Le et al., 2012): first, each context word is projected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representations are then concatenated to form the representation of the context, which is used as input for a feedforward NN predicting a target word. In such architecture, the size of output vocabulary is a bottleneck when normalized distributions are expected. Various workarounds have been proposed, relying for instance on a structured output layer using word-classes (Mnih and Hinton, 2008; Le et al., 2011). A more effective alternative, which however only delivers quasinormalized scores, is to train the network using the Noise Contrastive Estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score be(w, c) for each word w given its context c; this score is simply computed as be(w, c) = exp(ae(w, c)), where ae(w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of </context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, JeanLuc Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Proceedings of the International Conference on Audio, Speech and Signal Processing, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>39--48</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1365" citStr="Le et al., 2012" startWordPosition="204" endWordPosition="207"> two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann a</context>
<context position="2709" citStr="Le et al., 2012" startWordPosition="422" endWordPosition="425">em and (b) using a criterion that is unrelated to the actual performance of the SMT system (as measured for instance by BLEU). It is therefore likely that the resulting NN parameters are sub-optimal with respect to their intended use. In this paper, we study an alternative training regime aimed at addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clea</context>
<context position="5531" citStr="Le et al., 2012" startWordPosition="863" endWordPosition="866">nderlying set of events is thus much larger than for word-based models, while the training data (parallel corpora) are typically order of magnitude smaller than monolingual resources. As a consequence, data sparsity issues for such models are particularly severe. Effective workarounds consist in factorizing the conditional probabitily of tuples into terms involving smaller units: the resulting model thus splits bilingual phrases in two sequences of respectively source and target words, synchronised by the tuple segmentation. Such bilingual word-based n-gram models were initially described in (Le et al., 2012). We assume here a similar decomposition. 2.2 Neural Architectures The estimation of n-gram probabilities can be performed via multi-layer NN structures, as described in (Bengio et al., 2003; Schwenk, 2007) for a monolingual language model. The standard feedforward structure is used to estimate the translation models sketched in the previous section. We give here a brief description, more details are in (Le et al., 2012): first, each context word is projected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representat</context>
<context position="17787" citStr="Le et al., 2012" startWordPosition="2938" endWordPosition="2941">M+1 to 1, and normalize all other coefficients; here we initialize λ by optimizing it on the development set using the pre-trained CTM’s scores. This strategy forces the training of θ to focus on errors made by the system as a whole. The fundamental difference of this work hence lays in the use of the ranking loss described in Section 3.2, whereas previous works use expected BLEU loss. We plan a systematic comparison between these two criteria, along with some other discriminative losses in a future work. About the CTM’s structure, our used model is based on the feed-forward CTM described in (Le et al., 2012) and extended in (Devlin et al., 2014). This structure, though simple, have been shown to achieve impressive results, and with which efficient tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N-best rescoring is used in this work to facilitate the discrimin</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 39–48, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrick Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrick Lambert, Jos´e A.R. Fonollosa, and Marta R. Costa-Juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<volume>21</volume>
<pages>1081--1088</pages>
<editor>In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<contexts>
<context position="6514" citStr="Mnih and Hinton, 2008" startWordPosition="1018" endWordPosition="1021">ere a brief description, more details are in (Le et al., 2012): first, each context word is projected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representations are then concatenated to form the representation of the context, which is used as input for a feedforward NN predicting a target word. In such architecture, the size of output vocabulary is a bottleneck when normalized distributions are expected. Various workarounds have been proposed, relying for instance on a structured output layer using word-classes (Mnih and Hinton, 2008; Le et al., 2011). A more effective alternative, which however only delivers quasinormalized scores, is to train the network using the Noise Contrastive Estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score be(w, c) for each word w given its context c; this score is simply computed as be(w, c) = exp(ae(w, c)), where ae(w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, th</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, volume 21, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yeh Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference of Machine Learning (ICML).</booktitle>
<contexts>
<context position="6736" citStr="Mnih and Teh, 2012" startWordPosition="1054" endWordPosition="1057">us representations are then concatenated to form the representation of the context, which is used as input for a feedforward NN predicting a target word. In such architecture, the size of output vocabulary is a bottleneck when normalized distributions are expected. Various workarounds have been proposed, relying for instance on a structured output layer using word-classes (Mnih and Hinton, 2008; Le et al., 2011). A more effective alternative, which however only delivers quasinormalized scores, is to train the network using the Noise Contrastive Estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score be(w, c) for each word w given its context c; this score is simply computed as be(w, c) = exp(ae(w, c)), where ae(w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni e</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yeh Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the International Conference of Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Continuous space language models using restricted Boltzmann machines.</title>
<date>2012</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>164--170</pages>
<location>Hong-Kong, China.</location>
<contexts>
<context position="7590" citStr="Niehues and Waibel, 2012" startWordPosition="1206" endWordPosition="1209">, c)), where ae(w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as f</context>
<context position="18550" citStr="Niehues and Waibel, 2012" startWordPosition="3065" endWordPosition="3068">ent tricks are available to speed up both training and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation’s cost, we prefer the NCE selfnormalized output which is very efficient both in training and inference. Another form of selfnormalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N-best rescoring is used in this work to facilitate the discriminative training, other CTM’s integration into SMT systems exist, such as lattice reranking (Auli et al., 2013) or direct decoding with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). 6 Conclusions In this paper, we have proposed a new discriminative training procedure for continuous-space translation models, which correlates better with translation quality than conventional training methods. This procedure has been validated using an n-gram-based CTM, but the general idea could be applied to other continuous models which compute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and Pairwise Ranking approach in MT, which enables us to ef</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>Jan Niehues and Alex Waibel. 2012. Continuous space language models using restricted Boltzmann machines. In Proceedings of International Workshop on Spoken Language Translation (IWSLT), pages 164–170, Hong-Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="7348" citStr="Papineni et al., 2002" startWordPosition="1164" endWordPosition="1167">Teh, 2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score be(w, c) for each word w given its context c; this score is simply computed as be(w, c) = exp(ae(w, c)), where ae(w, c) is the activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the re</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Marta R Costa-jussa</author>
<author>Jose A R Fonollosa</author>
</authors>
<title>Smooth bilingual n-gram translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>430--438</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1348" citStr="Schwenk et al., 2007" startWordPosition="200" endWordPosition="203">proach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introdu</context>
</contexts>
<marker>Schwenk, Costa-jussa, Fonollosa, 2007</marker>
<rawString>Holger Schwenk, Marta R. Costa-jussa, and Jose A. R. Fonollosa. 2007. Smooth bilingual n-gram translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 430–438, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="1030" citStr="Schwenk, 2007" startWordPosition="152" endWordPosition="153">such models in inference is to use them in an N-best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximi</context>
<context position="5737" citStr="Schwenk, 2007" startWordPosition="896" endWordPosition="897">parsity issues for such models are particularly severe. Effective workarounds consist in factorizing the conditional probabitily of tuples into terms involving smaller units: the resulting model thus splits bilingual phrases in two sequences of respectively source and target words, synchronised by the tuple segmentation. Such bilingual word-based n-gram models were initially described in (Le et al., 2012). We assume here a similar decomposition. 2.2 Neural Architectures The estimation of n-gram probabilities can be performed via multi-layer NN structures, as described in (Bengio et al., 2003; Schwenk, 2007) for a monolingual language model. The standard feedforward structure is used to estimate the translation models sketched in the previous section. We give here a brief description, more details are in (Le et al., 2012): first, each context word is projected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representations are then concatenated to form the representation of the context, which is used as input for a feedforward NN predicting a target word. In such architecture, the size of output vocabulary is a bottlenec</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>11--21</pages>
<contexts>
<context position="3037" citStr="Simianer et al., 2012" startWordPosition="475" endWordPosition="478"> and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in</context>
<context position="9945" citStr="Simianer et al., 2012" startWordPosition="1625" endWordPosition="1628">013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted 5BLEU(hi). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de2http://www.statmt.org/moses/ fined as f</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>455--465</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1132" citStr="Socher et al., 2013" startWordPosition="168" endWordPosition="171">this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function r</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 455–465, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS*27,</booktitle>
<pages>3104--3112</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1493" citStr="Sutskever et al., 2014" startWordPosition="225" endWordPosition="228"> (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effective choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann and Hyv¨arinen, 2010). In any case, NN training is typically performed (a) in isolation from the other components of the SMT syst</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, NIPS*27, pages 3104–3112, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1387--1392</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="7612" citStr="Vaswani et al., 2013" startWordPosition="1210" endWordPosition="1213">he activation at the output layer; θ denotes all the network free parameters. 3 Discriminative Training of CTMs In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using automatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of continuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The important point is to realize that the CTM score will in any case be composed with several scores computed by other components: reordering model(s), monolingual language model(s), etc. In this section, we propose a discriminative training framework which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s. Each hypothesis h is composed of a target sentence t along with its associated derivation and is evaluated as follows: M Ga,e(s, h) =</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1387–1392, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>764--773</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2957" citStr="Watanabe et al., 2007" startWordPosition="462" endWordPosition="465"> paper, we study an alternative training regime aimed at addressing problems (a) and (b). To this end, we propose a new objective function used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n-gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from maxmargin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translat</context>
<context position="9861" citStr="Watanabe et al., 2007" startWordPosition="1610" endWordPosition="1614">hich has also been adopted in recent studies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the training data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to compute the sub-gradient of the training criterion (see section 3.2) and to update 0. After each training iteration of the CTM, As are retuned on the development set; we use here the K-Best Mira algorithm of Cherry and Foster (2012) as implemented in MOSES.2 3.2 Loss function The training criterion considered here draws inspiration both from max-margin methods (Watanabe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance. Translation hypotheses hi are scored using a sentence-level approximation of BLEU denoted 5BLEU(hi). Let ri be the rank of hypothesis hi when hypotheses are sorted according to their sen</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 764–773, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word alignment modeling with context dependent deep neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>166--175</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1172" citStr="Yang et al., 2013" startWordPosition="176" endWordPosition="179">ce gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 1 Introduction Over the past few years, research on neural networks (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recognition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Translation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation systems (Cho et al., 2014; Sutskever et al., 2014). In most SMT settings, CTMs are used as an additional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel training corpora. Since this objective function requires to normalize scores, several alt</context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context dependent deep neural networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 166–175, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In KI ’02: Proceedings of the 25th Annual German Conference on AI,</booktitle>
<pages>18--32</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="3622" citStr="Zens et al., 2002" startWordPosition="567" endWordPosition="570">May, 2011; Simianer et al., 2012). This proposal is evaluated in an N-best rescoring step, using the framework of n-gram-based systems, within which they integrate seamlessly. Note, however that it could be used with any phrase-based system. Experimental results for two translation tasks (section 4) clearly demonstrate the benefits of using discriminative training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. 2 n-gram-based CTMs The n-gram-based approach in Machine Translation is a variant of the phrase-based approach (Zens et al., 2002). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the 1046 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1046–1052, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source sentence has been reordered beforehand. 2.1 n-gram-based Machine Translation Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pai</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-based statistical machine translation. In KI ’02: Proceedings of the 25th Annual German Conference on AI, pages 18–32, London, UK. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
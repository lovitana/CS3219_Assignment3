<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001916">
<title confidence="0.976015">
Sentence Compression by Deletion with LSTMs
</title>
<author confidence="0.961191">
Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, Oriol Vinyals
</author>
<affiliation confidence="0.871498">
Google Research
</affiliation>
<email confidence="0.99829">
{katjaf,ealfonseca,crcarlos,lukaszkaiser,vinyals}@google.com
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990815">
We present an LSTM approach to
deletion-based sentence compression
where the task is to translate a sentence
into a sequence of zeros and ones, cor-
responding to token deletion decisions.
We demonstrate that even the most basic
version of the system, which is given no
syntactic information (no PoS or NE tags,
or dependencies) or desired compression
length, performs surprisingly well: around
30% of the compressions from a large test
set could be regenerated. We compare the
LSTM system with a competitive baseline
which is trained on the same amount of
data but is additionally provided with
all kinds of linguistic features. In an
experiment with human raters the LSTM-
based model outperforms the baseline
achieving 4.5 in readability and 3.8 in
informativeness.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954">
Sentence compression is a standard NLP task
where the goal is to generate a shorter paraphrase
of a sentence. Dozens of systems have been intro-
duced in the past two decades and most of them are
deletion-based: generated compressions are token
subsequences of the input sentences (Jing, 2000;
Knight &amp; Marcu, 2000; McDonald, 2006; Clarke
&amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to
name a few).
Existing compression systems heavily use syn-
tactic information to minimize chances of intro-
ducing grammatical mistakes in the output. A
common approach is to use only some syntactic
information (Jing, 2000; Clarke &amp; Lapata, 2008,
among others) or use syntactic features as signals
in a statistical model (McDonald, 2006). It is prob-
ably even more common to operate on syntactic
trees directly (dependency or constituency) and
generate compressions by pruning them (Knight
&amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011;
Filippova &amp; Altun, 2013, among others). Unfortu-
nately, this makes such systems vulnerable to error
propagation as there is no way to recover from an
incorrect parse tree. With the state-of-the-art pars-
ing systems achieving about 91 points in labeled
attachment accuracy (Zhang &amp; McDonald, 2014),
the problem is not a negligible one. To our knowl-
edge, there is no competitive compression system
so far which does not require any linguistic pre-
processing but tokenization.
In this paper we research the following ques-
tion: can a robust compression model be built
which only uses tokens and has no access to syn-
tactic or other linguistic information? While phe-
nomena like long-distance relations may seem to
make generation of grammatically correct com-
pressions impossible, we are going to present an
evidence to the contrary. In particular, we will
present a model which benefits from the very re-
cent advances in deep learning and uses word em-
beddings and Long Short Term Memory models
(LSTMs) to output surprisingly readable and in-
formative compressions. Trained on a corpus of
less than two million automatically extracted par-
allel sentences and using a standard tool to ob-
tain word embeddings, in its best and most sim-
ple configuration it achieves 4.5 points out of 5
in readability and 3.8 points in informativeness in
an extensive evaluation with human judges. We
believe that this is an important result as it may
suggest a new direction for sentence compression
research which is less tied to modeling linguistic
</bodyText>
<page confidence="0.971022">
360
</page>
<note confidence="0.98497">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 360–368,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9998206">
structures, especially syntactic ones, than the com-
pression work so far.
The paper is organized as follows: Section 3
presents a competitive baseline which implements
the system of McDonald (2006) for large training
sets. The LSTM model and its three configura-
tions are introduced in Section 4. The evaluation
set-up and a discussion on wins and losses with
examples are presented in Section 5 which is fol-
lowed by the conclusions.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999909956521739">
The problem formulation we adopt in this paper
is very simple: for every token in the input sen-
tence we ask whether it should be kept or dropped,
which translates into a sequence labeling problem
with just two labels: one and zero. The dele-
tion approach is a standard one in compression re-
search, although the problem is often formulated
over the syntactic structure and not the raw to-
ken sequence. That is, one usually drops con-
stituents or prunes dependency edges (Jing, 2000;
Knight &amp; Marcu, 2000; McDonald, 2006; Clarke
&amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011;
Filippova &amp; Altun, 2013). Thus, the relation to
existing compression work is that we also use the
deletion approach.
Recent advances in machine learning made it
possible to escape the typical paradigm of map-
ping a fixed dimensional input to a fixed dimen-
sional output to mapping an input sequence onto
an output sequence. Even though many of these
models were proposed more than a decade ago,
it is not until recently that they have empirically
been shown to perform well. Indeed, core prob-
lems in natural language processing such as trans-
lation (Cho et al., 2014; Sutskever et al., 2014;
Luong et al., 2014), parsing (Vinyals et al., 2014),
image captioning (Vinyals et al., 2015; Xu et al.,
2015), or learning to execute small programs
(Zaremba &amp; Sutskever, 2014) employed virtually
the same principles—the use of Recurrent Neural
Networks (RNNs). Thus, with regard to this line
of research, our work comes closest to the recent
machine translation work. An important differ-
ence is that we do not aim at building a model that
generates compressions directly but rather a model
which generates a sequence of deletion decisions.
A more complex translation model is also con-
ceivable and may significantly advance work on
compression by paraphrasing, of which there have
not been many examples yet (Cohn &amp; Lapata,
2008). However, in this paper our goal is to
demonstrate that a simple but robust deletion-
based system can be built without using any lin-
guistic features other than token boundaries. We
leave experiments with paraphrasing models to fu-
ture work.
</bodyText>
<sectionHeader confidence="0.998443" genericHeader="method">
3 Baseline
</sectionHeader>
<bodyText confidence="0.999654428571428">
We compare our model against the system of Mc-
Donald (2006) which also formulates sentence
compression as a binary sequence labeling prob-
lem. In contrast to our proposal, it makes use of
a large set of syntactic features which are treated
as soft evidence. The presence or absence of these
features is treated as signals which do not condi-
tion the output that the model can produce. There-
fore the model is robust against noise present in
the precomputed syntactic structures of the input
sentences.
The system was implemented based on the de-
scription by McDonald (2006) with two changes
which were necessary due to the large size of the
training data set used for model fitting. The first
change was related to the learning procedure and
the second one to the family of features used.
Regarding the learning procedure, the original
model uses a large-margin learning framework,
namely MIRA (Crammer &amp; Singer, 2003), but
with some minor changes as presented by McDon-
ald et al. (2005). In this set-up, online learn-
ing is performed, and at each step an optimiza-
tion procedure is made where K constraints are in-
cluded, which correspond to the top-K solutions
for a given training observation. This optimiza-
tion step is equivalent to a Quadratic Programming
problem if K &gt; 1, which is time-costly to solve,
and therefore not adequate for the large amount
of data we used for training the model. Further-
more, in his publication McDonald states clearly
that different values of K did not actually have
a major impact on the final performance of the
model. Consequently, and for the sake of being
able to successfully train the model with large-
scale data, the learning procedure is implemented
as a distributed structured perceptron with iterative
parameter mixing (McDonald et al., 2010), where
each shard is processed with MIRA and K is set to
1.
Setting K = 1 will only affect the weight up-
date described on line 4 of Figure 3 of McDonald
</bodyText>
<page confidence="0.953099">
361
</page>
<bodyText confidence="0.471425">
(2006), which is now expressed as:
</bodyText>
<equation confidence="0.974073125">
w(i+1) ← w(i) + τ x eyt,y&apos;
C0, L(yt, y0) − w · eyt,y&apos;
where τ = max ||eyt,y&apos;||2
eyt,y&apos; = F(xt, yt) − F(xt, y0)
y0 = best(x; w(i))
|y|
F(x, y) = 1: f(x,I(yj−1),I(yj))
j=2
</equation>
<bodyText confidence="0.999914615384615">
The second change concerns the feature set
used. While McDonald’s original model contains
deep syntactic features coming from both depen-
dency and constituency parse trees, we use only
dependency-based features. Additionally, and to
better compare the baseline with the LSTM mod-
els, we have included as an optional feature a
256-dimension embedding-vector representation
of each input word and its syntactic parent. The
vectors are pre-trained using the Skipgram model1
(Mikolov et al., 2013). Ultimately, our implemen-
tation of McDonald’s model contained 463,614 in-
dividual features, summarized in three categories:
</bodyText>
<listItem confidence="0.996406380952381">
• PoS features: Joint PoS tags of selected to-
kens. Unigram, bigram and trigram PoS con-
text of selected and dropped tokens. All the
previous features conjoined with one indicat-
ing if the last two selected tokens are adja-
cent.
• Deep syntactic features: Dependency labels
of taken and dropped tokens and their par-
ent dependencies. Boolean features indicat-
ing syntactic relations between selected to-
kens (i.e., siblings, parents, leaves, etc.). De-
pendency label of the least common ances-
tor in the dependency tree between a batch
of dropped tokens. All the previous features
conjoined with the PoS tag of the involved
tokens.
• Word features: Boolean features indicating
if a group of dropped nodes contain a com-
plete or incomplete parenthesization. Word-
embedding vectors of selected and dropped
tokens and their syntactic parents.
</listItem>
<bodyText confidence="0.9996145">
The model is fitted over ten epochs on the whole
training data, and for model selection a small de-
velopment set consisting of 5,000 previously un-
seen sentences is used (none of them belonging to
</bodyText>
<footnote confidence="0.905949">
1https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.9998835">
the evaluation set). The automated metric used for
this selection was accuracy@1 which is the pro-
portion of golden compressions which could be
fully reproduced. The performance on the devel-
opment set plateaus when getting close to the last
epoch.
</bodyText>
<sectionHeader confidence="0.997919" genericHeader="method">
4 The LSTM model
</sectionHeader>
<bodyText confidence="0.999877285714286">
Our approach is largely based on the sequence to
sequence paradigm proposed in Sutskever et al.
(2014). We train a model that maximizes the prob-
ability of the correct output given the input sen-
tence. Concretely, for each training pair (X, Y ),
we will learn a parametric model (with parameters
θ), by solving the following optimization problem:
</bodyText>
<equation confidence="0.983462333333333">
1:
θ∗ = arg max log p(Y |X; θ) (1)
θ X,Y
</equation>
<bodyText confidence="0.999160454545454">
where the sum is assumed to be over all train-
ing examples. To model the probability p, we
use the same architecture described by Sutskever
et al. (2014). In particular, we use a RNN
based on the Long Short Term Memory (LSTM)
unit (Hochreiter &amp; Schmidhuber, 1997), designed
to avoid vanishing gradients and to remember
some long-distance dependences from the input
sequence. Figure 1 shows a basic LSTM archi-
tecture. The RNN is fed with input words Xi (one
at a time), until we feed a special symbol “GO”. It
is now a common practice (Sutskever et al., 2014;
Li &amp; Jurafsky, 2015) to start feeding the input in
reversed order, as it has been shown to perform
better empirically. During the first pass over the
input, the network is expected to learn a com-
pact, distributed representation of the input sen-
tence, which will allow it to start generating the
right predictions when the second pass starts, after
the “GO” symbol is read.
We can apply the chain rule to decompose
Equation (1) as follows:
</bodyText>
<equation confidence="0.995608666666667">
T
p(Y |X; θ) = H p(Yt|Y1, ... , Yt−1, X; θ) (2)
t=1
</equation>
<bodyText confidence="0.998411666666667">
noting that we made no independence assump-
tions. Once we find the optimal θ∗, we construct
our estimated compression Yˆ as:
</bodyText>
<equation confidence="0.982134">
Yˆ = arg max p(Y |X; θ∗) (3)
Y
</equation>
<page confidence="0.974194">
362
</page>
<figure confidence="0.845258571428571">
Target sequence
Y0 Y1 Y2 ...
LSTM
layer
XN ... X2 X1 GO X1 X2 ...
t=0 ... t=N-1 t=N t=N+1 t=N+1 t=N+2 ...
Input sequence
</figure>
<figureCaption confidence="0.999692">
Figure 1: High-level overview of an LSTM unrolled through time.
</figureCaption>
<bodyText confidence="0.926692">
LSTM cell: Let us review the sequence-to-
sequence LSTM model. The Long Short Term
Memory model of Hochreiter &amp; Schmidhuber
(1997) is defined as follows. Let xt, ht, and
mt be the input, control state, and memory state
at timestep t. Then, given a sequence of inputs
(x1,... , xT), the LSTM computes the h-sequence
(h1, ... , hT) and the m-sequence (m1, ... , mT)
</bodyText>
<equation confidence="0.991010666666667">
sigm(W1xt + W2ht−1)
tanh(W3xt + W4ht−1)
sigm(W5xt + W6ht−1)
sigm(W7xt + W8ht−1)
mt = mt−1 O ft + it O i0t
ht = mt O ot
</equation>
<bodyText confidence="0.999209375">
The operator O denotes element-wise multiplica-
tion, the matrices W1, ... , W8 and the vector h0
are the parameters of the model, and all the non-
linearities are computed element-wise.
Stochastic gradient descent is used to maximize
the training objective (Eq. (1)) w.r.t. all the LSTM
parameters.
Network architecture: In these experiments we
have used the architecture depicted in Figure 3.
Following Vinyals et al. (2014), we have used
three stacked LSTM layers to allow the upper
layers to learn higher-order representations of the
input, interleaved with dropout layers to prevent
overfitting (Srivastava et al., 2014). The output
layer is a SoftMax classifier that predicts, after the
“GO” symbol is read, one of the following three
</bodyText>
<figure confidence="0.980755571428571">
softmax
LSTM layer
dropout
LSTM layer
dropout
LSTM layer
Embedding of current word Last label
</figure>
<figureCaption confidence="0.8418898">
Figure 3: Architecture of the network used for
sentence compression. Note that this basic struc-
ture is then unrolled 120 times, with the standard
dependences from LSTM networks (Hochreiter &amp;
Schmidhuber, 1997).
</figureCaption>
<bodyText confidence="0.99914775">
labels: 1, if a word is to be retained in the compres-
sion, 0 if a word is to be deleted, or EOS, which
is the output label used for the “GO” input and the
end-of-sentence final period.
Input representation: In the simplest imple-
mentation, that we call LSTM, the input layer
has 259 dimensions. The first 256 contain the
embedding-vector representation of the current in-
</bodyText>
<figure confidence="0.996288">
as follows
it =
i0t =
ft =
ot =
Output
Input
layer
</figure>
<page confidence="0.974229">
363
</page>
<table confidence="0.9432">
function DECODE(X)
� Initialize and feed the reversed input.
Lstm +— CREATELSTM
LayersState +— INITIALIZELAYERS(Lstm)
for all Xi E REVERSE(X) do
LayersState +— ONESTEP(Lstm, LayersState, Xi)
end for
LayersState +— ONESTEP(Lstm, LayersState, GO)
� Create the beam vector. Each item contains the state of the layers, the labels predicted so far, and probability.
Beam +— {(LayersState, (), 1.0)1
� Beam search
for all Xi E X do
NextBeam +— {1
for all (LayersState, Labels, Prob) E Beam do
(NextLayersState, Outputs) +— ONESTEP(Lstm, LayersState, Xi)
for all Output E Outputs do
NextBeam +— NextBeamU{(NextLayerState, Labels+Output.label, Prob*Output.prob)1
</table>
<tableCaption confidence="0.3434915">
end for
end for
</tableCaption>
<figure confidence="0.5168085">
Beam +— TOPN(NextBeam)
end for
return TOP(Beam)
end function
</figure>
<figureCaption confidence="0.999682">
Figure 2: Pseudocode of the beam-search algorithm for compressing an input sentence.
</figureCaption>
<bodyText confidence="0.998837565217391">
put word, pre-trained using the Skipgram model2
(Mikolov et al., 2013). The final three dimensions
contain a one-hot-spot representation of the gold-
standard label of the previous word (during train-
ing), or the generated label of the previous word
(during decoding).
For the LSTM+PAR architecture we first parse
the input sentence, and then we provide as input,
for each input word, the embedding-vector rep-
resentation of that word and its parent word in
the dependency tree. If the current input is the
root node, then a special parent embedding is con-
structed with all nodes set to zero except for one
node. In these settings we want to test the hypoth-
esis whether knowledge about the parent node can
be useful to decide if the current constituent is rel-
evant or not for the compression. The dimension-
ality of the input layer in this case is 515. Similarly
to McDonald (2006), syntax is used here as a soft
feature in the model.
For the LSTM+PAR+PRES architecture, we
again parse the input sentence, and use a 518-sized
embedding vector, that includes:
</bodyText>
<listItem confidence="0.999435">
• The embedding vector for the current word
(256 dimensions).
• The embedding vector for the parent word
(256 dimensions).
• The label predicted for the last word (3 di-
mensions).
• A bit indicating whether the parent word has
</listItem>
<footnote confidence="0.811033">
2https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.9438635">
already been seen and kept in the compres-
sion (1 dimension).
</bodyText>
<listItem confidence="0.996933">
• A bit indicating whether the parent word has
already been seen but discarded (1 dimen-
sion).
• A bit indicating whether the parent word
comes later in the input (1 dimension).
</listItem>
<bodyText confidence="0.999067">
Decoding: Eq. (3) involves searching through
all possible output sequences (given X). Con-
trary to the baseline, in the case of LSTMs the
complete previous history is taken into account
for each prediction and we cannot simplify Eq. (2)
with a Markov assumption. Therefore, the search
space at decoding time is exponential on the length
of the input, and we have used a beam-search pro-
cedure as described in Figure 2.
Fixed parameters: For training, we unfold the
network 120 times and make sure that none of our
training instances is longer than that. The learn-
ing rate is initialized at 2, with a decay factor of
0.96 every 300,000 traning steps. The dropping
probability for the dropout layers is 0.2. The num-
ber of nodes in each LSTM layer is always identi-
cal to the number of nodes in the input layer. We
have not tuned these parameters nor the number of
stacked layers.
</bodyText>
<page confidence="0.993501">
364
</page>
<figure confidence="0.967773">
5 Evaluation ratios are very close, a comparison of the systems’
5.1 Data scores is justified (Napoles et al., 2011).
</figure>
<bodyText confidence="0.996431111111111">
Both the LSTM systems we introduced and the
baseline require a training set of a considerable
size. In particular, the LSTM model uses 256-
dimensional embeddings of token sequences and
cannot be expected to perform well if trained on
a thousand parallel sentences, which is the size of
the commonly used data sets (Knight &amp; Marcu,
2000; Clarke &amp; Lapata, 2006). Following the
method of Filippova &amp; Altun (2013), we collect
a much larger corpus of about two million paral-
lel sentence-compression instances from the news
where every compression is a subsequence of to-
kens from the input. For testing, we use the pub-
licly released set of 10,000 sentence-compression
pairs3. We take the first 200 sentences from this set
for the manual evaluation with human raters, and
the first 1,000 sentences for the automatic evalua-
tion.
</bodyText>
<subsectionHeader confidence="0.980541">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999934846153846">
We evaluate the baseline and our systems on the
200-sentence test set in an experiment with human
raters. The raters were asked to rate readability
and informativeness of compressions given the in-
put which are the standard evaluation metrics for
compression. The former covers the grammatical
correctness, comprehensibility and fluency of the
output while the latter measures the amount of im-
portant content preserved in the compression.
Additionally, for experiments on the develop-
ment set, we used two metrics for automatic eval-
uation: per-sentence accuracy (i.e., how many
compressions could be fully reproduced) and
word-based F1-score. The latter differs from
the RASP-based relation F-score by Riezler et al.
(2003) in that we simply compute the recall and
precision in terms of tokens kept in the golden
and the generated compressions. We report these
results for completeness although it is the results
of the human evaluation from which we draw our
conclusions.
Compression ratio: The three versions of our
system (LSTM*) and the baseline (MIRA) have
comparable compression ratios (CR) which are
defined as the length of the compression in char-
acters divided over the sentence length. Since the
</bodyText>
<footnote confidence="0.8548915">
3http://storage.googleapis.com/
sentencecomp/compressiondata.json
</footnote>
<tableCaption confidence="0.767763">
Automatic evaluation: A total of 1,000 sen-
tence pairs from the test set4 were used in the au-
tomatic evaluation. The results are summarized in
Table 1.
</tableCaption>
<table confidence="0.9998596">
F1 Acc CR
MIRA 0.75 0.21 0.37
LSTM 0.80 0.30 0.39
LSTM+PAR 0.81 0.31 0.38
LSTM+PAR+PRES 0.82 0.34 0.38
</table>
<tableCaption confidence="0.927482">
Table 1: F1-score, per-sentence accuracy and
compression ratio for the baseline and the systems
</tableCaption>
<bodyText confidence="0.999585611111111">
There is a significant difference in performance of
the MIRA baseline and the LSTM models, both in
terms of F1-score and in accuracy. More than 30%
of golden compressions could be fully regenerated
by the LSTM systems which is in sharp contrast
with the 20% of MIRA. The differences in F-score
between the three versions of LSTM are not sig-
nificant, all scores are close to 0.81.
Evaluation with humans: The first 200 sen-
tences from the set of 1,000 used in the automatic
evaluation were compressed by each of the four
systems. Every sentence-compression pair was
rated by three raters who were asked to select a
rating on a five-point Likert scale, ranging from
one to five. In very few cases (around 1%) the
ratings were inconclusive (i.e., 1, 3, 5 were given
to the same pair) and had to be skipped. Table 2
summarizes the results.
</bodyText>
<table confidence="0.9981532">
read info
MIRA 4.31 3.55
LSTM 4.51† 3.78†
LSTM+PAR 4.40 3.73
LSTM+PAR+PRES 4.37 3.79†
</table>
<tableCaption confidence="0.991598">
Table 2: Readability and informativeness for the
</tableCaption>
<bodyText confidence="0.842044833333333">
baseline and the systems: † stands for significantly
better than MIRA with 0.95 confidence.
The results indicate that the LSTM models pro-
duce more readable and more informative com-
pressions. Interestingly, there is no benefit in us-
ing the syntactic information, at least not with
</bodyText>
<footnote confidence="0.991039">
4We used the very first 1,000 instances.
</footnote>
<page confidence="0.992669">
365
</page>
<table confidence="0.996877857142857">
Sentence &amp; LSTM Compression difficulty
A Virginia state senator and one-time candidate for governor stabbed by his son said Friday that he is quotes
“alive so must live,” his first public statement since the assault and his son’s suicide shortly thereafter.
State senator alive so must live.
Gwyneth Paltrow, 41 and husband Chris Martin, 37 are to separate after more than 10 years of marriage, commas
the actress announced on her website GOOP.
Gwyneth Paltrow are to separate.
Chris Hemsworth and the crew of his new movie ’In the Heart of the Sea’ were forced to flee flash floods quotes
in the Canary Islands yesterday.
Chris Hemsworth were forced to flee flash floods.
Police in Deltona, Fla., are trying to sniff out the identity of a man who allegedly attempted to pay nothing
his water bill with cocaine. to remove
Police are trying to sniff out the identity.
Just a week after a CISF trooper foiled a suicide bid by a woman in the Delhi metro, another woman important
trooper from the same force prevented two women commuters from ending their lives, an official context
said Monday.
Another woman trooper prevented two women commuters.
Whatever the crisis or embarrassment to his administration, Pres. Obama don’t know nuttin’ about it. nothing
Pres. Obama don’t know nuttin. to remove
TRADE and Industry Minister Rob Davies defended the government’s economic record in Parliament
on Tuesday, saying it had implemented structural reforms and countercyclical infrastructure projects
to help shore up the economy.
Rob Davies defended the government’s economic record.
Social activist Medha Patkar on Monday extended her “complete” support to Arvind Kejriwal-led
Aam Aadmi Party in Maharashtra.
Medha Patkar extended her support to Aam Aadmi Party.
State Sen. Stewart Greenleaf discusses his proposed human trafficking bill
at Calvery Baptist Church in Willow Grove Thursday night.
Stewart Greenleaf discusses his human trafficking bill.
Alan Turing, known as the father of computer science, the codebreaker that helped win World War 2,
and the man tortured by the state for being gay, is to receive a pardon nearly 60 years after his death.
Alan Turing is to receive a pardon.
Robert Levinson, an American who disappeared in Iran in 2007, was in the country working for the CIA,
according to a report from the Associated Press’s Matt Apuzzo and Adam Goldman.
Robert Levinson was working for the CIA.
</table>
<figureCaption confidence="0.998042">
Figure 4: Example sentences and compressions.
</figureCaption>
<bodyText confidence="0.999871527777778">
the amount of parallel data we had at our dis-
posal. The simple LSTM model which only uses
token embeddings to generate a sequence of dele-
tion decisions significantly outperforms the base-
line which was given not only embeddings but also
syntactic and other features.
Discussion: What are the wins and losses of the
LSTM systems? Figure 4 presents some of the
evaluated sentence-compression pairs. In terms
of readability, the basic LSTM system performed
surprisingly well. Only in a few cases (out of 200)
did it get an average score of two or three. Sen-
tences which pose difficulty to the model are the
ones with quotes, intervening commas, or other
uncommon punctuation patterns. For example, in
the second sentence in Figure 4, if one removes
from the input the age modifiers and the preced-
ing commas, the words and Chris Martin are not
dropped and the output compression is grammati-
cal, preserving both conjoined elements.
With regard to informativeness, the difficult
cases are those where there is very little to be re-
moved and where the model still removed more
than a half to achieve the compression ratio it ob-
served in the training data. For example, the only
part that can be removed from the fourth sentence
in Figure 4 is the modifier of police, everything
else being important content. Similarly, in the fifth
sentence the context of the event must be retained
in the compression for the event to be interpreted
correctly.
Arguably, such cases would also be difficult for
other systems. In particular, recognizing when the
context is crucial is a problem that can be solved
only by including deep semantic and discourse
features which has not been attempted yet. And
</bodyText>
<page confidence="0.996658">
366
</page>
<bodyText confidence="0.999927913043478">
sentences with quotes (direct speech, a song or a
book title, etc.) are challenging for parsers which
in turn provide important signals for most com-
pression systems.
The bottom of Figure 4 contains examples of
good compressions. Even though for a signifi-
cant number of input sentences the compression
was a continuous subsequence of tokens, there are
many discontinuous compressions. In particular,
the LSTM model learned to drop appositions, no
matter how long they are, temporal expressions,
optional modifiers, introductory clauses, etc.
Our understanding of why the extended model
(LSTM+PAR+PRES) performed worse in the hu-
man evlauation than the base model is that, in
the absence of syntactic features, the basic LSTM
learned a model of syntax useful for compression,
while LSTM++, which was given syntactic infor-
mation, learned to optimize for the particular way
the ”golden” set was created (tree pruning). While
the automatic evaluation penalized all deviations
from the single golden variant, in human evals
there was no penalty for readable alternatives.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998485">
We presented, to our knowledge, a first attempt at
building a competitive compression system which
is given no linguistic features from the input. The
two important components of the system are (1)
word embeddings, which can be obtained by any-
one either pre-trained, or by running word2vec
on a large corpus, and (2) an LSTM model which
draws on the very recent advances in research
on RNNs. The training data of about two mil-
lion sentence-compression pairs was collected au-
tomatically from the Internet.
Our results clearly indicate that a compression
model which is not given syntactic information ex-
plicitly in the form of features may still achieve
competitive performance. The high readability
and informativeness scores assigned by human
raters support this claim. In the future, we are
planning to experiment with more “interesting”
paraphrasing models which translate the input not
into a zero-one sequence but into words.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991880630434783">
Berg-Kirkpatrick, T., D. Gillick &amp; D. Klein
(2011). Jointly learning to extract and com-
press. In Proc. of ACL-11.
Cho, K., B. van Merrienboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk &amp; Y. Ben-
gio (2014). Learning phrase representations us-
ing rnn encoder-decoder for statistical machine
translation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, Doha, Qatar, 25–29 October
2014.
Clarke, J. &amp; M. Lapata (2006). Models for
sentence compression: A comparison across
domains, training requirements and evaluation
measures. In Proc. of COLING-ACL-06, pp.
377–385.
Clarke, J. &amp; M. Lapata (2008). Global infer-
ence for sentence compression: An integer lin-
ear programming approach. Journal ofArtificial
Intelligence Research, 31:399–429.
Cohn, T. &amp; M. Lapata (2008). Sentence com-
pression beyond word deletion. In Proc. of
COLING-08, pp. 137–144.
Crammer, K. &amp; Y. Singer (2003). Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–
991.
Filippova, K. &amp; Y. Altun (2013). Overcoming the
lack of parallel data in sentence compression. In
Proc. of EMNLP-13, pp. 1481–1491.
Hochreiter, S. &amp; J. Schmidhuber (1997). Long
short-term memory. Neural computation,
9(8):1735–1780.
Jing, H. (2000). Sentence reduction for automatic
text summarization. In Proc. of ANLP-00, pp.
310–315.
Knight, K. &amp; D. Marcu (2000). Statistics-based
summarization – step one: Sentence compres-
sion. In Proc. of AAAI-00, pp. 703–711.
Li, J. &amp; D. Jurafsky (2015). A hierarchical LSTM
autoencoder for paragraphs and documents. In
Proc. of ACL-1S.
Luong, M.-T., I. Sutskever, Q. V. Le, O. Vinyals &amp;
W. Zaremba (2014). Addressing the rare word
problem in neural machine translation. CoRR,
abs/1410.8206.
</reference>
<page confidence="0.9795">
367
</page>
<reference confidence="0.999917235294118">
McDonald, R. (2006). Discriminative sentence
compression with soft syntactic evidence. In
Proc. of EACL-06, pp. 297–304.
McDonald, R., K. Crammer &amp; F. Pereira (2005).
Online large-margin training of dependency
parsers. In Proc. ofACL-05, pp. 91–98.
McDonald, R., K. Hall &amp; G. Mann (2010). Dis-
tributed training strategies for the structured
perceptron. In Proc. of NAACL-HLT-10, pp.
456–464.
Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado
&amp; J. Dean (2013). Distributed representations of
words and phrases and their compositionality.
In Advances in Neural Information Processing
Systems, pp. 3111–3119.
Napoles, C., C. Callison-Burch &amp; B. Van Durme
(2011). Evaluating sentence compression: Pit-
falls and suggested remedies. In Proceedings of
the Workshop on Monolingual Text-to-text Gen-
eration, Portland, OR, June 24 2011, pp. 91–97.
Riezler, S., T. H. King, R. Crouch &amp; A. Zaenen
(2003). Statistical sentence condensation using
ambiguity packing and stochastic disambigua-
tion methods for Lexical-Functional Grammar.
In Proc. of HLT-NAACL-03, pp. 118–125.
Srivastava, N., G. Hinton, A. Krizhevsky,
I. Sutskever &amp; R. Salakhutdinov (2014).
Dropout: A simple way to prevent neural net-
works from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958.
Sutskever, I., O. Vinyals &amp; Q. V. Le (2014). Se-
quence to sequence learning with neural net-
works. In Proc. of NIPS-2014.
Vinyals, O., A. Toshev, S. Bengio &amp; D. Erhan
(2015). Show and tell: A neural image caption
generator. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recogni-
tion, CVPR-2015.
Vinyals, O., L. Kaiser, T. Koo, S. Petrov,
I. Sutskever &amp; G. E. Hinton (2014). Grammar
as a foreign language. CoRR, abs/1412.7449.
Xu, K., J. Ba, R. Kiros, K. Cho, A. Courville,
R. Salakhutdinov, R. Zemel &amp; Y. Bengio
(2015). Show, attend and tell: Neural image
caption generation with visual attention. In Pro-
ceedings of ICML-2015.
Zaremba, W. &amp; I. Sutskever (2014). Learning to
execute. CoRR, abs/1410.4615.
Zhang, H. &amp; R. McDonald (2014). Enforcing
structural diversity in cube-pruned dependency
parsing. In Proc. of ACL-14, pp. 656–661.
</reference>
<page confidence="0.998263">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336630">
<title confidence="0.652344">Sentence Compression by Deletion with LSTMs Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, Oriol Vinyals Google Research</title>
<abstract confidence="0.995629095238095">We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions. We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated. We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Gillick</author>
<author>D Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-11.</booktitle>
<contexts>
<context position="1379" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="204" endWordPosition="207">on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 1 Introduction Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013, among others). Unfortunately, this make</context>
<context position="4646" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="734" endWordPosition="737">tion 5 which is followed by the conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence. That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et </context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Berg-Kirkpatrick, T., D. Gillick &amp; D. Klein (2011). Jointly learning to extract and compress. In Proc. of ACL-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Cho</author>
<author>B van Merrienboer</author>
<author>C Gulcehre</author>
<author>D Bahdanau</author>
<author>F Bougares</author>
<author>H Schwenk</author>
<author>Y Bengio</author>
</authors>
<title>Learning phrase representations using rnn encoder-decoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Doha,</location>
<marker>Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk, Bengio, 2014</marker>
<rawString>Cho, K., B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk &amp; Y. Bengio (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL-06,</booktitle>
<pages>377--385</pages>
<contexts>
<context position="17753" citStr="Clarke &amp; Lapata, 2006" startWordPosition="2959" endWordPosition="2962">M layer is always identical to the number of nodes in the input layer. We have not tuned these parameters nor the number of stacked layers. 364 5 Evaluation ratios are very close, a comparison of the systems’ 5.1 Data scores is justified (Napoles et al., 2011). Both the LSTM systems we introduced and the baseline require a training set of a considerable size. In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight &amp; Marcu, 2000; Clarke &amp; Lapata, 2006). Following the method of Filippova &amp; Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input. For testing, we use the publicly released set of 10,000 sentence-compression pairs3. We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation. 5.2 Experiments We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters. The raters were </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>Clarke, J. &amp; M. Lapata (2006). Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proc. of COLING-ACL-06, pp. 377–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="1348" citStr="Clarke &amp; Lapata, 2008" startWordPosition="200" endWordPosition="203">eline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 1 Introduction Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013, among ot</context>
<context position="4615" citStr="Clarke &amp; Lapata, 2008" startWordPosition="730" endWordPosition="733">es are presented in Section 5 which is followed by the conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence. That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Su</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>Clarke, J. &amp; M. Lapata (2008). Global inference for sentence compression: An integer linear programming approach. Journal ofArtificial Intelligence Research, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proc. of COLING-08,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="5965" citStr="Cohn &amp; Lapata, 2008" startWordPosition="953" endWordPosition="956">), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have not been many examples yet (Cohn &amp; Lapata, 2008). However, in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries. We leave experiments with paraphrasing models to future work. 3 Baseline We compare our model against the system of McDonald (2006) which also formulates sentence compression as a binary sequence labeling problem. In contrast to our proposal, it makes use of a large set of syntactic features which are treated as soft evidence. The presence or absence of these features is treated as signals which do not condition the ou</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Cohn, T. &amp; M. Lapata (2008). Sentence compression beyond word deletion. In Proc. of COLING-08, pp. 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>991</pages>
<contexts>
<context position="7129" citStr="Crammer &amp; Singer, 2003" startWordPosition="1149" endWordPosition="1152">eatures is treated as signals which do not condition the output that the model can produce. Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences. The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used for model fitting. The first change was related to the learning procedure and the second one to the family of features used. Regarding the learning procedure, the original model uses a large-margin learning framework, namely MIRA (Crammer &amp; Singer, 2003), but with some minor changes as presented by McDonald et al. (2005). In this set-up, online learning is performed, and at each step an optimization procedure is made where K constraints are included, which correspond to the top-K solutions for a given training observation. This optimization step is equivalent to a Quadratic Programming problem if K &gt; 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model. Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on t</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Crammer, K. &amp; Y. Singer (2003). Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951– 991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>Y Altun</author>
</authors>
<title>Overcoming the lack of parallel data in sentence compression.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP-13,</booktitle>
<pages>1481--1491</pages>
<contexts>
<context position="1938" citStr="Filippova &amp; Altun, 2013" startWordPosition="291" endWordPosition="294">nald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013, among others). Unfortunately, this makes such systems vulnerable to error propagation as there is no way to recover from an incorrect parse tree. With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy (Zhang &amp; McDonald, 2014), the problem is not a negligible one. To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization. In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other</context>
<context position="4672" citStr="Filippova &amp; Altun, 2013" startWordPosition="738" endWordPosition="741"> conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence. That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinya</context>
<context position="17803" citStr="Filippova &amp; Altun (2013)" startWordPosition="2967" endWordPosition="2970">es in the input layer. We have not tuned these parameters nor the number of stacked layers. 364 5 Evaluation ratios are very close, a comparison of the systems’ 5.1 Data scores is justified (Napoles et al., 2011). Both the LSTM systems we introduced and the baseline require a training set of a considerable size. In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight &amp; Marcu, 2000; Clarke &amp; Lapata, 2006). Following the method of Filippova &amp; Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input. For testing, we use the publicly released set of 10,000 sentence-compression pairs3. We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation. 5.2 Experiments We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters. The raters were asked to rate readability and informativeness of c</context>
</contexts>
<marker>Filippova, Altun, 2013</marker>
<rawString>Filippova, K. &amp; Y. Altun (2013). Overcoming the lack of parallel data in sentence compression. In Proc. of EMNLP-13, pp. 1481–1491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hochreiter</author>
<author>J Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="10946" citStr="Hochreiter &amp; Schmidhuber, 1997" startWordPosition="1792" endWordPosition="1795">argely based on the sequence to sequence paradigm proposed in Sutskever et al. (2014). We train a model that maximizes the probability of the correct output given the input sentence. Concretely, for each training pair (X, Y ), we will learn a parametric model (with parameters θ), by solving the following optimization problem: 1: θ∗ = arg max log p(Y |X; θ) (1) θ X,Y where the sum is assumed to be over all training examples. To model the probability p, we use the same architecture described by Sutskever et al. (2014). In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter &amp; Schmidhuber, 1997), designed to avoid vanishing gradients and to remember some long-distance dependences from the input sequence. Figure 1 shows a basic LSTM architecture. The RNN is fed with input words Xi (one at a time), until we feed a special symbol “GO”. It is now a common practice (Sutskever et al., 2014; Li &amp; Jurafsky, 2015) to start feeding the input in reversed order, as it has been shown to perform better empirically. During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right pre</context>
<context position="12205" citStr="Hochreiter &amp; Schmidhuber (1997)" startWordPosition="2023" endWordPosition="2026">s starts, after the “GO” symbol is read. We can apply the chain rule to decompose Equation (1) as follows: T p(Y |X; θ) = H p(Yt|Y1, ... , Yt−1, X; θ) (2) t=1 noting that we made no independence assumptions. Once we find the optimal θ∗, we construct our estimated compression Yˆ as: Yˆ = arg max p(Y |X; θ∗) (3) Y 362 Target sequence Y0 Y1 Y2 ... LSTM layer XN ... X2 X1 GO X1 X2 ... t=0 ... t=N-1 t=N t=N+1 t=N+1 t=N+2 ... Input sequence Figure 1: High-level overview of an LSTM unrolled through time. LSTM cell: Let us review the sequence-tosequence LSTM model. The Long Short Term Memory model of Hochreiter &amp; Schmidhuber (1997) is defined as follows. Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, given a sequence of inputs (x1,... , xT), the LSTM computes the h-sequence (h1, ... , hT) and the m-sequence (m1, ... , mT) sigm(W1xt + W2ht−1) tanh(W3xt + W4ht−1) sigm(W5xt + W6ht−1) sigm(W7xt + W8ht−1) mt = mt−1 O ft + it O i0t ht = mt O ot The operator O denotes element-wise multiplication, the matrices W1, ... , W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise. Stochastic gradient descent is used to maximize the training obj</context>
<context position="13598" citStr="Hochreiter &amp; Schmidhuber, 1997" startWordPosition="2258" endWordPosition="2261">et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014). The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three softmax LSTM layer dropout LSTM layer dropout LSTM layer Embedding of current word Last label Figure 3: Architecture of the network used for sentence compression. Note that this basic structure is then unrolled 120 times, with the standard dependences from LSTM networks (Hochreiter &amp; Schmidhuber, 1997). labels: 1, if a word is to be retained in the compression, 0 if a word is to be deleted, or EOS, which is the output label used for the “GO” input and the end-of-sentence final period. Input representation: In the simplest implementation, that we call LSTM, the input layer has 259 dimensions. The first 256 contain the embedding-vector representation of the current inas follows it = i0t = ft = ot = Output Input layer 363 function DECODE(X) � Initialize and feed the reversed input. Lstm +— CREATELSTM LayersState +— INITIALIZELAYERS(Lstm) for all Xi E REVERSE(X) do LayersState +— ONESTEP(Lstm, </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Hochreiter, S. &amp; J. Schmidhuber (1997). Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proc. of ANLP-00,</booktitle>
<pages>310--315</pages>
<contexts>
<context position="1287" citStr="Jing, 2000" startWordPosition="192" endWordPosition="193"> We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 1 Introduction Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Ber</context>
<context position="4554" citStr="Jing, 2000" startWordPosition="722" endWordPosition="723">up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence. That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, H. (2000). Sentence reduction for automatic text summarization. In Proc. of ANLP-00, pp. 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization – step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI-00,</booktitle>
<pages>703--711</pages>
<contexts>
<context position="1309" citStr="Knight &amp; Marcu, 2000" startWordPosition="194" endWordPosition="197">the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 1 Introduction Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., </context>
<context position="4576" citStr="Knight &amp; Marcu, 2000" startWordPosition="724" endWordPosition="727">cussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence. That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing su</context>
<context position="17729" citStr="Knight &amp; Marcu, 2000" startWordPosition="2955" endWordPosition="2958">r of nodes in each LSTM layer is always identical to the number of nodes in the input layer. We have not tuned these parameters nor the number of stacked layers. 364 5 Evaluation ratios are very close, a comparison of the systems’ 5.1 Data scores is justified (Napoles et al., 2011). Both the LSTM systems we introduced and the baseline require a training set of a considerable size. In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight &amp; Marcu, 2000; Clarke &amp; Lapata, 2006). Following the method of Filippova &amp; Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input. For testing, we use the publicly released set of 10,000 sentence-compression pairs3. We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation. 5.2 Experiments We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Knight, K. &amp; D. Marcu (2000). Statistics-based summarization – step one: Sentence compression. In Proc. of AAAI-00, pp. 703–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>D Jurafsky</author>
</authors>
<title>A hierarchical LSTM autoencoder for paragraphs and documents.</title>
<date>2015</date>
<booktitle>In Proc. of ACL-1S.</booktitle>
<contexts>
<context position="11262" citStr="Li &amp; Jurafsky, 2015" startWordPosition="1848" endWordPosition="1851"> 1: θ∗ = arg max log p(Y |X; θ) (1) θ X,Y where the sum is assumed to be over all training examples. To model the probability p, we use the same architecture described by Sutskever et al. (2014). In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter &amp; Schmidhuber, 1997), designed to avoid vanishing gradients and to remember some long-distance dependences from the input sequence. Figure 1 shows a basic LSTM architecture. The RNN is fed with input words Xi (one at a time), until we feed a special symbol “GO”. It is now a common practice (Sutskever et al., 2014; Li &amp; Jurafsky, 2015) to start feeding the input in reversed order, as it has been shown to perform better empirically. During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right predictions when the second pass starts, after the “GO” symbol is read. We can apply the chain rule to decompose Equation (1) as follows: T p(Y |X; θ) = H p(Yt|Y1, ... , Yt−1, X; θ) (2) t=1 noting that we made no independence assumptions. Once we find the optimal θ∗, we construct our estimated compression Yˆ as: Yˆ = </context>
</contexts>
<marker>Li, Jurafsky, 2015</marker>
<rawString>Li, J. &amp; D. Jurafsky (2015). A hierarchical LSTM autoencoder for paragraphs and documents. In Proc. of ACL-1S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-T Luong</author>
<author>I Sutskever</author>
<author>Q V Le</author>
<author>O Vinyals</author>
<author>W Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2014</date>
<location>CoRR, abs/1410.8206.</location>
<contexts>
<context position="5256" citStr="Luong et al., 2014" startWordPosition="839" endWordPosition="842">al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly adva</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2014</marker>
<rawString>Luong, M.-T., I. Sutskever, Q. V. Le, O. Vinyals &amp; W. Zaremba (2014). Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="1325" citStr="McDonald, 2006" startWordPosition="198" endWordPosition="199"> competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 1 Introduction Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence. Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight &amp; Marcu, 2000; McDonald, 2006; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few). Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output. A common approach is to use only some syntactic information (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova </context>
<context position="3826" citStr="McDonald (2006)" startWordPosition="594" endWordPosition="595">ts in informativeness in an extensive evaluation with human judges. We believe that this is an important result as it may suggest a new direction for sentence compression research which is less tied to modeling linguistic 360 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 360–368, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. structures, especially syntactic ones, than the compression work so far. The paper is organized as follows: Section 3 presents a competitive baseline which implements the system of McDonald (2006) for large training sets. The LSTM model and its three configurations are introduced in Section 4. The evaluation set-up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions. 2 Related Work The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero. The deletion approach is a standard one in compression research, although the problem is often formulated over the sy</context>
<context position="6274" citStr="McDonald (2006)" startWordPosition="1007" endWordPosition="1009">ing a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have not been many examples yet (Cohn &amp; Lapata, 2008). However, in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries. We leave experiments with paraphrasing models to future work. 3 Baseline We compare our model against the system of McDonald (2006) which also formulates sentence compression as a binary sequence labeling problem. In contrast to our proposal, it makes use of a large set of syntactic features which are treated as soft evidence. The presence or absence of these features is treated as signals which do not condition the output that the model can produce. Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences. The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used</context>
<context position="15717" citStr="McDonald (2006)" startWordPosition="2611" endWordPosition="2612">ng). For the LSTM+PAR architecture we first parse the input sentence, and then we provide as input, for each input word, the embedding-vector representation of that word and its parent word in the dependency tree. If the current input is the root node, then a special parent embedding is constructed with all nodes set to zero except for one node. In these settings we want to test the hypothesis whether knowledge about the parent node can be useful to decide if the current constituent is relevant or not for the compression. The dimensionality of the input layer in this case is 515. Similarly to McDonald (2006), syntax is used here as a soft feature in the model. For the LSTM+PAR+PRES architecture, we again parse the input sentence, and use a 518-sized embedding vector, that includes: • The embedding vector for the current word (256 dimensions). • The embedding vector for the parent word (256 dimensions). • The label predicted for the last word (3 dimensions). • A bit indicating whether the parent word has 2https://code.google.com/p/word2vec/ already been seen and kept in the compression (1 dimension). • A bit indicating whether the parent word has already been seen but discarded (1 dimension). • A </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>McDonald, R. (2006). Discriminative sentence compression with soft syntactic evidence. In Proc. of EACL-06, pp. 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ofACL-05,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="7197" citStr="McDonald et al. (2005)" startWordPosition="1161" endWordPosition="1165">the model can produce. Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences. The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used for model fitting. The first change was related to the learning procedure and the second one to the family of features used. Regarding the learning procedure, the original model uses a large-margin learning framework, namely MIRA (Crammer &amp; Singer, 2003), but with some minor changes as presented by McDonald et al. (2005). In this set-up, online learning is performed, and at each step an optimization procedure is made where K constraints are included, which correspond to the top-K solutions for a given training observation. This optimization step is equivalent to a Quadratic Programming problem if K &gt; 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model. Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model. Consequently, and for the sake of</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., K. Crammer &amp; F. Pereira (2005). Online large-margin training of dependency parsers. In Proc. ofACL-05, pp. 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Hall</author>
<author>G Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT-10,</booktitle>
<pages>456--464</pages>
<contexts>
<context position="7995" citStr="McDonald et al., 2010" startWordPosition="1295" endWordPosition="1298"> a given training observation. This optimization step is equivalent to a Quadratic Programming problem if K &gt; 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model. Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model. Consequently, and for the sake of being able to successfully train the model with largescale data, the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing (McDonald et al., 2010), where each shard is processed with MIRA and K is set to 1. Setting K = 1 will only affect the weight update described on line 4 of Figure 3 of McDonald 361 (2006), which is now expressed as: w(i+1) ← w(i) + τ x eyt,y&apos; C0, L(yt, y0) − w · eyt,y&apos; where τ = max ||eyt,y&apos;||2 eyt,y&apos; = F(xt, yt) − F(xt, y0) y0 = best(x; w(i)) |y| F(x, y) = 1: f(x,I(yj−1),I(yj)) j=2 The second change concerns the feature set used. While McDonald’s original model contains deep syntactic features coming from both dependency and constituency parse trees, we use only dependency-based features. Additionally, and to bette</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>McDonald, R., K. Hall &amp; G. Mann (2010). Distributed training strategies for the structured perceptron. In Proc. of NAACL-HLT-10, pp. 456–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>K Chen</author>
<author>G S Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="8849" citStr="Mikolov et al., 2013" startWordPosition="1445" endWordPosition="1448"> − w · eyt,y&apos; where τ = max ||eyt,y&apos;||2 eyt,y&apos; = F(xt, yt) − F(xt, y0) y0 = best(x; w(i)) |y| F(x, y) = 1: f(x,I(yj−1),I(yj)) j=2 The second change concerns the feature set used. While McDonald’s original model contains deep syntactic features coming from both dependency and constituency parse trees, we use only dependency-based features. Additionally, and to better compare the baseline with the LSTM models, we have included as an optional feature a 256-dimension embedding-vector representation of each input word and its syntactic parent. The vectors are pre-trained using the Skipgram model1 (Mikolov et al., 2013). Ultimately, our implementation of McDonald’s model contained 463,614 individual features, summarized in three categories: • PoS features: Joint PoS tags of selected tokens. Unigram, bigram and trigram PoS context of selected and dropped tokens. All the previous features conjoined with one indicating if the last two selected tokens are adjacent. • Deep syntactic features: Dependency labels of taken and dropped tokens and their parent dependencies. Boolean features indicating syntactic relations between selected tokens (i.e., siblings, parents, leaves, etc.). Dependency label of the least comm</context>
<context position="14911" citStr="Mikolov et al., 2013" startWordPosition="2469" endWordPosition="2472">tor. Each item contains the state of the layers, the labels predicted so far, and probability. Beam +— {(LayersState, (), 1.0)1 � Beam search for all Xi E X do NextBeam +— {1 for all (LayersState, Labels, Prob) E Beam do (NextLayersState, Outputs) +— ONESTEP(Lstm, LayersState, Xi) for all Output E Outputs do NextBeam +— NextBeamU{(NextLayerState, Labels+Output.label, Prob*Output.prob)1 end for end for Beam +— TOPN(NextBeam) end for return TOP(Beam) end function Figure 2: Pseudocode of the beam-search algorithm for compressing an input sentence. put word, pre-trained using the Skipgram model2 (Mikolov et al., 2013). The final three dimensions contain a one-hot-spot representation of the goldstandard label of the previous word (during training), or the generated label of the previous word (during decoding). For the LSTM+PAR architecture we first parse the input sentence, and then we provide as input, for each input word, the embedding-vector representation of that word and its parent word in the dependency tree. If the current input is the root node, then a special parent embedding is constructed with all nodes set to zero except for one node. In these settings we want to test the hypothesis whether know</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado &amp; J. Dean (2013). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Napoles</author>
<author>C Callison-Burch</author>
<author>B Van Durme</author>
</authors>
<title>Evaluating sentence compression: Pitfalls and suggested remedies.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-to-text Generation,</booktitle>
<volume>24</volume>
<pages>91--97</pages>
<location>Portland, OR,</location>
<marker>Napoles, Callison-Burch, Van Durme, 2011</marker>
<rawString>Napoles, C., C. Callison-Burch &amp; B. Van Durme (2011). Evaluating sentence compression: Pitfalls and suggested remedies. In Proceedings of the Workshop on Monolingual Text-to-text Generation, Portland, OR, June 24 2011, pp. 91–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R Crouch</author>
<author>A Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL-03,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="18952" citStr="Riezler et al. (2003)" startWordPosition="3146" endWordPosition="3149">ters. The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression. The former covers the grammatical correctness, comprehensibility and fluency of the output while the latter measures the amount of important content preserved in the compression. Additionally, for experiments on the development set, we used two metrics for automatic evaluation: per-sentence accuracy (i.e., how many compressions could be fully reproduced) and word-based F1-score. The latter differs from the RASP-based relation F-score by Riezler et al. (2003) in that we simply compute the recall and precision in terms of tokens kept in the golden and the generated compressions. We report these results for completeness although it is the results of the human evaluation from which we draw our conclusions. Compression ratio: The three versions of our system (LSTM*) and the baseline (MIRA) have comparable compression ratios (CR) which are defined as the length of the compression in characters divided over the sentence length. Since the 3http://storage.googleapis.com/ sentencecomp/compressiondata.json Automatic evaluation: A total of 1,000 sentence pai</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Riezler, S., T. H. King, R. Crouch &amp; A. Zaenen (2003). Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar. In Proc. of HLT-NAACL-03, pp. 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Srivastava</author>
<author>G Hinton</author>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
</authors>
<title>Dropout: A simple way to prevent neural networks from overfitting.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="13179" citStr="Srivastava et al., 2014" startWordPosition="2191" endWordPosition="2194">ot The operator O denotes element-wise multiplication, the matrices W1, ... , W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise. Stochastic gradient descent is used to maximize the training objective (Eq. (1)) w.r.t. all the LSTM parameters. Network architecture: In these experiments we have used the architecture depicted in Figure 3. Following Vinyals et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014). The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three softmax LSTM layer dropout LSTM layer dropout LSTM layer Embedding of current word Last label Figure 3: Architecture of the network used for sentence compression. Note that this basic structure is then unrolled 120 times, with the standard dependences from LSTM networks (Hochreiter &amp; Schmidhuber, 1997). labels: 1, if a word is to be retained in the compression, 0 if a word is to be deleted, or EOS, which is the output label used for the “GO” input and the end-of-sentence final pe</context>
</contexts>
<marker>Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov, 2014</marker>
<rawString>Srivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever &amp; R. Salakhutdinov (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>O Vinyals</author>
<author>Q V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of NIPS-2014.</booktitle>
<contexts>
<context position="5235" citStr="Sutskever et al., 2014" startWordPosition="835" endWordPosition="838">08; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and m</context>
<context position="10400" citStr="Sutskever et al. (2014)" startWordPosition="1692" endWordPosition="1695">their syntactic parents. The model is fitted over ten epochs on the whole training data, and for model selection a small development set consisting of 5,000 previously unseen sentences is used (none of them belonging to 1https://code.google.com/p/word2vec/ the evaluation set). The automated metric used for this selection was accuracy@1 which is the proportion of golden compressions which could be fully reproduced. The performance on the development set plateaus when getting close to the last epoch. 4 The LSTM model Our approach is largely based on the sequence to sequence paradigm proposed in Sutskever et al. (2014). We train a model that maximizes the probability of the correct output given the input sentence. Concretely, for each training pair (X, Y ), we will learn a parametric model (with parameters θ), by solving the following optimization problem: 1: θ∗ = arg max log p(Y |X; θ) (1) θ X,Y where the sum is assumed to be over all training examples. To model the probability p, we use the same architecture described by Sutskever et al. (2014). In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter &amp; Schmidhuber, 1997), designed to avoid vanishing gradients and to remembe</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Sutskever, I., O. Vinyals &amp; Q. V. Le (2014). Sequence to sequence learning with neural networks. In Proc. of NIPS-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Vinyals</author>
<author>A Toshev</author>
<author>S Bengio</author>
<author>D Erhan</author>
</authors>
<title>Show and tell: A neural image caption generator.</title>
<date>2015</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR-2015.</booktitle>
<contexts>
<context position="5328" citStr="Vinyals et al., 2015" startWordPosition="850" endWordPosition="853">pression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have not been ma</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2015</marker>
<rawString>Vinyals, O., A. Toshev, S. Bengio &amp; D. Erhan (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Vinyals</author>
<author>L Kaiser</author>
<author>T Koo</author>
<author>S Petrov</author>
<author>I Sutskever</author>
<author>G E Hinton</author>
</authors>
<title>Grammar as a foreign language. CoRR,</title>
<date>2014</date>
<contexts>
<context position="5288" citStr="Vinyals et al., 2014" startWordPosition="844" endWordPosition="847">2013). Thus, the relation to existing compression work is that we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by parap</context>
<context position="12980" citStr="Vinyals et al. (2014)" startWordPosition="2161" endWordPosition="2164">e LSTM computes the h-sequence (h1, ... , hT) and the m-sequence (m1, ... , mT) sigm(W1xt + W2ht−1) tanh(W3xt + W4ht−1) sigm(W5xt + W6ht−1) sigm(W7xt + W8ht−1) mt = mt−1 O ft + it O i0t ht = mt O ot The operator O denotes element-wise multiplication, the matrices W1, ... , W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise. Stochastic gradient descent is used to maximize the training objective (Eq. (1)) w.r.t. all the LSTM parameters. Network architecture: In these experiments we have used the architecture depicted in Figure 3. Following Vinyals et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014). The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three softmax LSTM layer dropout LSTM layer dropout LSTM layer Embedding of current word Last label Figure 3: Architecture of the network used for sentence compression. Note that this basic structure is then unrolled 120 times, with the standard dependences from LSTM networks (Hochreiter &amp; </context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Vinyals, O., L. Kaiser, T. Koo, S. Petrov, I. Sutskever &amp; G. E. Hinton (2014). Grammar as a foreign language. CoRR, abs/1412.7449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Xu</author>
<author>J Ba</author>
<author>R Kiros</author>
<author>K Cho</author>
<author>A Courville</author>
<author>R Salakhutdinov</author>
<author>R Zemel</author>
<author>Y Bengio</author>
</authors>
<title>Show, attend and tell: Neural image caption generation with visual attention.</title>
<date>2015</date>
<booktitle>In Proceedings of ICML-2015.</booktitle>
<contexts>
<context position="5346" citStr="Xu et al., 2015" startWordPosition="854" endWordPosition="857">we also use the deletion approach. Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have not been many examples yet (C</context>
</contexts>
<marker>Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel, Bengio, 2015</marker>
<rawString>Xu, K., J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel &amp; Y. Bengio (2015). Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of ICML-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zaremba</author>
<author>I Sutskever</author>
</authors>
<date>2014</date>
<note>Learning to execute. CoRR, abs/1410.4615.</note>
<contexts>
<context position="5413" citStr="Zaremba &amp; Sutskever, 2014" startWordPosition="864" endWordPosition="867">hine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence. Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well. Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba &amp; Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs). Thus, with regard to this line of research, our work comes closest to the recent machine translation work. An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions. A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have not been many examples yet (Cohn &amp; Lapata, 2008). However, in this paper our goal is to demonstr</context>
</contexts>
<marker>Zaremba, Sutskever, 2014</marker>
<rawString>Zaremba, W. &amp; I. Sutskever (2014). Learning to execute. CoRR, abs/1410.4615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>R McDonald</author>
</authors>
<title>Enforcing structural diversity in cube-pruned dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL-14,</booktitle>
<pages>656--661</pages>
<contexts>
<context position="2209" citStr="Zhang &amp; McDonald, 2014" startWordPosition="333" endWordPosition="336">tion (Jing, 2000; Clarke &amp; Lapata, 2008, among others) or use syntactic features as signals in a statistical model (McDonald, 2006). It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight &amp; Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova &amp; Altun, 2013, among others). Unfortunately, this makes such systems vulnerable to error propagation as there is no way to recover from an incorrect parse tree. With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy (Zhang &amp; McDonald, 2014), the problem is not a negligible one. To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization. In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information? While phenomena like long-distance relations may seem to make generation of grammatically correct compressions impossible, we are going to present an evidence to the contrary. In particular, we will present a model which benefits from the very re</context>
</contexts>
<marker>Zhang, McDonald, 2014</marker>
<rawString>Zhang, H. &amp; R. McDonald (2014). Enforcing structural diversity in cube-pruned dependency parsing. In Proc. of ACL-14, pp. 656–661.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
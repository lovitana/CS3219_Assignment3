<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.996395">
Using Content-level Structures for Summarizing Microblog Repost Trees ∗
</title>
<author confidence="0.999777">
Jing Li1,2, Wei Gao3, Zhongyu Wei4, Baolin Peng1,2 and Kam-Fai Wong1,2
</author>
<affiliation confidence="0.99835725">
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2MoE Key Laboratory of High Confidence Software Technologies, China
3Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar
4The University of Texas at Dallas, Richardson, Texas, USA
</affiliation>
<email confidence="0.965714">
{lijing,blpeng,kfwong}@se.cuhk.edu.hk1,2
wgao@qf.org.qa3, zywei@@hlt.utdallas.edu4
</email>
<sectionHeader confidence="0.993727" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975703703704">
A microblog repost tree provides strong
clues on how an event described therein
develops. To help social media users
capture the main clues of events on mi-
croblogging sites, we propose a novel re-
post tree summarization framework by ef-
fectively differentiating two kinds of mes-
sages on repost trees called leaders and
followers, which are derived from content-
level structure information, i.e., contents
of messages and the reposting relations.
To this end, Conditional Random Fields
(CRF) model is used to detect leaders
across repost tree paths. We then present a
variant of random-walk-based summariza-
tion model to rank and select salient mes-
sages based on the result of leader detec-
tion. To reduce the error propagation cas-
caded from leader detection, we improve
the framework by enhancing the random
walk with adjustment steps for sampling
from leader probabilities given all the re-
posting messages. For evaluation, we
construct two annotated corpora, one for
leader detection, and the other for repost
tree summarization. Experimental results
confirm the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989780529411765">
Microblogging platforms have become the center
for reporting, discussing, and disseminating real-
life issues, on which users usually repost to share
microblog messages with their following users.
Also, users can repost with commentary for not
only further broadcasting but also extending the
∗ This work is partially supported by General Research
Fund of Hong Kong (417112), RGC Direct Grant (417613),
and Huawei Noah’s Ark Lab, Hong Kong. We would like to
thank anonymous reviewers for the useful comments.
original microblog post content. Because an in-
dividual post is generally too short to cover the
main clues of an event, microblogging users can-
not easily capture the key information from re-
ceived posts due to the lack of context. And re-
posting messages, namely reposts, can provide
valuable context information to the previous posts
including their background, development, public
opinions and so on. However, a popular post usu-
ally attracts a large number of reposts. It is imprac-
tical for users to read them all and fully understand
their contents.
The task of microblog context summarization
aims to produce succinct summaries to help users
better understand the main clues by extracting
salient information among massive reposts of the
original posts. An intuitive approach is to directly
apply existing extractive summarizers based on
the unstructured, plain microblog contents. But
such short and informal reposts render the lack
of structures in each individual message, and it is
difficult for conventional extractive summarizersto
identify salient messages. Chang et al. (2013) pro-
posed to summarize Twitter context trees by fo-
cusing on modeling user influence. However, the
reposts of influential users might not be salient
summary candidates necessarily. For instance,
celebrities might simply repost with nothing im-
portant. Also, modeling user influence accurately
needs tremendous historical user interaction data
external to the tree being summarized while such
kind of information cannot be utilized directly for
summarizing the messages on the tree.
In this paper, we propose a novel mi-
croblog context summarization framework based
on content-level structures, i.e., message contents
and reposting relations, rather than user-level in-
fluence signals. The reposting relations connect
the reposting messages and form a cohesive body
as a tree structure named repost tree. The root rep-
resents the original post and the edges denote re-
</bodyText>
<page confidence="0.947564">
2168
</page>
<note confidence="0.984669">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2168–2178,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999924928571429">
posting relations. Our idea is to exploit the struc-
ture of repost tree together with content of mes-
sages to help distinguish two different messages
on repost tree, i.e., leaders and followers. Specif-
ically, leader is referred to as a message on re-
post tree covering salient new information, which
can lead further comments or discussions in its de-
scendant reposts; follower is referred to as a mes-
sage that contains no comment, simply repeats or
naively responds to its ancestor leader message,
thus providing no important information. The ex-
ample below illustrates a repost tree path, where
we use [O] and [RZ] to indicate the original post
and the i-th repost, respectively:
</bodyText>
<listItem confidence="0.89564">
[O] @MAS: Malaysia Airlines has lost contact of
MH17 from Amsterdam. The last known position
was over Ukrainian airspace.
[R1] @Hanna: OMG... Poor on MH17... Preying...
[R2] @Victoria: OMG that’s horrible!!! I’m sorry to
hear that. God will bless u poor guys. Wish world
can be peaceful. And no one will get hurt.
[R3] @Dr.Dr: Six top HIV scientists are on MH17. They
go for AIDS and would NEVER come back!!!
[R4] @TomyBlack: 6 experts died?! Terrible loss to HIV
research :(
</listItem>
<bodyText confidence="0.994289103448276">
[O] reports the news about MH17 missing, which
brings about further comments in [R1] and [R2].
[R3] does not continue commenting on that but of-
fers some new information and triggers shocking
reaction in [R4]. So [O] and [R3] act as leaders;
[R1], [R2] and [R4] are followers.
Intuitively, leaders would be more important
than followers from the summarization’s perspec-
tive since leaders are supposed to capture the main
clues or aspects of event evolvement. The first
step of our summarization system is to distinguish
leaders and followers effectively. Leaders are de-
tected across repost tree paths which provide rich
context information owing to the tree structure.
We utilize sequence tagging model Conditional
Random Fields (CRF) to infer how likely it is each
repost being a leader or follower. Then we incor-
porate leader detection result into an unsupervised
summarization model based on random walk. Our
model uses content similarities between messages
and consider their possibilities of being leaders
to rank and select salient reposting messages that
form summaries. Furthermore, we improve the
framework by enhancing the random walk to re-
duce the impact of errors cascaded from the leader
detection module. Compared to the state-of-the-
art baselines, the experimental results confirm the
effectiveness of our proposed framework.
Our contributions are given as follows:
</bodyText>
<listItem confidence="0.998198117647059">
• We propose a novel microblog context sum-
marization framework, in which given reposting
messages organized as a repost tree (obtaining
repost tree is trivial using public microblogging
toolkit (Ren et al., 2014)), we summarize the re-
post trees based on content information and re-
posting relations of messages.
• We identify a novel problem of leader de-
tection for summarization, which aims to reduce
noise on repost trees, and present a CRF-based
method for effectively detecting leaders by utiliz-
ing the tree structure and message contents.
• We incorporate the leader detection result
into an unsupervised summarization model based
on random walk and substantially enhance the
model to reduce the impact of leader detection er-
rors on summarization.
</listItem>
<sectionHeader confidence="0.999241" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999869483870968">
The goal of text summarization is to automat-
ically produce a succinct summary for one or
more documents that preserves important infor-
mation (Radev et al., 2002). Generally, text sum-
marization techniques can be categorized into ex-
tractive and abstractive methods (Das and Mar-
tins, 2007). Extractive approaches focus on how
to identify and distill salient contents from orig-
inal texts whereas abstractive approaches aim at
producing grammatical summaries by text genera-
tion.
Recently, the development of social media has
made microblog summarization a hot topic. Most
prior works are on event-level or topic-level sum-
marization. Typically, the first step is to clus-
ter posts into sub-events (Chakrabarti and Punera,
2011; Duan et al., 2012; Shen et al., 2013) or sub-
topics (Long et al., 2011; Rosa et al., 2011; Meng
et al., 2012), and then the second step generates
summary for each cluster.
Some works tried to apply conventional
extractive summarization models directly,
e.g., LexRank (Erkan and Radev, 2004),
MEAD (Radev et al., 2004), TF-IDF (Inouye and
Kalita, 2011), Integer Linear Programming (Liu
et al., 2011; Takamura et al., 2011), etc. Sharif
et al. (2010) modeled the problem as optimal
path finding on a phrase reinforcement graph.
However, these general summarizers were found
not suitable for microblog posts, which are infor-
mal and noisy (Chang et al., 2013). Researchers
</bodyText>
<page confidence="0.99247">
2169
</page>
<bodyText confidence="0.999934761904762">
then considered social signals like user following
relations and retweet count (Duan et al., 2012;
Liu et al., 2012), and reported such features useful
to help summarize microblog posts. Our work
studies repost tree summarization by leverag-
ing content-level structure to enrich context of
messages, which is a different kind of signal.
Chang et al. (2013) proposed a task to summa-
rize Twitter context trees consisting of an original
tweet and all its reposts (i.e., replies and retweets).
They combined user influence signals into a super-
vised summarization framework. Our work is dif-
ferent from theirs: 1) They simply treat a context
tree as a tweets stream while we consider repost
tree structures in summarization; 2) They rely on
user interactions to calculate user influence for ex-
tracting salient messages while we focus on how
to utilize contents and repost tree structures to dif-
ferentiate leader and follower messages for sum-
marization; 3) Our summarization module is unsu-
pervised, thus no need of ground-truth summaries.
</bodyText>
<sectionHeader confidence="0.997395" genericHeader="method">
3 Leader Detection Model
</sectionHeader>
<bodyText confidence="0.999314423076923">
This section deals with how to differentiate leader
and follower messages on a repost tree. Intu-
itively, identifying leaders effectively makes one
step closer to obtaining a good summary.
Figure 1 illustrates an example of a repost tree1.
As shown in the figure, a leader message contains
contents that brings essential information increase,
such as a new clue about MH17 reported in [R6],
and potentially triggers a new round of informa-
tion propagation by attracting follower messages
to focus on the raised clue, like [R7], [R8] and
[R9]. As the repost tree grows, it also happens
that some new reposts join in, following the clue
raised by one of their ancestors, but further ex-
tend it by mentioning something new, thus some
of these messages may evolve into new leaders,
such as [R10].
A simple way to detect leaders on repost tree
is to directly apply a binary classifier like SVM
on each individual message. However, these mod-
els assume reposts are independent without effec-
tively leveraging abundant context along the re-
post tree paths, such as the reposting relations
among different reposts on a path. For instance,
[R2] covering rich content may be misclassified as
a leader if not leveraging context information. But
</bodyText>
<footnote confidence="0.999018">
1The example in Section 1 actually denotes the left-most
path extracted from this tree
</footnote>
<figureCaption confidence="0.9133188">
Figure 1: An example of repost tree. [O]: the
original post; [Ri]: the i-th repost; Solid arrow
lines: reposting relationship; Dotted lines: hidden
leader-follower relationship; Dark boxes: leaders
to be detected.
</figureCaption>
<bodyText confidence="0.9989511">
if we look into its context, we can find that [R2]
talks about similar things as [R1], then [R1] clas-
sified as a follower indicates the higher chance of
[R2] being a follower rather than a leader. There-
fore, context information is important for indicat-
ing the messages being leaders or followers.
We extract all root-to-leaf paths within a repost
tree structure and detect leaders across each path.
We formulate leader detection on repost tree paths
as a sequence tagging problem by utilizing a state-
of-the-art sequence learning model CRF (Lafferty
et al., 2001), and taking advantage of its power
in maximizing the likelihood of global label se-
quences. We adopt CRF rather than other compet-
itive context sensitive model like SVMhmm (Al-
tun et al., 2003) due to its probabilistic nature.
The probability of prediction by CRF can pro-
vide critical chances for the following summariza-
tion procedure to reduce the impact of errors made
by leader detection model on summarization (see
Section 4.2).
We map a repost tree path with n microblogs
(m1, m2, · · · , mn) to a training instance (X, Y ).
Let X = (x1, x2, · · · , xn) represents observed
sequence, where xi denotes the observed feature
vector extracted from the i-th microblog mi, and
Y = (y1, y2, · · · , yn) where yi is the label indicat-
ing whether mi is a leader or not. CRF defines the
discriminative function as a joint distribution over
Y given X as follows:
</bodyText>
<equation confidence="0.9756555">
⎛ ⎞
⎝X X
P (Y |X; θ) ∝ exp λjfj(yi, yi−1, X) + µkgk(yi, X) ⎠
i,j i,k
</equation>
<bodyText confidence="0.422702666666667">
where fj and gk are the fixed feature functions,
[O] MAS: Malaysia Airlines has lost contact of MH17 from Amsterdam. The
last known position was over Ukrainian airspace. More details to follow.
[R3] Dr.Dr: Six top HIV scientists are
on MH17. They go for AIDS and
would NEVER come back!!!
</bodyText>
<figure confidence="0.532564">
[R8] MrBig: That can’t
be true. CRASHED...I
really feel pity for u
poor guys...
[R9] WindWolf:
eh...MH17 lost and now
a MH plane is found
crashed. I feel terrible.
[R4] TomyBlack: 6 [R5] JustinBieber: [R10] X-man: #MH17 must have crashed.
experts died?! Terrible now i can’t listen to MH370 has not been found, and now MHJ7’ s
loss to HIV research :( #prey without crying lost, here’s something suspicious.
[R1] Hanna: OMG...Poor on
#MH17...Preying...
[R2]Victoria: OMG that’s horrible!!! I&apos;m
sorry to hear that. God will all bless u
poor guys. Wish world can be peaceful.
And no one will get hurt.
[R6] NajibRazak: I am shocked by reports that
an MH plane crashed. We are launching an
immediate investigation.
[R7]MrsBig: RT
</figure>
<page confidence="0.889111">
2170
</page>
<table confidence="0.983356727272727">
Feature category Feature name Feature description
# of terms The number of terms in mi
Lexical POS The part-of-speech of each term in mi
Type of sentence Whether mi contains a question mark or an exclamation
Microblog-specific # of emoticons The number of emoticons in mi
# of hashtags The number of hashtags in mi
# of urls The number of URLs in mi
# of mentions The number of mentions, or @UserName, in mi
Similarity to neighbors Cosine similarity between mi and mi+d where d E {±1, ±2, ±3}
Path-specific
Similarity to root Cosine similarity to the root microblog in repost tree path
</table>
<tableCaption confidence="0.999702">
Table 1: Features used for leader detection
</tableCaption>
<bodyText confidence="0.994483625">
θ = (λ1, λ2, ...; µ1, µ2, ...) are the parameters in-
dicating the weights of features that can be esti-
mated by maximum likelihood procedure in train-
ing process. The prediction is done based on dy-
namic programming. More details can be found
in (Lafferty et al., 2001). Table 1 lists the features
we use for leader detection.
CRF can utilize both historical and future infor-
mation for prediction so as to maximize the likeli-
hood of the global label sequences. But we would
encounter the problem of label conflict, i.e., the
predictions for the same repost in context of dif-
ferent paths might be different. For this reason,
we determine a repost as a leader if its average
marginal probabilities being a leader in context of
different paths exceeds 50%.
</bodyText>
<sectionHeader confidence="0.997151" genericHeader="method">
4 LeadSum Summarization Model
</sectionHeader>
<bodyText confidence="0.999964777777778">
Let T = (V, E) represent a repost tree to be
summarized, where V is a set of nodes cor-
responding to microblog messages, and E =
{(u, v)|v is the repost of u} is the edge set denot-
ing reposting relations. This section describes
how to rank nodes in V to produce repost tree
summaries. Enlightened by the general random-
walk-based ranking algorithm DivRank (Mei et
al., 2010), we propose an unsupervised summa-
rization model called LeadSum that aims to select
true and salient leaders into summaries utilizing a
variant of random walk based on content similari-
ties and reposting relations of messages. We first
present a basic LeadSum model, which assumes
leader detection is perfect. Then, we enhance it
to become a soft LeadSum model that reduces the
impact of leader detection errors on the summa-
rization.
</bodyText>
<subsectionHeader confidence="0.849005">
4.1 Basic-LeadSum Model
</subsectionHeader>
<bodyText confidence="0.999881333333334">
Due to the nature of leaders, they generally cover
more important contents than follows do. Thus
our first summarizer selects contents only from de-
tected leaders. For the leaders detected in a re-
post tree T, we build a similarity graph among
leaders denoted as GL = (VL, EL), where VL =
{v E V |v is a detected leader} is the vertex set
and EL = {(u,v)|u E VL, v E VL, and u =�v} is
the edge set. The weight for any edge (u, v) rep-
resents the content similarity between u and v, for
which we use cosine similarity.
DivRank (Mei et al., 2010) is a generic graph
ranking model that aims to balance high informa-
tion coverage and low redundancy in top ranking
vertices, which are also two key requirements for
choosing salient summarization sentences (Li et
al., 2009; Liu et al., 2015). Based on that, we
present a model to rank and select salient mes-
sages from leader set VL to form a summary. Since
this model simply assumes perfect leader detec-
tion, it is therefore named Basic-LeadSum.
Similar as DivRank (Mei et al., 2010), the tran-
sition probability at the t-th iteration of random
walk is given as follows:
</bodyText>
<equation confidence="0.9937334">
pt(u → v) = (1−µ)·p0(v)+µ· p0(u Z(� Nt−1(v)
(1)
and Z(u) is the normalizing factor:
Z(u) = � p0(u → w)Nt−1(w) (2)
w∈VL
</equation>
<bodyText confidence="0.9999046">
where p0(u → v) is the organic transition prob-
ability which represents the content similarity be-
tween u and v; Nt−1(v) denotes the times vertex
v is visited up to the (t − 1)-th iteration; p0(v) =
|VL |denotes random jumping probability similar
</bodyText>
<equation confidence="0.493321">
1
</equation>
<bodyText confidence="0.999348142857143">
to that in PageRank; and µ is the damping weight
set as 0.85 following most PageRank-based mod-
els. The probability of traveling to leader v can
accumulate as its weight increases during random
walk, and leaders already having high weight can
“absorb” weights from other leaders with high
similarity to it, thus avoids redundancy.
</bodyText>
<page confidence="0.958913">
2171
</page>
<bodyText confidence="0.999519666666667">
For any v E VL, the update function for its rank-
ing score at the t-th iteration Rt(v) is formulated
as:
</bodyText>
<equation confidence="0.9968845">
�Rt(v) = pt(u → v)Rt−1(u) (3)
uEVL
</equation>
<bodyText confidence="0.999687">
It has been proved that the Markov chain is er-
godic, thus can converge to a stationary distribu-
tion (Mei et al., 2010), which determines the final
rankings for leaders.
</bodyText>
<subsectionHeader confidence="0.649249">
4.2 Soft-LeadSum Model
</subsectionHeader>
<bodyText confidence="0.999746666666667">
As a two-step summarization system, the perfor-
mance of LeadSum relies on the leader detection,
which might be error-prone. Followers misiden-
tified as leaders participating in leader ranking
brings risks to extract real followers into summary.
Also, leaders misclassified as followers may leave
out strong summary candidates. To reduce such
error propagation, we enhance Basic-LeadSum by
using an even-length random walk with adjust-
ment steps that sample from leader probabilities
given all the reposting messages, which is referred
to as Soft-LeadSum.
Different from Basic-LeadSum, every message
on repost tree T, no matter detected as a leader or
a follower, participates in ranking process of Soft-
LeadSum. In other words, in the random walk,
visitor wanders on a complete graph G = (V, E&apos;)
whose vertex set V is identical to repost tree T,
and E&apos;={(u,v)JuE V,vEV, and u =�v} rep-
resents the edge set. Therefore, this makes it pos-
sible to include true leaders misclassified as fol-
lowers by leader detection module into summary.
However, allowing all messages to participate
in ranking also increases the risk of selecting real
followers. To avoid this problem, Soft-LeadSum
is composed of two types of walks on G, namely
WALK-1 and WALK-2. In WALK-1, visitor
moves based on content similarities between mes-
sages, which follows transition probabilities simi-
lar to equation (1), but is specifically given as:
</bodyText>
<equation confidence="0.997635333333333">
1 p0(u → v)Nt−1(v)
pt(u → v) = (1 − µ) · |V  |+ µ · (4)
Z(u)
</equation>
<bodyText confidence="0.999922269230769">
where u, v E V , p0(u —* v) is proportional to con-
tent similarity between u and v similar to Basic-
LeadSum, and Z(u) is the normalizing factor.
WALK-2 attempts to avoid selecting true fol-
lowers by adopting a sampling process, whose re-
sult determines the next vertex on G to be visited.
Suppose the current vertex being visited is u, then
we sample from pL(u), i.e., the probability of u
being a leader. Practically, pL(u) can be estimated
with the average of u’s marginal probabilities as
a leader over all root-to-leaf paths passing u on
T output by the leader detection module. If u is
sampled to be a leader, we claim that leader detec-
tion is correct and the visitor stays; otherwise, u is
sampled as a follower, indicating that leader detec-
tion module misclassifies u, so the visitor should
go to u’s leader. Here we assume that a follower
u’s leader is its nearest ancestor leader on T as
shown by the dotted lines in Figure 1. Based on
such simplification, we let the visitor trace back
one by one along the path on T from u to root and
sample from their leader probabilities until a node
v is sampled as a leader and then we determine v
as u’s leader.
So for any u’s ancestor v, the probability of v
being u’s leader is:
</bodyText>
<equation confidence="0.9978594">
Pr{v is u’s leader}
= pL(v)(1 − pL(u) − Pr{w is u’s leader})
wEP(v,u) (5)
�= pL(v) (1 − pL(w))
wEP(v,u) UJuJ
</equation>
<bodyText confidence="0.999928625">
where P(v, u) is the set of nodes between v and u
on v-to-u path of repost tree, i.e., P(v, u) = {w E
V Jw is v’s descendant and u’s ancestor on T}. In
particular, we assume that pL(r) = 1 so as to stop
the sampling process when the visitor arrives at
root r.
Therefore, WALK-2’s transition probabilities
can be calculated as follows:
</bodyText>
<equation confidence="0.990570666666667">
q(u —* v) = Pr{v is u’s leader} if v is u’s ancestor; (6)
�pL(v) if v = u;
0 otherwise
</equation>
<bodyText confidence="0.99991394117647">
Algorithm 1 shows the ranking process of Soft-
LeadSum, during which the visitor walks on G
following WALK-1 and WALK-2 alternately. The
fact that WALK-1 is ergodic ensures the ergodicity
and convergency of the algorithm. In implemen-
tation, we set max iteration N=1000 empirically
which is large enough to ensure convergence, or
stop random walk process in advance when the
condition of convergence is met, i.e., the change
of Euclidean difference of ranking scores for three
consecutive iterations are all less than 1e-6.
Soft-LeadSum can reduce the impact of errors
made by leader detection on summarization due to
the following two reasons: 1) It allows all mes-
sages to participate in ranking process, thus per-
mits those leaders leaving out by leader detec-
tion module to be selected into summary; 2) With
</bodyText>
<page confidence="0.974874">
2172
</page>
<table confidence="0.62708616">
Algorithm 1 Algorithm of Soft-LeadSum
Input: T, G, µ=0.85, max iteration N, length cut-off n
Output: Summary with n microblog messages
1: For all v E V , initialize R0(v) = p0(v) = 1
|V |
2: Initialize WALK-1’s transition probabilities p0(u --+ v)
with normalized cosine similarity between u and v.
3: Calculate WALK-2’s transition probabilities q(u --+ v)
by equation (5) and (6).
4: Initialize current walk=“WALK-1”
5: for t = 1 to N and not converged do
6: for all v E V do
7: if current walk==“WALK-1” then
8: Update pt(u --+ v) by equation (4)
9: Update Rt(v) as follows:
Rt(v) = E..EV Rt−1(u) - pt(u --+ v)
10: Set current walk=“WALK-2”
11: end if
12: if current walk==“WALK-2” then
13: Update Rv(v) as follows:
Rt(v) = E..EV Rt−1(u) - q(u --+ v)
14: Set current walk=“WALK-1”
15: end if
16: end for
17: end for
</table>
<listItem confidence="0.5841235">
18: Sort all v E V by RN(v) in descending order
19: Pick the top-n messages as summary
</listItem>
<bodyText confidence="0.959575666666666">
WALK-2 sampling from leader probabilities, it
also reduces the risk of including real followers
into summary.
</bodyText>
<sectionHeader confidence="0.995371" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999982">
To evaluate the two modules in our repost tree
summarization system, i.e., CRF-based model for
leader detection and LeadSum model for summa-
rization, we conducted two sets of experiments
based on microblog posts data collected from Sina
Weibo, which has a similar market penetration
as Twitter (Rapoza, 2011)2. Microblog messages
on Sina Weibo are in Chinese and we use Fu-
danNLP (Qiu et al., 2013) for text preprocessing
including word segmentation and POS tagging.
</bodyText>
<subsectionHeader confidence="0.912812">
5.1 Experiment for Leader Detection
</subsectionHeader>
<bodyText confidence="0.99991">
In this experiment, we evaluated the performance
of CRF model for leader detection task.
</bodyText>
<subsubsectionHeader confidence="0.734975">
5.1.1 Data Collection and Setup
</subsubsectionHeader>
<bodyText confidence="0.999878166666667">
We first crawled 1,300 different repost trees us-
ing the public PKUVIS toolkit (Ren et al., 2014).
Given an original microblog post, the toolkit can
automatically crawl its complete repost tree. For
each tree, we randomly selected one path and fur-
ther formed a set with 1,300 repost tree paths,
</bodyText>
<footnote confidence="0.932554666666667">
2The datasets are available at http://www1.se.
cuhk.edu.hk/˜lijing/data/repost_tree_
summ.zip
</footnote>
<table confidence="0.999879857142857">
Cross-validation Held-out
Prec Rec F1 Prec Rec F1
Random 29.8 49.5 37.3 31.6 49.6 38.6
LR 70.5 66.3 68.4 70.4 66.2 68.2
SVM 70.9 66.9 68.8 68.9 66.2 67.5
SVMhmm 74.8 65.5 69.8 69.3 70.1 69.7
CRF 75.5 72.0 73.7 71.1 70.7 70.9
</table>
<tableCaption confidence="0.999792">
Table 2: The performance of leader detection (%)
</tableCaption>
<bodyText confidence="0.999234482758621">
which ensures that paths have different roots and
the dataset can cover a wide variety of context in-
formation.
Then three annotators were invited to label each
repost as a leader or a follower in the context of
its repost tree path independently. The average
Cohen’s Kappa of each two of the three annota-
tors was 0.52, which is considered good agree-
ment (Fleiss et al., 2013). Then, we used the labels
agreed by at least two annotators as the ground
truth. The training and test of the leader detection
models were conducted on this corpus.
We compared the performance of CRF-based
leader detection model with four baselines: Ran-
dom Classifier (RC) as a weak baseline; two state-
of-the-art point-wise supervised models Logis-
tic Regression (LR) and Support Vector Machine
(SVM); and an effective context sensitive model
SVMhmm. We applied LibLinear toolkit (Fan et
al., 2008) to implement LR and SVM with linear
kernel. SVMhmm was implemented by SVMstruct
toolkit (Joachims et al., 2009). And CRF’s im-
plementation was based on CRF++3. For all the
baselines, we used features listed in Table 1. The
hyper-parameters of all leader detection models
were tuned to the same extent based on 5-fold
cross validation (with 1 fold as development set).
The evaluation metrics were precision, recall and
F1 score for the detected leaders.
</bodyText>
<sectionHeader confidence="0.872294" genericHeader="evaluation">
5.1.2 Results
</sectionHeader>
<bodyText confidence="0.9993556">
Table 2 shows the comparison result of 5-fold
cross validation on 1,000 repost tree paths and
held-out experiment on 300 complete fresh paths.
Among all baselines, SVMhmm performed the
best, which indicates the effectiveness of incor-
porating structure information for leader detec-
tion. And among context-sensitive models, both
SVMhmm and CRF were competitive. CRF out-
performed SVMhmm slightly with 5.6% and 1.7%
improved F1 score in cross validation and held-out
</bodyText>
<footnote confidence="0.952109">
3http://taku910.github.io/crfpp/
</footnote>
<page confidence="0.993169">
2173
</page>
<bodyText confidence="0.999818">
experiments, respectively. In spite of their compa-
rable performance, our framework applies CRF in-
stead of SVMhmm for leader detection because of
its probabilistic nature, which can be exploited by
the sampling process in Soft-LeadSum to reduce
the propagation of classification error to the sum-
marization stage. Section 5.2.2 shows the relevant
experiment.
</bodyText>
<subsectionHeader confidence="0.967122">
5.2 Experiment for Summarization
</subsectionHeader>
<bodyText confidence="0.99979575">
In this experiment, we evaluated end-to-end per-
formance of our basic and soft LeadSum summa-
rization models by comparing them with state-of-
the-art microblog summarizers.
</bodyText>
<subsubsectionHeader confidence="0.763703">
5.2.1 Data Collection and Evaluation Metrics
</subsubsectionHeader>
<bodyText confidence="0.999990636363636">
There is no public editorial repost tree dataset.
Therefore, we manually selected 10 hot events tak-
ing place during January 2nd – July 28th 2014, and
then used the PKUVIS toolkit (Ren et al., 2014) to
crawl the complete repost trees for all the events
given the corresponding original posts. Table 3
shows the details about the repost tree corpus4.
Note that this repost tree corpus has no overlap
with the repost tree path dataset for learning leader
detection models in Section 5.1.1.
After that, we invited three experienced editors
to write summaries for each repost tree. To en-
sure the quality of reference summaries, we first
extracted a list of frequent nouns from each repost
tree and generalized 7 to 10 topics based on the
nouns list, which provided a high-level overview
of a repost tree to editors. Then, our guideline
required editors to read all repost microblogs or-
dered sequentially on a repost tree. For every mes-
sage, its entire repost tree path was also provided
as supplementary context information. When fin-
ished reading, editors wrote down one or two sen-
tences to summarize each topic in the list.
We utilized ROUGE-N metric (Lin, 2004) for
benchmark, which is a standard for evaluating
automatic summaries based on N-gram overlap-
ping between a generated summary and a ref-
erence. Specifically, ROUGE-1 and ROUGE-2
F1-measure were used as our evaluation metrics.
Lin et al. (2004) has demonstrated that ROUGE-2
correlates well with humans in summarizing for-
mal texts. And ROUGE-1 is a better alternative
in evaluating summaries for short and informal
</bodyText>
<footnote confidence="0.7246445">
4All descriptions are English translations of the root mi-
croblogs originally in Chinese.
</footnote>
<bodyText confidence="0.9917249375">
microblog messages (Inouye and Kalita, 2011;
Chang et al., 2013).
In our human-generated summaries, the average
inter-annotator-agreement by ROUGE-1 is 0.431,
which means each pair of manual summaries have
no more than 50% words overlap on average even
written under topic constraints. This indicates that
microblog repost tree summarization is generally
a difficult task. The reason is that repost trees
have complex structure, and editors could hardly
reconstruct the repost trees even though they went
through all the microblogs. Therefore, in eval-
uation for each tree, we computed the average
ROUGE F1 score between the model-generated
summary and the three human-generated sum-
maries.
</bodyText>
<sectionHeader confidence="0.766912" genericHeader="evaluation">
5.2.2 Results
</sectionHeader>
<bodyText confidence="0.99977675">
In each automatic summarizer, we selected the
top-10 ranked reposts to form a summary. We
compared the end-to-end performance with the
following baseline systems:
</bodyText>
<listItem confidence="0.995021433333333">
• RandSum: RandSum is a weak baseline that
randomly selects reposts into summaries.
• RepSum: RepSum ranks and selects mes-
sages simply by their reposts count, i.e., the size
of their subtrees, based on reposting relations.
• UserRankSum: UserRankSum ranks and
selects reposts by their authors’ follower count
based on user following relations.
• LeadProSum: LeadProSum ranks and se-
lects reposting messages by their marginal prob-
abilities as leaders determined by our CRF-based
leader detection model.
• SVDSum: SVDSum adopts the Singular
Value Decomposition (SVD) to discover hidden
sub-topics for summarization (Gong and Liu,
2001). Reposting messages are ranked accord-
ing to latent semantic analysis with SVD on term-
message matrix.
• DivRankSum: DivRankSum directly ap-
plies DivRank (Mei et al., 2010) algorithm to rank
all messages unaware of leaders and followers. A
similar model is also reported in Yan et al. (2011).
Following their work, we set damping weight as
0.85.
• UserInfSum: Chang et al. (2013) ranks mes-
sages utilizing Gradient Boosted Decision Tree
(GBDT) algorithm with text, popularity, tempo-
ral and user influence signals to summarize Twit-
ter context tree. In particular, without the interac-
tion data with external users, we utilize users’ fol-
</listItem>
<page confidence="0.956416">
2174
</page>
<table confidence="0.998536090909091">
Name # of nodes # of nodes with comments Height Description
Tree (I) 21,353 15,409 16 HKU dropping out student wins the college entrance exam again.
Tree (II) 9,616 6,073 11 German boy complains hard schoolwork in Chinese High School.
Tree (III) 13,087 9,583 8 Movie Tiny Times 1.0 wins high grossing in criticism.
Tree (IV) 12,865 7,083 8 “I am A Singer” states that singer G.E.M asking for resinging conforms to rules.
Tree (V) 10,666 7,129 8 Crystal Huang clarified the rumor of her derailment.
Tree (VI) 21,127 15,057 11 Germany routs Brazil 7:1 in World-Cup semi-final.
Tree (VII) 18,974 12,399 13 The pretty girl pregnant with a second baby graduated with her master degree.
Tree (VIII) 2,021 925 18 Girls appealed for equality between men and women in college admission
Tree (IX) 9,230 5,408 14 Violent terrorist attack in Kunming railway station.
Tree (X) 10,052 4,257 25 MH17 crash killed many top HIV researchers.
</table>
<tableCaption confidence="0.993032">
Table 3: Description of repost tree summarization corpus consisting of 10 hot events
</tableCaption>
<table confidence="0.993851052631579">
F1 ROUGE-1 F1 ROUGE-2
v SIG v SIG
RandSum .159 .046 **$ .037 .009 **$
RepSum .162 .071 **$ .030 .016 **$
UserRankSum .292 .066 $ .087 .028 †
LeadProSum .270 .119 $ .064 .038 $
SVDSum .222 .070 **$ .048 .032 **$
DivRankSum .159 .079 **$ .029 .018 **$
UserInfSum .272 .091 $ .071 .028 $
B-LS+SVMh-- .301 .031 $ .085 .020 †
B-LS+CRF .300 .029 $ .082 .016 $
S-LS+CRF .351 .027 NA .105 .018 NA
Remarks:
B-LS: Basic-LeadSum model; S-LS: Soft-LeadSum model
F1: F1-measure of ROUGE-1 or ROUGE-2
v: Standard deviation of F1-measure over 10 repost trees
SIG: Significance indicator of F1-measure based on one-tailed pairwise t-test:
– Significantly different with B-LS+CRF: * (p &lt; 0.1); ** (p &lt; 0.05)
– Significantly different with S-LS+CRF: † (p &lt; 0.1); $ (p &lt; 0.05)
</table>
<tableCaption confidence="0.999907">
Table 4: Comparison of different summarizers
</tableCaption>
<bodyText confidence="0.999900833333333">
lower count to approximate user influence. GBDT
implementation is based on RankLib5, and as a
supervised method, UserInfSum is evaluated with
10-fold cross validation.
In addition, we observed that SVMhmm is a
competitive baseline for leader detection (see Ta-
ble 2). So we also study its impact on the Basic-
LeadSum model. Note that SVMhmm cannot be
combined with Soft-LeadSum since it is not prob-
abilistic.
Table 4 shows the result of overall comparisons.
We have the following observations:
</bodyText>
<listItem confidence="0.989852">
• RepSum utilized trivial structure informa-
tion, i.e., the size of sub-tree, and its performance
was poor, which was even worse than RandSum
on ROUGE-2. This implies that messages with a
lot of reposts may not be good candidates as other
reasons may lead to their popularity, e.g., a good
posting time or sense of humor.
• UserRankSum performed the best on
ROUGE-1&amp;2 among all baseline summarizers,
which confirms that user following relations
can indeed be a strong signal in microblog
summarization. UserRankSum is even slightly
</listItem>
<footnote confidence="0.9793795">
5http://sourceforge.net/p/lemur/wiki/
RankLib/
</footnote>
<bodyText confidence="0.999277333333333">
better than Basic-LeadSum on ROUGE-2. But, it
does not perform consistently well for all repost
trees, evidenced as the large standard deviation
on ROUGE-1&amp;2. This suggests that the user
following relations cannot always effectively
indicate salient candidates. It may not work for
repost trees where authors have similar number
of following users, or reposts of influential users
contain nothing salient.
</bodyText>
<listItem confidence="0.91315384375">
• LeadProSum achieved the second best per-
formance among all unsupervised baselines,
which indicates that the marginal probabilities as
leaders can signal good summary candidates. This
also confirms that leaders contain salient contents
and should be distinguished from followers in
summarization.
• Utilizing either SVMhmm or CRF as leader
detection model to filter out followers, Basic-
LeadSum almost doubled the ROUGE-1 and
tripled the ROUGE-2 scores compared to Di-
vRankSum’s performance. This indicates that dif-
ferentiating leaders and followers is very helpful
to summarization.
• Basic-LeadSum performed better than all
baselines on ROUGE-1&amp;2 except for a marginal
drop compared to UserRankSum on ROUGE-2.
But the differences with UserRankSum, LeadPro-
Sum and UserInfSum are not statistically signif-
icant. This may be ascribed to the error propa-
gated from leader detection module to summariza-
tion process.
• Soft-LeadSum outperformed all the baselines
with a large margin on ROUGE-1&amp;2, including
supervised summarizer UserInfSum. The one-
tailed pairwise t-test indicates that all the improve-
ments over baselines are significant at the 95%
confidence level except for UserRankSum with
90% confidence level on ROUGE-2. This con-
firms the effectiveness of our framework for pro-
ducing high-quality repost tree summaries.
• The supervised model UserInfSum did not
</listItem>
<page confidence="0.994924">
2175
</page>
<figureCaption confidence="0.9953975">
Figure 2: The impact of α on the ROUGE-1 F1-
measure of combined models
</figureCaption>
<bodyText confidence="0.955708947368421">
perform quite well. The reason is that the model
needs large amount of user interaction data exter-
nal to the tree which are not readily available, and
also it might be overfitting to the limited number
of training instances.
• Basic-LeadSum with CRF and SVMhmm
had very close ROUGE-1&amp;2 scores. Basic-
LeadSum+SVMhmm is even slightly better than
Basic-LeadSum+CRF. Though SVMhmm was
marginally worse in leader detection experiment
(Table 2), we can conclude that SVMhmm is
a comparable alternative as the leader detection
module for Basic-LeadSum.
• Among our models, Soft-LeadSum signif-
icantly outperformed both Basic-LeadSum with
CRF and that with SVMhmm. This implies that
sampling steps in the enhanced random walk of
Soft-LeadSum is effective in reducing the impact
of leader detection error on summarization.
</bodyText>
<subsectionHeader confidence="0.902259">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99999409375">
From Table 4, we observed that user following re-
lations used by UserRankSum is a strong signal
for microblog summarization. A natural question
is: “Can the user following relations commonly
used for modeling user influence be complemen-
tary to the content-level structure information used
in our summariztaion models?”
We thus linearly combine the normalized rank-
ing scores of LeadSum and UserRankSum using
the formula α*u+(1−α)*l, where u and l denote
the UserRankSum and LeadSum ranking scores,
respectively. Figure 2 demonstrates the impact of
α on our basic and soft LeadSum model with CRF.
Clearly, Basic-LeadSum can benefit from user
influence information by incorporating User-
RankSum scores into it. From the incremental
trend of summarization performance with the in-
crease of α for α E [0, 0.9], we can conclude
that user influence is helpful to it. This is because
Basic-LeadSum is not sufficiently robust to the er-
rors cascaded from leader detection module, thus
user-level structures can have the chance to com-
pensate these errors for content-level structures.
Incorporating the same information into Soft-
LeadSum cannot improve its performance regard-
less of the value of α. This implies that content-
level structures, i.e., message content and repost-
ing relations together, are better indicative of good
summary candidates. When these features are ap-
propriately modeled by Soft-LeadSum, user in-
fluence, a traditionally well-known strong signal,
cannot provide extra benefit at all.
</bodyText>
<sectionHeader confidence="0.997489" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999704666666667">
This work presents a study for microblog repost
tree summarization, whose output can provide im-
portant clues for event analysis on microblog-
ging platforms. Conventional works considering
only plain text streams is insufficient to summa-
rize noisy repost trees. We propose a novel sum-
marization system by effectively differentiating
leader and follower messages on repost tree based
on content-level structure information. Firstly, a
leader detection model categorizes each repost on
repost tree path as a leader or a follower. Then, a
random-walk variant summarization model called
LeadSum is proposed to rank and select salient
microblog messages on the basis of leader de-
tection result. To reduce errors cascaded from
leader detection, we enhance LeadSum based on
an even-length random walk by sampling from
leader probabilities for improving summarization.
Based on real-world microblog post dataset, the
experimental results confirm that our proposed
framework is effective for repost tree summariza-
tion by the end-to-end comparison with the state-
of-the-art baselines.
Constrained by the amount of annotation, we
adopt this two-step framework and an unsuper-
vised summarization algorithm. With the develop-
ment of our corpora, we plan to explore the useful-
ness of supervised structure learning approaches,
such as tree-structured CRF (Tang et al., 2006;
Mensink et al., 2013), to integrate leader detec-
tion and summarization into a unified framework,
and make global inference for important leaders
by capturing various non-linear dependencies.
</bodyText>
<figure confidence="0.996159615384615">
Basc−IeadSum
SoR−IeadSum
0.36
0.35
0.34
ROUGE−1 F1−measure
0.3
0.29
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
0.33
0.32
0.31
</figure>
<page confidence="0.994193">
2176
</page>
<sectionHeader confidence="0.98873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999838845454545">
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector
machines. In Proceedings of the Twentieth Inter-
national Conference on Machine Learning, ICML,
pages 3–10.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International Conference on Weblogs and So-
cial Media, pages 66–73.
Yi Chang, Xuanhui Wang, Qiaozhu Mei, and Yan
Liu. 2013. Towards twitter context summarization
with user influence models. In Sixth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM, pages 527–536.
Dipanjan Das and Andr´e FT Martins. 2007. A survey
on automatic text summarization. Literature Survey
for the Language and Statistics II course at CMU,
4:192–195.
Yajuan Duan, Zhimin Chen, Furu Wei, Ming Zhou, and
Heung-Yeung Shum. 2012. Twitter topic summa-
rization by ranking tweets using social influence and
content quality. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
COLING, pages 763–780.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Intell. Res. (JAIR), 22:457–
479.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik.
2013. Statistical methods for rates and proportions.
John Wiley &amp; Sons.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Interna-
tional ACM Conference on Research and Develop-
ment in Information Retrieval, SIGIR, pages 19–25.
David Inouye and Jugal K. Kalita. 2011. Comparing
twitter summarization algorithms for multiple post
summaries. In Privacy, Security, Risk and Trust
(PASSAT) and 2011 IEEE Third Inernational Con-
ference on Social Computing (SocialCom), pages
298–306.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural svms. Machine Learning, 77(1):27–59.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML,
pages 282–289.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through structure
learning. In Proceedings of the 18th International
Conference on World Wide Web, WWW, pages 71–
80.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proceedings of
the ACL-04 Workshop, pages 74–81.
Fei Liu, Yang Liu, and Fuliang Weng. 2011. Why is
sxsw trending?: exploring multiple text sources for
twitter topic summarization. In Proceedings of the
Workshop on Languages in Social Media, pages 66–
75. Association for Computational Linguistics.
Xiaohua Liu, Yitong Li, Furu Wei, and Ming Zhou.
2012. Graph-based multi-tweet summarization us-
ing social signals. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1699–1714.
He Liu, Hongliang Yu, and Zhi-Hong Deng. 2015.
Multi-document summarization based on two-level
sparse representation model. In Proceedings of the
Twenty-Ninth Conference on Artificial Intelligence,
AAAI, pages 196–202.
Rui Long, Haofen Wang, Yuqiang Chen, Ou Jin, and
Yong Yu. 2011. Towards effective event detection,
tracking and summarization on microblog data. In
Web-Age Information Management - 12th Interna-
tional Conference, WAIM, pages 652–663.
Qiaozhu Mei, Jian Guo, and Dragomir R. Radev.
2010. Divrank: the interplay of prestige and di-
versity in information networks. In Proceedings of
the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining,KDD, pages
1009–1018.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Su-
jian Li, and Houfeng Wang. 2012. Entity-centric
topic-oriented opinion summarization in twitter. In
Proceedings of the 18th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, KDD, pages 379–387.
Thomas Mensink, Jakob J. Verbeek, and Gabriela
Csurka. 2013. Tree-structured CRF models for in-
teractive image labeling. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(2):476–
489.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
Fudannlp: A toolkit for chinese natural language
processing. In Proceedings of Annual Meeting of
the Association for Computational Linguistics, ACL,
pages 49–54.
Dragomir R. Radev, Eduard H. Hovy, and Kathleen
McKeown. 2002. Introduction to the special is-
sue on summarization. Computational Linguistics,
28(4):399–408.
</reference>
<page confidence="0.833175">
2177
</page>
<reference confidence="0.999852630434783">
Dragomir R. Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C¸elebi, Stanko
Dimitrov, Elliott Dr´abek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - A plat-
form for multidocument multilingual text summa-
rization. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
LREC.
Kenneth Rapoza. 2011. China’s weibos vs us’s twitter:
And the winner is.
Donghao Ren, Xin Zhang, Zhenhuang Wang, Jing Li,
and Xiaoru Yuan. 2014. Weiboevents: A crowd
sourcing weibo visual analytic system. In IEEE
Pacific Visualization Symposium, PacificVis, pages
330–334.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Ger-
shman, and Robert Frederking. 2011. Topical clus-
tering of tweets. Proceedings of the ACM SIGIR:
SWSM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Automatic summarization of twitter topics.
In National Workshop on Design and Analysis ofAl-
gorithm.
Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013.
A participant-based approach for event summariza-
tion using twitter streams. In Proceedings of Human
Language Technologies: Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL, pages 1152–1162.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. In
Advances in Information Retrieval - 33rd European
Conference on IR Research, ECIR, pages 177–188.
Jie Tang, MingCai Hong, Juan-Zi Li, and Bangyong
Liang. 2006. Tree-structured conditional random
fields for semantic annotation. In The Proceed-
ings of 5th International Semantic Web Conference,
ISWC, pages 640–653.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP, pages 433–443.
</reference>
<page confidence="0.994519">
2178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811618">
<title confidence="0.999365">Content-level Structures for Summarizing Microblog Repost Trees</title>
<author confidence="0.995229">Wei Zhongyu Baolin</author>
<affiliation confidence="0.98857225">Chinese University of Hong Kong, Shatin, N.T., Hong Key Laboratory of High Confidence Software Technologies, Computing Research Institute, Hamad Bin Khalifa University, Doha, University of Texas at Dallas, Richardson, Texas,</affiliation>
<abstract confidence="0.994245785714286">A microblog repost tree provides strong clues on how an event described therein develops. To help social media users capture the main clues of events on microblogging sites, we propose a novel repost tree summarization framework by effectively differentiating two kinds of messages on repost trees called leaders and followers, which are derived from contentlevel structure information, i.e., contents of messages and the reposting relations. To this end, Conditional Random Fields (CRF) model is used to detect leaders across repost tree paths. We then present a variant of random-walk-based summarization model to rank and select salient messages based on the result of leader detection. To reduce the error propagation cascaded from leader detection, we improve the framework by enhancing the random walk with adjustment steps for sampling from leader probabilities given all the reposting messages. For evaluation, we construct two annotated corpora, one for leader detection, and the other for repost tree summarization. Experimental results confirm the effectiveness of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
</authors>
<title>Hidden markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning, ICML,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="12291" citStr="Altun et al., 2003" startWordPosition="1927" endWordPosition="1931">e higher chance of [R2] being a follower rather than a leader. Therefore, context information is important for indicating the messages being leaders or followers. We extract all root-to-leaf paths within a repost tree structure and detect leaders across each path. We formulate leader detection on repost tree paths as a sequence tagging problem by utilizing a stateof-the-art sequence learning model CRF (Lafferty et al., 2001), and taking advantage of its power in maximizing the likelihood of global label sequences. We adopt CRF rather than other competitive context sensitive model like SVMhmm (Altun et al., 2003) due to its probabilistic nature. The probability of prediction by CRF can provide critical chances for the following summarization procedure to reduce the impact of errors made by leader detection model on summarization (see Section 4.2). We map a repost tree path with n microblogs (m1, m2, · · · , mn) to a training instance (X, Y ). Let X = (x1, x2, · · · , xn) represents observed sequence, where xi denotes the observed feature vector extracted from the i-th microblog mi, and Y = (y1, y2, · · · , yn) where yi is the label indicating whether mi is a leader or not. CRF defines the discriminati</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann. 2003. Hidden markov support vector machines. In Proceedings of the Twentieth International Conference on Machine Learning, ICML, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepayan Chakrabarti</author>
<author>Kunal Punera</author>
</authors>
<title>Event summarization using tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Conference on Weblogs and Social Media,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="8278" citStr="Chakrabarti and Punera, 2011" startWordPosition="1273" endWordPosition="1276">re documents that preserves important information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog </context>
</contexts>
<marker>Chakrabarti, Punera, 2011</marker>
<rawString>Deepayan Chakrabarti and Kunal Punera. 2011. Event summarization using tweets. In Proceedings of the Fifth International Conference on Weblogs and Social Media, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Chang</author>
<author>Xuanhui Wang</author>
<author>Qiaozhu Mei</author>
<author>Yan Liu</author>
</authors>
<title>Towards twitter context summarization with user influence models.</title>
<date>2013</date>
<booktitle>In Sixth ACM International Conference on Web Search and Data Mining, WSDM,</booktitle>
<pages>527--536</pages>
<contexts>
<context position="3199" citStr="Chang et al. (2013)" startWordPosition="477" endWordPosition="480">. It is impractical for users to read them all and fully understand their contents. The task of microblog context summarization aims to produce succinct summaries to help users better understand the main clues by extracting salient information among massive reposts of the original posts. An intuitive approach is to directly apply existing extractive summarizers based on the unstructured, plain microblog contents. But such short and informal reposts render the lack of structures in each individual message, and it is difficult for conventional extractive summarizersto identify salient messages. Chang et al. (2013) proposed to summarize Twitter context trees by focusing on modeling user influence. However, the reposts of influential users might not be salient summary candidates necessarily. For instance, celebrities might simply repost with nothing important. Also, modeling user influence accurately needs tremendous historical user interaction data external to the tree being summarized while such kind of information cannot be utilized directly for summarizing the messages on the tree. In this paper, we propose a novel microblog context summarization framework based on content-level structures, i.e., mes</context>
<context position="8934" citStr="Chang et al., 2013" startWordPosition="1381" endWordPosition="1384">3) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kind of signal. Chang et al. (2013) proposed a task to summarize Twitter context trees consisting of an original tweet and all its reposts (i.e., replies and retweets). They combined user influence signals into a supervised summarization framework. Our work is </context>
<context position="28955" citStr="Chang et al., 2013" startWordPosition="4796" endWordPosition="4799"> in the list. We utilized ROUGE-N metric (Lin, 2004) for benchmark, which is a standard for evaluating automatic summaries based on N-gram overlapping between a generated summary and a reference. Specifically, ROUGE-1 and ROUGE-2 F1-measure were used as our evaluation metrics. Lin et al. (2004) has demonstrated that ROUGE-2 correlates well with humans in summarizing formal texts. And ROUGE-1 is a better alternative in evaluating summaries for short and informal 4All descriptions are English translations of the root microblogs originally in Chinese. microblog messages (Inouye and Kalita, 2011; Chang et al., 2013). In our human-generated summaries, the average inter-annotator-agreement by ROUGE-1 is 0.431, which means each pair of manual summaries have no more than 50% words overlap on average even written under topic constraints. This indicates that microblog repost tree summarization is generally a difficult task. The reason is that repost trees have complex structure, and editors could hardly reconstruct the repost trees even though they went through all the microblogs. Therefore, in evaluation for each tree, we computed the average ROUGE F1 score between the model-generated summary and the three hu</context>
<context position="30766" citStr="Chang et al. (2013)" startWordPosition="5073" endWordPosition="5076">osting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with external users, we utilize users’ fol2174 Name # of nodes # of nodes with comments Height Description Tree (I) 21,353 15,409 16 HKU dropping out student wins the college entrance exam again. Tree (II) 9,616 6,073 11 German boy complains hard schoolwork in Chinese High School. Tree (III) 13,087 9,583 8 Movie Tiny Times 1.0 wins high grossing in criticism. Tree (IV) 12,865 7,083 8 “</context>
</contexts>
<marker>Chang, Wang, Mei, Liu, 2013</marker>
<rawString>Yi Chang, Xuanhui Wang, Qiaozhu Mei, and Yan Liu. 2013. Towards twitter context summarization with user influence models. In Sixth ACM International Conference on Web Search and Data Mining, WSDM, pages 527–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Andr´e FT Martins</author>
</authors>
<title>A survey on automatic text summarization. Literature Survey for the Language and Statistics II course at CMU,</title>
<date>2007</date>
<pages>4--192</pages>
<contexts>
<context position="7845" citStr="Das and Martins, 2007" startWordPosition="1207" endWordPosition="1211">sent a CRF-based method for effectively detecting leaders by utilizing the tree structure and message contents. • We incorporate the leader detection result into an unsupervised summarization model based on random walk and substantially enhance the model to reduce the impact of leader detection errors on summarization. 2 Related Work The goal of text summarization is to automatically produce a succinct summary for one or more documents that preserves important information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each clu</context>
</contexts>
<marker>Das, Martins, 2007</marker>
<rawString>Dipanjan Das and Andr´e FT Martins. 2007. A survey on automatic text summarization. Literature Survey for the Language and Statistics II course at CMU, 4:192–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Duan</author>
<author>Zhimin Chen</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Heung-Yeung Shum</author>
</authors>
<title>Twitter topic summarization by ranking tweets using social influence and content quality.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics, COLING,</booktitle>
<pages>763--780</pages>
<contexts>
<context position="8297" citStr="Duan et al., 2012" startWordPosition="1277" endWordPosition="1280">portant information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are in</context>
</contexts>
<marker>Duan, Chen, Wei, Zhou, Shum, 2012</marker>
<rawString>Yajuan Duan, Zhimin Chen, Furu Wei, Ming Zhou, and Heung-Yeung Shum. 2012. Twitter topic summarization by ranking tweets using social influence and content quality. In Proceedings of the 24th International Conference on Computational Linguistics, COLING, pages 763–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>22--457</pages>
<contexts>
<context position="8569" citStr="Erkan and Radev, 2004" startWordPosition="1322" endWordPosition="1325">eas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res. (JAIR), 22:457– 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="25658" citStr="Fan et al., 2008" startWordPosition="4276" endWordPosition="4279">ge Cohen’s Kappa of each two of the three annotators was 0.52, which is considered good agreement (Fleiss et al., 2013). Then, we used the labels agreed by at least two annotators as the ground truth. The training and test of the leader detection models were conducted on this corpus. We compared the performance of CRF-based leader detection model with four baselines: Random Classifier (RC) as a weak baseline; two stateof-the-art point-wise supervised models Logistic Regression (LR) and Support Vector Machine (SVM); and an effective context sensitive model SVMhmm. We applied LibLinear toolkit (Fan et al., 2008) to implement LR and SVM with linear kernel. SVMhmm was implemented by SVMstruct toolkit (Joachims et al., 2009). And CRF’s implementation was based on CRF++3. For all the baselines, we used features listed in Table 1. The hyper-parameters of all leader detection models were tuned to the same extent based on 5-fold cross validation (with 1 fold as development set). The evaluation metrics were precision, recall and F1 score for the detected leaders. 5.1.2 Results Table 2 shows the comparison result of 5-fold cross validation on 1,000 repost tree paths and held-out experiment on 300 complete fre</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
<author>Bruce Levin</author>
<author>Myunghee Cho Paik</author>
</authors>
<title>Statistical methods for rates and proportions.</title>
<date>2013</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="25160" citStr="Fleiss et al., 2013" startWordPosition="4197" endWordPosition="4200">c Rec F1 Prec Rec F1 Random 29.8 49.5 37.3 31.6 49.6 38.6 LR 70.5 66.3 68.4 70.4 66.2 68.2 SVM 70.9 66.9 68.8 68.9 66.2 67.5 SVMhmm 74.8 65.5 69.8 69.3 70.1 69.7 CRF 75.5 72.0 73.7 71.1 70.7 70.9 Table 2: The performance of leader detection (%) which ensures that paths have different roots and the dataset can cover a wide variety of context information. Then three annotators were invited to label each repost as a leader or a follower in the context of its repost tree path independently. The average Cohen’s Kappa of each two of the three annotators was 0.52, which is considered good agreement (Fleiss et al., 2013). Then, we used the labels agreed by at least two annotators as the ground truth. The training and test of the leader detection models were conducted on this corpus. We compared the performance of CRF-based leader detection model with four baselines: Random Classifier (RC) as a weak baseline; two stateof-the-art point-wise supervised models Logistic Regression (LR) and Support Vector Machine (SVM); and an effective context sensitive model SVMhmm. We applied LibLinear toolkit (Fan et al., 2008) to implement LR and SVM with linear kernel. SVMhmm was implemented by SVMstruct toolkit (Joachims et </context>
</contexts>
<marker>Fleiss, Levin, Paik, 2013</marker>
<rawString>Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. 2013. Statistical methods for rates and proportions. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Gong</author>
<author>Xin Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM Conference on Research and Development in Information Retrieval, SIGIR,</booktitle>
<pages>pages</pages>
<contexts>
<context position="30389" citStr="Gong and Liu, 2001" startWordPosition="5010" endWordPosition="5013">ndSum is a weak baseline that randomly selects reposts into summaries. • RepSum: RepSum ranks and selects messages simply by their reposts count, i.e., the size of their subtrees, based on reposting relations. • UserRankSum: UserRankSum ranks and selects reposts by their authors’ follower count based on user following relations. • LeadProSum: LeadProSum ranks and selects reposting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with extern</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proceedings of the 24th Annual International ACM Conference on Research and Development in Information Retrieval, SIGIR, pages 19–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Inouye</author>
<author>Jugal K Kalita</author>
</authors>
<title>Comparing twitter summarization algorithms for multiple post summaries.</title>
<date>2011</date>
<booktitle>In Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom),</booktitle>
<pages>298--306</pages>
<contexts>
<context position="8630" citStr="Inouye and Kalita, 2011" startWordPosition="1332" endWordPosition="1335">maries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich</context>
<context position="28934" citStr="Inouye and Kalita, 2011" startWordPosition="4792" endWordPosition="4795">s to summarize each topic in the list. We utilized ROUGE-N metric (Lin, 2004) for benchmark, which is a standard for evaluating automatic summaries based on N-gram overlapping between a generated summary and a reference. Specifically, ROUGE-1 and ROUGE-2 F1-measure were used as our evaluation metrics. Lin et al. (2004) has demonstrated that ROUGE-2 correlates well with humans in summarizing formal texts. And ROUGE-1 is a better alternative in evaluating summaries for short and informal 4All descriptions are English translations of the root microblogs originally in Chinese. microblog messages (Inouye and Kalita, 2011; Chang et al., 2013). In our human-generated summaries, the average inter-annotator-agreement by ROUGE-1 is 0.431, which means each pair of manual summaries have no more than 50% words overlap on average even written under topic constraints. This indicates that microblog repost tree summarization is generally a difficult task. The reason is that repost trees have complex structure, and editors could hardly reconstruct the repost trees even though they went through all the microblogs. Therefore, in evaluation for each tree, we computed the average ROUGE F1 score between the model-generated sum</context>
</contexts>
<marker>Inouye, Kalita, 2011</marker>
<rawString>David Inouye and Jugal K. Kalita. 2011. Comparing twitter summarization algorithms for multiple post summaries. In Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), pages 298–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>ChunNam John Yu</author>
</authors>
<title>Cutting-plane training of structural svms.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="25770" citStr="Joachims et al., 2009" startWordPosition="4294" endWordPosition="4297">t al., 2013). Then, we used the labels agreed by at least two annotators as the ground truth. The training and test of the leader detection models were conducted on this corpus. We compared the performance of CRF-based leader detection model with four baselines: Random Classifier (RC) as a weak baseline; two stateof-the-art point-wise supervised models Logistic Regression (LR) and Support Vector Machine (SVM); and an effective context sensitive model SVMhmm. We applied LibLinear toolkit (Fan et al., 2008) to implement LR and SVM with linear kernel. SVMhmm was implemented by SVMstruct toolkit (Joachims et al., 2009). And CRF’s implementation was based on CRF++3. For all the baselines, we used features listed in Table 1. The hyper-parameters of all leader detection models were tuned to the same extent based on 5-fold cross validation (with 1 fold as development set). The evaluation metrics were precision, recall and F1 score for the detected leaders. 5.1.2 Results Table 2 shows the comparison result of 5-fold cross validation on 1,000 repost tree paths and held-out experiment on 300 complete fresh paths. Among all baselines, SVMhmm performed the best, which indicates the effectiveness of incorporating str</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and ChunNam John Yu. 2009. Cutting-plane training of structural svms. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="12100" citStr="Lafferty et al., 2001" startWordPosition="1895" endWordPosition="1898">-follower relationship; Dark boxes: leaders to be detected. if we look into its context, we can find that [R2] talks about similar things as [R1], then [R1] classified as a follower indicates the higher chance of [R2] being a follower rather than a leader. Therefore, context information is important for indicating the messages being leaders or followers. We extract all root-to-leaf paths within a repost tree structure and detect leaders across each path. We formulate leader detection on repost tree paths as a sequence tagging problem by utilizing a stateof-the-art sequence learning model CRF (Lafferty et al., 2001), and taking advantage of its power in maximizing the likelihood of global label sequences. We adopt CRF rather than other competitive context sensitive model like SVMhmm (Altun et al., 2003) due to its probabilistic nature. The probability of prediction by CRF can provide critical chances for the following summarization procedure to reduce the impact of errors made by leader detection model on summarization (see Section 4.2). We map a repost tree path with n microblogs (m1, m2, · · · , mn) to a training instance (X, Y ). Let X = (x1, x2, · · · , xn) represents observed sequence, where xi deno</context>
<context position="14937" citStr="Lafferty et al., 2001" startWordPosition="2405" endWordPosition="2408"> of hashtags The number of hashtags in mi # of urls The number of URLs in mi # of mentions The number of mentions, or @UserName, in mi Similarity to neighbors Cosine similarity between mi and mi+d where d E {±1, ±2, ±3} Path-specific Similarity to root Cosine similarity to the root microblog in repost tree path Table 1: Features used for leader detection θ = (λ1, λ2, ...; µ1, µ2, ...) are the parameters indicating the weights of features that can be estimated by maximum likelihood procedure in training process. The prediction is done based on dynamic programming. More details can be found in (Lafferty et al., 2001). Table 1 lists the features we use for leader detection. CRF can utilize both historical and future information for prediction so as to maximize the likelihood of the global label sequences. But we would encounter the problem of label conflict, i.e., the predictions for the same repost in context of different paths might be different. For this reason, we determine a repost as a leader if its average marginal probabilities being a leader in context of different paths exceeds 50%. 4 LeadSum Summarization Model Let T = (V, E) represent a repost tree to be summarized, where V is a set of nodes co</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangda Li</author>
<author>Ke Zhou</author>
<author>Gui-Rong Xue</author>
<author>Hongyuan Zha</author>
<author>Yong Yu</author>
</authors>
<title>Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web, WWW,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="17044" citStr="Li et al., 2009" startWordPosition="2777" endWordPosition="2780">ected leaders. For the leaders detected in a repost tree T, we build a similarity graph among leaders denoted as GL = (VL, EL), where VL = {v E V |v is a detected leader} is the vertex set and EL = {(u,v)|u E VL, v E VL, and u =�v} is the edge set. The weight for any edge (u, v) represents the content similarity between u and v, for which we use cosine similarity. DivRank (Mei et al., 2010) is a generic graph ranking model that aims to balance high information coverage and low redundancy in top ranking vertices, which are also two key requirements for choosing salient summarization sentences (Li et al., 2009; Liu et al., 2015). Based on that, we present a model to rank and select salient messages from leader set VL to form a summary. Since this model simply assumes perfect leader detection, it is therefore named Basic-LeadSum. Similar as DivRank (Mei et al., 2010), the transition probability at the t-th iteration of random walk is given as follows: pt(u → v) = (1−µ)·p0(v)+µ· p0(u Z(� Nt−1(v) (1) and Z(u) is the normalizing factor: Z(u) = � p0(u → w)Nt−1(w) (2) w∈VL where p0(u → v) is the organic transition probability which represents the content similarity between u and v; Nt−1(v) denotes the ti</context>
</contexts>
<marker>Li, Zhou, Xue, Zha, Yu, 2009</marker>
<rawString>Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th International Conference on World Wide Web, WWW, pages 71– 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="28388" citStr="Lin, 2004" startWordPosition="4711" endWordPosition="4712">maries for each repost tree. To ensure the quality of reference summaries, we first extracted a list of frequent nouns from each repost tree and generalized 7 to 10 topics based on the nouns list, which provided a high-level overview of a repost tree to editors. Then, our guideline required editors to read all repost microblogs ordered sequentially on a repost tree. For every message, its entire repost tree path was also provided as supplementary context information. When finished reading, editors wrote down one or two sentences to summarize each topic in the list. We utilized ROUGE-N metric (Lin, 2004) for benchmark, which is a standard for evaluating automatic summaries based on N-gram overlapping between a generated summary and a reference. Specifically, ROUGE-1 and ROUGE-2 F1-measure were used as our evaluation metrics. Lin et al. (2004) has demonstrated that ROUGE-2 correlates well with humans in summarizing formal texts. And ROUGE-1 is a better alternative in evaluating summaries for short and informal 4All descriptions are English translations of the root microblogs originally in Chinese. microblog messages (Inouye and Kalita, 2011; Chang et al., 2013). In our human-generated summarie</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
<author>Fuliang Weng</author>
</authors>
<title>Why is sxsw trending?: exploring multiple text sources for twitter topic summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>pages</pages>
<contexts>
<context position="8676" citStr="Liu et al., 2011" startWordPosition="1339" endWordPosition="1342">f social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kin</context>
</contexts>
<marker>Liu, Liu, Weng, 2011</marker>
<rawString>Fei Liu, Yang Liu, and Fuliang Weng. 2011. Why is sxsw trending?: exploring multiple text sources for twitter topic summarization. In Proceedings of the Workshop on Languages in Social Media, pages 66– 75. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Yitong Li</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Graph-based multi-tweet summarization using social signals.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>1699--1714</pages>
<contexts>
<context position="9069" citStr="Liu et al., 2012" startWordPosition="1402" endWordPosition="1405">ome works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kind of signal. Chang et al. (2013) proposed a task to summarize Twitter context trees consisting of an original tweet and all its reposts (i.e., replies and retweets). They combined user influence signals into a supervised summarization framework. Our work is different from theirs: 1) They simply treat a context tree as a tweets stream while we consider repost tree structures in summarization</context>
</contexts>
<marker>Liu, Li, Wei, Zhou, 2012</marker>
<rawString>Xiaohua Liu, Yitong Li, Furu Wei, and Ming Zhou. 2012. Graph-based multi-tweet summarization using social signals. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1699–1714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>He Liu</author>
<author>Hongliang Yu</author>
<author>Zhi-Hong Deng</author>
</authors>
<title>Multi-document summarization based on two-level sparse representation model.</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence, AAAI,</booktitle>
<pages>196--202</pages>
<contexts>
<context position="17063" citStr="Liu et al., 2015" startWordPosition="2781" endWordPosition="2784">r the leaders detected in a repost tree T, we build a similarity graph among leaders denoted as GL = (VL, EL), where VL = {v E V |v is a detected leader} is the vertex set and EL = {(u,v)|u E VL, v E VL, and u =�v} is the edge set. The weight for any edge (u, v) represents the content similarity between u and v, for which we use cosine similarity. DivRank (Mei et al., 2010) is a generic graph ranking model that aims to balance high information coverage and low redundancy in top ranking vertices, which are also two key requirements for choosing salient summarization sentences (Li et al., 2009; Liu et al., 2015). Based on that, we present a model to rank and select salient messages from leader set VL to form a summary. Since this model simply assumes perfect leader detection, it is therefore named Basic-LeadSum. Similar as DivRank (Mei et al., 2010), the transition probability at the t-th iteration of random walk is given as follows: pt(u → v) = (1−µ)·p0(v)+µ· p0(u Z(� Nt−1(v) (1) and Z(u) is the normalizing factor: Z(u) = � p0(u → w)Nt−1(w) (2) w∈VL where p0(u → v) is the organic transition probability which represents the content similarity between u and v; Nt−1(v) denotes the times vertex v is vis</context>
</contexts>
<marker>Liu, Yu, Deng, 2015</marker>
<rawString>He Liu, Hongliang Yu, and Zhi-Hong Deng. 2015. Multi-document summarization based on two-level sparse representation model. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence, AAAI, pages 196–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Long</author>
<author>Haofen Wang</author>
<author>Yuqiang Chen</author>
<author>Ou Jin</author>
<author>Yong Yu</author>
</authors>
<title>Towards effective event detection, tracking and summarization on microblog data.</title>
<date>2011</date>
<booktitle>In Web-Age Information Management - 12th International Conference, WAIM,</booktitle>
<pages>652--663</pages>
<contexts>
<context position="8349" citStr="Long et al., 2011" startWordPosition="1288" endWordPosition="1291"> text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2</context>
</contexts>
<marker>Long, Wang, Chen, Jin, Yu, 2011</marker>
<rawString>Rui Long, Haofen Wang, Yuqiang Chen, Ou Jin, and Yong Yu. 2011. Towards effective event detection, tracking and summarization on microblog data. In Web-Age Information Management - 12th International Conference, WAIM, pages 652–663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Jian Guo</author>
<author>Dragomir R Radev</author>
</authors>
<title>Divrank: the interplay of prestige and diversity in information networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD,</booktitle>
<pages>1009--1018</pages>
<contexts>
<context position="15824" citStr="Mei et al., 2010" startWordPosition="2561" endWordPosition="2564">r the same repost in context of different paths might be different. For this reason, we determine a repost as a leader if its average marginal probabilities being a leader in context of different paths exceeds 50%. 4 LeadSum Summarization Model Let T = (V, E) represent a repost tree to be summarized, where V is a set of nodes corresponding to microblog messages, and E = {(u, v)|v is the repost of u} is the edge set denoting reposting relations. This section describes how to rank nodes in V to produce repost tree summaries. Enlightened by the general randomwalk-based ranking algorithm DivRank (Mei et al., 2010), we propose an unsupervised summarization model called LeadSum that aims to select true and salient leaders into summaries utilizing a variant of random walk based on content similarities and reposting relations of messages. We first present a basic LeadSum model, which assumes leader detection is perfect. Then, we enhance it to become a soft LeadSum model that reduces the impact of leader detection errors on the summarization. 4.1 Basic-LeadSum Model Due to the nature of leaders, they generally cover more important contents than follows do. Thus our first summarizer selects contents only fro</context>
<context position="17305" citStr="Mei et al., 2010" startWordPosition="2824" endWordPosition="2827"> for any edge (u, v) represents the content similarity between u and v, for which we use cosine similarity. DivRank (Mei et al., 2010) is a generic graph ranking model that aims to balance high information coverage and low redundancy in top ranking vertices, which are also two key requirements for choosing salient summarization sentences (Li et al., 2009; Liu et al., 2015). Based on that, we present a model to rank and select salient messages from leader set VL to form a summary. Since this model simply assumes perfect leader detection, it is therefore named Basic-LeadSum. Similar as DivRank (Mei et al., 2010), the transition probability at the t-th iteration of random walk is given as follows: pt(u → v) = (1−µ)·p0(v)+µ· p0(u Z(� Nt−1(v) (1) and Z(u) is the normalizing factor: Z(u) = � p0(u → w)Nt−1(w) (2) w∈VL where p0(u → v) is the organic transition probability which represents the content similarity between u and v; Nt−1(v) denotes the times vertex v is visited up to the (t − 1)-th iteration; p0(v) = |VL |denotes random jumping probability similar 1 to that in PageRank; and µ is the damping weight set as 0.85 following most PageRank-based models. The probability of traveling to leader v can acc</context>
<context position="30559" citStr="Mei et al., 2010" startWordPosition="5037" endWordPosition="5040">btrees, based on reposting relations. • UserRankSum: UserRankSum ranks and selects reposts by their authors’ follower count based on user following relations. • LeadProSum: LeadProSum ranks and selects reposting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with external users, we utilize users’ fol2174 Name # of nodes # of nodes with comments Height Description Tree (I) 21,353 15,409 16 HKU dropping out student wins the college entran</context>
</contexts>
<marker>Mei, Guo, Radev, 2010</marker>
<rawString>Qiaozhu Mei, Jian Guo, and Dragomir R. Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD, pages 1009–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Sujian Li</author>
<author>Houfeng Wang</author>
</authors>
<title>Entity-centric topic-oriented opinion summarization in twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD,</booktitle>
<pages>379--387</pages>
<contexts>
<context position="8388" citStr="Meng et al., 2012" startWordPosition="1296" endWordPosition="1299">categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like</context>
</contexts>
<marker>Meng, Wei, Liu, Zhou, Li, Wang, 2012</marker>
<rawString>Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Sujian Li, and Houfeng Wang. 2012. Entity-centric topic-oriented opinion summarization in twitter. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages 379–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mensink</author>
<author>Jakob J Verbeek</author>
<author>Gabriela Csurka</author>
</authors>
<title>Tree-structured CRF models for interactive image labeling.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>2</issue>
<pages>489</pages>
<marker>Mensink, Verbeek, Csurka, 2013</marker>
<rawString>Thomas Mensink, Jakob J. Verbeek, and Gabriela Csurka. 2013. Tree-structured CRF models for interactive image labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(2):476– 489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Fudannlp: A toolkit for chinese natural language processing.</title>
<date>2013</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="23897" citStr="Qiu et al., 2013" startWordPosition="3988" endWordPosition="3991">: Sort all v E V by RN(v) in descending order 19: Pick the top-n messages as summary WALK-2 sampling from leader probabilities, it also reduces the risk of including real followers into summary. 5 Experiments and Results To evaluate the two modules in our repost tree summarization system, i.e., CRF-based model for leader detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011)2. Microblog messages on Sina Weibo are in Chinese and we use FudanNLP (Qiu et al., 2013) for text preprocessing including word segmentation and POS tagging. 5.1 Experiment for Leader Detection In this experiment, we evaluated the performance of CRF model for leader detection task. 5.1.1 Data Collection and Setup We first crawled 1,300 different repost trees using the public PKUVIS toolkit (Ren et al., 2014). Given an original microblog post, the toolkit can automatically crawl its complete repost tree. For each tree, we randomly selected one path and further formed a set with 1,300 repost tree paths, 2The datasets are available at http://www1.se. cuhk.edu.hk/˜lijing/data/repost_t</context>
</contexts>
<marker>Qiu, Zhang, Huang, 2013</marker>
<rawString>Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Fudannlp: A toolkit for chinese natural language processing. In Proceedings of Annual Meeting of the Association for Computational Linguistics, ACL, pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Eduard H Hovy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Introduction to the special issue on summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="7720" citStr="Radev et al., 2002" startWordPosition="1189" endWordPosition="1192">. • We identify a novel problem of leader detection for summarization, which aims to reduce noise on repost trees, and present a CRF-based method for effectively detecting leaders by utilizing the tree structure and message contents. • We incorporate the leader detection result into an unsupervised summarization model based on random walk and substantially enhance the model to reduce the impact of leader detection errors on summarization. 2 Related Work The goal of text summarization is to automatically produce a succinct summary for one or more documents that preserves important information (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Dragomir R. Radev, Eduard H. Hovy, and Kathleen McKeown. 2002. Introduction to the special issue on summarization. Computational Linguistics, 28(4):399–408.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dragomir R Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda C¸elebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Dr´abek</author>
<author>Ali Hakim</author>
<author>Wai Lam</author>
<author>Danyu Liu</author>
<author>Jahna Otterbacher</author>
<author>Hong Qi</author>
<author>Horacio Saggion</author>
<author>Simone Teufel</author>
<author>Michael Topper</author>
<author>Adam Winkel</author>
<author>Zhu Zhang</author>
</authors>
<title>MEAD - A platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC.</booktitle>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, C¸elebi, Dimitrov, Dr´abek, Hakim, Lam, Liu, Otterbacher, Qi, Saggion, Teufel, Topper, Winkel, Zhang, 2004</marker>
<rawString>Dragomir R. Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda C¸elebi, Stanko Dimitrov, Elliott Dr´abek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel, and Zhu Zhang. 2004. MEAD - A platform for multidocument multilingual text summarization. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rapoza</author>
</authors>
<title>China’s weibos vs us’s twitter: And the winner is.</title>
<date>2011</date>
<contexts>
<context position="23808" citStr="Rapoza, 2011" startWordPosition="3973" endWordPosition="3974">1(u) - q(u --+ v) 14: Set current walk=“WALK-1” 15: end if 16: end for 17: end for 18: Sort all v E V by RN(v) in descending order 19: Pick the top-n messages as summary WALK-2 sampling from leader probabilities, it also reduces the risk of including real followers into summary. 5 Experiments and Results To evaluate the two modules in our repost tree summarization system, i.e., CRF-based model for leader detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011)2. Microblog messages on Sina Weibo are in Chinese and we use FudanNLP (Qiu et al., 2013) for text preprocessing including word segmentation and POS tagging. 5.1 Experiment for Leader Detection In this experiment, we evaluated the performance of CRF model for leader detection task. 5.1.1 Data Collection and Setup We first crawled 1,300 different repost trees using the public PKUVIS toolkit (Ren et al., 2014). Given an original microblog post, the toolkit can automatically crawl its complete repost tree. For each tree, we randomly selected one path and further formed a set with 1,300 repost tre</context>
</contexts>
<marker>Rapoza, 2011</marker>
<rawString>Kenneth Rapoza. 2011. China’s weibos vs us’s twitter: And the winner is.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghao Ren</author>
<author>Xin Zhang</author>
<author>Zhenhuang Wang</author>
<author>Jing Li</author>
<author>Xiaoru Yuan</author>
</authors>
<title>Weiboevents: A crowd sourcing weibo visual analytic system.</title>
<date>2014</date>
<booktitle>In IEEE Pacific Visualization Symposium, PacificVis,</booktitle>
<pages>330--334</pages>
<contexts>
<context position="7004" citStr="Ren et al., 2014" startWordPosition="1073" endWordPosition="1076">heir possibilities of being leaders to rank and select salient reposting messages that form summaries. Furthermore, we improve the framework by enhancing the random walk to reduce the impact of errors cascaded from the leader detection module. Compared to the state-of-theart baselines, the experimental results confirm the effectiveness of our proposed framework. Our contributions are given as follows: • We propose a novel microblog context summarization framework, in which given reposting messages organized as a repost tree (obtaining repost tree is trivial using public microblogging toolkit (Ren et al., 2014)), we summarize the repost trees based on content information and reposting relations of messages. • We identify a novel problem of leader detection for summarization, which aims to reduce noise on repost trees, and present a CRF-based method for effectively detecting leaders by utilizing the tree structure and message contents. • We incorporate the leader detection result into an unsupervised summarization model based on random walk and substantially enhance the model to reduce the impact of leader detection errors on summarization. 2 Related Work The goal of text summarization is to automati</context>
<context position="24219" citStr="Ren et al., 2014" startWordPosition="4038" endWordPosition="4041"> detection and LeadSum model for summarization, we conducted two sets of experiments based on microblog posts data collected from Sina Weibo, which has a similar market penetration as Twitter (Rapoza, 2011)2. Microblog messages on Sina Weibo are in Chinese and we use FudanNLP (Qiu et al., 2013) for text preprocessing including word segmentation and POS tagging. 5.1 Experiment for Leader Detection In this experiment, we evaluated the performance of CRF model for leader detection task. 5.1.1 Data Collection and Setup We first crawled 1,300 different repost trees using the public PKUVIS toolkit (Ren et al., 2014). Given an original microblog post, the toolkit can automatically crawl its complete repost tree. For each tree, we randomly selected one path and further formed a set with 1,300 repost tree paths, 2The datasets are available at http://www1.se. cuhk.edu.hk/˜lijing/data/repost_tree_ summ.zip Cross-validation Held-out Prec Rec F1 Prec Rec F1 Random 29.8 49.5 37.3 31.6 49.6 38.6 LR 70.5 66.3 68.4 70.4 66.2 68.2 SVM 70.9 66.9 68.8 68.9 66.2 67.5 SVMhmm 74.8 65.5 69.8 69.3 70.1 69.7 CRF 75.5 72.0 73.7 71.1 70.7 70.9 Table 2: The performance of leader detection (%) which ensures that paths have diff</context>
<context position="27427" citStr="Ren et al., 2014" startWordPosition="4547" endWordPosition="4550">exploited by the sampling process in Soft-LeadSum to reduce the propagation of classification error to the summarization stage. Section 5.2.2 shows the relevant experiment. 5.2 Experiment for Summarization In this experiment, we evaluated end-to-end performance of our basic and soft LeadSum summarization models by comparing them with state-ofthe-art microblog summarizers. 5.2.1 Data Collection and Evaluation Metrics There is no public editorial repost tree dataset. Therefore, we manually selected 10 hot events taking place during January 2nd – July 28th 2014, and then used the PKUVIS toolkit (Ren et al., 2014) to crawl the complete repost trees for all the events given the corresponding original posts. Table 3 shows the details about the repost tree corpus4. Note that this repost tree corpus has no overlap with the repost tree path dataset for learning leader detection models in Section 5.1.1. After that, we invited three experienced editors to write summaries for each repost tree. To ensure the quality of reference summaries, we first extracted a list of frequent nouns from each repost tree and generalized 7 to 10 topics based on the nouns list, which provided a high-level overview of a repost tre</context>
</contexts>
<marker>Ren, Zhang, Wang, Li, Yuan, 2014</marker>
<rawString>Donghao Ren, Xin Zhang, Zhenhuang Wang, Jing Li, and Xiaoru Yuan. 2014. Weiboevents: A crowd sourcing weibo visual analytic system. In IEEE Pacific Visualization Symposium, PacificVis, pages 330–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Dela Rosa</author>
<author>Rushin Shah</author>
<author>Bo Lin</author>
<author>Anatole Gershman</author>
<author>Robert Frederking</author>
</authors>
<title>Topical clustering of tweets.</title>
<date>2011</date>
<booktitle>Proceedings of the ACM SIGIR: SWSM.</booktitle>
<contexts>
<context position="8368" citStr="Rosa et al., 2011" startWordPosition="1292" endWordPosition="1295"> techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered</context>
</contexts>
<marker>Rosa, Shah, Lin, Gershman, Frederking, 2011</marker>
<rawString>Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gershman, and Robert Frederking. 2011. Topical clustering of tweets. Proceedings of the ACM SIGIR: SWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beaux Sharifi</author>
<author>Mark-Anthony Hutton</author>
<author>Jugal Kalita</author>
</authors>
<title>Automatic summarization of twitter topics.</title>
<date>2010</date>
<booktitle>In National Workshop on Design and Analysis ofAlgorithm.</booktitle>
<marker>Sharifi, Hutton, Kalita, 2010</marker>
<rawString>Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita. 2010. Automatic summarization of twitter topics. In National Workshop on Design and Analysis ofAlgorithm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Tao Li</author>
</authors>
<title>A participant-based approach for event summarization using twitter streams.</title>
<date>2013</date>
<booktitle>In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL,</booktitle>
<pages>1152--1162</pages>
<contexts>
<context position="8317" citStr="Shen et al., 2013" startWordPosition="1281" endWordPosition="1284"> (Radev et al., 2002). Generally, text summarization techniques can be categorized into extractive and abstractive methods (Das and Martins, 2007). Extractive approaches focus on how to identify and distill salient contents from original texts whereas abstractive approaches aim at producing grammatical summaries by text generation. Recently, the development of social media has made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Ch</context>
</contexts>
<marker>Shen, Liu, Weng, Li, 2013</marker>
<rawString>Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013. A participant-based approach for event summarization using twitter streams. In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL, pages 1152–1162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Hikaru Yokono</author>
<author>Manabu Okumura</author>
</authors>
<title>Summarizing a document stream.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR,</booktitle>
<pages>177--188</pages>
<contexts>
<context position="8700" citStr="Takamura et al., 2011" startWordPosition="1343" endWordPosition="1346"> made microblog summarization a hot topic. Most prior works are on event-level or topic-level summarization. Typically, the first step is to cluster posts into sub-events (Chakrabarti and Punera, 2011; Duan et al., 2012; Shen et al., 2013) or subtopics (Long et al., 2011; Rosa et al., 2011; Meng et al., 2012), and then the second step generates summary for each cluster. Some works tried to apply conventional extractive summarization models directly, e.g., LexRank (Erkan and Radev, 2004), MEAD (Radev et al., 2004), TF-IDF (Inouye and Kalita, 2011), Integer Linear Programming (Liu et al., 2011; Takamura et al., 2011), etc. Sharif et al. (2010) modeled the problem as optimal path finding on a phrase reinforcement graph. However, these general summarizers were found not suitable for microblog posts, which are informal and noisy (Chang et al., 2013). Researchers 2169 then considered social signals like user following relations and retweet count (Duan et al., 2012; Liu et al., 2012), and reported such features useful to help summarize microblog posts. Our work studies repost tree summarization by leveraging content-level structure to enrich context of messages, which is a different kind of signal. Chang et al</context>
</contexts>
<marker>Takamura, Yokono, Okumura, 2011</marker>
<rawString>Hiroya Takamura, Hikaru Yokono, and Manabu Okumura. 2011. Summarizing a document stream. In Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR, pages 177–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Tang</author>
<author>MingCai Hong</author>
<author>Juan-Zi Li</author>
<author>Bangyong Liang</author>
</authors>
<title>Tree-structured conditional random fields for semantic annotation.</title>
<date>2006</date>
<booktitle>In The Proceedings of 5th International Semantic Web Conference, ISWC,</booktitle>
<pages>640--653</pages>
<marker>Tang, Hong, Li, Liang, 2006</marker>
<rawString>Jie Tang, MingCai Hong, Juan-Zi Li, and Bangyong Liang. 2006. Tree-structured conditional random fields for semantic annotation. In The Proceedings of 5th International Semantic Web Conference, ISWC, pages 640–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Congrui Huang</author>
<author>Xiaojun Wan</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Timeline generation through evolutionary trans-temporal summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP,</booktitle>
<pages>433--443</pages>
<contexts>
<context position="30678" citStr="Yan et al. (2011)" startWordPosition="5058" endWordPosition="5061">ount based on user following relations. • LeadProSum: LeadProSum ranks and selects reposting messages by their marginal probabilities as leaders determined by our CRF-based leader detection model. • SVDSum: SVDSum adopts the Singular Value Decomposition (SVD) to discover hidden sub-topics for summarization (Gong and Liu, 2001). Reposting messages are ranked according to latent semantic analysis with SVD on termmessage matrix. • DivRankSum: DivRankSum directly applies DivRank (Mei et al., 2010) algorithm to rank all messages unaware of leaders and followers. A similar model is also reported in Yan et al. (2011). Following their work, we set damping weight as 0.85. • UserInfSum: Chang et al. (2013) ranks messages utilizing Gradient Boosted Decision Tree (GBDT) algorithm with text, popularity, temporal and user influence signals to summarize Twitter context tree. In particular, without the interaction data with external users, we utilize users’ fol2174 Name # of nodes # of nodes with comments Height Description Tree (I) 21,353 15,409 16 HKU dropping out student wins the college entrance exam again. Tree (II) 9,616 6,073 11 German boy complains hard schoolwork in Chinese High School. Tree (III) 13,087 </context>
</contexts>
<marker>Yan, Kong, Huang, Wan, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 433–443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
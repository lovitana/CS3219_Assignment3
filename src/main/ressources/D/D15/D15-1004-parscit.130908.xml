<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.993149">
Dependency Graph-to-String Translation
</title>
<author confidence="0.999322">
Liangyou Li Andy Way Qun Liu
</author>
<affiliation confidence="0.9882395">
ADAPT Centre, School of Computing
Dublin City University
</affiliation>
<email confidence="0.983312">
{liangyouli,away,qliu}@computing.dcu.ie
</email>
<sectionHeader confidence="0.997148" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912380952381">
Compared to tree grammars, graph gram-
mars have stronger generative capacity
over structures. Based on an edge re-
placement grammar, in this paper we pro-
pose to use a synchronous graph-to-string
grammar for statistical machine transla-
tion. The graph we use is directly con-
verted from a dependency tree by labelling
edges. We build our translation model
in the log-linear framework with stan-
dard features. Large-scale experiments
on Chinese–English and German–English
tasks show that our model is significantly
better than the state-of-the-art hierarchical
phrase-based (HPB) model and a recently
improved dependency tree-to-string model
on BLEU, METEOR and TER scores. Ex-
periments also suggest that our model has
better capability to perform long-distance
reordering and is more suitable for trans-
lating long sentences.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942769230769">
Compared to trees, which have dominated the field
of natural language processing (NLP) for decades,
graphs are more general for modelling natural lan-
guages. The corresponding grammars for recog-
nizing and producing graphs are more flexible and
powerful than tree grammars. However, because
of their high complexity, graph grammars have not
been widely used in NLP.
Recently, along with progress on graph-based
meaning representation, hyperedge replacement
grammars (HRG) (Drewes et al., 1997) have been
revisited, explored and used for semantic-based
machine translation (Jones et al., 2012). How-
ever, the translation process is rather complex and
the resources it relies on, namely abstract meaning
corpora, are limited as well.
As most available syntactic resources and tools
are tree-based, in this paper we propose to con-
vert dependency trees, which are usually taken as
a kind of shallow semantic representation, to de-
pendency graphs by labelling edges. We then use
a synchronous version of edge replacement gram-
mar (ERG) (Section 2), a special case of HRG,
to translate these graphs. The resulting translation
model has the same order of magnitude in terms
of time complexity with the hierarchical phrase-
based model (HPB) (Chiang, 2005) under a certain
restriction (Section 3).
Compared to dependency tree-to-string models,
using ERG for graph-to-string translation brings
some benefits (Section 3). Thanks to the stronger
generative capacity of the grammar, our model
can naturally translate siblings in a tree struc-
ture, which are usually treated as non-syntactic
phrases and handled by other techniques (Huck et
al., 2014; Xie et al., 2014). Furthermore, com-
pared to the known treelet approach (Quirk et al.,
2005) and Dep2Str (Xie et al., 2011), our method
not only uses treelets but also has a full capacity
of reordering.
We define our translation model (Section 4) in
the log-linear framework (Och and Ney, 2002).
Large-scale experiments (Section 5) on Chinese–
English and German–English, two language pairs
that have a high degree of syntactic reordering,
show that our method significantly improves trans-
lation quality over both HPB and Dep2Str, as
measured by BLEU (Papineni et al., 2002), TER
(Snover et al., 2006) and METEOR (Denkowski
and Lavie, 2011). We also find that the rules in
our model are more suitable for long-distance re-
ordering and translating long sentences.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="method">
2 Edge Replacement Grammar
</sectionHeader>
<bodyText confidence="0.999730333333333">
As a special case of HRG, ERG is also a context-
free rewriting grammar to recognize and produce
graphs. Following HRG, the graph we use in this
</bodyText>
<page confidence="0.990483">
33
</page>
<note confidence="0.995923">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.99946">
Figure 1: An example of a derivation in an ERG. Dark circles are external nodes.
</figureCaption>
<figure confidence="0.998484678571428">
Shijiebei
1
Y
1
FIFA
S
Juxing
Juxing
X
Chenggong
2010 2010
Y
Zai
Nanfei
Chenggong
Shijiebei
FIFA
Zai
Nanfei
Chenggong
2010
Juxing
Y
Zai
Nanfei
1
X
1
</figure>
<figureCaption confidence="0.5586625">
paper is connected, nodes ordered, acyclic and
has edge labels but no node labels (Chiang et al.,
2013). We provide some formal definitions on
ERG.
Definition 1. A connected, edge-labeled, ordered
graph is a tuple H = (V, E, 0), where
</figureCaption>
<listItem confidence="0.994317">
• V is a finite set of nodes.
• E C V 2 is a finite set of edges.
• 0 : E —* C assigns a label (drawn from C)
to each edge.
</listItem>
<bodyText confidence="0.991164166666667">
In ERG, the elementary unit is a graph frag-
ment, which is also the right-hand side of a pro-
duction in the grammar. Its definition is as follows.
Definition 2. A graph fragment is a tuple H =
(V, E, 0, X), where (V, E, 0) is a graph and X E
(V U V 2) is a list of distinct nodes. Following
Chiang et al. (2013), we call these external nodes.
The external nodes indicate how to integrate a
graph into another one during a derivation. Dif-
ferent to HRG, ERG limits the number of external
nodes to 2 at most to make sure hyperedges do not
exist during a derivation. Now we define the ERG.
</bodyText>
<construct confidence="0.8498885">
Definition 3. An edge replacement grammar is a
tuple (N, T, P, 5), where
</construct>
<listItem confidence="0.998783428571429">
• N and T are disjoint finite sets of non-
terminal symbols and terminal symbols, re-
spectively.
• P is a finite set of productions of the form
A —* R, where A E N and R is a graph frag-
ment, where edge-labels are from N U T.
• 5 E N is the start symbol.
</listItem>
<bodyText confidence="0.998827857142857">
Figure 1 shows an example of a derivation in an
ERG to produce a graph. Starting from the start
symbol 5, when a rule (A —* R) is applied to an
edge e, the edge is replaced by the graph fragment
R. Just like in HRG, the ordering of nodes Ve in e
and external nodes XR in R implies the mapping
from Ve to XR (Chiang et al., 2013).
</bodyText>
<sectionHeader confidence="0.983153" genericHeader="method">
3 Graph-to-String Grammar
</sectionHeader>
<bodyText confidence="0.9999278">
In SMT, we need a synchronous grammar to si-
multaneously parse an input graph and produce
translations. The graph we use in this paper is
from a dependency structure which is capable of
modelling long-distance relations in a sentence.
</bodyText>
<subsectionHeader confidence="0.999587">
3.1 The Grammar
</subsectionHeader>
<bodyText confidence="0.988963857142857">
Before defining the synchronous grammar, we
firstly define a dependency graph which is a spe-
cial case of a graph.
Definition 4. A dependency graph is a tuple
(V, E, 0, A), where (V, E, 0) is a graph and A is
a restriction: edges are ordered.
A dependency graph is directly derived from
a dependency tree by labeling edges with words,
as shown in Figure 2. Although in general graph
edges are unordered, in Definition 4 we keep word
order by ordering edges, because the word order is
an important piece of information for translation.
Similar to the graph fragment, a dependency-
graph fragment is defined as below.
</bodyText>
<construct confidence="0.63704725">
Definition 5. A dependency-graph fragment is a
tuple (V, E, 0, A, X), where (V, E, 0, A) is a de-
pendency graph, X E (V UV 2) is a list of external
nodes.
</construct>
<page confidence="0.999171">
34
</page>
<figureCaption confidence="0.921337333333333">
Figure 2: An example of deriving a dependency
graph from a dependency tree by labelling edges
with words.
</figureCaption>
<bodyText confidence="0.954691166666667">
In this paper, we define a synchronous ERG
over dependency graphs as a dependency graph-
to-string grammar, which can be used for MT.
Definition 6. A dependency graph-to-string
grammar (DGSG) is a tuple (N, T, T0, P, 5),
where
</bodyText>
<listItem confidence="0.918463444444444">
• N is a finite set of non-terminal symbols.
• T and T0 are finite sets of terminal symbols.
• 5 E N is the start symbol.
• P is a finite set of productions of the form
(A —* R, A0 —* R0, —), where A, A0 E N, R
is a dependency-graph fragment over N U T
and R0 is a string over N U T0. — is a one-to-
one mapping between non-terminal symbols
in R and R0.
</listItem>
<bodyText confidence="0.995724642857143">
Figure 3 shows a derivation simultaneously pro-
ducing a Chinese dependency graph and an En-
glish string using a DGSG. Each time a rule is ap-
plied, the dependency-graph fragment in the rule
replaces an edge in the source graph, and the string
in the rule replaces a non-terminal in the target
string.
Proposition 1. DGSG has stronger generative ca-
pacity over graph-string pairs than both SCFG and
synchronous tree substitution grammar (STSG).
Proof. STSG has stronger generative capacity
over structures than SCFG (Chiang, 2012).1
Any STSG can easily be converted into a DGSG
by labelling edges in tree structures.
</bodyText>
<footnote confidence="0.735489">
1The following STSG generates a trivial example of a
tree-string pair that no SCFG can generate, as SCFG must
always have an equal number of non-terminal symbols.
</footnote>
<equation confidence="0.9960648">
X
I
:X
I
E
</equation>
<bodyText confidence="0.976530142857143">
The following DGSG generates a trivial exam-
ple of a graph-string pair, which no STSG can gen-
erate, as the left-head side has no head nodes while
STSG always requires one to form a tree.
a b
This proof is also verified in Figure 3 where
the third rule is used to translate a non-syntactic
phrase, which can be a problem for dependency
tree-to-string methods. In addition, the second
rule translates a treelet and the first rule encodes
reordering information inside. All these three
aspects are uniformly modeled in our grammar,
which makes it more powerful than other methods,
such as the treelet approach and the Dep2Str.
</bodyText>
<subsectionHeader confidence="0.999965">
3.2 Time Complexity and a Restriction
</subsectionHeader>
<bodyText confidence="0.999993111111111">
Given a dependency graph, training and decod-
ing time using DGSG depends on the number of
dependency-graph fragments. For example, for a
graph where the degree of a node is k, the number
of all possible fragments starting from the node is
O(2k). Therefore, the time complexity would be
exponential if we consider them all.
It is easy to find that the high complexity of
DGSG comes from the free combination of edges.
That means that a dependency-graph fragment can
cover discontinuous words of an input sentence.
However, this is not the convention in the field of
SMT.
For efficient training and decoding, we add a re-
striction to DGSG: each dependency-graph frag-
ment covers a continuous span of the source sen-
tence. This reduces the complexity from exponen-
tial time to cubic time.
</bodyText>
<subsectionHeader confidence="0.997064">
3.3 Non-terminal Symbols
</subsectionHeader>
<bodyText confidence="0.9998334">
In this paper we build a dependency graph-to-
string model, so we only use one non-terminal
symbol X as in HPB on the target side. However,
on the source side we define non-terminal symbols
over Part-of-Speech (POS) tags, which can be eas-
ily obtained as a by-product of dependency pars-
ing.
We define the head of a dependency-graph frag-
ment H as a list of edges, the dependency head of
each of which is not in this fragment. Then the
</bodyText>
<figure confidence="0.996272698113208">
Juxing
Juxing
Shijiebei
2010 FIFA
Chenggong
Nanfei
2010
FIFA
Zai
Nanfei
Chenggong
Zai
Shijiebei
X
I
E
: c
35
S
S
M2
S
N1
Shijiebei
Zai
Nanfei
M2
S X1 World Cup X2 in South Africa
Zai
Nanfei
X1 World Cup was held successfully in South Africa
Shijiebei
N1
N1
X1 World Cup X2 in South Africa
Juxing
Shijiebei
Zai
Nanfei
Chenggong
Juxing
M
Chenggong
X was held successfully
N 2010 FIFA X 2010 FIFA
Juxing
Shijiebei
2010
FIFA
Zai
Nanfei
Chenggong
2010 FIFA World Cup was held successfully in South Africa
</figure>
<figureCaption confidence="0.998229">
Figure 3: An example of a derivation in dependency graph-to-string grammar to produce a Chinese
</figureCaption>
<bodyText confidence="0.509599">
dependency graph and an English string. Rules are included in dashed rectangles. Target strings are in
solid rectangles. External nodes are dark circles. This example is under the restriction in Section 3.2. In
addition to the start symbol S, non-terminal symbols for the source side are M and N, while the target
side only has one non-terminal X. The index in each non-terminal of a rule indicates the mapping.
</bodyText>
<page confidence="0.91239">
36
</page>
<figure confidence="0.997336">
NR_P_AD
Shijiebei/NR
Zai/P
Nanfei/NR
Chenggong/AD
</figure>
<figureCaption confidence="0.989143">
Figure 4: An example inducing a non-terminal
</figureCaption>
<bodyText confidence="0.908260875">
symbol (left side) for a dependency-graph frag-
ment (right side). Each edge is labeled by a word
associated with its POS tag. The head of this frag-
ment includes three edges which are in the rectan-
gle.
non-terminal symbol for H is defined as the join-
ing of POS tags of its head (Li et al., 2012). Figure
4 shows an example.
</bodyText>
<subsectionHeader confidence="0.994458">
3.4 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.995701909090909">
As well as the restriction defined in Section 3.2
making the grammar much smaller, it also results
in a similar way of extracting rules as in HPB. In-
spired by HPB, we define the rule set over initial
pairs.
Given a word-aligned dependency graph-string
pair P = (G, e, ∼), let Gji stand for the sub-graph
(it may not be connected) covering words from po-
sition i to position j. Then a rule (Gji, ej0
i0 ) is an
initial pair of P, iff:
</bodyText>
<listItem confidence="0.89519775">
1. Gji is a dependency-graph fragment. That
means it is a connected sub-graph and has at
most two external nodes, nodes which con-
nect with nodes outside or are the root.
2. It is consistent with the word alignment ∼
(Och and Ney, 2004).
The set of rules from P satisfies the following:
1. If (Gji , ej0
</listItem>
<equation confidence="0.81096">
i0 ) is an initial pair, then
(N(Gji) → Gji , X → ej0
i0 )
</equation>
<bodyText confidence="0.78474">
is a rule, where N(G) defines the non-
terminal symbol for G.
</bodyText>
<listItem confidence="0.568301">
2. If (N(R) → R, X → R&apos;) is a rule of P and
(Gji , ej0
i0 ) is an initial pair such that Gji is a
sub-graph of R and R&apos; = r1ej0
</listItem>
<equation confidence="0.8126">
i0 r2, then
(N(R) → R\Gji k,X → r1Xkr2)
</equation>
<bodyText confidence="0.973227">
is a rule of P, where \ means replacing Gji
in R with an edge labelled with N(Gji) and
k is a unique index for a pair of non-terminal
symbols.
As in HPB, in addition to rules extracted from
the parallel corpus, we also use glue rules to com-
bine fragments and translations when no matched
rule can be found.
Furthermore, we can use the same rule extrac-
tion algorithm as that in HPB, except that we need
to check if a span of a source sentence indicates
a dependency-graph fragment, in which case we
keep the dependency structure and induce a non-
terminal for the fragment.
</bodyText>
<sectionHeader confidence="0.995137" genericHeader="method">
4 Model and Decoding
</sectionHeader>
<bodyText confidence="0.999859">
We define our model in the log-linear framework
over a derivation d, as in Equation (1):
</bodyText>
<equation confidence="0.988743">
P(d) a � Oi(d)λi (1)
i
</equation>
<bodyText confidence="0.9476775">
where Oi are features defined on derivations and
Ai are feature weights. In our experiments, we use
</bodyText>
<listItem confidence="0.990198538461538">
9 features:
• translation probabilities P(s|t) and P(t|s),
where s is the source graph fragment and t
is the target string.
• lexical translation probabilities Plex(s|t) and
Plex(t|s).
• language model lm(e) over translation e.
• rule penalty exp(−1).
• word penalty exp(|e|).
• glue penalty exp(−1).
• unknown words penalty exp(u(g)), where
u(g) is the number of unknown words in a
source graph g.
</listItem>
<bodyText confidence="0.8794688">
Our decoder is based on the conventional chart
parsing CYK algorithm (Kasami, 1965; Younger,
1967; Cocke and Schwartz, 1970). It searches for
the best derivation d* among all possible deriva-
tions D, as in Equation (2):
</bodyText>
<equation confidence="0.9763065">
d* = argmax P(d) (2)
dED
</equation>
<bodyText confidence="0.707492">
For each span of an input graph, the decoder
checks if it is a dependency-graph fragment. Then
</bodyText>
<page confidence="0.992275">
37
</page>
<table confidence="0.992629166666667">
ZH–EN
corpus #sent. #words(ZH) #words(EN)
train 1.5M+ 38M+ ∼45M
dev 878 22,655 26,905
MT04 1,597 43,719 52,705
MT05 1,082 29,880 35,326
DE–EN
corpus #sent. #words(DE) #words(EN)
train 2M+ 52M+ 55M+
dev 3,003 72,661 74,753
WMT12 3,003 72,603 72,988
WMT13 3,000 63,412 64,810
</table>
<tableCaption confidence="0.645445">
Table 1: Chinese–English (ZH–EN) and German–
English (DE–EN) corpora. For the English side of
dev and test sets, words counts are averaged across
all references.
</tableCaption>
<bodyText confidence="0.9895195">
for each fragment, the decoder finds rules to trans-
late it. The translation of a large span can be ob-
tained by combining translations from its sub-span
using rules which have non-terminals. Finally,
glue rules are used to make sure that at least one
translation is produced.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.9986455">
We conduct experiments on Chinese–English and
German–English translation tasks.
</bodyText>
<subsectionHeader confidence="0.915675">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999964416666667">
The Chinese–English training corpus is from
LDC, including LDC2002E18, LDC2003E07,
LDC2003E14, LDC2004T07, the Hansards por-
tion of LDC2004T08 and LDC2005T06. NIST
2002 is taken as a development set to tune weights,
and NIST 2004 (MT04) and NIST 2005 (MT05)
are two test sets to evaluate systems. Table 1 pro-
vides a summary of this corpus. The Stanford Chi-
nese word segmenter (Chang et al., 2008) is used
to segment Chinese sentences. The Stanford de-
pendency parser (Chang et al., 2009) parses a Chi-
nese sentence into a projective dependency tree
which is then converted to a dependency graph in
our model.
The German–English training corpus is from
WMT 2014, including Europarl V7 and News
Commentary. News-test 2011 is taken as a de-
velopment set, while News-test 2012 (WMT12)
and News-test 2013 (WMT13) are our test sets.
Table 1 provides a summary of this corpus. We
use mate-tools2 to perform morphological analysis
and parse German sentences (Bohnet, 2010). Then
MaltParser3 converts a parse result into a projec-
tive dependency tree (Nivre and Nilsson, 2005).
</bodyText>
<subsectionHeader confidence="0.996903">
5.2 Settings
</subsectionHeader>
<bodyText confidence="0.99993028">
In this paper, we mainly compare our system
(DGST) with HPB in Moses (Koehn et al., 2007).
We implement our model in Moses and take
the same settings as Moses HPB in all experi-
ments. In addition, translation results from a re-
cently open-source dependency tree-to-string sys-
tem, Dep2Str4 (Li et al., 2014), which is imple-
mented in Moses and improves the dependency-
based model in Xie et al. (2011), are also reported.
All systems use the same sets of features defined
in Section 4.
In all experiments, word alignment is performed
by GIZA++ (Och and Ney, 2003) with the heuris-
tic function grow-diag-final-and. We use SRILM
(Stolcke, 2002) to train a 5-gram language model
on the Xinhua portion of the English Gigaword
corpus 5th edition with modified Kneser-Ney dis-
counting (Chen and Goodman, 1996). Minimum
Error Rate Training (MERT) (Och, 2003) is used
to tune weights.
To obtain more reliable results, in each experi-
ment, we run MERT three times and report aver-
age scores. These scores are calculated by three
widely used automatic metrics in case-insensitive
mode: BLEU, METEOR and TER.
</bodyText>
<subsectionHeader confidence="0.728203">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9999078">
Table 2 shows the scores of all three metrics on all
systems. Similar to Li et al. (2014), in our experi-
ments Dep2Str has on average a comparable result
with Moses HPB in terms of BLEU and METEOR
scores. However, it obtains a significantly higher
(i.e. worse) TER score on the Chinese–English
task. This may suggest that translations produced
by Dep2Str need more post-editing effort (He et
al., 2010).
By contrast, on all test sets, measured by all
metrics, our system is significantly better than
Moses HPB. On the Chinese–English task, our
system achieves an average gain of 1.25 (abso-
lute, 3.6% relative) BLEU score and 0.55 (abso-
lute, 1.7% relative) METEOR score while also ob-
</bodyText>
<footnote confidence="0.99946125">
2http://code.google.com/p/mate-tools/
3http://www.maltparser.org/
4http://computing.dcu.ie/˜liangyouli/
dep2str.zip
</footnote>
<page confidence="0.995834">
38
</page>
<table confidence="0.999797363636364">
Metric System ZH–EN DE–EN
MT04 MT05 WMT12 WMT13
Moses HPB 35.6 33.8 20.2 22.7
BLEU T Dep2Str 35.4 33.9 20.3 22.8
DGST 36.6 35.3 20.7 23.3
Moses HPB 31.6 31.9 28.6 29.7
METEOR T Dep2Str 31.8 31.9 28.5 29.5*
DGST 32.1 32.5 28.7 29.8
Moses HPB 57.0 58.3 63.2 59.5
TER Dep2Str 58.2* 59.6* 63.1 59.6
DGST 56.1 57.0 62.6 59.0
</table>
<tableCaption confidence="0.979319666666667">
Table 2: Metric scores for all systems on Chinese–English (ZH–EN) and German–English (DE–EN) cor-
pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly
better than Moses HPB at p &lt; 0.01. Moses HPB is significantly better than systems with * at p &lt; 0.01.
</tableCaption>
<table confidence="0.999877285714286">
Length Percentage
MT04 MT05 WMT12 WMT13
(0, 10] 7.6% 8.6% 15.0% 19.2%
(10, 20] 28.2% 26.0% 31.4% 37.2%
(20, 30] 28.2% 26.5% 26.3% 24.5%
(30, 40] 20.2% 23.8% 14.4% 12.0%
(40, oc) 15.7% 15.2% 12.9% 7.2%
</table>
<tableCaption confidence="0.864537">
Table 3: Statistics of sentence length on four test
sets.
</tableCaption>
<bodyText confidence="0.9984825">
taining a reduction of 1.1 (absolute, 1.91% rela-
tive) TER score on average.
On the German–English task, our system
achieves an average gain of 0.55 (absolute, 2.56%
relative) BLEU score and 0.1 (absolute, 0.35% rel-
ative) METEOR score and also obtains a reduction
of 0.55 (absolute, 0.89% relative) TER score on
average.
</bodyText>
<subsectionHeader confidence="0.996136">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999978111111111">
As shown in Table 2, compared to Moses HPB
and Dep2Str, our system achieves higher transla-
tion quality as measured by three automatic met-
rics. In this section, we investigate whether de-
pendency structures bring benefits as expected on
long-distance reordering. Table 3 provides the
statistics on sentence length of our four test sets.
In both HPB and our model, the length range
of a reordering performed on an input sentence is
related to the use of glue grammars which bring
two benefits during decoding. When no matched
rule is found in the models, glue grammars are ap-
plied to make sure a translation is produced. In ad-
dition, because of the generalization capability of
rules, which typically are learned under a length
limitation, using them on long sentences could
cause translation quality to deteriorate. Therefore,
when the length of a phrase is greater than a cer-
tain value, glue grammars are also applied. There-
fore, our experiment of analysis is based on the
length limitation that a rule can cover (max. phrase
length) during decoding.
We set this max. phrase length to different val-
ues, including 10, 20 (default), 30, 40 and 50.
Figure 5 gives the BLEU scores on all test sets.
We find that on all different values, our system
achieves higher BLEU scores than Moses HPB.
In addition, when the max. phrase length be-
comes larger, Moses HPB shows a declining trend
in most cases, especially on the German–English
task (WMT12 and WMT13). However, our sys-
tem is less sensitive to this value. We hypothesize
that this is because rules from dependency graphs
have better generalization for translating longer
phrases and are more suitable for translating long
sentences.
</bodyText>
<subsectionHeader confidence="0.999268">
5.5 Case Study
</subsectionHeader>
<bodyText confidence="0.999992181818182">
On a manual check, we find that translations pro-
duced by our system are more fluent than those of
both Moses HPB and Dep2Str. Figure 6 gives an
example comparing translations produced by three
systems on the Chinese–English task.
We first find a case of long-distance relation,
i.e. the subject-verb-object (SVO) structure in the
source sentence. In this example, this relation im-
plies a long-distance reordering, which moves the
translation of the object to the front of its mod-
ifiers, as shown in the given reference. Com-
</bodyText>
<page confidence="0.995151">
39
</page>
<figure confidence="0.995297454545454">
MT04
MT05
36.5
36
35.5
35
34.5
34
BLEU
10 20 30 40 50
10 20 30 40 50
WMT12
23.2
23
22.8
22.6
WMT13
20.6
20.4
20.2
Moses HPB DGST
Max. Phrase Length
</figure>
<figureCaption confidence="0.9282015">
Figure 5: BLEU scores of Moses HPB and DGST (our system) when the length of maximum phrase that
a rule can cover during decoding is set to different values.
</figureCaption>
<figure confidence="0.932417423076923">
punct
dobj
nsubj
)�-T
about
A
establish
���
council
*M1
special
MO
court
�
,
WSJ
try
&gt;R)i
two sides
Va V4AA
welcome Iraqi
MiRf
interim
IftfN
governing
AAE
</figure>
<equation confidence="0.7111755">
murderer
N V�Z �
</equation>
<bodyText confidence="0.986845384615385">
of decision .
Ref: The two sides welcomed the decision by the Iraqi Interim Governing Council to establish a special court to try the murderers.
HPB: the two sides welcomed the
interim iraqi authority on establishing
a special court, trial of the murderer.
Dep2Str: the two sides welcomed the
decision on the Establishment of a
special court, justice murderers of the
provisional governing council of iraq.
DGST: the two sides welcomed the decision
of the iraqi interim governing council on the
establishment of a special court, justice
murderers.
</bodyText>
<figureCaption confidence="0.986450833333333">
Figure 6: An example of comparing translations produced by three systems on the Chinese–English
task. The source sentence is parsed into a dependency structure. Each source word is annotated by a
corresponding English word (or phrase).
Figure 7: An example of inducing a dependency structure in Figure 6 to ”X R`, (of) X” structure in our
system by using treelets and non-syntactic phrases. ➭ denotes one or more steps. All non-terminals are
simply represented by X.
</figureCaption>
<figure confidence="0.998740583333333">
伊拉克
Iraqi
临时
interim
管理
governing
委员会
council
关于
about
建立
establish
特别
special
法庭
court
、
,
审判
try
杀人犯
murderer
的
of
决定
decision
伊拉克 X 关于建立 X 的 决定
Iraqi about establish of decision
X 建立 X 的
establish of
决定
decision
X 的
of
决定
decision
</figure>
<page confidence="0.992799">
40
</page>
<bodyText confidence="0.9998444">
pared to Moses HPB, both Dep2Str and our sys-
tem, which rely on dependency structures, are ca-
pable of dealing with this. This also suggests that
dependency structures are useful for long-distance
reordering.
Furthermore, compared to Dep2Str, our system
produces a better translation for the ”X R`, (of)
X” expression, which is not explicitly represented
in the dependency structure and thus results in a
wrong translation in Dep2Str. After looking into
the details of the translation process, we find that
our system induces the dependency structure to the
”X R`, (of) X” structure by handling both treelets
and non-syntactic phrases. Figure 7 shows the pro-
cess of this induction.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9989676">
Dependency structures have been used in SMT for
a few years. Because of its better inter-lingual
phrasal cohesion properties (Fox, 2002), it is be-
lieved to be beneficial to translation.
Researchers have tried to use dependency struc-
tures on both target and source sides. Shen et
al. (2010) propose a string-to-dependency model
by using dependency fragments of neighbouring
words on the target side, which makes the model
easier to include a dependency-based language
model.
Menezes and Quirk (2005) and Quirk et al.
(2005) propose the treelet approach which uses de-
pendency structures on the source side. Xiong et
al. (2007) extend this approach by allowing gaps
in rules. However, their methods need a sepa-
rate reordering model to decide the position of
translated words (insertion problem). To avoid
this problem, Xie et al. (2011) propose to use
full head-dependent structures of a dependency
tree and build a new dependency-to-string model.
However, this model has difficulties in handling
non-syntactic phrasal rules and ignores treelets.
Meng et al. (2013) and Xie et al. (2014) further
augment this model by incorporating constituent
phrases and integrating fix/float structures (Shen
et al., 2010), respectively, to allow phrasal rules.
Li et al. (2014) extend this model by decomposing
head-dependent structures into treelets.
Different from these methods, by labelling
edges and using the ERG, our model considers the
three aspects in a unified way: treelet, reordering
and non-syntactic phrase. In addition, the ERG
also naturally provides a decision on what kind of
treelets and phrases should be used.
</bodyText>
<sectionHeader confidence="0.997483" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997556">
In this paper, we present a dependency graph-to-
string grammar based on a graph grammar, which
we call edge replacement grammar. This gram-
mar can simultaneously produce a pair of depen-
dency graph and string. With a restriction of us-
ing contiguous edges, our translation model built
using this grammar can decode an input depen-
dency graph, which is directly converted from a
dependency tree, in cubic time using the CYK al-
gorithm.
Experiments on Chinese–English and German–
English tasks show that our model is significantly
better than the hierarchical phrase-based model
and a recent dependency tree-to-string model
(Dep2Str) in Moses. We also find that the rules
used in our model are more suitable for long-
distance reordering and translating long sentences.
Although experiments show significant im-
provements over baselines, our model has limita-
tions that can be avenues for future work. The re-
striction used in this paper reduces the time com-
plexity but at the same time reduces the generative
capacity of graph grammars. Without allowing hy-
peredges or only using at most two external nodes
reduces the phrase coverage in our model as well.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999872">
This research has received funding from the Peo-
ple Programme (Marie Curie Actions) of the Euro-
pean Union’s Framework Programme (FP7/2007-
2013) under REA grant agreement no 317471. The
ADAPT Centre for Digital Content Technology
is funded under the SFI Research Centres Pro-
gramme (Grant 13/RC/2106) and is co-funded un-
der the European Regional Development Fund.
We thank anonymous reviewers for their insight-
ful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990039">
Bernd Bohnet. 2010. Very High Accuracy and Fast
Dependency Parsing is Not a Contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 89–97, Beijing,
China.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese Word Seg-
mentation for Machine Translation Performance. In
Proceedings of the Third Workshop on Statistical
</reference>
<page confidence="0.995556">
41
</page>
<reference confidence="0.998657767857143">
Machine Translation, pages 224–232, Columbus,
Ohio.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative Re-
ordering with Chinese Grammatical Relations Fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51–59, Boulder, Colorado.
Stanley F. Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. In Proceedings of the 34th Annual
Meeting on Association for Computational Linguis-
tics, ACL ’96, pages 310–318, Santa Cruz, Califor-
nia.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
924–932, Sofia, Bulgaria, August.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270, Ann
Arbor, Michigan.
David Chiang. 2012. Grammars for Language and
Genes: Theoretical and Empirical Investigations.
Springer.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, Courant Institute of Math-
ematical Sciences, New York University, New York,
NY.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, WMT ’11, pages 85–91, Ed-
inburgh, Scotland.
Frank Drewes, Hans J¨org Kreowski, and Annegret Ha-
bel. 1997. Handbook of graph grammars and com-
puting by graph transformation. chapter Hyper-
edge Replacement Graph Grammars, pages 95–162.
World Scientific Publishing Co., Inc., River Edge,
NJ, USA.
Heidi J. Fox. 2002. Phrasal Cohesion and Statis-
tical Machine Translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in Nat-
ural Language Processing - Volume 10, pages 304–
3111, Philadelphia.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with Translation
Recommendation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 622–630, Uppsala, Sweden, July.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the Ninth Workshop on Statistical
Machine Translation, pages 486–498, Baltimore,
Maryland, USA, June.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-Based Machine Translation with Hyper-
edge Replacement Grammars. In COLING 2012,
24th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, pages 1359–1376, Mumbai, India,
December.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-Analysis Algorithm for Context-Free Lan-
guages. Technical report, Air Force Cambridge Re-
search Lab, Bedford, MA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond‘ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, ACL ’07, pages 177–180, Prague, Czech Re-
public.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Head-Driven Hierarchical Phrase-
based Translation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 33–37,
Jeju Island, Korea, July.
Liangyou Li, Jun Xie, Andy Way, and Qun Liu. 2014.
Transformation and Decomposition for Efficiently
Implementing and Improving Dependency-to-String
Model In Moses. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, October.
Arul Menezes and Chris Quirk. 2005. Dependency
Treelet Translation: The Convergence of Statistical
and Example-Based Machine-translation? In Pro-
ceedings of the Workshop on Example-based Ma-
chine Translation at MT Summit X, September.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L‘,
and Qun Liu. 2013. Translation with Source Con-
stituency and Dependency Trees. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1066–1076, Seattle,
Washington, USA, October.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 99–106, Ann Arbor,
Michigan.
</reference>
<page confidence="0.981656">
42
</page>
<reference confidence="0.99984184375">
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302, Philadelphia, PA,
USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, 30(4):417–
449, December.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ’03, pages
160–167, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Philadelphia, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 271–279, Ann
Arbor, Michigan, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Transla-
tion. Computational Linguistics, 36(4):649–671,
December.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
ofAssociation for Machine Translation in the Amer-
icas, pages 223–231, Cambridge, Massachusetts,
USA, August.
Andreas Stolcke. 2002. SRILM “An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference Spoken Language Process-
ing, pages 901–904, Denver, CO.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-string Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216–226, Edinburgh, United Kingdom.
Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment
Dependency-to-String Translation with Fixed and
Floating Structures. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics, pages 2217–2226, Dublin, Ireland.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A De-
pendency Treelet String Correspondence Model for
Statistical Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 40–47, Prague, June.
Daniel H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n3. Information
and Control, 10(2):189–208.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651738">
<title confidence="0.999148">Dependency Graph-to-String Translation</title>
<author confidence="0.991113">Liangyou Li Andy Way Qun</author>
<affiliation confidence="0.717582">ADAPT Centre, School of Dublin City</affiliation>
<abstract confidence="0.999513">Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–English and German–English tasks show that our model is significantly better than the state-of-the-art hierarchical phrase-based (HPB) model and a recently improved dependency tree-to-string model on BLEU, METEOR and TER scores. Experiments also suggest that our model has better capability to perform long-distance reordering and is more suitable for translating long sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very High Accuracy and Fast Dependency Parsing is Not a Contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="15796" citStr="Bohnet, 2010" startWordPosition="2764" endWordPosition="2765">ford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very High Accuracy and Fast Dependency Parsing is Not a Contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 89–97, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese Word Segmentation for Machine Translation Performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="15231" citStr="Chang et al., 2008" startWordPosition="2670" endWordPosition="2673"> rules which have non-terminals. Finally, glue rules are used to make sure that at least one translation is produced. 5 Experiment We conduct experiments on Chinese–English and German–English translation tasks. 5.1 Datasets The Chinese–English training corpus is from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, the Hansards portion of LDC2004T08 and LDC2005T06. NIST 2002 is taken as a development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) are two test sets to evaluate systems. Table 1 provides a summary of this corpus. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese Word Segmentation for Machine Translation Performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Discriminative Reordering with Chinese Grammatical Relations Features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>51--59</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="15321" citStr="Chang et al., 2009" startWordPosition="2685" endWordPosition="2688">ne translation is produced. 5 Experiment We conduct experiments on Chinese–English and German–English translation tasks. 5.1 Datasets The Chinese–English training corpus is from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, the Hansards portion of LDC2004T08 and LDC2005T06. NIST 2002 is taken as a development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) are two test sets to evaluate systems. Table 1 provides a summary of this corpus. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this </context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009. Discriminative Reordering with Chinese Grammatical Relations Features. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages 51–59, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, California.</location>
<contexts>
<context position="16708" citStr="Chen and Goodman, 1996" startWordPosition="2915" endWordPosition="2918">periments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. 5.3 Results Table 2 shows the scores of all three metrics on all systems. Similar to Li et al. (2014), in our experiments Dep2Str has on average a comparable result with Moses HPB in terms of BLEU and METEOR scores. However, it obtains a significantly higher (i.e. worse) TER score on the Chinese–English ta</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96, pages 310–318, Santa Cruz, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Bevan Jones</author>
<author>Kevin Knight</author>
</authors>
<title>Parsing graphs with hyperedge replacement grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>924--932</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4096" citStr="Chiang et al., 2013" startWordPosition="630" endWordPosition="633">e rewriting grammar to recognize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: An example of a derivation in an ERG. Dark circles are external nodes. Shijiebei 1 Y 1 FIFA S Juxing Juxing X Chenggong 2010 2010 Y Zai Nanfei Chenggong Shijiebei FIFA Zai Nanfei Chenggong 2010 Juxing Y Zai Nanfei 1 X 1 paper is connected, nodes ordered, acyclic and has edge labels but no node labels (Chiang et al., 2013). We provide some formal definitions on ERG. Definition 1. A connected, edge-labeled, ordered graph is a tuple H = (V, E, 0), where • V is a finite set of nodes. • E C V 2 is a finite set of edges. • 0 : E —* C assigns a label (drawn from C) to each edge. In ERG, the elementary unit is a graph fragment, which is also the right-hand side of a production in the grammar. Its definition is as follows. Definition 2. A graph fragment is a tuple H = (V, E, 0, X), where (V, E, 0) is a graph and X E (V U V 2) is a list of distinct nodes. Following Chiang et al. (2013), we call these external nodes. The</context>
<context position="5588" citStr="Chiang et al., 2013" startWordPosition="945" endWordPosition="948">t grammar is a tuple (N, T, P, 5), where • N and T are disjoint finite sets of nonterminal symbols and terminal symbols, respectively. • P is a finite set of productions of the form A —* R, where A E N and R is a graph fragment, where edge-labels are from N U T. • 5 E N is the start symbol. Figure 1 shows an example of a derivation in an ERG to produce a graph. Starting from the start symbol 5, when a rule (A —* R) is applied to an edge e, the edge is replaced by the graph fragment R. Just like in HRG, the ordering of nodes Ve in e and external nodes XR in R implies the mapping from Ve to XR (Chiang et al., 2013). 3 Graph-to-String Grammar In SMT, we need a synchronous grammar to simultaneously parse an input graph and produce translations. The graph we use in this paper is from a dependency structure which is capable of modelling long-distance relations in a sentence. 3.1 The Grammar Before defining the synchronous grammar, we firstly define a dependency graph which is a special case of a graph. Definition 4. A dependency graph is a tuple (V, E, 0, A), where (V, E, 0) is a graph and A is a restriction: edges are ordered. A dependency graph is directly derived from a dependency tree by labeling edges </context>
</contexts>
<marker>Chiang, Andreas, Bauer, Hermann, Jones, Knight, 2013</marker>
<rawString>David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, and Kevin Knight. 2013. Parsing graphs with hyperedge replacement grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 924–932, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2248" citStr="Chiang, 2005" startWordPosition="333" endWordPosition="334">rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well. As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges. We then use a synchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We defin</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Grammars for Language and Genes: Theoretical and Empirical Investigations.</title>
<date>2012</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7844" citStr="Chiang, 2012" startWordPosition="1359" endWordPosition="1360"> string over N U T0. — is a one-toone mapping between non-terminal symbols in R and R0. Figure 3 shows a derivation simultaneously producing a Chinese dependency graph and an English string using a DGSG. Each time a rule is applied, the dependency-graph fragment in the rule replaces an edge in the source graph, and the string in the rule replaces a non-terminal in the target string. Proposition 1. DGSG has stronger generative capacity over graph-string pairs than both SCFG and synchronous tree substitution grammar (STSG). Proof. STSG has stronger generative capacity over structures than SCFG (Chiang, 2012).1 Any STSG can easily be converted into a DGSG by labelling edges in tree structures. 1The following STSG generates a trivial example of a tree-string pair that no SCFG can generate, as SCFG must always have an equal number of non-terminal symbols. X I :X I E The following DGSG generates a trivial example of a graph-string pair, which no STSG can generate, as the left-head side has no head nodes while STSG always requires one to form a tree. a b This proof is also verified in Figure 3 where the third rule is used to translate a non-syntactic phrase, which can be a problem for dependency tree-</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Grammars for Language and Genes: Theoretical and Empirical Investigations. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
<author>Jacob T Schwartz</author>
</authors>
<title>Programming Languages and Their Compilers: Preliminary Notes.</title>
<date>1970</date>
<tech>Technical report,</tech>
<institution>Courant Institute of Mathematical Sciences, New York University,</institution>
<location>New York, NY.</location>
<contexts>
<context position="13801" citStr="Cocke and Schwartz, 1970" startWordPosition="2437" endWordPosition="2440">i where Oi are features defined on derivations and Ai are feature weights. In our experiments, we use 9 features: • translation probabilities P(s|t) and P(t|s), where s is the source graph fragment and t is the target string. • lexical translation probabilities Plex(s|t) and Plex(t|s). • language model lm(e) over translation e. • rule penalty exp(−1). • word penalty exp(|e|). • glue penalty exp(−1). • unknown words penalty exp(u(g)), where u(g) is the number of unknown words in a source graph g. Our decoder is based on the conventional chart parsing CYK algorithm (Kasami, 1965; Younger, 1967; Cocke and Schwartz, 1970). It searches for the best derivation d* among all possible derivations D, as in Equation (2): d* = argmax P(d) (2) dED For each span of an input graph, the decoder checks if it is a dependency-graph fragment. Then 37 ZH–EN corpus #sent. #words(ZH) #words(EN) train 1.5M+ 38M+ ∼45M dev 878 22,655 26,905 MT04 1,597 43,719 52,705 MT05 1,082 29,880 35,326 DE–EN corpus #sent. #words(DE) #words(EN) train 2M+ 52M+ 55M+ dev 3,003 72,661 74,753 WMT12 3,003 72,603 72,988 WMT13 3,000 63,412 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the English side of dev and test s</context>
</contexts>
<marker>Cocke, Schwartz, 1970</marker>
<rawString>John Cocke and Jacob T. Schwartz. 1970. Programming Languages and Their Compilers: Preliminary Notes. Technical report, Courant Institute of Mathematical Sciences, New York University, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>85--91</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="3277" citStr="Denkowski and Lavie, 2011" startWordPosition="492" endWordPosition="495">014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. 2 Edge Replacement Grammar As a special case of HRG, ERG is also a contextfree rewriting grammar to recognize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: An example of a derivation in an ERG. Dark circles are external nodes. Shijiebei 1 Y 1 FIFA S Juxing Jux</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 85–91, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Drewes</author>
<author>Hans J¨org Kreowski</author>
<author>Annegret Habel</author>
</authors>
<title>Handbook of graph grammars and computing by graph transformation. chapter Hyperedge Replacement Graph Grammars,</title>
<date>1997</date>
<pages>95--162</pages>
<publisher>World Scientific Publishing Co., Inc.,</publisher>
<location>River Edge, NJ, USA.</location>
<contexts>
<context position="1498" citStr="Drewes et al., 1997" startWordPosition="211" endWordPosition="214">el has better capability to perform long-distance reordering and is more suitable for translating long sentences. 1 Introduction Compared to trees, which have dominated the field of natural language processing (NLP) for decades, graphs are more general for modelling natural languages. The corresponding grammars for recognizing and producing graphs are more flexible and powerful than tree grammars. However, because of their high complexity, graph grammars have not been widely used in NLP. Recently, along with progress on graph-based meaning representation, hyperedge replacement grammars (HRG) (Drewes et al., 1997) have been revisited, explored and used for semantic-based machine translation (Jones et al., 2012). However, the translation process is rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well. As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges. We then use a synchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. Th</context>
</contexts>
<marker>Drewes, Kreowski, Habel, 1997</marker>
<rawString>Frank Drewes, Hans J¨org Kreowski, and Annegret Habel. 1997. Handbook of graph grammars and computing by graph transformation. chapter Hyperedge Replacement Graph Grammars, pages 95–162. World Scientific Publishing Co., Inc., River Edge, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal Cohesion and Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,</booktitle>
<pages>304--3111</pages>
<location>Philadelphia.</location>
<contexts>
<context position="23807" citStr="Fox, 2002" startWordPosition="4119" endWordPosition="4120">ompared to Dep2Str, our system produces a better translation for the ”X R`, (of) X” expression, which is not explicitly represented in the dependency structure and thus results in a wrong translation in Dep2Str. After looking into the details of the translation process, we find that our system induces the dependency structure to the ”X R`, (of) X” structure by handling both treelets and non-syntactic phrases. Figure 7 shows the process of this induction. 6 Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to deci</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal Cohesion and Statistical Machine Translation. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, pages 304– 3111, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Bridging SMT and TM with Translation Recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>622--630</pages>
<location>Uppsala, Sweden,</location>
<marker>He, Ma, van Genabith, Way, 2010</marker>
<rawString>Yifan He, Yanjun Ma, Josef van Genabith, and Andy Way. 2010. Bridging SMT and TM with Translation Recommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622–630, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Augmenting String-to-Tree and Tree-toString Translation with Non-Syntactic Phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>486--498</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2636" citStr="Huck et al., 2014" startWordPosition="388" endWordPosition="391">grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 200</context>
</contexts>
<marker>Huck, Hoang, Koehn, 2014</marker>
<rawString>Matthias Huck, Hieu Hoang, and Philipp Koehn. 2014. Augmenting String-to-Tree and Tree-toString Translation with Non-Syntactic Phrases. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<title>Semantics-Based Machine Translation with Hyperedge Replacement Grammars.</title>
<date>2012</date>
<booktitle>In COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,</booktitle>
<pages>1359--1376</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1597" citStr="Jones et al., 2012" startWordPosition="225" endWordPosition="228">ng sentences. 1 Introduction Compared to trees, which have dominated the field of natural language processing (NLP) for decades, graphs are more general for modelling natural languages. The corresponding grammars for recognizing and producing graphs are more flexible and powerful than tree grammars. However, because of their high complexity, graph grammars have not been widely used in NLP. Recently, along with progress on graph-based meaning representation, hyperedge replacement grammars (HRG) (Drewes et al., 1997) have been revisited, explored and used for semantic-based machine translation (Jones et al., 2012). However, the translation process is rather complex and the resources it relies on, namely abstract meaning corpora, are limited as well. As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges. We then use a synchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the </context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-Based Machine Translation with Hyperedge Replacement Grammars. In COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, pages 1359–1376, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An Efficient Recognition and Syntax-Analysis Algorithm for Context-Free Languages. Technical report, Air Force Cambridge Research Lab,</title>
<date>1965</date>
<location>Bedford, MA.</location>
<contexts>
<context position="13759" citStr="Kasami, 1965" startWordPosition="2433" endWordPosition="2434">on (1): P(d) a � Oi(d)λi (1) i where Oi are features defined on derivations and Ai are feature weights. In our experiments, we use 9 features: • translation probabilities P(s|t) and P(t|s), where s is the source graph fragment and t is the target string. • lexical translation probabilities Plex(s|t) and Plex(t|s). • language model lm(e) over translation e. • rule penalty exp(−1). • word penalty exp(|e|). • glue penalty exp(−1). • unknown words penalty exp(u(g)), where u(g) is the number of unknown words in a source graph g. Our decoder is based on the conventional chart parsing CYK algorithm (Kasami, 1965; Younger, 1967; Cocke and Schwartz, 1970). It searches for the best derivation d* among all possible derivations D, as in Equation (2): d* = argmax P(d) (2) dED For each span of an input graph, the decoder checks if it is a dependency-graph fragment. Then 37 ZH–EN corpus #sent. #words(ZH) #words(EN) train 1.5M+ 38M+ ∼45M dev 878 22,655 26,905 MT04 1,597 43,719 52,705 MT05 1,082 29,880 35,326 DE–EN corpus #sent. #words(DE) #words(EN) train 2M+ 52M+ 55M+ dev 3,003 72,661 74,753 WMT12 3,003 72,603 72,988 WMT13 3,000 63,412 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpo</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Tadao Kasami. 1965. An Efficient Recognition and Syntax-Analysis Algorithm for Context-Free Languages. Technical report, Air Force Cambridge Research Lab, Bedford, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ond‘ej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="16002" citStr="Koehn et al., 2007" startWordPosition="2797" endWordPosition="2800">hich is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portio</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond‘ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Zhaopeng Tu</author>
<author>Guodong Zhou</author>
<author>Josef van Genabith</author>
</authors>
<title>Head-Driven Hierarchical Phrasebased Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>33--37</pages>
<location>Jeju Island, Korea,</location>
<marker>Li, Tu, Zhou, van Genabith, 2012</marker>
<rawString>Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van Genabith. 2012. Head-Driven Hierarchical Phrasebased Translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 33–37, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangyou Li</author>
<author>Jun Xie</author>
<author>Andy Way</author>
<author>Qun Liu</author>
</authors>
<title>Transformation and Decomposition for Efficiently Implementing and Improving Dependency-to-String Model In Moses.</title>
<date>2014</date>
<booktitle>In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<contexts>
<context position="16217" citStr="Li et al., 2014" startWordPosition="2833" endWordPosition="2836">012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in</context>
<context position="24932" citStr="Li et al. (2014)" startWordPosition="4293" endWordPosition="4296"> by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to allow phrasal rules. Li et al. (2014) extend this model by decomposing head-dependent structures into treelets. Different from these methods, by labelling edges and using the ERG, our model considers the three aspects in a unified way: treelet, reordering and non-syntactic phrase. In addition, the ERG also naturally provides a decision on what kind of treelets and phrases should be used. 7 Conclusion In this paper, we present a dependency graph-tostring grammar based on a graph grammar, which we call edge replacement grammar. This grammar can simultaneously produce a pair of dependency graph and string. With a restriction of usin</context>
</contexts>
<marker>Li, Xie, Way, Liu, 2014</marker>
<rawString>Liangyou Li, Jun Xie, Andy Way, and Qun Liu. 2014. Transformation and Decomposition for Efficiently Implementing and Improving Dependency-to-String Model In Moses. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Chris Quirk</author>
</authors>
<title>Dependency Treelet Translation: The Convergence of Statistical and Example-Based Machine-translation?</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Example-based Machine Translation at MT Summit X,</booktitle>
<contexts>
<context position="24169" citStr="Menezes and Quirk (2005)" startWordPosition="4174" endWordPosition="4177">” structure by handling both treelets and non-syntactic phrases. Figure 7 shows the process of this induction. 6 Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augme</context>
</contexts>
<marker>Menezes, Quirk, 2005</marker>
<rawString>Arul Menezes and Chris Quirk. 2005. Dependency Treelet Translation: The Convergence of Statistical and Example-Based Machine-translation? In Proceedings of the Workshop on Example-based Machine Translation at MT Summit X, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fandong Meng</author>
<author>Jun Xie</author>
<author>Linfeng Song</author>
<author>Yajuan L‘</author>
<author>Qun Liu</author>
</authors>
<title>Translation with Source Constituency and Dependency Trees.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1066--1076</pages>
<location>Seattle, Washington, USA,</location>
<marker>Meng, Xie, Song, L‘, Liu, 2013</marker>
<rawString>Fandong Meng, Jun Xie, Linfeng Song, Yajuan L‘, and Qun Liu. 2013. Translation with Source Constituency and Dependency Trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066–1076, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>PseudoProjective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>99--106</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="15898" citStr="Nivre and Nilsson, 2005" startWordPosition="2778" endWordPosition="2781">Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. PseudoProjective Dependency Parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 99–106, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2931" citStr="Och and Ney, 2002" startWordPosition="439" endWordPosition="442">tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. 2 Edge Replacement Grammar As a special case of HRG, ERG is also a contextfree rewriting grammar to recognize and produce graphs. Fo</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16470" citStr="Och and Ney, 2003" startWordPosition="2878" endWordPosition="2881">tive dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. 5.3 Results Table 2 shows the scores of all three metrics on all syst</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>449</pages>
<contexts>
<context position="12119" citStr="Och and Ney, 2004" startWordPosition="2117" endWordPosition="2120">making the grammar much smaller, it also results in a similar way of extracting rules as in HPB. Inspired by HPB, we define the rule set over initial pairs. Given a word-aligned dependency graph-string pair P = (G, e, ∼), let Gji stand for the sub-graph (it may not be connected) covering words from position i to position j. Then a rule (Gji, ej0 i0 ) is an initial pair of P, iff: 1. Gji is a dependency-graph fragment. That means it is a connected sub-graph and has at most two external nodes, nodes which connect with nodes outside or are the root. 2. It is consistent with the word alignment ∼ (Och and Ney, 2004). The set of rules from P satisfies the following: 1. If (Gji , ej0 i0 ) is an initial pair, then (N(Gji) → Gji , X → ej0 i0 ) is a rule, where N(G) defines the nonterminal symbol for G. 2. If (N(R) → R, X → R&apos;) is a rule of P and (Gji , ej0 i0 ) is an initial pair such that Gji is a sub-graph of R and R&apos; = r1ej0 i0 r2, then (N(R) → R\Gji k,X → r1Xkr2) is a rule of P, where \ means replacing Gji in R with an edge labelled with N(Gji) and k is a unique index for a pair of non-terminal symbols. As in HPB, in addition to rules extracted from the parallel corpus, we also use glue rules to combine </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417– 449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16756" citStr="Och, 2003" startWordPosition="2924" endWordPosition="2925">pen-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. 5.3 Results Table 2 shows the scores of all three metrics on all systems. Similar to Li et al. (2014), in our experiments Dep2Str has on average a comparable result with Moses HPB in terms of BLEU and METEOR scores. However, it obtains a significantly higher (i.e. worse) TER score on the Chinese–English task. This may suggest that translations produced </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160–167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="3211" citStr="Papineni et al., 2002" startWordPosition="481" endWordPosition="484"> handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. 2 Edge Replacement Grammar As a special case of HRG, ERG is also a contextfree rewriting grammar to recognize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: An example of a derivation in an ERG. </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2729" citStr="Quirk et al., 2005" startWordPosition="404" endWordPosition="407">translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more </context>
<context position="24193" citStr="Quirk et al. (2005)" startWordPosition="4179" endWordPosition="4182">treelets and non-syntactic phrases. Figure 7 shows the process of this induction. 6 Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorpo</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 271–279, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<date>2010</date>
<journal>String-to-Dependency Statistical Machine Translation. Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="23960" citStr="Shen et al. (2010)" startWordPosition="4144" endWordPosition="4147">ency structure and thus results in a wrong translation in Dep2Str. After looking into the details of the translation process, we find that our system induces the dependency structure to the ”X R`, (of) X” structure by handling both treelets and non-syntactic phrases. Figure 7 shows the process of this induction. 6 Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a depe</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-Dependency Statistical Machine Translation. Computational Linguistics, 36(4):649–671, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="3238" citStr="Snover et al., 2006" startWordPosition="486" endWordPosition="489">(Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. 2 Edge Replacement Grammar As a special case of HRG, ERG is also a contextfree rewriting grammar to recognize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: An example of a derivation in an ERG. Dark circles are external n</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings ofAssociation for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM “An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="16548" citStr="Stolcke, 2002" startWordPosition="2891" endWordPosition="2892">nly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. 5.3 Results Table 2 shows the scores of all three metrics on all systems. Similar to Li et al. (2014), in our experiments Dep2Str has on average a </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM “An Extensible Language Modeling Toolkit. In Proceedings of the International Conference Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>A Novel Dependency-to-string Model for Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--226</pages>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="2760" citStr="Xie et al., 2011" startWordPosition="410" endWordPosition="413">der of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reor</context>
<context position="16308" citStr="Xie et al. (2011)" startWordPosition="2850" endWordPosition="2853">is corpus. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.2 Settings In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calcu</context>
<context position="24504" citStr="Xie et al. (2011)" startWordPosition="4230" endWordPosition="4233">dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to allow phrasal rules. Li et al. (2014) extend this model by decomposing head-dependent structures into treelets. Different from these methods, by labelling edges and using the ERG, our model considers the three</context>
</contexts>
<marker>Xie, Mi, Liu, 2011</marker>
<rawString>Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel Dependency-to-string Model for Statistical Machine Translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 216–226, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Jinan Xu</author>
<author>Qun Liu</author>
</authors>
<title>Augment Dependency-to-String Translation with Fixed and Floating Structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<pages>2217--2226</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2655" citStr="Xie et al., 2014" startWordPosition="392" endWordPosition="395">ion 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denk</context>
<context position="24755" citStr="Xie et al. (2014)" startWordPosition="4268" endWordPosition="4271">model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to allow phrasal rules. Li et al. (2014) extend this model by decomposing head-dependent structures into treelets. Different from these methods, by labelling edges and using the ERG, our model considers the three aspects in a unified way: treelet, reordering and non-syntactic phrase. In addition, the ERG also naturally provides a decision on what kind of treelets and phrases should be used. 7 Conclusion In this paper, we present a dependency graph-tostring gr</context>
</contexts>
<marker>Xie, Xu, Liu, 2014</marker>
<rawString>Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment Dependency-to-String Translation with Fixed and Floating Structures. In Proceedings of the 25th International Conference on Computational Linguistics, pages 2217–2226, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A Dependency Treelet String Correspondence Model for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>40--47</pages>
<location>Prague,</location>
<contexts>
<context position="24295" citStr="Xiong et al. (2007)" startWordPosition="4196" endWordPosition="4199">dency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to </context>
</contexts>
<marker>Xiong, Liu, Lin, 2007</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A Dependency Treelet String Correspondence Model for Statistical Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 40–47, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and Parsing of Context-Free Languages</title>
<date>1967</date>
<booktitle>in Time n3. Information and Control,</booktitle>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="13774" citStr="Younger, 1967" startWordPosition="2435" endWordPosition="2436"> � Oi(d)λi (1) i where Oi are features defined on derivations and Ai are feature weights. In our experiments, we use 9 features: • translation probabilities P(s|t) and P(t|s), where s is the source graph fragment and t is the target string. • lexical translation probabilities Plex(s|t) and Plex(t|s). • language model lm(e) over translation e. • rule penalty exp(−1). • word penalty exp(|e|). • glue penalty exp(−1). • unknown words penalty exp(u(g)), where u(g) is the number of unknown words in a source graph g. Our decoder is based on the conventional chart parsing CYK algorithm (Kasami, 1965; Younger, 1967; Cocke and Schwartz, 1970). It searches for the best derivation d* among all possible derivations D, as in Equation (2): d* = argmax P(d) (2) dED For each span of an input graph, the decoder checks if it is a dependency-graph fragment. Then 37 ZH–EN corpus #sent. #words(ZH) #words(EN) train 1.5M+ 38M+ ∼45M dev 878 22,655 26,905 MT04 1,597 43,719 52,705 MT05 1,082 29,880 35,326 DE–EN corpus #sent. #words(DE) #words(EN) train 2M+ 52M+ 55M+ dev 3,003 72,661 74,753 WMT12 3,003 72,603 72,988 WMT13 3,000 63,412 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the Eng</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H. Younger. 1967. Recognition and Parsing of Context-Free Languages in Time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000793">
<title confidence="0.99585">
The Forest Convolutional Network: Compositional Distributional
Semantics with a Neural Chart and without Binarization
</title>
<author confidence="0.997422">
Phong Le and Willem Zuidema
</author>
<affiliation confidence="0.998026">
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
</affiliation>
<email confidence="0.996207">
{p.le,zuidema}@uva.nl
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935423076923">
According to the principle of composi-
tionality, the meaning of a sentence is
computed from the meaning of its parts
and the way they are syntactically com-
bined. In practice, however, the syntactic
structure is computed by automatic parsers
which are far-from-perfect and not tuned
to the specifics of the task. Current re-
cursive neural network (RNN) approaches
for computing sentence meaning therefore
run into a number of practical difficulties,
including the need to carefully select a
parser appropriate for the task, deciding
how and to what extent syntactic context
modifies the semantic composition func-
tion, as well as on how to transform parse
trees to conform to the branching settings
(typically, binary branching) of the RNN.
This paper introduces a new model, the
Forest Convolutional Network, that avoids
all of these challenges, by taking a parse
forest as input, rather than a single tree,
and by allowing arbitrary branching fac-
tors. We report improvements over the
state-of-the-art in sentiment analysis and
question classification.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990615384615">
For many natural language processing tasks we
need to compute meaning representations for sen-
tences from meaning representations of words. In
a recent line of research on ‘recursive neural net-
works’ (e.g., Socher et al. (2010)), both the word
and sentence representations are vectors, and the
word vectors (“embeddings”) are borrowed from
work in distributional semantics or neural lan-
guage modelling. Sentence representations, in this
approach, are computed by recursively applying a
neural network that combines two vectors into one
(typically according to the syntactic structure pro-
vided by an external parser). The network, which
thus implements a ‘composition function’, is opti-
mized for delivering sentence representations that
support a given semantic task: sentiment analysis
(Irsoy and Cardie, 2014; Le and Zuidema, 2015),
paraphrase detection (Socher et al., 2011), seman-
tic relatedness (Tai et al., 2015) etc. Studies with
recursive neural networks have yielded promising
results on a variety of such tasks.
In this paper, we represent a new recursive neu-
ral network architecture that fits squarely in this
tradition, but aims to solve a number of difficulties
that have arisen in existing work. In particular, the
model we propose addresses three issues:
</bodyText>
<listItem confidence="0.923971222222222">
1. how to make the composition functions adap-
tive, in the sense that they operate adequately
for the many different types of combina-
tions (e.g., adjective-noun combinations are
of a very different type than VP-PP combina-
tions);
2. how to deal with different branching factors
of nodes in the relevant syntactic trees (i.e.,
we want to avoid having to binarize syntac-
tic trees,1 but also do not want ternary pro-
ductions to be completely independent from
binary productions);
3. how to deal with uncertainty about the correct
parse inside the neural architecture (i.e., we
don’t want to work with just the best or k-best
parses for a sentence according to an external
model, but receive an entire distribution over
possible parsers).
</listItem>
<footnote confidence="0.982087285714286">
1Eisner (2001, Chapter 2) shows that using flat rules is
linguistically beneficial, “most crucially, a flat lexical entry
corresponds to the local domain of a headword-the word to-
gether with all its semantic arguments and modifiers”. From
the computational perspective, flat rules make trees less deep,
thus avoiding the vanishing gradient problem and capturing
long range dependencies.
</footnote>
<page confidence="0.880016">
1155
</page>
<note confidence="0.9959595">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1155–1164,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.985598">
Figure 1: Recursive Neural Network. For simplic-
ity, bias vectors are removed.
</figureCaption>
<bodyText confidence="0.99993275">
To solve these challenges we take inspiration
from two other traditions: the convolutional neu-
ral networks and classic parsing algorithms based
on dynamic programming. Including convolution
in our network provides a direct solution for is-
sue (2), and turns out, somewhat unexpectedly, to
also provide a solution for issue (1). Introduc-
ing the chart representation from classic parsing
into our architecture then allows us to tackle issue
(3). The resulting model, the Forest Convolutional
Network, outperforms all other models on a senti-
ment analysis and question classification task.
</bodyText>
<sectionHeader confidence="0.995263" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999306666666667">
This section is to introduce the recursive neural
network (RNN) and convolutional neural network
(CNN) models, on which our work is based.
</bodyText>
<subsectionHeader confidence="0.955572">
2.1 Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.9997354">
A recursive neural network (RNN) (Goller and
K¨uchler, 1996) is a feed-forward neural network
where, given a tree structure, we recursively ap-
ply the same weight matrices at each inner node
in a bottom-up manner. In order to see how
an RNN works, consider the following exam-
ple. Assume that there is a constituent with parse
tree (S I (V P like it)) (Figure 1), and that
xI, xlike, xit E Rd are the vectorial representa-
tions of the three words I, like and it, respec-
tively. We use a neural network which consists of
a weight matrix W1 E Rdxd for left children and
a weight matrix W2 E Rdxd for right children to
compute the vector for a parent node in a bottom
up manner. Thus, we compute xV P
</bodyText>
<equation confidence="0.812585">
xV P = f(W1xlike + W2xit + b) (1)
</equation>
<bodyText confidence="0.9942225">
where b is a bias vector and f is an (non-linear)
activation function. Having computed xV P, we
</bodyText>
<figureCaption confidence="0.93700925">
Figure 2: Convolutional Neural Network (one
convolutional layer and one fully connected layer)
with a window-size-3 kernel.
can then move one level up in the hierarchy and
</figureCaption>
<equation confidence="0.8664575">
compute xS
xS = f(W1xI + W2xV P + b)
</equation>
<bodyText confidence="0.9878752">
This process is continued until we reach the root
node.
For classification tasks, we put a softmax layer
on the top of the root node, and compute the prob-
ability of assigning a class c to an input x by
</bodyText>
<equation confidence="0.997680333333333">
eu(c,ytop)
Pr(c|x) = softmax(c) = u(c,,ytop) (2)
Ec,cC e
</equation>
<bodyText confidence="0.996203">
where [u(c1, ytop), ..., u(c|C|, ytop) IT =
Wuytop + bu; C is the set of all possible
classes; Wu E R|C|xd,bu E R|C |are a weight
matrix and a bias vector.
Training an RNN uses the gradient descent
method to minimize an objective function J(O).
The gradient aJ/aO is efficiently computed thanks
to the back-propagation through structure algo-
rithm (Goller and K¨uchler, 1996).
Departing from the original RNN model, many
extensions have been proposed to enhance its
compositionality (Socher et al., 2013; Irsoy and
Cardie, 2014; Le and Zuidema, 2015) and appli-
cability (Le and Zuidema, 2014b). The model we
are going to propose can be considered as an ex-
tension of RNN with an ability to solve the three
issues introduced in Section 1.
</bodyText>
<subsectionHeader confidence="0.999492">
2.2 Convolutional Neural Network
</subsectionHeader>
<bodyText confidence="0.9998372">
A convolutional neural network (CNN) (LeCun et
al., 1998) is also a feed-forward neural network; it
consists of one or more convolutional layers (of-
ten with a pooling operation) followed by one or
more fully connected layers. This architecture was
</bodyText>
<page confidence="0.988599">
1156
</page>
<bodyText confidence="0.998329166666667">
invented for computer vision. It then has been
widely applied to solve natural language process-
ing tasks (Collobert et al., 2011; Kalchbrenner et
al., 2014; Kim, 2014).
To illustrate how a CNN works, the following
example uses a simplified model proposed by Col-
lobert et al. (2011) which consists of one con-
volutional layer with the max pooling operation,
followed by one fully connected layer (Figure 2).
This CNN uses a kernel with window size 3; when
we slide this kernel along the sentence “(s) I like
it very much (/s)”, we get five vectors:
</bodyText>
<equation confidence="0.999241">
u(1) = W1x&lt;s&gt; + W2xI + W3xlike + bc
u(2) = W1xI + W2xlike + W3xit + bc
...
u(5) = W1xvery + W2xmuch + W3x&lt;/s&gt; + bc
</equation>
<bodyText confidence="0.99993675">
where W1, W2, W3 E Rdxm are weight matri-
ces, bc E Rm is a bias vector. The max pooling
operation is then applied to those resulted vectors
in an element-wise manner:
</bodyText>
<equation confidence="0.9457345">
x = max uii), ..., max
1≤i≤5 1≤i≤5 u j
Finally, a fully connected layer is employed
y = f(Wx + b)
</equation>
<bodyText confidence="0.999831666666667">
where W, b are a real weight matrix and bias vec-
tor, respectively; f is an activation function.
Intuitively, a window-size-k kernel extracts (lo-
cal) features from k-grams, and is thus able to cap-
ture k-gram composition. The max pooling oper-
ation is for reducing dimension, forcing the net-
work to discriminate important features from oth-
ers by assigning high values to them. For instance,
if the network is used for sentiment analysis, local
features corresponding to k-grams containing the
word “like” should receive high values in order to
be propagated to the top layer.
</bodyText>
<sectionHeader confidence="0.965657" genericHeader="method">
3 Forest Convolutional Network
</sectionHeader>
<bodyText confidence="0.999680333333333">
We now first propose a solution to the issues (1)
and (2) (i.e., making the composition functions
adaptive and dealing with different branching fac-
tors), called Recursive convolutional neural net-
work (RCNN), and then a solution to the third is-
sue (i.e., dealing with uncertainty about the cor-
rect parse), called Chart Neural Network (ChNN).
A combination of them, Forest Convolutional Net-
work (FCN), will be introduced lastly.
</bodyText>
<figureCaption confidence="0.9947585">
Figure 3: Recursive Convolutional Neural Net-
work with a nonlinear window-size-3 kernel.
</figureCaption>
<subsectionHeader confidence="0.734623">
3.1 Recursive Convolutional Neural
Network2
</subsectionHeader>
<bodyText confidence="0.999991772727273">
Given a subtree p —* x1 ... xl, an RCNN (Fig-
ure 3), like a CNN, slides a window-size-k kernel
along the sequence of children (x1,..., xl) to com-
pute a pool of vectors. The max pooling operation
followed by a fully connected layer is then applied
to this pool to compute a vector for the parent p.
This RCNN differs from the CNN introduced
in Section 2.2 at two points. First, we use a
non-linear kernel: after linearly transforming in-
put vectors, an activation function is applied. Sec-
ond, we put k − 1 padding tokens &lt;b&gt; at the be-
ginning of the children sequence and k−1 padding
tokens &lt;e&gt; at the end. This thus guarantees that
all the children contribute equally to the resulted
vector pool, which now has l + k − 1 vectors.
It is obvious that this RCNN can solve the sec-
ond issue (i.e., dealing with different branching
factors), we now show how it can make the com-
position functions adaptive. We first see what hap-
pens if the window size k is larger than the number
of children l, for instance k = 3 and l = 2. There
are four vectors in the pool
</bodyText>
<equation confidence="0.9999345">
u(1) = f(W1x&lt;b&gt; + W2x&lt;b&gt; + W3x1 + bc)
u(2) = f(W1x&lt;b&gt; + W2x1 + W3x2 + bc)
u(3) = f(W1x1 + W2x2 + W3x&lt;e&gt; + bc)
u(4) = f(W1x2 + W2x&lt;e&gt; + W3x&lt;e&gt; + bc)
</equation>
<bodyText confidence="0.994162">
where W1, W2, W3 are weight matrices, bc is a
</bodyText>
<footnote confidence="0.8242124">
2While finalizing the current paper we discovered a pa-
per by Zhu et al. (2015) proposing a similar model which is
evaluated on syntactic parsing. Our work goes substantially
beyond theirs, however, as it takes a parse forest rather than a
single tree as input.
</footnote>
<page confidence="0.992629">
1157
</page>
<figureCaption confidence="0.999733">
Figure 4: Chart Neural Network.
</figureCaption>
<bodyText confidence="0.945668413793103">
bias vector, f is an activation function. These four
resulted vectors correspond to four ways of com-
posing the two children:
(1) the first child stands alone (e.g., when the in-
formation of the second child is not impor-
tant, it is better to ignore it),
(2,3) the two children are composed with two dif-
ferent weight matrix sets,
(4) the second child stands alone.
Now, imagine that we must handle binary syn-
tactic rules with different head positions such as
S → NP V P (e.g. “Jones runs”) where the
second child is the head and V P → VBD NP
(e.g., “ate spaghetti”) where the first child is the
head. We can set those weight matrices such that
when multiplying W2 by the vector of a head, we
have a vector with high-value entries. And when
multiplying W2 by the vector of a non-head, or
when multiplying W1 or W3 by a vector, the re-
sulted vector has low-value entries. This is possi-
ble thanks to the max pooling operation and that
heads are often more informative than non-heads.
If the window size k is smaller than the number
of children l, the argument above is still valid in
some cases such as head position. However, there
is no longer a direct interaction between any two
children whose distance is larger than k.3 In prac-
tice, this problem is not serious because rules with
a large number of children are very rare.
</bodyText>
<subsectionHeader confidence="0.999595">
3.2 Chart Neural Network
</subsectionHeader>
<bodyText confidence="0.999815333333333">
Unseen sentences are always parsed by an auto-
matic parser, which is far from perfect and task-
independent. Therefore, a good solution is to give
</bodyText>
<footnote confidence="0.464699">
3An indirect interaction can be set up through pooling.
</footnote>
<bodyText confidence="0.99918745">
the system a set of parses and let it decide which
parse is the best or to combine some of them. The
RNN model handles one extreme where this set
contains only one parse. We now consider the
other extreme where the set contains all possible
parses. Because the number of all possible bi-
nary parse trees of a length-n sentence is the n-
th Catalan number, processing individual parses is
not practical. We thus propose a new model work-
ing on charts in the CKY style (Younger, 1967),
called Chart Neural Network (ChNN).
We describe this model by the following exam-
ple. Given a phrase “ate pho with Milos”, a ChNN
will process its parse chart as in Figure 4. Be-
cause any 2-word constituent has only one parse,
the computation for p1, p2, p3 is identical to Equa-
tion 1. For 3-word constituent p4, because there
are two possible productions p4 → ate p2 and
p4 → p1 with, we compute one vector for each
production
</bodyText>
<equation confidence="0.971173">
U(1) = f(W1xate + W2P2 + b)
</equation>
<subsectionHeader confidence="0.999303">
3.3 Forest Convolutional Network
</subsectionHeader>
<bodyText confidence="0.9998648">
We now introduce the Forest Convolutional Net-
work (FCN) model, which is a combination of the
RCNN and the ChNN. The idea is to use an au-
tomatic parser to prune a chart5, debinarize pro-
ductions (if applicable), and then apply a ChNN
</bodyText>
<footnote confidence="0.762787714285714">
4In each cell, we apply the matrix-vector multiplication
two times and (if the cell is not a leaf) apply the max pooling
to a pool of maximally n d-D vectors.
5Pruning a chart by an automatic parser is also not per-
fect. However, the quality of a pruned chart can get very
close to human annotation. For instance, the chart pruner
proposed by Huang (2008) has a forest oracle of 97.8% F-
</footnote>
<equation confidence="0.9933545">
(3)
U(2) = f(W1P1 + W2xwith + b)
</equation>
<bodyText confidence="0.999921875">
and then apply the max pooling operation to these
two vectors to compute P4. We do the same to
compute P5. Finally, at the top, there are three
productions p6 → ate p5, p6 → p1 p3 and
p6 → p4 Milos. Similarly, we compute one vec-
tor for each production and employ the max pool-
ing operation to compute P6.
Because this ChNN processes a chart like the
CKY algorithm, its time complexity is O(n2d2 +
n3d) where n and d are the sentence length and
the dimension of vectors, respectively.4 A ChNN
is thus notably more complex than an RNN, whose
complexity is O(nd2). Like chart parsing, the
complexity can be reduced significantly by prun-
ing the chart before applying the ChNN. This will
be discussed right below.
</bodyText>
<page confidence="0.991089">
1158
</page>
<figureCaption confidence="0.985766">
Figure 5: Forest of parses (left) and Forest Convolutional Network (right). ⊗ denotes a convolutional
</figureCaption>
<bodyText confidence="0.98389437037037">
layer followed by the max pooling operation and a fully connected layer as in Figure 3.
where the computation in Equation 3 is replaced
by a convolutional layer followed by the max pool-
ing operation and a fully connected layer as in the
RCNN.
Figure 5 shows an illustration how the
FCN works on the phrase “ate pho with
Milos”. A forest of parses, given by
an external parser, comprises two parses
(V P ate pho (PP with Milos)) (solid lines)
and (V P ate (NP pho (PP with Milos)))
(dash-dotted lines). The first parse is the preferred
reading if Milos is a person, but the second one
is a possible reading (for instance, if Milos is the
name of a sauce). Instead of forcing the external
parser to decide which one is correct, we let
the FCN network do that because it has more
information about the context and domain, which
are embedded in training data. What the network
should do is depicted in Figure 5-right.
Training Training an FCN is similar to train-
ing an RNN. We use the mini-batch gradient de-
scent method to minimize an objective function J,
which depends on which task this network is ap-
plied to. For instance, if the task is sentiment anal-
ysis, J is the cross-entropy over the training sen-
tence set D plus an L2-norm regularization term
</bodyText>
<equation confidence="0.871946">
A
log Pr(cp|p) + 2||0||2
</equation>
<bodyText confidence="0.999204846153846">
where 0 is the parameter set, cp is the sentiment
class of phrase p, p is the vector representation
at the node covering p, Pr(cp|p) is computed by
score on section 23 of the Penn Treebank whereas resulted
forests are very compact: the average number of hyperedges
per forest is 123.1.
the softmax function, and A is the regularization
parameter.
The gradient ∂J/∂0 is computed efficiently
thanks to the back-propagation through structure
(Goller and K¨uchler, 1996). We use the AdaGrad
method (Duchi et al., 2011) to automatically up-
date the learning rate for each parameter.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997408125">
We evaluate the FCN model with two tasks: ques-
tion classification and sentiment analysis. The
evaluation metric is the classification accuracy.
Our networks were initialized with the 300-D
GloVe word embeddings trained on a corpus of
840B words6 (Pennington et al., 2014). The ini-
tial values for a weight matrix were uniformly
sampled from the symmetric interval I− 1 1 I
√n,√n
where n is the number of total input units. In each
experiment, a development set was used to tune
the model. We run the model ten times and chose
the run with the highest performance on the devel-
opment set. We employed early stopping: training
is halted if performance on the development set
does not improve after three consecutive epochs.
</bodyText>
<subsectionHeader confidence="0.998852">
4.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999500375">
The Stanford Sentiment Treebank (SST)7 (Socher
et al., 2013) which consists of 5-way fine-grained
sentiment labels (very negative, negative, neu-
tral, positive, very positive) for 215,154 phrases
of 11,855 sentences. We used the standard split-
ting: 8544 sentences for training, 1101 for devel-
opment, and 2210 for testing. The average sen-
tence length is 19.1. In addition, the treebank
</bodyText>
<footnote confidence="0.9997955">
6http://nlp.stanford.edu/projects/GloVe/
7http://nlp.stanford.edu/sentiment/treebank.html
</footnote>
<table confidence="0.995676923076923">
1 J(0) = −|D |�
sED pEs
1159
Model Fine-grained Binary
RNTN 45.7 85.4
CNN 48.0 88.1
DCNN 48.5 86.8
PV 48.7 87.8
DRNN 49.8 86.6
LSTM-RNN 49.9 88.0
CT-LSTM 51.0 88.0
FCN (dep.) 50.4 88.2
FCN (const.) 51.0 89.1
</table>
<tableCaption confidence="0.997893">
Table 1: Accuracies at sentence level on the SST
</tableCaption>
<bodyText confidence="0.983830918918919">
dataset. FCN (dep.) and FCN (const.) denote the
FCN with dependency forests and with constituent
forests, respectively. The accuracies of RNTN,
CNN, DCNN, PV, DRNN, LSTM-RNN and CT-
LSTM are copied from the corresponding papers
(see text).
also supports binary sentiment (positive, negative)
classification by removing neutral labels, leading
to: 6920 sentences for training, 872 for develop-
ment, and 1821 for testing.
All sentences were parsed by Liang Huang’s de-
pendency parser8 (Huang and Sagae, 2010). We
used this parser because it generates parse forests
and that dependency trees are less deep than con-
stituent trees. In addition, because the SST was
annotated in a constituency manner, we also em-
ployed the Charniak’s constituent parser (Char-
niak and Johnson, 2005) with Huang (2008)’s for-
est pruner. We found that the beam width 16
for the dependency parser and the log probability
beam 10 for the other worked best. Lower values
harmed the system’s performance and higher val-
ues were not beneficial.
Our FCN has the dimension of vectors at inner
nodes 200, a window size for the convolutional
kernel of 7, and the activation function tanh. It
was trained with the learning rate 0.01, the regu-
larization parameter 10−4, and the mini batch size
5. To reduce the average depth of the network, the
fully connected layer following the convolutional
layer was removed (i.e., p = x, see Figure 3).
We compare the FCN against other models:
the Recursive neural tensor network (RNTN)
(Socher et al., 2013), the Convolutional neural net-
work (CNN) (Kim, 2014), the Dynamic convolu-
tional neural network (DCNN) (Kalchbrenner et
al., 2014), the Paragraph vectors (PV) (Le and
</bodyText>
<footnote confidence="0.762209">
8http://acl.cs.qc.edu/∼lhuang/software
</footnote>
<bodyText confidence="0.998907714285714">
Mikolov, 2014), the Deep recursive neural net-
work (DRNN) (Irsoy and Cardie, 2014), the Re-
cursive neural network with Long short term mem-
ory (LSTM-RNN) (Le and Zuidema, 2015) and
the Constituent Tree LSTM (CT-LSTM) (Tai et
al., 2015).9
Table 1 shows the results. Our FCN using con-
stituent forests achieved the highest accuracies in
both fine-grained task and binary task, 51% and
89.1%. Comparing to CT-LSTM, although there
is no difference in the fine-grained task, the dif-
ference in the binary task is significant (1.1%).
Comparing to LSTM-RNN, the differences in both
tasks are all remarkable (1.1% and 1.1%).
Constituent parsing is clearly more helpful than
dependency parsing: the improvements that the
FCN got are 0.6% in the fine-grained task and
0.9% in the binary task. We conjecture that, be-
cause sentences in the treebank were parsed by
a constituent parser (here is the Stanford parser),
training with constituent forests is easier.
</bodyText>
<subsectionHeader confidence="0.995445">
4.2 Question Classification
</subsectionHeader>
<bodyText confidence="0.999972125">
In this task we used the TREC question dataset10
(Li and Roth, 2002) which contains 5952 ques-
tions (5452 questions for training and 500 ques-
tions for testing). The task is to assign a ques-
tion to one in six types: ABBREVIATION, EN-
TITY, DESCRIPTION, HUMAN, LOCATION,
NUMERIC. The average length of the questions
in the training set is 10.2 whereas in the test set
is 7.5. This difference is due to the fact that those
questions are from different sources. All questions
were parsed by Liang Huang’s dependency parser
with the beam width 16.
We randomly picked 5% of the training set (272
questions) for validation. Our FCN has the dimen-
sion of vectors at inner nodes 200, a window size
for the convolutional kernel of 5, and the activa-
tion function tanh. It was trained with the learn-
ing rate 0.01, the regularization parameter 10−4,
and the mini batch size 1. The vectors represent-
ing the two padding tokens &lt;b&gt;, &lt;e&gt; were fixed
to 0.
We compare the FCN against the Convolutional
neural network (CNN) (Kim, 2014), the Dynamic
convolutional neural network (DCNN) (Kalch-
</bodyText>
<footnote confidence="0.9311314">
9LSTM-RNN and CT-LSTM are very similar: they are
RNNs using LSTMs for composition. Their difference is that
LSTM-RNN uses one input gate for each child where as CT-
LSTM uses only one input gate for all children.
10http://cogcomp.cs.illinois.edu/Data/QA/QC
</footnote>
<page confidence="0.948367">
1160
</page>
<table confidence="0.997819142857143">
Model Acc. (%)
DCNN 93.0
MaxEntH 93.6
CNN-non-static 93.6
SVMS 95.0
LSTM-RNN 93.4
FCN 94.8
</table>
<tableCaption confidence="0.994916">
Table 2: Accuracies on the TREC question type
</tableCaption>
<bodyText confidence="0.979046535714286">
classification dataset. The accuracies of DCNN,
MaxEntH, CNN-non-static, and SVMS are copied
from the corresponding papers (see text).
brenner et al., 2014), MaxEntH (Huang et al.,
2008) (which uses MaxEnt with uni-bi-trigrams,
POS, wh-word, head word, word shape, parser,
hypernyms, WordNet) and the SVMS (Silva et al.,
2011) (which uses SVM with, in addition to fea-
tures used by MaxEntH, 60 hand-coded rules). We
also include the LSTM-RNN (Le and Zuidema,
2015) whose accuracy was computed by running
their published source code11 on binary trees from
the Stanford Parser12 (Klein and Manning, 2003).
This network was also initialized by the 300-D
GloVe word embeddings.
Table 2 shows the results.13 The FCN achieved
the second best accuracy, only lightly lower than
SVMS (0.2%). This is a promising result because
our network used only parse forests, unsupervis-
edly pre-trained word embeddings whereas SVMS
used heavily engineered resources. The differ-
ence between FCN and the third best is remark-
able (1.2%). Interestingly, LSTM-RNN did not
perform well on this dataset. This is likely be-
cause the questions are short and the parse trees
quite shallow, such that the two problems that the
LSTM was invented for (long range dependency
and vanishing gradient) do not play much of a role.
</bodyText>
<subsectionHeader confidence="0.999582">
4.3 Visualization
</subsectionHeader>
<bodyText confidence="0.99992">
We visualize the charts we obtained in the sen-
timent analysis task as in Figure 6. To identify
how important each cell is for determining the fi-
nal vector at the root, we compute the number of
features of each that are actually propagated all the
way to the root in the successive max pooling op-
</bodyText>
<footnote confidence="0.867602">
11https://github.com/lephong/lstm-rnn
12http://nlp.stanford.edu/software/lex-parser.shtml
13While finalizing the current paper we discovered a paper
by Ma et al. (2015) proposing a convolutional network model
for dependency trees. They report a new state-of-the-art ac-
curacy of 95.6%.
</footnote>
<bodyText confidence="0.999901933333333">
erations. The circles in a graph are proportional
to this number. Here, to make the contribution of
each individual cell clearer, we have set the win-
dow size to 1 to avoid direct interactions between
cells.
At the lexical level, we can see that the FCN
can discriminate important words from the others.
Two words “most” and “incoherent” are the key
of the sentiment of this sentence: if one of them is
replaced by another word (e.g. replacing “most”
by “few” or “incoherent” by “coherent”), the sen-
timent will flip. The punctuation “.” however also
has a high contribution to the root. This happens
to other charts as well. We conjecture that the net-
work uses the vector of “.” to store neutral features
and propagate them to the root whenever it can not
find more useful features in other vectors. Our fu-
ture work is to examine this.
At the phrasal level, the network tends to group
words in grammatical constituents, such as “most
of the action setups”, “are incoherent”. Ill-formed
constituents such as “of the action” and “incoher-
ent .” receive little attention from the network.
Interestingly, we can see that the circle of “inco-
herent” is larger than the circles of any inner cells,
suggesting that the network is able to make use
of parses containing direct links from that word to
the root. This is evidence that the network has an
ability of selecting (or combining) parses that are
beneficial to this sentiment analysis task.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999941526315789">
The idea that a composition function must be able
to change its behaviour on the fly according to in-
put vectors is explored by Socher et al. (2013), Le
and Zuidema (2015), among others. The tensor
in the former is multiplied with the vector repre-
sentations of the phrases it is going to combine
to define a composition function (a matrix) on the
fly, and then multiplies again with these vectors
to yield a compound representation. In the LSTM
architecture of the latter, there is one input gate
for each child in order to control how the vector
of the child affects the composition at the parent
node. Because the input gate is a function of the
vector of the child, the composition function has
an infinite number of behaviours. In this paper, we
instead slide a kernel function along the sequence
of children to generate different ways of composi-
tion. Although the number of behaviours is limited
(and depends on the window size), it simultane-
</bodyText>
<page confidence="0.995903">
1161
</page>
<figureCaption confidence="0.958892">
Figure 6: Chart of sentence “Most of the action setups are incoherent .” The size of a circle is proposi-
tional to the number of the cell’s features that are propagated to the root.
</figureCaption>
<bodyText confidence="0.999886482758621">
ously provides us with a solution to handle rules
with different branching sizes.
Some approaches try to overcome the prob-
lem of varying branching sizes. Le and Zuidema
(2014b) use different sets of weight matrices for
different branching sizes, thus requiring a large
number of parameters. Because large branching-
size rules are rare, many parameters are infre-
quently updated during training. Socher et al.
(2014), for dependency trees, use a weight matrix
for each relative position to the head word (e.g.,
first-left, second-right). Le and Zuidema (2014a)
replace relative positions by dependency relations
(e.g., OBJ, SUBJ). These approaches strongly de-
pend on input parse trees and are very sensitive to
parsing errors. The approach presented in this pa-
per, on the other hand, does not need the informa-
tion about the head word position and is less sensi-
tive to parsing errors. Moreover, its number of pa-
rameters is independent from the maximal branch-
ing size.
Convolutional networks have been widely ap-
plied to solve natural language processing tasks.
Collobert et al. (2011), Kalchbrenner et al. (2014),
and Kim (2014) use convolutional networks to
deal with varying length sequences. Recently, Zhu
et al. (2015) and Ma et al. (2015) try to intergrate
syntactic information by employing parse trees.
Ma et al. (2015) extend the work of Kim (2014)
by taking into acount dependency relations so that
long range dependencies could be captured. The
model proposed by Zhu et al. (2015), which is
very similar to our Recursive convolutional neural
network model, is to use a convolutional network
for the composition purpose. Our work, although
also employing a convolutional network and syn-
tactic information, goes beyond them: we address
the issue of how to deal with uncertainty about the
correct parse inside the neural architecture. There-
fore, instead of using a single parse, our proposed
FCN model takes as input a forest of parses.
Related to our FCN is the Gated recursive con-
volutional neural network model proposed by Cho
et al. (2014) which is stacking n −1 convolutional
neural layers using a window-size-2 gated kernel
(where n is the sentence length). Mapping their
network into a chart, each cell is only connected
to the two cells right below it. What makes this
network special is the gated kernel which is a 3-
gate switcher for choosing one of three options:
directly transmit the left/right child’s vector to the
parent node, or compose the vectors of the two
children. Thanks to this, the network can capture
any binary parse trees by setting those gates prop-
erly. However, because only one gate is allowed to
open in a cell, the network is not able to capture an
arbitrary forest. Our FCN is thus more expressive
and flexible than their model.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997">
We proposed the Forest Convolutional Network
(FCN) model that addresses the three issues: (1)
how to make the composition functions adaptive,
(2) how to deal with different branching factors of
nodes in the relevant syntactic trees, (3) how to
deal with uncertainty about the correct parse in-
side the neural architecture. The key principle is
to carry out many different ways of computation
</bodyText>
<page confidence="0.98521">
1162
</page>
<bodyText confidence="0.999963266666667">
and then choose or combine some of them. For
more details, the two first issues are solved by em-
ploying a convolutional net for composition. To
the third issue, the network takes input as a forest
of parses instead of a single parse as in traditional
approaches.
Our future work is to focus on how to
choose/combine different ways of computation.
For instance, we might replace the max pooling
by different pooling operations such as mean pool-
ing, k-max pooling (Kalchbrenner et al., 2014),
and stochastic pooling (Zeiler and Fergus, 2013).
We can even bias the selection/combination to-
ward grammatical constituents by weighing cells
by their inside probabilities.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999965166666667">
We thank our three anonymous reviewers for their
comments. This work was funded by the Faculty
of Humanities of the University of Amsterdam,
through a Digital Humanities fellowship to PL and
a position in the New Generation Initiative (NGO)
for WZ.
</bodyText>
<sectionHeader confidence="0.998228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995734152866242">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173–180. Association for Computational Lin-
guistics.
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. Syntax, Semantics and Structure in Statis-
tical Translation, page 103.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121–2159.
Jason Eisner. 2001. Smoothing a Probabilistic Lexicon
via Syntactic Transformations. Ph.D. thesis, Univer-
sity of Pennsylvania, July. 318 pages.
Christoph Goller and Andreas K¨uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In International
Conference on Neural Networks, pages 347–352.
IEEE.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086. Association for Computational Linguistics.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using head words and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 927–936. Association for Computa-
tional Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586–
594.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665, Baltimore, Maryland, June. Association
for Computational Linguistics.
Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.
Phong Le and Willem Zuidema. 2014a. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics.
Phong Le and Willem Zuidema. 2014b. Inside-outside
semantics: A framework for neural models of se-
mantic composition. In NIPS 2014 Workshop on
Deep Learning and Representation Learning.
Phong Le and Willem Zuidema. 2015. Compositional
distributional semantics with long short term mem-
ory. In Proceedings of the Joint Conference on Lex-
ical and Computational Semantics (*SEM). Associ-
ation for Computational Linguistics.
1163
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.
Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xi-
ang. 2015. Dependency-based convolutional neural
networks for sentence embedding. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 174–179, Beijing,
China, July. Association for Computational Linguis-
tics.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and
Andreas Wichert. 2011. From symbolic to sub-
symbolic information in question classification. Ar-
tificial Intelligence Review, 35(2):137–154.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. Advances in Neural
Information Processing Systems, 24:801–809.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings EMNLP.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.
Daniel H Younger. 1967. Recognition and parsing of
context-free languages in time n 3. Information and
control, 10(2):189–208.
Matthew D Zeiler and Rob Fergus. 2013. Stochas-
tic pooling for regularization of deep convolutional
neural networks. arXiv preprint arXiv:1301.3557.
Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing
Huang. 2015. A re-ranking model for dependency
parser with recursive convolutional neural network.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1159–1168, Beijing, China, July. Association for
Computational Linguistics.
</reference>
<page confidence="0.996261">
1164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476046">
<title confidence="0.9678125">The Forest Convolutional Network: Compositional Semantics with a Neural Chart and without Binarization</title>
<author confidence="0.816434">Le</author>
<affiliation confidence="0.7858335">Institute for Logic, Language and University of Amsterdam, the</affiliation>
<abstract confidence="0.99650862962963">According to the principle of compositionality, the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined. In practice, however, the syntactic structure is computed by automatic parsers which are far-from-perfect and not tuned to the specifics of the task. Current recursive neural network (RNN) approaches for computing sentence meaning therefore run into a number of practical difficulties, including the need to carefully select a parser appropriate for the task, deciding how and to what extent syntactic context modifies the semantic composition function, as well as on how to transform parse trees to conform to the branching settings (typically, binary branching) of the RNN. This paper introduces a new model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a single tree, and by allowing arbitrary branching factors. We report improvements over the state-of-the-art in sentiment analysis and question classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18793" citStr="Charniak and Johnson, 2005" startWordPosition="3208" endWordPosition="3212">CNN, DCNN, PV, DRNN, LSTM-RNN and CTLSTM are copied from the corresponding papers (see text). also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing. All sentences were parsed by Liang Huang’s dependency parser8 (Huang and Sagae, 2010). We used this parser because it generates parse forests and that dependency trees are less deep than constituent trees. In addition, because the SST was annotated in a constituency manner, we also employed the Charniak’s constituent parser (Charniak and Johnson, 2005) with Huang (2008)’s forest pruner. We found that the beam width 16 for the dependency parser and the log probability beam 10 for the other worked best. Lower values harmed the system’s performance and higher values were not beneficial. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merri¨enboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder–decoder approaches. Syntax, Semantics and Structure in Statistical Translation,</title>
<date>2014</date>
<pages>103</pages>
<marker>Cho, van Merri¨enboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="7201" citStr="Collobert et al., 2011" startWordPosition="1155" endWordPosition="1158"> Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). To illustrate how a CNN works, the following example uses a simplified model proposed by Collobert et al. (2011) which consists of one convolutional layer with the max pooling operation, followed by one fully connected layer (Figure 2). This CNN uses a kernel with window size 3; when we slide this kernel along the sentence “(s) I like it very much (/s)”, we get five vectors: u(1) = W1x&lt;s&gt; + W2xI + W3xlike + bc u(2) = W1xI + W2xlike + W3xit + bc ... u(5) = W1xvery + W2xmuch + W3x&lt;/s&gt; + bc where W1, W2, W3 E Rdxm are weight matrices, bc E Rm is a bias vec</context>
<context position="27717" citStr="Collobert et al. (2011)" startWordPosition="4689" endWordPosition="4692"> for each relative position to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional netw</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2121--2159</pages>
<contexts>
<context position="16471" citStr="Duchi et al., 2011" startWordPosition="2842" endWordPosition="2845">ss-entropy over the training sentence set D plus an L2-norm regularization term A log Pr(cp|p) + 2||0||2 where 0 is the parameter set, cp is the sentiment class of phrase p, p is the vector representation at the node covering p, Pr(cp|p) is computed by score on section 23 of the Penn Treebank whereas resulted forests are very compact: the average number of hyperedges per forest is 123.1. the softmax function, and A is the regularization parameter. The gradient ∂J/∂0 is computed efficiently thanks to the back-propagation through structure (Goller and K¨uchler, 1996). We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for each parameter. 4 Experiments We evaluate the FCN model with two tasks: question classification and sentiment analysis. The evaluation metric is the classification accuracy. Our networks were initialized with the 300-D GloVe word embeddings trained on a corpus of 840B words6 (Pennington et al., 2014). The initial values for a weight matrix were uniformly sampled from the symmetric interval I− 1 1 I √n,√n where n is the number of total input units. In each experiment, a development set was used to tune the model. We run the model ten times and chos</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, pages 2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Smoothing a Probabilistic Lexicon via Syntactic Transformations.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<volume>318</volume>
<pages>pages.</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="3342" citStr="Eisner (2001" startWordPosition="512" endWordPosition="513">ions (e.g., adjective-noun combinations are of a very different type than VP-PP combinations); 2. how to deal with different branching factors of nodes in the relevant syntactic trees (i.e., we want to avoid having to binarize syntactic trees,1 but also do not want ternary productions to be completely independent from binary productions); 3. how to deal with uncertainty about the correct parse inside the neural architecture (i.e., we don’t want to work with just the best or k-best parses for a sentence according to an external model, but receive an entire distribution over possible parsers). 1Eisner (2001, Chapter 2) shows that using flat rules is linguistically beneficial, “most crucially, a flat lexical entry corresponds to the local domain of a headword-the word together with all its semantic arguments and modifiers”. From the computational perspective, flat rules make trees less deep, thus avoiding the vanishing gradient problem and capturing long range dependencies. 1155 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1155–1164, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Recursive Neural</context>
</contexts>
<marker>Eisner, 2001</marker>
<rawString>Jason Eisner. 2001. Smoothing a Probabilistic Lexicon via Syntactic Transformations. Ph.D. thesis, University of Pennsylvania, July. 318 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In International Conference on Neural Networks,</booktitle>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In International Conference on Neural Networks, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="18524" citStr="Huang and Sagae, 2010" startWordPosition="3165" endWordPosition="3168">9.9 88.0 CT-LSTM 51.0 88.0 FCN (dep.) 50.4 88.2 FCN (const.) 51.0 89.1 Table 1: Accuracies at sentence level on the SST dataset. FCN (dep.) and FCN (const.) denote the FCN with dependency forests and with constituent forests, respectively. The accuracies of RNTN, CNN, DCNN, PV, DRNN, LSTM-RNN and CTLSTM are copied from the corresponding papers (see text). also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing. All sentences were parsed by Liang Huang’s dependency parser8 (Huang and Sagae, 2010). We used this parser because it generates parse forests and that dependency trees are less deep than constituent trees. In addition, because the SST was annotated in a constituency manner, we also employed the Charniak’s constituent parser (Charniak and Johnson, 2005) with Huang (2008)’s forest pruner. We found that the beam width 16 for the dependency parser and the log probability beam 10 for the other worked best. Lower values harmed the system’s performance and higher values were not beneficial. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional k</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077– 1086. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiheng Huang</author>
<author>Marcus Thint</author>
<author>Zengchang Qin</author>
</authors>
<title>Question classification using head words and their hypernyms.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>927--936</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22353" citStr="Huang et al., 2008" startWordPosition="3793" endWordPosition="3796">utional neural network (DCNN) (Kalch9LSTM-RNN and CT-LSTM are very similar: they are RNNs using LSTMs for composition. Their difference is that LSTM-RNN uses one input gate for each child where as CTLSTM uses only one input gate for all children. 10http://cogcomp.cs.illinois.edu/Data/QA/QC 1160 Model Acc. (%) DCNN 93.0 MaxEntH 93.6 CNN-non-static 93.6 SVMS 95.0 LSTM-RNN 93.4 FCN 94.8 Table 2: Accuracies on the TREC question type classification dataset. The accuracies of DCNN, MaxEntH, CNN-non-static, and SVMS are copied from the corresponding papers (see text). brenner et al., 2014), MaxEntH (Huang et al., 2008) (which uses MaxEnt with uni-bi-trigrams, POS, wh-word, head word, word shape, parser, hypernyms, WordNet) and the SVMS (Silva et al., 2011) (which uses SVM with, in addition to features used by MaxEntH, 60 hand-coded rules). We also include the LSTM-RNN (Le and Zuidema, 2015) whose accuracy was computed by running their published source code11 on binary trees from the Stanford Parser12 (Klein and Manning, 2003). This network was also initialized by the 300-D GloVe word embeddings. Table 2 shows the results.13 The FCN achieved the second best accuracy, only lightly lower than SVMS (0.2%). This</context>
</contexts>
<marker>Huang, Thint, Qin, 2008</marker>
<rawString>Zhiheng Huang, Marcus Thint, and Zengchang Qin. 2008. Question classification using head words and their hypernyms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 927–936. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="13803" citStr="Huang (2008)" startWordPosition="2365" endWordPosition="2366">est Convolutional Network We now introduce the Forest Convolutional Network (FCN) model, which is a combination of the RCNN and the ChNN. The idea is to use an automatic parser to prune a chart5, debinarize productions (if applicable), and then apply a ChNN 4In each cell, we apply the matrix-vector multiplication two times and (if the cell is not a leaf) apply the max pooling to a pool of maximally n d-D vectors. 5Pruning a chart by an automatic parser is also not perfect. However, the quality of a pruned chart can get very close to human annotation. For instance, the chart pruner proposed by Huang (2008) has a forest oracle of 97.8% F(3) U(2) = f(W1P1 + W2xwith + b) and then apply the max pooling operation to these two vectors to compute P4. We do the same to compute P5. Finally, at the top, there are three productions p6 → ate p5, p6 → p1 p3 and p6 → p4 Milos. Similarly, we compute one vector for each production and employ the max pooling operation to compute P6. Because this ChNN processes a chart like the CKY algorithm, its time complexity is O(n2d2 + n3d) where n and d are the sentence length and the dimension of vectors, respectively.4 A ChNN is thus notably more complex than an RNN, who</context>
<context position="18811" citStr="Huang (2008)" startWordPosition="3214" endWordPosition="3215"> CTLSTM are copied from the corresponding papers (see text). also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing. All sentences were parsed by Liang Huang’s dependency parser8 (Huang and Sagae, 2010). We used this parser because it generates parse forests and that dependency trees are less deep than constituent trees. In addition, because the SST was annotated in a constituency manner, we also employed the Charniak’s constituent parser (Charniak and Johnson, 2005) with Huang (2008)’s forest pruner. We found that the beam width 16 for the dependency parser and the log probability beam 10 for the other worked best. Lower values harmed the system’s performance and higher values were not beneficial. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see F</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL, pages 586– 594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="2137" citStr="Irsoy and Cardie, 2014" startWordPosition="313" endWordPosition="316">al networks’ (e.g., Socher et al. (2010)), both the word and sentence representations are vectors, and the word vectors (“embeddings”) are borrowed from work in distributional semantics or neural language modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser). The network, which thus implements a ‘composition function’, is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, 2015), paraphrase detection (Socher et al., 2011), semantic relatedness (Tai et al., 2015) etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks. In this paper, we represent a new recursive neural network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues: 1. how to make the composition functions adaptive, in the sense that they operate adequately for the many different types of combinations (e</context>
<context position="6578" citStr="Irsoy and Cardie, 2014" startWordPosition="1050" endWordPosition="1053">f assigning a class c to an input x by eu(c,ytop) Pr(c|x) = softmax(c) = u(c,,ytop) (2) Ec,cC e where [u(c1, ytop), ..., u(c|C|, ytop) IT = Wuytop + bu; C is the set of all possible classes; Wu E R|C|xd,bu E R|C |are a weight matrix and a bias vector. Training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks </context>
<context position="19812" citStr="Irsoy and Cardie, 2014" startWordPosition="3375" endWordPosition="3378"> the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%). Constituent parsing is clearly more helpful than dependency parsing: </context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="7228" citStr="Kalchbrenner et al., 2014" startWordPosition="1159" endWordPosition="1162">nd applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). To illustrate how a CNN works, the following example uses a simplified model proposed by Collobert et al. (2011) which consists of one convolutional layer with the max pooling operation, followed by one fully connected layer (Figure 2). This CNN uses a kernel with window size 3; when we slide this kernel along the sentence “(s) I like it very much (/s)”, we get five vectors: u(1) = W1x&lt;s&gt; + W2xI + W3xlike + bc u(2) = W1xI + W2xlike + W3xit + bc ... u(5) = W1xvery + W2xmuch + W3x&lt;/s&gt; + bc where W1, W2, W3 E Rdxm are weight matrices, bc E Rm is a bias vector. The max pooling operat</context>
<context position="19655" citStr="Kalchbrenner et al., 2014" startWordPosition="3355" endWordPosition="3358"> Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). C</context>
<context position="27745" citStr="Kalchbrenner et al. (2014)" startWordPosition="4693" endWordPosition="4696">on to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syntactic informatio</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1746--1751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="7240" citStr="Kim, 2014" startWordPosition="1163" endWordPosition="1164">idema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). To illustrate how a CNN works, the following example uses a simplified model proposed by Collobert et al. (2011) which consists of one convolutional layer with the max pooling operation, followed by one fully connected layer (Figure 2). This CNN uses a kernel with window size 3; when we slide this kernel along the sentence “(s) I like it very much (/s)”, we get five vectors: u(1) = W1x&lt;s&gt; + W2xI + W3xlike + bc u(2) = W1xI + W2xlike + W3xit + bc ... u(5) = W1xvery + W2xmuch + W3x&lt;/s&gt; + bc where W1, W2, W3 E Rdxm are weight matrices, bc E Rm is a bias vector. The max pooling operation is then </context>
<context position="19578" citStr="Kim, 2014" startWordPosition="3346" endWordPosition="3347">e system’s performance and higher values were not beneficial. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the </context>
<context position="21714" citStr="Kim, 2014" startWordPosition="3699" endWordPosition="3700">fact that those questions are from different sources. All questions were parsed by Liang Huang’s dependency parser with the beam width 16. We randomly picked 5% of the training set (272 questions) for validation. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 5, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 1. The vectors representing the two padding tokens &lt;b&gt;, &lt;e&gt; were fixed to 0. We compare the FCN against the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalch9LSTM-RNN and CT-LSTM are very similar: they are RNNs using LSTMs for composition. Their difference is that LSTM-RNN uses one input gate for each child where as CTLSTM uses only one input gate for all children. 10http://cogcomp.cs.illinois.edu/Data/QA/QC 1160 Model Acc. (%) DCNN 93.0 MaxEntH 93.6 CNN-non-static 93.6 SVMS 95.0 LSTM-RNN 93.4 FCN 94.8 Table 2: Accuracies on the TREC question type classification dataset. The accuracies of DCNN, MaxEntH, CNN-non-static, and SVMS are copied from the corresponding papers (see text). brenner et a</context>
<context position="27761" citStr="Kim (2014)" startWordPosition="4698" endWordPosition="4699">-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syntactic information, goes beyond t</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746– 1751, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22768" citStr="Klein and Manning, 2003" startWordPosition="3859" endWordPosition="3862">the TREC question type classification dataset. The accuracies of DCNN, MaxEntH, CNN-non-static, and SVMS are copied from the corresponding papers (see text). brenner et al., 2014), MaxEntH (Huang et al., 2008) (which uses MaxEnt with uni-bi-trigrams, POS, wh-word, head word, word shape, parser, hypernyms, WordNet) and the SVMS (Silva et al., 2011) (which uses SVM with, in addition to features used by MaxEntH, 60 hand-coded rules). We also include the LSTM-RNN (Le and Zuidema, 2015) whose accuracy was computed by running their published source code11 on binary trees from the Stanford Parser12 (Klein and Manning, 2003). This network was also initialized by the 300-D GloVe word embeddings. Table 2 shows the results.13 The FCN achieved the second best accuracy, only lightly lower than SVMS (0.2%). This is a promising result because our network used only parse forests, unsupervisedly pre-trained word embeddings whereas SVMS used heavily engineered resources. The difference between FCN and the third best is remarkable (1.2%). Interestingly, LSTM-RNN did not perform well on this dataset. This is likely because the questions are short and the parse trees quite shallow, such that the two problems that the LSTM was</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="19745" citStr="Mikolov, 2014" startWordPosition="3366" endWordPosition="3367">f 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%). Co</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6641" citStr="Zuidema, 2014" startWordPosition="1063" endWordPosition="1064"> u(c,,ytop) (2) Ec,cC e where [u(c1, ytop), ..., u(c|C|, ytop) IT = Wuytop + bu; C is the set of all possible classes; Wu E R|C|xd,bu E R|C |are a weight matrix and a bias vector. Training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).</context>
<context position="26812" citStr="Zuidema (2014" startWordPosition="4547" endWordPosition="4548">has an infinite number of behaviours. In this paper, we instead slide a kernel function along the sequence of children to generate different ways of composition. Although the number of behaviours is limited (and depends on the window size), it simultane1161 Figure 6: Chart of sentence “Most of the action setups are incoherent .” The size of a circle is propositional to the number of the cell’s features that are propagated to the root. ously provides us with a solution to handle rules with different branching sizes. Some approaches try to overcome the problem of varying branching sizes. Le and Zuidema (2014b) use different sets of weight matrices for different branching sizes, thus requiring a large number of parameters. Because large branchingsize rules are rare, many parameters are infrequently updated during training. Socher et al. (2014), for dependency trees, use a weight matrix for each relative position to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014a. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>Inside-outside semantics: A framework for neural models of semantic composition.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Workshop on Deep Learning and Representation Learning.</booktitle>
<contexts>
<context position="6641" citStr="Zuidema, 2014" startWordPosition="1063" endWordPosition="1064"> u(c,,ytop) (2) Ec,cC e where [u(c1, ytop), ..., u(c|C|, ytop) IT = Wuytop + bu; C is the set of all possible classes; Wu E R|C|xd,bu E R|C |are a weight matrix and a bias vector. Training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).</context>
<context position="26812" citStr="Zuidema (2014" startWordPosition="4547" endWordPosition="4548">has an infinite number of behaviours. In this paper, we instead slide a kernel function along the sequence of children to generate different ways of composition. Although the number of behaviours is limited (and depends on the window size), it simultane1161 Figure 6: Chart of sentence “Most of the action setups are incoherent .” The size of a circle is propositional to the number of the cell’s features that are propagated to the root. ously provides us with a solution to handle rules with different branching sizes. Some approaches try to overcome the problem of varying branching sizes. Le and Zuidema (2014b) use different sets of weight matrices for different branching sizes, thus requiring a large number of parameters. Because large branchingsize rules are rare, many parameters are infrequently updated during training. Socher et al. (2014), for dependency trees, use a weight matrix for each relative position to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014b. Inside-outside semantics: A framework for neural models of semantic composition. In NIPS 2014 Workshop on Deep Learning and Representation Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>Compositional distributional semantics with long short term memory.</title>
<date>2015</date>
<booktitle>In Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics.</booktitle>
<pages>1163</pages>
<contexts>
<context position="2160" citStr="Zuidema, 2015" startWordPosition="319" endWordPosition="320">l. (2010)), both the word and sentence representations are vectors, and the word vectors (“embeddings”) are borrowed from work in distributional semantics or neural language modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser). The network, which thus implements a ‘composition function’, is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, 2015), paraphrase detection (Socher et al., 2011), semantic relatedness (Tai et al., 2015) etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks. In this paper, we represent a new recursive neural network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues: 1. how to make the composition functions adaptive, in the sense that they operate adequately for the many different types of combinations (e.g., adjective-noun com</context>
<context position="6601" citStr="Zuidema, 2015" startWordPosition="1056" endWordPosition="1057">ut x by eu(c,ytop) Pr(c|x) = softmax(c) = u(c,,ytop) (2) Ec,cC e where [u(c1, ytop), ..., u(c|C|, ytop) IT = Wuytop + bu; C is the set of all possible classes; Wu E R|C|xd,bu E R|C |are a weight matrix and a bias vector. Training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011</context>
<context position="19904" citStr="Zuidema, 2015" startWordPosition="3393" endWordPosition="3394">average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%). Constituent parsing is clearly more helpful than dependency parsing: the improvements that the FCN got are 0.6% in the fine-grained task and 0.9% in the binary t</context>
<context position="22630" citStr="Zuidema, 2015" startWordPosition="3840" endWordPosition="3841">ta/QA/QC 1160 Model Acc. (%) DCNN 93.0 MaxEntH 93.6 CNN-non-static 93.6 SVMS 95.0 LSTM-RNN 93.4 FCN 94.8 Table 2: Accuracies on the TREC question type classification dataset. The accuracies of DCNN, MaxEntH, CNN-non-static, and SVMS are copied from the corresponding papers (see text). brenner et al., 2014), MaxEntH (Huang et al., 2008) (which uses MaxEnt with uni-bi-trigrams, POS, wh-word, head word, word shape, parser, hypernyms, WordNet) and the SVMS (Silva et al., 2011) (which uses SVM with, in addition to features used by MaxEntH, 60 hand-coded rules). We also include the LSTM-RNN (Le and Zuidema, 2015) whose accuracy was computed by running their published source code11 on binary trees from the Stanford Parser12 (Klein and Manning, 2003). This network was also initialized by the 300-D GloVe word embeddings. Table 2 shows the results.13 The FCN achieved the second best accuracy, only lightly lower than SVMS (0.2%). This is a promising result because our network used only parse forests, unsupervisedly pre-trained word embeddings whereas SVMS used heavily engineered resources. The difference between FCN and the third best is remarkable (1.2%). Interestingly, LSTM-RNN did not perform well on th</context>
<context position="25676" citStr="Zuidema (2015)" startWordPosition="4349" endWordPosition="4350">of the action” and “incoherent .” receive little attention from the network. Interestingly, we can see that the circle of “incoherent” is larger than the circles of any inner cells, suggesting that the network is able to make use of parses containing direct links from that word to the root. This is evidence that the network has an ability of selecting (or combining) parses that are beneficial to this sentiment analysis task. 5 Related Work The idea that a composition function must be able to change its behaviour on the fly according to input vectors is explored by Socher et al. (2013), Le and Zuidema (2015), among others. The tensor in the former is multiplied with the vector representations of the phrases it is going to combine to define a composition function (a matrix) on the fly, and then multiplies again with these vectors to yield a compound representation. In the LSTM architecture of the latter, there is one input gate for each child in order to control how the vector of the child affects the composition at the parent node. Because the input gate is a function of the vector of the child, the composition function has an infinite number of behaviours. In this paper, we instead slide a kerne</context>
</contexts>
<marker>Zuidema, 2015</marker>
<rawString>Phong Le and Willem Zuidema. 2015. Compositional distributional semantics with long short term memory. In Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics. 1163</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>86--11</pages>
<contexts>
<context position="6877" citStr="LeCun et al., 1998" startWordPosition="1102" endWordPosition="1105">o minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural language processing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). To illustrate how a CNN works, the following example uses a simplified model proposed by Collobert et al. (2011) which consists of one convolutional layer with the max pooling operation, followed by one fully connected layer (Figure 2)</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20771" citStr="Li and Roth, 2002" startWordPosition="3531" endWordPosition="3534"> is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%). Constituent parsing is clearly more helpful than dependency parsing: the improvements that the FCN got are 0.6% in the fine-grained task and 0.9% in the binary task. We conjecture that, because sentences in the treebank were parsed by a constituent parser (here is the Stanford parser), training with constituent forests is easier. 4.2 Question Classification In this task we used the TREC question dataset10 (Li and Roth, 2002) which contains 5952 questions (5452 questions for training and 500 questions for testing). The task is to assign a question to one in six types: ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC. The average length of the questions in the training set is 10.2 whereas in the test set is 7.5. This difference is due to the fact that those questions are from different sources. All questions were parsed by Liang Huang’s dependency parser with the beam width 16. We randomly picked 5% of the training set (272 questions) for validation. Our FCN has the dimension of vectors at inner nodes 20</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingbo Ma</author>
<author>Liang Huang</author>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
</authors>
<title>Dependency-based convolutional neural networks for sentence embedding.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),</booktitle>
<pages>174--179</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="23938" citStr="Ma et al. (2015)" startWordPosition="4046" endWordPosition="4049">, such that the two problems that the LSTM was invented for (long range dependency and vanishing gradient) do not play much of a role. 4.3 Visualization We visualize the charts we obtained in the sentiment analysis task as in Figure 6. To identify how important each cell is for determining the final vector at the root, we compute the number of features of each that are actually propagated all the way to the root in the successive max pooling op11https://github.com/lephong/lstm-rnn 12http://nlp.stanford.edu/software/lex-parser.shtml 13While finalizing the current paper we discovered a paper by Ma et al. (2015) proposing a convolutional network model for dependency trees. They report a new state-of-the-art accuracy of 95.6%. erations. The circles in a graph are proportional to this number. Here, to make the contribution of each individual cell clearer, we have set the window size to 1 to avoid direct interactions between cells. At the lexical level, we can see that the FCN can discriminate important words from the others. Two words “most” and “incoherent” are the key of the sentiment of this sentence: if one of them is replaced by another word (e.g. replacing “most” by “few” or “incoherent” by “cohe</context>
<context position="27876" citStr="Ma et al. (2015)" startWordPosition="4715" endWordPosition="4718">UBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syntactic information, goes beyond them: we address the issue of how to deal with uncertainty about the correct parse inside the neural architecture. T</context>
</contexts>
<marker>Ma, Huang, Zhou, Xiang, 2015</marker>
<rawString>Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xiang. 2015. Dependency-based convolutional neural networks for sentence embedding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 174–179, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="16819" citStr="Pennington et al., 2014" startWordPosition="2896" endWordPosition="2899">erage number of hyperedges per forest is 123.1. the softmax function, and A is the regularization parameter. The gradient ∂J/∂0 is computed efficiently thanks to the back-propagation through structure (Goller and K¨uchler, 1996). We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for each parameter. 4 Experiments We evaluate the FCN model with two tasks: question classification and sentiment analysis. The evaluation metric is the classification accuracy. Our networks were initialized with the 300-D GloVe word embeddings trained on a corpus of 840B words6 (Pennington et al., 2014). The initial values for a weight matrix were uniformly sampled from the symmetric interval I− 1 1 I √n,√n where n is the number of total input units. In each experiment, a development set was used to tune the model. We run the model ten times and chose the run with the highest performance on the development set. We employed early stopping: training is halted if performance on the development set does not improve after three consecutive epochs. 4.1 Sentiment Analysis The Stanford Sentiment Treebank (SST)7 (Socher et al., 2013) which consists of 5-way fine-grained sentiment labels (very negativ</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Silva</author>
<author>Lu´ısa Coheur</author>
<author>Ana Cristina Mendes</author>
<author>Andreas Wichert</author>
</authors>
<title>From symbolic to subsymbolic information in question classification.</title>
<date>2011</date>
<journal>Artificial Intelligence Review,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="22493" citStr="Silva et al., 2011" startWordPosition="3814" endWordPosition="3817">hat LSTM-RNN uses one input gate for each child where as CTLSTM uses only one input gate for all children. 10http://cogcomp.cs.illinois.edu/Data/QA/QC 1160 Model Acc. (%) DCNN 93.0 MaxEntH 93.6 CNN-non-static 93.6 SVMS 95.0 LSTM-RNN 93.4 FCN 94.8 Table 2: Accuracies on the TREC question type classification dataset. The accuracies of DCNN, MaxEntH, CNN-non-static, and SVMS are copied from the corresponding papers (see text). brenner et al., 2014), MaxEntH (Huang et al., 2008) (which uses MaxEnt with uni-bi-trigrams, POS, wh-word, head word, word shape, parser, hypernyms, WordNet) and the SVMS (Silva et al., 2011) (which uses SVM with, in addition to features used by MaxEntH, 60 hand-coded rules). We also include the LSTM-RNN (Le and Zuidema, 2015) whose accuracy was computed by running their published source code11 on binary trees from the Stanford Parser12 (Klein and Manning, 2003). This network was also initialized by the 300-D GloVe word embeddings. Table 2 shows the results.13 The FCN achieved the second best accuracy, only lightly lower than SVMS (0.2%). This is a promising result because our network used only parse forests, unsupervisedly pre-trained word embeddings whereas SVMS used heavily eng</context>
</contexts>
<marker>Silva, Coheur, Mendes, Wichert, 2011</marker>
<rawString>Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and Andreas Wichert. 2011. From symbolic to subsymbolic information in question classification. Artificial Intelligence Review, 35(2):137–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="1555" citStr="Socher et al. (2010)" startWordPosition="230" endWordPosition="233"> the branching settings (typically, binary branching) of the RNN. This paper introduces a new model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a single tree, and by allowing arbitrary branching factors. We report improvements over the state-of-the-art in sentiment analysis and question classification. 1 Introduction For many natural language processing tasks we need to compute meaning representations for sentences from meaning representations of words. In a recent line of research on ‘recursive neural networks’ (e.g., Socher et al. (2010)), both the word and sentence representations are vectors, and the word vectors (“embeddings”) are borrowed from work in distributional semantics or neural language modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser). The network, which thus implements a ‘composition function’, is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24--801</pages>
<contexts>
<context position="2204" citStr="Socher et al., 2011" startWordPosition="323" endWordPosition="326"> representations are vectors, and the word vectors (“embeddings”) are borrowed from work in distributional semantics or neural language modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser). The network, which thus implements a ‘composition function’, is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, 2015), paraphrase detection (Socher et al., 2011), semantic relatedness (Tai et al., 2015) etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks. In this paper, we represent a new recursive neural network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues: 1. how to make the composition functions adaptive, in the sense that they operate adequately for the many different types of combinations (e.g., adjective-noun combinations are of a very different type than </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in Neural Information Processing Systems, 24:801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings EMNLP.</booktitle>
<contexts>
<context position="6554" citStr="Socher et al., 2013" startWordPosition="1046" endWordPosition="1049">ute the probability of assigning a class c to an input x by eu(c,ytop) Pr(c|x) = softmax(c) = u(c,,ytop) (2) Ec,cC e where [u(c1, ytop), ..., u(c|C|, ytop) IT = Wuytop + bu; C is the set of all possible classes; Wu E R|C|xd,bu E R|C |are a weight matrix and a bias vector. Training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and applicability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an extension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (often with a pooling operation) followed by one or more fully connected layers. This architecture was 1156 invented for computer vision. It then has been widely applied to solve natural la</context>
<context position="17351" citStr="Socher et al., 2013" startWordPosition="2988" endWordPosition="2991">300-D GloVe word embeddings trained on a corpus of 840B words6 (Pennington et al., 2014). The initial values for a weight matrix were uniformly sampled from the symmetric interval I− 1 1 I √n,√n where n is the number of total input units. In each experiment, a development set was used to tune the model. We run the model ten times and chose the run with the highest performance on the development set. We employed early stopping: training is halted if performance on the development set does not improve after three consecutive epochs. 4.1 Sentiment Analysis The Stanford Sentiment Treebank (SST)7 (Socher et al., 2013) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences. We used the standard splitting: 8544 sentences for training, 1101 for development, and 2210 for testing. The average sentence length is 19.1. In addition, the treebank 6http://nlp.stanford.edu/projects/GloVe/ 7http://nlp.stanford.edu/sentiment/treebank.html 1 J(0) = −|D |� sED pEs 1159 Model Fine-grained Binary RNTN 45.7 85.4 CNN 48.0 88.1 DCNN 48.5 86.8 PV 48.7 87.8 DRNN 49.8 86.6 LSTM-RNN 49.9 88.0 CT-LSTM 51.0 88.0 FCN (dep.) 50.4 88.2 F</context>
<context position="19526" citStr="Socher et al., 2013" startWordPosition="3336" endWordPosition="3339">lity beam 10 for the other worked best. Lower values harmed the system’s performance and higher values were not beneficial. Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh. It was trained with the learning rate 0.01, the regularization parameter 10−4, and the mini batch size 5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing</context>
<context position="25653" citStr="Socher et al. (2013)" startWordPosition="4343" endWordPosition="4346">formed constituents such as “of the action” and “incoherent .” receive little attention from the network. Interestingly, we can see that the circle of “incoherent” is larger than the circles of any inner cells, suggesting that the network is able to make use of parses containing direct links from that word to the root. This is evidence that the network has an ability of selecting (or combining) parses that are beneficial to this sentiment analysis task. 5 Related Work The idea that a composition function must be able to change its behaviour on the fly according to input vectors is explored by Socher et al. (2013), Le and Zuidema (2015), among others. The tensor in the former is multiplied with the vector representations of the phrases it is going to combine to define a composition function (a matrix) on the fly, and then multiplies again with these vectors to yield a compound representation. In the LSTM architecture of the latter, there is one input gate for each child in order to control how the vector of the child affects the composition at the parent node. Because the input gate is a function of the vector of the child, the composition function has an infinite number of behaviours. In this paper, w</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="27051" citStr="Socher et al. (2014)" startWordPosition="4581" endWordPosition="4584">w size), it simultane1161 Figure 6: Chart of sentence “Most of the action setups are incoherent .” The size of a circle is propositional to the number of the cell’s features that are propagated to the root. ously provides us with a solution to handle rules with different branching sizes. Some approaches try to overcome the problem of varying branching sizes. Le and Zuidema (2014b) use different sets of weight matrices for different branching sizes, thus requiring a large number of parameters. Because large branchingsize rules are rare, many parameters are infrequently updated during training. Socher et al. (2014), for dependency trees, use a weight matrix for each relative position to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied t</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>1556--1566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="2245" citStr="Tai et al., 2015" startWordPosition="330" endWordPosition="333">ectors (“embeddings”) are borrowed from work in distributional semantics or neural language modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure provided by an external parser). The network, which thus implements a ‘composition function’, is optimized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, 2015), paraphrase detection (Socher et al., 2011), semantic relatedness (Tai et al., 2015) etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks. In this paper, we represent a new recursive neural network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues: 1. how to make the composition functions adaptive, in the sense that they operate adequately for the many different types of combinations (e.g., adjective-noun combinations are of a very different type than VP-PP combinations); 2. how to deal with </context>
<context position="19963" citStr="Tai et al., 2015" startWordPosition="3401" endWordPosition="3404">following the convolutional layer was removed (i.e., p = x, see Figure 3). We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and 8http://acl.cs.qc.edu/∼lhuang/software Mikolov, 2014), the Deep recursive neural network (DRNN) (Irsoy and Cardie, 2014), the Recursive neural network with Long short term memory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et al., 2015).9 Table 1 shows the results. Our FCN using constituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the difference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%). Constituent parsing is clearly more helpful than dependency parsing: the improvements that the FCN got are 0.6% in the fine-grained task and 0.9% in the binary task. We conjecture that, because sentences in the treebank </context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556–1566, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n 3. Information and control,</title>
<date>1967</date>
<pages>10--2</pages>
<contexts>
<context position="12728" citStr="Younger, 1967" startWordPosition="2167" endWordPosition="2168"> is far from perfect and taskindependent. Therefore, a good solution is to give 3An indirect interaction can be set up through pooling. the system a set of parses and let it decide which parse is the best or to combine some of them. The RNN model handles one extreme where this set contains only one parse. We now consider the other extreme where the set contains all possible parses. Because the number of all possible binary parse trees of a length-n sentence is the nth Catalan number, processing individual parses is not practical. We thus propose a new model working on charts in the CKY style (Younger, 1967), called Chart Neural Network (ChNN). We describe this model by the following example. Given a phrase “ate pho with Milos”, a ChNN will process its parse chart as in Figure 4. Because any 2-word constituent has only one parse, the computation for p1, p2, p3 is identical to Equation 1. For 3-word constituent p4, because there are two possible productions p4 → ate p2 and p4 → p1 with, we compute one vector for each production U(1) = f(W1xate + W2P2 + b) 3.3 Forest Convolutional Network We now introduce the Forest Convolutional Network (FCN) model, which is a combination of the RCNN and the ChNN.</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H Younger. 1967. Recognition and parsing of context-free languages in time n 3. Information and control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
<author>Rob Fergus</author>
</authors>
<title>Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557.</title>
<date>2013</date>
<marker>Zeiler, Fergus, 2013</marker>
<rawString>Matthew D Zeiler and Rob Fergus. 2013. Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chenxi Zhu</author>
<author>Xipeng Qiu</author>
<author>Xinchi Chen</author>
<author>Xuanjing Huang</author>
</authors>
<title>A re-ranking model for dependency parser with recursive convolutional neural network.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>1159--1168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="10484" citStr="Zhu et al. (2015)" startWordPosition="1758" endWordPosition="1761">as l + k − 1 vectors. It is obvious that this RCNN can solve the second issue (i.e., dealing with different branching factors), we now show how it can make the composition functions adaptive. We first see what happens if the window size k is larger than the number of children l, for instance k = 3 and l = 2. There are four vectors in the pool u(1) = f(W1x&lt;b&gt; + W2x&lt;b&gt; + W3x1 + bc) u(2) = f(W1x&lt;b&gt; + W2x1 + W3x2 + bc) u(3) = f(W1x1 + W2x2 + W3x&lt;e&gt; + bc) u(4) = f(W1x2 + W2x&lt;e&gt; + W3x&lt;e&gt; + bc) where W1, W2, W3 are weight matrices, bc is a 2While finalizing the current paper we discovered a paper by Zhu et al. (2015) proposing a similar model which is evaluated on syntactic parsing. Our work goes substantially beyond theirs, however, as it takes a parse forest rather than a single tree as input. 1157 Figure 4: Chart Neural Network. bias vector, f is an activation function. These four resulted vectors correspond to four ways of composing the two children: (1) the first child stands alone (e.g., when the information of the second child is not important, it is better to ignore it), (2,3) the two children are composed with two different weight matrix sets, (4) the second child stands alone. Now, imagine that </context>
<context position="27855" citStr="Zhu et al. (2015)" startWordPosition="4710" endWordPosition="4713">elations (e.g., OBJ, SUBJ). These approaches strongly depend on input parse trees and are very sensitive to parsing errors. The approach presented in this paper, on the other hand, does not need the information about the head word position and is less sensitive to parsing errors. Moreover, its number of parameters is independent from the maximal branching size. Convolutional networks have been widely applied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syntactic information, goes beyond them: we address the issue of how to deal with uncertainty about the correct parse inside the n</context>
</contexts>
<marker>Zhu, Qiu, Chen, Huang, 2015</marker>
<rawString>Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing Huang. 2015. A re-ranking model for dependency parser with recursive convolutional neural network. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1159–1168, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000321">
<title confidence="0.9995175">
A Joint Dependency Model of Morphological and Syntactic Structure for
Statistical Machine Translation
</title>
<author confidence="0.993055">
Rico Sennrich and Barry Haddow
</author>
<affiliation confidence="0.99973">
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.990736">
rico.sennrich@ed.ac.uk,bhaddow@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.9973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999638095238095">
When translating between two languages
that differ in their degree of morpholog-
ical synthesis, syntactic structures in one
language may be realized as morphologi-
cal structures in the other, and SMT mod-
els need a mechanism to learn such trans-
lations. Prior work has used morpheme
splitting with flat representations that do
not encode the hierarchical structure be-
tween morphemes, but this structure is rel-
evant for learning morphosyntactic con-
straints and selectional preferences. We
propose to model syntactic and morpho-
logical structure jointly in a dependency
translation model, allowing the system
to generalize to the level of morphemes.
We present a dependency representation
of German compounds and particle verbs
that results in improvements in transla-
tion quality of 1.4–1.8 BLEU in the WMT
English–German translation task.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998480333333333">
When translating between two languages that dif-
fer in their degree of morphological synthesis,
syntactic structures in one language may be re-
alized as morphological structures in the other.
Machine Translation models that treat words as
atomic units have poor learning capabilities for
such translation units, and morphological segmen-
tations are commonly used (Koehn and Knight,
2003). Like words in a sentence, the morphemes
of a word have a hierarchical structure that is rel-
evant in translation. For instance, compounds in
Germanic languages are head-final, and the head is
the segment that determines agreement within the
noun phrase, and is relevant for selectional prefer-
ences of verbs.
</bodyText>
<footnote confidence="0.285193">
1. sie erheben eine Hand|gepäck|gebühr.
</footnote>
<table confidence="0.509502">
function/postion English/German example
finite (main) he walks away quickly
er geht schnell weg
finite (sub.) [...] because he walks away quickly
[...] weil er schnell weggeht
bare infinitive he can walk away quickly
er kann schnell weggehen
to/zu-infinitive he promises to walk away quickly
er verspricht, schnell wegzugehen
</table>
<tableCaption confidence="0.974163">
Table 1: Surface realizations of particle verb
weggehen ’walk away’.
</tableCaption>
<bodyText confidence="0.992790933333333">
they charge a carry-on bag fee.
In example 1, agreement in case, number and
gender is enforced between eine ’a’ and Gebühr
’fee’, and selectional preference between erheben
’charge’ and Gebühr ’fee’. A flat representation,
as is common in phrase-based SMT, does not en-
code these relationships, but a dependency repre-
sentation does so through dependency links.
In this paper, we investigate a dependency rep-
resentation of morphologically segmented words
for SMT. Our representation encodes syntactic and
morphological structure jointly, allowing a single
model to learn the translation of both. Specifi-
cally, we work with a string-to-tree model with
GHKM-style rules (Galley et al., 2006), and a
relational dependency language model (Sennrich,
2015). We focus on the representation of German
syntax and morphology in an English-to-German
system, and two morphologically complex word
classes in German that are challenging for transla-
tion, compounds and particle verbs.
German makes heavy use of compounding, and
compounds such as Abwasserbehandlungsanlage
‘waste water treatment plant’ are translated into
complex noun phrases in other languages, such as
French station d’épuration des eaux résiduaires.
German particle verbs are difficult to model be-
cause their surface realization differs depending
on the finiteness of the verb and the type of clause.
Verb particles are separated from the finite verb in
</bodyText>
<page confidence="0.924819">
2081
</page>
<note confidence="0.646688">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2081–2087,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9998326">
main clauses, but prefixed to the verb in subordi-
nated clauses, or when the verb is non-finite. The
infinitive marker zu ’to’, which is normally a pre-
modifying particle, appears as an infix in particle
verbs. Table 1 shows an illustrating example.
</bodyText>
<sectionHeader confidence="0.9599765" genericHeader="method">
2 A Dependency Representation of
Compounds and Particle Verbs
</sectionHeader>
<bodyText confidence="0.999980868421052">
The main focus of research on compound split-
ting has been on the splitting algorithm (Popovic
et al., 2006; NieBen and Ney, 2000; Weller et al.,
2014; Macherey et al., 2011). Our focus is not the
splitting algorithm, but the representation of com-
pounds. For splitting, we use an approach simi-
lar to (Fritzinger and Fraser, 2010), with segmen-
tation candidates identified by a finite-state mor-
phology (Schmid et al., 2004; Sennrich and Kunz,
2014), and statistical evidence from the training
corpus to select a split (Koehn and Knight, 2003).
German compounds are head-final, and pre-
modifiers can be added recursively. Compounds
are structurally ambiguous if there is more than
one modifier. Consider the distinction between
(Stadtteil)projekt (literally: ’(city part) project)’)
and Stadt(teilprojekt) ’city sub-project’. We opt
for a left-branching representation by default.1 We
also split linking elements, and represent them as
a postmodifier of each non-final segment, includ-
ing the empty string (&amp;quot;c&amp;quot;). We use the same repre-
sentation for noun compounds and adjective com-
pounds.
An example of the original2 and the proposed
compound representation is shown in Figure 1.
Importantly, the head of the compound is also
the parent of the determiners and attributes in
the noun phrase, which makes a bigram depen-
dency language model sufficient to enforce agree-
ment. Since we model morphosyntactic agree-
ment within the main translation step, and not in
a separate step as in (Fraser et al., 2012), we deem
it useful that inflection is marked at the head of
the compound. Consequently, we do not split off
inflectional or derivational morphemes.
For German particle verbs, we define a common
representation that abstracts away from the vari-
ous surface realizations (see Table 1). Separated
</bodyText>
<footnote confidence="0.721063769230769">
1We follow prior work in leaving frequent words or sub-
words unsplit, which has a disambiguating effect. With more
aggressive splitting, frequency information could be used for
the structural disambiguation of internal structure.
2The original dependency trees follow the annotation
guidelines by Foth (2005).
sie erheben eine Handgepäckgebühr
PPER VVFIN ART NN
they charge a carry-on bag fee
obja
sie erheben eine Hand a gepäck a gebühr
PPER VVFIN ART SEG LN SEG LN SEG
they charge a carry-on bag fee
</footnote>
<figureCaption confidence="0.980031">
Figure 1: Original and proposed representation of
German compound.
</figureCaption>
<figure confidence="0.963206">
obji
er verspricht , schnell wegzugehen
PPER VVFIN $, ADJD VVIZU
he promises to go away quickly
obji
er verspricht , schnell zu weg gehen
PPER VVFIN $, ADJD PTKZU PTKVZ VVINF
he promises to go away quickly
</figure>
<figureCaption confidence="0.972752">
Figure 2: Original and proposed representation of
</figureCaption>
<bodyText confidence="0.970853916666667">
German particle verb with infixed zu-marker.
verb particles are reordered to be the closest pre-
modifier of the verb. Prefixed particles and the zu-
infix are identified by the finite-state-morphology,
and split from the verb so that the particle is
the closest, the zu marker the next-closest pre-
modifier of the verb, as shown in Figure 2. Agree-
ment, selectional preferences, and other phenom-
ena involve the verb and its dependents, and the
proposed representation retains these dependency
links, but reduces data sparsity from affixation and
avoids discontinuity of the verb and its particle.
</bodyText>
<sectionHeader confidence="0.994655" genericHeader="method">
3 Tree Binarization
</sectionHeader>
<bodyText confidence="0.999806">
We follow Williams et al. (2014) and map de-
pendency trees into a constituency representation,
which allows for the extraction of GHKM-style
translation rules (Galley et al., 2006). This con-
version is lossless, and we can still apply a de-
</bodyText>
<figure confidence="0.996682904761905">
root
obja
det
subj
det
root
mod
mod
subj
link
link
root
comma
subj
adv
comma
root
adv
part
subj
avz
</figure>
<page confidence="0.984659">
2082
</page>
<bodyText confidence="0.988031893617021">
pendency language model (RDLM). Figure 3 (a)
shows the constituency representation of the ex-
ample in Figure 1.
Our model should not only be able to produce
new words productively, but also to memorize
words it has observed during training. Looking at
the compound Handgepäckgebühr in Figure 3 (a),
we can see that it does not form a constituent, and
cannot be extracted with GHKM extraction heuris-
tics. To address this, we binarize the trees in our
training data (Wang et al., 2007).
A complicating factor is that the binarization
should not impair the RDLM. During decoding,
we map the internal tree structure of each hypoth-
esis back to the unbinarized form, which is then
scored by the RDLM. Virtual nodes introduced by
the binarization must also be scorable by RDLM
if they form the root of a translation hypothesis. A
simple right or left binarization would produce vir-
tual nodes without head and without meaningful
dependency representation. We ensure that each
virtual node dominates the head of the full con-
stituent through a mixed binarization.3 Specifi-
cally, we perform right binarization of the head
and all pre-modifiers, then left binarization of all
post-modifiers. This head-binarized representa-
tion is illustrated in Figure 3 (b).4
Head binarization ensures that even hypotheses
whose root is a virtual node can be scored by the
RDLM. This score is only relevant for pruning,
and discarded when the full constituent is scored.
Still, these hypotheses require special treatment in
the RDLM to mitigate search errors. The virtual
LINK
node labels (such as OBJA) are unknown symbols
to the RDLM, and we simply replace them with
the original label (OBJA). The RDLM uses sibling
context, and this is normally padded with special
start and stop symbols, analogous to BOS/EOS
symbols in n-gram models. These start and stop
symbols let the RDLM compute the probability
that a node is the first or last child of its ances-
tor node. However, computing these probabilities
for virtual nodes would unfairly bias the search,
since the first/last child of a virtual node is not nec-
essarily the first/last child of the full constituent.
We adapt the representation of virtual nodes in
</bodyText>
<footnote confidence="0.938949714285714">
3In other words, every node is a fixed well-formed depen-
dency structure (Shen et al., 2010) with our binarization.
4Note that our definition of head binarization is different
from that of Wang et al. (2007), who left-binarize a node if
the head is the first child, and right-binarize otherwise. Our
algorithm also covers cases where the head has both pre- and
post-modifiers, as erheben and gepäck do in Figure 3.
</footnote>
<figure confidence="0.60178675">
ROOT
OBJA
MOD SEG
gebühr
</figure>
<page confidence="0.928677">
2083
</page>
<bodyText confidence="0.998178461538461">
particle moved to the right clause bracket.5
Previous work on particle verb translation into
German proposed to predict the position of parti-
cles with an n-gram language model (Nießen and
Ney, 2001). Our rules have the advantage that they
are informed by the syntax of the sentence and
consider the finiteness of the verb.
Our rules only produce projective trees. Verb
particles may also appear in positions that violate
projectivity, and we leave it to future research to
determine if our limitation to projective trees af-
fects translation quality, and how to produce non-
projective trees.
</bodyText>
<sectionHeader confidence="0.997604" genericHeader="method">
5 SMT experiments
</sectionHeader>
<subsectionHeader confidence="0.969385">
5.1 Data and Models
</subsectionHeader>
<bodyText confidence="0.9735945">
We train English–German string-to-tree SMT sys-
tems on the training data of the shared transla-
tion task of the Workshop on Statistical Machine
Translation (WMT) 2015. The data set consists of
4.2 million sentence pairs of parallel data, and 160
million sentences of monolingual German data.
We base our systems on that of Williams et
al. (2014). It is a string-to-tree GHKM transla-
tion system implemented in Moses (Koehn et al.,
2007), and using the dependency annotation by
ParZu (Sennrich et al., 2013). Additionally, our
baseline system contains a dependency language
model (RDLM) (Sennrich, 2015), trained on the
target-side of the parallel training data.
We report case-sensitive BLEU scores on the
newstest2014/5 test sets from WMT, averaged
over 3 optimization runs of k-batch MIRA (Cherry
and Foster, 2012) on a subset of newstest2008-12.6
We split all particle verbs and hyphenated com-
pounds, but other compounds are only split if they
are rare (frequency in parallel text &lt; 5).
For comparison with the state-of-the-art, we
train a full system on our restructured representa-
tion, which incorporates all models and settings of
our WMT 2015 submission system (Williams et
al., 2015).7 Note that our WMT 2015 submission
5We use the last position in the clause as default location,
but put the particle before any subordinated and coordinated
clauses, which occur in the Nachfeld (the ‘final field’ in topo-
logical field theory).
</bodyText>
<footnote confidence="0.986667833333333">
6We use mteval-v13a.pl for comparability to official
WMT results; all significance values reported are obtained
with MultEval (Clark et al., 2011).
7In contrast to our other systems in this paper, RDLM is
trained on all monolingual data for the full system, and two
models are added: a 5-gram Neural Network language model
</footnote>
<table confidence="0.999654875">
system newstest2014 newstest2015
baseline 20.7 22.0
+split compounds 21.3 22.4
+particle verbs 21.4 22.8
head binarization 20.9 22.7
+split compounds 22.0 23.4
+particle verbs 22.1 23.8
full system 22.6 24.4
</table>
<tableCaption confidence="0.987525">
Table 2: English–German translation results
(BLEU). Average of three optimization runs.
</tableCaption>
<table confidence="0.995079285714286">
system compound sep. particle verb
pref. zu-infix
reference 2841 553 1195 176
baseline 845 96 847 71
+head binarization 798 157 858 106
+split compounds 1850 160 877 94
+particle verbs 1992 333 953 169
</table>
<tableCaption confidence="0.65848525">
Table 3: Number of compounds [that would be
split by compound splitter] and particle verbs
(separated, prefixed and with zu-infix) in new-
stest2014/5. Average of three optimization runs.
</tableCaption>
<bodyText confidence="0.99577175">
uses the dependency representation of compounds
and tree binarization introduced in this paper; we
achieve additional gains over the submission sys-
tem through particle verb restructuring.
</bodyText>
<subsectionHeader confidence="0.99896">
5.2 SMT Results
</subsectionHeader>
<bodyText confidence="0.999858772727273">
Table 2 shows translation quality (BLEU) with dif-
ferent representations of German compounds and
particle verbs. Head binarization not only yields
improvements over the baseline, but also allows
for larger gains from morphological segmenta-
tion. We attribute this to the fact that full com-
pounds, and prefixed particle verbs, are not al-
ways a constituent in the segmented representa-
tion, and that binarization compensates this the-
oretical drawback.
With head binarization, we find substantial im-
provements from compound splitting of 0.7–1.1
BLEU. On newstest2014, the improvement is
almost twice of that reported in related work
(Williams et al., 2014), which also uses a hier-
archical representation of compounds, albeit one
that does not allow for dependency modelling.
Examples of correct, unseen compounds gener-
ated include Staubsauger|roboter ’vacuum cleaner
robot’, Gravitation|s|wellen ’gravitational waves’,
and NPD|-|verbot|s|verfahren ’NPD banning pro-
cess’.8
</bodyText>
<footnote confidence="0.706195">
(Vaswani et al., 2013), and soft source-syntactic constraints
(Huck et al., 2014).
8Note that Staubsauger, despite being a compound, is not
</footnote>
<page confidence="0.996523">
2084
</page>
<bodyText confidence="0.999817028571429">
Particle verb restructuring yields additional
gains of 0.1–0.4 BLEU. One reason for the smaller
effect of particle verb restructuring is that the diffi-
cult cases – separated particle verbs and those with
infixation – are rarer than compounds, with 2841
rare compounds [that would be split by our com-
pound splitter] in the reference texts, in contrast
to 553 separated particle verbs, and 176 particle
verbs with infixation, as Table 3 illustrates. If we
only evaluate the sentences containing a particle
verb with zu-infix in the reference, 165 in total
for newstest2014/5, we observe an improvement
of 0.8 BLEU on this subset (22.1→22.9), signifi-
cant with p &lt; 0.05.
The positive effect of restructuring is also ap-
parent in frequency statistics. Table 3 shows that
the baseline system severely undergenerates com-
pounds and separated/infixed particle verbs. Bi-
narization, compound splitting, and particle verb
restructuring all contribute to bringing the distri-
bution of compounds and particle verbs closer to
the reference.
In total, the restructured representation yields
improvements of 1.4–1.8 BLEU over our base-
line. The full system is competitive with official
submissions to the WMT 2015 shared translation
tasks. It outperforms our submission (Williams
et al., 2015) by 0.4 BLEU, and outperforms other
phrase-based and syntax-based submissions by 0.8
BLEU or more. The best reported result accord-
ing to BLEU is an ensemble of Neural MT systems
(Jean et al., 2015), which achieves 24.9 BLEU. In
the human evaluation, both our submission and the
Neural MT system were ranked 1–2 (out of 16),
with no significant difference between them.
</bodyText>
<subsectionHeader confidence="0.994343">
5.3 Synthetic LM Experiment
</subsectionHeader>
<bodyText confidence="0.997738307692308">
We perform a synthetic experiment to test our
claim that a dependency representation allows for
the modelling of agreement between morphemes.
For 200 rare compounds [that would be split by
our compound splitter] in the newstest2014/5 ref-
erences, we artificially introduce agreement errors
by changing the gender of the determiner. For in-
stance, we create the erroneous sentence sie er-
heben ein Handgepäckgebühr as a complement to
Example 1. We measure the ability of language
models to prefer (give a higher probability to)
the original reference sentence over the erroneous
one. In the original representation, both a Kneser-
segmented due to its frequency.
Ney 5-gram LM and RDLM perform poorly due to
data sparseness, with 70% and 57.5% accuracy, re-
spectively. In the split representation, the RDLM
reliably prefers the correct agreement (96.5% ac-
curacy), whilst the performance of the 5-gram
model even deteriorates (to 60% accuracy). This
is because the gender of the first segment(s) is ir-
relevant, or even misleading, for agreement. For
instance, Handgepäck is neuter, which could lead
a morpheme-level n-gram model to prefer the de-
terminer ein, but Handgepäckgebühr is feminine
and requires eine.
</bodyText>
<sectionHeader confidence="0.995474" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999950111111111">
Our main contribution is that we exploit the hi-
erarchical structure of morphemes to model them
jointly with syntax in a dependency-based string-
to-tree SMT model. We describe the dependency
annotation of two morphologically complex word
classes in German, compounds and particle verbs,
and show that our tree representation yields im-
provements in translation quality of 1.4–1.8 BLEU
in the WMT English–German translation task.9
The principle of jointly representing syntactic
and morphological structure in dependency trees
can be applied to other language pairs, and we ex-
pect this to be helpful for languages with a high
degree of morphological synthesis. However, the
annotation needs to be adapted to the respective
languages. For example, French compounds such
as arc-en-ciel ’rainbow’ are head-initial, in con-
trast to head-final Germanic compounds.
</bodyText>
<sectionHeader confidence="0.990764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916">
This project received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreements 645452
(QT21), 644402 (HimL), 644333 (TraMOOC),
and from the Swiss National Science Foundation
under grant P2ZHP1_148717.
</bodyText>
<sectionHeader confidence="0.994098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982173714285714">
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, NAACL HLT ’12, pages 427–436, Montreal,
Canada. Association for Computational Linguistics.
</reference>
<footnote confidence="0.994100666666667">
9We released source code and configuration
files at https://github.com/rsennrich/
wmt2014-scripts.
</footnote>
<page confidence="0.989693">
2085
</page>
<reference confidence="0.999719294642857">
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better Hypothesis Testing for
Statistical Machine Translation: Controlling for Op-
timizer Instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 176–181, Portland, Oregon. As-
sociation for Computational Linguistics.
Killian A. Foth. 2005. Eine umfassende Constraint-
Dependenz-Grammatik des Deutschen. University
of Hamburg, Hamburg.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664–674, Avi-
gnon, France. Association for Computational Lin-
guistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ’10, pages 224–234, Uppsala,
Sweden. Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961–968, Sydney, Australia. As-
sociation for Computational Linguistics.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Preference Grammars and Soft Syntactic
Constraints for GHKM Syntax-based Statistical Ma-
chine Translation. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 148–156, Doha, Qatar.
Association for Computational Linguistics.
Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland
Memisevic, and Yoshua Bengio. 2015. Montreal
Neural Machine Translation Systems for WMT’15 .
In Proceedings of the Tenth Workshop on Statistical
Machine Translation.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL ’03:
Proceedings of the Tenth Conference on European
Chapter of the Association for Computational Lin-
guistics, pages 187–193, Budapest, Hungary. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL-2007 Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic.
Association for Computational Linguistics.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-
independent compound splitting with morphological
operations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1395–
1404, Portland, Oregon, USA. Association for Com-
putational Linguistics.
Sonja Niel3en and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
18th Int. Conf. on Computational Linguistics, pages
1081–1085.
Sonja Niel3en and Hermann Ney. 2001. Morpho-
syntactic analysis for Reordering in Statistical Ma-
chine Translation. In Machine Translation Summit,
pages 247–252, Santiago de Compostela, Spain.
Maja Popovic, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Advances in Natural Language
Processing, 5th International Conference on NLP,
FinTAL 2006, pages 616–624, Turku, Finland.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. A German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of the IVth International Conference on
Language Resources and Evaluation (LREC 2004),
pages 1263–1266.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601–609, Hissar, Bulgaria.
Rico Sennrich. 2015. Modelling and Optimizing on
Syntactic N-Grams for Statistical Machine Transla-
tion. Transactions of the Association for Computa-
tional Linguistics, 3:169–182.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency Statistical Machine Transla-
tion. Comput. Linguist., 36(4):649–671.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-Scale
Neural Language Models Improves Translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2013, pages 1387–1392, Seattle, Washington, USA.
</reference>
<page confidence="0.828981">
2086
</page>
<reference confidence="0.998962653846154">
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing Syntax Trees to Improve Syntax-Based
Machine Translation Accuracy. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Marion Weller, Fabienne Cap, Stefan Müller, Sabine
Schulte im Walde, and Alexander Fraser. 2014. Dis-
tinguishing Degrees of Compositionality in Com-
pound Splitting for Statistical Machine Translation.
In Proceedings of the First Workshop on Computa-
tional Approaches to Compound Analysis (ComA-
ComA 2014), pages 81–90, Dublin, Ireland.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh’s Syntax-Based Systems at WMT
2014. In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, pages 207–214, Bal-
timore, Maryland, USA. Association for Computa-
tional Linguistics.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, and Philipp Koehn. 2015. Edin-
burgh’s Syntax-Based Systems at WMT 2015. In
Proceedings of the Tenth Workshop on Statistical
Machine Translation, Lisbon, Portugal. Association
for Computational Linguistics.
</reference>
<page confidence="0.994365">
2087
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.779527">
<title confidence="0.993492">A Joint Dependency Model of Morphological and Syntactic Structure Statistical Machine Translation</title>
<author confidence="0.80902">Sennrich</author>
<affiliation confidence="0.990846">School of Informatics, University of Edinburgh</affiliation>
<abstract confidence="0.998646045454545">When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other, and SMT models need a mechanism to learn such translations. Prior work has used morpheme splitting with flat representations that do not encode the hierarchical structure between morphemes, but this structure is relevant for learning morphosyntactic constraints and selectional preferences. We propose to model syntactic and morphological structure jointly in a dependency translation model, allowing the system to generalize to the level of morphemes. We present a dependency representation of German compounds and particle verbs that results in improvements in translaquality of 1.4–1.8 the WMT English–German translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>427--436</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="11759" citStr="Cherry and Foster, 2012" startWordPosition="1853" endWordPosition="1856">f 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topolog</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 427–436, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="12525" citStr="Clark et al., 2011" startWordPosition="1975" endWordPosition="1978">ncy in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model system newstest2014 newstest2015 baseline 20.7 22.0 +split compounds 21.3 22.4 +particle verbs 21.4 22.8 head binarization 20.9 22.7 +split compounds 22.0 23.4 +particle verbs 22.1 23.8 full system 22.6 24.4 Table 2: English–German translation results (BLEU). Average of three optimization runs. system compound sep. particle verb pref. zu-infix reference 2841 553 1195 176 baseline 845 96 847 71 +head binarization 798 157</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 176–181, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Killian A Foth</author>
</authors>
<date>2005</date>
<booktitle>Eine umfassende ConstraintDependenz-Grammatik des</booktitle>
<institution>Deutschen. University of Hamburg,</institution>
<location>Hamburg.</location>
<contexts>
<context position="6219" citStr="Foth (2005)" startWordPosition="937" endWordPosition="938"> al., 2012), we deem it useful that inflection is marked at the head of the compound. Consequently, we do not split off inflectional or derivational morphemes. For German particle verbs, we define a common representation that abstracts away from the various surface realizations (see Table 1). Separated 1We follow prior work in leaving frequent words or subwords unsplit, which has a disambiguating effect. With more aggressive splitting, frequency information could be used for the structural disambiguation of internal structure. 2The original dependency trees follow the annotation guidelines by Foth (2005). sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja sie erheben eine Hand a gepäck a gebühr PPER VVFIN ART SEG LN SEG LN SEG they charge a carry-on bag fee Figure 1: Original and proposed representation of German compound. obji er verspricht , schnell wegzugehen PPER VVFIN $, ADJD VVIZU he promises to go away quickly obji er verspricht , schnell zu weg gehen PPER VVFIN $, ADJD PTKZU PTKVZ VVINF he promises to go away quickly Figure 2: Original and proposed representation of German particle verb with infixed zu-marker. verb particles are reordered to be the</context>
</contexts>
<marker>Foth, 2005</marker>
<rawString>Killian A. Foth. 2005. Eine umfassende ConstraintDependenz-Grammatik des Deutschen. University of Hamburg, Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
<author>Fabienne Cap</author>
</authors>
<title>Modeling Inflection and WordFormation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>664--674</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France.</location>
<contexts>
<context position="5619" citStr="Fraser et al., 2012" startWordPosition="845" endWordPosition="848">t.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&amp;quot;c&amp;quot;). We use the same representation for noun compounds and adjective compounds. An example of the original2 and the proposed compound representation is shown in Figure 1. Importantly, the head of the compound is also the parent of the determiners and attributes in the noun phrase, which makes a bigram dependency language model sufficient to enforce agreement. Since we model morphosyntactic agreement within the main translation step, and not in a separate step as in (Fraser et al., 2012), we deem it useful that inflection is marked at the head of the compound. Consequently, we do not split off inflectional or derivational morphemes. For German particle verbs, we define a common representation that abstracts away from the various surface realizations (see Table 1). Separated 1We follow prior work in leaving frequent words or subwords unsplit, which has a disambiguating effect. With more aggressive splitting, frequency information could be used for the structural disambiguation of internal structure. 2The original dependency trees follow the annotation guidelines by Foth (2005)</context>
</contexts>
<marker>Fraser, Weller, Cahill, Cap, 2012</marker>
<rawString>Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and WordFormation in SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–674, Avignon, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>224--234</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="4454" citStr="Fritzinger and Fraser, 2010" startWordPosition="665" endWordPosition="668">es, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them </context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 224–234, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In ACL44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="2896" citStr="Galley et al., 2006" startWordPosition="426" endWordPosition="429">mber and gender is enforced between eine ’a’ and Gebühr ’fee’, and selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization diffe</context>
<context position="7527" citStr="Galley et al., 2006" startWordPosition="1151" endWordPosition="1154">he finite-state-morphology, and split from the verb so that the particle is the closest, the zu marker the next-closest premodifier of the verb, as shown in Figure 2. Agreement, selectional preferences, and other phenomena involve the verb and its dependents, and the proposed representation retains these dependency links, but reduces data sparsity from affixation and avoids discontinuity of the verb and its particle. 3 Tree Binarization We follow Williams et al. (2014) and map dependency trees into a constituency representation, which allows for the extraction of GHKM-style translation rules (Galley et al., 2006). This conversion is lossless, and we can still apply a deroot obja det subj det root mod mod subj link link root comma subj adv comma root adv part subj avz 2082 pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the example in Figure 1. Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgepäckgebühr in Figure 3 (a), we can see that it does not form a constituent, and cannot be extracted with GHKM extraction heuristics. To address this, we binarize the t</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In ACL44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Preference Grammars and Soft Syntactic Constraints for GHKM Syntax-based Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>148--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="14637" citStr="Huck et al., 2014" startWordPosition="2289" endWordPosition="2292">retical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 BLEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 BLEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sentences containing a particle verb with zu-infix in the refere</context>
</contexts>
<marker>Huck, Hoang, Koehn, 2014</marker>
<rawString>Matthias Huck, Hieu Hoang, and Philipp Koehn. 2014. Preference Grammars and Soft Syntactic Constraints for GHKM Syntax-based Statistical Machine Translation. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sébastien Jean</author>
<author>Orhan Firat</author>
<author>Kyunghyun Cho</author>
<author>Roland Memisevic</author>
<author>Yoshua Bengio</author>
</authors>
<date>2015</date>
<booktitle>Montreal Neural Machine Translation Systems for WMT’15 . In Proceedings of the Tenth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16171" citStr="Jean et al., 2015" startWordPosition="2530" endWordPosition="2533">rbs. Binarization, compound splitting, and particle verb restructuring all contribute to bringing the distribution of compounds and particle verbs closer to the reference. In total, the restructured representation yields improvements of 1.4–1.8 BLEU over our baseline. The full system is competitive with official submissions to the WMT 2015 shared translation tasks. It outperforms our submission (Williams et al., 2015) by 0.4 BLEU, and outperforms other phrase-based and syntax-based submissions by 0.8 BLEU or more. The best reported result according to BLEU is an ensemble of Neural MT systems (Jean et al., 2015), which achieves 24.9 BLEU. In the human evaluation, both our submission and the Neural MT system were ranked 1–2 (out of 16), with no significant difference between them. 5.3 Synthetic LM Experiment We perform a synthetic experiment to test our claim that a dependency representation allows for the modelling of agreement between morphemes. For 200 rare compounds [that would be split by our compound splitter] in the newstest2014/5 references, we artificially introduce agreement errors by changing the gender of the determiner. For instance, we create the erroneous sentence sie erheben ein Handge</context>
</contexts>
<marker>Jean, Firat, Cho, Memisevic, Bengio, 2015</marker>
<rawString>Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. Montreal Neural Machine Translation Systems for WMT’15 . In Proceedings of the Tenth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting. In</title>
<date>2003</date>
<booktitle>EACL ’03: Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<institution>Budapest, Hungary. Association for Computational Linguistics.</institution>
<contexts>
<context position="1465" citStr="Koehn and Knight, 2003" startWordPosition="208" endWordPosition="211">ralize to the level of morphemes. We present a dependency representation of German compounds and particle verbs that results in improvements in translation quality of 1.4–1.8 BLEU in the WMT English–German translation task. 1 Introduction When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other. Machine Translation models that treat words as atomic units have poor learning capabilities for such translation units, and morphological segmentations are commonly used (Koehn and Knight, 2003). Like words in a sentence, the morphemes of a word have a hierarchical structure that is relevant in translation. For instance, compounds in Germanic languages are head-final, and the head is the segment that determines agreement within the noun phrase, and is relevant for selectional preferences of verbs. 1. sie erheben eine Hand|gepäck|gebühr. function/postion English/German example finite (main) he walks away quickly er geht schnell weg finite (sub.) [...] because he walks away quickly [...] weil er schnell weggeht bare infinitive he can walk away quickly er kann schnell weggehen to/zu-inf</context>
<context position="4665" citStr="Koehn and Knight, 2003" startWordPosition="698" endWordPosition="701">illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&amp;quot;c&amp;quot;). We use the same representation for noun compounds and adjective compounds. An example of the original2 and the proposed compound rep</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In EACL ’03: Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics, pages 187–193, Budapest, Hungary. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="11379" citStr="Koehn et al., 2007" startWordPosition="1797" endWordPosition="1800">leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we t</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the ACL-2007 Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<marker></marker>
<rawString>In Proceedings of the ACL-2007 Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew Dai</author>
<author>David Talbot</author>
<author>Ashok Popat</author>
<author>Franz Och</author>
</authors>
<title>Languageindependent compound splitting with morphological operations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1395--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="4299" citStr="Macherey et al., 2011" startWordPosition="639" endWordPosition="642">in Natural Language Processing, pages 2081–2087, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. main clauses, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) projec</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Klaus Macherey, Andrew Dai, David Talbot, Ashok Popat, and Franz Och. 2011. Languageindependent compound splitting with morphological operations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1395– 1404, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niel3en</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1081--1085</pages>
<marker>Niel3en, Ney, 2000</marker>
<rawString>Sonja Niel3en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In 18th Int. Conf. on Computational Linguistics, pages 1081–1085.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niel3en</author>
<author>Hermann Ney</author>
</authors>
<title>Morphosyntactic analysis for Reordering in Statistical Machine Translation.</title>
<date>2001</date>
<booktitle>In Machine Translation Summit,</booktitle>
<pages>247--252</pages>
<location>Santiago de Compostela,</location>
<marker>Niel3en, Ney, 2001</marker>
<rawString>Sonja Niel3en and Hermann Ney. 2001. Morphosyntactic analysis for Reordering in Statistical Machine Translation. In Machine Translation Summit, pages 247–252, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation of German Compound Words.</title>
<date>2006</date>
<booktitle>In Advances in Natural Language Processing, 5th International Conference on NLP, FinTAL</booktitle>
<pages>616--624</pages>
<location>Turku, Finland.</location>
<contexts>
<context position="4232" citStr="Popovic et al., 2006" startWordPosition="627" endWordPosition="630"> in 2081 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2081–2087, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. main clauses, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the dis</context>
</contexts>
<marker>Popovic, Stein, Ney, 2006</marker>
<rawString>Maja Popovic, Daniel Stein, and Hermann Ney. 2006. Statistical Machine Translation of German Compound Words. In Advances in Natural Language Processing, 5th International Conference on NLP, FinTAL 2006, pages 616–624, Turku, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>A German Computational Morphology Covering Derivation, Composition, and Inflection.</title>
<date>2004</date>
<booktitle>In Proceedings of the IVth International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1263--1266</pages>
<contexts>
<context position="4545" citStr="Schmid et al., 2004" startWordPosition="679" endWordPosition="682"> marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&amp;quot;c&amp;quot;). We use the s</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. A German Computational Morphology Covering Derivation, Composition, and Inflection. In Proceedings of the IVth International Conference on Language Resources and Evaluation (LREC 2004), pages 1263–1266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Beat Kunz</author>
</authors>
<title>Zmorge: A German Morphological Lexicon Extracted from Wiktionary.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="4571" citStr="Sennrich and Kunz, 2014" startWordPosition="683" endWordPosition="686">h is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&amp;quot;c&amp;quot;). We use the same representation for nou</context>
</contexts>
<marker>Sennrich, Kunz, 2014</marker>
<rawString>Rico Sennrich and Beat Kunz. 2014. Zmorge: A German Morphological Lexicon Extracted from Wiktionary. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Martin Volk</author>
<author>Gerold Schneider</author>
</authors>
<title>Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing</booktitle>
<pages>601--609</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="11449" citStr="Sennrich et al., 2013" startWordPosition="1808" endWordPosition="1811">ctive trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorpora</context>
</contexts>
<marker>Sennrich, Volk, Schneider, 2013</marker>
<rawString>Rico Sennrich, Martin Volk, and Gerold Schneider. 2013. Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2013, pages 601–609, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--169</pages>
<contexts>
<context position="2957" citStr="Sennrich, 2015" startWordPosition="436" endWordPosition="437"> selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization differs depending on the finiteness of the verb and the type of cl</context>
<context position="11545" citStr="Sennrich, 2015" startWordPosition="1822" endWordPosition="1823">Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note tha</context>
</contexts>
<marker>Sennrich, 2015</marker>
<rawString>Rico Sennrich. 2015. Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation. Transactions of the Association for Computational Linguistics, 3:169–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<date>2010</date>
<journal>String-to-dependency Statistical Machine Translation. Comput. Linguist.,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="9965" citStr="Shen et al., 2010" startWordPosition="1562" endWordPosition="1565">l (OBJA). The RDLM uses sibling context, and this is normally padded with special start and stop symbols, analogous to BOS/EOS symbols in n-gram models. These start and stop symbols let the RDLM compute the probability that a node is the first or last child of its ancestor node. However, computing these probabilities for virtual nodes would unfairly bias the search, since the first/last child of a virtual node is not necessarily the first/last child of the full constituent. We adapt the representation of virtual nodes in 3In other words, every node is a fixed well-formed dependency structure (Shen et al., 2010) with our binarization. 4Note that our definition of head binarization is different from that of Wang et al. (2007), who left-binarize a node if the head is the first child, and right-binarize otherwise. Our algorithm also covers cases where the head has both pre- and post-modifiers, as erheben and gepäck do in Figure 3. ROOT OBJA MOD SEG gebühr 2083 particle moved to the right clause bracket.5 Previous work on particle verb translation into German proposed to predict the position of particles with an n-gram language model (Nießen and Ney, 2001). Our rules have the advantage that they are info</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency Statistical Machine Translation. Comput. Linguist., 36(4):649–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with Large-Scale Neural Language Models Improves Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,</booktitle>
<pages>1387--1392</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="14578" citStr="Vaswani et al., 2013" startWordPosition="2281" endWordPosition="2284">ed representation, and that binarization compensates this theoretical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 BLEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 BLEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sente</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with Large-Scale Neural Language Models Improves Translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, pages 1387–1392, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="8172" citStr="Wang et al., 2007" startWordPosition="1268" endWordPosition="1271">ss, and we can still apply a deroot obja det subj det root mod mod subj link link root comma subj adv comma root adv part subj avz 2082 pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the example in Figure 1. Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgepäckgebühr in Figure 3 (a), we can see that it does not form a constituent, and cannot be extracted with GHKM extraction heuristics. To address this, we binarize the trees in our training data (Wang et al., 2007). A complicating factor is that the binarization should not impair the RDLM. During decoding, we map the internal tree structure of each hypothesis back to the unbinarized form, which is then scored by the RDLM. Virtual nodes introduced by the binarization must also be scorable by RDLM if they form the root of a translation hypothesis. A simple right or left binarization would produce virtual nodes without head and without meaningful dependency representation. We ensure that each virtual node dominates the head of the full constituent through a mixed binarization.3 Specifically, we perform rig</context>
<context position="10080" citStr="Wang et al. (2007)" startWordPosition="1581" endWordPosition="1584"> to BOS/EOS symbols in n-gram models. These start and stop symbols let the RDLM compute the probability that a node is the first or last child of its ancestor node. However, computing these probabilities for virtual nodes would unfairly bias the search, since the first/last child of a virtual node is not necessarily the first/last child of the full constituent. We adapt the representation of virtual nodes in 3In other words, every node is a fixed well-formed dependency structure (Shen et al., 2010) with our binarization. 4Note that our definition of head binarization is different from that of Wang et al. (2007), who left-binarize a node if the head is the first child, and right-binarize otherwise. Our algorithm also covers cases where the head has both pre- and post-modifiers, as erheben and gepäck do in Figure 3. ROOT OBJA MOD SEG gebühr 2083 particle moved to the right clause bracket.5 Previous work on particle verb translation into German proposed to predict the position of particles with an n-gram language model (Nießen and Ney, 2001). Our rules have the advantage that they are informed by the syntax of the sentence and consider the finiteness of the verb. Our rules only produce projective trees</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Weller</author>
<author>Fabienne Cap</author>
<author>Stefan Müller</author>
<author>Sabine Schulte im Walde</author>
<author>Alexander Fraser</author>
</authors>
<title>Distinguishing Degrees of Compositionality in Compound Splitting for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014),</booktitle>
<pages>81--90</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="4275" citStr="Weller et al., 2014" startWordPosition="635" endWordPosition="638">on Empirical Methods in Natural Language Processing, pages 2081–2087, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. main clauses, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 A Dependency Representation of Compounds and Particle Verbs The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; NieBen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (litera</context>
</contexts>
<marker>Weller, Cap, Müller, Walde, Fraser, 2014</marker>
<rawString>Marion Weller, Fabienne Cap, Stefan Müller, Sabine Schulte im Walde, and Alexander Fraser. 2014. Distinguishing Degrees of Compositionality in Compound Splitting for Statistical Machine Translation. In Proceedings of the First Workshop on Computational Approaches to Compound Analysis (ComAComA 2014), pages 81–90, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Rico Sennrich</author>
<author>Maria Nadejde</author>
<author>Matthias Huck</author>
<author>Eva Hasler</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Systems at WMT</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>207--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="7380" citStr="Williams et al. (2014)" startWordPosition="1129" endWordPosition="1132">ith infixed zu-marker. verb particles are reordered to be the closest premodifier of the verb. Prefixed particles and the zuinfix are identified by the finite-state-morphology, and split from the verb so that the particle is the closest, the zu marker the next-closest premodifier of the verb, as shown in Figure 2. Agreement, selectional preferences, and other phenomena involve the verb and its dependents, and the proposed representation retains these dependency links, but reduces data sparsity from affixation and avoids discontinuity of the verb and its particle. 3 Tree Binarization We follow Williams et al. (2014) and map dependency trees into a constituency representation, which allows for the extraction of GHKM-style translation rules (Galley et al., 2006). This conversion is lossless, and we can still apply a deroot obja det subj det root mod mod subj link link root comma subj adv comma root adv part subj avz 2082 pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the example in Figure 1. Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgepäckgebühr in Figure</context>
<context position="11289" citStr="Williams et al. (2014)" startWordPosition="1782" endWordPosition="1785">jective trees. Verb particles may also appear in positions that violate projectivity, and we leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they</context>
<context position="14240" citStr="Williams et al., 2014" startWordPosition="2238" endWordPosition="2241">ality (BLEU) with different representations of German compounds and particle verbs. Head binarization not only yields improvements over the baseline, but also allows for larger gains from morphological segmentation. We attribute this to the fact that full compounds, and prefixed particle verbs, are not always a constituent in the segmented representation, and that binarization compensates this theoretical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 BLEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 BLEU. One reason for the smaller effect of particle verb restructuring is th</context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Hasler, Koehn, 2014</marker>
<rawString>Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, Eva Hasler, and Philipp Koehn. 2014. Edinburgh’s Syntax-Based Systems at WMT 2014. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, Baltimore, Maryland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Rico Sennrich</author>
<author>Maria Nadejde</author>
<author>Matthias Huck</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Systems at WMT</title>
<date>2015</date>
<booktitle>In Proceedings of the Tenth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="12134" citStr="Williams et al., 2015" startWordPosition="1914" endWordPosition="1917">age model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive BLEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model system newstest2014 newstest2015</context>
<context position="15974" citStr="Williams et al., 2015" startWordPosition="2496" endWordPosition="2499"> with p &lt; 0.05. The positive effect of restructuring is also apparent in frequency statistics. Table 3 shows that the baseline system severely undergenerates compounds and separated/infixed particle verbs. Binarization, compound splitting, and particle verb restructuring all contribute to bringing the distribution of compounds and particle verbs closer to the reference. In total, the restructured representation yields improvements of 1.4–1.8 BLEU over our baseline. The full system is competitive with official submissions to the WMT 2015 shared translation tasks. It outperforms our submission (Williams et al., 2015) by 0.4 BLEU, and outperforms other phrase-based and syntax-based submissions by 0.8 BLEU or more. The best reported result according to BLEU is an ensemble of Neural MT systems (Jean et al., 2015), which achieves 24.9 BLEU. In the human evaluation, both our submission and the Neural MT system were ranked 1–2 (out of 16), with no significant difference between them. 5.3 Synthetic LM Experiment We perform a synthetic experiment to test our claim that a dependency representation allows for the modelling of agreement between morphemes. For 200 rare compounds [that would be split by our compound s</context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Koehn, 2015</marker>
<rawString>Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, and Philipp Koehn. 2015. Edinburgh’s Syntax-Based Systems at WMT 2015. In Proceedings of the Tenth Workshop on Statistical Machine Translation, Lisbon, Portugal. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>